<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDAN&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#22312;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#33258;&#21160;&#29983;&#25104;&#38544;&#34109;&#30340;&#36234;&#29425;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#36234;&#29425;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#34109;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04451</link><description>&lt;p&gt;
AutoDAN: &#22312;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#29983;&#25104;&#38544;&#34109;&#30340;&#36234;&#29425;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. (arXiv:2310.04451v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDAN&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#22312;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#33258;&#21160;&#29983;&#25104;&#38544;&#34109;&#30340;&#36234;&#29425;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#36234;&#29425;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#34109;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26159;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#20915;&#31574;&#24037;&#20855;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24191;&#27867;&#23545;&#40784;&#32780;&#21019;&#24314;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25805;&#32437;&#25552;&#31034;&#26469;&#24341;&#21457;&#23545;&#40784;&#30340;LLM&#19981;&#24212;&#32473;&#20986;&#30340;&#24694;&#24847;&#36755;&#20986;&#12290;&#30740;&#31350;&#36234;&#29425;&#25552;&#31034;&#21487;&#20197;&#35753;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;LLM&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#23548;&#25105;&#20204;&#22914;&#20309;&#20445;&#25252;&#23427;&#20204;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#36234;&#29425;&#25216;&#26415;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;(1) &#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#25915;&#20987;&#22823;&#37327;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#25552;&#31034;&#65307;(2) &#38544;&#34109;&#24615;&#38382;&#39064;&#65292;&#25915;&#20987;&#20381;&#36182;&#22522;&#20110;&#26631;&#35760;&#30340;&#31639;&#27861;&#29983;&#25104;&#24120;&#24120;&#35821;&#20041;&#26080;&#24847;&#20041;&#30340;&#25552;&#31034;&#65292;&#23481;&#26131;&#36890;&#36807;&#22522;&#26412;&#22256;&#24785;&#24230;&#27979;&#35797;&#26816;&#27979;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24819;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65306;&#33021;&#21542;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#38544;&#34109;&#36234;&#29425;&#25552;&#31034;&#30340;&#26041;&#27861;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoDAN&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce Auto
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#35780;&#20272;&#29702;&#35770;&#21644;&#24212;&#28608;&#19982;&#24212;&#23545;&#36807;&#31243;&#38382;&#21367;&#65288;SCPQ&#65289;&#26469;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24773;&#24863;&#30340;&#24863;&#30693;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#30340;&#21709;&#24212;&#22312;&#35780;&#20272;&#21644;&#24212;&#23545;&#30340;&#21160;&#24577;&#26041;&#38754;&#19982;&#20154;&#31867;&#31867;&#20284;&#65292;&#20294;&#22312;&#20851;&#38190;&#35780;&#20272;&#32500;&#24230;&#19978;&#19982;&#39044;&#27979;&#30340;&#19981;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.04450</link><description>&lt;p&gt;
&#20351;&#29992;&#35780;&#20272;&#29702;&#35770;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24773;&#24863;&#30340;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Investigating Large Language Models' Perception of Emotion Using Appraisal Theory. (arXiv:2310.04450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#35780;&#20272;&#29702;&#35770;&#21644;&#24212;&#28608;&#19982;&#24212;&#23545;&#36807;&#31243;&#38382;&#21367;&#65288;SCPQ&#65289;&#26469;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24773;&#24863;&#30340;&#24863;&#30693;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#30340;&#21709;&#24212;&#22312;&#35780;&#20272;&#21644;&#24212;&#23545;&#30340;&#21160;&#24577;&#26041;&#38754;&#19982;&#20154;&#31867;&#31867;&#20284;&#65292;&#20294;&#22312;&#20851;&#38190;&#35780;&#20272;&#32500;&#24230;&#19978;&#19982;&#39044;&#27979;&#30340;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#26368;&#36817;&#20960;&#24180;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#19988;&#29616;&#22312;&#27491;&#34987;&#20844;&#20247;&#20351;&#29992;&#12290;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#19982;&#36825;&#20123;&#31995;&#32479;&#20114;&#21160;&#65292;&#25552;&#39640;&#25105;&#20204;&#23545;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#30340;&#29702;&#35299;&#23588;&#20026;&#20851;&#38190;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#23427;&#20204;&#23545;&#20154;&#31867;&#24515;&#29702;&#26041;&#38754;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24212;&#23545;&#21644;&#35780;&#20272;&#29702;&#35770;&#20013;&#30340;&#35780;&#20272;&#32500;&#24230;&#26469;&#35843;&#26597;&#23427;&#20204;&#23545;&#24773;&#24863;&#30340;&#24863;&#30693;&#65292;&#20351;&#29992;&#20102;&#24212;&#28608;&#19982;&#24212;&#23545;&#36807;&#31243;&#38382;&#21367;&#65288;SCPQ&#65289;&#12290;SCPQ&#26159;&#19968;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#20020;&#24202;&#24037;&#20855;&#65292;&#30001;&#22810;&#20010;&#38543;&#26102;&#38388;&#32780;&#28436;&#21464;&#19988;&#22312;&#20851;&#38190;&#35780;&#20272;&#21464;&#37327;&#65288;&#22914;&#21487;&#25511;&#24615;&#21644;&#21487;&#21464;&#24615;&#65289;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#25925;&#20107;&#32452;&#25104;&#12290;&#25105;&#20204;&#23558;SCPQ&#24212;&#29992;&#20110;OpenAI&#30340;&#19977;&#20010;&#26368;&#26032;LLM&#65288;davinci-003&#12289;ChatGPT&#21644;GPT-4&#65289;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#35780;&#20272;&#29702;&#35770;&#21644;&#20154;&#31867;&#25968;&#25454;&#30340;&#39044;&#27979;&#36827;&#34892;&#23545;&#27604;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#30340;&#21709;&#24212;&#22312;&#35780;&#20272;&#21644;&#24212;&#23545;&#30340;&#21160;&#24577;&#26041;&#38754;&#19982;&#20154;&#31867;&#31867;&#20284;&#65292;&#20294;&#23427;&#20204;&#30340;&#21709;&#24212;&#22312;&#20851;&#38190;&#35780;&#20272;&#32500;&#24230;&#19978;&#19982;&#39044;&#27979;&#30340;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) like ChatGPT have significantly advanced in recent years and are now being used by the general public. As more people interact with these systems, improving our understanding of these black box models is crucial, especially regarding their understanding of human psychological aspects. In this work, we investigate their emotion perception through the lens of appraisal and coping theory using the Stress and Coping Process Questionaire (SCPQ). SCPQ is a validated clinical instrument consisting of multiple stories that evolve over time and differ in key appraisal variables such as controllability and changeability. We applied SCPQ to three recent LLMs from OpenAI, davinci-003, ChatGPT, and GPT-4 and compared the results with predictions from the appraisal theory and human data. The results show that LLMs' responses are similar to humans in terms of dynamics of appraisal and coping, but their responses did not differ along key appraisal dimensions as predicted by
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04444</link><description>&lt;p&gt;
&#39764;&#27861;&#35789;&#26159;&#20160;&#20040;&#65311;LLM&#25552;&#31034;&#30340;&#25511;&#21046;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#22312;LLM&#30340;&#37096;&#32626;&#20013;&#26159;&#26377;&#25928;&#21644;&#37325;&#35201;&#30340;&#65292;&#20294;&#22312;&#25968;&#23398;&#19978;&#29702;&#35299;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35748;&#20026;&#26159;&#35843;&#33410;LLM&#36755;&#20986;&#20998;&#24067;&#30340;&#25511;&#21046;&#21464;&#37327;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;token&#24207;&#21015;&#65292;&#26159;&#21542;&#24635;&#23384;&#22312;&#19968;&#20010;&#25105;&#20204;&#21487;&#20197;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65311;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#26368;&#20248;&#25552;&#31034;&#31216;&#20026;&#39764;&#27861;&#35789;&#65292;&#22240;&#20026;&#28155;&#21152;&#25552;&#31034;&#20250;&#23548;&#33268;LLM&#36755;&#20986;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#22914;&#26524;&#23384;&#22312;&#39764;&#27861;&#35789;&#65292;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#23427;&#20204;&#65311;&#22914;&#26524;&#21487;&#20197;&#65292;&#23427;&#20204;&#30340;&#29305;&#24615;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#25511;&#21046;&#29702;&#35770;&#24212;&#29992;&#20110;&#33258;&#27880;&#24847;&#21147;&#22836;&#30340;&#20998;&#26512;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20854;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20989;&#25968;&#20026;&#21487;&#25511;&#21046;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#20511;&#37492;&#25511;&#21046;&#29702;&#35770;&#26469;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;$k-\epsilon$&#21487;&#25511;&#21046;&#24615;&#30340;&#25351;&#26631;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;MobQA&#65289;&#65292;&#26088;&#22312;&#35753;&#26234;&#33021;&#31995;&#32479;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#20851;&#20110;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#20026;&#31227;&#21160;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#33539;&#24335;&#21464;&#38761;&#12290;</title><link>http://arxiv.org/abs/2310.04443</link><description>&lt;p&gt;
&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;&#23637;&#26395;&#35770;&#25991;&#65289;
&lt;/p&gt;
&lt;p&gt;
Human Mobility Question Answering (Vision Paper). (arXiv:2310.04443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;MobQA&#65289;&#65292;&#26088;&#22312;&#35753;&#26234;&#33021;&#31995;&#32479;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#20851;&#20110;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#20026;&#31227;&#21160;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#33539;&#24335;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#31995;&#32479;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26681;&#25454;&#32473;&#23450;&#30340;&#30693;&#35782;&#28304;&#65288;&#20363;&#22914;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#22270;&#20687;&#65289;&#23398;&#20064;&#22238;&#31572;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#25366;&#25496;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#23545;&#20110;&#26234;&#33021;&#22478;&#24066;&#35268;&#21010;&#12289;&#30123;&#24773;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#31561;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24341;&#20837;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;MobQA&#65289;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#35753;&#26234;&#33021;&#31995;&#32479;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#12290;&#35813;&#20219;&#21153;&#20026;&#31227;&#21160;&#39044;&#27979;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#33539;&#24335;&#21464;&#38761;&#65292;&#24182;&#36827;&#19968;&#27493;&#20419;&#36827;&#20102;&#20154;&#31867;&#31227;&#21160;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25903;&#25345;&#36825;&#20010;&#26032;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#36825;&#31687;&#23637;&#26395;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#35774;&#35745;&#21644;&#19968;&#20010;&#28508;&#22312;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (QA) systems have attracted much attention from the artificial intelligence community as they can learn to answer questions based on the given knowledge source (e.g., images in visual question answering). However, the research into question answering systems with human mobility data remains unexplored. Mining human mobility data is crucial for various applications such as smart city planning, pandemic management, and personalised recommendation system. In this paper, we aim to tackle this gap and introduce a novel task, that is, human mobility question answering (MobQA). The aim of the task is to let the intelligent system learn from mobility data and answer related questions. This task presents a new paradigm change in mobility prediction research and further facilitates the research of human mobility recommendation systems. To better support this novel research topic, this vision paper also proposes an initial design of the dataset and a potential deep learning mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#31354;&#38388;-&#26102;&#38388;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#21644;&#20248;&#21270;&#27169;&#22359;&#65292;&#26088;&#22312;&#30740;&#31350;&#21644;&#25913;&#36827;&#20026;&#36135;&#36816;&#21345;&#36710;&#25552;&#20379;&#30005;&#27744;&#26356;&#25442;&#26381;&#21153;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#20998;&#26512;&#37325;&#22411;&#21345;&#36710;&#25968;&#25454;&#65292;&#32467;&#26524;&#34920;&#26126;&#39044;&#27979;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#26410;&#26469;&#20915;&#31574;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.04440</link><description>&lt;p&gt;
&#20026;&#36135;&#36816;&#21345;&#36710;&#25552;&#20379;&#30005;&#27744;&#26356;&#25442;&#26381;&#21153;&#30340;&#31354;&#38388;-&#26102;&#38388;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Facilitating Battery Swapping Services for Freight Trucks with Spatial-Temporal Demand Prediction. (arXiv:2310.04440v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#31354;&#38388;-&#26102;&#38388;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#21644;&#20248;&#21270;&#27169;&#22359;&#65292;&#26088;&#22312;&#30740;&#31350;&#21644;&#25913;&#36827;&#20026;&#36135;&#36816;&#21345;&#36710;&#25552;&#20379;&#30005;&#27744;&#26356;&#25442;&#26381;&#21153;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#20998;&#26512;&#37325;&#22411;&#21345;&#36710;&#25968;&#25454;&#65292;&#32467;&#26524;&#34920;&#26126;&#39044;&#27979;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#26410;&#26469;&#20915;&#31574;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27668;&#21270;&#37325;&#22411;&#21345;&#36710;&#20026;&#23454;&#29616;&#30899;&#20013;&#21644;&#30340;&#26410;&#26469;&#25552;&#20379;&#20102;&#37325;&#35201;&#26426;&#36935;&#65292;&#28982;&#32780;&#26377;&#38480;&#30005;&#27744;&#33021;&#37327;&#21644;&#37325;&#22411;&#21345;&#36710;&#30340;&#37325;&#37327;&#20351;&#24471;&#32493;&#33322;&#37324;&#31243;&#20943;&#23569;&#21644;&#20805;&#30005;&#26102;&#38388;&#24310;&#38271;&#25104;&#20026;&#22266;&#26377;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#30005;&#27744;&#26356;&#25442;&#26381;&#21153;&#25104;&#20026;&#36825;&#20123;&#21345;&#36710;&#30340;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#37319;&#29992;&#21452;&#37325;&#26041;&#27861;&#65292;&#30740;&#31350;&#21644;&#25552;&#39640;&#27492;&#31867;&#26381;&#21153;&#30340;&#25928;&#26524;&#12290;&#39318;&#20808;&#65292;&#37319;&#29992;&#31354;&#38388;-&#26102;&#38388;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#39044;&#27979;&#25509;&#19979;&#26469;&#20960;&#20010;&#23567;&#26102;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#35813;&#39044;&#27979;&#25351;&#23548;&#20248;&#21270;&#27169;&#22359;&#36827;&#34892;&#39640;&#25928;&#30340;&#30005;&#27744;&#20998;&#37197;&#21644;&#37096;&#32626;&#12290;&#36890;&#36807;&#20998;&#26512;2,500&#33521;&#37324;&#30340;&#20844;&#36335;&#32593;&#32476;&#19978;&#30340;&#37325;&#22411;&#21345;&#36710;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#20998;&#26512;&#20984;&#26174;&#39044;&#27979;/&#26426;&#22120;&#23398;&#20064;&#22312;&#20419;&#36827;&#26410;&#26469;&#20915;&#31574;&#20013;&#30340;&#20215;&#20540;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#23454;&#26045;&#30005;&#27744;&#26356;&#25442;&#30340;&#21021;&#26399;&#38454;&#27573;&#26159;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrifying heavy-duty trucks offers a substantial opportunity to curtail carbon emissions, advancing toward a carbon-neutral future. However, the inherent challenges of limited battery energy and the sheer weight of heavy-duty trucks lead to reduced mileage and prolonged charging durations. Consequently, battery-swapping services emerge as an attractive solution for these trucks. This paper employs a two-fold approach to investigate the potential and enhance the efficacy of such services. Firstly, spatial-temporal demand prediction models are adopted to predict the traffic patterns for the upcoming hours. Subsequently, the prediction guides an optimization module for efficient battery allocation and deployment. Analyzing the heavy-duty truck data on a highway network spanning over 2,500 miles, our model and analysis underscore the value of prediction/machine learning in facilitating future decision-makings. In particular, we find that the initial phase of implementing battery-swappin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#28436;&#36827;&#21382;&#31243;&#65292;&#21253;&#25324;&#26089;&#26399;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24341;&#20837;&#65292;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#20197;&#35299;&#20915;&#20559;&#35265;&#21644;&#26292;&#38706;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#36824;&#35752;&#35770;&#20102;&#24494;&#35843;&#31574;&#30053;&#12289;&#25511;&#21046;&#20195;&#30721;&#21644;&#22522;&#20110;&#27169;&#26495;&#30340;&#29983;&#25104;&#30340;&#37325;&#22823;&#36129;&#29486;&#65292;&#20197;&#21450;&#20844;&#24179;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#21644;&#20302;&#36164;&#28304;&#36866;&#24212;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04438</link><description>&lt;p&gt;
&#19968;&#20221;&#20851;&#20110;&#25552;&#31034;&#24037;&#31243;&#30340;&#31616;&#35201;&#21382;&#21490;: &#21033;&#29992;&#35821;&#35328;&#27169;&#22411; (arXiv:2310.04438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
A Brief History of Prompt: Leveraging Language Models. (arXiv:2310.04438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04438
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#28436;&#36827;&#21382;&#31243;&#65292;&#21253;&#25324;&#26089;&#26399;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24341;&#20837;&#65292;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#20197;&#35299;&#20915;&#20559;&#35265;&#21644;&#26292;&#38706;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#36824;&#35752;&#35770;&#20102;&#24494;&#35843;&#31574;&#30053;&#12289;&#25511;&#21046;&#20195;&#30721;&#21644;&#22522;&#20110;&#27169;&#26495;&#30340;&#29983;&#25104;&#30340;&#37325;&#22823;&#36129;&#29486;&#65292;&#20197;&#21450;&#20844;&#24179;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#21644;&#20302;&#36164;&#28304;&#36866;&#24212;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#30340;&#28436;&#36827;&#21382;&#31243;&#12290;&#20174;&#26089;&#26399;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#24320;&#22987;&#65292;&#25105;&#20204;&#36861;&#28335;&#20102;&#36825;&#20123;&#24180;&#26469;&#22609;&#36896;&#25552;&#31034;&#24037;&#31243;&#30340;&#20851;&#38190;&#21457;&#23637;&#12290;2015&#24180;&#24341;&#20837;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#21487;&#25511;&#24615;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36827;&#27493;&#12290;&#38543;&#21518;&#22312;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#26041;&#38754;&#30340;&#31361;&#30772;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25552;&#31034;&#24037;&#31243;&#65292;&#35299;&#20915;&#20102;&#26292;&#38706;&#20559;&#24046;&#21644;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#37325;&#28857;&#32771;&#23519;&#20102;2018&#24180;&#21644;2019&#24180;&#30340;&#37325;&#22823;&#36129;&#29486;&#65292;&#38598;&#20013;&#22312;&#24494;&#35843;&#31574;&#30053;&#12289;&#25511;&#21046;&#20195;&#30721;&#21644;&#22522;&#20110;&#27169;&#26495;&#30340;&#29983;&#25104;&#19978;&#12290;&#26412;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#20197;&#21450;&#20302;&#36164;&#28304;&#36866;&#24212;&#30340;&#26085;&#30410;&#37325;&#35201;&#24615;&#12290;&#22312;2020&#24180;&#21644;2021&#24180;&#65292;&#19978;&#19979;&#25991;&#25552;&#31034;&#21644;&#36801;&#31227;&#23398;&#20064;&#21464;&#24471;&#31361;&#20986;&#65292;&#32780;2022&#24180;&#21644;2023&#24180;&#35265;&#35777;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive exploration of the evolution of prompt engineering and generation in the field of natural language processing (NLP). Starting from the early language models and information retrieval systems, we trace the key developments that have shaped prompt engineering over the years. The introduction of attention mechanisms in 2015 revolutionized language understanding, leading to advancements in controllability and context-awareness. Subsequent breakthroughs in reinforcement learning techniques further enhanced prompt engineering, addressing issues like exposure bias and biases in generated text. We examine the significant contributions in 2018 and 2019, focusing on fine-tuning strategies, control codes, and template-based generation. The paper also discusses the growing importance of fairness, human-AI collaboration, and low-resource adaptation. In 2020 and 2021, contextual prompting and transfer learning gained prominence, while 2022 and 2023 witnessed the e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#22270;&#20687;&#21453;&#28436;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#27969;&#27169;&#22411;&#65292;&#22312;&#20943;&#23569;&#25163;&#21160;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04432</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#22270;&#20687;&#21453;&#28436;&#26041;&#27861;&#65306;&#36890;&#36807;&#27969;&#36827;&#34892;
&lt;/p&gt;
&lt;p&gt;
Training-free Linear Image Inversion via Flows. (arXiv:2310.04432v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04432
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#22270;&#20687;&#21453;&#28436;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#27969;&#27169;&#22411;&#65292;&#22312;&#20943;&#23569;&#25163;&#21160;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#21453;&#28436;&#26041;&#27861;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#36866;&#24403;&#20462;&#25913;&#26469;&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35843;&#20248;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#20808;&#21069;&#26041;&#27861;&#24050;&#32463;&#25506;&#32034;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#20294;&#20173;&#38656;&#35201;&#25163;&#21160;&#35843;&#25972;&#35768;&#22810;&#36229;&#21442;&#25968;&#26469;&#24212;&#23545;&#19981;&#21516;&#30340;&#36870;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27969;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#21453;&#28436;&#30340;&#26080;&#38656;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#27969;&#21305;&#37197;&#27169;&#22411;&#30340;&#31616;&#27905;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#20351;&#29992;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#21152;&#26435;&#26041;&#26696;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#25163;&#21160;&#35843;&#25972;&#30340;&#24037;&#20316;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#20027;&#35201;&#28304;&#22836;&#27762;&#21462;&#28789;&#24863;&#65306;&#23558;&#20808;&#21069;&#30340;&#26799;&#24230;&#26657;&#27491;&#26041;&#27861;&#24212;&#29992;&#20110;&#27969;&#39046;&#22495;&#65292;&#20197;&#21450;&#22522;&#20110;&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#36335;&#24452;&#30340;&#27714;&#35299;&#22120;&#26041;&#26696;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#24191;&#27867;&#21487;&#29992;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25193;&#25955;&#27169;&#22411;&#23454;&#38469;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#36870;&#38382;&#39064;&#19978;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training-free linear inversion involves the use of a pretrained generative model and -- through appropriate modifications to the generation process -solving inverse problems without any finetuning of the generative model. While recent prior methods have explored the use of diffusion models, they still require the manual tuning of many hyperparameters for different inverse problems. In this work, we propose a training-free method for image inversion using pretrained flow models, leveraging the simplicity and efficiency of Flow Matching models, using theoretically-justified weighting schemes and thereby significantly reducing the amount of manual tuning. In particular, we draw inspiration from two main sources: adopting prior gradient correction methods to the flow regime, and a solver scheme based on conditional Optimal Transport paths. As pretrained diffusion models are widely accessible, we also show how to practically adapt diffusion models for our method. Empirically, our approach
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#25968;&#23383;&#39057;&#29575;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.04431</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#33021;&#21542;&#35745;&#31639;&#25968;&#23383;&#39057;&#29575;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can neural networks count digit frequency?. (arXiv:2310.04431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#25968;&#23383;&#39057;&#29575;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#27604;&#36739;&#19981;&#21516;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#32473;&#23450;&#25968;&#23383;&#20013;&#27599;&#20010;&#25968;&#23383;&#20986;&#29616;&#39057;&#29575;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#23427;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26377;&#22810;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#33719;&#21462;&#30446;&#26631;&#23545;&#35937;&#30340;&#39057;&#29575;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#28151;&#21512;&#12290;&#25105;&#20204;&#31934;&#24515;&#21019;&#24314;&#20102;&#33258;&#24049;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35266;&#23519;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#25351;&#26631;&#35780;&#20272;&#27599;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#25152;&#20351;&#29992;&#30340;&#24615;&#33021;&#25351;&#26631;&#21253;&#25324;&#22238;&#24402;&#35780;&#20272;&#20013;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65292;&#20197;&#21450;&#20998;&#31867;&#24615;&#33021;&#35780;&#20272;&#20013;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#20250;&#36807;&#25311;&#21512;&#21040;&#25968;&#25454;&#38598;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#22266;&#26377;&#20559;&#24046;&#65292;&#26080;&#27861;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#31070;&#32463;&#32593;&#32476;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we aim to compare the performance of different classical machine learning models and neural networks in identifying the frequency of occurrence of each digit in a given number. It has various applications in machine learning and computer vision, e.g. for obtaining the frequency of a target object in a visual scene. We considered this problem as a hybrid of classification and regression tasks. We carefully create our own datasets to observe systematic differences between different methods. We evaluate each of the methods using different metrics across multiple datasets.The metrics of performance used were the root mean squared error and mean absolute error for regression evaluation, and accuracy for classification performance evaluation. We observe that decision trees and random forests overfit to the dataset, due to their inherent bias, and are not able to generalize well. We also observe that the neural networks significantly outperform the classical machine learning
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25351;&#20986;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#22312;&#24314;&#31569;&#34892;&#19994;&#20013;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#30340;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#24378;&#35843;&#20102;GenAI&#26089;&#26399;&#37319;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04427</link><description>&lt;p&gt;
&#24314;&#31569;&#34892;&#19994;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Generative AI in the Construction Industry: Opportunities &amp; Challenges. (arXiv:2310.04427v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04427
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25351;&#20986;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#22312;&#24314;&#31569;&#34892;&#19994;&#20013;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#30340;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#24378;&#35843;&#20102;GenAI&#26089;&#26399;&#37319;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#25913;&#21464;&#20102;&#35768;&#22810;&#34892;&#19994;&#30340;&#20570;&#27861;&#65292;&#20294;&#24314;&#31569;&#34892;&#19994;&#30340;&#37319;&#29992;&#36828;&#36828;&#28382;&#21518;&#12290;&#26368;&#36817;&#65292;&#20687;OpenAI&#30340;GPT&#12289;Google&#30340;PaLM&#21644;Meta&#30340;Llama&#36825;&#26679;&#30340;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#21644;&#36805;&#36895;&#37319;&#29992;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#24182;&#24341;&#36215;&#20102;&#20840;&#29699;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#28608;&#22686;&#32570;&#20047;&#30740;&#31350;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#22312;&#24314;&#31569;&#39046;&#22495;&#23454;&#26045;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#30693;&#35782;&#31354;&#30333;&#12290;&#36825;&#20984;&#26174;&#20102;&#25506;&#32034;GenAI&#25972;&#21512;&#21069;&#26223;&#21644;&#22797;&#26434;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#23545;&#20110;&#20248;&#21270;&#24314;&#31569;&#34892;&#19994;&#26089;&#26399;&#37319;&#29992;GenAI&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;GenAI&#26681;&#25454;&#23545;&#29616;&#26377;&#20869;&#23481;&#30340;&#23398;&#20064;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#20869;&#23481;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20004;&#20010;&#25351;&#23548;&#24615;&#38382;&#39064;&#65306;GenAI&#22312;&#24314;&#31569;&#34892;&#19994;&#23558;&#20250;&#24102;&#26469;&#20160;&#20040;&#26410;&#26469;&#65311;
&lt;/p&gt;
&lt;p&gt;
In the last decade, despite rapid advancements in artificial intelligence (AI) transforming many industry practices, construction largely lags in adoption. Recently, the emergence and rapid adoption of advanced large language models (LLM) like OpenAI's GPT, Google's PaLM, and Meta's Llama have shown great potential and sparked considerable global interest. However, the current surge lacks a study investigating the opportunities and challenges of implementing Generative AI (GenAI) in the construction sector, creating a critical knowledge gap for researchers and practitioners. This underlines the necessity to explore the prospects and complexities of GenAI integration. Bridging this gap is fundamental to optimizing GenAI's early-stage adoption within the construction sector. Given GenAI's unprecedented capabilities to generate human-like content based on learning from existing content, we reflect on two guiding questions: What will the future bring for GenAI in the construction industry?
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#22522;&#22240;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;&#65288;GRNN&#65289;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#65292;&#29992;&#20110;&#31283;&#23450;&#24615;&#20998;&#26512;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#24212;&#29992;&#22312;&#29983;&#29289;&#20154;&#24037;&#26234;&#33021;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.04424</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#22240;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#31283;&#23450;&#24615;&#20998;&#26512;&#22312;&#29983;&#29289;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Stability Analysis of Non-Linear Classifiers using Gene Regulatory Neural Network for Biological AI. (arXiv:2310.04424v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#22522;&#22240;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;&#65288;GRNN&#65289;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#65292;&#29992;&#20110;&#31283;&#23450;&#24615;&#20998;&#26512;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#24212;&#29992;&#22312;&#29983;&#29289;&#20154;&#24037;&#26234;&#33021;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#32454;&#32990;&#30340;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65288;GRN&#65289;&#31649;&#29702;&#30528;&#35768;&#22810;&#20851;&#38190;&#21151;&#33021;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#36866;&#24212;&#21644;&#22312;&#19981;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#29983;&#23384;&#12290;&#23545;GRN&#30340;&#20180;&#32454;&#35266;&#23519;&#34920;&#26126;&#65292;&#20854;&#32467;&#26500;&#21644;&#25805;&#20316;&#21407;&#21017;&#31867;&#20284;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#65292;&#20026;&#29983;&#29289;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#22522;&#22240;&#30340;&#36716;&#24405;&#21644;&#32763;&#35793;&#36807;&#31243;&#20013;&#65292;&#22522;&#20110;&#36716;&#24405;&#22240;&#23376;&#36755;&#20837;&#30340;&#29305;&#24615;&#31867;&#20284;&#20110;S&#22411;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21452;&#23618;&#36716;&#24405;-&#32763;&#35793;&#21270;&#23398;&#21453;&#24212;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#22240;-&#24863;&#30693;&#22120;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;GRN&#36716;&#25442;&#20026;&#22522;&#22240;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;&#65288;GRNN&#65289;&#12290;&#25105;&#20204;&#23545;&#23436;&#20840;&#36830;&#25509;&#30340;GRNN&#23376;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#22522;&#22240;-&#24863;&#30693;&#22120;&#36827;&#34892;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20250;&#20135;&#29983;&#21487;&#38752;&#35745;&#31639;&#24615;&#33021;&#30340;&#26102;&#38388;&#21644;&#31283;&#23450;&#27987;&#24230;&#36755;&#20986;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Gene Regulatory Network (GRN) of biological cells governs a number of key functionalities that enables them to adapt and survive through different environmental conditions. Close observation of the GRN shows that the structure and operational principles resembles an Artificial Neural Network (ANN), which can pave the way for the development of Biological Artificial Intelligence. In particular, a gene's transcription and translation process resembles a sigmoidal-like property based on transcription factor inputs. In this paper, we develop a mathematical model of gene-perceptron using a dual-layered transcription-translation chemical reaction model, enabling us to transform a GRN into a Gene Regulatory Neural Network (GRNN). We perform stability analysis for each gene-perceptron within the fully-connected GRNN sub network to determine temporal as well as stable concentration outputs that will result in reliable computing performance. We focus on a non-linear classifier application fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#23398;&#20064;&#34920;&#31034;&#30340;&#19981;&#21516;&#36741;&#21161;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#29615;&#22659;&#19978;&#35757;&#32451;&#25968;&#30334;&#20010;&#26234;&#33021;&#20307;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#23545;&#29615;&#22659;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#22238;&#25253;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.04241</link><description>&lt;p&gt;
&#27604;&#36739;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36741;&#21161;&#20219;&#21153;&#30340;&#23398;&#20064;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comparing Auxiliary Tasks for Learning Representations for Reinforcement Learning. (arXiv:2310.04241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#23398;&#20064;&#34920;&#31034;&#30340;&#19981;&#21516;&#36741;&#21161;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#29615;&#22659;&#19978;&#35757;&#32451;&#25968;&#30334;&#20010;&#26234;&#33021;&#20307;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#23545;&#29615;&#22659;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#22238;&#25253;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#29615;&#22659;&#22238;&#25253;&#65292;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#19968;&#31181;&#30452;&#25509;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19968;&#20010;&#19982;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19981;&#21516;&#30340;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#19968;&#20010;&#29420;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#34920;&#31034;&#12290;&#34429;&#28982;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#36825;&#26679;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#20294;&#22312;&#20856;&#22411;&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#29615;&#22659;&#19978;&#36827;&#34892;&#27604;&#36739;&#35745;&#31639;&#37327;&#22823;&#19988;&#25454;&#25105;&#20204;&#25152;&#30693;&#20197;&#21069;&#26410;&#36827;&#34892;&#36807;&#12290;&#26412;&#25991;&#22312;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#30340;&#25968;&#30334;&#20010;&#26234;&#33021;&#20307;&#19978;&#36827;&#34892;&#20102;&#36825;&#26679;&#30340;&#36741;&#21161;&#20219;&#21153;&#27604;&#36739;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20174;&#31616;&#21333;&#25670;&#32447;&#21040;&#22797;&#26434;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#22238;&#25253;&#30340;&#21487;&#33021;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#23545;&#29615;&#22659;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning state representations has gained steady popularity in reinforcement learning (RL) due to its potential to improve both sample efficiency and returns on many environments. A straightforward and efficient method is to generate representations with a distinct neural network trained on an auxiliary task, i.e. a task that differs from the actual RL task. While a whole range of such auxiliary tasks has been proposed in the literature, a comparison on typical continuous control benchmark environments is computationally expensive and has, to the best of our knowledge, not been performed before. This paper presents such a comparison of common auxiliary tasks, based on hundreds of agents trained with state-of-the-art off-policy RL algorithms. We compare possible improvements in both sample efficiency and returns for environments ranging from simple pendulum to a complex simulated robotics task. Our findings show that representation learning with auxiliary tasks is beneficial for environ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#30340;&#32858;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.04171</link><description>&lt;p&gt;
&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection. (arXiv:2310.04171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#30340;&#32858;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27450;&#35784;&#26816;&#27979;&#26088;&#22312;&#21457;&#29616;&#27450;&#35784;&#32773;&#36890;&#36807;&#30041;&#19979;&#20551;&#35780;&#35770;&#25110;&#36827;&#34892;&#24322;&#24120;&#20132;&#26131;&#27450;&#39575;&#20854;&#20182;&#29992;&#25143;&#12290;&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#23558;&#36825;&#20010;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#21253;&#21547;&#20004;&#20010;&#31867;&#21035;&#65288;&#27450;&#35784;&#25110;&#27491;&#24120;&#65289;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#23454;&#38469;&#19990;&#30028;&#22270;&#34920;&#20013;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#20851;&#31995;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#24314;&#35758;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#32858;&#21512;&#33410;&#28857;&#34920;&#31034;&#65292;&#35813;&#20989;&#25968;&#20026;&#27599;&#20010;&#20851;&#31995;&#20998;&#37197;&#19981;&#21516;&#30340;&#27880;&#24847;&#31995;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20197;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#36825;&#26377;&#21161;&#20110;&#25552;&#39640;&#22312;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#19978;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#25152;&#26377;&#32858;&#21512;&#36807;&#31243;&#20013;&#37319;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fraud detection aims to discover fraudsters deceiving other users by, for example, leaving fake reviews or making abnormal transactions. Graph-based fraud detection methods consider this task as a classification problem with two classes: frauds or normal. We address this problem using Graph Neural Networks (GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based on the observation that many real-world graphs include different types of relations, we propose to learn a node representation per relation and aggregate the node representations using a learnable attention function that assigns a different attention coefficient to each relation. Furthermore, we combine the node representations from different layers to consider both the local and global structures of a target node, which is beneficial to improving the performance of fraud detection on graphs with heterophily. By employing dynamic graph attention in all the aggregation processes, our method adaptively comput
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24605;&#32500;&#20256;&#25773;&#65288;TP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#31867;&#27604;&#38382;&#39064;&#21644;&#21033;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03965</link><description>&lt;p&gt;
&#24605;&#32500;&#20256;&#25773;&#65306;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#26041;&#27861;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22797;&#26434;&#25512;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models. (arXiv:2310.03965v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03965
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24605;&#32500;&#20256;&#25773;&#65288;TP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#31867;&#27604;&#38382;&#39064;&#21644;&#21033;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#26080;&#27861;&#37325;&#29992;&#35299;&#20915;&#31867;&#20284;&#38382;&#39064;&#30340;&#35265;&#35299;&#65292;&#24182;&#19988;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#32047;&#31215;&#20102;&#38169;&#35823;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;LLMs&#20174;&#38646;&#24320;&#22987;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#20256;&#25773;&#8221;&#65288;TP&#65289;&#65292;&#23427;&#25506;&#32034;&#31867;&#20284;&#38382;&#39064;&#24182;&#21033;&#29992;&#23427;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;LLMs&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#31867;&#27604;&#38382;&#39064;&#19982;&#36755;&#20837;&#38382;&#39064;&#30456;&#20851;&#65292;&#20855;&#26377;&#21487;&#37325;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#23558;&#35299;&#20915;&#20808;&#21069;&#31867;&#20284;&#38382;&#39064;&#30340;&#35265;&#35299;&#20256;&#25773;&#20197;&#28608;&#21457;&#26032;&#30340;&#38382;&#39064;&#35299;&#20915;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;TP&#39318;&#20808;&#25552;&#31034;LLMs&#25552;&#20986;&#24182;&#35299;&#20915;&#19968;&#32452;&#19982;&#36755;&#20837;&#38382;&#39064;&#30456;&#20851;&#30340;&#31867;&#27604;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;TP&#37325;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#32467;&#26524;&#30452;&#25509;&#20135;&#29983;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#25110;&#32773;&#25512;&#23548;&#19968;&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \textit{from scratch}. To address these issues, we propose \textbf{\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs. These analogous problems are related to the input one, with reusable solutions and problem-solving strategies. Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03912</link><description>&lt;p&gt;
RTDK-BO&#65306;&#20855;&#26377;Reinforced Transformer&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels. (arXiv:2310.03912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20195;&#29702;&#25351;&#23548;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#23545;&#20110;&#39640;&#32500;&#40657;&#30418;&#20248;&#21270;&#38750;&#24120;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#22312;&#24037;&#19994;&#35774;&#35745;&#21644;&#31185;&#23398;&#35745;&#31639;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#21333;&#20989;&#25968;&#20248;&#21270;&#21644;&#23569;&#26679;&#26412;&#22810;&#30446;&#26631;&#20248;&#21270;&#19978;&#24341;&#20837;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#23569;&#26679;&#26412;&#25216;&#26415;&#20063;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#32039;&#23494;&#30456;&#20851;&#30446;&#26631;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#32467;&#21512;&#20102;&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25913;&#36827;&#20102;GP&#20195;&#29702;&#30340;&#24314;&#27169;&#33021;&#21147;&#19982;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;DKL&#20013;&#26469;&#25913;&#36827;&#20803;&#23398;&#20064;BO&#20195;&#29702;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;BO&#36807;&#31243;&#20013;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;Transformer&#28145;&#24230;&#26680;&#26041;&#27861;&#19982;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#25552;&#39640;BO&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO), guided by Gaussian process (GP) surrogates, has proven to be an invaluable technique for efficient, high-dimensional, black-box optimization, a critical problem inherent to many applications such as industrial design and scientific computing. Recent contributions have introduced reinforcement learning (RL) to improve the optimization performance on both single function optimization and \textit{few-shot} multi-objective optimization. However, even few-shot techniques fail to exploit similarities shared between closely related objectives. In this paper, we combine recent developments in Deep Kernel Learning (DKL) and attention-based Transformer models to improve the modeling powers of GP surrogates with meta-learning. We propose a novel method for improving meta-learning BO surrogates by incorporating attention mechanisms into DKL, empowering the surrogates to adapt to contextual information gathered during the BO process. We combine this Transformer Deep Kern
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CoHeat&#31639;&#27861;&#65292;&#19968;&#31181;&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#21644;&#20851;&#32852;&#20449;&#24687;&#65292;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#30340;&#20542;&#26012;&#65292;&#24182;&#26377;&#25928;&#22320;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.03813</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#65306;&#22522;&#20110;&#27969;&#34892;&#24230;&#30340;&#32858;&#21512;&#21644;&#35838;&#31243;&#21152;&#28909;
&lt;/p&gt;
&lt;p&gt;
Accurate Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum Heating. (arXiv:2310.03813v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CoHeat&#31639;&#27861;&#65292;&#19968;&#31181;&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#21644;&#20851;&#32852;&#20449;&#24687;&#65292;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#30340;&#20542;&#26012;&#65292;&#24182;&#26377;&#25928;&#22320;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#20934;&#30830;&#22320;&#21521;&#29992;&#25143;&#25512;&#33616;&#20919;&#21551;&#21160;&#25414;&#32465;&#65311;&#25414;&#32465;&#25512;&#33616;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#26032;&#24314;&#25414;&#32465;&#19981;&#26029;&#20986;&#29616;&#20197;&#28385;&#36275;&#21508;&#31181;&#33829;&#38144;&#30446;&#30340;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20043;&#21069;&#27809;&#26377;&#30740;&#31350;&#28041;&#21450;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20919;&#21551;&#21160;&#29289;&#21697;&#25512;&#33616;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#21382;&#21490;&#20449;&#24687;&#65292;&#21363;&#20351;&#23545;&#20110;&#19981;&#21463;&#27426;&#36814;&#30340;&#25414;&#32465;&#20063;&#26159;&#22914;&#27492;&#65292;&#26080;&#27861;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#39640;&#24230;&#20542;&#26012;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoHeat&#65288;&#22522;&#20110;&#27969;&#34892;&#24230;&#30340;&#32858;&#21512;&#21644;&#35838;&#31243;&#21152;&#28909;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#26041;&#27861;&#12290;CoHeat&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#20449;&#24687;&#21644;&#20851;&#32852;&#20449;&#24687;&#26469;&#20272;&#35745;&#29992;&#25143;&#19982;&#25414;&#32465;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#30340;&#39640;&#24230;&#20542;&#26012;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;CoHeat&#36824;&#36890;&#36807;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#32858;&#21512;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#22320;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we accurately recommend cold-start bundles to users? The cold-start problem in bundle recommendation is critical in practical scenarios since new bundles are continuously created for various marketing purposes. Despite its importance, no previous studies have addressed cold-start bundle recommendation. Moreover, existing methods for cold-start item recommendation overly rely on historical information, even for unpopular bundles, failing to tackle the primary challenge of the highly skewed distribution of bundle interactions. In this work, we propose CoHeat (Popularity-based Coalescence and Curriculum Heating), an accurate approach for the cold-start bundle recommendation. CoHeat tackles the highly skewed distribution of bundle interactions by incorporating both historical and affiliation information based on the bundle's popularity when estimating the user-bundle relationship. Furthermore, CoHeat effectively learns latent representations by exploiting curriculum learning and co
&lt;/p&gt;</description></item><item><title>Know2BIO&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#21452;&#35270;&#22270;&#28436;&#21464;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#22522;&#20934;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#26679;&#21270;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;KG&#30340;&#23454;&#20307;&#23545;&#40784;&#12289;&#25193;&#23637;&#24615;&#21644;&#26356;&#26032;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.03221</link><description>&lt;p&gt;
Know2BIO: &#19968;&#20010;&#20840;&#38754;&#30340;&#21452;&#35270;&#22270;&#28436;&#21464;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Know2BIO: A Comprehensive Dual-View Benchmark for Evolving Biomedical Knowledge Graphs. (arXiv:2310.03221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03221
&lt;/p&gt;
&lt;p&gt;
Know2BIO&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#21452;&#35270;&#22270;&#28436;&#21464;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#22522;&#20934;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#26679;&#21270;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;KG&#30340;&#23454;&#20307;&#23545;&#40784;&#12289;&#25193;&#23637;&#24615;&#21644;&#26356;&#26032;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#24050;&#32463;&#25104;&#20026;&#34920;&#31034;&#21644;&#38598;&#25104;&#22797;&#26434;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20174;&#22810;&#26679;&#21270;&#30340;&#26469;&#28304;&#32452;&#35013;KG&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#23454;&#20307;&#23545;&#40784;&#65292;&#21487;&#25193;&#23637;&#24615;&#20197;&#21450;&#38656;&#35201;&#19981;&#26029;&#26356;&#26032;&#20197;&#36319;&#19978;&#31185;&#23398;&#36827;&#23637;&#12290;&#27492;&#22806;&#65292;&#30693;&#35782;&#22270;&#35889;&#30340;&#20195;&#34920;&#33021;&#21147;&#36890;&#24120;&#21463;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Know2BIO&#65292;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#36890;&#29992;&#24322;&#26500;KG&#22522;&#20934;&#12290;Know2BIO&#25972;&#21512;&#20102;&#26469;&#33258;30&#20010;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#25429;&#25417;&#20102;11&#20010;&#29983;&#29289;&#21307;&#23398;&#31867;&#21035;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#23427;&#30446;&#21069;&#21253;&#21547;&#32422;219,000&#20010;&#33410;&#28857;&#21644;&#32422;6,200,000&#20010;&#36793;&#12290;Know2BIO&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#25351;&#31034;&#33258;&#21160;&#26356;&#26032;&#20197;&#21453;&#26144;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#20013;&#30340;&#26368;&#26032;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;Know2BIO&#36824;&#38468;&#24102;&#22810;&#27169;&#24577;&#25968;&#25454;&#65306;&#21253;&#25324;&#25991;&#26412;&#25551;&#36848;&#12289;&#34507;&#30333;&#36136;&#21644;&#21270;&#21512;&#29289;&#24207;&#21015;&#31561;&#33410;&#28857;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) have emerged as a powerful framework for representing and integrating complex biomedical information. However, assembling KGs from diverse sources remains a significant challenge in several aspects, including entity alignment, scalability, and the need for continuous updates to keep pace with scientific advancements. Moreover, the representative power of KGs is often limited by the scarcity of multi-modal data integration. To overcome these challenges, we propose Know2BIO, a general-purpose heterogeneous KG benchmark for the biomedical domain. Know2BIO integrates data from 30 diverse sources, capturing intricate relationships across 11 biomedical categories. It currently consists of ~219,000 nodes and ~6,200,000 edges. Know2BIO is capable of user-directed automated updating to reflect the latest knowledge in biomedical science. Furthermore, Know2BIO is accompanied by multi-modal data: node features including text descriptions, protein and compound sequences and s
&lt;/p&gt;</description></item><item><title>&#22823;&#33041;&#20855;&#26377;&#19968;&#31995;&#21015;&#37325;&#22797;&#30340;&#35268;&#33539;&#35745;&#31639;&#21333;&#20803;&#65292;&#20294;&#31070;&#32463;&#34920;&#31034;&#26159;&#20998;&#24067;&#24335;&#30340;&#65292;&#22240;&#27492;&#22914;&#20309;&#23450;&#20041;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#20174;&#22823;&#35268;&#27169;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#20013;&#25512;&#26029;&#20986;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;&#22312;&#31639;&#27861;&#32423;&#21035;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#32467;&#26500;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#20449;&#24687;&#20256;&#36882;&#31639;&#27861;&#12290;&#36890;&#36807;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#26102;&#38388;&#24207;&#21015;&#65292;&#21487;&#20197;&#25214;&#21040;&#31070;&#32463;&#27963;&#21160;&#19982;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#22240;&#26524;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.03186</link><description>&lt;p&gt;
&#25512;&#27979;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Inferring Inference. (arXiv:2310.03186v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03186
&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#20855;&#26377;&#19968;&#31995;&#21015;&#37325;&#22797;&#30340;&#35268;&#33539;&#35745;&#31639;&#21333;&#20803;&#65292;&#20294;&#31070;&#32463;&#34920;&#31034;&#26159;&#20998;&#24067;&#24335;&#30340;&#65292;&#22240;&#27492;&#22914;&#20309;&#23450;&#20041;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#20174;&#22823;&#35268;&#27169;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#20013;&#25512;&#26029;&#20986;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;&#22312;&#31639;&#27861;&#32423;&#21035;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#32467;&#26500;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#20449;&#24687;&#20256;&#36882;&#31639;&#27861;&#12290;&#36890;&#36807;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#26102;&#38388;&#24207;&#21015;&#65292;&#21487;&#20197;&#25214;&#21040;&#31070;&#32463;&#27963;&#21160;&#19982;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#22240;&#26524;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#30005;&#36335;&#22270;&#26696;&#34920;&#26126;&#22823;&#33041;&#20855;&#26377;&#19968;&#31995;&#21015;&#37325;&#22797;&#30340;&#35268;&#33539;&#35745;&#31639;&#21333;&#20803;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#34920;&#31034;&#26159;&#20998;&#24067;&#24335;&#30340;&#65292;&#22240;&#27492;&#30456;&#20851;&#35745;&#31639;&#21487;&#33021;&#20165;&#19982;&#21333;&#20010;&#31070;&#32463;&#20803;&#21464;&#25442;&#38388;&#25509;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#23450;&#20041;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#23558;&#31070;&#32463;&#35745;&#31639;&#30340;&#35268;&#33539;&#21644;&#31639;&#27861;&#29702;&#35770;&#25972;&#21512;&#21040;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#20174;&#22823;&#35268;&#27169;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#20013;&#25512;&#26029;&#20986;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;&#22312;&#35268;&#33539;&#32423;&#21035;&#19978;&#65292;&#25105;&#20204;&#20551;&#35774;&#22823;&#33041;&#21019;&#24314;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20869;&#37096;&#27169;&#22411;&#65292;&#20551;&#35774;&#35299;&#37322;&#20854;&#24863;&#23448;&#36755;&#20837;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#24863;&#23448;&#36755;&#20837;&#26469;&#25512;&#26029;&#28508;&#22312;&#21407;&#22240;&#12290;&#22312;&#31639;&#27861;&#32423;&#21035;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#36825;&#20010;&#25512;&#29702;&#36807;&#31243;&#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#32467;&#26500;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#20449;&#24687;&#20256;&#36882;&#31639;&#27861;&#12290;&#36890;&#36807;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#26102;&#38388;&#24207;&#21015;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#65288;i&#65289;&#31070;&#32463;&#27963;&#21160;&#19982;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#22240;&#26524;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patterns of microcircuitry suggest that the brain has an array of repeated canonical computational units. Yet neural representations are distributed, so the relevant computations may only be related indirectly to single-neuron transformations. It thus remains an open challenge how to define canonical distributed computations. We integrate normative and algorithmic theories of neural computation into a mathematical framework for inferring canonical distributed computations from large-scale neural activity patterns. At the normative level, we hypothesize that the brain creates a structured internal model of its environment, positing latent causes that explain its sensory inputs, and uses those sensory inputs to infer the latent causes. At the algorithmic level, we propose that this inference process is a nonlinear message-passing algorithm on a graph-structured model of the world. Given a time series of neural activity during a perceptual inference task, our framework finds (i) the neura
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.03094</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#31181;&#24378;&#22823;&#30340;&#24615;&#33021;&#36890;&#24120;&#20276;&#38543;&#30528;&#20351;&#29992;&#20184;&#36153;API&#26381;&#21153;&#30340;&#39640;&#26114;&#36153;&#29992;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#21160;&#26426;&#26159;&#20026;&#20102;&#30740;&#31350;&#26500;&#24314;LLM&#32423;&#32852;&#20197;&#33410;&#32422;&#20351;&#29992;LLM&#30340;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#36827;&#34892;&#25512;&#29702;&#65288;&#20363;&#22914;&#25968;&#23398;&#12289;&#22240;&#26524;&#25512;&#29702;&#65289;&#20219;&#21153;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#32423;&#32852;&#31649;&#36947;&#36981;&#24490;&#19968;&#20010;&#30452;&#35266;&#30340;&#24605;&#24819;&#65292;&#21363;&#31616;&#21333;&#30340;&#38382;&#39064;&#21487;&#20197;&#30001;&#19968;&#20010;&#26356;&#24369;&#20294;&#26356;&#23454;&#24800;&#30340;LLM&#26469;&#35299;&#20915;&#65292;&#32780;&#21482;&#26377;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#25165;&#38656;&#35201;&#26356;&#24378;&#22823;&#12289;&#26356;&#26114;&#36149;&#30340;LLM&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#20915;&#31574;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#26356;&#24369;&#30340;LLM&#30340;&#8220;&#31572;&#26696;&#19968;&#33268;&#24615;&#8221;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31572;&#26696;&#37319;&#26679;&#21644;&#19968;&#33268;&#24615;&#26816;&#26597;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#24605;&#32500;&#34920;&#31034;&#65288;&#21363;&#36830;&#32493;&#24605;&#32500;&#21644;&#31243;&#24207;&#24605;&#32500;&#65289;&#30340;&#28151;&#21512;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5-turbo&#21644;GPT-4&#20316;&#20026;&#36739;&#24369;&#30340;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#19981;&#21516;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02469</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#33391;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Good Privacy Protection Learners. (arXiv:2310.02469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#19981;&#21516;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20351;&#29992;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#21019;&#24314;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#25935;&#24863;&#30340;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#65288;PII&#65289;&#12290;&#22312;&#27809;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#24494;&#35843; LLMs &#20250;&#23384;&#22312;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26032;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#27604;&#22914;&#35821;&#26009;&#24211;&#31574;&#23637;&#12289;&#22522;&#20110;&#24809;&#32602;&#30340;&#38750;&#27010;&#28982;&#24615;&#35757;&#32451;&#25439;&#22833;&#20197;&#21450;&#22522;&#20110;&#25351;&#20196;&#30340;&#24494;&#35843;&#31561;&#31561;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65292;&#26174;&#31034;&#20986;&#24456;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains sensitive personally identifiable information (PII). Direct fine-tuning LLMs on this data without privacy protection poses a risk of leakage. To address this challenge, we introduce Privacy Protection Language Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding data privacy. Our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples, stands out as a promising method, eff
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#21517;&#20026;MedTem&#30340;&#20020;&#24202;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#21644;&#26102;&#38388;&#20851;&#31995;&#25552;&#21462;&#65292;&#26469;&#25552;&#21462;&#20020;&#24202;&#25991;&#26412;&#20013;&#30340;&#33647;&#29289;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#26356;&#22909;&#22320;&#20102;&#35299;&#24739;&#32773;&#30340;&#27835;&#30103;&#21382;&#21490;&#12290;</title><link>http://arxiv.org/abs/2310.02229</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#33647;&#29289;&#21644;&#26102;&#38388;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Extraction of Medication and Temporal Relation from Clinical Text using Neural Language Models. (arXiv:2310.02229v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#21517;&#20026;MedTem&#30340;&#20020;&#24202;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#21644;&#26102;&#38388;&#20851;&#31995;&#25552;&#21462;&#65292;&#26469;&#25552;&#21462;&#20020;&#24202;&#25991;&#26412;&#20013;&#30340;&#33647;&#29289;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#26356;&#22909;&#22320;&#20102;&#35299;&#24739;&#32773;&#30340;&#27835;&#30103;&#21382;&#21490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#25991;&#26412;&#22312;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#65288;EMRs&#65289;&#20013;&#34920;&#31034;&#65292;&#21253;&#21547;&#20016;&#23500;&#30340;&#21307;&#23398;&#20449;&#24687;&#65292;&#23545;&#30142;&#30149;&#39044;&#27979;&#12289;&#20010;&#24615;&#21270;&#20449;&#24687;&#25512;&#33616;&#12289;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#20197;&#21450;&#33647;&#29289;&#27169;&#24335;&#25366;&#25496;&#21644;&#27979;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#33647;&#29289;&#25552;&#21462;&#21644;&#26102;&#38388;&#20851;&#31995;&#20998;&#31867;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#19968;&#27493;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#26356;&#22909;&#22320;&#20102;&#35299;&#24739;&#32773;&#30340;&#27835;&#30103;&#21382;&#21490;&#12290;&#20026;&#20102;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33647;&#29289;&#25552;&#21462;&#21644;&#26102;&#38388;&#20851;&#31995;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#21517;&#20026;MedTem&#30340;&#20020;&#24202;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#20102;&#20960;&#31181;&#20808;&#36827;&#30340;&#23398;&#20064;&#32467;&#26500;&#65292;&#21253;&#25324;BiLSTM-CRF&#21644;CNN-BiLSTM&#65292;&#20197;&#21450;&#29992;&#20110;&#26102;&#38388;&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#30340;BERT-CNN&#65292;&#27492;&#22806;&#65292;&#36824;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#35789;&#23884;&#20837;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#22871;&#21518;&#22788;&#29702;&#35268;&#21017;&#65292;&#20197;&#29983;&#25104;&#20851;&#20110;&#33647;&#29289;&#21644;&#26102;&#38388;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical texts, represented in electronic medical records (EMRs), contain rich medical information and are essential for disease prediction, personalised information recommendation, clinical decision support, and medication pattern mining and measurement. Relation extractions between medication mentions and temporal information can further help clinicians better understand the patients' treatment history. To evaluate the performances of deep learning (DL) and large language models (LLMs) in medication extraction and temporal relations classification, we carry out an empirical investigation of \textbf{MedTem} project using several advanced learning structures including BiLSTM-CRF and CNN-BiLSTM for a clinical domain named entity recognition (NER), and BERT-CNN for temporal relation extraction (RE), in addition to the exploration of different word embedding techniques. Furthermore, we also designed a set of post-processing roles to generate structured output on medications and the tempor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#19981;&#21516;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#20250;&#21066;&#24369;&#35843;&#25972;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01875</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#28418;&#31227;&#35843;&#25972;&#23454;&#29616;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Backdoor Purification through Feature Shift Tuning. (arXiv:2310.01875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#19981;&#21516;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#20250;&#21066;&#24369;&#35843;&#25972;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#19968;&#23567;&#32452;&#35757;&#32451;&#26679;&#26412;&#26469;&#24694;&#24847;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#38450;&#24481;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#23041;&#32961;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#38656;&#35201;&#23545;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#22797;&#26434;&#20462;&#25913;&#65292;&#35201;&#20040;&#20005;&#37325;&#20381;&#36182;&#29305;&#23450;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#24494;&#35843;&#24320;&#22987;&#65292;&#36890;&#36807;&#23545;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#30340;&#20840;&#38754;&#35780;&#20272;&#26469;&#25506;&#32034;&#26368;&#24120;&#35265;&#21644;&#26131;&#20110;&#37096;&#32626;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#12290;&#36890;&#36807;&#21021;&#27493;&#23454;&#39564;&#35266;&#23519;&#21457;&#29616;&#65292;&#19982;&#39640;&#27745;&#26579;&#29575;&#30340;&#26377;&#24076;&#26395;&#30340;&#38450;&#24481;&#32467;&#26524;&#30456;&#27604;&#65292;&#26222;&#36890;&#30340;&#35843;&#25972;&#26041;&#27861;&#22312;&#20302;&#27745;&#26579;&#29575;&#22330;&#26223;&#19979;&#23436;&#20840;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#30772;&#22351;&#20102;&#22522;&#20110;&#35843;&#25972;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been widely observed that deep neural networks (DNN) are vulnerable to backdoor attacks where attackers could manipulate the model behavior maliciously by tampering with a small set of training samples. Although a line of defense methods is proposed to mitigate this threat, they either require complicated modifications to the training process or heavily rely on the specific model architecture, which makes them hard to deploy into real-world applications. Therefore, in this paper, we instead start with fine-tuning, one of the most common and easy-to-deploy backdoor defenses, through comprehensive evaluations against diverse attack scenarios. Observations made through initial experiments show that in contrast to the promising defensive results on high poisoning rates, vanilla tuning methods completely fail at low poisoning rate scenarios. Our analysis shows that with the low poisoning rate, the entanglement between backdoor and clean features undermines the effect of tuning-based 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#29983;&#25104;&#28304;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#20854;&#19982;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.01701</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#36328;&#36234;&#39046;&#22495;&#65306;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transcending Domains through Text-to-Image Diffusion: A Source-Free Approach to Domain Adaptation. (arXiv:2310.01701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#29983;&#25104;&#28304;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#20854;&#19982;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#24212;&#29992;&#27169;&#22411;&#20174;&#30456;&#20851;&#28304;&#39046;&#22495;&#33719;&#21462;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#19981;&#20805;&#36275;&#26631;&#27880;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#25968;&#25454;&#38544;&#31169;&#27861;&#35268;&#65288;&#22914;HIPAA&#12289;COPPA&#12289;FERPA&#31561;&#65289;&#30340;&#19981;&#26029;&#21152;&#24378;&#24341;&#21457;&#20102;&#23545;&#22312;&#32469;&#36807;&#23545;&#28304;&#25968;&#25454;&#30340;&#30452;&#25509;&#35775;&#38382;&#30340;&#24773;&#20917;&#19979;&#65292;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#27169;&#22411;&#30340;&#20852;&#36259;&#65292;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;SFDA&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;SFDA&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#39046;&#22495;&#26679;&#26412;&#19978;&#35757;&#32451;&#19968;&#20010;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#28304;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#22312;&#26631;&#35760;&#30340;&#30446;&#26631;&#39046;&#22495;&#26679;&#26412;&#19978;&#35757;&#32451;&#19968;&#20010;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#25509;&#36817;&#28304;&#25968;&#25454;&#30340;&#26679;&#26412;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#20154;&#24037;&#29983;&#25104;&#30340;&#28304;&#25968;&#25454;&#19982;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#65292;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation (DA) is a method for enhancing a model's performance on a target domain with inadequate annotated data by applying the information the model has acquired from a related source domain with sufficient labeled data. The escalating enforcement of data-privacy regulations like HIPAA, COPPA, FERPA, etc. have sparked a heightened interest in adapting models to novel domains while circumventing the need for direct access to the source data, a problem known as Source-Free Domain Adaptation (SFDA). In this paper, we propose a novel framework for SFDA that generates source data using a text-to-image diffusion model trained on the target domain samples. Our method starts by training a text-to-image diffusion model on the labeled target domain samples, which is then fine-tuned using the pre-trained source model to generate samples close to the source data. Finally, we use Domain Adaptation techniques to align the artificially generated source data with the target domain data, resu
&lt;/p&gt;</description></item><item><title>PORTIA&#26159;&#19968;&#20010;&#26088;&#22312;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22120;&#30340;&#20301;&#32622;&#20559;&#24046;&#30340;&#23545;&#40784;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#31572;&#26696;&#20998;&#21106;&#25104;&#22810;&#20010;&#29255;&#27573;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#23545;&#40784;&#65292;&#28982;&#21518;&#23558;&#20854;&#21512;&#24182;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01432</link><description>&lt;p&gt;
&#20998;&#21106;&#19982;&#21512;&#24182;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20301;&#32622;&#20559;&#24046;&#36827;&#34892;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Split and Merge: Aligning Position Biases in Large Language Model based Evaluators. (arXiv:2310.01432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01432
&lt;/p&gt;
&lt;p&gt;
PORTIA&#26159;&#19968;&#20010;&#26088;&#22312;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22120;&#30340;&#20301;&#32622;&#20559;&#24046;&#30340;&#23545;&#40784;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#31572;&#26696;&#20998;&#21106;&#25104;&#22810;&#20010;&#29255;&#27573;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#23545;&#40784;&#65292;&#28982;&#21518;&#23558;&#20854;&#21512;&#24182;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20316;&#20026;&#33258;&#21160;&#21270;&#35780;&#20272;&#22120;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#31995;&#32479;&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#22312;&#20351;&#29992;&#23545;&#27604;&#35780;&#20272;&#20505;&#36873;&#31572;&#26696;&#26102;&#23384;&#22312;&#20301;&#32622;&#20559;&#24046;&#25110;&#19981;&#19968;&#33268;&#24615;&#65292;&#26080;&#35270;&#20869;&#23481;&#32780;&#20559;&#21521;&#20110;&#31532;&#19968;&#20010;&#25110;&#31532;&#20108;&#20010;&#31572;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PORTIA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#40784;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#30340;&#27604;&#36739;&#31574;&#30053;&#65292;&#20197;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#26041;&#24335;&#26657;&#20934;&#20301;&#32622;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PORTIA&#23558;&#31572;&#26696;&#20998;&#21106;&#25104;&#22810;&#20010;&#29255;&#27573;&#65292;&#23545;&#27604;&#20505;&#36873;&#31572;&#26696;&#20013;&#30340;&#30456;&#20284;&#20869;&#23481;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#23558;&#23427;&#20204;&#21512;&#24182;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#20197;&#20379;LLMs&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#30340;LLM&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;11,520&#20010;&#31572;&#26696;&#23545;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PORTIA&#26174;&#33879;&#25552;&#39640;&#20102;&#25152;&#26377;&#27169;&#22411;&#21644;&#23545;&#27604;&#24418;&#24335;&#30340;&#19968;&#33268;&#24615;&#29575;&#65292;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#29575;&#36798;&#21040;47.46%&#12290;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#65292;PORTIA&#20351;&#24471;LLMs&#33021;&#22815;&#35780;&#20272;&#20013;&#23545;&#20301;&#32622;&#20559;&#24046;&#36827;&#34892;&#26657;&#20934;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, these LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances the consistency rates for all the models and comparison forms tested, achieving an average relative improvement of 47.46%. Remarkably, PORTIA enables le
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;RA-DIT&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#19968;&#26159;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#20108;&#26159;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#27599;&#20010;&#27493;&#39588;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#27493;&#39588;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.01352</link><description>&lt;p&gt;
RA-DIT: &#26816;&#32034;&#22686;&#24378;&#30340;&#21452;&#37325;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
RA-DIT: Retrieval-Augmented Dual Instruction Tuning. (arXiv:2310.01352v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;RA-DIT&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#19968;&#26159;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#20108;&#26159;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#27599;&#20010;&#27493;&#39588;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#27493;&#39588;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RALMs&#65289;&#36890;&#36807;&#35775;&#38382;&#22806;&#37096;&#25968;&#25454;&#23384;&#20648;&#20013;&#30340;&#38271;&#23614;&#21644;&#26368;&#26032;&#30693;&#35782;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#26500;&#24314;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#26816;&#32034;&#29305;&#23450;&#20462;&#25913;&#26469;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#35201;&#20040;&#20351;&#29992;&#20107;&#21518;&#38598;&#25104;&#25968;&#25454;&#23384;&#20648;&#30340;&#26041;&#27861;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;&#8212;&#8212;&#26816;&#32034;&#22686;&#24378;&#30340;&#21452;&#37325;&#25351;&#20196;&#35843;&#20248;&#65288;RA-DIT&#65289;&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#24494;&#35843;&#27493;&#39588;&#65306;&#65288;1&#65289;&#19968;&#20010;&#26356;&#26032;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#65288;2&#65289;&#21478;&#19968;&#20010;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#65292;&#31526;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#38656;&#35201;&#30693;&#35782;&#21033;&#29992;&#21644;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#20010;&#38454;&#27573;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#26159;RA-DIT 65B&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#25512;&#29702;&#65292;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19978;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#21644;&#22810;&#20010;&#20107;&#20214;&#30340;&#32508;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.01074</link><description>&lt;p&gt;
&#37325;&#36820;&#26410;&#26469;&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models. (arXiv:2310.01074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#25512;&#29702;&#65292;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19978;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#21644;&#22810;&#20010;&#20107;&#20214;&#30340;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#25512;&#29702;&#26159;&#19968;&#39033;&#20851;&#38190;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#21487;&#20197;&#22312;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#20379;&#23545;&#26102;&#38388;&#25935;&#24863;&#29615;&#22659;&#30340;&#32454;&#33268;&#29702;&#35299;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;LLM&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#26102;&#38388;&#25512;&#29702;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#35832;&#22914;&#26102;&#38388;&#34920;&#36798;&#21644;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#31561;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#25552;&#21462;&#30452;&#25509;&#21644;&#36807;&#21435;&#30340;&#26102;&#38388;&#32447;&#32034;&#65292;&#24182;&#36827;&#34892;&#31616;&#21333;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#32771;&#34385;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#20107;&#20214;&#39044;&#27979;&#65289;&#26102;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#65292;&#36825;&#38656;&#35201;&#23545;&#20107;&#20214;&#36827;&#34892;&#22810;&#27493;&#30340;&#26102;&#38388;&#25512;&#29702;&#65292;&#24182;&#23545;&#26410;&#26469;&#26102;&#38388;&#25139;&#36827;&#34892;&#39044;&#27979;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#21478;&#19968;&#20010;&#26174;&#33879;&#23616;&#38480;&#26159;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#25512;&#29702;&#36807;&#31243;&#30340;&#35828;&#26126;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#25512;&#29702;&#30340;&#31532;&#19968;&#20010;&#20219;&#21153;&#65292;&#29992;&#20110;&#22522;&#20110;&#19978;&#19979;&#25991;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19978;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#36825;&#38656;&#35201;&#23545;&#22810;&#20010;&#20107;&#20214;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal reasoning is a crucial NLP task, providing a nuanced understanding of time-sensitive contexts within textual data. Although recent advancements in LLMs have demonstrated their potential in temporal reasoning, the predominant focus has been on tasks such as temporal expression and temporal relation extraction. These tasks are primarily designed for the extraction of direct and past temporal cues and to engage in simple reasoning processes. A significant gap remains when considering complex reasoning tasks such as event forecasting, which requires multi-step temporal reasoning on events and prediction on the future timestamp. Another notable limitation of existing methods is their incapability to provide an illustration of their reasoning process, hindering explainability. In this paper, we introduce the first task of explainable temporal reasoning, to predict an event's occurrence at a future timestamp based on context which requires multiple reasoning over multiple events, and
&lt;/p&gt;</description></item><item><title>GRID&#26159;&#19968;&#20010;&#26500;&#24314;&#36890;&#29992;&#26426;&#22120;&#20154;&#26234;&#33021;&#30340;&#24320;&#21457;&#24179;&#21488;&#65292;&#36890;&#36807;&#22522;&#30784;&#27169;&#22411;&#21644;&#25193;&#23637;&#24615;&#35774;&#35745;&#26469;&#35299;&#20915;&#29305;&#23450;&#24212;&#29992;&#21644;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00887</link><description>&lt;p&gt;
GRID: &#36890;&#29992;&#26426;&#22120;&#20154;&#26234;&#33021;&#24320;&#21457;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
GRID: A Platform for General Robot Intelligence Development. (arXiv:2310.00887v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00887
&lt;/p&gt;
&lt;p&gt;
GRID&#26159;&#19968;&#20010;&#26500;&#24314;&#36890;&#29992;&#26426;&#22120;&#20154;&#26234;&#33021;&#30340;&#24320;&#21457;&#24179;&#21488;&#65292;&#36890;&#36807;&#22522;&#30784;&#27169;&#22411;&#21644;&#25193;&#23637;&#24615;&#35774;&#35745;&#26469;&#35299;&#20915;&#29305;&#23450;&#24212;&#29992;&#21644;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#20013;&#24320;&#21457;&#26426;&#22120;&#26234;&#33021;&#33021;&#21147;&#26159;&#19968;&#39033;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#65292;&#38590;&#20197;&#25512;&#24191;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#22686;&#21152;&#20102;&#37096;&#32626;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#26426;&#22120;&#20154;&#26234;&#33021;&#24320;&#21457;&#24179;&#21488;&#65288;GRID&#65289;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#35813;&#24179;&#21488;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23398;&#20064;&#12289;&#32452;&#21512;&#21644;&#36866;&#24212;&#20854;&#29289;&#29702;&#33021;&#21147;&#12289;&#29615;&#22659;&#38480;&#21046;&#21644;&#30446;&#26631;&#12290;&#35813;&#24179;&#21488;&#36890;&#36807;&#20102;&#35299;&#29289;&#29702;&#19990;&#30028;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#12290;GRID&#20174;&#26681;&#26412;&#19978;&#35774;&#35745;&#25104;&#21487;&#25193;&#23637;&#30340;&#65292;&#20197;&#36866;&#24212;&#26032;&#31867;&#22411;&#30340;&#26426;&#22120;&#20154;&#12289;&#36710;&#36742;&#12289;&#30828;&#20214;&#24179;&#21488;&#21644;&#36719;&#20214;&#21327;&#35758;&#12290;&#27492;&#22806;&#65292;&#27169;&#22359;&#21270;&#35774;&#35745;&#20351;&#21508;&#31181;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#21644;&#29616;&#26377;&#22522;&#30784;&#27169;&#22411;&#22312;&#26356;&#24191;&#27867;&#30340;&#20197;&#26426;&#22120;&#20154;&#20026;&#20013;&#24515;&#30340;&#38382;&#39064;&#20013;&#23481;&#26131;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#23637;&#31034;&#20102;&#36825;&#20010;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing machine intelligence abilities in robots and autonomous systems is an expensive and time consuming process. Existing solutions are tailored to specific applications and are harder to generalize. Furthermore, scarcity of training data adds a layer of complexity in deploying deep machine learning models. We present a new platform for General Robot Intelligence Development (GRID) to address both of these issues. The platform enables robots to learn, compose and adapt skills to their physical capabilities, environmental constraints and goals. The platform addresses AI problems in robotics via foundation models that know the physical world. GRID is designed from the ground up to be extensible to accommodate new types of robots, vehicles, hardware platforms and software protocols. In addition, the modular design enables various deep ML components and existing foundation models to be easily usable in a wider variety of robot-centric problems. We demonstrate the platform in various 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#21033;&#29992;&#21322;&#23450;&#35268;&#21010;&#24037;&#20855;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#21644;&#20256;&#32479;&#31639;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;OptGNN&#30340;&#33021;&#21147;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.00526</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#21542;&#20316;&#20026;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Graph Neural Networks Optimal Approximation Algorithms?. (arXiv:2310.00526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#21033;&#29992;&#21322;&#23450;&#35268;&#21010;&#24037;&#20855;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#21644;&#20256;&#32479;&#31639;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;OptGNN&#30340;&#33021;&#21147;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#22815;&#20351;&#29992;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#24378;&#22823;&#30340;&#31639;&#27861;&#24037;&#20855;&#26469;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#21487;&#20197;&#34920;&#31034;&#26368;&#24378;&#22823;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#21069;&#25552;&#26159;&#20551;&#35774;&#21807;&#19968;&#28216;&#25103;&#29468;&#24819;&#25104;&#31435;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#23427;&#22312;&#35832;&#22914;&#26368;&#22823;&#21106;&#21644;&#26368;&#22823;&#29420;&#31435;&#38598;&#31561;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#36817;&#20284;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#19981;&#20165;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#65292;&#36824;&#36229;&#36807;&#20102;&#20256;&#32479;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;OptGNN&#25429;&#25417;&#20984;&#26494;&#24347;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#65288;&#30830;&#23450;&#24615;&#19978;&#30028;&#65289;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we design graph neural network architectures that can be used to obtain optimal approximation algorithms for a large class of combinatorial optimization problems using powerful algorithmic tools from semidefinite programming (SDP). Concretely, we prove that polynomial-sized message passing algorithms can represent the most powerful polynomial time algorithms for Max Constraint Satisfaction Problems assuming the Unique Games Conjecture. We leverage this result to construct efficient graph neural network architectures, OptGNN, that obtain high-quality approximate solutions on landmark combinatorial optimization problems such as Max Cut and maximum independent set. Our approach achieves strong empirical results across a wide range of real-world and synthetic datasets against both neural baselines and classical algorithms. Finally, we take advantage of OptGNN's ability to capture convex relaxations to design an algorithm for producing dual certificates of optimality (bounds on
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;LLMs&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;LLMs&#30340;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#26377;&#36739;&#22823;&#24433;&#21709;&#65292;&#32780;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.00378</link><description>&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#20215;&#20540;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Measuring Value Understanding in Language Models through Discriminator-Critique Gap. (arXiv:2310.00378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00378
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;LLMs&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;LLMs&#30340;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#26377;&#36739;&#22823;&#24433;&#21709;&#65292;&#32780;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20043;&#38388;&#28508;&#22312;&#19981;&#19968;&#33268;&#24615;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#22797;&#26434;&#21644;&#36866;&#24212;&#24615;&#65292;&#35780;&#20272;&#23427;&#20204;&#23545;&#36825;&#20123;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#26159;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#30495;&#27491;&#29702;&#35299;LLMs&#20013;&#30340;&#20215;&#20540;&#35266;&#38656;&#35201;&#32771;&#34385;&#21040;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#21644;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#20004;&#20010;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#26469;&#23450;&#37327;&#35780;&#20272;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#21644;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#12290;&#21033;&#29992;&#26045;&#29926;&#33576;&#20215;&#20540;&#35266;&#35843;&#26597;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#35780;&#20272;&#20215;&#20540;&#35266;&#30340;&#26631;&#20934;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32771;&#23519;&#20102;LLMs&#30340;&#36755;&#20986;&#19982;&#22522;&#20934;&#31572;&#26696;&#20043;&#38388;&#30340;&#20215;&#20540;&#35266;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;LLMs&#30340;&#22238;&#31572;&#19982;GPT-4&#30340;&#27880;&#37322;&#22312;&#20215;&#20540;&#35748;&#30693;&#21407;&#22240;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;&#20195;&#34920;&#24615;LLMs&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#30340;&#24433;&#21709;&#36739;&#22823;&#65292;&#20294;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) have heightened concerns about their potential misalignment with human values. However, evaluating their grasp of these values is complex due to their intricate and adaptable nature. We argue that truly understanding values in LLMs requires considering both "know what" and "know why". To this end, we present the Value Understanding Measurement (VUM) framework that quantitatively assess both "know what" and "know why" by measuring the discriminator-critique gap related to human values. Using the Schwartz Value Survey, we specify our evaluation values and develop a thousand-level dialogue dataset with GPT-4. Our assessment looks at both the value alignment of LLM's outputs compared to baseline answers and how LLM responses align with reasons for value recognition versus GPT-4's annotations. We evaluate five representative LLMs and provide strong evidence that the scaling law significantly impacts "know what" but not much on "know why", 
&lt;/p&gt;</description></item><item><title>AdaptNet&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30340;&#31574;&#30053;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21487;&#20197;&#20174;&#31867;&#20284;&#20219;&#21153;&#20013;&#24555;&#36895;&#23398;&#20064;&#21040;&#26032;&#30340;&#34892;&#20026;&#65292;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.00239</link><description>&lt;p&gt;
AdaptNet: &#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30340;&#31574;&#30053;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
AdaptNet: Policy Adaptation for Physics-Based Character Control. (arXiv:2310.00239v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00239
&lt;/p&gt;
&lt;p&gt;
AdaptNet&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30340;&#31574;&#30053;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21487;&#20197;&#20174;&#31867;&#20284;&#20219;&#21153;&#20013;&#24555;&#36895;&#23398;&#20064;&#21040;&#26032;&#30340;&#34892;&#20026;&#65292;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#22312;&#23398;&#20064;&#26032;&#25216;&#33021;&#26102;&#33021;&#22815;&#36866;&#24212;&#29616;&#26377;&#25216;&#33021;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaptNet&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#31867;&#20284;&#20219;&#21153;&#20013;&#24555;&#36895;&#23398;&#20064;&#21040;&#26032;&#30340;&#34892;&#20026;&#65292;&#30456;&#27604;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#12290;AdaptNet&#22312;&#32473;&#23450;&#30340;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#22686;&#21152;&#21407;&#22987;&#29366;&#24577;&#23884;&#20837;&#26469;&#25903;&#25345;&#34892;&#20026;&#30340;&#36866;&#24230;&#21464;&#21270;&#65292;&#24182;&#36827;&#19968;&#27493;&#20462;&#25913;&#31574;&#30053;&#32593;&#32476;&#23618;&#26469;&#23454;&#29616;&#26356;&#28145;&#36828;&#30340;&#21464;&#21270;&#12290;&#35813;&#25216;&#26415;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#29616;&#26377;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#25511;&#21046;&#22120;&#20197;&#36866;&#24212;&#24191;&#27867;&#30340;&#26032;&#30340;&#36816;&#21160;&#39118;&#26684;&#12289;&#26032;&#30340;&#20219;&#21153;&#30446;&#26631;&#12289;&#35282;&#33394;&#24418;&#24577;&#30340;&#21464;&#21270;&#20197;&#21450;&#29615;&#22659;&#30340;&#24191;&#27867;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25110;&#20351;&#29992;&#20854;&#20182;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#26174;&#31034;&#20986;&#26174;&#33879;&#25552;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#34920;&#29616;&#20026;&#22823;&#22823;&#32553;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20195;&#30721;&#21487;&#22312;https://motion-&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by humans' ability to adapt skills in the learning of new ones, this paper presents AdaptNet, an approach for modifying the latent space of existing policies to allow new behaviors to be quickly learned from like tasks in comparison to learning from scratch. Building on top of a given reinforcement learning controller, AdaptNet uses a two-tier hierarchy that augments the original state embedding to support modest changes in a behavior and further modifies the policy network layers to make more substantive changes. The technique is shown to be effective for adapting existing physics-based controllers to a wide range of new styles for locomotion, new task targets, changes in character morphology and extensive changes in environment. Furthermore, it exhibits significant increase in learning efficiency, as indicated by greatly reduced training times when compared to training from scratch or using other approaches that modify existing policies. Code is available at https://motion-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2310.00212</link><description>&lt;p&gt;
&#20004;&#20004;&#37051;&#36817;&#31574;&#30053;&#20248;&#21270;: &#21033;&#29992;&#30456;&#23545;&#21453;&#39304;&#36827;&#34892;LLM&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#39044;&#20808;&#35757;&#32451;&#26469;&#33719;&#21462;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25509;&#35302;&#21040;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;LLMs&#21487;&#33021;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#19981;&#19968;&#33268;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#24341;&#23548;LLMs&#26397;&#30528;&#26377;&#30410;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#30340;&#20027;&#23548;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20854;&#20013;Proximal Policy Optimization&#65288;PPO&#65289;&#26159;&#40664;&#35748;&#30340;RL&#20248;&#21270;&#22120;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;PPO&#22312;&#20248;&#21270;&#22522;&#20110;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#38656;&#35201;&#26657;&#20934;&#22870;&#21169;&#23610;&#24230;&#65292;PPO&#23545;&#20110;&#21253;&#21547;&#30456;&#21516;&#20559;&#22909;&#20449;&#24687;&#30340;&#31561;&#20215;&#22870;&#21169;&#20989;&#25968;&#19981;&#20855;&#22791;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;&#36712;&#36857;&#30340;&#20248;&#21270;&#30456;&#27604;&#65292;PPO&#23545;&#20110;&#22522;&#20110;&#20196;&#29260;&#30340;&#26356;&#26032;&#30340;&#38656;&#27714;&#24341;&#20837;&#20102;&#20989;&#25968;&#36924;&#36817;&#21644;&#31639;&#27861;&#35774;&#35745;&#26041;&#38754;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#30456;&#23545;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;Pairwise Proximal Policy Optimization&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pair
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#24418;&#29366;&#30340;&#24037;&#20855;&#65292;&#20174;&#32780;&#20351;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.00156</link><description>&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#23398;&#20064;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Learning Generalizable Tool-use Skills through Trajectory Generation. (arXiv:2310.00156v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00156
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#24418;&#29366;&#30340;&#24037;&#20855;&#65292;&#20174;&#32780;&#20351;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#21033;&#29992;&#24037;&#20855;&#30340;&#33258;&#20027;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#23436;&#25104;&#35768;&#22810;&#24120;&#35265;&#20219;&#21153;&#65292;&#22914;&#28921;&#39274;&#21644;&#28165;&#27905;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#31995;&#32479;&#22312;&#36866;&#24212;&#26032;&#24037;&#20855;&#26041;&#38754;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#30340;&#26234;&#33021;&#27700;&#24179;&#12290;&#22522;&#20110;&#21487;&#21450;&#24615;&#30340;&#20808;&#21069;&#24037;&#20316;&#36890;&#24120;&#23545;&#29615;&#22659;&#20570;&#20986;&#20102;&#24456;&#24378;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#26080;&#27861;&#25193;&#23637;&#21040;&#26356;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#20219;&#21153;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#20351;&#29992;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#24037;&#20855;&#26469;&#25805;&#32437;&#21487;&#21464;&#24418;&#29289;&#20307;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#20316;&#20026;&#19968;&#31995;&#21015;&#28857;&#20113;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#24037;&#20855;&#24418;&#29366;&#12290;&#23545;&#20110;&#20219;&#20309;&#26032;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#39318;&#20808;&#29983;&#25104;&#19968;&#20010;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#65292;&#28982;&#21518;&#20248;&#21270;&#24037;&#20855;&#23039;&#21183;&#24207;&#21015;&#20197;&#19982;&#29983;&#25104;&#30340;&#36712;&#36857;&#23545;&#40784;&#12290;&#25105;&#20204;&#20026;&#22235;&#31181;&#19981;&#21516;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#32437;&#20219;&#21153;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20165;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#30340;&#21333;&#20010;&#24037;&#20855;&#30340;&#31034;&#33539;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Autonomous systems that efficiently utilize tools can assist humans in completing many common tasks such as cooking and cleaning. However, current systems fall short of matching human-level of intelligence in terms of adapting to novel tools. Prior works based on affordance often make strong assumptions about the environments and cannot scale to more complex, contact-rich tasks. In this work, we tackle this challenge and explore how agents can learn to use previously unseen tools to manipulate deformable objects. We propose to learn a generative model of the tool-use trajectories as a sequence of point clouds, which generalizes to different tool shapes. Given any novel tool, we first generate a tool-use trajectory and then optimize the sequence of tool poses to align with the generated trajectory. We train a single model for four different challenging deformable object manipulation tasks. Our model is trained with demonstration data from just a single tool for each task and is able to 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#21548;&#20247;&#32918;&#20687;&#65288;ELP&#65289;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#31163;&#25955;&#35774;&#35745;&#65292;&#33021;&#26681;&#25454;&#23545;&#35805;&#20013;&#19981;&#21516;&#24773;&#32490;&#29983;&#25104;&#33258;&#28982;&#22810;&#26679;&#21448;&#21487;&#25511;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#38754;&#37096;&#34920;&#24773;&#29983;&#25104;&#20013;&#30340;&#38750;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00068</link><description>&lt;p&gt;
&#24773;&#24863;&#21548;&#20247;&#32918;&#20687;&#65306;&#30495;&#23454;&#30340;&#21548;&#20247;&#21160;&#20316;&#27169;&#25311;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Emotional Listener Portrait: Realistic Listener Motion Simulation in Conversation. (arXiv:2310.00068v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#21548;&#20247;&#32918;&#20687;&#65288;ELP&#65289;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#31163;&#25955;&#35774;&#35745;&#65292;&#33021;&#26681;&#25454;&#23545;&#35805;&#20013;&#19981;&#21516;&#24773;&#32490;&#29983;&#25104;&#33258;&#28982;&#22810;&#26679;&#21448;&#21487;&#25511;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#38754;&#37096;&#34920;&#24773;&#29983;&#25104;&#20013;&#30340;&#38750;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#32773;&#22836;&#37096;&#29983;&#25104;&#20027;&#35201;&#20851;&#27880;&#22312;&#26681;&#25454;&#35762;&#35805;&#32773;&#20256;&#36882;&#30340;&#20449;&#24687;&#29983;&#25104;&#21548;&#32773;&#30340;&#38750;&#35821;&#35328;&#34892;&#20026;&#65288;&#20363;&#22914;&#24494;&#31505;&#65289;&#12290;&#29983;&#25104;&#36825;&#26679;&#30340;&#21709;&#24212;&#26102;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#23545;&#35805;&#20013;&#31934;&#32454;&#38754;&#37096;&#34920;&#24773;&#30340;&#38750;&#30830;&#23450;&#24615;&#29305;&#24615;&#65292;&#36825;&#21462;&#20915;&#20110;&#35762;&#35805;&#32773;&#21644;&#21548;&#32773;&#30340;&#24773;&#32490;&#21644;&#24577;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24773;&#24863;&#21548;&#20247;&#32918;&#20687;&#65288;ELP&#65289;&#65292;&#23427;&#23558;&#27599;&#20010;&#32454;&#31890;&#24230;&#38754;&#37096;&#21160;&#20316;&#35270;&#20026;&#33509;&#24178;&#31163;&#25955;&#21160;&#20316;&#32534;&#30721;&#35789;&#30340;&#32452;&#21512;&#65292;&#24182;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;&#19981;&#21516;&#24773;&#24863;&#19979;&#21160;&#20316;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#30001;&#20110;&#8220;&#26174;&#24335;&#8221;&#21644;&#8220;&#31163;&#25955;&#8221;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#30340;ELP&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#20174;&#23398;&#20064;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#33258;&#21160;&#29983;&#25104;&#23545;&#32473;&#23450;&#35762;&#35805;&#32773;&#30340;&#33258;&#28982;&#22810;&#26679;&#30340;&#21709;&#24212;&#65292;&#36824;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#39044;&#20808;&#30830;&#23450;&#24577;&#24230;&#30340;&#21487;&#25511;&#21709;&#24212;&#12290;&#22312;&#20960;&#20010;&#23450;&#37327;&#24230;&#37327;&#25351;&#26631;&#19979;&#65292;&#25105;&#20204;&#30340;ELP&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Listener head generation centers on generating non-verbal behaviors (e.g., smile) of a listener in reference to the information delivered by a speaker. A significant challenge when generating such responses is the non-deterministic nature of fine-grained facial expressions during a conversation, which varies depending on the emotions and attitudes of both the speaker and the listener. To tackle this problem, we propose the Emotional Listener Portrait (ELP), which treats each fine-grained facial motion as a composition of several discrete motion-codewords and explicitly models the probability distribution of the motions under different emotion in conversation. Benefiting from the ``explicit'' and ``discrete'' design, our ELP model can not only automatically generate natural and diverse responses toward a given speaker via sampling from the learned distribution but also generate controllable responses with a predetermined attitude. Under several quantitative metrics, our ELP exhibits sig
&lt;/p&gt;</description></item><item><title>CoBEVFlow&#26159;&#19968;&#31181;&#40065;&#26834;&#30340;&#24322;&#27493;&#21327;&#20316;&#24335;&#19977;&#32500;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#34917;&#20607;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#24322;&#27493;&#21327;&#20316;&#20449;&#24687;&#23545;&#40784;&#26469;&#35299;&#20915;&#20449;&#24687;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16940</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#24322;&#27493;&#21327;&#20316;&#24335;&#40479;&#30640;&#35270;&#35282;&#19977;&#32500;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Asynchronous Collaborative 3D Detection via Bird's Eye View Flow. (arXiv:2309.16940v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16940
&lt;/p&gt;
&lt;p&gt;
CoBEVFlow&#26159;&#19968;&#31181;&#40065;&#26834;&#30340;&#24322;&#27493;&#21327;&#20316;&#24335;&#19977;&#32500;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#34917;&#20607;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#24322;&#27493;&#21327;&#20316;&#20449;&#24687;&#23545;&#40784;&#26469;&#35299;&#20915;&#20449;&#24687;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20419;&#36827;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#21327;&#20316;&#24863;&#30693;&#21487;&#20197;&#22823;&#22823;&#25552;&#21319;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#30001;&#20110;&#36890;&#20449;&#24310;&#36831;&#12289;&#20013;&#26029;&#21644;&#26102;&#38047;&#19981;&#23545;&#40784;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#26102;&#38388;&#19981;&#21516;&#27493;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#22810;&#26234;&#33021;&#20307;&#34701;&#21512;&#36807;&#31243;&#20013;&#23548;&#33268;&#20449;&#24687;&#19981;&#21305;&#37197;&#65292;&#20005;&#37325;&#21160;&#25671;&#20102;&#21327;&#20316;&#30340;&#22522;&#30784;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoBEVFlow&#65292;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#27969;&#30340;&#24322;&#27493;&#40065;&#26834;&#21327;&#20316;&#24335;&#19977;&#32500;&#24863;&#30693;&#31995;&#32479;&#12290;CoBEVFlow&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#36890;&#36807;&#34917;&#20607;&#36816;&#21160;&#26469;&#20351;&#22810;&#20010;&#26234;&#33021;&#20307;&#21457;&#36865;&#30340;&#24322;&#27493;&#21327;&#20316;&#20449;&#24687;&#23545;&#40784;&#12290;&#20026;&#20102;&#24314;&#27169;&#22330;&#26223;&#20013;&#30340;&#36816;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40479;&#30640;&#35270;&#35282;&#27969;&#65292;&#23427;&#26159;&#19982;&#27599;&#20010;&#31354;&#38388;&#20301;&#32622;&#23545;&#24212;&#30340;&#36816;&#21160;&#21521;&#37327;&#30340;&#38598;&#21512;&#12290;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#27969;&#65292;&#24322;&#27493;&#24863;&#30693;&#29305;&#24449;&#21487;&#20197;&#37325;&#26032;&#20998;&#37197;&#21040;&#36866;&#24403;&#30340;&#20301;&#32622;&#65292;&#20943;&#36731;&#24322;&#27493;&#30340;&#24433;&#21709;&#12290;CoBEVFlow&#20855;&#26377;&#20004;&#20010;&#20248;&#28857;&#65306;(i) CoBEVFlow&#21487;&#20197;&#22788;&#29702;&#24322;&#27493;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
By facilitating communication among multiple agents, collaborative perception can substantially boost each agent's perception ability. However, temporal asynchrony among agents is inevitable in real-world due to communication delays, interruptions, and clock misalignments. This issue causes information mismatch during multi-agent fusion, seriously shaking the foundation of collaboration. To address this issue, we propose CoBEVFlow, an asynchrony-robust collaborative 3D perception system based on bird's eye view (BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align asynchronous collaboration messages sent by multiple agents. To model the motion in a scene, we propose BEV flow, which is a collection of the motion vector corresponding to each spatial location. Based on BEV flow, asynchronous perceptual features can be reassigned to appropriate positions, mitigating the impact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle asynchronous collabor
&lt;/p&gt;</description></item><item><title>XVO&#26159;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#24577;&#33258;&#25105;&#35757;&#32451;&#30340;&#27867;&#21270;&#35270;&#35273;&#37324;&#31243;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#35774;&#32622;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#33258;&#32473;&#33258;&#36275;&#25805;&#20316;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#20854;&#20851;&#38190;&#21019;&#26032;&#21644;&#36129;&#29486;&#21253;&#25324;&#36890;&#36807;&#21322;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#36890;&#29992;&#30340;&#30452;&#25509;VO&#22238;&#24402;&#32593;&#32476;&#20197;&#21450;&#20351;&#29992;&#22810;&#27169;&#24335;&#30417;&#30563;&#20219;&#21153;&#26469;&#20419;&#36827;&#27867;&#21270;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.16772</link><description>&lt;p&gt;
XVO: &#36890;&#36807;&#36328;&#27169;&#24577;&#33258;&#25105;&#35757;&#32451;&#30340;&#27867;&#21270;&#35270;&#35273;&#37324;&#31243;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
XVO: Generalized Visual Odometry via Cross-Modal Self-Training. (arXiv:2309.16772v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16772
&lt;/p&gt;
&lt;p&gt;
XVO&#26159;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#24577;&#33258;&#25105;&#35757;&#32451;&#30340;&#27867;&#21270;&#35270;&#35273;&#37324;&#31243;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#35774;&#32622;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#33258;&#32473;&#33258;&#36275;&#25805;&#20316;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#20854;&#20851;&#38190;&#21019;&#26032;&#21644;&#36129;&#29486;&#21253;&#25324;&#36890;&#36807;&#21322;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#36890;&#29992;&#30340;&#30452;&#25509;VO&#22238;&#24402;&#32593;&#32476;&#20197;&#21450;&#20351;&#29992;&#22810;&#27169;&#24335;&#30417;&#30563;&#20219;&#21153;&#26469;&#20419;&#36827;&#27867;&#21270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;XVO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#35774;&#32622;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#33258;&#32473;&#33258;&#36275;&#25805;&#20316;&#30340;&#27867;&#21270;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#65288;VO&#65289;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#19982;&#36890;&#24120;&#30740;&#31350;&#21333;&#20010;&#25968;&#25454;&#38598;&#20869;&#24050;&#30693;&#26657;&#20934;&#30340;&#26631;&#20934;&#21333;&#30446;VO&#26041;&#27861;&#19981;&#21516;&#65292;XVO&#21487;&#20197;&#39640;&#25928;&#22320;&#36890;&#36807;&#35270;&#35273;&#22330;&#26223;&#35821;&#20041;&#65288;&#21363;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#24050;&#30693;&#30456;&#26426;&#21442;&#25968;&#65289;&#23398;&#20064;&#24674;&#22797;&#30456;&#23545;&#20301;&#23039;&#65292;&#24182;&#20174;YouTube&#19978;&#30340;&#22823;&#37327;&#26080;&#32422;&#26463;&#21644;&#24322;&#26500;&#30340;&#36710;&#36733;&#25668;&#20687;&#22836;&#35270;&#39057;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#26469;&#20248;&#21270;&#36816;&#21160;&#20272;&#35745;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#31532;&#19968;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#20102;&#21322;&#30417;&#30563;&#35757;&#32451;&#23545;&#20110;&#23398;&#20064;&#36890;&#29992;&#30340;&#30452;&#25509;VO&#22238;&#24402;&#32593;&#32476;&#30340;&#22909;&#22788;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#27169;&#24335;&#30417;&#30563;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20998;&#21106;&#12289;&#20809;&#27969;&#12289;&#28145;&#24230;&#21644;&#38899;&#39057;&#36741;&#21161;&#39044;&#27979;&#20219;&#21153;&#65292;&#20197;&#20419;&#36827;VO&#20219;&#21153;&#30340;&#27867;&#21270;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#38899;&#39057;&#39044;&#27979;&#20219;&#21153;&#23545;&#20110;&#24635;&#32467;&#25688;&#35201;&#30340;&#20851;&#38190;&#21019;&#26032;&#21644;&#36129;&#29486;&#26377;&#20419;&#36827;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose XVO, a semi-supervised learning method for training generalized monocular Visual Odometry (VO) models with robust off-the-self operation across diverse datasets and settings. In contrast to standard monocular VO approaches which often study a known calibration within a single dataset, XVO efficiently learns to recover relative pose with real-world scale from visual scene semantics, i.e., without relying on any known camera parameters. We optimize the motion estimation model via self-training from large amounts of unconstrained and heterogeneous dash camera videos available on YouTube. Our key contribution is twofold. First, we empirically demonstrate the benefits of semi-supervised training for learning a general-purpose direct VO regression network. Second, we demonstrate multi-modal supervision, including segmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate generalized representations for the VO task. Specifically, we find audio prediction task to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16742</link><description>&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#39118;&#38505;&#30340;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients. (arXiv:2309.16742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#65292;&#23588;&#20854;&#26159;2&#22411;&#31958;&#23615;&#30149;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#20581;&#24247;&#38382;&#39064;&#12290;&#19982;&#31958;&#23615;&#30149;&#30456;&#20851;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#20854;&#24182;&#21457;&#30151;&#30340;&#21457;&#23637;&#12290;&#31958;&#23615;&#30149;&#32958;&#30149;&#26159;&#31958;&#23615;&#30149;&#30340;&#19968;&#31181;&#24930;&#24615;&#24182;&#21457;&#30151;&#65292;&#19981;&#21033;&#22320;&#24433;&#21709;&#32958;&#33039;&#65292;&#23548;&#33268;&#32958;&#33039;&#25439;&#20260;&#12290;&#35786;&#26029;&#31958;&#23615;&#30149;&#32958;&#30149;&#28041;&#21450;&#32771;&#34385;&#21508;&#31181;&#26631;&#20934;&#20043;&#19968;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#30340;&#30149;&#29702;&#23398;&#30149;&#29702;&#23398;&#25968;&#37327;&#65292;&#31216;&#20026;&#30333;&#34507;&#30333;&#23615;&#12290;&#22240;&#27492;&#65292;&#23545;&#31958;&#23615;&#30149;&#24739;&#32773;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#20855;&#26377;&#21450;&#26102;&#39044;&#38450;&#25514;&#26045;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#24739;&#26377;&#30333;&#34507;&#30333;&#23615;&#30340;&#39118;&#38505;&#12290;&#25152;&#36873;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#21253;&#25324;&#26420;&#32032;&#36125;&#21494;&#26031;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;AdaBoost&#65292;XGBoost&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12290;&#25105;&#20204;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#21253;&#25324;184&#26465;&#31958;&#23615;&#30149;&#24182;&#21457;&#30151;&#39118;&#38505;&#22240;&#32032;&#30340;&#26465;&#30446;&#34987;&#29992;&#26469;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diabetes, especially T2DM, continues to be a significant health problem. One of the major concerns associated with diabetes is the development of its complications. Diabetic nephropathy, one of the chronic complication of diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing diabetic nephropathy involves considering various criteria, one of which is the presence of a pathologically significant quantity of albumin in urine, known as albuminuria. Thus, early prediction of albuminuria in diabetic patients holds the potential for timely preventive measures. This study aimed to develop a supervised learning model to predict the risk of developing albuminuria in T2DM patients. The selected supervised learning algorithms included Na\"ive Bayes, Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost, and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries of diabetes complications risk factors, was used to train the algorithm
&lt;/p&gt;</description></item><item><title>&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20854;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.15857</link><description>&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
A Survey on Image-text Multimodal Models. (arXiv:2309.15857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15857
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20854;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#25104;&#20026;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#23548;&#33268;&#20102;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#26412;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30340;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#20215;&#20540;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#21457;&#23637;&#37324;&#31243;&#30865;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#23427;&#20204;&#30340;&#21457;&#23637;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#22522;&#20110;&#23427;&#20204;&#34987;&#24341;&#20837;&#30340;&#26102;&#38388;&#21644;&#23545;&#23398;&#31185;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20219;&#21153;&#22312;&#23398;&#26415;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#26222;&#21450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#19982;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30456;&#20851;&#30340;&#20219;&#21153;&#21010;&#20998;&#20026;&#20116;&#20010;&#20027;&#35201;&#31867;&#22411;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#38416;&#26126;&#20102;&#27599;&#20010;&#31867;&#21035;&#20869;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#20851;&#38190;&#25216;&#26415;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#20294;&#20173;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amidst the evolving landscape of artificial intelligence, the convergence of visual and textual information has surfaced as a crucial frontier, leading to the advent of image-text multimodal models. This paper provides a comprehensive review of the evolution and current state of image-text multimodal models, exploring their application value, challenges, and potential research trajectories. Initially, we revisit the basic concepts and developmental milestones of these models, introducing a novel classification that segments their evolution into three distinct phases, based on their time of introduction and subsequent impact on the discipline. Furthermore, based on the tasks' significance and prevalence in the academic landscape, we propose a categorization of the tasks associated with image-text multimodal models into five major types, elucidating the recent progress and key technologies within each category. Despite the remarkable accomplishments of these models, numerous challenges a
&lt;/p&gt;</description></item><item><title>Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15806</link><description>&lt;p&gt;
Lyra: &#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#30340;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Lyra: Orchestrating Dual Correction in Automated Theorem Proving. (arXiv:2309.15806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15806
&lt;/p&gt;
&lt;p&gt;
Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#25506;&#32034;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#24187;&#35273;&#30340;&#20943;&#36731;&#21644;&#36890;&#36807;&#35777;&#26126;&#22120;&#38169;&#35823;&#28040;&#24687;&#30340;&#32454;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#22686;&#24378;LLMs&#22312;&#35813;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Lyra&#65292;&#19968;&#31181;&#37319;&#29992;&#20004;&#31181;&#19981;&#21516;&#20462;&#27491;&#26426;&#21046;&#30340;&#26032;&#26694;&#26550;&#65306;&#24037;&#20855;&#20462;&#27491;&#65288;TC&#65289;&#21644;&#29468;&#24819;&#20462;&#27491;&#65288;CC&#65289;&#12290;&#20026;&#20102;&#22312;&#24418;&#24335;&#35777;&#26126;&#30340;&#21518;&#22788;&#29702;&#20013;&#23454;&#29616;&#24037;&#20855;&#20462;&#27491;&#65292;&#25105;&#20204;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#35777;&#26126;&#24037;&#20855;&#65288;&#22914;Sledgehammer&#65289;&#26469;&#25351;&#23548;&#26367;&#25442;&#19981;&#27491;&#30830;&#30340;&#24037;&#20855;&#12290;&#24037;&#20855;&#20462;&#27491;&#26174;&#33879;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29468;&#24819;&#20462;&#27491;&#65292;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#26426;&#21046;&#65292;&#26088;&#22312;&#19982;&#35777;&#26126;&#22120;&#20114;&#21160;&#65292;&#36890;&#36807;&#35777;&#26126;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#36827;&#19968;&#27493;&#23436;&#21892;&#24418;&#24335;&#35777;&#26126;&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) present an intriguing avenue for exploration in the field of formal theorem proving. Nevertheless, their full potential, particularly concerning the mitigation of hallucinations and refinement through prover error messages, remains an area that has yet to be thoroughly investigated. To enhance the effectiveness of LLMs in the field, we introduce the Lyra, a new framework that employs two distinct correction mechanisms: Tool Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in the post-processing of formal proofs, we leverage prior knowledge to utilize predefined prover tools (e.g., Sledgehammer) for guiding the replacement of incorrect tools. Tool Correction significantly contributes to mitigating hallucinations, thereby improving the overall accuracy of the proof. In addition, we introduce Conjecture Correction, an error feedback mechanism designed to interact with prover to refine formal proof conjectures with prover error messa
&lt;/p&gt;</description></item><item><title>&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.14970</link><description>&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recurrent Hypernetworks are Surprisingly Strong in Meta-RL. (arXiv:2309.14970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14970
&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#26102;&#22240;&#26679;&#26412;&#25928;&#29575;&#20302;&#32780;&#19981;&#26131;&#37096;&#32626;&#12290;&#20803;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#22312;&#20803;&#35757;&#32451;&#26102;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#20998;&#24067;&#26469;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#30452;&#25509;&#35299;&#20915;&#20102;&#36825;&#20010;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#19987;&#38376;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#19982;&#19968;&#20010;&#36890;&#29992;&#30340;&#24207;&#21015;&#27169;&#22411;&#65288;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26159;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35266;&#28857;&#30001;&#20110;&#26377;&#38480;&#30340;&#25903;&#25345;&#35777;&#25454;&#32780;&#24341;&#36215;&#20102;&#20105;&#35758;&#65292;&#29305;&#21035;&#26159;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#30830;&#31435;&#20102;&#23436;&#20840;&#30456;&#21453;&#30340;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#34429;&#28982;&#25105;&#20204;&#21516;&#26679;&#21457;&#29616;&#24490;&#29615;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#36229;&#32593;&#32476;&#30340;&#20351;&#29992;&#23545;&#20110;&#21457;&#25381;&#24490;&#29615;&#22522;&#32447;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19982;&#36229;&#32593;&#32476;&#30456;&#32467;&#21512;&#26102;&#65292;&#36825;&#31181;&#36828;&#27604;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#31616;&#21333;&#30340;&#24490;&#29615;&#22522;&#20934;&#23454;&#38469;&#19978;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) is notoriously impractical to deploy due to sample inefficiency. Meta-RL directly addresses this sample inefficiency by learning to perform few-shot learning when a distribution of related tasks is available for meta-training. While many specialized meta-RL methods have been proposed, recent work suggests that end-to-end learning in conjunction with an off-the-shelf sequential model, such as a recurrent network, is a surprisingly strong baseline. However, such claims have been controversial due to limited supporting evidence, particularly in the face of prior work establishing precisely the opposite. In this paper, we conduct an empirical investigation. While we likewise find that a recurrent network can achieve strong performance, we demonstrate that the use of hypernetworks is crucial to maximizing their potential. Surprisingly, when combined with hypernetworks, the recurrent baselines that are far simpler than existing specialized methods actually ac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#30340;&#25345;&#32493;&#25919;&#31574;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38381;&#29615;&#33258;&#21160;&#39550;&#39542;&#20013;&#24341;&#20837;&#32972;&#26223;&#36710;&#36742;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20043;&#38388;&#30340;&#21338;&#24328;&#24335;&#20132;&#20114;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#38271;&#23614;&#20998;&#24067;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14235</link><description>&lt;p&gt;
Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#22330;&#26223;&#30340;&#38381;&#29615;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#25345;&#32493;&#25919;&#31574;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Stackelberg Driver Model for Continual Policy Improvement in Scenario-Based Closed-Loop Autonomous Driving. (arXiv:2309.14235v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14235
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#30340;&#25345;&#32493;&#25919;&#31574;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38381;&#29615;&#33258;&#21160;&#39550;&#39542;&#20013;&#24341;&#20837;&#32972;&#26223;&#36710;&#36742;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20043;&#38388;&#30340;&#21338;&#24328;&#24335;&#20132;&#20114;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#38271;&#23614;&#20998;&#24067;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#22256;&#38590;&#65292;&#22240;&#20026;&#38271;&#23614;&#20998;&#24067;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#23384;&#22312;&#32597;&#35265;&#20294;&#20851;&#38190;&#30340;&#36793;&#38469;&#24773;&#20917;&#65292;&#36825;&#20250;&#23545;&#23427;&#20204;&#30340;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#23545;&#25239;&#24615;&#29983;&#25104;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#19968;&#31867;&#26377;&#25928;&#30340;&#36884;&#24452;&#65292;&#29992;&#20110;&#21512;&#25104;AV&#27979;&#35797;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#22330;&#26223;&#36890;&#24120;&#34987;&#29992;&#20110;AV&#35757;&#32451;&#30340;&#26426;&#20250;&#26377;&#38480;&#65292;&#36896;&#25104;&#20102;&#25345;&#32493;AV&#25919;&#31574;&#25913;&#36827;&#30340;&#28508;&#21147;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#65292;&#21516;&#26102;&#20063;&#32570;&#20047;&#38381;&#29615;&#35774;&#35745;&#26469;&#23454;&#29616;&#36825;&#19968;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#65288;SDM&#65289;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#20934;&#30830;&#25551;&#36848;&#36710;&#36742;&#20132;&#20114;&#21160;&#21147;&#23398;&#30340;&#23618;&#27425;&#24615;&#36136;&#65292;&#36890;&#36807;&#23558;&#32972;&#26223;&#36710;&#36742;&#65288;BVs&#65289;&#21644;AV&#22312;&#19968;&#31181;&#39034;&#24207;&#21338;&#24328;&#24335;&#30340;&#20132;&#20114;&#33539; Paradigm&#20869;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#36890;&#36807;AV&#20805;&#24403;&#39046;&#23548;&#32773;&#65292;BVs&#20316;&#20026;&#36861;&#38543;&#32773;&#65292;&#36825;&#31181;&#39046;&#23548;&#32773;-&#36861;&#38543;&#32773;&#27169;&#22411;&#30830;&#20445;&#20102;AV&#22987;&#32456;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of autonomous vehicles (AVs) has faced hurdles due to the dominance of rare but critical corner cases within the long-tail distribution of driving scenarios, which negatively affects their overall performance. To address this challenge, adversarial generation methods have emerged as a class of efficient approaches to synthesize safety-critical scenarios for AV testing. However, these generated scenarios are often underutilized for AV training, resulting in the potential for continual AV policy improvement remaining untapped, along with a deficiency in the closed-loop design needed to achieve it. Therefore, we tailor the Stackelberg Driver Model (SDM) to accurately characterize the hierarchical nature of vehicle interaction dynamics, facilitating iterative improvement by engaging background vehicles (BVs) and AV in a sequential game-like interaction paradigm. With AV acting as the leader and BVs as followers, this leader-follower modeling ensures that AV would consistentl
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;&#27979;&#37327;LLMs&#30340;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#65292;&#26089;&#26399;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;ChatGPT&#12289;Llama2-Chat&#12289;PaLM-2&#21644;GPT-4&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#19982;&#25104;&#24180;&#20154;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#22256;&#22659;&#19979;&#30340;&#34920;&#29616;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.13356</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;&#25506;&#31350;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Probing the Moral Development of Large Language Models through Defining Issues Test. (arXiv:2309.13356v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13356
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;&#27979;&#37327;LLMs&#30340;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#65292;&#26089;&#26399;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;ChatGPT&#12289;Llama2-Chat&#12289;PaLM-2&#21644;GPT-4&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#19982;&#25104;&#24180;&#20154;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#22256;&#22659;&#19979;&#30340;&#34920;&#29616;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;(DIT)&#26469;&#27979;&#37327;LLMs&#30340;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#65292;DIT&#26159;&#19968;&#31181;&#24515;&#29702;&#27979;&#37327;&#24037;&#20855;&#65292;&#29992;&#20110;&#26681;&#25454;&#31185;&#23572;&#20271;&#26684;&#30340;&#35748;&#30693;&#36947;&#24503;&#21457;&#23637;&#27169;&#22411;&#26469;&#34913;&#37327;&#20010;&#20154;&#30340;&#36947;&#24503;&#21457;&#23637;&#38454;&#27573;&#12290;DIT&#20351;&#29992;&#36947;&#24503;&#22256;&#22659;&#65292;&#24182;&#35201;&#27714;&#34987;&#35843;&#26597;&#32773;&#26681;&#25454;&#36947;&#24503;&#32771;&#34385;&#30340;&#37325;&#35201;&#24615;&#26469;&#21028;&#26029;&#21644;&#25490;&#24207;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26089;&#26399;&#30340;LLMs&#65288;&#22914;GPT-3&#65289;&#22312;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#19978;&#24182;&#19981;&#27604;&#38543;&#26426;&#22522;&#32447;&#26356;&#22909;&#65292;&#32780;ChatGPT&#12289;Llama2-Chat&#12289;PaLM-2&#21644;GPT-4&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26126;&#26174;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#19982;&#25104;&#24180;&#20154;&#30456;&#23218;&#32654;&#12290;&#23454;&#38469;&#19978;&#65292;GPT-4&#20855;&#26377;&#26368;&#39640;&#30340;&#21518;&#24120;&#35268;&#36947;&#24503;&#25512;&#29702;&#20998;&#25968;&#65292;&#30456;&#24403;&#20110;&#20856;&#22411;&#30740;&#31350;&#29983;&#30340;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#36825;&#20123;&#27169;&#22411;&#22312;&#25152;&#26377;&#22256;&#22659;&#19978;&#30340;&#34920;&#29616;&#24182;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we measure the moral reasoning ability of LLMs using the Defining Issues Test - a psychometric instrument developed for measuring the moral development stage of a person according to the Kohlberg's Cognitive Moral Development Model. DIT uses moral dilemmas followed by a set of ethical considerations that the respondent has to judge for importance in resolving the dilemma, and then rank-order them by importance. A moral development stage score of the respondent is then computed based on the relevance rating and ranking.  Our study shows that early LLMs such as GPT-3 exhibit a moral reasoning ability no better than that of a random baseline, while ChatGPT, Llama2-Chat, PaLM-2 and GPT-4 show significantly better performance on this task, comparable to adult humans. GPT-4, in fact, has the highest post-conventional moral reasoning score, equivalent to that of typical graduate school students. However, we also observe that the models do not perform consistently across all dil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#20844;&#21496;&#30340;Robot Induction&#65288;Robin&#65289;&#33328;&#38431;&#20013;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#65292;&#36890;&#36807;&#20351;&#29992;&#25315;&#36873;&#25104;&#21151;&#39044;&#27979;&#22120;&#20197;&#21450;&#35757;&#32451;&#30340;&#25315;&#36873;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;&#30495;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#36827;&#34892;&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.13224</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#30340;&#25315;&#36873;&#35745;&#21010;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Pick Planning Strategies for Large-Scale Package Manipulation. (arXiv:2309.13224v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#20844;&#21496;&#30340;Robot Induction&#65288;Robin&#65289;&#33328;&#38431;&#20013;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#65292;&#36890;&#36807;&#20351;&#29992;&#25315;&#36873;&#25104;&#21151;&#39044;&#27979;&#22120;&#20197;&#21450;&#35757;&#32451;&#30340;&#25315;&#36873;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;&#30495;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#36827;&#34892;&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#21487;&#20197;&#38477;&#20302;&#29289;&#27969;&#25104;&#26412;&#65292;&#26368;&#32456;&#38477;&#20302;&#28040;&#36153;&#32773;&#30340;&#20215;&#26684;&#65292;&#21152;&#24555;&#20132;&#36135;&#36895;&#24230;&#65292;&#24182;&#22686;&#24378;&#23545;&#24066;&#22330;&#27874;&#21160;&#30340;&#36866;&#24212;&#21147;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#20844;&#21496;&#30340;&#26426;&#22120;&#20154;&#24341;&#23548;&#65288;Robin&#65289;&#33328;&#38431;&#20013;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#65292;&#29992;&#20110;&#27599;&#22825;&#25315;&#36873;&#21644;&#21333;&#29420;&#22788;&#29702;600&#19975;&#20010;&#21253;&#35065;&#65292;&#24182;&#19988;&#30446;&#21069;&#24050;&#32463;&#22788;&#29702;&#20102;20&#20159;&#20010;&#21253;&#35065;&#12290;&#23427;&#25551;&#36848;&#20102;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#24320;&#21457;&#30340;&#21508;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#21450;&#20854;&#21518;&#32487;&#26041;&#27861;&#65292;&#21518;&#32487;&#26041;&#27861;&#21033;&#29992;&#20102;&#22312;&#30495;&#23454;&#29983;&#20135;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25315;&#36873;&#25104;&#21151;&#39044;&#27979;&#22120;&#12290;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#22312;&#30495;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#39318;&#27425;&#22823;&#35268;&#27169;&#37096;&#32626;&#23398;&#20064;&#30340;&#25315;&#36873;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to market fluctuations.  This extended abstract showcases a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which is used for picking and singulating up to 6 million packages per day and so far has manipulated over 2 billion packages. It describes the various heuristic methods developed over time and their successor, which utilizes a pick success predictor trained on real production data.  To the best of the authors' knowledge, this work is the first large-scale deployment of learned pick quality estimation methods in a real production system.
&lt;/p&gt;</description></item><item><title>MetaMath&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.12284</link><description>&lt;p&gt;
MetaMath&#65306;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#33258;&#24049;&#30340;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12284
&lt;/p&gt;
&lt;p&gt;
MetaMath&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#26497;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;LLMs&#65288;&#20363;&#22914;LLaMA-2&#65289;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#20173;&#28982;&#36828;&#36828;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#21407;&#22240;&#26159;&#22797;&#26434;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaMath&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27809;&#26377;&#39069;&#22806;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20197;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#20889;&#20837;&#38382;&#39064;&#26469;&#24341;&#23548;&#25968;&#23398;&#38382;&#39064;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;MetaMathQA&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;MetaMathQA&#19978;&#23545;LLaMA-2&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23545;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#20004;&#20010;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#65288;&#21363;GSM8K&#21644;MATH&#65289;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MetaMath&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#19968;&#22871;&#24320;&#28304;LLMs&#12290;&#25105;&#20204;&#30340;MetaMath-7B&#27169;&#22411;&#22312;GSM8K&#19978;&#36798;&#21040;&#20102;66.4&#65285;&#65292;&#22312;MATH&#19978;&#36798;&#21040;&#20102;19.4&#65285;&#65292;&#36229;&#36807;&#20102;&#30456;&#21516;&#35268;&#27169;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose \emph{MetaMath}, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (\ie, GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves $66.4\%$ on GSM8K and $19.4\%$ on MATH, exceeding the state-of-the-art models of the same size by 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20998;&#23376;&#35299;&#31163;&#35270;&#20026;&#23545;&#20854;&#32452;&#25104;&#21407;&#23376;&#26045;&#21152;&#36880;&#28176;&#22686;&#22823;&#30340;&#21147;&#22330;&#65292;&#20174;&#32780;&#25913;&#21464;&#21407;&#23376;&#38388;&#36317;&#31163;&#30340;&#20998;&#24067;&#65292;&#35813;&#26041;&#27861;&#22312;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.09985</link><description>&lt;p&gt;
&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#30340;&#20301;&#31227;&#24471;&#20998;&#27861;
&lt;/p&gt;
&lt;p&gt;
Molecular Conformation Generation via Shifting Scores. (arXiv:2309.09985v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20998;&#23376;&#35299;&#31163;&#35270;&#20026;&#23545;&#20854;&#32452;&#25104;&#21407;&#23376;&#26045;&#21152;&#36880;&#28176;&#22686;&#22823;&#30340;&#21147;&#22330;&#65292;&#20174;&#32780;&#25913;&#21464;&#21407;&#23376;&#38388;&#36317;&#31163;&#30340;&#20998;&#24067;&#65292;&#35813;&#26041;&#27861;&#22312;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26159;&#35745;&#31639;&#21270;&#23398;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#28041;&#21450;&#20026;&#32473;&#23450;&#30340;&#20998;&#23376;&#29983;&#25104;&#19977;&#32500;&#26500;&#35937;&#20960;&#20309;&#12290;&#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#20998;&#23376;&#26500;&#35937;&#38656;&#35201;&#23398;&#20064;&#36870;&#21521;&#22122;&#22768;&#36807;&#31243;&#12290;&#20351;&#29992;&#21407;&#23376;&#38388;&#36317;&#31163;&#25193;&#25955;&#32780;&#19981;&#26159;&#26500;&#35937;&#26469;&#20445;&#25345;SE(3)-&#31561;&#20215;&#24615;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#25216;&#26415;&#30456;&#27604;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#30456;&#20851;&#30340;&#29983;&#25104;&#27169;&#22411;&#20027;&#35201;&#22522;&#20110;&#21551;&#21457;&#24335;&#20551;&#35774;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#20854;&#21160;&#26426;&#26159;&#35748;&#20026;&#20998;&#23376;&#30340;&#35299;&#31163;&#21487;&#20197;&#34987;&#35270;&#20026;&#23545;&#20854;&#32452;&#25104;&#21407;&#23376;&#26045;&#21152;&#36880;&#28176;&#22686;&#22823;&#30340;&#21147;&#22330;&#65292;&#20174;&#32780;&#20351;&#24471;&#21407;&#23376;&#38388;&#36317;&#31163;&#30340;&#21464;&#21270;&#20998;&#24067;&#20174;&#39640;&#26031;&#20998;&#24067;&#36716;&#21464;&#20026;&#40614;&#20811;&#26031;&#38886;-&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#12290;&#30456;&#24212;&#30340;&#29983;&#25104;&#27169;&#22411;&#30830;&#20445;&#20102;&#21487;&#34892;&#30340;&#21407;&#23376;&#38388;&#36317;&#31163;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#21576;&#29616;&#26102;&#38388;&#21487;&#36870;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular conformation generation, a critical aspect of computational chemistry, involves producing the three-dimensional conformer geometry for a given molecule. Generating molecular conformation via diffusion requires learning to reverse a noising process. Diffusion on inter-atomic distances instead of conformation preserves SE(3)-equivalence and shows superior performance compared to alternative techniques, whereas related generative modelings are predominantly based upon heuristical assumptions. In response to this, we propose a novel molecular conformation generation approach driven by the observation that the disintegration of a molecule can be viewed as casting increasing force fields to its composing atoms, such that the distribution of the change of inter-atomic distance shifts from Gaussian to Maxwell-Boltzmann distribution. The corresponding generative modeling ensures a feasible inter-atomic distance geometry and exhibits time reversibility. Experimental results on molecula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#22788;&#29702;&#25968;&#25454;&#39532;&#25289;&#26494;&#20013;&#30340;&#25968;&#25454;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#24314;&#35758;&#65292;&#36890;&#36807;10&#20010;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09770</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#25968;&#25454;&#39532;&#25289;&#26494;&#20013;&#22788;&#29702;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
How to Data in Datathons. (arXiv:2309.09770v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#22788;&#29702;&#25968;&#25454;&#39532;&#25289;&#26494;&#20013;&#30340;&#25968;&#25454;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#24314;&#35758;&#65292;&#36890;&#36807;10&#20010;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39532;&#25289;&#26494;&#30340;&#20852;&#36215;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#30701;&#26102;&#38388;&#20869;&#21512;&#20316;&#12289;&#23398;&#20064;&#21644;&#21019;&#26032;&#30340;&#24179;&#21488;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#20294;&#32452;&#32455;&#24448;&#24448;&#22240;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#26368;&#20339;&#23454;&#36341;&#32780;&#38590;&#20197;&#26377;&#25928;&#22788;&#29702;&#25968;&#25454;&#12290;&#26681;&#25454;&#25105;&#20204;&#33258;&#24049;&#30340;&#32463;&#39564;&#20197;&#21450;&#33258;2016&#24180;&#20197;&#26469;&#32452;&#32455;&#20102;&#36229;&#36807;80&#20010;&#25968;&#25454;&#39532;&#25289;&#26494;&#25361;&#25112;&#36187;&#19982;60&#20010;&#21512;&#20316;&#20249;&#20276;&#32452;&#32455;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25351;&#23548;&#26041;&#38024;&#21644;&#24314;&#35758;&#65292;&#20316;&#20026;&#32452;&#32455;&#32773;&#22312;&#22788;&#29702;&#25968;&#25454;&#30456;&#20851;&#22797;&#26434;&#24615;&#26102;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;10&#20010;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of datathons, also known as data or data science hackathons, has provided a platform to collaborate, learn, and innovate in a short timeframe. Despite their significant potential benefits, organizations often struggle to effectively work with data due to a lack of clear guidelines and best practices for potential issues that might arise. Drawing on our own experiences and insights from organizing &gt;80 datathon challenges with &gt;60 partnership organizations since 2016, we provide guidelines and recommendations that serve as a resource for organizers to navigate the data-related complexities of datathons. We apply our proposed framework to 10 case studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33041;&#21551;&#21457;&#24335;&#30340;&#21487;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#32452;&#32455;&#35843;&#33410;&#32593;&#32476;&#23558;&#21333;&#19968;&#26377;&#38480;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#37325;&#26032;&#32452;&#32455;&#20026;&#20016;&#23500;&#30340;&#31232;&#30095;&#31070;&#32463;&#36335;&#24452;&#65292;&#20197;&#39640;&#25928;&#24212;&#23545;&#36882;&#22686;&#20219;&#21153;&#65292;&#24182;&#22312;&#21508;&#31181;&#21487;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#20197;&#21450;&#27867;&#21270;&#30340;CIFAR100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#20248;&#21183;&#12289;&#33021;&#32791;&#21644;&#20869;&#23384;&#23481;&#37327;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.09550</link><description>&lt;p&gt;
&#20855;&#26377;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25345;&#32493;&#23398;&#20064;&#30340;&#31070;&#32463;&#36335;&#24452;&#30340;&#33258;&#36866;&#24212;&#37325;&#32452;
&lt;/p&gt;
&lt;p&gt;
Adaptive Reorganization of Neural Pathways for Continual Learning with Spiking Neural Networks. (arXiv:2309.09550v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33041;&#21551;&#21457;&#24335;&#30340;&#21487;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#32452;&#32455;&#35843;&#33410;&#32593;&#32476;&#23558;&#21333;&#19968;&#26377;&#38480;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#37325;&#26032;&#32452;&#32455;&#20026;&#20016;&#23500;&#30340;&#31232;&#30095;&#31070;&#32463;&#36335;&#24452;&#65292;&#20197;&#39640;&#25928;&#24212;&#23545;&#36882;&#22686;&#20219;&#21153;&#65292;&#24182;&#22312;&#21508;&#31181;&#21487;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#20197;&#21450;&#27867;&#21270;&#30340;CIFAR100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#20248;&#21183;&#12289;&#33021;&#32791;&#21644;&#20869;&#23384;&#23481;&#37327;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33041;&#21487;&#20197;&#33258;&#32452;&#32455;&#20986;&#20016;&#23500;&#22810;&#26679;&#30340;&#31232;&#30095;&#31070;&#32463;&#36335;&#24452;&#65292;&#36880;&#27493;&#25484;&#25569;&#25968;&#30334;&#20010;&#35748;&#30693;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#28145;&#24230;&#20154;&#24037;&#21644;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#26080;&#27861;&#20805;&#20998;&#33258;&#21160;&#35843;&#33410;&#32593;&#32476;&#20013;&#26377;&#38480;&#30340;&#36164;&#28304;&#65292;&#36825;&#23548;&#33268;&#38543;&#30528;&#20219;&#21153;&#22686;&#21152;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#33021;&#32791;&#19978;&#21319;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33041;&#21551;&#21457;&#24335;&#30340;&#21487;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#32452;&#32455;&#35843;&#33410;&#32593;&#32476;&#23558;&#21333;&#19968;&#26377;&#38480;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SOR-SNN&#65289;&#37325;&#26032;&#32452;&#32455;&#20026;&#20016;&#23500;&#30340;&#31232;&#30095;&#31070;&#32463;&#36335;&#24452;&#65292;&#20197;&#39640;&#25928;&#24212;&#23545;&#36882;&#22686;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#21487;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#20248;&#21183;&#12289;&#33021;&#32791;&#21644;&#20869;&#23384;&#23481;&#37327;&#20248;&#21183;&#65292;&#21253;&#25324;&#20174;&#20799;&#31461;&#31616;&#21333;&#20219;&#21153;&#21040;&#22797;&#26434;&#20219;&#21153;&#12289;&#20197;&#21450;&#27867;&#21270;&#30340;CIFAR100&#21644;ImageNet&#25968;&#25454;&#38598;&#12290;&#23588;&#20854;&#26159;&#65292;SOR-SNN&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12289;&#33021;&#32791;&#21644;&#20869;&#23384;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human brain can self-organize rich and diverse sparse neural pathways to incrementally master hundreds of cognitive tasks. However, most existing continual learning algorithms for deep artificial and spiking neural networks are unable to adequately auto-regulate the limited resources in the network, which leads to performance drop along with energy consumption rise as the increase of tasks. In this paper, we propose a brain-inspired continual learning algorithm with adaptive reorganization of neural pathways, which employs Self-Organizing Regulation networks to reorganize the single and limited Spiking Neural Network (SOR-SNN) into rich sparse neural pathways to efficiently cope with incremental tasks. The proposed model demonstrates consistent superiority in performance, energy consumption, and memory capacity on diverse continual learning tasks ranging from child-like simple to complex tasks, as well as on generalized CIFAR100 and ImageNet datasets. In particular, the SOR-SNN mod
&lt;/p&gt;</description></item><item><title>FunCodec&#26159;&#19968;&#20010;&#22522;&#30784;&#30340;&#12289;&#21487;&#22797;&#29616;&#30340;&#12289;&#21487;&#25972;&#21512;&#30340;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#37325;&#26500;&#36136;&#37327;&#21644;&#25972;&#21512;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2309.07405</link><description>&lt;p&gt;
FunCodec:&#19968;&#20010;&#22522;&#30784;&#30340;&#12289;&#21487;&#22797;&#29616;&#30340;&#12289;&#21487;&#25972;&#21512;&#30340;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#24320;&#28304;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec. (arXiv:2309.07405v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07405
&lt;/p&gt;
&lt;p&gt;
FunCodec&#26159;&#19968;&#20010;&#22522;&#30784;&#30340;&#12289;&#21487;&#22797;&#29616;&#30340;&#12289;&#21487;&#25972;&#21512;&#30340;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#37325;&#26500;&#36136;&#37327;&#21644;&#25972;&#21512;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FunCodec&#65292;&#19968;&#20010;&#22522;&#30784;&#30340;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#24037;&#20855;&#21253;&#65292;&#23427;&#26159;&#24320;&#28304;&#35821;&#38899;&#22788;&#29702;&#24037;&#20855;&#21253;FunASR&#30340;&#25193;&#23637;&#12290;FunCodec&#25552;&#20379;&#20102;&#21487;&#22797;&#29616;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#25512;&#26029;&#33050;&#26412;&#65292;&#29992;&#20110;&#26368;&#26032;&#30340;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#22914;SoundStream&#21644;Encodec&#12290;&#30001;&#20110;&#19982;FunASR&#30340;&#32479;&#19968;&#35774;&#35745;&#65292;FunCodec&#21487;&#20197;&#36731;&#26494;&#22320;&#25972;&#21512;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22914;&#35821;&#38899;&#35782;&#21035;&#12290;&#38500;&#20102;FunCodec&#65292;&#36824;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#23398;&#26415;&#25110;&#26222;&#36941;&#29992;&#36884;&#12290;&#22522;&#20110;&#35813;&#24037;&#20855;&#21253;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#39057;&#22495;&#32534;&#35299;&#30721;&#22120;&#27169;&#22411;FreqCodec&#65292;&#23427;&#21487;&#20197;&#20197;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#21442;&#25968;&#22797;&#26434;&#24230;&#23454;&#29616;&#30456;&#24403;&#30340;&#35821;&#38899;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#21387;&#32553;&#27604;&#19979;&#65292;FunCodec&#21487;&#20197;&#23454;&#29616;&#19982;&#20854;&#20182;&#24037;&#20855;&#21253;&#21644;&#21457;&#24067;&#27169;&#22411;&#30456;&#27604;&#26356;&#22909;&#30340;&#37325;&#26500;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#29992;&#20110;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#22312;&#20869;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pre-trained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;Attention Loss Adjusted Prioritized Experience Replay (ALAP)&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#21452;&#37319;&#26679;&#26426;&#21046;&#65292;&#35843;&#33410;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#65292;&#28040;&#38500;&#20102;&#20808;&#36827;&#30340;&#32463;&#39564;&#22238;&#25918;&#31639;&#27861;&#20013;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#22312;OPENAI gym&#29615;&#22659;&#20013;&#30340;&#27979;&#35797;&#21644;&#23545;&#27604;&#30740;&#31350;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#21183;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.06684</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;Attention Loss Adjusted Prioritized Experience Replay&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attention Loss Adjusted Prioritized Experience Replay. (arXiv:2309.06684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;Attention Loss Adjusted Prioritized Experience Replay (ALAP)&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#21452;&#37319;&#26679;&#26426;&#21046;&#65292;&#35843;&#33410;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#65292;&#28040;&#38500;&#20102;&#20808;&#36827;&#30340;&#32463;&#39564;&#22238;&#25918;&#31639;&#27861;&#20013;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#22312;OPENAI gym&#29615;&#22659;&#20013;&#30340;&#27979;&#35797;&#21644;&#23545;&#27604;&#30740;&#31350;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#21183;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#32463;&#39564;&#22238;&#25918;&#31639;&#27861;(Prioritized Experience Replay, PER)&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#26356;&#22810;&#30693;&#35782;&#37327;&#30340;&#32463;&#39564;&#26679;&#26412;&#26469;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;PER&#20013;&#20351;&#29992;&#30340;&#38750;&#22343;&#21248;&#37319;&#26679;&#19981;&#21487;&#36991;&#20813;&#22320;&#20351;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#24102;&#26469;Q&#20540;&#20989;&#25968;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Attention Loss Adjusted Prioritized (ALAP) Experience Replay&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#21452;&#37319;&#26679;&#26426;&#21046;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#36866;&#24212;&#33021;&#22815;&#35843;&#33410;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#30340;&#36229;&#21442;&#25968;&#65292;&#20174;&#32780;&#28040;&#38500;&#22240;PER&#24341;&#36215;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#20026;&#20102;&#39564;&#35777;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#22312;OPENAI gym&#29615;&#22659;&#20013;&#23545;&#22522;&#20110;&#20540;&#20989;&#25968;&#12289;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#21644;&#22810;&#20027;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#36827;&#34892;&#20102;&#23545;&#27604;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26694;&#26550;&#30340;&#20248;&#21183;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prioritized Experience Replay (PER) is a technical means of deep reinforcement learning by selecting experience samples with more knowledge quantity to improve the training rate of neural network. However, the non-uniform sampling used in PER inevitably shifts the state-action space distribution and brings the estimation error of Q-value function. In this paper, an Attention Loss Adjusted Prioritized (ALAP) Experience Replay algorithm is proposed, which integrates the improved Self-Attention network with Double-Sampling mechanism to fit the hyperparameter that can regulate the importance sampling weights to eliminate the estimation error caused by PER. In order to verify the effectiveness and generality of the algorithm, the ALAP is tested with value-function based, policy-gradient based and multi-agent reinforcement learning algorithms in OPENAI gym, and comparison studies verify the advantage and efficiency of the proposed training framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#25300;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#36866;&#24212;&#19981;&#21516;&#30340;&#37319;&#26679;&#35774;&#32622;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#27424;&#37319;&#26679;&#27169;&#24335;&#21644;&#37319;&#26679;&#29575;&#19979;&#25552;&#20379;&#20102;&#33391;&#22909;&#32780;&#31283;&#20581;&#30340;&#21152;&#36895;&#22270;&#20687;&#37325;&#24314;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06681</link><description>&lt;p&gt;
&#22522;&#20110;&#25554;&#25300;&#24335;&#21512;&#25104;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#27424;&#37319;&#26679;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
A plug-and-play synthetic data deep learning for undersampled magnetic resonance image reconstruction. (arXiv:2309.06681v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#25300;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#36866;&#24212;&#19981;&#21516;&#30340;&#37319;&#26679;&#35774;&#32622;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#27424;&#37319;&#26679;&#27169;&#24335;&#21644;&#37319;&#26679;&#29575;&#19979;&#25552;&#20379;&#20102;&#33391;&#22909;&#32780;&#31283;&#20581;&#30340;&#21152;&#36895;&#22270;&#20687;&#37325;&#24314;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#22312;&#29616;&#20195;&#21307;&#23398;&#35786;&#26029;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#25195;&#25551;&#26102;&#38388;&#36739;&#38271;&#12290;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22270;&#20687;&#21435;&#28151;&#21472;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#30340;k&#31354;&#38388;&#27424;&#37319;&#26679;&#22330;&#26223;&#36827;&#34892;&#23450;&#21046;&#21270;&#12290;&#20294;&#26159;&#24403;&#37319;&#26679;&#35774;&#32622;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#37197;&#32622;&#19981;&#21516;&#30340;&#28145;&#24230;&#32593;&#32476;&#38750;&#24120;&#40635;&#28902;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#25300;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#36866;&#24212;&#19981;&#21516;&#30340;&#37319;&#26679;&#35774;&#32622;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#36890;&#36807;&#19968;&#20010;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#21435;&#22122;&#32593;&#32476;&#23398;&#20064;&#22270;&#20687;&#21435;&#28151;&#21472;&#20808;&#39564;&#30693;&#35782;&#65292;&#28982;&#21518;&#23558;&#23398;&#20064;&#21040;&#30340;&#28145;&#24230;&#21435;&#22122;&#32593;&#32476;&#25554;&#20837;&#21040;&#36845;&#20195;&#31639;&#27861;&#20013;&#36827;&#34892;&#22270;&#20687;&#37325;&#24314;&#12290;&#36890;&#36807;&#23545;&#20307;&#20869;&#25968;&#25454;&#30340;&#32467;&#26524;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#27424;&#37319;&#26679;&#27169;&#24335;&#21644;&#37319;&#26679;&#29575;&#19979;&#25552;&#20379;&#20102;&#33391;&#22909;&#32780;&#31283;&#20581;&#30340;&#21152;&#36895;&#22270;&#20687;&#37325;&#24314;&#24615;&#33021;&#65292;&#20174;&#35270;&#35273;&#21644;&#23450;&#37327;&#25351;&#26631;&#19978;&#22343;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Magnetic resonance imaging (MRI) plays an important role in modern medical diagnostic but suffers from prolonged scan time. Current deep learning methods for undersampled MRI reconstruction exhibit good performance in image de-aliasing which can be tailored to the specific kspace undersampling scenario. But it is very troublesome to configure different deep networks when the sampling setting changes. In this work, we propose a deep plug-and-play method for undersampled MRI reconstruction, which effectively adapts to different sampling settings. Specifically, the image de-aliasing prior is first learned by a deep denoiser trained to remove general white Gaussian noise from synthetic data. Then the learned deep denoiser is plugged into an iterative algorithm for image reconstruction. Results on in vivo data demonstrate that the proposed method provides nice and robust accelerated image reconstruction performance under different undersampling patterns and sampling rates, both visually and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#26426;&#21046;&#65292;&#28789;&#27963;&#30340;&#25513;&#27169;&#21644;&#29420;&#29305;&#30340;&#32593;&#32476;&#21098;&#26525;&#65292;&#20351;&#24471;&#19968;&#20010;&#21069;&#39304;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#65292;&#24182;&#19988;&#22312;&#22270;&#24418;&#27169;&#22411;&#20013;&#21487;&#20197;&#29992;&#26469;&#27979;&#35797;&#20381;&#36182;&#20851;&#31995;&#25110;&#20132;&#20114;&#39034;&#24207;&#12290;</title><link>http://arxiv.org/abs/2309.06382</link><description>&lt;p&gt;
&#38598;&#25104;&#25513;&#27169;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Ensemble Mask Networks. (arXiv:2309.06382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#26426;&#21046;&#65292;&#28789;&#27963;&#30340;&#25513;&#27169;&#21644;&#29420;&#29305;&#30340;&#32593;&#32476;&#21098;&#26525;&#65292;&#20351;&#24471;&#19968;&#20010;&#21069;&#39304;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#65292;&#24182;&#19988;&#22312;&#22270;&#24418;&#27169;&#22411;&#20013;&#21487;&#20197;&#29992;&#26469;&#27979;&#35797;&#20381;&#36182;&#20851;&#31995;&#25110;&#20132;&#20114;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;$\mathbb{R}^n\rightarrow \mathbb{R}^n$&#30340;&#21069;&#39304;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#21527;&#65311;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#26426;&#21046;&#65306;&#28789;&#27963;&#30340;&#25513;&#27169;&#29992;&#20110;&#25509;&#25910;&#30697;&#38453;&#36755;&#20837;&#65292;&#20197;&#21450;&#19968;&#31181;&#29420;&#29305;&#30340;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#20197;&#23562;&#37325;&#25513;&#27169;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;&#32593;&#32476;&#21487;&#20197;&#36817;&#20284;&#22266;&#23450;&#25805;&#20316;&#65292;&#22914;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;$\phi(A,x) \rightarrow Ax$&#65292;&#36825;&#28608;&#21457;&#20102;&#24341;&#20837;&#30340;&#26426;&#21046;&#22312;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#20013;&#27979;&#35797;&#20381;&#36182;&#20851;&#31995;&#25110;&#20132;&#20114;&#39034;&#24207;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can an $\mathbb{R}^n\rightarrow \mathbb{R}^n$ feedforward network learn matrix-vector multiplication? This study introduces two mechanisms - flexible masking to take matrix inputs, and a unique network pruning to respect the mask's dependency structure. Networks can approximate fixed operations such as matrix-vector multiplication $\phi(A,x) \rightarrow Ax$, motivating the mechanisms introduced with applications towards litmus-testing dependencies or interaction order in graph-based models.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#30001;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#32534;&#35793;&#30340;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#21333;&#20301;&#32763;&#36716;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#35774;&#35745;&#20102;&#33258;&#21160;&#25628;&#32034;&#24037;&#20855;&#20197;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#20301;&#65292;&#24182;&#30830;&#23450;&#20102;&#23454;&#38469;&#25915;&#20987;&#21521;&#37327;&#65292;&#25581;&#31034;&#20102;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#25915;&#20987;&#38754;&#12290;</title><link>http://arxiv.org/abs/2309.06223</link><description>&lt;p&gt;
&#25581;&#31034;&#23545;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#21333;&#20301;&#32763;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Unveiling Signle-Bit-Flip Attacks on DNN Executables. (arXiv:2309.06223v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06223
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30001;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#32534;&#35793;&#30340;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#21333;&#20301;&#32763;&#36716;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#35774;&#35745;&#20102;&#33258;&#21160;&#25628;&#32034;&#24037;&#20855;&#20197;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#20301;&#65292;&#24182;&#30830;&#23450;&#20102;&#23454;&#38469;&#25915;&#20987;&#21521;&#37327;&#65292;&#25581;&#31034;&#20102;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#25915;&#20987;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20301;&#32763;&#36716;&#25915;&#20987;(BFA)&#21487;&#20197;&#36890;&#36807;DRAM Rowhammer&#21033;&#29992;&#26469;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#20027;&#35201;&#38024;&#23545;&#39640;&#32423;DNN&#26694;&#26550;&#65288;&#22914;PyTorch&#65289;&#20013;&#30340;&#27169;&#22411;&#26435;&#37325;&#25991;&#20214;&#36827;&#34892;&#20301;&#32763;&#36716;&#12290;&#28982;&#32780;&#65292;DNN&#32463;&#24120;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#32534;&#35793;&#25104;&#20302;&#32423;&#21487;&#25191;&#34892;&#25991;&#20214;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20302;&#32423;&#30828;&#20214;&#21407;&#35821;&#12290;&#32534;&#35793;&#21518;&#30340;&#20195;&#30721;&#36890;&#24120;&#36895;&#24230;&#24456;&#24555;&#65292;&#24182;&#19988;&#19982;&#39640;&#32423;DNN&#26694;&#26550;&#20855;&#26377;&#26126;&#26174;&#19981;&#21516;&#30340;&#25191;&#34892;&#33539;&#24335;&#12290;&#26412;&#25991;&#38024;&#23545;&#30001;DL&#32534;&#35793;&#22120;&#32534;&#35793;&#30340;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;BFA&#25915;&#20987;&#38754;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#21160;&#25628;&#32034;&#24037;&#20855;&#65292;&#29992;&#20110;&#35782;&#21035;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#20013;&#30340;&#26131;&#21463;&#25915;&#20987;&#20301;&#65292;&#24182;&#30830;&#23450;&#21033;&#29992;BFAs&#25915;&#20987;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#20013;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#23454;&#38469;&#25915;&#20987;&#21521;&#37327;&#65288;&#32780;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#23545;&#25915;&#20987;&#27169;&#22411;&#26435;&#37325;&#20570;&#20986;&#20102;&#24378;&#20551;&#35774;&#65289;&#12290;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#20284;&#20046;&#27604;&#39640;&#32423;DNN&#20013;&#30340;&#27169;&#22411;&#26356;&#21152;&#8220;&#19981;&#36879;&#26126;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown that bit-flip attacks (BFAs) can manipulate deep neural networks (DNNs) via DRAM Rowhammer exploitations. Existing attacks are primarily launched over high-level DNN frameworks like PyTorch and flip bits in model weight files. Nevertheless, DNNs are frequently compiled into low-level executables by deep learning (DL) compilers to fully leverage low-level hardware primitives. The compiled code is usually high-speed and manifests dramatically distinct execution paradigms from high-level DNN frameworks.  In this paper, we launch the first systematic study on the attack surface of BFA specifically for DNN executables compiled by DL compilers. We design an automated search tool to identify vulnerable bits in DNN executables and identify practical attack vectors that exploit the model structure in DNN executables with BFAs (whereas prior works make likely strong assumptions to attack model weights). DNN executables appear more "opaque" than models in high-level DNN 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#31579;&#36873;&#31995;&#32479;&#24615;&#23457;&#26597;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25506;&#32034;&#20351;&#29992;&#19981;&#21516;&#30340;&#26597;&#35810;&#26469;&#28304;&#65292;&#22914;&#29992;&#20110;&#26816;&#32034;&#25991;&#26723;&#21644;&#22522;&#20110;&#25351;&#20196;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#31579;&#36873;&#36807;&#31243;&#20013;&#26356;&#20934;&#30830;&#22320;&#25490;&#21517;&#37325;&#35201;&#25991;&#26723;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05238</link><description>&lt;p&gt;
&#20026;&#26356;&#26377;&#25928;&#30340;&#31995;&#32479;&#24615;&#23457;&#26597;&#31579;&#36873;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Generating Natural Language Queries for More Effective Systematic Review Screening Prioritisation. (arXiv:2309.05238v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#31579;&#36873;&#31995;&#32479;&#24615;&#23457;&#26597;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25506;&#32034;&#20351;&#29992;&#19981;&#21516;&#30340;&#26597;&#35810;&#26469;&#28304;&#65292;&#22914;&#29992;&#20110;&#26816;&#32034;&#25991;&#26723;&#21644;&#22522;&#20110;&#25351;&#20196;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#31579;&#36873;&#36807;&#31243;&#20013;&#26356;&#20934;&#30830;&#22320;&#25490;&#21517;&#37325;&#35201;&#25991;&#26723;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#31995;&#32479;&#24615;&#23457;&#26597;&#20013;&#30340;&#31579;&#36873;&#20248;&#20808;&#32423;&#30446;&#26631;&#26159;&#36890;&#36807;&#22797;&#26434;&#30340;&#24067;&#23572;&#26597;&#35810;&#23545;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#38598;&#36827;&#34892;&#25490;&#21517;&#12290;&#20248;&#20808;&#22788;&#29702;&#26368;&#37325;&#35201;&#30340;&#25991;&#26723;&#21487;&#20197;&#30830;&#20445;&#21518;&#32493;&#23457;&#26597;&#27493;&#39588;&#33021;&#22815;&#26356;&#39640;&#25928;&#12289;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#12290;&#30446;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#20351;&#29992;&#23457;&#26597;&#30340;&#26368;&#32456;&#26631;&#39064;&#20316;&#20026;&#26597;&#35810;&#65292;&#21033;&#29992;&#22522;&#20110;BERT&#30340;&#31070;&#32463;&#25490;&#24207;&#22120;&#23545;&#25991;&#26723;&#36827;&#34892;&#25490;&#21517;&#12290;&#28982;&#32780;&#65292;&#26368;&#32456;&#26631;&#39064;&#21482;&#22312;&#23457;&#26597;&#36807;&#31243;&#32467;&#26463;&#26102;&#24418;&#25104;&#65292;&#36825;&#20351;&#24471;&#35813;&#26041;&#27861;&#19981;&#20999;&#23454;&#38469;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;ex post facto&#30340;&#20449;&#24687;&#12290;&#22312;&#31579;&#36873;&#30340;&#26102;&#20505;&#65292;&#21482;&#26377;&#19968;&#20010;&#31895;&#30053;&#30340;&#24037;&#20316;&#26631;&#39064;&#21487;&#29992;&#65292;&#20351;&#29992;BERT-based&#25490;&#24207;&#22120;&#26102;&#25928;&#26524;&#26126;&#26174;&#19981;&#22914;&#26368;&#32456;&#26631;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29992;&#20110;&#31579;&#36873;&#20248;&#20808;&#32423;&#30340;&#26597;&#35810;&#30340;&#26367;&#20195;&#26469;&#28304;&#65292;&#20363;&#22914;&#29992;&#20110;&#26816;&#32034;&#24453;&#31579;&#36873;&#25991;&#26723;&#30340;&#24067;&#23572;&#26597;&#35810;&#65292;&#20197;&#21450;&#30001;&#22522;&#20110;&#25351;&#20196;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;Alpaca&#65289;&#29983;&#25104;&#30340;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#26041;&#27861;&#19981;&#20165;&#20165;&#26159;
&lt;/p&gt;
&lt;p&gt;
Screening prioritisation in medical systematic reviews aims to rank the set of documents retrieved by complex Boolean queries. Prioritising the most important documents ensures that subsequent review steps can be carried out more efficiently and effectively. The current state of the art uses the final title of the review as a query to rank the documents using BERT-based neural rankers. However, the final title is only formulated at the end of the review process, which makes this approach impractical as it relies on ex post facto information. At the time of screening, only a rough working title is available, with which the BERT-based ranker performs significantly worse than with the final title. In this paper, we explore alternative sources of queries for prioritising screening, such as the Boolean query used to retrieve the documents to be screened and queries generated by instruction-based generative large-scale language models such as ChatGPT and Alpaca. Our best approach is not only
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#27169;&#22359;&#21270;&#26435;&#37325;&#21644;&#39046;&#22495;&#36801;&#31227;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#26435;&#37325;&#25513;&#30721;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#20445;&#25345;&#28304;&#20219;&#21153;&#30693;&#35782;&#30340;&#21516;&#26102;&#20801;&#35768;&#39640;&#25928;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13957</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#26435;&#37325;&#25513;&#30721;&#29992;&#20110;&#39046;&#22495;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Differentiable Weight Masks for Domain Transfer. (arXiv:2308.13957v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#27169;&#22359;&#21270;&#26435;&#37325;&#21644;&#39046;&#22495;&#36801;&#31227;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#26435;&#37325;&#25513;&#30721;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#20445;&#25345;&#28304;&#20219;&#21153;&#30693;&#35782;&#30340;&#21516;&#26102;&#20801;&#35768;&#39640;&#25928;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#26080;&#27861;&#20197;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#20445;&#30041;&#22810;&#20010;&#20449;&#24687;&#28304;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#20010;&#22312;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#36807;&#30340;&#32593;&#32476;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#20445;&#25345;&#20854;&#22312;&#28304;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23558;&#20854;&#37325;&#26032;&#35757;&#32451;&#21040;&#19968;&#20010;&#30456;&#20284;&#20294;&#19981;&#21516;&#30340;&#30446;&#26631;&#20219;&#21153;&#19978;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#32593;&#32476;&#26435;&#37325;&#30340;&#27169;&#22359;&#21270;&#65292;&#20197;&#23450;&#20301;&#21644;&#30830;&#23450;&#23545;&#20110;&#35302;&#21457;&#32473;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#26435;&#37325;&#38598;&#21512;&#12290;&#19968;&#20123;&#24037;&#20316;&#30740;&#31350;&#20102;&#36890;&#36807;&#23398;&#20064;&#21644;&#20998;&#26512;&#26435;&#37325;&#25513;&#30721;&#24341;&#20837;&#30340;&#32593;&#32476;&#26435;&#37325;&#30340;&#27169;&#22359;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#39046;&#22495;&#32467;&#21512;&#36215;&#26469;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#26435;&#37325;&#25513;&#30721;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#32531;&#35299;&#28304;&#20219;&#21153;&#30340;&#8220;&#36951;&#24536;&#8221;&#21516;&#26102;&#20801;&#35768;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#39640;&#25928;&#24494;&#35843;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#25513;&#30721;&#25216;&#26415;&#22312;&#20445;&#30041;&#28304;&#20219;&#21153;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the major drawbacks of deep learning models for computer vision has been their inability to retain multiple sources of information in a modular fashion. For instance, given a network that has been trained on a source task, we would like to re-train this network on a similar, yet different, target task while maintaining its performance on the source task. Simultaneously, researchers have extensively studied modularization of network weights to localize and identify the set of weights culpable for eliciting the observed performance on a given task. One set of works studies the modularization induced in the weights of a neural network by learning and analysing weight masks. In this work, we combine these fields to study three such weight masking methods and analyse their ability to mitigate "forgetting'' on the source task while also allowing for efficient finetuning on the target task. We find that different masking techniques have trade-offs in retaining knowledge in the source t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13570</link><description>&lt;p&gt;
&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38543;&#26426;&#37197;&#32622;&#26426;
&lt;/p&gt;
&lt;p&gt;
Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#65288;IAI&#65289;&#20013;&#65292;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#24314;&#27169;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#24378;&#22823;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#35774;&#22791;&#26469;&#22788;&#29702;&#22823;&#37327;&#30340;&#28014;&#28857;&#25968;&#25454;&#12290;&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20197;&#24378;&#35843;&#23545;&#20110;&#24037;&#19994;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#21644;&#26377;&#20215;&#20540;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;&#19982;&#20855;&#26377;&#20108;&#20540;&#21270;&#23454;&#29616;&#30340;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#30456;&#27604;&#65292;SCMs&#30340;&#27169;&#22411;&#23384;&#20648;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#38500;&#20102;SCM&#23398;&#20064;&#22120;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#20316;&#20026;&#26412;&#25991;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;SCMs&#30340;&#23398;&#20064;&#33021;&#21147;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23454;&#39564;&#30740;&#31350;&#20063;&#36827;&#34892;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20249;&#20276;&#33310;&#32773;&#29983;&#25104;&#30340;&#22810;&#33310;&#32773;&#21512;&#25104;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#20445;&#25345;&#19982;&#20027;&#23548;&#33310;&#32773;&#26102;&#38388;&#21327;&#35843;&#30340;&#21516;&#26102;&#30830;&#20445;&#20249;&#20276;&#33310;&#32773;&#30340;&#21487;&#25511;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#19982;&#20320;&#20849;&#33310;&#8221;&#30340;&#19977;&#38454;&#27573;&#26694;&#26550;&#65288;DanY&#65289;&#65292;&#23427;&#33021;&#33258;&#21160;&#35774;&#35745;&#20249;&#20276;&#33310;&#32773;&#30340;&#23039;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.13551</link><description>&lt;p&gt;
&#19982;&#20320;&#20849;&#33310;&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#22810;&#26679;&#24615;&#21487;&#25511;&#30340;&#33310;&#32773;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dance with You: The Diversity Controllable Dancer Generation via Diffusion Models. (arXiv:2308.13551v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20249;&#20276;&#33310;&#32773;&#29983;&#25104;&#30340;&#22810;&#33310;&#32773;&#21512;&#25104;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#20445;&#25345;&#19982;&#20027;&#23548;&#33310;&#32773;&#26102;&#38388;&#21327;&#35843;&#30340;&#21516;&#26102;&#30830;&#20445;&#20249;&#20276;&#33310;&#32773;&#30340;&#21487;&#25511;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#19982;&#20320;&#20849;&#33310;&#8221;&#30340;&#19977;&#38454;&#27573;&#26694;&#26550;&#65288;DanY&#65289;&#65292;&#23427;&#33021;&#33258;&#21160;&#35774;&#35745;&#20249;&#20276;&#33310;&#32773;&#30340;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#34394;&#25311;&#29615;&#22659;&#20013;&#29992;&#20110;&#20154;&#38469;&#20132;&#20114;&#30340;&#25968;&#23383;&#20154;&#31867;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#22810;&#33310;&#32773;&#21512;&#25104;&#20219;&#21153;&#65292;&#31216;&#20026;&#20249;&#20276;&#33310;&#32773;&#29983;&#25104;&#65292;&#20854;&#28041;&#21450;&#21512;&#25104;&#33021;&#22815;&#19982;&#29992;&#25143;&#19968;&#36215;&#36339;&#33310;&#30340;&#34394;&#25311;&#20154;&#31867;&#33310;&#32773;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#25511;&#21046;&#20027;&#23548;&#33310;&#32773;&#21644;&#20249;&#20276;&#33310;&#32773;&#20043;&#38388;&#30340;&#23039;&#21183;&#22810;&#26679;&#24615;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#26680;&#24515;&#26159;&#30830;&#20445;&#29983;&#25104;&#30340;&#20249;&#20276;&#33310;&#32773;&#20855;&#26377;&#21487;&#25511;&#30340;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#19982;&#20027;&#23548;&#33310;&#32773;&#20445;&#25345;&#26102;&#38388;&#19978;&#30340;&#21327;&#35843;&#12290;&#19982;&#20197;&#24448;&#36890;&#36807;&#38899;&#20048;&#39537;&#21160;&#29983;&#25104;&#33310;&#36424;&#21160;&#20316;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#22810;&#26679;&#24615;&#12289;&#20027;&#23548;&#33310;&#32773;&#30340;&#23039;&#21183;&#20197;&#21450;&#20276;&#22863;&#38899;&#20048;&#33258;&#21160;&#35774;&#35745;&#20249;&#20276;&#33310;&#32773;&#30340;&#23039;&#21183;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#19982;&#20320;&#20849;&#33310;&#8221;&#30340;&#19977;&#38454;&#27573;&#26694;&#26550;&#65288;DanY&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#32500;&#23039;&#21183;&#25910;&#38598;&#38454;&#27573;&#26469;&#25910;&#38598;&#21508;&#31181;&#22522;&#26412;&#33310;&#36424;&#23039;&#21183;&#20316;&#20026;&#21442;&#32771;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, digital humans for interpersonal interaction in virtual environments have gained significant attention. In this paper, we introduce a novel multi-dancer synthesis task called partner dancer generation, which involves synthesizing virtual human dancers capable of performing dance with users. The task aims to control the pose diversity between the lead dancer and the partner dancer. The core of this task is to ensure the controllable diversity of the generated partner dancer while maintaining temporal coordination with the lead dancer. This scenario varies from earlier research in generating dance motions driven by music, as our emphasis is on automatically designing partner dancer postures according to pre-defined diversity, the pose of lead dancer, as well as the accompanying tunes. To achieve this objective, we propose a three-stage framework called Dance-with-You (DanY). Initially, we employ a 3D Pose Collection stage to collect a wide range of basic dance poses as referenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11534</link><description>&lt;p&gt;
&#20316;&#20026;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#28304;ChatGPT&#30340;&#21331;&#36234;&#24615;&#33021;&#24341;&#21457;&#20102;&#23545;&#20854;&#27665;&#20027;&#21270;&#30340;&#21162;&#21147;&#65292;&#20511;&#21161;&#30495;&#23454;&#29992;&#25143;&#21644;ChatGPT&#23545;&#35805;&#30340;&#21162;&#21147;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;Vicuna&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;Baize&#21644;UltraChat&#31561;&#21162;&#21147;&#20027;&#35201;&#20381;&#38752;ChatGPT&#26681;&#25454;&#25351;&#20196;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#30340;&#20154;&#31867;&#23398;&#20064;&#65292;&#23548;&#33268;&#33539;&#22260;&#26377;&#38480;&#65292;&#22810;&#26679;&#24615;&#20943;&#24369;&#65292;&#32570;&#20047;&#30495;&#27491;&#30340;&#22810;&#36718;&#23545;&#35805;&#21160;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#25226;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#12290;&#38543;&#21518;&#65292;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#25105;&#20204;&#30340;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;Vicuna-Bench&#21644;MT-Bench&#20013;&#22343;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, while current endeavors like Baize and UltraChat aim to auto-generate conversational data due to challenges in gathering human participation, they primarily rely on ChatGPT to simulate human behaviors based on directives rather than genuine human learning. This results in a limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we innovatively target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator, UserGPT, to produce a high-quality human-centric synthetic conversation dataset, RealChat. Subsequently, this dataset trains our assistant model, ReaLM. Experimentally, ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
&lt;/p&gt;</description></item><item><title>"&#24341;&#29992;GPT&#30340;&#8220;&#35930;&#40736;&#35797;&#39564;&#8221;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#20225;&#19994;&#36827;&#34892;&#31454;&#20105;&#21644;&#21246;&#32467;&#30740;&#31350;&#12290;&#23427;&#27604;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#23454;&#39564;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#36229;&#36234;&#20256;&#32479;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;"</title><link>http://arxiv.org/abs/2308.10974</link><description>&lt;p&gt;
"&#24341;&#29992;GPT&#30340;&#8220;&#35930;&#40736;&#35797;&#39564;&#8221;&#65306;&#19968;&#31181;&#30740;&#31350;&#20225;&#19994;&#31454;&#20105;&#21644;&#21246;&#32467;&#30340;&#21019;&#26032;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;"
&lt;/p&gt;
&lt;p&gt;
"Guinea Pig Trials" Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion. (arXiv:2308.10974v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10974
&lt;/p&gt;
&lt;p&gt;
"&#24341;&#29992;GPT&#30340;&#8220;&#35930;&#40736;&#35797;&#39564;&#8221;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#20225;&#19994;&#36827;&#34892;&#31454;&#20105;&#21644;&#21246;&#32467;&#30740;&#31350;&#12290;&#23427;&#27604;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#23454;&#39564;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#36229;&#36234;&#20256;&#32479;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#31454;&#20105;&#21644;&#21246;&#32467;&#28041;&#21450;&#22797;&#26434;&#30340;&#21160;&#24577;&#65292;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#20225;&#19994;&#20043;&#38388;&#30340;&#27807;&#36890;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#22797;&#26434;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#20256;&#32479;&#19978;&#36890;&#36807;&#28041;&#21450;&#20154;&#31867;&#20027;&#20307;&#25110;&#22522;&#20110;&#20195;&#29702;&#30340;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#25506;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#65288;SABM&#65289;&#65292;&#20854;&#20013;&#30001;GPT-4&#25216;&#26415;&#25903;&#25345;&#30340;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#20225;&#19994;&#24182;&#30456;&#20114;&#20132;&#20114;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#25511;&#21046;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#26465;&#20214;&#19979;&#20225;&#19994;&#20215;&#26684;&#31454;&#20105;&#21644;&#21246;&#32467;&#34892;&#20026;&#12290;&#19982;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#23454;&#39564;&#30456;&#27604;&#65292;SABM&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#28789;&#27963;&#24615;&#12290;&#26234;&#33021;&#20195;&#29702;&#25317;&#26377;&#20915;&#31574;&#30340;&#24191;&#27867;&#30693;&#35782;&#24211;&#65292;&#23637;&#29616;&#20986;&#31867;&#20284;&#20154;&#31867;&#30340;&#25112;&#30053;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#24314;&#27169;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#24182;&#20010;&#24615;&#21270;&#65292;&#20351;&#20854;&#25104;&#20026;&#30740;&#31350;&#28041;&#21450;&#27807;&#36890;&#30340;&#22797;&#26434;&#24773;&#20917;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Firm competition and collusion involve complex dynamics, particularly when considering communication among firms. Such issues can be modeled as problems of complex systems, traditionally approached through experiments involving human subjects or agent-based modeling methods. We propose an innovative framework called Smart Agent-Based Modeling (SABM), wherein smart agents, supported by GPT-4 technologies, represent firms, and interact with one another. We conducted a controlled experiment to study firm price competition and collusion behaviors under various conditions. SABM is more cost-effective and flexible compared to conducting experiments with human subjects. Smart agents possess an extensive knowledge base for decision-making and exhibit human-like strategic abilities, surpassing traditional ABM agents. Furthermore, smart agents can simulate human conversation and be personalized, making them ideal for studying complex situations involving communication. Our results demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.10792</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#65288;IT&#65289;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#31181;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#25351;&#20197;&#30417;&#30563;&#26041;&#24335;&#22312;&#21253;&#21547;&#8220;&#25351;&#20196;-&#36755;&#20986;&#8221;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;LLM&#65292;&#36825;&#23558;LLM&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#30446;&#26631;&#19982;&#29992;&#25143;&#24076;&#26395;LLM&#36981;&#23432;&#20154;&#31867;&#25351;&#20196;&#30340;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;IT&#30340;&#24120;&#35268;&#26041;&#27861;&#12289;IT&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#12289;IT&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#24212;&#29992;&#20110;&#19981;&#21516;&#27169;&#24577;&#12289;&#39046;&#22495;&#21644;&#24212;&#29992;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23545;&#24433;&#21709;IT&#32467;&#26524;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20998;&#26512;&#65288;&#20363;&#22914;&#65292;&#25351;&#20196;&#36755;&#20986;&#30340;&#29983;&#25104;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#31561;&#65289;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;IT&#30340;&#28508;&#22312;&#38382;&#39064;&#20197;&#21450;&#38024;&#23545;&#20854;&#30340;&#25209;&#35780;&#65292;&#20197;&#21450;&#25351;&#20986;&#24403;&#21069;&#19981;&#36275;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#27604;&#20998;&#26512;&#20102;12&#31181;&#33258;&#28982;&#21551;&#21457;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#21033;&#29992;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#32467;&#21512;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#29305;&#24449;&#38598;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2308.08574</link><description>&lt;p&gt;
&#33258;&#28982;&#21551;&#21457;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#33021;&#21147;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of the Capabilities of Nature-inspired Feature Selection Algorithms in Predicting Student Performance. (arXiv:2308.08574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#27604;&#20998;&#26512;&#20102;12&#31181;&#33258;&#28982;&#21551;&#21457;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#21033;&#29992;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#32467;&#21512;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#29305;&#24449;&#38598;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#23545;&#20110;&#26377;&#25928;&#38450;&#27490;&#39118;&#38505;&#23398;&#29983;&#22833;&#36133;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#22871;12&#20010;&#33258;&#28982;&#21551;&#21457;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#23454;&#20363;&#30340;&#28857;&#20987;&#27969;&#25968;&#25454;&#12289;&#35838;&#20869;&#21333;&#19968;&#35838;&#31243;&#34920;&#29616;&#20197;&#21450;&#21516;&#26102;&#21442;&#21152;&#22810;&#20010;&#35838;&#31243;&#26102;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#21033;&#29992;&#33258;&#28982;&#21551;&#21457;&#30340;&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#32467;&#21512;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#29305;&#24449;&#38598;&#22823;&#23567;&#30340;2/3&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting student performance is key in leveraging effective pre-failure interventions for at-risk students. In this paper, I have analyzed the relative performance of a suite of 12 nature-inspired algorithms when used to predict student performance across 3 datasets consisting of instance-based clickstream data, intra-course single-course performance, and performance when taking multiple courses simultaneously. I found that, for all datasets, leveraging an ensemble approach using NIAs for feature selection and traditional ML algorithms for classification increased predictive accuracy while also reducing feature set size by 2/3.
&lt;/p&gt;</description></item><item><title>NeFL&#26159;&#19968;&#20010;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#21010;&#20998;&#20026;&#23376;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07761</link><description>&lt;p&gt;
NeFL: &#38024;&#23545;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NeFL: Nested Federated Learning for Heterogeneous Clients. (arXiv:2308.07761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07761
&lt;/p&gt;
&lt;p&gt;
NeFL&#26159;&#19968;&#20010;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#21010;&#20998;&#20026;&#23376;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#25345;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#65288;&#21363;&#38459;&#22622;&#32773;&#65289;&#20250;&#20943;&#24930;&#24635;&#20307;&#35757;&#32451;&#26102;&#38388;&#24182;&#38477;&#20302;&#24615;&#33021;&#12290;&#31995;&#32479;&#30340;&#24322;&#26500;&#24615;&#65292;&#21253;&#25324;&#24322;&#26500;&#35745;&#31639;&#21644;&#32593;&#32476;&#24102;&#23485;&#65292;&#24050;&#32463;&#34987;&#29992;&#26469;&#20943;&#36731;&#38459;&#22622;&#32773;&#30340;&#24433;&#21709;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#27169;&#22411;&#20998;&#21106;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;&#27169;&#22411;&#26550;&#26500;&#26041;&#38754;&#30340;&#33258;&#30001;&#24230;&#36739;&#23567;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#65288;NeFL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#20998;&#25104;&#23376;&#27169;&#22411;&#12290;NeFL&#36890;&#36807;&#23558;&#27169;&#22411;&#35299;&#37322;&#20026;&#35299;&#20915;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#27493;&#38271;&#26469;&#23454;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#22810;&#20010;&#23376;&#27169;&#22411;&#26102;&#20986;&#29616;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#35299;&#32806;&#20102;&#19968;&#20123;&#21442;&#25968;&#12290;NeFL&#20351;&#36164;&#28304;&#21463;&#38480;&#30340;&#23458;&#25143;&#31471;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#20837;&#32852;&#37030;&#23398;&#20064;&#27969;&#31243;&#65292;&#24182;&#20351;&#27169;&#22411;&#33021;&#22815;&#34987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31070;&#32463;&#20998;&#31867;&#20808;&#39564;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2308.07200</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#20998;&#31867;&#20808;&#39564;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Categorical Priors for Physics-Based Character Control. (arXiv:2308.07200v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31070;&#32463;&#20998;&#31867;&#20808;&#39564;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#23398;&#20064;&#21487;&#37325;&#29992;&#36816;&#21160;&#20808;&#39564;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#33258;&#28982;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#65292;&#30456;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#36816;&#21160;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#36861;&#36394;&#21644;&#27169;&#20223;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#36816;&#21160;&#21098;&#36753;&#30340;&#36924;&#30495;&#21160;&#20316;&#65292;&#20351;&#29992;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#65292;&#22914;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#20013;&#25152;&#37319;&#29992;&#30340;&#37027;&#26679;&#12290;&#35813;&#32467;&#26500;&#23558;&#26469;&#33258;&#36816;&#21160;&#21098;&#36753;&#30340;&#26368;&#30456;&#20851;&#20449;&#24687;&#21387;&#32553;&#25104;&#19968;&#20010;&#32039;&#20945;&#32780;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21363;&#19968;&#20010;&#31163;&#25955;&#30340;&#21521;&#37327;&#37327;&#21270;&#30721;&#31354;&#38388;&#12290;&#36890;&#36807;&#20174;&#32463;&#36807;&#35757;&#32451;&#30340;&#20998;&#31867;&#20808;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#31354;&#38388;&#20013;&#30340;&#30721;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20351;&#29992;VQ-VAE&#12290;&#34429;&#28982;&#36825;&#20010;&#20808;&#39564;&#20998;&#24067;&#21487;&#20197;&#36890;&#36807;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advances in learning reusable motion priors have demonstrated their effectiveness in generating naturalistic behaviors. In this paper, we propose a new learning framework in this paradigm for controlling physics-based characters with significantly improved motion quality and diversity over existing state-of-the-art methods. The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips using the discrete information bottleneck, as adopted in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure compresses the most relevant information from the motion clips into a compact yet informative latent space, i.e., a discrete space over vector quantized codes. By sampling codes in the space from a trained categorical prior distribution, high-quality life-like behaviors can be generated, similar to the usage of VQ-VAE in computer vision. Although this prior distribution can be trained with the supervis
&lt;/p&gt;</description></item><item><title>ConvFormer&#26159;&#19968;&#31181;&#23545;Transformer&#26550;&#26500;&#36827;&#34892;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#39034;&#24207;&#29992;&#25143;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;Transformer&#30340;&#26680;&#24515;&#26500;&#24314;&#27169;&#22359;&#21644;&#20998;&#26512;&#39033;&#30446;&#23545;&#39033;&#30446;&#26426;&#21046;&#65292;&#22312;&#36827;&#34892;&#23454;&#39564;&#20998;&#26512;&#21518;&#30830;&#23450;&#20102;&#19977;&#20010;&#22522;&#26412;&#26631;&#20934;&#65292;&#24182;&#24341;&#20837;&#20102;ConvFormer&#26469;&#28385;&#36275;&#36825;&#20123;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.02925</link><description>&lt;p&gt;
ConvFormer&#65306;&#37325;&#26032;&#23457;&#35270;Transformer&#29992;&#20110;&#39034;&#24207;&#29992;&#25143;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
ConvFormer: Revisiting Transformer for Sequential User Modeling. (arXiv:2308.02925v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02925
&lt;/p&gt;
&lt;p&gt;
ConvFormer&#26159;&#19968;&#31181;&#23545;Transformer&#26550;&#26500;&#36827;&#34892;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#39034;&#24207;&#29992;&#25143;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;Transformer&#30340;&#26680;&#24515;&#26500;&#24314;&#27169;&#22359;&#21644;&#20998;&#26512;&#39033;&#30446;&#23545;&#39033;&#30446;&#26426;&#21046;&#65292;&#22312;&#36827;&#34892;&#23454;&#39564;&#20998;&#26512;&#21518;&#30830;&#23450;&#20102;&#19977;&#20010;&#22522;&#26412;&#26631;&#20934;&#65292;&#24182;&#24341;&#20837;&#20102;ConvFormer&#26469;&#28385;&#36275;&#36825;&#20123;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#29992;&#25143;&#24314;&#27169;&#26159;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#20854;&#30528;&#37325;&#20110;&#39044;&#27979;&#29992;&#25143;&#26368;&#21916;&#27426;&#30340;&#19979;&#19968;&#20010;&#39033;&#30446;&#65292;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#29992;&#25143;&#30340;&#34892;&#20026;&#24207;&#21015;&#12290;&#23613;&#31649;Transformer&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#21151;&#65292;&#20294;&#22312;&#29702;&#35299;&#29992;&#25143;&#34892;&#20026;&#26041;&#38754;&#23578;&#26410;&#20805;&#20998;&#21457;&#25381;&#20854;&#28508;&#21147;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;Transformer&#31867;&#20284;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#25512;&#36827;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#37325;&#26032;&#23457;&#35270;Transformer&#26041;&#27861;&#30340;&#26680;&#24515;&#26500;&#24314;&#27169;&#22359;&#65292;&#22312;&#39034;&#24207;&#29992;&#25143;&#24314;&#27169;&#30340;&#32972;&#26223;&#19979;&#20998;&#26512;&#39033;&#30446;&#23545;&#39033;&#30446;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36827;&#34892;&#24443;&#24213;&#30340;&#23454;&#39564;&#20998;&#26512;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#20010;&#35774;&#35745;&#39640;&#25928;&#39034;&#24207;&#29992;&#25143;&#27169;&#22411;&#30340;&#22522;&#26412;&#26631;&#20934;&#65292;&#24076;&#26395;&#36825;&#20123;&#26631;&#20934;&#33021;&#20316;&#20026;&#23454;&#29992;&#25351;&#21335;&#65292;&#28608;&#21457;&#21644;&#22609;&#36896;&#26410;&#26469;&#30340;&#35774;&#35745;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ConvFormer&#65292;&#19968;&#31181;&#23545;Transformer&#26550;&#26500;&#36827;&#34892;&#31616;&#21333;&#20294;&#24378;&#22823;&#20462;&#25913;&#30340;&#26041;&#27861;&#65292;&#28385;&#36275;&#20102;&#36825;&#20123;&#26631;&#20934;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential user modeling, a critical task in personalized recommender systems, focuses on predicting the next item a user would prefer, requiring a deep understanding of user behavior sequences. Despite the remarkable success of Transformer-based models across various domains, their full potential in comprehending user behavior remains untapped. In this paper, we re-examine Transformer-like architectures aiming to advance state-of-the-art performance. We start by revisiting the core building blocks of Transformer-based methods, analyzing the effectiveness of the item-to-item mechanism within the context of sequential user modeling. After conducting a thorough experimental analysis, we identify three essential criteria for devising efficient sequential user models, which we hope will serve as practical guidelines to inspire and shape future designs. Following this, we introduce ConvFormer, a simple but powerful modification to the Transformer architecture that meets these criteria, yiel
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#25216;&#26415;&#65288;DuRM&#65289;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#32463;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DuRM&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#26799;&#24230;&#30340;&#26041;&#24046;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DuRM&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02287</link><description>&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#23454;&#29616;&#20196;&#20154;&#27822;&#20007;&#30340;&#27169;&#22411;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Frustratingly Easy Model Generalization by Dummy Risk Minimization. (arXiv:2308.02287v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02287
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#25216;&#26415;&#65288;DuRM&#65289;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#32463;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DuRM&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#26799;&#24230;&#30340;&#26041;&#24046;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DuRM&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;&#23427;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;DuRM&#65289;&#65292;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#21644;&#36890;&#29992;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;ERM&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;DuRM&#38750;&#24120;&#31616;&#21333;&#23454;&#29616;&#65306;&#21482;&#38656;&#25193;&#22823;&#36755;&#20986;logits&#30340;&#32500;&#24230;&#65292;&#28982;&#21518;&#20351;&#29992;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#32463;&#39564;&#39564;&#35777;DuRM&#30340;&#26377;&#25928;&#24615;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DuRM&#23548;&#33268;&#26356;&#22823;&#30340;&#26799;&#24230;&#26041;&#24046;&#65292;&#36890;&#36807;&#35266;&#23519;&#26356;&#22909;&#30340;&#24179;&#22374;&#23616;&#37096;&#26368;&#23567;&#20540;&#20419;&#36827;&#27169;&#22411;&#27867;&#21270;&#12290;&#20174;&#32463;&#39564;&#19978;&#35762;&#65292;&#25105;&#20204;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#27169;&#24577;&#21644;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;DuRM&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#20256;&#32479;&#20998;&#31867;&#65292;&#35821;&#20041;&#20998;&#21106;&#65292;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#65292;&#23545;&#25239;&#35757;&#32451;&#21644;&#38271;&#23614;&#35782;&#21035;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;DuRM&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical risk minimization (ERM) is a fundamental machine learning paradigm. However, its generalization ability is limited in various tasks. In this paper, we devise Dummy Risk Minimization (DuRM), a frustratingly easy and general technique to improve the generalization of ERM. DuRM is extremely simple to implement: just enlarging the dimension of the output logits and then optimizing using standard gradient descent. Moreover, we validate the efficacy of DuRM on both theoretical and empirical analysis. Theoretically, we show that DuRM derives greater variance of the gradient, which facilitates model generalization by observing better flat local minima. Empirically, we conduct evaluations of DuRM across different datasets, modalities, and network architectures on diverse tasks, including conventional classification, semantic segmentation, out-of-distribution generalization, adverserial training, and long-tailed recognition. Results demonstrate that DuRM could consistently improve the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01313</link><description>&lt;p&gt;
&#26356;&#22810;&#19978;&#19979;&#25991;&#65292;&#26356;&#23569;&#24178;&#25200;&#65306;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#36827;&#34892;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#20316;&#20026;&#19968;&#31181;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#29702;&#35299;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#33021;&#21147;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;CLIP&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#20154;&#31867;&#33324;&#29702;&#35299;&#33021;&#21147;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#20154;&#31867;&#30340;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#20013;&#24471;&#21040;&#21551;&#21457;&#65306;&#29616;&#20195;&#31070;&#32463;&#31185;&#23398;&#35266;&#28857;&#35748;&#20026;&#65292;&#22312;&#23545;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;&#20154;&#31867;&#39318;&#20808;&#25512;&#26029;&#20854;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#23646;&#24615;&#65288;&#22914;&#32972;&#26223;&#21644;&#26041;&#21521;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#23558;&#21069;&#26223;&#23545;&#35937;&#19982;&#32972;&#26223;&#21306;&#20998;&#24320;&#26469;&#65292;&#28982;&#21518;&#20197;&#27492;&#20449;&#24687;&#20026;&#22522;&#30784;&#36827;&#34892;&#20915;&#31574;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20026;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#21487;&#20197;&#25913;&#21892;&#38646;&#26679;&#26412;&#20998;&#31867;&#24182;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;CLIP&#26412;&#36523;&#21487;&#20197;&#21512;&#29702;&#22320;&#20174;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#36825;&#20123;&#23646;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#35757;&#32451;&#12289;&#20004;&#27493;&#39588;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot cl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.15337</link><description>&lt;p&gt;
&#24605;&#32500;&#30340;&#39592;&#26550;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24182;&#34892;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding. (arXiv:2307.15337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#24310;&#36831;&#12290;&#39640;&#29983;&#25104;&#24310;&#36831;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#20960;&#20046;&#25152;&#26377;&#26368;&#20808;&#36827;&#30340;LLMs&#37117;&#37319;&#29992;&#20102;&#39034;&#24207;&#35299;&#30721;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#21463;&#21040;&#20154;&#31867;&#30340;&#24605;&#32771;&#21644;&#20889;&#20316;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#65288;SoT&#65289;&#65292;&#23427;&#25351;&#23548;LLMs&#39318;&#20808;&#29983;&#25104;&#31572;&#26696;&#30340;&#39592;&#26550;&#65292;&#28982;&#21518;&#36890;&#36807;&#24182;&#34892;API&#35843;&#29992;&#25110;&#25209;&#37327;&#35299;&#30721;&#26469;&#24182;&#34892;&#23436;&#25104;&#27599;&#20010;&#39592;&#26550;&#28857;&#30340;&#20869;&#23481;&#12290;SoT&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65288;&#22312;11&#20010;&#19981;&#21516;&#30340;LLMs&#19978;&#25552;&#39640;&#20102;&#26368;&#22810;2.39&#20493;&#65289;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#22312;&#22810;&#20010;&#38382;&#39064;&#31867;&#21035;&#19978;&#30340;&#31572;&#26696;&#36136;&#37327;&#65292;&#21253;&#25324;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;SoT&#26159;&#19968;&#31181;&#38024;&#23545;&#25928;&#29575;&#30340;&#25968;&#25454;&#23548;&#21521;&#20248;&#21270;&#30340;&#21021;&#27493;&#23581;&#35797;&#65292;&#24182;&#25581;&#31034;&#20102;&#23558;LLMs&#25512;&#21160;&#26356;&#20687;&#20154;&#31867;&#24605;&#32771;&#20197;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose "Skeleton-of-Thought" (SoT), which guides LLMs to first generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-up (up to 2.39x across 11 different LLMs), but it can also potentially improve the answer quality on several question categories in terms of diversity and relevance. SoT is an initial attempt at data-centric optimization for efficiency, and reveal the potential of pushing LLMs to think more like a human for answer quality.
&lt;/p&gt;</description></item><item><title>TF-ICON&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22270;&#20687;&#21512;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#23383;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#36328;&#39046;&#22495;&#22270;&#20687;&#23548;&#21521;&#21512;&#25104;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;TF-ICON&#21487;&#20197;&#22312;&#19981;&#38656;&#39069;&#22806;&#35757;&#32451;&#12289;&#24494;&#35843;&#25110;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#26080;&#32541;&#21512;&#25104;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20363;&#22806;&#25552;&#31034;&#26469;&#20934;&#30830;&#22320;&#21453;&#36716;&#30495;&#23454;&#22270;&#20687;&#20026;&#28508;&#22312;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.12493</link><description>&lt;p&gt;
TF-ICON: &#22522;&#20110;&#25193;&#25955;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36328;&#39046;&#22495;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition. (arXiv:2307.12493v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12493
&lt;/p&gt;
&lt;p&gt;
TF-ICON&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22270;&#20687;&#21512;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#23383;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#36328;&#39046;&#22495;&#22270;&#20687;&#23548;&#21521;&#21512;&#25104;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;TF-ICON&#21487;&#20197;&#22312;&#19981;&#38656;&#39069;&#22806;&#35757;&#32451;&#12289;&#24494;&#35843;&#25110;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#26080;&#32541;&#21512;&#25104;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20363;&#22806;&#25552;&#31034;&#26469;&#20934;&#30830;&#22320;&#21453;&#36716;&#30495;&#23454;&#22270;&#20687;&#20026;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#21487;&#20197;&#23454;&#29616;&#21508;&#31181;&#22270;&#20687;&#32534;&#36753;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TF-ICON&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#22270;&#20687;&#21512;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#23383;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#34892;&#36328;&#39046;&#22495;&#22270;&#20687;&#23548;&#21521;&#21512;&#25104;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23558;&#29992;&#25143;&#25552;&#20379;&#30340;&#23545;&#35937;&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#29305;&#23450;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#12290;&#30446;&#21069;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#26114;&#36149;&#30340;&#22522;&#20110;&#23454;&#20363;&#30340;&#20248;&#21270;&#25110;&#22312;&#23450;&#21046;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#25439;&#23475;&#20854;&#20016;&#23500;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#30456;&#21453;&#65292;TF-ICON&#21487;&#20197;&#21033;&#29992;&#29616;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#22270;&#20687;&#23548;&#21521;&#21512;&#25104;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12289;&#24494;&#35843;&#25110;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20363;&#22806;&#25552;&#31034;(&#21547;&#26080;&#20449;&#24687;)&#26469;&#24110;&#21161;&#25991;&#23383;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#20934;&#30830;&#22320;&#23558;&#30495;&#23454;&#22270;&#20687;&#21453;&#36716;&#20026;&#28508;&#22312;&#34920;&#31034;&#65292;&#20026;&#21512;&#25104;&#25552;&#20379;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TF-ICON&#22312;&#19981;&#21516;&#30340;&#21512;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#20687;&#20043;&#38388;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#26080;&#32541;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-driven diffusion models have exhibited impressive generative capabilities, enabling various image editing tasks. In this paper, we propose TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the power of text-driven diffusion models for cross-domain image-guided composition. This task aims to seamlessly integrate user-provided objects into a specific visual context. Current diffusion-based methods often involve costly instance-based optimization or finetuning of pretrained models on customized datasets, which can potentially undermine their rich prior. In contrast, TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain image-guided composition without requiring additional training, finetuning, or optimization. Moreover, we introduce the exceptional prompt, which contains no information, to facilitate text-driven diffusion models in accurately inverting real images into latent representations, forming the basis for compositing. Our experim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10864</link><description>&lt;p&gt;
&#23558;&#27880;&#24847;&#21147;&#20998;&#21106;&#19982;&#32465;&#23450;&#29992;&#20110;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#65292;&#23637;&#31034;&#20102;&#39640;&#24230;&#36924;&#30495;&#30340;&#21387;&#20498;&#24615;&#32467;&#26524;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#23436;&#20840;&#20381;&#29031;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#8212;&#8212;&#20851;&#27880;&#19982;&#28608;&#21457;&#65292;&#24341;&#20837;&#20102;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#65288;GSN&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#25512;&#26029;&#26102;&#20248;&#21270;&#36328;&#27880;&#24847;&#21147;&#20197;&#26356;&#22909;&#22320;&#34701;&#20837;&#35821;&#20041;&#12290;&#23427;&#22312;&#29983;&#25104;&#31616;&#21333;&#25552;&#31034;&#65292;&#22914;&#8220;&#19968;&#21482;&#29483;&#21644;&#19968;&#21482;&#29399;&#8221;&#65292;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25552;&#31034;&#20197;&#21450;&#35299;&#20915;&#19981;&#36866;&#24403;&#30340;&#23646;&#24615;&#32465;&#23450;&#38382;&#39064;&#26041;&#38754;&#30340;&#21151;&#25928;&#26377;&#25152;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#22797;&#26434;&#25552;&#31034;&#25110;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#22330;&#26223;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#25913;&#36827;&#30340;&#23646;&#24615;&#32465;&#23450;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#21106;&#19982;&#32465;&#23450;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;GSN&#25439;&#22833;&#30446;&#26631;&#65306;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#20002;&#22833;&#21644;&#19968;&#31181;&#32465;&#23450;&#20002;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#35821;&#20041;&#32435;&#20837;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#29305;&#28857;&#19978;&#33073;&#39062;&#32780;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend &amp; Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide &amp; Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to fa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#21517;&#20154;&#34892;&#31243;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#25991;&#31456;&#38388;&#30340;&#24322;&#36136;&#24615;&#21644;&#22122;&#22768;&#24178;&#25200;&#65292;&#20026;&#36827;&#34892;&#22823;&#35268;&#27169;&#21644;&#32593;&#32476;&#20998;&#26512;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;</title><link>http://arxiv.org/abs/2307.08721</link><description>&lt;p&gt;
&#19978;&#21608;&#24635;&#32479;&#21435;&#20102;&#21738;&#37324;&#65311;&#20174;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#21517;&#20154;&#34892;&#31243;
&lt;/p&gt;
&lt;p&gt;
Where Did the President Visit Last Week? Detecting Celebrity Trips from News Articles. (arXiv:2307.08721v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08721
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#21517;&#20154;&#34892;&#31243;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#25991;&#31456;&#38388;&#30340;&#24322;&#36136;&#24615;&#21644;&#22122;&#22768;&#24178;&#25200;&#65292;&#20026;&#36827;&#34892;&#22823;&#35268;&#27169;&#21644;&#32593;&#32476;&#20998;&#26512;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21517;&#20154;&#30340;&#34892;&#36394;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#20363;&#22914;&#65292;&#25919;&#27835;&#23478;&#21435;&#21738;&#37324;&#65292;&#20182;&#20204;&#22810;&#20037;&#35775;&#38382;&#19968;&#27425;&#65292;&#20197;&#21450;&#20182;&#20204;&#20250;&#35265;&#35841;&#65292;&#37117;&#24102;&#26377;&#28145;&#36828;&#30340;&#22320;&#32536;&#25919;&#27835;&#21644;&#32463;&#27982;&#24433;&#21709;&#12290;&#34429;&#28982;&#26032;&#38395;&#25991;&#31456;&#21253;&#21547;&#20102;&#21517;&#20154;&#30340;&#26053;&#34892;&#20449;&#24687;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#33258;&#21160;&#34892;&#31243;&#26816;&#27979;&#24037;&#20855;&#65292;&#26080;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#21644;&#32593;&#32476;&#20998;&#26512;&#12290;&#20026;&#20102;&#35774;&#35745;&#36825;&#26679;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#24517;&#39035;&#20811;&#26381;&#26032;&#38395;&#25991;&#31456;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#24102;&#26469;&#30340;&#22256;&#38590;&#65306;1)&#19968;&#20010;&#21333;&#29420;&#30340;&#25991;&#31456;&#21487;&#33021;&#22122;&#38899;&#24456;&#22823;&#65292;&#28041;&#21450;&#26080;&#20851;&#30340;&#20154;&#29289;&#21644;&#22320;&#28857;&#65292;&#29305;&#21035;&#26159;&#24403;&#25991;&#31456;&#24456;&#38271;&#26102;&#12290;2)&#34429;&#28982;&#32771;&#34385;&#22810;&#31687;&#25991;&#31456;&#19968;&#36215;&#26469;&#30830;&#23450;&#19968;&#20010;&#29305;&#23450;&#30340;&#34892;&#31243;&#21487;&#33021;&#20250;&#26377;&#24110;&#21161;&#65292;&#20294;&#20851;&#38190;&#30340;&#35821;&#20041;&#20173;&#28982;&#20998;&#25955;&#22312;&#19981;&#21516;&#30340;&#25991;&#31456;&#20013;&#65292;&#19982;&#21508;&#31181;&#22122;&#22768;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#20351;&#20854;&#38590;&#20197;&#26377;&#25928;&#22320;&#27719;&#24635;&#12290;3)&#36229;&#36807;20%&#30340;&#25991;&#31456;&#38388;&#25509;&#25552;&#21450;&#20102;&#21517;&#20154;&#30340;&#34892;&#31243;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#20351;&#29992;&#20934;&#30830;&#30340;&#21517;&#20154;&#22995;&#21517;&#25110;&#22320;&#28857;&#21517;&#31216;&#65292;&#23548;&#33268;&#20102;&#22823;&#37096;&#20998;&#34892;&#31243;&#20449;&#24687;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Celebrities' whereabouts are of pervasive importance. For instance, where politicians go, how often they visit, and who they meet, come with profound geopolitical and economic implications. Although news articles contain travel information of celebrities, it is not possible to perform large-scale and network-wise analysis due to the lack of automatic itinerary detection tools. To design such tools, we have to overcome difficulties from the heterogeneity among news articles: 1)One single article can be noisy, with irrelevant people and locations, especially when the articles are long. 2)Though it may be helpful if we consider multiple articles together to determine a particular trip, the key semantics are still scattered across different articles intertwined with various noises, making it hard to aggregate them effectively. 3)Over 20% of the articles refer to the celebrities' trips indirectly, instead of using the exact celebrity names or location names, leading to large portions of tri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#28145;&#24230;&#36328;&#27169;&#24577;&#38544;&#20889;&#26415;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#38544;&#34255;&#21508;&#31181;&#26684;&#24335;&#30340;&#31192;&#23494;&#25968;&#25454;&#65292;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08671</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#34920;&#31034;&#30340;&#28145;&#24230;&#36328;&#27169;&#24577;&#38544;&#20889;&#26415;
&lt;/p&gt;
&lt;p&gt;
Deep Cross-Modal Steganography Using Neural Representations. (arXiv:2307.08671v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#28145;&#24230;&#36328;&#27169;&#24577;&#38544;&#20889;&#26415;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#38544;&#34255;&#21508;&#31181;&#26684;&#24335;&#30340;&#31192;&#23494;&#25968;&#25454;&#65292;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#20889;&#26415;&#26159;&#23558;&#31192;&#23494;&#25968;&#25454;&#23884;&#20837;&#21040;&#21478;&#19968;&#26465;&#28040;&#24687;&#25110;&#25968;&#25454;&#20013;&#65292;&#20197;&#19981;&#23481;&#26131;&#34987;&#23519;&#35273;&#30340;&#26041;&#24335;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#26368;&#36817;&#22312;&#38544;&#20889;&#26415;&#20013;&#24320;&#22987;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#38544;&#20889;&#26415;&#25216;&#26415;&#22312;&#33539;&#22260;&#19978;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#38024;&#23545;&#29305;&#23450;&#30340;&#25968;&#25454;&#31867;&#22411;&#65292;&#24182;&#19988;&#23545;&#20110;&#36328;&#27169;&#24577;&#38544;&#20889;&#26415;&#19981;&#22815;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#36827;&#34892;&#28145;&#24230;&#36328;&#27169;&#24577;&#38544;&#20889;&#26415;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35206;&#30422;&#22270;&#20687;&#20013;&#38544;&#34255;&#21508;&#31181;&#26684;&#24335;&#30340;&#31192;&#23494;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21033;&#29992;INRs&#26469;&#34920;&#31034;&#31192;&#23494;&#25968;&#25454;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#24418;&#24335;&#21644;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#12290;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#31192;&#23494;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#21487;&#25193;&#23637;&#30340;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steganography is the process of embedding secret data into another message or data, in such a way that it is not easily noticeable. With the advancement of deep learning, Deep Neural Networks (DNNs) have recently been utilized in steganography. However, existing deep steganography techniques are limited in scope, as they focus on specific data types and are not effective for cross-modal steganography. Therefore, We propose a deep cross-modal steganography framework using Implicit Neural Representations (INRs) to hide secret data of various formats in cover images. The proposed framework employs INRs to represent the secret data, which can handle data of various modalities and resolutions. Experiments on various secret datasets of diverse types demonstrate that the proposed approach is expandable and capable of accommodating different modalities.
&lt;/p&gt;</description></item><item><title>DualMind&#20351;&#29992;&#21452;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#25511;&#21046;&#20219;&#21153;&#20013;&#23398;&#20064;&#20849;&#21516;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#27169;&#20223;&#34892;&#20026;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;DualMind&#22312;MetaWorld&#21644;Habitat&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#24615;&#20195;&#29702;&#65292;&#20855;&#26377;&#36229;&#36807;50%&#21644;70%&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.07909</link><description>&lt;p&gt;
&#20320;&#38656;&#35201;&#30340;&#21482;&#26159;&#27169;&#20223;&#21527;&#65311;&#20855;&#26377;&#21452;&#38454;&#27573;&#35757;&#32451;&#30340;&#27867;&#21270;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training. (arXiv:2307.07909v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07909
&lt;/p&gt;
&lt;p&gt;
DualMind&#20351;&#29992;&#21452;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#25511;&#21046;&#20219;&#21153;&#20013;&#23398;&#20064;&#20849;&#21516;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#27169;&#20223;&#34892;&#20026;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;DualMind&#22312;MetaWorld&#21644;Habitat&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#24615;&#20195;&#29702;&#65292;&#20855;&#26377;&#36229;&#36807;50%&#21644;70%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;DualMind&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#24615;&#20195;&#29702;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#36807;&#24230;&#25311;&#21512;&#34892;&#20026;&#21644;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#31934;&#32454;&#35843;&#25972;&#12290;DualMind&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#21452;&#38454;&#27573;&#8221;&#35757;&#32451;&#31574;&#30053;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#23398;&#20064;&#22312;&#19990;&#30028;&#20013;&#34892;&#21160;&#30340;&#26041;&#24335;&#12290;&#27169;&#22411;&#39318;&#20808;&#36890;&#36807;&#38024;&#23545;&#25511;&#21046;&#20219;&#21153;&#23450;&#21046;&#30340;&#33258;&#30417;&#30563;&#30446;&#26631;&#26469;&#23398;&#20064;&#22522;&#26412;&#30340;&#20849;&#21516;&#30693;&#35782;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#20223;&#22522;&#20110;&#32473;&#23450;&#25552;&#31034;&#30340;&#34892;&#20026;&#26469;&#23398;&#20064;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#20570;&#20986;&#20915;&#31574;&#12290; DualMind&#21487;&#20197;&#22788;&#29702;&#36328;&#22495;&#12289;&#22330;&#26223;&#21644;&#20855;&#20307;&#38382;&#39064;&#65292;&#24182;&#20165;&#20351;&#29992;&#21333;&#32452;&#27169;&#22411;&#26435;&#37325;&#26469;&#25191;&#34892;&#38646;&#26679;&#26412;&#25552;&#31034;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#21153;&#29305;&#23450;&#30340;&#31934;&#32454;&#35843;&#25972;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;MetaWorld&#21644;Habitat&#19978;&#35780;&#20272;&#20102;DualMind&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;Habitat&#21644;MetaWorld&#19978;&#30340;&#34920;&#29616;&#20998;&#21035;&#36229;&#36807;&#20102;&#20854;&#20182;&#36890;&#29992;&#24615;&#20195;&#29702;&#30340;50%&#21644;70%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DualMind, a generalist agent designed to tackle various decision-making tasks that addresses challenges posed by current methods, such as overfitting behaviors and dependence on task-specific fine-tuning. DualMind uses a novel "Dual-phase" training strategy that emulates how humans learn to act in the world. The model first learns fundamental common knowledge through a self-supervised objective tailored for control tasks and then learns how to make decisions based on different contexts through imitating behaviors conditioned on given prompts. DualMind can handle tasks across domains, scenes, and embodiments using just a single set of model weights and can execute zero-shot prompting without requiring task-specific fine-tuning. We evaluate DualMind on MetaWorld and Habitat through extensive experiments and demonstrate its superior generalizability compared to previous techniques, outperforming other generalist agents by over 50$\%$ and 70$\%$ on Habitat and MetaWorld, respe
&lt;/p&gt;</description></item><item><title>Safe DreamerV3&#26159;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#32500;&#24230;&#21644;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#20960;&#20046;&#38646;&#25104;&#26412;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07176</link><description>&lt;p&gt;
Safe DreamerV3&#65306;&#24102;&#26377;&#19990;&#30028;&#27169;&#22411;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe DreamerV3: Safe Reinforcement Learning with World Models. (arXiv:2307.07176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07176
&lt;/p&gt;
&lt;p&gt;
Safe DreamerV3&#26159;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#32500;&#24230;&#21644;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#20960;&#20046;&#38646;&#25104;&#26412;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#36824;&#27809;&#26377;&#23454;&#29616;, &#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#26410;&#33021;&#28385;&#36275;&#36825;&#20123;&#31995;&#32479;&#30340;&#22522;&#26412;&#23433;&#20840;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#25104;&#26412;&#20989;&#25968;&#26469;&#22686;&#24378;&#23433;&#20840;&#24615;&#65292;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#65292;&#21253;&#25324;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#65292;&#21363;&#20351;&#36827;&#34892;&#20840;&#38754;&#30340;&#25968;&#25454;&#37319;&#26679;&#21644;&#35757;&#32451;&#65292;&#20063;&#26080;&#27861;&#23454;&#29616;&#38646;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Safe DreamerV3&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#22312;SafeRL&#20013;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#36827;&#27493;&#65292;&#26159;&#31532;&#19968;&#20010;&#22312;Safety-Gymnasium&#22522;&#20934;&#20013;&#23454;&#29616;&#36817;&#20046;&#38646;&#25104;&#26412;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#32593;&#31449;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://sites.google.com/view/safedreamerv3&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread application of Reinforcement Learning (RL) in real-world situations is yet to come to fruition, largely as a result of its failure to satisfy the essential safety demands of such systems. Existing safe reinforcement learning (SafeRL) methods, employing cost functions to enhance safety, fail to achieve zero-cost in complex scenarios, including vision-only tasks, even with comprehensive data sampling and training. To address this, we introduce Safe DreamerV3, a novel algorithm that integrates both Lagrangian-based and planning-based methods within a world model. Our methodology represents a significant advancement in SafeRL as the first algorithm to achieve nearly zero-cost in both low-dimensional and vision-only tasks within the Safety-Gymnasium benchmark. Our project website can be found in: https://sites.google.com/view/safedreamerv3.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#37325;&#35201;&#30340;&#20196;&#29260;&#21644;&#21547;&#26377;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#34987;&#21516;&#31561;&#25110;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01379</link><description>&lt;p&gt;
&#23558;&#20851;&#27880;&#28857;&#36716;&#31227;&#21040;&#30456;&#20851;&#24615;&#19978;: &#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#37325;&#35201;&#30340;&#20196;&#29260;&#21644;&#21547;&#26377;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#34987;&#21516;&#31561;&#25110;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#23545;&#20110;&#27169;&#22411;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#29305;&#24449;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#29992;&#25143;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#19968;&#20123;&#21551;&#21457;&#24615;&#30340;&#20107;&#23454;&#65292;&#21363;&#22312;&#33258;&#22238;&#24402;&#30340;LLMs&#20013;&#65292;&#20196;&#29260;&#22312;&#21453;&#26144;&#29983;&#25104;&#30340;&#21547;&#20041;&#26041;&#38754;&#26159;&#19981;&#24179;&#31561;&#30340;&#65292;&#21363;&#19968;&#20123;&#20196;&#29260;&#27604;&#20854;&#20182;&#20196;&#29260;&#26356;&#30456;&#20851;&#65288;&#25110;&#26356;&#20855;&#20195;&#34920;&#24615;&#65289;&#65292;&#28982;&#32780;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#25152;&#26377;&#30340;&#20196;&#29260;&#34987;&#31561;&#20540;&#23545;&#24453;&#12290;&#36825;&#26159;&#30001;&#20110;&#35821;&#35328;&#20887;&#20313;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;&#20960;&#20010;&#20851;&#38190;&#35789;&#23601;&#36275;&#20197;&#20256;&#36798;&#19968;&#20010;&#38271;&#21477;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#19981;&#24179;&#31561;&#31216;&#20026;&#29983;&#25104;&#30340;&#19981;&#24179;&#31561;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#19981;&#30830;&#23450;&#24615;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#65292;&#30456;&#24403;&#25968;&#37327;&#30340;&#20196;&#29260;&#21644;&#21253;&#21547;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#65292;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#34987;&#21516;&#31561;&#25110;&#29978;&#33267;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#29983;&#25104;&#30340;&#19981;&#24179;&#31561;&#24341;&#36215;&#30340;&#36825;&#20123;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;ContextSpeech&#65292;&#36890;&#36807;&#35774;&#35745;&#20869;&#23384;&#32531;&#23384;&#30340;&#24490;&#29615;&#26426;&#21046;&#21644;&#26500;&#24314;&#23618;&#27425;&#21270;&#30340;&#25991;&#26412;&#35821;&#20041;&#32467;&#26500;&#65292;&#23558;&#20840;&#23616;&#25991;&#26412;&#21644;&#35821;&#38899;&#19978;&#19979;&#25991;&#34701;&#20837;&#21040;&#21477;&#23376;&#32534;&#30721;&#20013;&#65292;&#20197;&#35299;&#20915;&#27573;&#33853;&#38405;&#35835;&#20013;&#30340;&#35821;&#38899;&#29983;&#25104;&#25361;&#25112;&#65292;&#24182;&#19988;&#20351;&#29992;&#32447;&#24615;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.00782</link><description>&lt;p&gt;
ContextSpeech&#65306;&#29992;&#20110;&#27573;&#33853;&#38405;&#35835;&#30340;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading. (arXiv:2307.00782v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;ContextSpeech&#65292;&#36890;&#36807;&#35774;&#35745;&#20869;&#23384;&#32531;&#23384;&#30340;&#24490;&#29615;&#26426;&#21046;&#21644;&#26500;&#24314;&#23618;&#27425;&#21270;&#30340;&#25991;&#26412;&#35821;&#20041;&#32467;&#26500;&#65292;&#23558;&#20840;&#23616;&#25991;&#26412;&#21644;&#35821;&#38899;&#19978;&#19979;&#25991;&#34701;&#20837;&#21040;&#21477;&#23376;&#32534;&#30721;&#20013;&#65292;&#20197;&#35299;&#20915;&#27573;&#33853;&#38405;&#35835;&#20013;&#30340;&#35821;&#38899;&#29983;&#25104;&#25361;&#25112;&#65292;&#24182;&#19988;&#20351;&#29992;&#32447;&#24615;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#21487;&#20197;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#29983;&#25104;&#38750;&#24120;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#38899;&#65292;&#20294;&#23427;&#20204;&#22312;&#27573;&#33853;/&#38271;&#31687;&#38405;&#35835;&#30340;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#20173;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#19981;&#36275;&#20043;&#22788;&#20027;&#35201;&#26159;&#22240;&#20026;&#65306;&#19968;&#26159;&#24573;&#35270;&#20102;&#36328;&#21477;&#23376;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20108;&#26159;&#38271;&#31687;&#21512;&#25104;&#36807;&#31243;&#20013;&#30340;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;ContextSpeech&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#20869;&#23384;&#32531;&#23384;&#30340;&#24490;&#29615;&#26426;&#21046;&#65292;&#23558;&#20840;&#23616;&#25991;&#26412;&#21644;&#35821;&#38899;&#19978;&#19979;&#25991;&#34701;&#20837;&#21040;&#21477;&#23376;&#32534;&#30721;&#20013;&#65307;&#28982;&#21518;&#26500;&#24314;&#20102;&#23618;&#27425;&#21270;&#30340;&#25991;&#26412;&#35821;&#20041;&#32467;&#26500;&#65292;&#20197;&#25193;&#22823;&#20840;&#23616;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#33539;&#22260;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25972;&#21512;&#20102;&#32447;&#24615;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ContextSpeech&#22312;&#27573;&#33853;&#38405;&#35835;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#35821;&#38899;&#36136;&#37327;&#21644;&#38901;&#24459;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#22791;&#31454;&#20105;&#21147;&#30340;&#27169;&#22411;&#25928;&#29575;&#12290;&#38899;&#39057;&#26679;&#26412;&#21487;&#35775;&#38382;&#38142;&#25509;&#65306;https://contextspeech.
&lt;/p&gt;
&lt;p&gt;
While state-of-the-art Text-to-Speech systems can generate natural speech of very high quality at sentence level, they still meet great challenges in speech generation for paragraph / long-form reading. Such deficiencies are due to i) ignorance of cross-sentence contextual information, and ii) high computation and memory cost for long-form synthesis. To address these issues, this work develops a lightweight yet effective TTS system, ContextSpeech. Specifically, we first design a memory-cached recurrence mechanism to incorporate global text and speech context into sentence encoding. Then we construct hierarchically-structured textual semantics to broaden the scope for global context enhancement. Additionally, we integrate linearized self-attention to improve model efficiency. Experiments show that ContextSpeech significantly improves the voice quality and prosody expressiveness in paragraph reading with competitive model efficiency. Audio samples are available at: https://contextspeech.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#21407;&#22987;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#20013;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17175</link><description>&lt;p&gt;
&#20174;&#21407;&#22987;&#30340;GP&#31508;&#35760;&#20013;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#65292;&#29992;&#20110;&#36828;&#31243;COVID-19&#21021;&#32423;&#20445;&#20581;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19 Assessment in Primary Care. (arXiv:2306.17175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#21407;&#22987;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#20013;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20915;&#31574;&#26159;&#21521;&#24739;&#32773;&#25552;&#20379;&#36866;&#24403;&#25252;&#29702;&#30340;&#22522;&#26412;&#38454;&#27573;&#12290;&#36817;&#24180;&#26469;&#65292;&#20026;&#20102;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#20570;&#20986;&#20915;&#31574;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#20010;&#20915;&#31574;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20351;&#29992;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#31616;&#21333;&#30340;&#22238;&#24402;&#27169;&#22411;&#65292;&#21482;&#33021;&#32771;&#34385;&#31616;&#21333;&#30340;&#39044;&#23450;&#20041;&#22810;&#36873;&#29305;&#24449;&#65292;&#22914;&#24739;&#32773;&#24180;&#40836;&#12289;&#26082;&#24448;&#30149;&#21490;&#12289;&#21560;&#28895;&#32773;&#29366;&#20917;&#31561;&#12290;&#20915;&#31574;&#31995;&#32479;&#24403;&#21069;&#26080;&#27861;&#22788;&#29702;&#30340;&#19968;&#20010;&#29305;&#23450;&#24739;&#32773;&#25968;&#25454;&#26469;&#28304;&#26159;&#24739;&#32773;&#20250;&#35786;&#30340;GP&#31508;&#35760;&#30340;&#25910;&#38598;&#12290;&#36825;&#20123;&#31508;&#35760;&#21253;&#21547;&#20102;&#20020;&#24202;&#21307;&#29983;&#29992;&#26469;&#20570;&#20986;&#26368;&#32456;&#20915;&#31574;&#24182;&#23558;&#24739;&#32773;&#24341;&#23548;&#21040;&#36866;&#24403;&#25252;&#29702;&#30340;&#20851;&#38190;&#20307;&#24449;&#21644;&#30151;&#29366;&#12290;&#20174;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20449;&#24687;&#26159;&#19968;&#20010;&#25216;&#26415;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#21253;&#21547;&#32553;&#20889;&#12289;&#25171;&#23383;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20010;&#20844;&#24320;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25191;&#34892;&#20174;&#21407;&#22987;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20986;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical decision-making is a fundamental stage in delivering appropriate care to patients. In recent years several decision-making systems designed to aid the clinician in this process have been developed. However, technical solutions currently in use are based on simple regression models and are only able to take into account simple pre-defined multiple-choice features, such as patient age, pre-existing conditions, smoker status, etc. One particular source of patient data, that available decision-making systems are incapable of processing is the collection of patient consultation GP notes. These contain crucial signs and symptoms - the information used by clinicians in order to make a final decision and direct the patient to the appropriate care. Extracting information from GP notes is a technically challenging problem, as they tend to include abbreviations, typos, and incomplete sentences.  This paper addresses this open challenge. We present a framework that performs knowledge grap
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.15749</link><description>&lt;p&gt;
&#20309;&#21435;&#20309;&#20174;&#65306;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#30340;&#25968;&#23383;&#30828;&#20214;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15749
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#22312;&#28085;&#30422;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#31454;&#20105;&#21147;&#65307;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#25928;&#29575;&#20026;&#20195;&#20215;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#33021;&#21147;&#12290;&#29983;&#29289;&#33041;&#30340;&#21151;&#32791;&#25928;&#29575;&#36229;&#36807;&#20219;&#20309;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65307;&#22240;&#27492;&#65292;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#35797;&#22270;&#27169;&#20223;&#33041;&#37096;&#25805;&#20316;&#65292;&#20363;&#22914;&#22522;&#20110;&#33033;&#20914;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;DL&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#33041;&#37096;&#26377;&#35832;&#22914;&#39640;&#25928;&#30340;&#20449;&#24687;&#20256;&#36755;&#12289;&#23494;&#38598;&#30340;&#31070;&#32463;&#20803;&#36830;&#25509;&#21644;&#35745;&#31639;&#19982;&#23384;&#20648;&#30340;&#20849;&#21516;&#20301;&#32622;&#31561;&#20248;&#21183;&#65292;&#20294;&#21487;&#29992;&#30340;&#29983;&#29289;&#22522;&#24213;&#20005;&#37325;&#38480;&#21046;&#20102;&#29983;&#29289;&#22823;&#33041;&#30340;&#36827;&#21270;&#12290;&#30005;&#23376;&#30828;&#20214;&#27809;&#26377;&#30456;&#21516;&#30340;&#32422;&#26463;&#65307;&#22240;&#27492;&#65292;&#34429;&#28982;&#24314;&#27169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21487;&#33021;&#25581;&#31034;&#20102;&#19968;&#20010;&#35868;&#39064;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
&lt;/p&gt;</description></item><item><title>MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15128</link><description>&lt;p&gt;
MIMIC: &#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MIMIC: Masked Image Modeling with Image Correspondences. (arXiv:2306.15128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15128
&lt;/p&gt;
&lt;p&gt;
MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20687;&#32032;&#32423;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#8212;&#8212;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#8212;&#8212;&#22914;&#20170;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#31579;&#36873;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20165;&#36890;&#36807;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#24102;&#26377;&#27880;&#37322;&#30340;3D&#32593;&#26684;&#12289;&#28857;&#20113;&#21644;&#30456;&#26426;&#21442;&#25968;&#31579;&#36873;&#32780;&#26469;&#65292;&#24182;&#19981;&#20855;&#22791;&#22810;&#35270;&#35282;&#22330;&#26223;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#31579;&#36873;&#26426;&#21046;&#12290;&#25105;&#20204;&#20174;&#24320;&#28304;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#30340;3D&#29615;&#22659;&#20013;&#25366;&#25496;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;MIMIC-1M(&#21253;&#21547;1.3M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#21644;MIMIC-3M(&#21253;&#21547;3.1M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#30446;&#26631;&#65292;&#23637;&#31034;&#20102;&#20197;&#19979;&#21457;&#29616;&#65306;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;MIMIC-3M&#35757;&#32451;&#30340;&#34920;&#31034;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#28145;&#24230;&#20272;&#35745;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#34920;&#38754;&#27861;&#32447;&#21644;&#23039;&#24577;&#20272;&#35745;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many pixelwise dense prediction tasks-depth estimation and semantic segmentation in computer vision today rely on pretrained image representations. Therefore, curating effective pretraining datasets is vital. Unfortunately, the effective pretraining datasets are those with multi-view scenes and have only been curated using annotated 3D meshes, point clouds, and camera parameters from simulated environments. We propose a dataset-curation mechanism that does not require any annotations. We mine two datasets: MIMIC-1M with 1.3M and MIMIC-3M with 3.1M multi-view image pairs from open-sourced video datasets and from synthetic 3D environments. We train multiple self-supervised models with different masked image modeling objectives to showcase the following findings: Representations trained on MIMIC-3M outperform those mined using annotations on multiple downstream tasks, including depth estimation, semantic segmentation, surface normals, and pose estimation. They also outperform representati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#27169;&#22359;&#20197;&#35299;&#20915; ChatGPT &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24369;&#28857;&#65292;&#21253;&#25324;&#21033;&#29992;&#22810;&#20010;&#25552;&#31034;&#31526;&#26469;&#36866;&#24212;&#26356;&#22810;&#28436;&#31034;&#12289;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#26816;&#32034;&#12289;&#36716;&#25442;&#20219;&#21153;&#20026;&#26356;&#36866;&#21512;&#29983;&#25104;&#24615;&#36136;&#30340;&#26684;&#24335;&#20197;&#21450;&#37319;&#29992;&#38024;&#23545; NLP &#20219;&#21153;&#35774;&#35745;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.09719</link><description>&lt;p&gt;
&#25512;&#21160; ChatGPT &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Pushing the Limits of ChatGPT on NLP Tasks. (arXiv:2306.09719v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#27169;&#22359;&#20197;&#35299;&#20915; ChatGPT &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24369;&#28857;&#65292;&#21253;&#25324;&#21033;&#29992;&#22810;&#20010;&#25552;&#31034;&#31526;&#26469;&#36866;&#24212;&#26356;&#22810;&#28436;&#31034;&#12289;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#26816;&#32034;&#12289;&#36716;&#25442;&#20219;&#21153;&#20026;&#26356;&#36866;&#21512;&#29983;&#25104;&#24615;&#36136;&#30340;&#26684;&#24335;&#20197;&#21450;&#37319;&#29992;&#38024;&#23545; NLP &#20219;&#21153;&#35774;&#35745;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649; ChatGPT &#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#65292;&#20854;&#34920;&#29616;&#20173;&#36828;&#20302;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20854;&#20013;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#34920;&#29616;&#27424;&#20339;&#30340;&#21407;&#22240;&#20027;&#35201;&#26377;&#65306;&#65288;1&#65289;&#25552;&#31034;&#31526;&#20013;&#30340;&#20196;&#29260;&#38480;&#21046;&#19981;&#20801;&#35768;&#20805;&#20998;&#21033;&#29992;&#30417;&#30563;&#25968;&#25454;&#38598;&#65307;&#65288;2&#65289;ChatGPT &#29983;&#25104;&#24615;&#36136;&#19982; NLP &#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#65307;&#65288;3&#65289;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#24369;&#28857;&#65292;&#22914;&#20135;&#29983;&#24187;&#35273;&#12289;&#36807;&#24230;&#20851;&#27880;&#29305;&#23450;&#20851;&#38190;&#35789;&#31561;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#27169;&#22359;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26088;&#22312;&#25512;&#21160; ChatGPT &#22312; NLP &#20219;&#21153;&#19978;&#30340;&#26497;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22359;&#21253;&#25324;&#65306;&#65288;1&#65289;&#19968;&#31181;&#36755;&#20837;&#22810;&#25552;&#31034;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#31526;&#26469;&#36866;&#24212;&#26356;&#22810;&#28436;&#31034;&#65307;&#65288;2&#65289;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#26816;&#32034;&#65307;&#65288;3&#65289;&#23558;&#20219;&#21153;&#36716;&#25442;&#20026;&#26356;&#36866;&#21512;&#29983;&#25104;&#24615;&#36136;&#30340;&#26684;&#24335;&#65307;&#65288;4&#65289;&#37319;&#29992;&#38024;&#23545; NLP &#20219;&#21153;&#35774;&#35745;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of ChatGPT, its performances on most NLP tasks are still well below the supervised baselines. In this work, we looked into the causes, and discovered that its subpar performance was caused by the following factors: (1) token limit in the prompt does not allow for the full utilization of the supervised datasets; (2) mismatch between the generation nature of ChatGPT and NLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly focus on certain keywords, etc.  In this work, we propose a collection of general modules to address these issues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed modules include (1) a one-input-multiple-prompts strategy that employs multiple prompts for one input to accommodate more demonstrations; (2) using fine-tuned models for better demonstration retrieval; (3) transforming tasks to formats that are more tailored to the generation nature; (4) employing reasoning strategies that are tailored to addr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#65292;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20851;&#20110;&#8220;&#34920;&#31034;&#8221;&#30340;&#21746;&#23398;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2306.08193</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#31034;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Operationalising Representation in Natural Language Processing. (arXiv:2306.08193v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#65292;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20851;&#20110;&#8220;&#34920;&#31034;&#8221;&#30340;&#21746;&#23398;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#8220;&#34920;&#31034;&#8221;&#22312;&#35748;&#30693;&#31185;&#23398;&#21746;&#23398;&#20013;&#20855;&#26377;&#26680;&#24515;&#22320;&#20301;&#65292;&#20294;&#22312;&#24403;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23454;&#36341;&#20013;&#65292;&#20960;&#20046;&#27809;&#26377;&#21746;&#23398;&#39046;&#22495;&#30340;&#20808;&#21069;&#30740;&#31350;&#19982;&#20043;&#28041;&#21450;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65306;&#32467;&#21512;&#35748;&#30693;&#31185;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#32452;&#20214;&#25152;&#20316;&#20986;&#30340;&#34920;&#31034;&#24615;&#22768;&#26126;&#65292;&#24182;&#25552;&#20986;&#19977;&#20010;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#30340;&#26631;&#20934;&#65292;&#24182;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#23454;&#29616;&#36825;&#20123;&#26631;&#20934;&#30340;&#25805;&#20316;&#21270;&#65292;&#25506;&#27979;&#20998;&#31867;&#22120;&#26159;NLP&#65288;&#21644;&#26356;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#65289;&#20013;&#27969;&#34892;&#30340;&#20998;&#26512;&#25216;&#26415;&#12290;&#25805;&#20316;&#21270;&#19968;&#20010;&#22312;&#21746;&#23398;&#19978;&#21463;&#21040;&#21551;&#21457;&#30340;&#8220;&#34920;&#31034;&#8221;&#27010;&#24565;&#30340;&#39033;&#30446;&#24212;&#35813;&#24341;&#36215;&#31185;&#23398;&#21746;&#23398;&#23478;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23454;&#36341;&#32773;&#30340;&#20852;&#36259;&#12290;&#23545;&#20110;&#21746;&#23398;&#23478;&#26469;&#35828;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#26377;&#20851;&#34920;&#31034;&#30340;&#26412;&#36136;&#30340;&#35770;&#25454;&#30340;&#26032;&#39062;&#22330;&#22320;&#65292;&#24182;&#24110;&#21161;NLPers&#32452;&#32455;&#26377;&#20851;&#25506;&#27979;&#23454;&#39564;&#30340;&#22823;&#37327;&#25991;&#29486;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32463;&#39564;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its centrality in the philosophy of cognitive science, there has been little prior philosophical work engaging with the notion of representation in contemporary NLP practice. This paper attempts to fill that lacuna: drawing on ideas from cognitive science, I introduce a framework for evaluating the representational claims made about components of neural NLP models, proposing three criteria with which to evaluate whether a component of a model represents a property and operationalising these criteria using probing classifiers, a popular analysis technique in NLP (and deep learning more broadly).  The project of operationalising a philosophically-informed notion of representation should be of interest to both philosophers of science and NLP practitioners. It affords philosophers a novel testing-ground for claims about the nature of representation, and helps NLPers organise the large literature on probing experiments, suggesting novel avenues for empirical research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Valley&#30340;&#35270;&#39057;&#21161;&#25163;&#65292;&#23427;&#26159;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#29702;&#35299;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2306.07207</link><description>&lt;p&gt;
Valley: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#35270;&#39057;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Valley: Video Assistant with Large Language model Enhanced abilitY. (arXiv:2306.07207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Valley&#30340;&#35270;&#39057;&#21161;&#25163;&#65292;&#23427;&#26159;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#29702;&#35299;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#20854;&#21331;&#36234;&#30340;&#20250;&#35805;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#25104;&#20026;&#24378;&#22823;&#30340;AI&#21161;&#25163;&#12290;&#37492;&#20110;&#27492;&#65292;&#19968;&#20010;&#30452;&#35266;&#30340;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#26500;&#24314;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;&#24212;&#29992;AI&#21161;&#25163;&#65311;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#12290;&#23427;&#20204;&#36890;&#24120;&#39044;&#20808;&#35757;&#32451;&#19968;&#20010;&#36866;&#24212;&#27169;&#22359;&#26469;&#23545;&#40784;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#65292;&#28982;&#21518;&#22312;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20010;&#27969;&#31243;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22312;&#35270;&#39057;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#36824;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#29702;&#35299;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Valley&#65292;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#35270;&#39057;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), with their remarkable conversational capabilities, have demonstrated impressive performance across various applications and have emerged as formidable AI assistants. In view of this, it raises an intuitive question: Can we harness the power of LLMs to build multimodal AI assistants for visual applications? Recently, several multi-modal models have been developed for this purpose. They typically pre-train an adaptation module to align the semantics of the vision encoder and language model, followed by fine-tuning on instruction-following data. However, despite the success of this pipeline in image and language understanding, its effectiveness in joint video and language understanding has not been widely explored. In this paper, we aim to develop a novel multi-modal foundation model capable of comprehending video, image, and language within a general framework. To achieve this goal, we introduce Valley, a Video Assistant with Large Language model Enhanced ab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#21106;&#25513;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22312;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#26032;&#23545;&#35937;&#30340;&#25235;&#21462;&#25805;&#20316;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#25512;&#24191;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05716</link><description>&lt;p&gt;
&#20026;&#25235;&#20303;&#20219;&#20309;&#29289;&#21697;&#38138;&#24179;&#36947;&#36335;&#65306;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#36890;&#29992;&#25235;&#21462;&#25918;&#32622;&#26426;&#22120;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots. (arXiv:2306.05716v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#21106;&#25513;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22312;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#26032;&#23545;&#35937;&#30340;&#25235;&#21462;&#25805;&#20316;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#25512;&#24191;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#19968;&#30452;&#26159;&#30740;&#31350;&#31038;&#21306;&#38271;&#26399;&#36861;&#27714;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25910;&#38598;&#22823;&#35268;&#27169;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25968;&#25454;&#65292;&#22914; RT-1 &#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#26032;&#23545;&#35937;&#21644;&#22810;&#26679;&#32972;&#26223;&#30340;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#20197;&#35299;&#20915;&#26085;&#24120;&#22330;&#26223;&#20013;&#24191;&#27867;&#30340;&#25342;&#25918;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#25513;&#27169;&#20256;&#36798;&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#20960;&#20309;&#24418;&#29366;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#22810;&#35270;&#35282;&#31574;&#30053;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24863;&#30693;&#20934;&#30830;&#30340;&#29289;&#20307;&#23039;&#24577;&#24182;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#26377;&#25928;&#30340;&#26032;&#23545;&#35937;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#21487;&#20197;&#23454;&#29616;&#22312;&#35757;&#32451;&#26102;&#35266;&#23519;&#21040;&#30456;&#20284;&#24418;&#29366;&#30340;&#26032;&#29289;&#20307;&#30340;&#25235;&#21462;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the generalization capabilities of general-purpose robotic agents has long been a significant challenge actively pursued by research communities. Existing approaches often rely on collecting large-scale real-world robotic data, such as the RT-1 dataset. However, these approaches typically suffer from low efficiency, limiting their capability in open-domain scenarios with new objects, and diverse backgrounds. In this paper, we propose a novel paradigm that effectively leverages language-grounded segmentation masks generated by state-of-the-art foundation models, to address a wide range of pick-and-place robot manipulation tasks in everyday scenarios. By integrating precise semantics and geometries conveyed from masks into our multi-view policy model, our approach can perceive accurate object poses and enable sample-efficient learning. Besides, such design facilitates effective generalization for grasping new objects with similar shapes observed during training. Our approach co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102; ChatGPT &#21644; GPT-4 &#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#36816;&#29992;&#21644;&#24615;&#33021;&#34920;&#29616;&#65292;&#36890;&#36807;&#20197;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#30740;&#31350;&#25361;&#25112;&#20026;&#20363;&#65292;&#32467;&#35770;&#26159; ChatGPT &#21644; GPT-4 &#30340;&#32452;&#21512;&#26159;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#19968;&#31181;&#38750;&#24120;&#39640;&#25928;&#19988;&#33410;&#30465;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05036</link><description>&lt;p&gt;
HCI&#25361;&#25112;&#30340;&#26144;&#23556;&#65306;ChatGPT&#21644;GPT-4&#22312;&#25104;&#26412;&#25928;&#30410;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Cost-Efficient Question Answering. (arXiv:2306.05036v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102; ChatGPT &#21644; GPT-4 &#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#36816;&#29992;&#21644;&#24615;&#33021;&#34920;&#29616;&#65292;&#36890;&#36807;&#20197;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#30740;&#31350;&#25361;&#25112;&#20026;&#20363;&#65292;&#32467;&#35770;&#26159; ChatGPT &#21644; GPT-4 &#30340;&#32452;&#21512;&#26159;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#19968;&#31181;&#38750;&#24120;&#39640;&#25928;&#19988;&#33410;&#30465;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;GPT-4&#27491;&#22312;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#38469;&#24773;&#20917;&#12290;&#20294;&#26159;&#65292;&#36825;&#20004;&#31181;LLM&#26159;&#38381;&#28304;&#30340;&#65292;&#24182;&#19988;&#24456;&#23569;&#26377;&#20851;&#20110;&#23427;&#20204;&#22312;&#23454;&#38469;&#20351;&#29992;&#26696;&#20363;&#20013;&#30340;&#24615;&#33021;&#30340;&#20102;&#35299;&#12290;&#22312;&#23398;&#26415;&#30028;&#20013;&#65292;LLM&#30340;&#24615;&#33021;&#36890;&#24120;&#26159;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#37327;&#30340;&#65292;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#21487;&#33021;&#24050;&#27844;&#28431;&#21040;ChatGPT&#21644;GPT-4&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ChatGPT&#21644;GPT-4&#24212;&#29992;&#20110;&#25104;&#26412;&#25928;&#30410;&#38382;&#31572;&#30340;&#23454;&#38469;&#20219;&#21153;&#65292;&#20197;&#20174;2023&#24180;&#20154;&#26426;&#20132;&#20114;&#20250;&#35758;&#65288;CHI&#65289;&#30340;&#35770;&#25991;&#38598;&#20013;&#25552;&#21462;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;LLM&#22312;&#36825;&#20010;&#23454;&#38469;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;ChatGPT&#21644;GPT-4&#30340;&#32452;&#21512;&#26159;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#26497;&#20339;&#25104;&#26412;&#25928;&#30410;&#25163;&#27573;&#12290;&#25104;&#26412;&#25928;&#29575;&#23545;&#20110;&#21407;&#22411;&#30740;&#31350;&#24819;&#27861;&#21644;&#20174;&#19981;&#21516;&#35282;&#24230;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT and GPT-4, are gaining wide-spread real world use. Yet, the two LLMs are closed source, and little is known about the LLMs' performance in real-world use cases. In academia, LLM performance is often measured on benchmarks which may have leaked into ChatGPT's and GPT-4's training data. In this paper, we apply and evaluate ChatGPT and GPT-4 for the real-world task of cost-efficient extractive question answering over a text corpus that was published after the two LLMs completed training. More specifically, we extract research challenges for researchers in the field of HCI from the proceedings of the 2023 Conference on Human Factors in Computing Systems (CHI). We critically evaluate the LLMs on this practical task and conclude that the combination of ChatGPT and GPT-4 makes an excellent cost-efficient means for analyzing a text corpus at scale. Cost-efficiency is key for prototyping research ideas and analyzing text corpora from different persp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;&#32858;&#21512;&#25552;&#39640;GNN&#22312;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01323</link><description>&lt;p&gt;
&#25581;&#31034;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#32467;&#26500;&#24046;&#24322;&#24615;&#65306;&#19968;&#20010;&#23610;&#30721;&#36866;&#29992;&#20110;&#25152;&#26377;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?. (arXiv:2306.01323v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;&#32858;&#21512;&#25552;&#39640;GNN&#22312;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#35777;&#21644;&#29702;&#35770;&#35777;&#25454;&#65292;&#25903;&#25345;&#23427;&#20204;&#22312;&#25429;&#25417;&#21516;&#26500;&#21644;&#26576;&#20123;&#24322;&#26500;&#22270;&#19978;&#30340;&#32467;&#26500;&#27169;&#24335;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#23454;&#38469;&#20013;&#30340;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#37117;&#30001;&#21516;&#26500;&#21644;&#24322;&#26500;&#32467;&#26500;&#27169;&#24335;&#30340;&#28151;&#21512;&#33410;&#28857;&#32452;&#25104;&#65292;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#24046;&#24322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#19979;&#30340;&#33410;&#28857;&#65288;&#20363;&#22914;&#22312;&#24322;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#65289;&#22312;GNN&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20998;&#26512;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#36890;&#24120;&#26159;&#20986;&#33394;&#30340;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#34920;&#29616;&#20986;&#24615;&#33021;&#24046;&#24322;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35782;&#21035;&#20102;&#27979;&#35797;&#23637;&#31034;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#26102;GNN&#30340;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;GNN&#30340;&#21152;&#26435;&#32858;&#21512;&#20197;&#36866;&#24212;&#24615;&#32467;&#26500;&#24046;&#24322;&#24615;&#30340;&#26032;&#26694;&#26550;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#21644;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on Graph Neural Networks(GNNs) provide both empirical and theoretical evidence supporting their effectiveness in capturing structural patterns on both homophilic and certain heterophilic graphs. Notably, most real-world homophilic and heterophilic graphs are comprised of a mixture of nodes in both homophilic and heterophilic structural patterns, exhibiting a structural disparity. However, the analysis of GNN performance with respect to nodes exhibiting different structural patterns, e.g., homophilic nodes in heterophilic graphs, remains rather limited. In the present study, we provide evidence that Graph Neural Networks(GNNs) on node classification typically perform admirably on homophilic nodes within homophilic graphs and heterophilic nodes within heterophilic graphs while struggling on the opposite node set, exhibiting a performance disparity. We theoretically and empirically identify effects of GNNs on testing nodes exhibiting distinct structural patterns. We then pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#25552;&#31034;&#26041;&#27861;&#8212;&#8212;&#20195;&#30721;&#25552;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35302;&#21457;&#20195;&#30721;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#27604;&#65292;&#20195;&#30721;&#25552;&#31034;&#26377;&#30528;&#20960;&#20010;&#29420;&#29305;&#20248;&#21183;&#65292;&#33021;&#22815;&#25552;&#39640;&#31526;&#21495;&#25512;&#29702;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#24120;&#20248;&#20110;&#24605;&#36335;&#38142;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.18507</link><description>&lt;p&gt;
&#20195;&#30721;&#25552;&#31034;&#65306;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25512;&#29702;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models. (arXiv:2305.18507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#25552;&#31034;&#26041;&#27861;&#8212;&#8212;&#20195;&#30721;&#25552;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35302;&#21457;&#20195;&#30721;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#27604;&#65292;&#20195;&#30721;&#25552;&#31034;&#26377;&#30528;&#20960;&#20010;&#29420;&#29305;&#20248;&#21183;&#65292;&#33021;&#22815;&#25552;&#39640;&#31526;&#21495;&#25512;&#29702;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#24120;&#20248;&#20110;&#24605;&#36335;&#38142;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#36890;&#36807;&#21508;&#31181;&#25552;&#31034;&#26041;&#27861;&#25193;&#22823;&#20102;&#35268;&#27169;&#65292;&#20197;&#35299;&#38145;&#24191;&#27867;&#30340;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25552;&#31034;&#26041;&#27861;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#20013;&#38388;&#27493;&#39588;&#20197;&#24110;&#21161;&#25512;&#29702;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#23436;&#21892;&#30340;&#20219;&#21153;&#32553;&#20943;&#21644;&#28151;&#28102;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#26679;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20195;&#30721;&#25552;&#31034;&#65292;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#25552;&#31034;&#26041;&#27861;&#65292;&#20855;&#26377;&#38646;-shot&#21644;&#23569;-shot&#29256;&#26412;&#65292;&#21487;&#20197;&#35302;&#21457;&#20195;&#30721;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#31526;&#21495;&#25512;&#29702;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;7&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#20195;&#30721;&#25552;&#31034;&#36890;&#24120;&#20248;&#20110;&#24605;&#36335;&#38142;&#25552;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20102;&#35299;&#20195;&#30721;&#25552;&#31034;&#30340;&#24615;&#33021;&#21644;&#38480;&#21046;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#21644;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20102;&#20351;&#29992;&#31526;&#21495;&#25552;&#31034;&#30456;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20960;&#20010;&#29420;&#29305;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#20195;&#30721;&#25552;&#31034;&#21644;&#24605;&#36335;&#38142;&#25552;&#31034;&#30340;&#38598;&#21512;&#65292;&#20197;&#32467;&#21512;&#20004;&#32773;&#30340;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have scaled up to unlock a wide range of complex reasoning tasks with the aid of various prompting methods. However, current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion. To mitigate such limitations, we explore code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps. We conduct experiments on 7 widely-used benchmarks involving symbolic reasoning and arithmetic reasoning. Code prompting generally outperforms chain-of-thought (CoT) prompting. To further understand the performance and limitations of code prompting, we perform extensive ablation studies and error analyses, and identify several exclusive advantages of using symbolic promptings compared to natural language. We also consider the ensemble of code prompting and CoT prompting to combine the strengths of both. Finally, we show throu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#21450;&#20854;&#25968;&#25454;&#38598;VAST-27M&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#36890;&#36807;&#33258;&#21160;&#38598;&#25104;&#22810;&#27169;&#24577;&#23383;&#24149;&#21644;&#36164;&#28304;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#25991;&#26412;&#20851;&#32852;&#30340;&#22810;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18500</link><description>&lt;p&gt;
VAST&#65306;&#19968;&#31181;&#35270;&#21548;&#23383;&#24149;&#25991;&#26412;&#20840;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. (arXiv:2305.18500v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#21450;&#20854;&#25968;&#25454;&#38598;VAST-27M&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#36890;&#36807;&#33258;&#21160;&#38598;&#25104;&#22810;&#27169;&#24577;&#23383;&#24149;&#21644;&#36164;&#28304;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#25991;&#26412;&#20851;&#32852;&#30340;&#22810;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#23436;&#20840;&#25506;&#32034;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#65292;&#32780;&#20854;&#20182;&#27169;&#24577;&#65292;&#22914;&#35270;&#39057;&#20013;&#30340;&#38899;&#39057;&#21644;&#23383;&#24149;&#65292;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#20840;&#27169;&#24577;&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;VAST-27M&#65292;&#24314;&#31435;&#22810;&#27169;&#24577;&#35270;&#39057;&#36712;&#36857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#65292;&#24182;&#19982;&#25991;&#26412;&#36827;&#34892;&#20851;&#32852;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;2700&#19975;&#20010;&#24320;&#25918;&#39046;&#22495;&#35270;&#39057;&#29255;&#27573;&#65292;&#24182;&#20998;&#21035;&#35757;&#32451;&#35270;&#35273;&#21644;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#22120;&#20197;&#29983;&#25104;&#35270;&#35273;&#21644;&#38899;&#39057;&#23383;&#24149;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29616;&#26377;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#29983;&#25104;&#30340;&#23383;&#24149;&#12289;&#23383;&#24149;&#21644;&#25351;&#23548;&#25552;&#31034;&#38598;&#25104;&#21040;&#20840;&#27169;&#24577;&#23383;&#24149;&#20013;&#12290;&#22522;&#20110;&#25552;&#20986;&#30340;VAST-27M&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#65292;&#23427;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#24182;&#26356;&#22909;&#22320;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision and text have been fully explored in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including Vision, Audio, and Subtitle, and Text by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MindEye&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#25193;&#25955;&#20808;&#39564;&#26469;&#37325;&#24314;&#22823;&#33041;&#27963;&#21160;&#23545;&#24212;&#30340;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22270;&#20687;&#37325;&#24314;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;MindEye&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#32034;&#21040;&#21407;&#22987;&#22270;&#20687;&#65292;&#29978;&#33267;&#22312;&#39640;&#24230;&#30456;&#20284;&#30340;&#20505;&#36873;&#39033;&#20013;&#20063;&#33021;&#20570;&#21040;&#12290;</title><link>http://arxiv.org/abs/2305.18274</link><description>&lt;p&gt;
&#37325;&#24314;&#24515;&#28789;&#20043;&#30524;&#65306;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#25193;&#25955;&#20808;&#39564;&#30340;fMRI&#21040;&#22270;&#20687;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors. (arXiv:2305.18274v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MindEye&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#25193;&#25955;&#20808;&#39564;&#26469;&#37325;&#24314;&#22823;&#33041;&#27963;&#21160;&#23545;&#24212;&#30340;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22270;&#20687;&#37325;&#24314;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;MindEye&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#32034;&#21040;&#21407;&#22987;&#22270;&#20687;&#65292;&#29978;&#33267;&#22312;&#39640;&#24230;&#30456;&#20284;&#30340;&#20505;&#36873;&#39033;&#20013;&#20063;&#33021;&#20570;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MindEye&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;fMRI&#21040;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22823;&#33041;&#27963;&#21160;&#20013;&#26816;&#32034;&#21644;&#37325;&#24314;&#35270;&#35273;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#20004;&#20010;&#24182;&#34892;&#23376;&#27169;&#22359;&#32452;&#25104;&#65292;&#20998;&#21035;&#29992;&#20110;&#26816;&#32034;&#65288;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#65289;&#21644;&#37325;&#24314;&#65288;&#20351;&#29992;&#25193;&#25955;&#20808;&#39564;&#65289;&#12290;MindEye&#21487;&#20197;&#23558;fMRI&#33041;&#27963;&#21160;&#26144;&#23556;&#21040;&#20219;&#20309;&#39640;&#32500;&#22810;&#27169;&#24577;&#28508;&#22312;&#31354;&#38388;&#65292;&#22914;CLIP&#22270;&#20687;&#31354;&#38388;&#65292;&#20174;&#32780;&#20351;&#29992;&#25509;&#21463;&#26469;&#33258;&#35813;&#28508;&#22312;&#31354;&#38388;&#23884;&#20837;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#37325;&#24314;&#12290;&#25105;&#20204;&#20840;&#38754;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#65292;&#21253;&#25324;&#23450;&#24615;&#30340;&#24182;&#25490;&#27604;&#36739;&#21644;&#23450;&#37327;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;MindEye&#22312;&#37325;&#24314;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;MindEye&#21487;&#20197;&#22312;&#39640;&#24230;&#30456;&#20284;&#30340;&#20505;&#36873;&#39033;&#20013;&#20934;&#30830;&#26816;&#32034;&#21040;&#21407;&#22987;&#22270;&#20687;&#65292;&#34920;&#26126;&#20854;&#33041;&#23884;&#20837;&#20445;&#30041;&#20102;&#32454;&#31890;&#24230;&#30340;&#22270;&#20687;&#29305;&#23450;&#20449;&#24687;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#20013;&#20934;&#30830;&#22320;&#26816;&#32034;&#22270;&#20687;&#65292;&#22914;LAION-5&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MindEye, a novel fMRI-to-image approach to retrieve and reconstruct viewed images from brain activity. Our model comprises two parallel submodules that are specialized for retrieval (using contrastive learning) and reconstruction (using a diffusion prior). MindEye can map fMRI brain activity to any high dimensional multimodal latent space, like CLIP image space, enabling image reconstruction using generative models that accept embeddings from this latent space. We comprehensively compare our approach with other existing methods, using both qualitative side-by-side comparisons and quantitative evaluations, and show that MindEye achieves state-of-the-art performance in both reconstruction and retrieval tasks. In particular, MindEye can retrieve the exact original image even among highly similar candidates indicating that its brain embeddings retain fine-grained image-specific information. This allows us to accurately retrieve images even from large-scale databases like LAION-5
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39034;&#24207;&#28436;&#21270;&#26465;&#20214;&#20114;&#21160;&#30693;&#35782;&#22270;&#35889; (SCEIKG) &#26694;&#26550;&#65292;&#29992;&#20110;&#20013;&#21307;&#33647;&#25512;&#33616;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#32771;&#34385;&#24739;&#32773;&#22312;&#22810;&#27425;&#23601;&#35786;&#20013;&#30340;&#30149;&#24773;&#21160;&#24577;&#21644;&#33609;&#33647;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25552;&#20379;&#20934;&#30830;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2305.17866</link><description>&lt;p&gt;
&#12298;&#39034;&#24207;&#28436;&#21270;&#26465;&#20214;&#20114;&#21160;&#30693;&#35782;&#22270;&#35889;&#22312;&#20013;&#21307;&#33647;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;&#12299;
&lt;/p&gt;
&lt;p&gt;
Sequential Condition Evolved Interaction Knowledge Graph for Traditional Chinese Medicine Recommendation. (arXiv:2305.17866v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39034;&#24207;&#28436;&#21270;&#26465;&#20214;&#20114;&#21160;&#30693;&#35782;&#22270;&#35889; (SCEIKG) &#26694;&#26550;&#65292;&#29992;&#20110;&#20013;&#21307;&#33647;&#25512;&#33616;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#32771;&#34385;&#24739;&#32773;&#22312;&#22810;&#27425;&#23601;&#35786;&#20013;&#30340;&#30149;&#24773;&#21160;&#24577;&#21644;&#33609;&#33647;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25552;&#20379;&#20934;&#30830;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#20013;&#21307;&#33647; (TCM) &#22312;&#27835;&#30103;&#21508;&#31181;&#30142;&#30149;&#26102;&#26377;&#30528;&#20016;&#23500;&#30340;&#21382;&#21490;&#65292;&#21033;&#29992;&#22825;&#28982;&#33609;&#33647;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;TCM&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#39640;&#24230;&#20010;&#24615;&#21270;&#65292;&#26377;&#26426;&#32508;&#21512;&#65292;&#38656;&#35201;&#20840;&#38754;&#32771;&#34385;&#24739;&#32773;&#30340;&#29366;&#20917;&#21644;&#30151;&#29366;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;TCM&#25512;&#33616;&#26041;&#27861;&#24573;&#30053;&#20102;&#24739;&#32773;&#29366;&#24577;&#30340;&#21464;&#21270;&#65292;&#21482;&#25506;&#32034;&#30151;&#29366;&#21644;&#22788;&#26041;&#20043;&#38388;&#30340;&#28508;&#22312;&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39034;&#24207;&#28436;&#21270;&#26465;&#20214;&#20114;&#21160;&#30693;&#35782;&#22270;&#35889; (SCEIKG) &#26694;&#26550;&#65292;&#23558;&#27169;&#22411;&#35270;&#20026;&#19968;&#20010;&#39034;&#24207;&#22788;&#26041;&#21046;&#23450;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#24739;&#32773;&#22312;&#22810;&#27425;&#23601;&#35786;&#20013;&#30340;&#30149;&#24773;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#20114;&#21160;&#30693;&#35782;&#22270;&#35889;&#32435;&#20837;&#21040;&#25512;&#33616;&#20013;&#65292;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#33609;&#33647;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#24739;&#32773;&#30340;&#29366;&#20917;&#26469;&#25552;&#39640;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;TCM&#25512;&#33616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional Chinese Medicine (TCM) has a rich history of utilizing natural herbs to treat a diversity of illnesses. In practice, TCM diagnosis and treatment are highly personalized and organically holistic, requiring comprehensive consideration of the patient's state and symptoms over time. However, existing TCM recommendation approaches overlook the changes in patient status and only explore potential patterns between symptoms and prescriptions. In this paper, we propose a novel Sequential Condition Evolved Interaction Knowledge Graph (SCEIKG), a framework that treats the model as a sequential prescription-making problem by considering the dynamics of the patient's condition across multiple visits. In addition, we incorporate an interaction knowledge graph to enhance the accuracy of recommendations by considering the interactions between different herbs and the patient's condition. Experimental results on a real-world dataset demonstrate that our approach outperforms existing TCM reco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GFlowNets&#30340;&#26426;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.17010</link><description>&lt;p&gt;
&#21033;&#29992;GFlowNets&#35299;&#20915;&#22270;&#24418;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Let the Flows Tell: Solving Graph Combinatorial Optimization Problems with GFlowNets. (arXiv:2305.17010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GFlowNets&#30340;&#26426;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#26159;NP&#38590;&#39064;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#31934;&#30830;&#31639;&#27861;&#65292;&#36825;&#20351;&#23427;&#20204;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#29702;&#24819;&#39046;&#22495;&#12290;&#36825;&#20123;&#38382;&#39064;&#20013;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#38480;&#21046;&#21487;&#33021;&#20250;&#30452;&#25509;&#38459;&#30861;&#20248;&#21270;&#25110;&#37319;&#26679;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;GFlowNets&#26368;&#36817;&#34987;&#21457;&#29616;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26426;&#22120;&#65292;&#21487;&#20197;&#39034;&#24207;&#22320;&#20174;&#22797;&#21512;&#38750;&#35268;&#33539;&#21270;&#23494;&#24230;&#20013;&#26377;&#25928;&#22320;&#37319;&#26679;&#65292;&#24182;&#20855;&#26377;&#22312;CO&#20013;&#20998;&#25674;&#27492;&#31867;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#36807;&#31243;&#20197;&#21450;&#29983;&#25104;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#20505;&#36873;&#39033;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#32452;&#21512;&#38382;&#39064;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#25552;&#20986;&#35757;&#32451;&#26377;&#26465;&#20214;&#30340;GFlowNets&#20174;&#35299;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#31574;&#30053;&#12290;&#36824;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#25216;&#26415;&#26469;&#21463;&#30410;&#20110;&#36828;&#31243;&#20449;&#29992;&#20998;&#37197;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#20351;&#29992;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#19981;&#21516;CO&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GFlowNet&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combinatorial optimization (CO) problems are often NP-hard and thus out of reach for exact algorithms, making them a tempting domain to apply machine learning methods. The highly structured constraints in these problems can hinder either optimization or sampling directly in the solution space. On the other hand, GFlowNets have recently emerged as a powerful machinery to efficiently sample from composite unnormalized densities sequentially and have the potential to amortize such solution-searching processes in CO, as well as generate diverse solution candidates. In this paper, we design Markov decision processes (MDPs) for different combinatorial problems and propose to train conditional GFlowNets to sample from the solution space. Efficient training techniques are also developed to benefit long-range credit assignment. Through extensive experiments on a variety of different CO tasks with synthetic and realistic data, we demonstrate that GFlowNet policies can efficiently find high-quali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#32477;&#23545;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16646</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#26679;&#26412;&#30340;&#32477;&#23545;&#25512;&#29702;&#26469;&#25552;&#39640;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning. (arXiv:2305.16646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#32477;&#23545;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#25512;&#29702;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20107;&#20214;&#65292;&#24110;&#21161;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#20854;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#32477;&#23545;&#25512;&#29702;&#20197;&#36741;&#21161;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#65306;&#20107;&#20214;&#27169;&#22411;&#22312;&#32473;&#23450;&#36807;&#21435;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;; &#22312;&#20960;&#20010;&#19987;&#23478;&#27880;&#37322;&#31034;&#33539;&#30340;&#25351;&#23548;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#20102;&#20026;&#27599;&#20010;&#25552;&#35758;&#25552;&#20379;&#21487;&#33021;&#30340;&#21407;&#22240;; &#19968;&#20010;&#25628;&#32034;&#27169;&#22359;&#25214;&#21040;&#19982;&#21407;&#22240;&#21305;&#37197;&#30340;&#20808;&#21069;&#20107;&#20214;; &#19968;&#20010;&#35780;&#20998;&#20989;&#25968;&#23398;&#20250;&#26816;&#26597;&#26816;&#32034;&#21040;&#30340;&#20107;&#20214;&#26159;&#21542;&#23454;&#38469;&#19978;&#21487;&#20197;&#23548;&#33268;&#25552;&#35758;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;&#20122;&#39532;&#36874;&#35780;&#35770;&#21644;GDELT&#65289;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550; - &#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147; - &#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have shown astonishing performance on a wide range of reasoning tasks. In this paper, we investigate whether they could reason about real-world events and help improve the prediction accuracy of event sequence models. We design a modeling and prediction framework where a large language model performs abductive reasoning to assist an event sequence model: the event model proposes predictions on future events given the past; instructed by a few expert-annotated demonstrations, the language model learns to suggest possible causes for each proposal; a search module finds out the previous events that match the causes; a scoring function learns to examine whether the retrieved events could actually cause the proposal. Through extensive experiments on two challenging real-world datasets (Amazon Review and GDELT), we demonstrate that our framework -- thanks to the reasoning ability of language models -- could significantly outperform the state-of-the-art event sequence mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#21517;&#20026;"Commotions"&#30340;&#26032;&#22411;&#35748;&#30693;&#21512;&#29702;&#27169;&#22411;&#65292;&#22312;&#39044;&#27979;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#23637;&#31034;&#20102;&#20854;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.15187</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#35748;&#30693;&#29702;&#35770;&#30340;&#27169;&#22411;&#39044;&#27979;&#20132;&#36890;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#65306;&#20197;&#26696;&#20363;&#30740;&#31350;&#20026;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Using Models Based on Cognitive Theory to Predict Human Behavior in Traffic: A Case Study. (arXiv:2305.15187v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#21517;&#20026;"Commotions"&#30340;&#26032;&#22411;&#35748;&#30693;&#21512;&#29702;&#27169;&#22411;&#65292;&#22312;&#39044;&#27979;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#23637;&#31034;&#20102;&#20854;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#21457;&#23637;&#26377;&#28508;&#21147;&#24443;&#24213;&#25913;&#21464;&#20132;&#36890;&#65292;&#20294;&#30446;&#21069;&#26080;&#27861;&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#39550;&#39542;&#39118;&#26684;&#12290;&#21487;&#38752;&#30340;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#30340;&#27169;&#22411;&#23545;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#27492;&#30446;&#30340;&#65292;&#20294;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#36793;&#30028;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#23384;&#22312;&#39118;&#38505;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#25972;&#21512;&#35748;&#30693;&#29702;&#35770;&#30340;&#27169;&#22411;&#30340;&#20852;&#36259;&#65292;&#20294;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26159;&#20026;&#35299;&#37322;&#24615;&#30446;&#30340;&#32780;&#24320;&#21457;&#30340;&#65292;&#22240;&#27492;&#36825;&#31181;&#26041;&#27861;&#22312;&#34892;&#20026;&#39044;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#32463;&#36807;&#27979;&#35797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;"Commotions"&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#24341;&#20837;&#20102;&#26368;&#26032;&#30340;&#20154;&#31867;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#36816;&#21160;&#25511;&#21046;&#29702;&#35770;&#30340;&#35748;&#30693;&#21512;&#29702;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20132;&#36890;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#21253;&#25324;&#36710;&#36947;&#21464;&#26356;&#21644;&#20132;&#21449;&#36335;&#21475;&#31561;&#35768;&#22810;&#37325;&#35201;&#30340;&#20132;&#36890;&#20114;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#33021;&#22815;&#19982;&#29978;&#33267;&#36229;&#36234;&#24050;&#26377;&#27169;&#22411;&#31454;&#20105;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of automated vehicles has the potential to revolutionize transportation, but they are currently unable to ensure a safe and time-efficient driving style. Reliable models predicting human behavior are essential for overcoming this issue. While data-driven models are commonly used to this end, they can be vulnerable in safety-critical edge cases. This has led to an interest in models incorporating cognitive theory, but as such models are commonly developed for explanatory purposes, this approach's effectiveness in behavior prediction has remained largely untested so far. In this article, we investigate the usefulness of the \emph{Commotions} model -- a novel cognitively plausible model incorporating the latest theories of human perception, decision-making, and motor control -- for predicting human behavior in gap acceptance scenarios, which entail many important traffic interactions such as lane changes and intersections. We show that this model can compete with or even o
&lt;/p&gt;</description></item><item><title>INSTRUCTSCORE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#24230;&#37327;&#65292;&#36890;&#36807;&#21033;&#29992;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#20196;&#21644;GPT-4&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#23427;&#33021;&#29983;&#25104;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#25968;&#21644;&#20154;&#31867;&#21487;&#35835;&#30340;&#35786;&#26029;&#25253;&#21578;&#65292;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#24230;&#37327;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.14282</link><description>&lt;p&gt;
INSTRUCTSCORE: &#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#19982;&#32454;&#31890;&#24230;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback. (arXiv:2305.14282v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14282
&lt;/p&gt;
&lt;p&gt;
INSTRUCTSCORE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#24230;&#37327;&#65292;&#36890;&#36807;&#21033;&#29992;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#20196;&#21644;GPT-4&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#23427;&#33021;&#29983;&#25104;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#25968;&#21644;&#20154;&#31867;&#21487;&#35835;&#30340;&#35786;&#26029;&#25253;&#21578;&#65292;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#24230;&#37327;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#36817;&#23398;&#20064;&#24230;&#37327;&#34920;&#26174;&#31034;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#26080;&#27861;&#35299;&#37322;&#20854;&#21028;&#26029;&#25110;&#23558;&#20998;&#25968;&#19982;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#32570;&#38519;&#20851;&#32852;&#36215;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructScore&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#24230;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#20196;&#21644;GPT-4&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#25105;&#20204;&#22522;&#20110;LLaMA&#23545;&#25991;&#26412;&#35780;&#20272;&#24230;&#37327;&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#25968;&#21644;&#20154;&#31867;&#21487;&#35835;&#30340;&#35786;&#26029;&#25253;&#21578;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;InstructScore&#65292;&#21253;&#25324;&#32763;&#35793;&#12289;&#23383;&#24149;&#29983;&#25104;&#12289;&#25968;&#25454;&#21040;&#25991;&#26412;&#21644;&#24120;&#35782;&#29983;&#25104;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;7B&#27169;&#22411;&#36229;&#36807;&#20102;&#25152;&#26377;&#20854;&#20182;&#26080;&#30417;&#30563;&#24230;&#37327;&#65292;&#21253;&#25324;&#22522;&#20110;175B GPT-3&#21644;GPT-4&#30340;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#27809;&#26377;&#26469;&#33258;&#20154;&#24037;&#35780;&#32423;&#25968;&#25454;&#30340;&#30452;&#25509;&#30417;&#30563;&#65292;&#25105;&#20204;&#30340;InstructScore&#30340;&#24615;&#33021;&#27700;&#24179;&#20063;&#19982;COMET2&#31561;&#26368;&#20808;&#36827;&#30340;&#24230;&#37327;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics can not explain their verdict or associate the scores with defects in generated text. To address this limitation, we present InstructScore, an explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate InstructScore on a variety of generation tasks, including translation, captioning, data-to-text and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14259</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25991;&#29486;&#30340;&#35821;&#22659;&#21270;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#65288;LBD&#65289;&#26088;&#22312;&#36890;&#36807;&#25366;&#25496;&#35770;&#25991;&#24182;&#29983;&#25104;&#20551;&#35774;&#26469;&#21457;&#29616;&#26032;&#30340;&#31185;&#23398;&#30693;&#35782;&#12290;&#26631;&#20934;&#30340;LBD&#20165;&#38480;&#20110;&#39044;&#27979;&#31163;&#25955;&#27010;&#24565;&#20043;&#38388;&#30340;&#20004;&#20004;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#20851;&#32852;&#65289;&#12290;LBD&#36824;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23454;&#39564;&#35774;&#32622;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#35780;&#20272;&#30340;&#29305;&#23450;&#24739;&#32773;&#32676;&#20307;&#65289;&#21644;&#20154;&#31867;&#31185;&#23398;&#23478;&#32771;&#34385;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#21160;&#26426;&#65288;&#20363;&#22914;&#65292;&#25214;&#21040;&#27809;&#26377;&#29305;&#23450;&#21103;&#20316;&#29992;&#30340;&#33647;&#29289;&#20505;&#36873;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#21270;LBD&#65288;C-LBD&#65289;&#34920;&#36848;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65306;&#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31185;&#23398;&#20551;&#35774;&#65292;&#21516;&#26102;&#23558;&#23427;&#20204;&#32852;&#31995;&#21040;&#25511;&#21046;&#20551;&#35774;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#29992;&#33719;&#24471;&#30340;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#24322;&#26500;&#32593;&#32476;&#20013;&#30340;&#8220;&#28789;&#24863;&#8221;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;&#35770;&#25991;&#20013;&#27966;&#29983;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#35843;&#26597;&#25506;&#35752;&#20102;&#22312;&#20196;&#29260;&#21361;&#26426;&#19979;&#25193;&#23637;LLM&#30340;&#37325;&#22797;&#39044;&#35757;&#32451;&#25968;&#25454;&#26041;&#27861;&#65292;&#21457;&#29616;&#27169;&#22411;&#23481;&#26131;&#36807;&#25311;&#21512;&#24182;&#23548;&#33268;&#22810;&#36718;&#27425;&#24615;&#33021;&#19979;&#38477;&#65292;&#20851;&#38190;&#22240;&#32032;&#21253;&#25324;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#27491;&#21017;&#21270;&#25216;&#26415;&#24182;&#19981;&#33021;&#26126;&#26174;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13230</link><description>&lt;p&gt;
&#26159;&#21542;&#37325;&#22797;&#30340;&#30097;&#38382;: &#22312;&#20196;&#29260;&#21361;&#26426;&#19979;&#25193;&#23637;LLM&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis. (arXiv:2305.13230v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#35843;&#26597;&#25506;&#35752;&#20102;&#22312;&#20196;&#29260;&#21361;&#26426;&#19979;&#25193;&#23637;LLM&#30340;&#37325;&#22797;&#39044;&#35757;&#32451;&#25968;&#25454;&#26041;&#27861;&#65292;&#21457;&#29616;&#27169;&#22411;&#23481;&#26131;&#36807;&#25311;&#21512;&#24182;&#23548;&#33268;&#22810;&#36718;&#27425;&#24615;&#33021;&#19979;&#38477;&#65292;&#20851;&#38190;&#22240;&#32032;&#21253;&#25324;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#27491;&#21017;&#21270;&#25216;&#26415;&#24182;&#19981;&#33021;&#26126;&#26174;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#25968;&#25454;&#38598;&#35268;&#27169;&#23545;&#20110;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#38750;&#24120;&#20381;&#36182;&#20110;&#20196;&#29260;&#65292;&#24182;&#19988;&#32593;&#32476;&#19978;&#30340;&#39640;&#36136;&#37327;&#25991;&#26412;&#25968;&#25454;&#24050;&#25509;&#36817;LLMs&#30340;&#25193;&#23637;&#38480;&#21046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26159;&#37325;&#22797;&#39044;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#36718;&#27425;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#23454;&#35777;&#35282;&#24230;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#19979;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#37325;&#22797;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#21518;&#26524;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#22810;&#36718;&#27425;&#24615;&#33021;&#19979;&#38477;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23548;&#33268;&#22810;&#36718;&#27425;&#24615;&#33021;&#19979;&#38477;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21457;&#29616;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#30446;&#26631;&#26159;&#26174;&#33879;&#22240;&#32032;&#65292;&#32780;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#27169;&#22411;FLOP&#21017;&#24433;&#21709;&#36739;&#23567;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26159;&#21542;&#21487;&#20197;&#32531;&#35299;&#22810;&#36718;&#27425;&#24615;&#33021;&#19979;&#38477;&#12290;&#22823;&#22810;&#25968;&#27491;&#21017;&#21270;&#25216;&#26415;&#24182;&#19981;&#33021;&#26126;&#26174;&#32531;&#35299;&#36825;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has highlighted the importance of dataset size in scaling language models. However, large language models (LLMs) are notoriously token-hungry during pre-training, and high-quality text data on the web is approaching its scaling limit for LLMs. To further enhance LLMs, a straightforward approach is to repeat the pre-training data for additional epochs. In this study, we empirically investigate three key aspects under this approach. First, we explore the consequences of repeating pre-training data, revealing that the model is susceptible to overfitting, leading to multi-epoch degradation. Second, we examine the key factors contributing to multi-epoch degradation, finding that significant factors include dataset size, model parameters, and training objectives, while less influential factors consist of dataset quality and model FLOPs. Finally, we explore whether widely used regularization can alleviate multi-epoch degradation. Most regularization techniques do not yield sig
&lt;/p&gt;</description></item><item><title>GRACE++&#26159;&#19968;&#20010;&#25239;&#20002;&#21253;&#30340;&#23454;&#26102;&#35270;&#39057;&#31995;&#32479;&#65292;&#36890;&#36807;&#31070;&#32463;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#20002;&#21253;&#24773;&#20917;&#19979;&#20445;&#25345;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.12333</link><description>&lt;p&gt;
GRACE++&#65306;&#36890;&#36807;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#25239;&#20002;&#21253;&#30340;&#23454;&#26102;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
GRACE++: Loss-Resilient Real-Time Video through Neural Codecs. (arXiv:2305.12333v2 [cs.MM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12333
&lt;/p&gt;
&lt;p&gt;
GRACE++&#26159;&#19968;&#20010;&#25239;&#20002;&#21253;&#30340;&#23454;&#26102;&#35270;&#39057;&#31995;&#32479;&#65292;&#36890;&#36807;&#31070;&#32463;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#20002;&#21253;&#24773;&#20917;&#19979;&#20445;&#25345;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#26102;&#35270;&#39057;&#36890;&#20449;&#20013;&#65292;&#30001;&#20110;&#20005;&#26684;&#30340;&#24310;&#36831;&#35201;&#27714;&#65292;&#37325;&#26032;&#20256;&#36755;&#20002;&#22833;&#30340;&#25968;&#25454;&#21253;&#22312;&#39640;&#24310;&#36831;&#32593;&#32476;&#19979;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#27809;&#26377;&#37325;&#20256;&#30340;&#20002;&#21253;&#24773;&#20917;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#20027;&#35201;&#31574;&#30053;--&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#21069;&#21521;&#24046;&#38169;&#32416;&#27491;&#65288;FEC&#65289;&#21644;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#38169;&#35823;&#38544;&#34255;&#12290;&#21069;&#32773;&#22312;&#20256;&#36755;&#20043;&#21069;&#29992;&#20887;&#20313;&#32534;&#30721;&#25968;&#25454;&#65292;&#20294;&#25552;&#21069;&#30830;&#23450;&#26368;&#20339;&#20887;&#20313;&#32423;&#21035;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21518;&#32773;&#20174;&#37096;&#20998;&#25910;&#21040;&#30340;&#24103;&#20013;&#37325;&#24314;&#35270;&#39057;&#65292;&#20294;&#23558;&#24103;&#21010;&#20998;&#20026;&#29420;&#31435;&#32534;&#30721;&#30340;&#20998;&#21306;&#20250;&#38477;&#20302;&#21387;&#32553;&#25928;&#29575;&#65292;&#24182;&#19988;&#20002;&#22833;&#30340;&#20449;&#24687;&#22312;&#27809;&#26377;&#36866;&#24212;&#32534;&#30721;&#22120;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#26377;&#25928;&#22320;&#34987;&#35299;&#30721;&#22120;&#24674;&#22797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRACE++&#30340;&#25239;&#20002;&#21253;&#23454;&#26102;&#35270;&#39057;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#65292;&#23427;&#33021;&#22815;&#22312;&#21508;&#31181;&#20002;&#21253;&#24773;&#20917;&#19979;&#20445;&#25345;&#29992;&#25143;&#30340;&#20307;&#39564;&#36136;&#37327;&#65288;QoE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-time video communication, retransmitting lost packets over high-latency networks is not viable due to strict latency requirements. To counter packet losses without retransmission, two primary strategies are employed -- encoder-based forward error correction (FEC) and decoder-based error concealment. The former encodes data with redundancy before transmission, yet determining the optimal redundancy level in advance proves challenging. The latter reconstructs video from partially received frames, but dividing a frame into independently coded partitions inherently compromises compression efficiency, and the lost information cannot be effectively recovered by the decoder without adapting the encoder.  We present a loss-resilient real-time video system called GRACE++, which preserves the user's quality of experience (QoE) across a wide range of packet losses through a new neural video codec. Central to GRACE++'s enhanced loss resilience is its joint training of the neural encoder an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#26080;&#38656;&#27880;&#37322;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#29983;&#25104;&#38899;&#35270;&#39057;&#20998;&#21106;&#20219;&#21153;&#30340;&#20154;&#24037;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#38899;&#39057;&#24863;&#30693;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;Transformer&#35299;&#30721;&#22120;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#38899;&#39057;&#20449;&#21495;&#30340;&#25351;&#23548;&#19979;&#25628;&#32034;&#22768;&#38899;&#23545;&#35937;&#65292;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2305.11019</link><description>&lt;p&gt;
&#26080;&#26631;&#27880;&#38899;&#35270;&#39057;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Annotation-free Audio-Visual Segmentation. (arXiv:2305.11019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#26080;&#38656;&#27880;&#37322;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#29983;&#25104;&#38899;&#35270;&#39057;&#20998;&#21106;&#20219;&#21153;&#30340;&#20154;&#24037;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#38899;&#39057;&#24863;&#30693;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;Transformer&#35299;&#30721;&#22120;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#38899;&#39057;&#20449;&#21495;&#30340;&#25351;&#23548;&#19979;&#25628;&#32034;&#22768;&#38899;&#23545;&#35937;&#65292;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#35270;&#39057;&#20998;&#21106;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20934;&#30830;&#22320;&#39044;&#27979;&#20687;&#32032;&#32423;&#20998;&#21106;&#25513;&#30721;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#23450;&#20301;&#22768;&#38899;&#23545;&#35937;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#26080;&#38656;&#27880;&#37322;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#29983;&#25104;&#38899;&#35270;&#39057;&#20998;&#21106;&#20219;&#21153;&#30340;&#20154;&#24037;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#38899;&#39057;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#31867;&#21035;&#26631;&#31614;&#12289;&#22270;&#20687;&#25513;&#27169;&#23545;&#21644;&#38899;&#39057;&#26679;&#26412;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#21487;&#20197;&#36731;&#26494;&#32452;&#21512;&#35757;&#32451;AVS&#27169;&#22411;&#30340;&#65288;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#25513;&#27169;&#65289;&#19977;&#20803;&#32452;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#24863;&#30693;&#21464;&#21387;&#22120;&#65288;AuTR&#65289;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#38899;&#39057;&#24863;&#30693;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;Transformer&#35299;&#30721;&#22120;&#12290;&#35813;&#26550;&#26500;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#38899;&#39057;&#20449;&#21495;&#30340;&#25351;&#23548;&#19979;&#25628;&#32034;&#22768;&#38899;&#23545;&#35937;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#65307;&#65288;iii&#65289;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#31649;&#36947;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;AVS&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of Audio-Visual Segmentation (AVS) is to locate sounding objects within visual scenes by accurately predicting pixelwise segmentation masks. In this paper, we present the following contributions: (i), we propose a scalable and annotation-free pipeline for generating artificial data for the AVS task. We leverage existing image segmentation and audio datasets to draw links between category labels, image-mask pairs, and audio samples, which allows us to easily compose (image, audio, mask) triplets for training AVS models; (ii), we introduce a novel Audio-Aware Transformer (AuTR) architecture that features an audio-aware query-based transformer decoder. This architecture enables the model to search for sounding objects with the guidance of audio signals, resulting in more accurate segmentation; (iii), we present extensive experiments conducted on both synthetic and real datasets, which demonstrate the effectiveness of training AVS models with synthetic data generated by our p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#30340;&#35856;&#27874;&#35780;&#20272;&#26041;&#27861;RobustFair&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#34394;&#20551;&#20844;&#24179;&#30456;&#32467;&#21512;&#30340;&#40065;&#26834;&#24615;&#32570;&#38519;&#65292;&#25552;&#39640;DNN&#30340;&#40065;&#26834;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10906</link><description>&lt;p&gt;
RobustFair: &#36890;&#36807;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#30340;&#25932;&#23545;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RobustFair: Adversarial Evaluation through Fairness Confusion Directed Gradient Search. (arXiv:2305.10906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#30340;&#35856;&#27874;&#35780;&#20272;&#26041;&#27861;RobustFair&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#34394;&#20551;&#20844;&#24179;&#30456;&#32467;&#21512;&#30340;&#40065;&#26834;&#24615;&#32570;&#38519;&#65292;&#25552;&#39640;DNN&#30340;&#40065;&#26834;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#30340;&#21487;&#20449;&#24230;&#32463;&#24120;&#21463;&#21040;&#36731;&#24494;&#25932;&#23545;&#25200;&#21160;&#30340;&#25361;&#25112;&#65292;&#36825;&#19981;&#20165;&#20250;&#30772;&#22351;&#39044;&#27979;&#20934;&#30830;&#24615;&#65288;&#40065;&#26834;&#24615;&#65289;&#32780;&#19988;&#21487;&#33021;&#20026;&#31867;&#20284;&#30340;&#36755;&#20837;&#23548;&#33268;&#26377;&#20559;&#39044;&#27979;&#65288;&#20010;&#20307;&#20844;&#24179;&#24615;&#65289;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#20934;&#30830;&#20844;&#27491;&#24230;&#26469;&#24378;&#21046;&#23454;&#26045;&#20934;&#30830;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#20043;&#38388;&#30340;&#35856;&#21644;&#24179;&#34913;&#12290;&#23427;&#24341;&#20837;&#20102;&#20844;&#24179;&#28151;&#28102;&#30697;&#38453;&#30340;&#27010;&#24565;&#26469;&#23558;&#39044;&#27979;&#20998;&#31867;&#20026;&#30495;&#27491;&#20844;&#24179;&#12289;&#30495;&#27491;&#26377;&#20559;&#12289;&#20551;&#27491;&#20844;&#24179;&#21644;&#20551;&#26377;&#20559;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35856;&#27874;&#35780;&#20272;&#26041;&#27861;RobustFair&#65292;&#20351;&#29992;&#36890;&#36807;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#21046;&#20316;&#30340;&#25932;&#23545;&#25200;&#21160;&#65292;&#23545;DNN&#30340;&#20934;&#30830;&#20844;&#27491;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;Taylor&#23637;&#24320;&#26469;&#36817;&#20284;&#25932;&#23545;&#23454;&#20363;&#30340;&#22522;&#26412;&#30495;&#23454;&#24615;&#65292;RobustFair&#21487;&#20197;&#29305;&#21035;&#35782;&#21035;&#19982;&#34394;&#20551;&#20844;&#24179;&#32416;&#32544;&#22312;&#19968;&#36215;&#30340;&#40065;&#26834;&#24615;&#32570;&#38519;&#65292;&#36825;&#36890;&#24120;&#22312;&#40065;&#26834;&#24615;&#35780;&#20272;&#20013;&#38590;&#20197;&#25417;&#25720;&#65292;&#22312;&#20010;&#20307;&#20844;&#24179;&#35780;&#20272;&#20013;&#32570;&#22833;&#12290;RobustFair&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The trustworthiness of DNNs is often challenged by their vulnerability to minor adversarial perturbations, which may not only undermine prediction accuracy (robustness) but also cause biased predictions for similar inputs (individual fairness). Accurate fairness has been recently proposed to enforce a harmonic balance between accuracy and individual fairness. It induces the notion of fairness confusion matrix to categorize predictions as true fair, true biased, false fair, and false biased. This paper proposes a harmonic evaluation approach, RobustFair, for the accurate fairness of DNNs, using adversarial perturbations crafted through fairness confusion directed gradient search. By using Taylor expansions to approximate the ground truths of adversarial instances, RobustFair can particularly identify the robustness defects entangled for spurious fairness, which are often elusive in robustness evaluation, and missing in individual fairness evaluation. RobustFair can boost robustness and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#24050;&#32463;&#32534;&#30721;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#30340;&#30456;&#20851;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#25552;&#31034;&#65292;&#24182;&#23558;&#30456;&#20851;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08732</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Knowledge Rumination for Pre-trained Language Models. (arXiv:2305.08732v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#24050;&#32463;&#32534;&#30721;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#30340;&#30456;&#20851;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#25552;&#31034;&#65292;&#24182;&#23558;&#30456;&#20851;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26222;&#36890;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21333;&#29420;&#22788;&#29702;&#30693;&#35782;&#23494;&#38598;&#22411;NLP&#20219;&#21153;&#30340;&#33021;&#21147;&#19981;&#36275;&#65292;&#22240;&#27492;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#23558;&#22806;&#37096;&#30693;&#35782;&#38598;&#25104;&#21040;PLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#65292;PLM&#21487;&#33021;&#24050;&#32463;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#20294;&#22312;&#24212;&#29992;&#21040;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26102;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#20197;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#30456;&#20851;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#23427;&#20204;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;PLMs&#20013;&#28155;&#21152;&#19968;&#20010;&#22914;&#8220;&#25454;&#25105;&#25152;&#30693;&#8221;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#35797;&#22270;&#22238;&#39038;&#30456;&#20851;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#27169;&#22411;&#20197;&#36827;&#34892;&#30693;&#35782;&#25972;&#21512;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#30693;&#35782;&#21453;&#24605;&#24212;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;RoBERTa&#12289;DeBERTa&#21644;GPT-3&#12290;&#22312;&#20845;&#20010;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;.....
&lt;/p&gt;
&lt;p&gt;
Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fail to fully utilize them when applying them to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving it from the external corpus. By simply adding a prompt like "As far as I know" to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks dem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06176</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#30340;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;RLHF&#21463;&#21040;&#20154;&#31867;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#29983;&#20135;&#21147;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;: &#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#20195;&#26367;RLHF&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#65292;RLGAF&#21487;&#20197;&#24110;&#21161;&#23545;&#40784;LLM&#30340;&#36755;&#20986;&#65292;&#21516;&#26102;&#19981;&#20250;&#21463;&#21040;RLHF&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#20026;&#36827;&#19968;&#27493;&#33258;&#21160;&#21270;AI&#23545;&#40784;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. In this study, we investigate an alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings indicate that RLGAF can help align LLMs outputs while not suffering from the inherent restrictions of RLHF, suggesting promising avenues for further research on automating AI alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.05640</link><description>&lt;p&gt;
&#38754;&#21521;&#20010;&#20154;&#25110;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare. (arXiv:2305.05640v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#19968;&#31181;&#25353;&#26412;&#20307;&#25110;&#27169;&#24335;&#32452;&#32455;&#20449;&#24687;&#30340;&#27969;&#34892;&#26041;&#24335;&#65292;&#24050;&#32463;&#22312;&#20174;&#25628;&#32034;&#21040;&#25512;&#33616;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#30693;&#35782;&#22270;&#35889;&#26041;&#38754;&#26377;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30693;&#35782;&#34920;&#31034;&#20173;&#28982;&#26159;&#36328;&#34892;&#19994;&#30340;&#19968;&#20010;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#30001;&#20110;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20851;&#31995;&#12289;&#24322;&#36136;&#24615;&#12289;&#32570;&#20047;&#26631;&#20934;&#21270;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#31561;&#22240;&#32032;&#65292;&#36825;&#19968;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#25429;&#25417;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21517;&#20026;HEER&#65288;Healthcare Entity-Entity Representation learning&#65289;&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;HEER&#22312;&#25913;&#21892;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are a popular way to organise information based on ontologies or schemas and have been used across a variety of scenarios from search to recommendation. Despite advances in KGs, representing knowledge remains a non-trivial task across industries and it is especially challenging in the biomedical and healthcare domains due to complex interdependent relations between entities, heterogeneity, lack of standardization, and sparseness of data. KGs are used to discover diagnoses or prioritize genes relevant to disease, but they often rely on schemas that are not centred around a node or entity of interest, such as a person. Entity-centric KGs are relatively unexplored but hold promise in representing important facets connected to a central node and unlocking downstream tasks beyond graph traversal and reasoning, such as generating graph embeddings and training graph neural networks for a wide range of predictive tasks. This paper presents an end-to-end representation le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; MoT &#30340;&#26694;&#26550;&#65292;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#8220;&#24605;&#24819;&#35760;&#24518;&#8221;&#33258;&#25105;&#36827;&#21270;&#65292;&#26080;&#38656;&#27880;&#37322;&#25968;&#25454;&#38598;&#21644;&#21442;&#25968;&#26356;&#26032;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640; ChatGPT &#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#20107;&#23454;&#25512;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05181</link><description>&lt;p&gt;
MoT&#65306;&#39044;&#24605;&#32771;&#21644;&#22238;&#24518;&#21151;&#33021;&#20351; ChatGPT &#22312;&#8220;&#24605;&#24819;&#35760;&#24518;&#8221;&#20013;&#33258;&#25105;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
MoT: Pre-thinking and Recalling Enable ChatGPT to Self-Improve with Memory-of-Thoughts. (arXiv:2305.05181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; MoT &#30340;&#26694;&#26550;&#65292;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#8220;&#24605;&#24819;&#35760;&#24518;&#8221;&#33258;&#25105;&#36827;&#21270;&#65292;&#26080;&#38656;&#27880;&#37322;&#25968;&#25454;&#38598;&#21644;&#21442;&#25968;&#26356;&#26032;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640; ChatGPT &#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#20107;&#23454;&#25512;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#20294;&#35201;&#23454;&#29616;&#23427;&#20204;&#30340;&#26681;&#26412;&#24615;&#25913;&#36827;&#65292;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#25110;&#35745;&#31639;&#26114;&#36149;&#30340;&#24494;&#35843;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#24605;&#32771;&#21644;&#35760;&#24518;&#36731;&#26494;&#25552;&#39640;&#33258;&#25105;&#27700;&#24179;&#65292;&#32780;&#19981;&#38656;&#35201;&#22806;&#37096;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550; MoT&#65292;&#22312;&#27809;&#26377;&#27880;&#37322;&#25968;&#25454;&#38598;&#21644;&#21442;&#25968;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24605;&#24819;&#35760;&#24518;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#36827;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;1. &#22312;&#27979;&#35797;&#38454;&#27573;&#20043;&#21069;&#65292;&#25105;&#20204;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#21152;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#24605;&#32771;&#65292;&#24182;&#23558;&#39640;&#32622;&#20449;&#24230;&#30340;&#24819;&#27861;&#20445;&#23384;&#20026;&#22806;&#37096;&#35760;&#24518;&#12290;2. &#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#27979;&#35797;&#38382;&#39064;&#65292;&#25105;&#20204;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#24518;&#30456;&#20851;&#30340;&#35760;&#24518;&#65292;&#24110;&#21161;&#33258;&#24049;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161; ChatGPT &#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#20107;&#23454;&#25512;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27599;&#20010;&#32452;&#20214;&#37117;&#21457;&#25381;&#20102;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown impressive abilities on various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, human can easily improve themselves by thinking and memory, without external resources. In this paper, we propose a framework, MoT, to let the LLM self-improve through Memory of Thoughts, without annotated datasets and parameter updates. Specifically, the framework is divided into two stages: 1. before the test stage, we let the LLM pre-think on the unlabeled dataset and save the high-confidence thoughts as external memory; 2. during inference, given a test question, we let the LLM recall relevant memory to help itself reason and answer it. Experimental results show that the proposed framework can help ChatGPT significantly improve its abilities in math reasoning, commonsense reasoning, factual reasoning and natural language inference. Further analyses show that each component contribute
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;</title><link>http://arxiv.org/abs/2305.01498</link><description>&lt;p&gt;
&#26088;&#22312;&#24635;&#32467;&#24102;&#26377;&#23618;&#27425;&#20851;&#31995;&#30340;&#22810;&#31687;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
Towards Summarizing Multiple Documents with Hierarchical Relationships. (arXiv:2305.01498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#29616;&#23384;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#25968;&#25454;&#38598;&#32570;&#23569;&#20154;&#24037;&#29983;&#25104;&#30340;&#12289;&#30495;&#23454;&#30340;(&#21363;&#38750;&#21512;&#25104;&#30340;)&#25688;&#35201;&#25110;&#32773;&#24102;&#26377;&#26174;&#24335;&#25991;&#26723;&#38388;&#20851;&#31995;&#30340;&#28304;&#25991;&#26723;&#12290;&#20026;&#20102;&#22686;&#24378;MDS&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;PeerSum&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#20854;&#20013;&#20803;&#35780;&#35770;&#26159;&#23545;&#35780;&#35770;&#21644;&#30456;&#24212;&#35752;&#35770;&#30340;&#39640;&#24230;&#27010;&#25324;&#19988;&#30495;&#23454;&#30340;&#25688;&#35201;&#12290;&#36825;&#20123;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#21253;&#25324;&#20132;&#21449;&#24341;&#29992;&#21644;&#32463;&#24120;&#20986;&#29616;&#30340;&#20914;&#31361;&#12290;&#37492;&#20110;&#24456;&#23569;&#26377;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#25805;&#32437;&#26469;&#23558;&#23618;&#27425;&#20851;&#31995;&#32435;&#20837;MDS&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Rammer(&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#20803;&#35780;&#35770;&#29983;&#25104;&#22120;)&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#35780;&#35770;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;&#23618;&#27425;&#20851;&#31995;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#21644;&#22810;&#20219;&#21153;&#30446;&#26631;&#65292;&#21487;&#20197;&#39044;&#27979;&#22810;&#20010;&#24230;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing multi-document summarization (MDS) datasets lack human-generated and genuine (i.e., not synthetic) summaries or source documents with explicit inter-document relationships that a summary must capture. To enhance the capabilities of MDS systems we present PeerSum, a novel dataset for generating meta-reviews of scientific papers, where the meta-reviews are highly abstractive and genuine summaries of reviews and corresponding discussions. These source documents have rich inter-document relationships of an explicit hierarchical structure with cross-references and often feature conflicts. As there is a scarcity of research that incorporates hierarchical relationships into MDS systems through attention manipulation on pre-trained language models, we additionally present Rammer (Relationship-aware Multi-task Meta-review Generator), a meta-review generation model that uses sparse attention based on the hierarchical relationships and a multi-task objective that predicts several me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#31361;&#30772;&#23433;&#20840;&#31995;&#32479;&#21644;&#31363;&#21462;&#25935;&#24863;&#25968;&#25454;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#22312;&#39044;&#27979;&#21644;&#38450;&#27490;&#32593;&#32476;&#29359;&#32618;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#21040;&#20102;&#26368;&#36817;&#30740;&#31350;&#20013;&#25928;&#26524;&#26368;&#22909;&#30340;&#36882;&#24402;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.04819</link><description>&lt;p&gt;
&#32593;&#32476;&#29359;&#32618;&#39044;&#27979;&#30340;&#36827;&#23637;&#65306;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Advances in Cybercrime Prediction: A Survey of Machine, Deep, Transfer, and Adaptive Learning Techniques. (arXiv:2304.04819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#31361;&#30772;&#23433;&#20840;&#31995;&#32479;&#21644;&#31363;&#21462;&#25935;&#24863;&#25968;&#25454;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#22312;&#39044;&#27979;&#21644;&#38450;&#27490;&#32593;&#32476;&#29359;&#32618;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#21040;&#20102;&#26368;&#36817;&#30740;&#31350;&#20013;&#25928;&#26524;&#26368;&#22909;&#30340;&#36882;&#24402;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#29359;&#32618;&#26159;&#19968;&#31181;&#19981;&#26029;&#22686;&#38271;&#30340;&#23041;&#32961;&#65292;&#32618;&#29359;&#20351;&#29992;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#25216;&#26415;&#26469;&#31361;&#30772;&#23433;&#20840;&#31995;&#32479;&#24182;&#31363;&#21462;&#25935;&#24863;&#25968;&#25454;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#39044;&#27979;&#32593;&#32476;&#29359;&#32618;&#21644;&#22312;&#20854;&#21457;&#29983;&#20043;&#21069;&#38450;&#27490;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#20351;&#29992;&#19978;&#36848;&#25216;&#26415;&#39044;&#27979;&#32593;&#32476;&#29359;&#32618;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#37325;&#28857;&#20171;&#32461;&#27599;&#31181;&#26041;&#27861;&#30456;&#20851;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;150&#22810;&#31687;&#30740;&#31350;&#25991;&#31456;&#65292;&#24182;&#35752;&#35770;&#20102;&#22823;&#32422;50&#31687;&#26368;&#36817;&#21644;&#26368;&#30456;&#20851;&#30340;&#30740;&#31350;&#25991;&#31456;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#19968;&#20123;&#32593;&#32476;&#29359;&#32618;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#37325;&#28857;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20363;&#22914;&#36882;&#24402;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#21644;&#35782;&#21035;&#28508;&#22312;&#23041;&#32961;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cybercrime is a growing threat to organizations and individuals worldwide, with criminals using increasingly sophisticated techniques to breach security systems and steal sensitive data. In recent years, machine learning, deep learning, and transfer learning techniques have emerged as promising tools for predicting cybercrime and preventing it before it occurs. This paper aims to provide a comprehensive survey of the latest advancements in cybercrime prediction using above mentioned techniques, highlighting the latest research related to each approach. For this purpose, we reviewed more than 150 research articles and discussed around 50 most recent and relevant research articles. We start the review by discussing some common methods used by cyber criminals and then focus on the latest machine learning techniques and deep learning techniques, such as recurrent and convolutional neural networks, which were effective in detecting anomalous behavior and identifying potential threats. We al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;POMP&#65292;&#21487;&#20197;&#22312;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#22312;&#20869;&#30340;&#21508;&#31181;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#25552;&#21319;&#35782;&#21035;&#24615;&#33021;&#65292;&#36890;&#36807;&#21387;&#32553;&#35821;&#20041;&#20449;&#24687;&#65292;&#25903;&#25345;&#36229;&#36807;&#20004;&#19975;&#20010;&#31867;&#21035;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;POMP&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.04704</link><description>&lt;p&gt;
&#20351;&#29992;&#20004;&#19975;&#20010;&#31867;&#21035;&#36827;&#34892;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#35782;&#21035;&#30340;&#25552;&#31034;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition. (arXiv:2304.04704v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;POMP&#65292;&#21487;&#20197;&#22312;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#22312;&#20869;&#30340;&#21508;&#31181;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#25552;&#21319;&#35782;&#21035;&#24615;&#33021;&#65292;&#36890;&#36807;&#21387;&#32553;&#35821;&#20041;&#20449;&#24687;&#65292;&#25903;&#25345;&#36229;&#36807;&#20004;&#19975;&#20010;&#31867;&#21035;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;POMP&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;POMP&#65292;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;POMP&#26082;&#20855;&#26377;&#23384;&#20648;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#21448;&#33021;&#22815;&#20026;&#36229;&#36807;&#20004;&#19975;&#20010;&#31867;&#21035;&#30340;&#20016;&#23500;&#35270;&#35273;&#27010;&#24565;&#21387;&#32553;&#35821;&#20041;&#20449;&#24687;&#12290;&#19968;&#26086;&#39044;&#35757;&#32451;&#23436;&#25104;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#20256;&#36882;&#33021;&#21147;&#30340;&#25552;&#31034;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#65292;&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#25552;&#21319;&#35782;&#21035;&#24615;&#33021;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;POMP&#22312;21&#20010;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#22312;10&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;67.0%&#65288;&#27604;CoOp&#39640;&#20986;3.1%&#65289;&#65292;&#22312;&#24320;&#25918;&#35789;&#27719;&#30340;Pascal VOC&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;hIoU&#20026;84.4&#65288;&#27604;ZSSeg&#39640;&#20986;6.9&#65289;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/amazon-science/prompt-pretraining&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes POMP, a prompt pre-training method for vision-language models. Being memory and computation efficient, POMP enables the learned prompt to condense semantic information for a rich set of visual concepts with over twenty-thousand classes. Once pre-trained, the prompt with a strong transferable ability can be directly plugged into a variety of visual recognition tasks including image classification, semantic segmentation, and object detection, to boost recognition performances in a zero-shot manner. Empirical evaluation shows that POMP achieves state-of-the-art performances on 21 datasets, e.g., 67.0% average accuracy on 10 classification datasets (+3.1% compared to CoOp) and 84.4 hIoU on open-vocabulary Pascal VOC segmentation (+6.9 compared to ZSSeg). Our code is available at https://github.com/amazon-science/prompt-pretraining.
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#21487;&#35299;&#37322;&#26426;&#22120;&#20154;&#35268;&#21010;&#26041;&#27861;&#65288;SEP&#65289;&#25193;&#23637;&#20102;&#21487;&#35299;&#37322;&#35268;&#21010;&#65292;&#25903;&#25345;&#23433;&#20840;&#30028;&#38480;&#30340;&#35268;&#23450;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#21644;&#21487;&#35299;&#37322;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03773</link><description>&lt;p&gt;
&#23433;&#20840;&#21487;&#35299;&#37322;&#26426;&#22120;&#20154;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe Explicable Robot Planning. (arXiv:2304.03773v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03773
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#21487;&#35299;&#37322;&#26426;&#22120;&#20154;&#35268;&#21010;&#26041;&#27861;&#65288;SEP&#65289;&#25193;&#23637;&#20102;&#21487;&#35299;&#37322;&#35268;&#21010;&#65292;&#25903;&#25345;&#23433;&#20840;&#30028;&#38480;&#30340;&#35268;&#23450;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#21644;&#21487;&#35299;&#37322;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#30340;&#26399;&#26395;&#28304;&#33258;&#20110;&#20182;&#20204;&#23545;&#20854;&#20182;&#20154;&#21644;&#19990;&#30028;&#30340;&#20102;&#35299;&#12290;&#22312;&#28041;&#21450;&#21040;&#20154;&#26426;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#26426;&#22120;&#20154;&#30340;&#20102;&#35299;&#21487;&#33021;&#19982;&#29616;&#23454;&#19981;&#31526;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#19981;&#33021;&#28385;&#36275;&#20154;&#20204;&#30340;&#26399;&#26395;&#12290;&#21487;&#35299;&#37322;&#35268;&#21010;&#34987;&#24341;&#20837;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#20197;&#21327;&#35843;&#20154;&#31867;&#26399;&#26395;&#21644;&#26368;&#20248;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#36827;&#34892;&#26356;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#20154;&#20915;&#31574;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#37027;&#23601;&#26159;&#22312;&#21487;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#23433;&#20840;&#30340;&#21487;&#35299;&#37322;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23433;&#20840;&#21487;&#35299;&#37322;&#35268;&#21010;&#65288;SEP&#65289;&#65292;&#23427;&#25193;&#23637;&#20102;&#21487;&#35299;&#37322;&#35268;&#21010;&#65292;&#25903;&#25345;&#23433;&#20840;&#30028;&#38480;&#30340;&#35268;&#23450;&#12290; SEP&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#31181;&#31574;&#30053;&#65292;&#29983;&#25104;&#25509;&#36817;&#20110;&#20154;&#31867;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#28385;&#36275;&#23433;&#20840;&#32422;&#26463;&#30340;&#35201;&#27714;&#12290;&#36825;&#26159;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;SEP&#30340;&#35299;&#20915;&#26041;&#26696;&#20301;&#20110;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20999;&#23454;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#19981;&#29306;&#29298;&#20219;&#20309;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#20135;&#29983;&#20102;&#23433;&#20840;&#24615;&#21644;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#19968;&#20010;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human expectations stem from their knowledge of the others and the world. Where human-robot interaction is concerned, such knowledge about the robot may be inconsistent with the ground truth, resulting in the robot not meeting its expectations. Explicable planning was previously introduced as a novel planning approach to reconciling human expectations and the optimal robot behavior for more interpretable robot decision-making. One critical issue that remains unaddressed is safety during explicable decision-making which can lead to explicable behaviors that are unsafe. We propose Safe Explicable Planning (SEP), which extends explicable planning to support the specification of a safety bound. The objective of SEP is to find a policy that generates a behavior close to human expectations while satisfying the safety constraints introduced by the bound, which is a special case of multi-objective optimization where the solution to SEP lies on the Pareto frontier. Under such a formulation, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.15714</link><description>&lt;p&gt;
&#26174;&#24335;&#35268;&#21010;&#26377;&#21161;&#20110;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#26174;&#24335;&#35268;&#21010;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23637;&#26395;&#26410;&#26469;&#30340;&#25928;&#26524;&#26469;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20840;&#22871;&#31995;&#32479;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#39064;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#65292;&#23613;&#31649;&#21482;&#26377;&#32422;15&#20159;&#20010;&#21442;&#25968;&#65292;&#20294;&#19982;GPT-3-davinci&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#28040;&#34701;&#30740;&#31350;&#20197;&#35777;&#26126;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#34588;&#34562;&#31639;&#27861;&#20248;&#21270;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#39640;&#20934;&#30830;&#29575;&#22312;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;99.63%&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;88%&#12290;</title><link>http://arxiv.org/abs/2303.08021</link><description>&lt;p&gt;
&#29992;&#34588;&#34562;&#31639;&#27861;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#25552;&#39640;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimizing Deep Learning Model Parameters with the Bees Algorithm for Improved Medical Text Classification. (arXiv:2303.08021v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#34588;&#34562;&#31639;&#27861;&#20248;&#21270;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#39640;&#20934;&#30830;&#29575;&#22312;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;99.63%&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;88%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#34588;&#34562;&#31639;&#27861;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#26426;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#24456;&#26377;&#21069;&#36884;&#30340;&#32676;&#26234;&#33021;&#31639;&#27861;&#12290;&#20248;&#21270;&#38382;&#39064;&#26159;&#22312;&#32473;&#23450;&#21021;&#22987;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#30830;&#23450;&#30340;&#36845;&#20195;&#27425;&#25968;&#26469;&#26368;&#22823;&#21270;&#22522;&#20110;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#30142;&#30149;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65306;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#12290;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518; (LSTM) &#21644;&#34588;&#34562;&#31639;&#27861;&#65292;&#22312;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;99.63%&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;AraBERT&#33719;&#24471;&#20102;88%&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel mechanism to obtain the optimal parameters of a deep learning model using the Bees Algorithm, which is a recent promising swarm intelligence algorithm. The optimization problem is to maximize the accuracy of classifying ailments based on medical text given the initial hyper-parameters to be adjusted throughout a definite number of iterations. Experiments included two different datasets: English and Arabic. The highest accuracy achieved is 99.63% on the English dataset using Long Short-Term Memory (LSTM) along with the Bees Algorithm, and 88% on the Arabic dataset using AraBERT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20219;&#21153;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08010</link><description>&lt;p&gt;
&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#32423;&#32852;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65306;&#24403;&#28145;&#24230;&#38598;&#25104;&#27604;&#21333;&#19968;&#27169;&#22411;&#26356;&#26377;&#25928;&#26102;
&lt;/p&gt;
&lt;p&gt;
Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models. (arXiv:2303.08010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20219;&#21153;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#31616;&#21333;&#12289;&#21487;&#38752;&#21644;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#37096;&#32626;&#22810;&#20010;&#29420;&#31435;&#27169;&#22411;&#65292;&#23427;&#20204;&#34987;&#24191;&#27867;&#25209;&#35780;&#20026;&#35745;&#31639;&#24320;&#38144;&#22823;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25361;&#25112;&#20102;&#36825;&#31181;&#35266;&#28857;&#65292;&#34920;&#26126;&#23545;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#38598;&#25104;&#21487;&#20197;&#27604;&#22312;&#21516;&#19968;&#26550;&#26500;&#26063;&#20013;&#32553;&#25918;&#21333;&#19968;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#36890;&#36807;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#32423;&#32852;&#38598;&#25104;&#25104;&#21592;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#23558;&#36825;&#20123;&#25928;&#29575;&#25552;&#39640;&#25193;&#23637;&#21040;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#35768;&#22810;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#37117;&#26159;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#26032;&#39062;&#35265;&#35299;&#26159;&#20165;&#23558;&#25509;&#36817;&#20108;&#20998;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#20256;&#36882;&#21040;&#21518;&#32493;&#32423;&#32852;&#38454;&#27573;&#12290;&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#22312;&#20351;&#29992;&#27604;&#22522;&#32447;&#26356;&#23569;&#30340;&#27169;&#22411;&#35780;&#20272;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#19982;&#23436;&#25972;&#38598;&#25104;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Ensembles are a simple, reliable, and effective method of improving both the predictive performance and uncertainty estimates of deep learning approaches. However, they are widely criticised as being computationally expensive, due to the need to deploy multiple independent models. Recent work has challenged this view, showing that for predictive accuracy, ensembles can be more computationally efficient (at inference) than scaling single models within an architecture family. This is achieved by cascading ensemble members via an early-exit approach. In this work, we investigate extending these efficiency gains to tasks related to uncertainty estimation. As many such tasks, e.g. selective classification, are binary classification, our key novel insight is to only pass samples within a window close to the binary decision boundary to later cascade stages. Experiments on ImageNet-scale data across a number of network architectures and uncertainty tasks show that the proposed window-base
&lt;/p&gt;</description></item><item><title>SLCA&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24930;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#20915;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.05118</link><description>&lt;p&gt;
SLCA: &#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#24930;&#23398;&#20064;&#32773;&#19982;&#20998;&#31867;&#22120;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05118
&lt;/p&gt;
&lt;p&gt;
SLCA&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24930;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#20915;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#22312;&#23398;&#20064;&#39034;&#24207;&#21040;&#36798;&#30340;&#25968;&#25454;&#20013;&#25552;&#39640;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#29616;&#26377;&#24037;&#20316;&#37117;&#24314;&#31435;&#22312;&#20174;&#22836;&#23398;&#20064;&#30340;&#21069;&#25552;&#19979;&#65292;&#20294;&#36234;&#26469;&#36234;&#22810;&#30340;&#21162;&#21147;&#24050;&#32463;&#33268;&#21147;&#20110;&#34701;&#20837;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#27599;&#20010;&#22686;&#37327;&#20219;&#21153;&#20013;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#30340;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#24182;&#23558;&#20851;&#38190;&#25361;&#25112;&#24402;&#22240;&#20110;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#35266;&#23519;&#21040;&#22312;&#34920;&#24449;&#23618;&#27425;&#19978;&#36873;&#25321;&#24615;&#38477;&#20302;&#23398;&#20064;&#29575;&#20960;&#20046;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#24930;&#23398;&#20064;&#32773;&#19982;&#20998;&#31867;&#22120;&#23545;&#40784;&#65288;SLCA&#65289;&#65292;&#36890;&#36807;&#24314;&#27169;&#31867;&#21035;&#20998;&#24067;&#24182;&#22312;&#20107;&#21518;&#23545;&#40784;&#20998;&#31867;&#23618;&#27425;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20998;&#31867;&#23618;&#27425;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SLCA&#22312;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of continual learning is to improve the performance of recognition models in learning sequentially arrived data. Although most existing works are established on the premise of learning from scratch, growing efforts have been devoted to incorporating the benefits of pre-training. However, how to adaptively exploit the pre-trained knowledge for each incremental task while maintaining its generalizability remains an open question. In this work, we present an extensive analysis for continual learning on a pre-trained model (CLPM), and attribute the key challenge to a progressive overfitting problem. Observing that selectively reducing the learning rate can almost resolve this issue in the representation layer, we propose a simple but extremely effective approach named Slow Learner with Classifier Alignment (SLCA), which further improves the classification layer by modeling the class-wise distributions and aligning the classification layers in a post-hoc fashion. Across a variety o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LSTM&#32534;&#30721;&#22120;&#8212;&#35299;&#30721;&#22120;&#20849;&#21516;&#23398;&#20064;&#35266;&#27979;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#20174;&#27491;&#24120;&#26679;&#26412;&#20013;&#20272;&#35745;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21463;&#21040;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#21487;&#20197;&#29992;&#39532;&#27663;&#36317;&#31163;&#35780;&#20272;&#24322;&#24120;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2303.03324</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection with reconstruction-based state-space models. (arXiv:2303.03324v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LSTM&#32534;&#30721;&#22120;&#8212;&#35299;&#30721;&#22120;&#20849;&#21516;&#23398;&#20064;&#35266;&#27979;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#20174;&#27491;&#24120;&#26679;&#26412;&#20013;&#20272;&#35745;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21463;&#21040;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#21487;&#20197;&#29992;&#39532;&#27663;&#36317;&#31163;&#35780;&#20272;&#24322;&#24120;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#23548;&#33268;&#21508;&#31181;&#39046;&#22495;&#20013;&#20986;&#29616;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20351;&#24471;&#23454;&#26102;&#30417;&#27979;&#36816;&#33829;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#24322;&#24120;&#25968;&#25454;&#27169;&#24335;&#21644;&#26816;&#27979;&#28508;&#22312;&#25925;&#38556;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#20294;&#20063;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20849;&#21516;&#23398;&#20064;&#35266;&#27979;&#27169;&#22411;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#20174;&#27491;&#24120;&#26679;&#26412;&#20013;&#20272;&#35745;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#30340;&#65292;&#37319;&#29992;&#22522;&#20110;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#30340;&#32534;&#30721;&#22120;&#8212;&#35299;&#30721;&#22120;&#34920;&#31034;&#35266;&#27979;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;, &#34701;&#21512;&#20102;&#21521;&#21518;&#21644;&#21521;&#21069;&#30340;&#26102;&#38388;&#20449;&#24687;&#20197;&#21516;&#26102;&#24314;&#27169;&#29366;&#24577;&#30340;&#21452;&#21521;&#36716;&#25442;&#12290;&#28508;&#22312;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#32422;&#26463;&#20102;&#27491;&#24120;&#26679;&#26412;&#30340;&#29366;&#24577;&#65292;&#24182;&#20351;&#29992;&#39532;&#27663;&#36317;&#31163;&#35780;&#20272;&#24322;&#24120;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in digitization have led to the availability of multivariate time series data in various domains, enabling real-time monitoring of operations. Identifying abnormal data patterns and detecting potential failures in these scenarios are important yet rather challenging. In this work, we propose a novel unsupervised anomaly detection method for time series data. The proposed framework jointly learns the observation model and the dynamic model, and model uncertainty is estimated from normal samples. Specifically, a long short-term memory (LSTM)-based encoder-decoder is adopted to represent the mapping between the observation space and the latent space. Bidirectional transitions of states are simultaneously modeled by leveraging backward and forward temporal information. Regularization of the latent space places constraints on the states of normal samples, and Mahalanobis distance is used to evaluate the abnormality level. Empirical studies on synthetic and real-world dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;InfoSeek&#12290;&#20351;&#29992;InfoSeek&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2302.11713</link><description>&lt;p&gt;
Pre-trained Vision and Language Models&#33021;&#21542;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;InfoSeek&#12290;&#20351;&#29992;InfoSeek&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models&#22312;&#28041;&#21450;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#39046;&#20808;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#35270;&#35273;&#38382;&#31572;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20855;&#22791;&#22238;&#31572;&#19981;&#20165;&#20165;&#26597;&#35810;&#35270;&#35273;&#20869;&#23481;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#30693;&#35782;&#23494;&#38598;&#21644;&#20449;&#24687;&#23547;&#27714;&#24615;&#36136;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;InfoSeek&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;InfoSeek&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#24182;&#28145;&#20837;&#20102;&#35299;&#23427;&#20204;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;PaLI-X&#65292;BLIP2&#31561;&#65289;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;InfoSeek&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#22815;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#20182;&#20204;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#21040;&#30340;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20934;&#30830;&#30340;&#35270;&#35273;&#23454;&#20307;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models have demonstrated state-of-the-art capabilities over existing tasks involving images and texts, including visual question answering. However, it remains unclear whether these models possess the capability to answer questions that are not only querying visual content but knowledge-intensive and information-seeking. In this study, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training. Furthermore, we show that accurate visual entit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#24120;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#65292;&#25506;&#31350;&#19981;&#21516;&#35780;&#20272;&#24230;&#37327;&#19979;&#30340;&#34920;&#29616;&#20197;&#21450;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#21457;&#29616;&#26041;&#27861;&#30340;&#34920;&#29616;&#32463;&#24120;&#19981;&#19968;&#33268;&#19988;&#36873;&#25321;&#35780;&#20272;&#24230;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2302.10764</link><description>&lt;p&gt;
&#20851;&#20110;&#35270;&#35273;&#35299;&#37322;&#23450;&#37327;&#35780;&#20272;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On The Coherence of Quantitative Evaluation of Visual Explanations. (arXiv:2302.10764v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#24120;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#65292;&#25506;&#31350;&#19981;&#21516;&#35780;&#20272;&#24230;&#37327;&#19979;&#30340;&#34920;&#29616;&#20197;&#21450;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#21457;&#29616;&#26041;&#27861;&#30340;&#34920;&#29616;&#32463;&#24120;&#19981;&#19968;&#33268;&#19988;&#36873;&#25321;&#35780;&#20272;&#24230;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#35270;&#35273;&#35299;&#37322;&#26469;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#22686;&#24378;&#21457;&#23637;&#12290;&#36825;&#20123;&#35299;&#37322;&#36890;&#24120;&#37319;&#29992;&#28909;&#22270;&#30340;&#24418;&#24335;&#65292;&#20026;&#36755;&#20837;&#22270;&#20687;&#30340;&#27599;&#20010;&#20687;&#32032;&#20998;&#37197;&#19968;&#20010;&#26174;&#33879;&#24615;&#20540;&#65292;&#34920;&#31034;&#20687;&#32032;&#23545;&#26631;&#31614;&#39044;&#27979;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#12290;&#19968;&#20123;&#36825;&#26679;&#30340;&#35780;&#20272;&#26041;&#27861;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20294;&#36825;&#26679;&#20250;&#24341;&#20837;&#22312;&#26356;&#29616;&#23454;&#30340;&#24773;&#26223;&#19979;&#36866;&#29992;&#24615;&#30340;&#26377;&#38480;&#20445;&#35777;&#12290;&#21478;&#19968;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#23458;&#35266;&#35780;&#20272;&#30340;&#24230;&#37327;&#12290;&#20294;&#26159;&#26377;&#20851;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#30340;&#25191;&#34892;&#27700;&#24179;&#30340;&#19981;&#30830;&#23450;&#24615;&#24456;&#22823;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;ImageNet-1k&#39564;&#35777;&#38598;&#30340;&#19968;&#20010;&#23376;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#20351;&#29992;&#22810;&#20010;&#35780;&#20272;&#24230;&#37327;&#26469;&#35780;&#20272;&#19981;&#21516;&#30340;&#24120;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#19981;&#21516;&#30340;&#35780;&#20272;&#35774;&#32622;&#19979;&#21508;&#20010;&#26041;&#27861;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#22914;&#20309;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25152;&#20351;&#29992;&#30340;&#35780;&#20272;&#24230;&#37327;&#19978;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#34920;&#29616;&#32463;&#24120;&#26159;&#19981;&#19968;&#33268;&#30340;&#65292;&#32780;&#19988;&#22312;&#35266;&#23519;&#30340;&#34920;&#29616;&#20013;&#65292;&#36873;&#25321;&#35780;&#20272;&#24230;&#37327;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have shown an increased development of methods for justifying the predictions of neural networks through visual explanations. These explanations usually take the form of heatmaps which assign a saliency (or relevance) value to each pixel of the input image that expresses how relevant the pixel is for the prediction of a label.  Complementing this development, evaluation methods have been proposed to assess the "goodness" of such explanations. On the one hand, some of these methods rely on synthetic datasets. However, this introduces the weakness of having limited guarantees regarding their applicability on more realistic settings. On the other hand, some methods rely on metrics for objective evaluation. However the level to which some of these evaluation methods perform with respect to each other is uncertain.  Taking this into account, we conduct a comprehensive study on a subset of the ImageNet-1k validation set where we evaluate a number of different commonly-used expla
&lt;/p&gt;</description></item><item><title>RETVec&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#24377;&#24615;&#21644;&#22810;&#35821;&#35328;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#22120;&#65292;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#30340;&#23383;&#31526;&#32534;&#30721;&#21644;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26356;&#22909;&#36866;&#24212;&#24615;&#12290;&#19982;&#20854;&#20182;&#21521;&#37327;&#21270;&#22120;&#21644;&#35789;&#23884;&#20837;&#27169;&#22411;&#30456;&#27604;&#65292;RETVec&#22312;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#21644;&#26174;&#33879;&#30340;&#24377;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.09207</link><description>&lt;p&gt;
RETVec&#65306;&#24377;&#24615;&#21644;&#39640;&#25928;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
RETVec: Resilient and Efficient Text Vectorizer. (arXiv:2302.09207v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09207
&lt;/p&gt;
&lt;p&gt;
RETVec&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#24377;&#24615;&#21644;&#22810;&#35821;&#35328;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#22120;&#65292;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#30340;&#23383;&#31526;&#32534;&#30721;&#21644;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26356;&#22909;&#36866;&#24212;&#24615;&#12290;&#19982;&#20854;&#20182;&#21521;&#37327;&#21270;&#22120;&#21644;&#35789;&#23884;&#20837;&#27169;&#22411;&#30456;&#27604;&#65292;RETVec&#22312;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#21644;&#26174;&#33879;&#30340;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RETVec&#65292;&#19968;&#31181;&#19987;&#20026;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#22788;&#29702;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#12289;&#24377;&#24615;&#21644;&#22810;&#35821;&#35328;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#22120;&#12290;RETVec&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23383;&#31526;&#32534;&#30721;&#21644;&#21487;&#36873;&#30340;&#23567;&#22411;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#35789;&#35821;&#23884;&#20837;&#21040;256&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;RETVec&#30340;&#23884;&#20837;&#27169;&#22411;&#20351;&#29992;&#23545;&#27604;&#24230;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#38024;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23383;&#31526;&#32423;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;RETVec&#22312;&#27969;&#34892;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#36825;&#20123;&#27604;&#36739;&#34920;&#26126;&#65292;RETVec&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23545;&#25239;&#24615;&#25991;&#26412;&#25915;&#20987;&#20855;&#26377;&#26174;&#33879;&#30340;&#24377;&#24615;&#12290;RETVec&#22312;Apache 2&#35768;&#21487;&#19979;&#21487;&#22312;https://github.com/google-research/retvec&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes RETVec, an efficient, resilient, and multilingual text vectorizer designed for neural-based text processing. RETVec combines a novel character encoding with an optional small embedding model to embed words into a 256-dimensional vector space. The RETVec embedding model is pre-trained using pair-wise metric learning to be robust against typos and character-level adversarial attacks. In this paper, we evaluate and compare RETVec to state-of-the-art vectorizers and word embeddings on popular model architectures and datasets. These comparisons demonstrate that RETVec leads to competitive, multilingual models that are significantly more resilient to typos and adversarial text attacks. RETVec is available under the Apache 2 license at https://github.com/google-research/retvec.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;Penguin&#65292;&#26088;&#22312;&#20419;&#36827;&#20013;&#25991;&#38405;&#35835;&#29702;&#35299;&#20013;&#33258;&#28982;&#21709;&#24212;&#29983;&#25104;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#23545;&#36739;&#22823;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#24179;&#21488;&#12290;&#36890;&#36807;&#24320;&#21457;&#20004;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;Penguin&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.08817</link><description>&lt;p&gt;
&#20013;&#25991;&#38405;&#35835;&#29702;&#35299;&#30340;&#33258;&#28982;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Natural Response Generation for Chinese Reading Comprehension. (arXiv:2302.08817v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;Penguin&#65292;&#26088;&#22312;&#20419;&#36827;&#20013;&#25991;&#38405;&#35835;&#29702;&#35299;&#20013;&#33258;&#28982;&#21709;&#24212;&#29983;&#25104;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#23545;&#36739;&#22823;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#24179;&#21488;&#12290;&#36890;&#36807;&#24320;&#21457;&#20004;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;Penguin&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;(MRC)&#26159;&#23545;&#35805;&#20195;&#29702;&#30340;&#37325;&#35201;&#39046;&#22495;&#65292;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;MRC&#22522;&#20934;&#30340;&#19968;&#20010;&#26126;&#26174;&#38480;&#21046;&#26159;&#65306;&#26631;&#35760;&#30340;&#31572;&#26696;&#22823;&#22810;&#25968;&#26159;&#20174;&#30446;&#26631;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#30340;&#29255;&#27573;&#25110;&#32473;&#23450;&#20505;&#36873;&#39033;&#30340;&#36873;&#25321;&#65292;&#24573;&#30053;&#20102;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#33258;&#28982;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;MRC&#27169;&#22411;&#26080;&#27861;&#22312;&#30495;&#23454;&#30340;&#38382;&#31572;&#22330;&#26223;&#20013;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#21709;&#24212;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Penguin&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;MRC&#30740;&#31350;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#25552;&#20379;&#33258;&#28982;&#21709;&#24212;&#29983;&#25104;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#22522;&#30784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Penguin&#21253;&#21547;20&#19975;&#20010;&#35757;&#32451;&#25968;&#25454;&#65292;&#20855;&#22791;&#39640;&#36136;&#37327;&#12289;&#27969;&#30021;&#12289;&#20805;&#20998;&#20449;&#24687;&#30340;&#21709;&#24212;&#12290;Penguin&#26159;&#30456;&#23545;&#35268;&#27169;&#36739;&#22823;&#30340;&#20013;&#25991;MRC&#39046;&#22495;&#33258;&#28982;&#21709;&#24212;&#29983;&#25104;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;Penguin&#20013;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#27169;&#22411;&#65306;&#31471;&#21040;&#31471;&#21644;&#20004;&#38454;&#27573;&#26694;&#26550;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20986;Prompt-BART&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine reading comprehension (MRC) is an important area of conversation agents and draws a lot of attention. However, there is a notable limitation to current MRC benchmarks: The labeled answers are mostly either spans extracted from the target corpus or the choices of the given candidates, ignoring the natural aspect of high-quality responses. As a result, MRC models trained on these datasets can not generate human-like responses in real QA scenarios. To this end, we construct a new dataset called Penguin to promote the research of MRC, providing a training and test bed for natural response generation to real scenarios. Concretely, Penguin consists of 200k training data with high-quality fluent, and well-informed responses. Penguin is the first benchmark towards natural response generation in Chinese MRC on a relatively large scale. To address the challenges in Penguin, we develop two strong baselines: end-to-end and two-stage frameworks. Following that, we further design Prompt-BART
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.04823</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#29983;&#25104;&#23545;&#25239;&#27169;&#25311;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments. (arXiv:2302.04823v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#23454;&#20013;&#30340;&#22478;&#24066;&#23548;&#33322;&#22330;&#26223;&#65292;&#35774;&#35745;&#20581;&#22766;&#30340;&#25511;&#21046;&#31574;&#30053;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#20013;&#65292;&#36825;&#20123;&#31574;&#30053;&#24517;&#39035;&#23558;&#36710;&#36742;&#25668;&#20687;&#22836;&#33719;&#24471;&#30340;&#39640;&#32500;&#22270;&#20687;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#65292;&#22914;&#36716;&#21521;&#21644;&#27833;&#38376;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deriving robust control policies for realistic urban navigation scenarios is not a trivial task. In an end-to-end approach, these policies must map high-dimensional images from the vehicle's cameras to low-level actions such as steering and throttle. While pure Reinforcement Learning (RL) approaches are based exclusively on rewards,Generative Adversarial Imitation Learning (GAIL) agents learn from expert demonstrations while interacting with the environment, which favors GAIL on tasks for which a reward signal is difficult to derive. In this work, the hGAIL architecture was proposed to solve the autonomous navigation of a vehicle in an end-to-end approach, mapping sensory perceptions directly to low-level actions, while simultaneously learning mid-level input representations of the agent's environment. The proposed hGAIL consists of an hierarchical Adversarial Imitation Learning architecture composed of two main modules: the GAN (Generative Adversarial Nets) which generates the Bird's-
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25968;&#25454;&#23548;&#21521;&#26550;&#26500;&#65288;DOA&#65289;&#30340;&#37319;&#29992;&#24773;&#20917;&#65292;&#21457;&#29616;&#23613;&#31649;&#27809;&#26377;&#26126;&#30830;&#25552;&#21450;DOA&#65292;&#20294;&#35768;&#22810;&#35770;&#25991;&#20013;&#30340;&#35774;&#35745;&#20915;&#31574;&#40664;&#40664;&#22320;&#36981;&#24490;&#20102;DOA&#30340;&#21407;&#21017;&#12290;</title><link>http://arxiv.org/abs/2302.04810</link><description>&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65306;&#22522;&#20110;&#25968;&#25454;&#23548;&#21521;&#26550;&#26500;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Real-world Machine Learning Systems: A survey from a Data-Oriented Architecture Perspective. (arXiv:2302.04810v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04810
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25968;&#25454;&#23548;&#21521;&#26550;&#26500;&#65288;DOA&#65289;&#30340;&#37319;&#29992;&#24773;&#20917;&#65292;&#21457;&#29616;&#23613;&#31649;&#27809;&#26377;&#26126;&#30830;&#25552;&#21450;DOA&#65292;&#20294;&#35768;&#22810;&#35770;&#25991;&#20013;&#30340;&#35774;&#35745;&#20915;&#31574;&#40664;&#40664;&#22320;&#36981;&#24490;&#20102;DOA&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27491;&#22312;&#20316;&#20026;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#37096;&#32626;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#32500;&#25252;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#29615;&#22659;&#20135;&#29983;&#20102;&#26356;&#22810;&#30340;&#24322;&#26500;&#25968;&#25454;&#65292;&#29992;&#25143;&#38656;&#35201;&#26356;&#24555;&#30340;&#21709;&#24212;&#36895;&#24230;&#21644;&#39640;&#25928;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36825;&#20123;&#35201;&#27714;&#23558;&#26222;&#36941;&#23384;&#22312;&#30340;&#36719;&#20214;&#26550;&#26500;&#25512;&#21521;&#20102;&#26497;&#38480;&#65292;&#24403;&#37096;&#32626;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#26102;&#12290;&#25968;&#25454;&#23548;&#21521;&#26550;&#26500;&#65288;DOA&#65289;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#27010;&#24565;&#65292;&#23427;&#33021;&#26356;&#22909;&#22320;&#20026;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31995;&#32479;&#25552;&#20379;&#25903;&#25345;&#12290;DOA&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#26550;&#26500;&#65292;&#21019;&#24314;&#20102;&#25968;&#25454;&#39537;&#21160;&#12289;&#26494;&#32806;&#21512;&#12289;&#21435;&#20013;&#24515;&#21270;&#21644;&#24320;&#25918;&#30340;&#31995;&#32479;&#12290;&#23613;&#31649;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#35770;&#25991;&#20013;&#27809;&#26377;&#25552;&#21040;DOA&#65292;&#20294;&#23427;&#20204;&#30340;&#20316;&#32773;&#22312;&#35774;&#35745;&#19978;&#38544;&#21547;&#22320;&#36981;&#24490;&#20102;DOA&#12290;&#20026;&#20160;&#20040;&#12289;&#22914;&#20309;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#37319;&#29992;DOA&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#23578;&#19981;&#28165;&#26970;&#12290;&#38544;&#21547;&#30340;&#35774;&#35745;&#20915;&#31574;&#38480;&#21046;&#20102;&#20174;&#19994;&#32773;&#23545;&#20110;&#35774;&#35745;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#26102;DOA&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning models are being deployed as parts of real-world systems with the upsurge of interest in artificial intelligence. The design, implementation, and maintenance of such systems are challenged by real-world environments that produce larger amounts of heterogeneous data and users requiring increasingly faster responses with efficient resource consumption. These requirements push prevalent software architectures to the limit when deploying ML-based systems. Data-oriented Architecture (DOA) is an emerging concept that equips systems better for integrating ML models. DOA extends current architectures to create data-driven, loosely coupled, decentralised, open systems. Even though papers on deployed ML-based systems do not mention DOA, their authors made design decisions that implicitly follow DOA. The reasons why, how, and the extent to which DOA is adopted in these systems are unclear. Implicit design decisions limit the practitioners' knowledge of DOA to design ML-based syst
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.04054</link><description>&lt;p&gt;
&#36861;&#27714;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#25512;&#29702;&#22797;&#29616;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#8212;&#8212;&#21363;&#22312;&#22797;&#21046;&#30340;&#27169;&#22411;&#35757;&#32451;&#36816;&#34892;&#20013;&#35266;&#23519;&#21040;&#30340;&#35780;&#20272;&#20998;&#25968;&#30340;&#19968;&#33268;&#24615;&#8212;&#8212;&#21463;&#21040;&#20960;&#31181;&#38750;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#27979;&#37327;&#22122;&#22768;&#12290;&#30446;&#21069;&#30340;&#36235;&#21183;&#26159;&#21435;&#38500;&#22122;&#22768;&#65292;&#20197;&#24378;&#21046;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#24573;&#30053;&#20102;&#23454;&#29616;&#23618;&#38754;&#22266;&#26377;&#30340;&#38750;&#30830;&#23450;&#24615;&#20197;&#21450;&#31639;&#27861;&#22122;&#22768;&#22240;&#32032;&#21644;&#25968;&#25454;&#29305;&#24615;&#20043;&#38388;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;&#36825;&#38480;&#21046;&#20102;&#20174;&#36825;&#20123;&#23454;&#39564;&#20013;&#21487;&#20197;&#24471;&#20986;&#30340;&#32467;&#35770;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23558;&#20960;&#20010;&#26041;&#24046;&#26469;&#28304;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982;&#25968;&#25454;&#29305;&#24615;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#26174;&#33879;&#24615;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#20013;&#65292;&#20197;&#26399;&#20174;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#23450;&#23454;&#20363;&#24471;&#20986;&#25512;&#29702;&#32467;&#35770;, &#32780;&#38750;&#21435;&#38500;&#22122;&#22768;&#12290;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#29992;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#24335;&#26469;&#32771;&#34385;&#31639;&#27861;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#28304;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#21508;&#20010;&#26041;&#24046;&#26469;&#28304;&#23545;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability of machine learning evaluation -- the consistency of observed evaluation scores across replicated model training runs -- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized lik
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#22522;&#20110;&#20559;&#22909;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#21644;&#35774;&#35745;&#26032;&#30340;&#31574;&#30053;&#35780;&#20998;&#25351;&#26631;&#65292;&#33021;&#22815;&#20174;&#32473;&#23450;&#30340;&#20559;&#22909;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12842</link><description>&lt;p&gt;
&#19981;&#20381;&#36182;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#22522;&#20110;&#20559;&#22909;&#30340;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Preference-based Policy Optimization without Reward Modeling. (arXiv:2301.12842v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#22522;&#20110;&#20559;&#22909;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#21644;&#35774;&#35745;&#26032;&#30340;&#31574;&#30053;&#35780;&#20998;&#25351;&#26631;&#65292;&#33021;&#22815;&#20174;&#32473;&#23450;&#30340;&#20559;&#22909;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;(PbRL)&#26159;&#19968;&#31181;&#20351;RL&#20195;&#29702;&#33021;&#22815;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22312;&#21046;&#23450;&#22870;&#21169;&#20989;&#25968;&#26102;&#23384;&#22312;&#25361;&#25112;&#30340;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;PbRL&#26041;&#27861;&#19968;&#33324;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#39318;&#20808;&#26681;&#25454;&#32473;&#23450;&#30340;&#20559;&#22909;&#25968;&#25454;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#27169;&#22411;&#37319;&#29992;&#29616;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20165;&#36890;&#36807;&#20559;&#22909;&#20449;&#24687;&#33719;&#21462;&#20934;&#30830;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22312;&#20559;&#22909;&#26469;&#33258;&#20154;&#31867;&#25945;&#24072;&#26102;&#65292;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#22870;&#21169;&#27169;&#22411;&#30340;&#30452;&#25509;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;PbRL&#31639;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#35780;&#20998;&#25351;&#26631;&#65292;&#20026;&#19982;&#32473;&#23450;&#20559;&#22909;&#19968;&#33268;&#30340;&#31574;&#30053;&#20998;&#37197;&#39640;&#20998;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#24102;&#26377;&#23454;&#38469;&#20154;&#31867;&#20559;&#22909;&#26631;&#31614;&#30340;&#31163;&#32447;RL&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#25110;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference-based reinforcement learning (PbRL) is an approach that enables RL agents to learn from preference, which is particularly useful when formulating a reward function is challenging. Existing PbRL methods generally involve a two-step procedure: they first learn a reward model based on given preference data and then employ off-the-shelf reinforcement learning algorithms using the learned reward model. However, obtaining an accurate reward model solely from preference information, especially when the preference is from human teachers, can be difficult. Instead, we propose a PbRL algorithm that directly learns from preference without requiring any reward modeling. To achieve this, we adopt a contrastive learning framework to design a novel policy scoring metric that assigns a high score to policies that align with the given preferences. We apply our algorithm to offline RL tasks with actual human preference labels and show that our algorithm outperforms or is on par with the exist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31526;&#21495;&#38899;&#20048;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#24207;&#21015;&#38271;&#24230;&#24182;&#22686;&#21152;&#35789;&#27719;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11975</link><description>&lt;p&gt;
&#31526;&#21495;&#38899;&#20048;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Byte Pair Encoding for Symbolic Music. (arXiv:2301.11975v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31526;&#21495;&#38899;&#20048;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#24207;&#21015;&#38271;&#24230;&#24182;&#22686;&#21152;&#35789;&#27719;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19982;&#28145;&#24230;&#23398;&#20064;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#31526;&#21495;&#38899;&#20048;&#36890;&#24120;&#19982;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#30456;&#32467;&#21512;&#12290;&#20026;&#27492;&#65292;&#38899;&#20048;&#38656;&#35201;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#21363;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#31163;&#25955;&#30340;&#26631;&#35760;&#12290;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#22240;&#20026;&#38899;&#20048;&#21487;&#20197;&#30001;&#21516;&#26102;&#23384;&#22312;&#30340;&#36712;&#36947;&#65292;&#20855;&#26377;&#22810;&#20010;&#23646;&#24615;&#30340;&#21516;&#26102;&#38899;&#31526;&#32452;&#25104;&#12290;&#30446;&#21069;&#65292;&#25152;&#25552;&#20986;&#30340;&#26631;&#35760;&#21270;&#20381;&#36182;&#20110;&#25551;&#36848;&#38899;&#31526;&#23646;&#24615;&#21644;&#26102;&#38388;&#20107;&#20214;&#30340;&#23567;&#22411;&#26631;&#35760;&#23383;&#20856;&#65292;&#23548;&#33268;&#26631;&#35760;&#24207;&#21015;&#30456;&#24403;&#38271;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#30340;&#20351;&#29992;&#19981;&#22815;&#20248;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#21512;&#24182;&#23884;&#20837;&#25110;&#32452;&#21512;&#26631;&#35760;&#26469;&#20943;&#23569;&#25972;&#20307;&#24207;&#21015;&#38271;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23383;&#33410;&#23545;&#32534;&#30721;&#65292;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#20854;&#26174;&#33879;&#20943;&#23567;&#20102;&#24207;&#21015;&#38271;&#24230;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#35789;&#27719;&#37327;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#23884;&#20837;&#33021;&#21147;&#19982;&#26356;&#26377;&#34920;&#29616;&#21147;&#30340;&#26631;&#35760;&#32467;&#21512;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
When used with deep learning, the symbolic music modality is often coupled with language model architectures. To do so, the music needs to be tokenized, i.e. converted into a sequence of discrete tokens. This can be achieved by different approaches, as music can be composed of simultaneous tracks, of simultaneous notes with several attributes. Until now, the proposed tokenizations rely on small vocabularies of tokens describing the note attributes and time events, resulting in fairly long token sequences, and a sub-optimal use of the embedding space of language models. Recent research has put efforts on reducing the overall sequence length by merging embeddings or combining tokens. In this paper, we show that Byte Pair Encoding, a compression technique widely used for natural language, significantly decreases the sequence length while increasing the vocabulary size. By doing so, we leverage the embedding capabilities of such models with more expressive tokens, resulting in both better 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#26465;&#20214;&#21477;&#24448;&#24448;&#23384;&#22312;&#30528;&#19981;&#19968;&#33268;&#24615;&#65292;&#26080;&#27861;&#20174;&#19968;&#20010;&#36830;&#36143;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#25512;&#23548;&#20986;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21457;&#29616;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#21477;&#38598;&#21512;&#26041;&#27861;&#26469;&#22312;&#25512;&#26029;&#38454;&#27573;&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.00068</link><description>&lt;p&gt;
&#20851;&#20110;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#26465;&#20214;&#21477;&#30340;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Inconsistencies of Conditionals Learned by Masked Language Models. (arXiv:2301.00068v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#26465;&#20214;&#21477;&#24448;&#24448;&#23384;&#22312;&#30528;&#19981;&#19968;&#33268;&#24615;&#65292;&#26080;&#27861;&#20174;&#19968;&#20010;&#36830;&#36143;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#25512;&#23548;&#20986;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21457;&#29616;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#21477;&#38598;&#21512;&#26041;&#27861;&#26469;&#22312;&#25512;&#26029;&#38454;&#27573;&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#20102;&#22312;&#24207;&#21015;&#20013;&#23398;&#20064;&#39044;&#27979;&#36974;&#34109;&#26631;&#35760;&#26159;&#19968;&#20010;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#24456;&#26377;&#21147;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#35757;&#32451;&#21518;&#65292;&#36825;&#20123;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#21452;&#21521;&#19978;&#19979;&#25991;&#30340;&#26631;&#35760;&#20998;&#24067;&#12290;&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19982;&#24120;&#35265;&#20551;&#35774;&#30456;&#21453;&#65292;&#36825;&#31181;&#21452;&#21521;&#26465;&#20214;&#21477;&#32463;&#24120;&#34920;&#29616;&#20986;&#30456;&#24403;&#22823;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#22312;&#32771;&#34385;&#22312;&#19968;&#36215;&#26102;&#19981;&#33021;&#20174;&#19968;&#20010;&#36830;&#36143;&#30340;&#32852;&#21512;&#20998;&#24067;&#23548;&#20986;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#20004;&#31181;&#24120;&#35265;&#39118;&#26684;&#65288;T5&#39118;&#26684;&#21644;BERT&#39118;&#26684;&#65289;&#30340;&#31616;&#21333;&#21452;&#23383;&#27597;&#35789;&#27604;&#36739;&#22330;&#26223;&#20013;&#36890;&#36807;&#23454;&#35777;&#37327;&#21270;&#20102;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;T5&#27169;&#22411;&#32463;&#24120;&#28151;&#28102;&#33258;&#24049;&#23545;&#20004;&#20010;&#30456;&#20284;&#21452;&#23383;&#27597;&#35789;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19981;&#19968;&#33268;&#24615;&#22312;&#19981;&#21516;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20174;RoBERTa-base&#21040;GLM-130B&#12290;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#21021;&#22987;&#23581;&#35797;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#21477;&#38598;&#21512;&#65292;&#22312;&#25512;&#26029;&#38454;&#27573;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to predict masked tokens in a sequence has been shown to be a powerful pretraining objective for large language models. After training, such masked language models can provide distributions of tokens conditioned on bidirectional context.  In this paper, we show that contrary to popular assumptions, such bidirectional conditionals often demonstrate considerable inconsistencies, i.e., they cannot be derived from a coherent joint distribution when considered together. We empirically quantify such inconsistencies in the simple scenario of bigram comparison for two common styles of masked language models: T5-style and BERT-style. For example, we show that T5 models often confuse their own preference regarding two similar bigrams. We show that inconsistencies exist ubiquitously in masked language models of diverse sizes and configurations, from RoBERTa-base to GLM-130B.  As an initial attempt to address this issue during the inference phase, we propose Ensemble of Conditionals, a se
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#21644;&#22810;&#23610;&#24230;&#32593;&#32476;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#20174;&#19979;&#21040;&#19978;&#21644;&#20174;&#19978;&#21040;&#19979;&#30340;&#20449;&#24687;&#27969;&#26356;&#26032;&#65292;&#22686;&#24378;&#20102;&#19981;&#21516;&#32593;&#32476;&#23618;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#27169;&#22411;&#36890;&#36807;&#22810;&#23610;&#24230;&#26041;&#27861;&#23454;&#29616;&#31895;&#31961;&#21644;&#32454;&#33410;&#39044;&#27979;&#65292;&#21516;&#26102;&#32467;&#21512;&#32534;&#30721;-&#35299;&#30721;&#32593;&#32476;&#21644;LSTM&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#36755;&#20837;&#19982;&#21382;&#21490;&#29366;&#24577;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2212.11642</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#30340;&#22810;&#23610;&#24230;&#32593;&#32476;&#21644;&#32534;&#30721;-&#35299;&#30721;LSTM&#29992;&#20110;&#35270;&#39057;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Predictive Coding Based Multiscale Network with Encoder-Decoder LSTM for Video Prediction. (arXiv:2212.11642v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#21644;&#22810;&#23610;&#24230;&#32593;&#32476;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#20174;&#19979;&#21040;&#19978;&#21644;&#20174;&#19978;&#21040;&#19979;&#30340;&#20449;&#24687;&#27969;&#26356;&#26032;&#65292;&#22686;&#24378;&#20102;&#19981;&#21516;&#32593;&#32476;&#23618;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#27169;&#22411;&#36890;&#36807;&#22810;&#23610;&#24230;&#26041;&#27861;&#23454;&#29616;&#31895;&#31961;&#21644;&#32454;&#33410;&#39044;&#27979;&#65292;&#21516;&#26102;&#32467;&#21512;&#32534;&#30721;-&#35299;&#30721;&#32593;&#32476;&#21644;LSTM&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#36755;&#20837;&#19982;&#21382;&#21490;&#29366;&#24577;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#39044;&#27979;&#32534;&#30721;&#27169;&#22411;&#29992;&#20110;&#26410;&#26469;&#35270;&#39057;&#24103;&#30340;&#39044;&#27979;&#12290;&#20511;&#37492;&#20102;&#35748;&#30693;&#31185;&#23398;&#20013;&#30340;&#8220;&#39044;&#27979;&#32534;&#30721;&#8221;&#29702;&#35770;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20174;&#19979;&#21040;&#19978;&#21644;&#20174;&#19978;&#21040;&#19979;&#30340;&#20449;&#24687;&#27969;&#26356;&#26032;&#65292;&#22686;&#24378;&#20102;&#19981;&#21516;&#32593;&#32476;&#23618;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#39044;&#27979;&#32534;&#30721;&#27169;&#22411;&#21482;&#33021;&#25353;&#23618;&#27425;&#39044;&#27979;&#30446;&#21069;&#27491;&#22312;&#21457;&#29983;&#30340;&#20107;&#24773;&#65292;&#32780;&#19981;&#33021;&#39044;&#27979;&#26410;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#22810;&#23610;&#24230;&#26041;&#27861;&#65288;&#20174;&#31895;&#31961;&#21040;&#31934;&#32454;&#65289;&#65292;&#20854;&#20013;&#39640;&#23618;&#31070;&#32463;&#20803;&#29983;&#25104;&#31895;&#31961;&#39044;&#27979;&#65288;&#20302;&#20998;&#36776;&#29575;&#65289;&#65292;&#32780;&#20302;&#23618;&#31070;&#32463;&#20803;&#29983;&#25104;&#32454;&#33410;&#39044;&#27979;&#65288;&#39640;&#20998;&#36776;&#29575;&#65289;&#12290;&#22312;&#32593;&#32476;&#26550;&#26500;&#26041;&#38754;&#65292;&#25105;&#20204;&#30452;&#25509;&#23558;&#32534;&#30721;-&#35299;&#30721;&#32593;&#32476;&#34701;&#20837;LSTM&#27169;&#22359;&#65292;&#24182;&#22312;&#19981;&#21516;&#32593;&#32476;&#23618;&#20043;&#38388;&#20849;&#20139;&#26368;&#32456;&#32534;&#30721;&#30340;&#39640;&#23618;&#35821;&#20041;&#20449;&#24687;&#12290;&#19982;&#20256;&#32479;&#30340;&#32534;&#30721;&#35299;&#30721;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20351;&#24471;&#24403;&#21069;&#36755;&#20837;&#19982;LSTM&#30340;&#21382;&#21490;&#29366;&#24577;&#20043;&#38388;&#33021;&#22815;&#36827;&#34892;&#20840;&#38754;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a multi-scale predictive coding model for future video frames prediction. Drawing inspiration on the ``Predictive Coding" theories in cognitive science, it is updated by a combination of bottom-up and top-down information flows, which can enhance the interaction between different network levels. However, traditional predictive coding models only predict what is happening hierarchically rather than predicting the future. To address the problem, our model employs a multi-scale approach (Coarse to Fine), where the higher level neurons generate coarser predictions (lower resolution), while the lower level generate finer predictions (higher resolution). In terms of network architecture, we directly incorporate the encoder-decoder network within the LSTM module and share the final encoded high-level semantic information across different network levels. This enables comprehensive interaction between the current input and the historical states of LSTM compared with the traditional E
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#21704;&#21033;&#183;&#27874;&#29305;&#23545;&#35805;&#65288;HPD&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#23545;&#35805;&#20195;&#29702;&#21644;&#35282;&#33394;&#23545;&#40784;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#21704;&#21033;&#183;&#27874;&#29305;&#31995;&#21015;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#24182;&#27880;&#37322;&#20102;&#23545;&#35805;&#32972;&#26223;&#20449;&#24687;&#12289;&#35828;&#35805;&#32773;&#12289;&#35282;&#33394;&#20851;&#31995;&#21644;&#23646;&#24615;&#12290;&#36890;&#36807;&#22312;HPD&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21487;&#20197;&#25512;&#21160;&#23545;&#35805;&#20195;&#29702;&#30340;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#29305;&#23450;&#35282;&#33394;&#23545;&#40784;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.06869</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21704;&#21033;&#183;&#27874;&#29305;&#30456;&#36935;&#65306;&#29992;&#20110;&#19982;&#35282;&#33394;&#23545;&#40784;&#30340;&#21452;&#35821;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Meet Harry Potter: A Bilingual Dataset for Aligning Dialogue Agents with Characters. (arXiv:2211.06869v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06869
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#21704;&#21033;&#183;&#27874;&#29305;&#23545;&#35805;&#65288;HPD&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#23545;&#35805;&#20195;&#29702;&#21644;&#35282;&#33394;&#23545;&#40784;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#21704;&#21033;&#183;&#27874;&#29305;&#31995;&#21015;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#24182;&#27880;&#37322;&#20102;&#23545;&#35805;&#32972;&#26223;&#20449;&#24687;&#12289;&#35828;&#35805;&#32773;&#12289;&#35282;&#33394;&#20851;&#31995;&#21644;&#23646;&#24615;&#12290;&#36890;&#36807;&#22312;HPD&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21487;&#20197;&#25512;&#21160;&#23545;&#35805;&#20195;&#29702;&#30340;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#29305;&#23450;&#35282;&#33394;&#23545;&#40784;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20687;ChatGPT&#21644;GPT4&#36825;&#26679;&#30340;&#23545;&#35805;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#26500;&#24314;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35282;&#33394;&#34920;&#29616;&#30340;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#20840;&#38754;&#30340;&#27880;&#37322;&#65292;&#23558;&#36825;&#20123;&#20195;&#29702;&#19982;&#29305;&#23450;&#35282;&#33394;&#25110;&#20010;&#20307;&#23545;&#40784;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21704;&#21033;&#183;&#27874;&#29305;&#23545;&#35805;&#65288;HPD&#65289;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#21160;&#23545;&#35805;&#20195;&#29702;&#21644;&#35282;&#33394;&#23545;&#40784;&#30340;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#21704;&#21033;&#183;&#27874;&#29305;&#31995;&#21015;&#30340;&#25152;&#26377;&#23545;&#35805;&#22330;&#26223;&#65288;&#21253;&#25324;&#33521;&#25991;&#21644;&#20013;&#25991;&#65289;&#65292;&#24182;&#27880;&#37322;&#20102;&#37325;&#35201;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#21253;&#25324;&#23545;&#35805;&#22330;&#26223;&#12289;&#35828;&#35805;&#32773;&#12289;&#35282;&#33394;&#20851;&#31995;&#21644;&#23646;&#24615;&#12290;&#36825;&#20123;&#35814;&#32454;&#30340;&#27880;&#37322;&#21487;&#33021;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#22522;&#20110;&#35282;&#33394;&#30340;&#23545;&#35805;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#36890;&#29992;&#22522;&#20934;&#65292;&#35780;&#20272;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#29305;&#23450;&#35282;&#33394;&#23545;&#40784;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;HPD&#19978;&#20351;&#29992;&#32454;&#33268;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT and GPT4 have demonstrated immense potential in constructing open-domain dialogue agents. However, aligning these agents with specific characters or individuals remains a considerable challenge due to the complexities of character representation and the lack of comprehensive annotations. In this paper, we introduce the Harry Potter Dialogue (HPD) dataset, designed to advance the study of dialogue agents and character alignment. The dataset encompasses all dialogue sessions (in both English and Chinese) from the Harry Potter series and is annotated with vital background information, including dialogue scenes, speakers, character relationships, and attributes. These extensive annotations may empower LLMs to unlock character-driven dialogue capabilities. Furthermore, it can serve as a universal benchmark for evaluating how well can a LLM aligning with a specific character. We benchmark LLMs on HPD using both fine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Softmax&#32452;&#20214;&#20013;&#24341;&#20837;&#26799;&#24230;&#34928;&#20943;&#36229;&#21442;&#25968;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#27867;&#21270;&#24615;&#33021;&#19982;&#26799;&#24230;&#34928;&#20943;&#29575;&#26174;&#33879;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#36739;&#23567;&#30340;&#26799;&#24230;&#34928;&#20943;&#30340;&#20248;&#21270;&#26041;&#27861;&#31867;&#20284;&#20110;&#35838;&#31243;&#23398;&#20064;&#24207;&#21015;&#65292;&#20351;&#24471;&#22256;&#38590;&#26679;&#26412;&#22312;&#26131;&#26679;&#26412;&#30830;&#20449;&#20043;&#21518;&#24471;&#21040;&#20851;&#27880;&#12290;&#22823;&#36793;&#38469;Softmax&#20250;&#24433;&#21709;&#23616;&#37096;Lipschitz&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2210.17145</link><description>&lt;p&gt;
&#22823;&#36793;&#38469;Softmax&#20013;&#30340;&#27010;&#29575;&#30456;&#20851;&#26799;&#24230;&#34928;&#20943;
&lt;/p&gt;
&lt;p&gt;
Probability-Dependent Gradient Decay in Large Margin Softmax. (arXiv:2210.17145v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Softmax&#32452;&#20214;&#20013;&#24341;&#20837;&#26799;&#24230;&#34928;&#20943;&#36229;&#21442;&#25968;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#27867;&#21270;&#24615;&#33021;&#19982;&#26799;&#24230;&#34928;&#20943;&#29575;&#26174;&#33879;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#36739;&#23567;&#30340;&#26799;&#24230;&#34928;&#20943;&#30340;&#20248;&#21270;&#26041;&#27861;&#31867;&#20284;&#20110;&#35838;&#31243;&#23398;&#20064;&#24207;&#21015;&#65292;&#20351;&#24471;&#22256;&#38590;&#26679;&#26412;&#22312;&#26131;&#26679;&#26412;&#30830;&#20449;&#20043;&#21518;&#24471;&#21040;&#20851;&#27880;&#12290;&#22823;&#36793;&#38469;Softmax&#20250;&#24433;&#21709;&#23616;&#37096;Lipschitz&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;Softmax&#24050;&#32463;&#25104;&#20026;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#24120;&#35265;&#30340;&#32452;&#20214;&#12290;&#26412;&#25991;&#22312;Softmax&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26799;&#24230;&#34928;&#20943;&#36229;&#21442;&#25968;&#65292;&#20197;&#25511;&#21046;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#27010;&#29575;&#30456;&#20851;&#26799;&#24230;&#34928;&#20943;&#29575;&#12290;&#36890;&#36807;&#23545;&#22522;&#20110;MNIST&#12289;CIFAR-10/100&#21644;SVHN&#30340;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#27867;&#21270;&#24615;&#33021;&#19982;&#26799;&#24230;&#34928;&#20943;&#29575;&#26174;&#33879;&#30456;&#20851;&#65292;&#21363;&#38543;&#30528;&#32622;&#20449;&#27010;&#29575;&#30340;&#19978;&#21319;&#65292;&#26799;&#24230;&#20250;&#21576;&#20984;&#20989;&#25968;&#25110;&#20985;&#20989;&#25968;&#36882;&#20943;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#36739;&#23567;&#30340;&#26799;&#24230;&#34928;&#20943;&#30340;&#20248;&#21270;&#26041;&#27861;&#31867;&#20284;&#20110;&#35838;&#31243;&#23398;&#20064;&#24207;&#21015;&#65292;&#21363;&#22312;&#26131;&#26679;&#26412;&#36275;&#22815;&#30830;&#20449;&#20043;&#21518;&#65292;&#25165;&#20250;&#20851;&#27880;&#22256;&#38590;&#26679;&#26412;&#65292;&#24182;&#19988;&#23545;&#20110;&#26679;&#26412;&#20043;&#38388;&#30340;&#31867;&#20869;&#36317;&#31163;&#36739;&#22823;&#30340;&#24773;&#20917;&#20250;&#33719;&#24471;&#26356;&#39640;&#30340;&#26799;&#24230;&#20197;&#20943;&#23567;&#36317;&#31163;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;&#35777;&#25454;&#35777;&#26126;&#22823;&#36793;&#38469;Softmax&#23558;&#24433;&#21709;&#23616;&#37096;Lipschitz&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, Softmax has become a common component in neural network frameworks. In this paper, a gradient decay hyperparameter is introduced in Softmax to control the probability-dependent gradient decay rate during training. By following the theoretical analysis and empirical results of a variety of model architectures trained on MNIST, CIFAR-10/100 and SVHN, we find that the generalization performance depends significantly on the gradient decay rate as the confidence probability rises, i.e., the gradient decreases convexly or concavely as the sample probability increases. Moreover, optimization with the small gradient decay shows a similar curriculum learning sequence where hard samples are in the spotlight only after easy samples are convinced sufficiently, and well-separated samples gain a higher gradient to reduce intra-class distance. Based on the analysis results, we can provide evidence that the large margin Softmax will affect the local Lipschitz constraint of the l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPUIR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33609;&#22270;&#25216;&#26415;&#23436;&#25104;&#26410;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#34917;&#20805;&#65292;&#20174;&#32780;&#36817;&#20284;&#20102;&#20840;&#20449;&#24687;&#21453;&#39304;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25511;&#21046;&#20559;&#24046;&#21644;&#27604;&#27809;&#26377;&#22870;&#21169;&#34917;&#20805;&#26041;&#27861;&#26356;&#23567;&#30340;&#26041;&#24046;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#30636;&#26102;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2210.06719</link><description>&lt;p&gt;
&#20351;&#29992;&#33609;&#22270;&#25216;&#26415;&#36827;&#34892;&#22870;&#21169;&#34917;&#20805;&#30340;&#19978;&#19979;&#25991;&#25209;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reward Imputation with Sketching for Contextual Batched Bandits. (arXiv:2210.06719v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPUIR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33609;&#22270;&#25216;&#26415;&#23436;&#25104;&#26410;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#34917;&#20805;&#65292;&#20174;&#32780;&#36817;&#20284;&#20102;&#20840;&#20449;&#24687;&#21453;&#39304;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25511;&#21046;&#20559;&#24046;&#21644;&#27604;&#27809;&#26377;&#22870;&#21169;&#34917;&#20805;&#26041;&#27861;&#26356;&#23567;&#30340;&#26041;&#24046;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#30636;&#26102;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#25209;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#35774;&#32622;&#65292;&#20854;&#20013;&#22312;&#27599;&#20010;episode&#32467;&#26463;&#26102;&#35266;&#23519;&#21040;&#29615;&#22659;&#20013;&#30340;&#19968;&#25209;&#22870;&#21169;&#65292;&#20294;&#26159;&#26410;&#25191;&#34892;&#25805;&#20316;&#30340;&#22870;&#21169;&#26159;&#26410;&#30693;&#30340;&#65292;&#23548;&#33268;&#20102;&#37096;&#20998;&#20449;&#24687;&#21453;&#39304;&#12290;&#29616;&#26377;&#30340;&#19978;&#19979;&#25991;&#25209;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#26410;&#25191;&#34892;&#25805;&#20316;&#30340;&#22870;&#21169;&#65292;&#23548;&#33268;&#21453;&#39304;&#20449;&#24687;&#30340;&#28010;&#36153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Sketched Policy Updating with Imputed Rewards (SPUIR)&#65292;&#36890;&#36807;&#20351;&#29992;&#33609;&#22270;&#25216;&#26415;&#23436;&#25104;&#26410;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#34917;&#20805;&#65292;&#20174;&#32780;&#36817;&#20284;&#20102;&#20840;&#20449;&#24687;&#21453;&#39304;&#12290;&#25105;&#20204;&#23558;&#22870;&#21169;&#34917;&#20805;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#27714;&#35299;&#25191;&#34892;&#21644;&#26410;&#25191;&#34892;&#25805;&#20316;&#30340;&#21453;&#39304;&#26426;&#21046;&#30340;&#27491;&#21017;&#21270;&#23725;&#22238;&#24402;&#38382;&#39064;&#12290;&#20026;&#20102;&#38477;&#20302;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#33609;&#22270;&#25216;&#26415;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25511;&#21046;&#20559;&#24046;&#21644;&#27604;&#27809;&#26377;&#22870;&#21169;&#34917;&#20805;&#26041;&#27861;&#26356;&#23567;&#30340;&#26041;&#24046;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#30636;&#26102;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual batched bandit (CBB) is a setting where a batch of rewards is observed from the environment at the end of each episode, but the rewards of the non-executed actions are unobserved, resulting in partial-information feedback. Existing approaches for CBB often ignore the rewards of the non-executed actions, leading to underutilization of feedback information. In this paper, we propose an efficient approach called Sketched Policy Updating with Imputed Rewards (SPUIR) that completes the unobserved rewards using sketching, which approximates the full-information feedbacks. We formulate reward imputation as an imputation regularized ridge regression problem that captures the feedback mechanisms of both executed and non-executed actions. To reduce time complexity, we solve the regression problem using randomized sketching. We prove that our approach achieves an instantaneous regret with controllable bias and smaller variance than approaches without reward imputation. Furthermore, our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#28508;&#22312;&#34920;&#31034;&#30340;&#35889;&#20998;&#26512;&#21457;&#29616;&#65292;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#20250;&#23548;&#33268;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20559;&#21521;&#32534;&#30721;&#36739;&#20302;&#26377;&#25928;&#31209;&#30340;&#34920;&#31034;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#31209;&#27491;&#21017;&#21270;&#39044;&#35757;&#32451;&#26377;&#20559;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.05248</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#20302;&#31209;&#27491;&#21017;&#21270;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-supervised debiasing using low rank regularization. (arXiv:2210.05248v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#28508;&#22312;&#34920;&#31034;&#30340;&#35889;&#20998;&#26512;&#21457;&#29616;&#65292;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#20250;&#23548;&#33268;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20559;&#21521;&#32534;&#30721;&#36739;&#20302;&#26377;&#25928;&#31209;&#30340;&#34920;&#31034;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#31209;&#27491;&#21017;&#21270;&#39044;&#35757;&#32451;&#26377;&#20559;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#30456;&#20851;&#24615;&#21487;&#33021;&#23548;&#33268;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24378;&#20559;&#35265;&#65292;&#24433;&#21709;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21435;&#20559;&#26041;&#27861;&#35201;&#27714;&#23545;&#34394;&#20551;&#23646;&#24615;&#25110;&#30446;&#26631;&#26631;&#31614;&#36827;&#34892;&#23436;&#20840;&#30417;&#30563;&#65292;&#20294;&#22914;&#20309;&#20165;&#36890;&#36807;&#26377;&#38480;&#30340;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#21435;&#20559;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#28508;&#22312;&#34920;&#31034;&#36827;&#34892;&#35889;&#20998;&#26512;&#30740;&#31350;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65306;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#20351;&#31070;&#32463;&#32593;&#32476;&#24402;&#32435;&#22320;&#20559;&#21521;&#32534;&#30721;&#36739;&#20302;&#26377;&#25928;&#31209;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31209;&#27491;&#21017;&#21270;&#21487;&#20197;&#25918;&#22823;&#36825;&#31181;&#20559;&#24046;&#65292;&#20197;&#40723;&#21169;&#39640;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#30340;&#21435;&#20559;&#26694;&#26550;&#65292;&#21487;&#33021;&#19982;&#26080;&#26631;&#31614;&#26679;&#26412;&#20860;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#31209;&#27491;&#21017;&#21270;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#39044;&#35757;&#32451;&#19968;&#20010;&#26377;&#20559;&#32534;&#30721;&#22120;&#65292;&#20316;&#20026;&#35821;&#20041;&#29942;&#39048;&#26469;&#24378;&#21046;&#32534;&#30721;&#22120;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spurious correlations can cause strong biases in deep neural networks, impairing generalization ability. While most existing debiasing methods require full supervision on either spurious attributes or target labels, training a debiased model from a limited amount of both annotations is still an open question. To address this issue, we investigate an interesting phenomenon using the spectral analysis of latent representations: spuriously correlated attributes make neural networks inductively biased towards encoding lower effective rank representations. We also show that a rank regularization can amplify this bias in a way that encourages highly correlated features. Leveraging these findings, we propose a self-supervised debiasing framework potentially compatible with unlabeled samples. Specifically, we first pretrain a biased encoder in a self-supervised manner with the rank regularization, serving as a semantic bottleneck to enforce the encoder to learn the spuriously correlated attrib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21327;&#35843;&#19982;&#29615;&#22659;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;HECOGrid&#29615;&#22659;&#22871;&#20214;&#65292;&#36890;&#36807;&#23545;&#21327;&#35843;&#21644;&#24322;&#36136;&#24615;&#27700;&#24179;&#30340;&#23450;&#37327;&#25511;&#21046;&#65292;&#20415;&#20110;&#23545;&#19981;&#21516;MARL&#26041;&#27861;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2210.03022</link><description>&lt;p&gt;
&#26377;&#29366;&#24577;&#30340;&#20027;&#21160;&#21327;&#35843;&#22120;&#65306;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21327;&#35843;&#19982;&#29615;&#22659;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stateful active facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2210.03022v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21327;&#35843;&#19982;&#29615;&#22659;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;HECOGrid&#29615;&#22659;&#22871;&#20214;&#65292;&#36890;&#36807;&#23545;&#21327;&#35843;&#21644;&#24322;&#36136;&#24615;&#27700;&#24179;&#30340;&#23450;&#37327;&#25511;&#21046;&#65292;&#20415;&#20110;&#23545;&#19981;&#21516;MARL&#26041;&#27861;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#19968;&#32452;&#26234;&#33021;&#20307;&#20849;&#21516;&#21162;&#21147;&#23454;&#29616;&#19968;&#20010;&#20849;&#21516;&#30340;&#30446;&#26631;&#12290;&#19981;&#21516;&#30340;&#29615;&#22659;&#25110;&#20219;&#21153;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#21327;&#35843;&#26469;&#20197;&#26368;&#20248;&#30340;&#26041;&#24335;&#23454;&#29616;&#30446;&#26631;&#12290;&#21327;&#35843;&#30340;&#24615;&#36136;&#21462;&#20915;&#20110;&#29615;&#22659;&#30340;&#23646;&#24615;&#65292;&#20363;&#22914;&#31354;&#38388;&#24067;&#23616;&#12289;&#38556;&#30861;&#29289;&#20998;&#24067;&#12289;&#21160;&#24577;&#31561;&#12290;&#25105;&#20204;&#23558;&#29615;&#22659;&#20869;&#23646;&#24615;&#30340;&#36825;&#31181;&#21464;&#21270;&#31216;&#20026;&#24322;&#36136;&#24615;&#12290;&#29616;&#26377;&#25991;&#29486;&#23578;&#26410;&#20805;&#20998;&#35299;&#20915;&#19981;&#21516;&#29615;&#22659;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#27700;&#24179;&#30340;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#29615;&#22659;&#30340;&#21327;&#35843;&#27700;&#24179;&#21644;&#24322;&#36136;&#24615;&#27700;&#24179;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;HECOGrid&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;RL&#29615;&#22659;&#22871;&#20214;&#65292;&#36890;&#36807;&#25552;&#20379;&#23545;&#29615;&#22659;&#30340;&#21327;&#35843;&#21644;&#24322;&#36136;&#24615;&#27700;&#24179;&#36827;&#34892;&#23450;&#37327;&#25511;&#21046;&#65292;&#20415;&#20110;&#23545;&#19981;&#21516;&#21327;&#20316;&#21644;&#29615;&#22659;&#24322;&#36136;&#24615;&#30340;MARL&#26041;&#27861;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In cooperative multi-agent reinforcement learning, a team of agents works together to achieve a common goal. Different environments or tasks may require varying degrees of coordination among agents in order to achieve the goal in an optimal way. The nature of coordination will depend on the properties of the environment -- its spatial layout, distribution of obstacles, dynamics, etc. We term this variation of properties within an environment as heterogeneity. Existing literature has not sufficiently addressed the fact that different environments may have different levels of heterogeneity. We formalize the notions of coordination level and heterogeneity level of an environment and present HECOGrid, a suite of multi-agent RL environments that facilitates empirical evaluation of different MARL approaches across different levels of coordination and environmental heterogeneity by providing a quantitative control over coordination and heterogeneity levels of the environment. Further, we prop
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;EvoGen&#65292;&#19968;&#20010;&#20803;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26657;&#20934;&#25110;&#34394;&#25311;&#29983;&#25104;&#30340;&#21516;&#28304;&#24207;&#21015;&#26469;&#24341;&#23548;AlphaFold2&#27169;&#22411;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;&#25240;&#21472;&#21644;&#32467;&#26500;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2208.09652</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#24341;&#23548;AlphaFold2&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20934;&#30830;&#25240;&#21472;&#36335;&#24452;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervisedly Prompting AlphaFold2 for Few-Shot Learning of Accurate Folding Landscape and Protein Structure Prediction. (arXiv:2208.09652v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;EvoGen&#65292;&#19968;&#20010;&#20803;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26657;&#20934;&#25110;&#34394;&#25311;&#29983;&#25104;&#30340;&#21516;&#28304;&#24207;&#21015;&#26469;&#24341;&#23548;AlphaFold2&#27169;&#22411;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;&#25240;&#21472;&#21644;&#32467;&#26500;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#27979;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#36716;&#21270;&#20026;&#29983;&#29289;&#27963;&#24615;&#32467;&#26500;&#65292;&#23545;&#31185;&#23398;&#30740;&#31350;&#21644;&#21307;&#23398;&#21457;&#23637;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#20351;&#29992;&#20849;&#36827;&#21270;&#20449;&#24687;&#30830;&#23450;&#20934;&#30830;&#30340;&#25240;&#21472;&#36335;&#24452;&#26159;&#29616;&#20195;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#25104;&#21151;&#30340;&#22522;&#30784;&#12290;&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;AlphaFold2&#22312;&#19981;&#36827;&#34892;&#26174;&#24335;&#20849;&#36827;&#21270;&#20998;&#26512;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#24615;&#33021;&#20173;&#28982;&#24378;&#28872;&#20381;&#36182;&#20110;&#21487;&#29992;&#30340;&#24207;&#21015;&#21516;&#28304;&#20307;&#12290;&#22522;&#20110;&#23545;&#36825;&#31181;&#20381;&#36182;&#21407;&#22240;&#30340;&#25506;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvoGen&#65292;&#19968;&#20010;&#20803;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;AlphaFold2&#22312;&#36139;&#20047;MSA&#30446;&#26631;&#19978;&#30340;&#24615;&#33021;&#19981;&#36275;&#12290;&#36890;&#36807;&#20351;&#29992;&#26657;&#20934;&#25110;&#34394;&#25311;&#29983;&#25104;&#30340;&#21516;&#28304;&#24207;&#21015;&#26469;&#24341;&#23548;&#27169;&#22411;&#65292;EvoGen&#24110;&#21161;AlphaFold2&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#20934;&#30830;&#25240;&#21472;&#65292;&#29978;&#33267;&#22312;&#21333;&#24207;&#21015;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#24615;&#33021;&#12290;&#33021;&#22815;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Data-driven predictive methods which can efficiently and accurately transform protein sequences into biologically active structures are highly valuable for scientific research and medical development. Determining accurate folding landscape using co-evolutionary information is fundamental to the success of modern protein structure prediction methods. As the state of the art, AlphaFold2 has dramatically raised the accuracy without performing explicit co-evolutionary analysis. Nevertheless, its performance still shows strong dependence on available sequence homologs. Based on the interrogation on the cause of such dependence, we presented EvoGen, a meta generative model, to remedy the underperformance of AlphaFold2 for poor MSA targets. By prompting the model with calibrated or virtually generated homologue sequences, EvoGen helps AlphaFold2 fold accurately in low-data regime and even achieve encouraging performance with single-sequence predictions. Being able to make accurate predictions
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SsaA&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#26631;&#27880;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#21046;&#36896;&#33258;&#21160;&#21270;&#22330;&#26223;&#19979;&#25345;&#32493;&#36827;&#34892;&#22312;&#32447;&#35270;&#35273;&#26816;&#27979;&#65292;&#24182;&#33021;&#20026;&#25972;&#20010;&#21046;&#36896;&#29983;&#21629;&#21608;&#26399;&#24314;&#31435;&#35270;&#35273;&#26816;&#27979;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2208.04173</link><description>&lt;p&gt;
SIAD: &#33258;&#30417;&#30563;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SIAD: Self-supervised Image Anomaly Detection System. (arXiv:2208.04173v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SsaA&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#26631;&#27880;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#21046;&#36896;&#33258;&#21160;&#21270;&#22330;&#26223;&#19979;&#25345;&#32493;&#36827;&#34892;&#22312;&#32447;&#35270;&#35273;&#26816;&#27979;&#65292;&#24182;&#33021;&#20026;&#25972;&#20010;&#21046;&#36896;&#29983;&#21629;&#21608;&#26399;&#24314;&#31435;&#35270;&#35273;&#26816;&#27979;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#35270;&#35273;&#26816;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31995;&#32479;&#37117;&#26159;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#65292;&#24182;&#19988;&#26080;&#27861;&#20026;&#22312;&#32447;&#24212;&#29992;&#25552;&#20379;&#38271;&#26399;&#25903;&#25345;&#12290;&#20026;&#20102;&#21521;&#21069;&#36808;&#19968;&#27493;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SsaA&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#26631;&#27880;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#21046;&#36896;&#33258;&#21160;&#21270;&#22330;&#26223;&#19979;&#25345;&#32493;&#36827;&#34892;&#22312;&#32447;&#35270;&#35273;&#26816;&#27979;&#12290;&#20511;&#21161;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;SsaA&#33021;&#22815;&#26377;&#25928;&#22320;&#20026;&#25972;&#20010;&#21046;&#36896;&#29983;&#21629;&#21608;&#26399;&#24314;&#31435;&#35270;&#35273;&#26816;&#27979;&#24212;&#29992;&#12290;&#22312;&#26089;&#26399;&#38454;&#27573;&#65292;&#20165;&#26377;&#26080;&#24322;&#24120;&#25968;&#25454;&#26102;&#65292;&#37319;&#29992;&#26080;&#30417;&#30563;&#31639;&#27861;&#22788;&#29702;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#20026;&#21518;&#32493;&#25968;&#25454;&#29983;&#25104;&#31895;&#31961;&#30340;&#26631;&#31614;&#12290;&#28982;&#21518;&#35757;&#32451;&#26377;&#30417;&#30563;&#31639;&#27861;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#12290;&#20511;&#21161;&#29992;&#25143;&#21451;&#22909;&#30340;&#22522;&#20110;Web&#30340;&#25509;&#21475;&#65292;SsaA&#38750;&#24120;&#20415;&#20110;&#38598;&#25104;&#21644;&#37096;&#32626;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#31639;&#27861;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;SsaA&#31995;&#32479;&#24050;&#32463;&#23454;&#29616;&#20102;&#38271;&#26399;&#22312;&#32447;&#30340;&#35270;&#35273;&#26816;&#27979;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent trends in AIGC effectively boosted the application of visual inspection. However, most of the available systems work in a human-in-the-loop manner and can not provide long-term support to the online application. To make a step forward, this paper outlines an automatic annotation system called SsaA, working in a self-supervised learning manner, for continuously making the online visual inspection in the manufacturing automation scenarios. Benefit from the self-supervised learning, SsaA is effective to establish a visual inspection application for the whole life-cycle of manufacturing. In the early stage, with only the anomaly-free data, the unsupervised algorithms are adopted to process the pretext task and generate coarse labels for the following data. Then supervised algorithms are trained for the downstream task. With user-friendly web-based interfaces, SsaA is very convenient to integrate and deploy both of the unsupervised and supervised algorithms. So far, the SsaA system h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#26080;&#35770;&#39044;&#35757;&#32451;&#37319;&#29992;&#20309;&#31181;&#21327;&#35758;&#65292;&#32447;&#24615;&#39044;&#27979;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#21463;&#20854;&#22522;&#30784;&#34920;&#31034;&#40065;&#26834;&#24615;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25439;&#22833;&#19978;&#30028;&#21644;&#40065;&#26834;&#20998;&#31867;&#20934;&#21017;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.03835</link><description>&lt;p&gt;
&#20851;&#20110;&#20174;&#39044;&#35757;&#32451;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
On Transfer of Adversarial Robustness from Pretraining to Downstream Tasks. (arXiv:2208.03835v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#26080;&#35770;&#39044;&#35757;&#32451;&#37319;&#29992;&#20309;&#31181;&#21327;&#35758;&#65292;&#32447;&#24615;&#39044;&#27979;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#21463;&#20854;&#22522;&#30784;&#34920;&#31034;&#40065;&#26834;&#24615;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25439;&#22833;&#19978;&#30028;&#21644;&#40065;&#26834;&#20998;&#31867;&#20934;&#21017;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35757;&#32451;&#26041;&#26696;&#30340;&#27969;&#34892;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#34429;&#28982;&#23454;&#36341;&#20013;&#24050;&#32463;&#35777;&#26126;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#20174;&#39044;&#35757;&#32451;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#23646;&#24615;&#30340;&#36716;&#31227;&#20173;&#28982;&#19981;&#22815;&#29702;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32447;&#24615;&#39044;&#27979;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#21487;&#20197;&#30001;&#20854;&#22522;&#30784;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#38480;&#21046;&#65292;&#32780;&#19981;&#31649;&#39044;&#35757;&#32451;&#20351;&#29992;&#30340;&#21327;&#35758;&#22914;&#20309;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;(i)&#19968;&#20010;&#22312;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#20013;&#37117;&#25104;&#31435;&#30340;&#25439;&#22833;&#19978;&#30028;&#65292;&#20197;&#21450;(ii)&#29305;&#23450;&#20110;&#40065;&#26834;&#20998;&#31867;&#30340;&#20934;&#21017;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#22914;&#20309;&#29992;&#20110;&#26657;&#20934;&#19979;&#28216;&#40065;&#26834;&#24615;&#30340;&#26399;&#26395;&#65292;&#20197;&#21450;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#26368;&#20248;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#29992;&#36884;&#12290;&#32508;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#34920;&#24449;&#35201;&#27714;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large-scale training regimes have gained popularity, the use of pretrained models for downstream tasks has become common practice in machine learning. While pretraining has been shown to enhance the performance of models in practice, the transfer of robustness properties from pretraining to downstream tasks remains poorly understood. In this study, we demonstrate that the robustness of a linear predictor on downstream tasks can be constrained by the robustness of its underlying representation, regardless of the protocol used for pretraining. We prove (i) a bound on the loss that holds independent of any downstream task, as well as (ii) a criterion for robust classification in particular. We validate our theoretical results in practical applications, show how our results can be used for calibrating expectations of downstream robustness, and when our results are useful for optimal transfer learning. Taken together, our results offer an initial step towards characterizing the requireme
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#25913;&#36827;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#20811;&#26381;&#29616;&#26377;&#27169;&#22411;&#22312;&#22788;&#29702;&#36793;&#32536;&#20449;&#24687;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#30340;&#26032;&#27169;&#22411;&#33021;&#22815;&#22312;&#20559;&#22909;&#25490;&#24207;&#38382;&#39064;&#20013;&#25552;&#20379;&#26377;&#25928;&#30340;&#37051;&#22495;&#25805;&#20316;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2206.00383</link><description>&lt;p&gt;
&#22270;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#25913;&#36827;&#21551;&#21457;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural Improvement Heuristics for Graph Combinatorial Optimization Problems. (arXiv:2206.00383v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00383
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#25913;&#36827;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#20811;&#26381;&#29616;&#26377;&#27169;&#22411;&#22312;&#22788;&#29702;&#36793;&#32536;&#20449;&#24687;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#30340;&#26032;&#27169;&#22411;&#33021;&#22815;&#22312;&#20559;&#22909;&#25490;&#24207;&#38382;&#39064;&#20013;&#25552;&#20379;&#26377;&#25928;&#30340;&#37051;&#22495;&#25805;&#20316;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#36827;&#23637;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#25552;&#39640;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#12290;&#22312;&#24050;&#26377;&#30340;&#32452;&#21512;&#20248;&#21270;&#27169;&#22411;&#20013;&#65292;&#31070;&#32463;&#25913;&#36827;&#65288;NI&#65289;&#27169;&#22411;&#23588;&#20854;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#23616;&#38480;&#20110;&#23558;&#20851;&#38190;&#20449;&#24687;&#32534;&#30721;&#22312;&#36793;&#19978;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#32771;&#34385;&#33410;&#28857;&#29305;&#24449;&#21644;&#33410;&#28857;&#20301;&#32622;&#32534;&#30721;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NI&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#23558;&#20449;&#24687;&#32534;&#30721;&#22312;&#33410;&#28857;&#12289;&#36793;&#25110;&#20004;&#32773;&#20013;&#30340;&#22522;&#20110;&#22270;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20316;&#20026;&#22522;&#20110;&#29228;&#23665;&#31639;&#27861;&#30340;&#31639;&#27861;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#25351;&#23548;&#27599;&#27425;&#36845;&#20195;&#30340;&#37051;&#22495;&#25805;&#20316;&#30340;&#36873;&#25321;&#12290;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#25512;&#33616;&#20248;&#20110;&#20256;&#32479;&#29256;&#26412;&#30340;&#37051;&#22495;&#25805;&#20316;&#65292;&#23545;&#20110;&#20559;&#22909;&#25490;&#24207;&#38382;&#39064;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;99&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in graph neural network architectures and increased computation power have revolutionized the field of combinatorial optimization (CO). Among the proposed models for CO problems, Neural Improvement (NI) models have been particularly successful. However, existing NI approaches are limited in their applicability to problems where crucial information is encoded in the edges, as they only consider node features and node-wise positional encodings. To overcome this limitation, we introduce a novel NI model capable of handling graph-based problems where information is encoded in the nodes, edges, or both. The presented model serves as a fundamental component for hill-climbing-based algorithms that guide the selection of neighborhood operations for each iteration. Conducted experiments demonstrate that the proposed model can recommend neighborhood operations that outperform conventional versions for the Preference Ranking Problem with a performance in the 99th percentile. We al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#23646;&#24615;&#36951;&#24536;&#26469;&#23545;&#25239;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65292;&#20294;&#21457;&#29616;&#35813;&#26041;&#27861;&#34429;&#28982;&#23545;&#20110;&#29305;&#23450;&#23545;&#25163;&#30340;&#30446;&#26631;&#27169;&#22411;&#38450;&#24481;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26080;&#27861;&#23545;&#25239;&#25972;&#20010;PIA&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2205.08821</link><description>&lt;p&gt;
&#12298;&#32463;&#39564;&#25945;&#35757;&#65306;&#25269;&#24481;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#12299;
&lt;/p&gt;
&lt;p&gt;
Lessons Learned: Defending Against Property Inference Attacks. (arXiv:2205.08821v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#23646;&#24615;&#36951;&#24536;&#26469;&#23545;&#25239;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65292;&#20294;&#21457;&#29616;&#35813;&#26041;&#27861;&#34429;&#28982;&#23545;&#20110;&#29305;&#23450;&#23545;&#25163;&#30340;&#30446;&#26631;&#27169;&#22411;&#38450;&#24481;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26080;&#27861;&#23545;&#25239;&#25972;&#20010;PIA&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#21644;&#35780;&#20272;&#22810;&#31181;&#38450;&#24481;&#31574;&#30053;&#26469;&#23545;&#25239;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65288;PIA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#25915;&#20987;&#12290;&#22312;&#32473;&#23450;&#19968;&#20010;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;PIA&#26088;&#22312;&#25552;&#21462;&#20854;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#20363;&#22914;&#25581;&#31034;&#21307;&#30103;&#25968;&#25454;&#38598;&#20013;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;&#27604;&#20363;&#12290;&#34429;&#28982;&#38024;&#23545;&#20854;&#20182;&#38544;&#31169;&#25915;&#20987;&#65292;&#20363;&#22914;&#25104;&#21592;&#25512;&#26029;&#65292;&#24050;&#32463;&#26377;&#24456;&#22810;&#20851;&#20110;&#38450;&#24481;&#26426;&#21046;&#30340;&#30740;&#31350;&#21457;&#34920;&#65292;&#20294;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#38450;&#24481;PIA&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#36890;&#29992;&#30340;&#25269;&#24481;&#30333;&#30418;PIA&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;-&#23646;&#24615;&#36951;&#24536;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23646;&#24615;&#36951;&#24536;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;&#23646;&#24615;&#36951;&#24536;&#23545;&#20110;&#38024;&#23545;&#29305;&#23450;&#23545;&#25163;&#30340;&#30446;&#26631;&#27169;&#22411;&#30340;&#38450;&#24481;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26080;&#27861;&#27010;&#25324;&#65292;&#21363;&#26080;&#27861;&#20445;&#25252;&#25972;&#20010;PIA&#31867;&#21035;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#31181;&#38480;&#21046;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#39564;&#20013;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates and evaluates multiple defense strategies against property inference attacks (PIAs), a privacy attack against machine learning models. Given a trained machine learning model, PIAs aim to extract statistical properties of its underlying training data, e.g., reveal the ratio of men and women in a medical training data set. While for other privacy attacks like membership inference, a lot of research on defense mechanisms has been published, this is the first work focusing on defending against PIAs. With the primary goal of developing a generic mitigation strategy against white-box PIAs, we propose the novel approach property unlearning. Extensive experiments with property unlearning show that while it is very effective when defending target models against specific adversaries, property unlearning is not able to generalize, i.e., protect against a whole class of PIAs. To investigate the reasons behind this limitation, we present the results of experiments with the ex
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23547;&#25214;&#31574;&#30053;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#23433;&#20840;&#21306;&#22495;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#20934;&#21017;&#36924;&#36817;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#20986;&#36867;&#36920;&#27010;&#29575;&#21644;&#23433;&#20840;&#21306;&#22495;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2202.11593</link><description>&lt;p&gt;
&#23547;&#25214;&#31574;&#30053;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#23433;&#20840;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Finding Safe Zones of policies Markov Decision Processes. (arXiv:2202.11593v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.11593
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23547;&#25214;&#31574;&#30053;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#23433;&#20840;&#21306;&#22495;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#20934;&#21017;&#36924;&#36817;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#20986;&#36867;&#36920;&#27010;&#29575;&#21644;&#23433;&#20840;&#21306;&#22495;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#23433;&#20840;&#21306;&#22495;&#65292;&#21363;&#29366;&#24577;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#22823;&#22810;&#25968;&#31574;&#30053;&#30340;&#36712;&#36857;&#37117;&#34987;&#38480;&#21046;&#22312;&#35813;&#23376;&#38598;&#20869;&#12290;&#23433;&#20840;&#21306;&#22495;&#30340;&#36136;&#37327;&#30001;&#29366;&#24577;&#25968;&#21644;&#36867;&#36920;&#27010;&#29575;&#21442;&#25968;&#21270;&#65292;&#21363;&#38543;&#26426;&#36712;&#36857;&#31163;&#24320;&#23376;&#38598;&#30340;&#27010;&#29575;&#12290;&#24403;&#23433;&#20840;&#21306;&#22495;&#20855;&#26377;&#23569;&#37327;&#30340;&#29366;&#24577;&#21644;&#36739;&#20302;&#30340;&#36867;&#36920;&#27010;&#29575;&#26102;&#65292;&#23588;&#20854;&#26377;&#36259;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23547;&#25214;&#26368;&#20248;&#23433;&#20840;&#21306;&#22495;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#33324;&#24773;&#20917;&#19979;&#35813;&#38382;&#39064;&#35745;&#31639;&#19978;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#19968;&#20010;&#21452;&#20934;&#21017;&#36924;&#36817;&#23398;&#20064;&#31639;&#27861;&#65292;&#20934;&#30830;&#24230;&#36817;&#20284;&#20026;$2$&#20493;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#36867;&#36920;&#27010;&#29575;&#21644;&#23433;&#20840;&#21306;&#22495;&#22823;&#23567;&#65292;&#24182;&#19988;&#20351;&#29992;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a policy of a Markov Decision Process, we define a SafeZone as a subset of states, such that most of the policy's trajectories are confined to this subset. The quality of a SafeZone is parameterized by the number of states and the escape probability, i.e., the probability that a random trajectory will leave the subset. SafeZones are especially interesting when they have a small number of states and low escape probability. We study the complexity of finding optimal SafeZones, and show that in general, the problem is computationally hard. Our main result is a bi-criteria approximation learning algorithm with a factor of almost $2$ approximation for both the escape probability and SafeZone size, using a polynomial size sample complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24471;&#20998;&#24341;&#23548;&#32593;&#32476;&#65292;&#36890;&#36807;&#24471;&#20998;&#24341;&#23548;&#31574;&#30053;&#22686;&#24378;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#36807;&#28193;&#39046;&#22495;&#20013;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#28151;&#21512;&#12289;&#23450;&#20041;&#26377;&#25928;&#24230;&#37327;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2109.04684</link><description>&lt;p&gt;
&#29992;&#24471;&#20998;&#24341;&#23548;&#32593;&#32476;&#22686;&#24378;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Unsupervised Anomaly Detection with Score-Guided Network. (arXiv:2109.04684v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24471;&#20998;&#24341;&#23548;&#32593;&#32476;&#65292;&#36890;&#36807;&#24471;&#20998;&#24341;&#23548;&#31574;&#30053;&#22686;&#24378;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#36807;&#28193;&#39046;&#22495;&#20013;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#28151;&#21512;&#12289;&#23450;&#20041;&#26377;&#25928;&#24230;&#37327;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#21253;&#25324;&#21307;&#30103;&#21644;&#37329;&#34701;&#31995;&#32479;&#22312;&#20869;&#30340;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#30001;&#20110;&#36825;&#20123;&#22797;&#26434;&#31995;&#32479;&#20013;&#24322;&#24120;&#26631;&#31614;&#25968;&#37327;&#26377;&#38480;&#65292;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#26080;&#30417;&#30563;&#26041;&#27861;&#38754;&#20020;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#65306;&#65288;&#19968;&#65289;&#22312;&#36807;&#28193;&#39046;&#22495;&#20013;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#65292;&#20854;&#20013;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#39640;&#24230;&#28151;&#21512;&#65307;&#65288;&#20108;&#65289;&#23450;&#20041;&#19968;&#31181;&#26377;&#25928;&#30340;&#24230;&#37327;&#26469;&#26368;&#22823;&#21270;&#22312;&#20551;&#35774;&#31354;&#38388;&#20013;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#35813;&#31354;&#38388;&#26159;&#30001;&#34920;&#31034;&#23398;&#20064;&#22120;&#26500;&#24314;&#30340;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24471;&#20998;&#24341;&#23548;&#32593;&#32476;&#24182;&#37319;&#29992;&#24471;&#20998;&#24341;&#23548;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#21644;&#25193;&#22823;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#20043;&#38388;&#30340;&#24322;&#24120;&#20998;&#25968;&#24046;&#24322;&#12290;&#36890;&#36807;&#36825;&#31181;&#24471;&#20998;&#24341;&#23548;&#31574;&#30053;&#65292;&#34920;&#31034;&#23398;&#20064;&#22120;&#21487;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#36880;&#28176;&#23398;&#20064;&#21040;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#36807;&#28193;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection plays a crucial role in various real-world applications, including healthcare and finance systems. Owing to the limited number of anomaly labels in these complex systems, unsupervised anomaly detection methods have attracted great attention in recent years. Two major challenges faced by the existing unsupervised methods are: (i) distinguishing between normal and abnormal data in the transition field, where normal and abnormal data are highly mixed together; (ii) defining an effective metric to maximize the gap between normal and abnormal data in a hypothesis space, which is built by a representation learner. To that end, this work proposes a novel scoring network with a score-guided regularization to learn and enlarge the anomaly score disparities between normal and abnormal data. With such score-guided strategy, the representation learner can gradually learn more informative representation during the model training stage, especially for the samples in the transition 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#38544;&#34255;&#23618;&#23454;&#29616;&#23545;&#21151;&#33021;&#21709;&#24212;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#27169;&#22411;&#25311;&#21512;&#31574;&#30053;&#65288;FDNN&#21644;FBNN&#65289;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#25216;&#26415;&#24471;&#21040;&#26356;&#21152;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2107.14151</link><description>&lt;p&gt;
&#29616;&#20195;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65306;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Modern Non-Linear Function-on-Function Regression. (arXiv:2107.14151v1 [stat.ME] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.14151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#38544;&#34255;&#23618;&#23454;&#29616;&#23545;&#21151;&#33021;&#21709;&#24212;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#27169;&#22411;&#25311;&#21512;&#31574;&#30053;&#65288;FDNN&#21644;FBNN&#65289;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#25216;&#26415;&#24471;&#21040;&#26356;&#21152;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#31867;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#30001;&#36830;&#32493;&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#38544;&#34255;&#23618;&#65292;&#31216;&#20026;&#36830;&#32493;&#38544;&#34255;&#23618;&#65292;&#29992;&#20110;&#21151;&#33021;&#21709;&#24212;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#27169;&#22411;&#25311;&#21512;&#31574;&#30053;&#65306;&#21151;&#33021;&#30452;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FDNN&#65289;&#21644;&#21151;&#33021;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65288;FBNN&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#26159;&#19987;&#38376;&#35774;&#35745;&#26469;&#21033;&#29992;&#21151;&#33021;&#25968;&#25454;&#22266;&#26377;&#30340;&#32467;&#26500;&#65292;&#24182;&#25429;&#25417;&#21151;&#33021;&#39044;&#27979;&#21464;&#37327;&#21644;&#21151;&#33021;&#21709;&#24212;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#27714;&#35299;&#20989;&#25968;&#26799;&#24230;&#24182;&#23454;&#26045;&#27491;&#21017;&#21270;&#25216;&#26415;&#36827;&#34892;&#27169;&#22411;&#25311;&#21512;&#65292;&#24471;&#21040;&#26356;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#21151;&#33021;&#27169;&#22411;&#26041;&#38754;&#30340;&#24378;&#22823;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new class of non-linear function-on-function regression models for functional data using neural networks. We propose a framework using a hidden layer consisting of continuous neurons, called a continuous hidden layer, for functional response modeling and give two model fitting strategies, Functional Direct Neural Network (FDNN) and Functional Basis Neural Network (FBNN). Both are designed explicitly to exploit the structure inherent in functional data and capture the complex relations existing between the functional predictors and the functional response. We fit these models by deriving functional gradients and implement regularization techniques for more parsimonious results. We demonstrate the power and flexibility of our proposed method in handling complex functional models through extensive simulation studies as well as real data examples.
&lt;/p&gt;</description></item><item><title>GraphFormers&#26159;&#19968;&#31181;&#23558;GNN&#23884;&#22871;&#21040;Transformer&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24335;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#20934;&#30830;&#29702;&#35299;&#25991;&#26412;&#22270;&#20013;&#27599;&#20010;&#33410;&#28857;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#24341;&#20837;&#28176;&#36827;&#24335;&#23398;&#20064;&#21152;&#36895;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2105.02605</link><description>&lt;p&gt;
GraphFormers: GNN&#23884;&#22871;Transformer&#29992;&#20110;&#25991;&#26412;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph. (arXiv:2105.02605v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.02605
&lt;/p&gt;
&lt;p&gt;
GraphFormers&#26159;&#19968;&#31181;&#23558;GNN&#23884;&#22871;&#21040;Transformer&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24335;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#20934;&#30830;&#29702;&#35299;&#25991;&#26412;&#22270;&#20013;&#27599;&#20010;&#33410;&#28857;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#24341;&#20837;&#28176;&#36827;&#24335;&#23398;&#20064;&#21152;&#36895;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#26159;&#22522;&#20110;&#20010;&#20307;&#25991;&#26412;&#29305;&#24449;&#21644;&#37051;&#22495;&#20449;&#24687;&#29983;&#25104;&#33410;&#28857;&#20302;&#32500;&#23884;&#20837;&#30340;&#36807;&#31243;&#12290;&#26368;&#36817;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31361;&#30772;&#25512;&#21160;&#20102;&#30456;&#24212;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#20381;&#36182;&#32423;&#32852;&#27169;&#22411;&#26550;&#26500;&#65306;&#39318;&#20808;&#65292;&#33410;&#28857;&#30340;&#25991;&#26412;&#29305;&#24449;&#30001;&#35821;&#35328;&#27169;&#22411;&#29420;&#31435;&#32534;&#30721;&#65307;&#28982;&#21518;&#65292;&#25991;&#26412;&#23884;&#20837;&#30001;&#22270;&#31070;&#32463;&#32593;&#32476;&#32858;&#21512;&#12290;&#28982;&#32780;&#65292;&#19978;&#36848;&#26550;&#26500;&#30001;&#20110;&#23545;&#25991;&#26412;&#29305;&#24449;&#30340;&#29420;&#31435;&#24314;&#27169;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphFormers&#65292;&#20854;&#20013;GNN&#30340;&#20998;&#23618;&#32452;&#20214;&#23884;&#22871;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;Transformer&#22359;&#26049;&#36793;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26550;&#26500;&#65292;&#25991;&#26412;&#32534;&#30721;&#21644;&#22270;&#32858;&#21512;&#34701;&#21512;&#20026;&#19968;&#20010;&#36845;&#20195;&#24335;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#20174;&#20840;&#23616;&#35270;&#35282;&#20934;&#30830;&#29702;&#35299;&#27599;&#20010;&#33410;&#28857;&#30340;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#28176;&#36827;&#24335;&#23398;&#20064;&#26041;&#27861;&#34987;&#24341;&#20837;&#20197;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation learning on textual graph is to generate low-dimensional embeddings for the nodes based on the individual textual features and the neighbourhood information. Recent breakthroughs on pretrained language models and graph neural networks push forward the development of corresponding techniques. The existing works mainly rely on the cascaded model architecture: the textual features of nodes are independently encoded by language models at first; the textual embeddings are aggregated by graph neural networks afterwards. However, the above architecture is limited due to the independent modeling of textual features. In this work, we propose GraphFormers, where layerwise GNN components are nested alongside the transformer blocks of language models. With the proposed architecture, the text encoding and the graph aggregation are fused into an iterative workflow, {making} each node's semantic accurately comprehended from the global perspective. In addition, a {progressive} learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#20915;&#31574;&#25361;&#25112;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#36890;&#36807;&#24212;&#29992;&#22810;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25511;&#21046;&#20248;&#21270;&#38382;&#39064;&#65292;&#20026;&#33258;&#20027;&#23398;&#20064;&#21644;&#33258;&#25105;&#25913;&#36827;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2008.01302</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39640;&#36895;&#20844;&#36335;&#20915;&#31574;&#20013;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Deep Reinforcement Learning-enabled Freeway Decision-making for Automated Vehicles. (arXiv:2008.01302v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.01302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#20915;&#31574;&#25361;&#25112;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#36890;&#36807;&#24212;&#29992;&#22810;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25511;&#21046;&#20248;&#21270;&#38382;&#39064;&#65292;&#20026;&#33258;&#20027;&#23398;&#20064;&#21644;&#33258;&#25105;&#25913;&#36827;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#30001;&#20110;&#20854;&#22312;&#33258;&#20027;&#23398;&#20064;&#21644;&#33258;&#25105;&#25913;&#36827;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;DRL&#22312;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#38754;&#20020;&#30340;&#20915;&#31574;&#25361;&#25112;&#36827;&#34892;&#20102;&#20960;&#31181;DRL&#26041;&#27861;&#30340;&#32508;&#21512;&#27604;&#36739;&#12290;&#36825;&#20123;&#25216;&#26415;&#21253;&#25324;&#24120;&#35265;&#30340;&#28145;&#24230;Q&#23398;&#20064;(DQL)&#12289;&#21452;&#28145;&#24230;Q&#23398;&#20064;(DDQL)&#12289;&#23545;&#20915;&#28145;&#24230;Q&#23398;&#20064;&#21644;&#20248;&#20808;&#37325;&#25918;&#28145;&#24230;Q&#23398;&#20064;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#24378;&#21270;&#23398;&#20064;(RL)&#26694;&#26550;&#65292;&#28982;&#21518;&#23545;&#19978;&#36848;DRL&#26041;&#27861;&#30340;&#23454;&#29616;&#36827;&#34892;&#20102;&#25968;&#23398;&#24314;&#27169;&#12290;&#38543;&#21518;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39640;&#36895;&#20844;&#36335;&#39550;&#39542;&#22330;&#26223;&#65292;&#23558;&#20915;&#31574;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#25511;&#21046;&#20248;&#21270;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#27169;&#25311;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) has emerged as a pervasive and potent methodology for addressing artificial intelligence challenges. Due to its substantial potential for autonomous self-learning and self-improvement, DRL finds broad applications across various research domains. This article undertakes a comprehensive comparison of several DRL approaches con-cerning the decision-making challenges encountered by autono-mous vehicles on freeways. These techniques encompass common deep Q-learning (DQL), double deep Q-learning (DDQL), dueling deep Q-learning, and prioritized replay deep Q-learning. Initially, the reinforcement learning (RL) framework is introduced, fol-lowed by a mathematical establishment of the implementations of the aforementioned DRL methods. Subsequently, a freeway driving scenario for automated vehicles is constructed, wherein the decision-making problem is reformulated as a control opti-mization challenge. Finally, a series of simulation experiments are conducted t
&lt;/p&gt;</description></item><item><title>&#35745;&#31639;&#27169;&#24335;&#29702;&#35770;&#25552;&#20986;&#20102;&#23545;&#35745;&#31639;&#30340;&#22810;&#26679;&#24615;&#30340;&#23618;&#27425;&#32467;&#26500;&#25551;&#36848;&#65292;&#20197;&#21450;&#23558;&#22823;&#33041;&#20013;&#30340;&#24515;&#29702;&#36807;&#31243;&#35299;&#37322;&#20026;&#35745;&#31639;&#36807;&#31243;&#30340;&#35266;&#28857;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#33258;&#28982;&#35745;&#31639;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/1903.10559</link><description>&lt;p&gt;
&#35745;&#31639;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
The Mode of Computing. (arXiv:1903.10559v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1903.10559
&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#27169;&#24335;&#29702;&#35770;&#25552;&#20986;&#20102;&#23545;&#35745;&#31639;&#30340;&#22810;&#26679;&#24615;&#30340;&#23618;&#27425;&#32467;&#26500;&#25551;&#36848;&#65292;&#20197;&#21450;&#23558;&#22823;&#33041;&#20013;&#30340;&#24515;&#29702;&#36807;&#31243;&#35299;&#37322;&#20026;&#35745;&#31639;&#36807;&#31243;&#30340;&#35266;&#28857;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#33258;&#28982;&#35745;&#31639;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#28789;&#26426;&#26159;&#35745;&#31639;&#26426;&#33539;&#24335;&#30340;&#20856;&#22411;&#26696;&#20363;&#65292;&#20294;&#36824;&#26377;&#20854;&#20182;&#30340;&#35745;&#31639;&#26426;&#65292;&#22914;&#31867;&#27604;&#12289;&#36830;&#25509;&#20027;&#20041;&#12289;&#37327;&#23376;&#21644;&#21508;&#31181;&#38750;&#20256;&#32479;&#35745;&#31639;&#24418;&#24335;&#65292;&#27599;&#31181;&#37117;&#22522;&#20110;&#23545;&#35745;&#31639;&#29616;&#35937;&#30340;&#29305;&#23450;&#30452;&#35273;&#12290;&#36825;&#31181;&#22810;&#26679;&#24615;&#21487;&#20197;&#29992;&#31995;&#32479;&#23618;&#27425;&#26469;&#25429;&#25417;&#65292;&#37325;&#26032;&#35299;&#37322;&#21644;&#27010;&#25324;&#32445;&#22467;&#23572;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#39030;&#37096;&#30340;&#30693;&#35782;&#23618;&#21644;&#31435;&#21363;&#19979;&#38754;&#30340;&#31526;&#21495;&#23618;&#12290;&#22312;&#36825;&#20010;&#37325;&#26032;&#35299;&#37322;&#20013;&#65292;&#30693;&#35782;&#23618;&#21253;&#25324;&#20154;&#31867;&#30693;&#35782;&#65292;&#31526;&#21495;&#23618;&#34987;&#27867;&#21270;&#20026;&#19968;&#20010;&#26032;&#23618;&#32423;&#65292;&#36825;&#37324;&#31216;&#20026;&#35745;&#31639;&#27169;&#24335;&#12290;&#30001;&#33258;&#28982;&#22823;&#33041;&#25191;&#34892;&#30340;&#24515;&#29702;&#36807;&#31243;&#32463;&#24120;&#34987;&#38750;&#27491;&#24335;&#22320;&#35748;&#20026;&#26159;&#35745;&#31639;&#36807;&#31243;&#65292;&#32780;&#22823;&#33041;&#31867;&#20284;&#20110;&#35745;&#31639;&#26426;&#22120;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#33258;&#28982;&#35745;&#31639;&#30830;&#23454;&#23384;&#22312;&#65292;&#23427;&#24212;&#35813;&#26377;&#33258;&#24049;&#30340;&#29305;&#24449;&#12290;&#23545;&#27492;&#25552;&#35758;&#26159;&#65292;&#33258;&#28982;&#35745;&#31639;&#20986;&#29616;&#22312;&#29983;&#29289;&#23454;&#20307;&#39318;&#27425;&#36827;&#34892;&#35299;&#37322;&#26102;&#65292;&#22240;&#27492;&#33258;&#28982;&#35745;&#31639;&#21644;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Turing Machine is the paradigmatic case of computing machines, but there are others such as analogical, connectionist, quantum and diverse forms of unconventional computing, each based on a particular intuition of the phenomenon of computing. This variety can be captured in terms of system levels, re-interpreting and generalizing Newell's hierarchy, which includes the knowledge level at the top and the symbol level immediately below it. In this re-interpretation the knowledge level consists of human knowledge and the symbol level is generalized into a new level that here is called The Mode of Computing. Mental processes performed by natural brains are often thought of informally as computing process and that the brain is alike to computing machinery. However, if natural computing does exist it should be characterized on its own. A proposal to such an effect is that natural computing appeared when interpretations were first made by biological entities, so natural computing and inter
&lt;/p&gt;</description></item></channel></rss>