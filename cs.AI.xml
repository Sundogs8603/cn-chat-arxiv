<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#35843;&#24230;&#25216;&#26415;&#65292;&#38024;&#23545;&#21333;&#20010;&#38382;&#39064;&#23454;&#20363;&#37319;&#29992;&#36866;&#24212;&#24615;&#26694;&#26550;&#65292;&#36229;&#36234;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24037;&#20316;&#65292;&#21516;&#26102;&#25511;&#21046;&#20004;&#20010;&#19981;&#21516;&#31867;&#22411;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32463;&#36807;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#26377;&#25928;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03755</link><description>&lt;p&gt;
&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21551;&#21457;&#24335;&#35843;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Learning for Scheduling MIP Heuristics. (arXiv:2304.03755v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#35843;&#24230;&#25216;&#26415;&#65292;&#38024;&#23545;&#21333;&#20010;&#38382;&#39064;&#23454;&#20363;&#37319;&#29992;&#36866;&#24212;&#24615;&#26694;&#26550;&#65292;&#36229;&#36234;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24037;&#20316;&#65292;&#21516;&#26102;&#25511;&#21046;&#20004;&#20010;&#19981;&#21516;&#31867;&#22411;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32463;&#36807;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#26377;&#25928;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;(MIP)&#26159;NP&#38590;&#38382;&#39064;&#65292;&#20294;&#29616;&#20195;&#27714;&#35299;&#22120;&#36890;&#24120;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#35299;&#20915;&#22823;&#22411;&#23454;&#38469;&#38382;&#39064;&#12290;&#36825;&#31181;&#25104;&#21151;&#37096;&#20998;&#24402;&#21151;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#34892;&#20026;&#39640;&#24230;&#20381;&#36182;&#20110;&#23454;&#20363;&#65292;&#22240;&#27492;&#20381;&#36182;&#20110;&#20174;&#22823;&#22411;&#24322;&#26500;&#22522;&#20934;&#23454;&#20363;&#30340;&#32463;&#39564;&#27979;&#35797;&#25512;&#23548;&#20986;&#30340;&#30828;&#32534;&#30721;&#35268;&#21017;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#21551;&#21457;&#24335;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#24403;&#21069;&#38382;&#39064;&#23454;&#20363;&#12290;&#25105;&#20204;&#29992;&#19968;&#20010;&#36866;&#24212;&#24615;&#26694;&#26550;&#21462;&#20195;&#20102;&#36890;&#24120;&#20351;&#29992;&#30340;&#38745;&#24577;&#21551;&#21457;&#24335;&#22788;&#29702;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20851;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#34892;&#20026;&#30340;&#36807;&#21435;&#35266;&#23519;&#32467;&#26524;&#26469;&#20570;&#20986;&#26410;&#26469;&#20915;&#31574;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#25511;&#21046;&#22823;&#37051;&#22495;&#25628;&#32034;&#21644;&#28508;&#27700; - &#20004;&#20010;&#24191;&#27867;&#19988;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#38382;&#39064;&#24314;&#27169;&#20026;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36229;&#36234;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#36890;&#36807;&#21333;&#20010;&#23398;&#20064;&#20195;&#29702;&#21516;&#26102;&#25511;&#21046;&#20102;&#20004;&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;MIPLIB 2017&#22522;&#20934;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21551;&#21457;&#24335;&#22788;&#29702;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixed Integer Programming (MIP) is NP-hard, and yet modern solvers often solve large real-world problems within minutes. This success can partially be attributed to heuristics. Since their behavior is highly instance-dependent, relying on hard-coded rules derived from empirical testing on a large heterogeneous corpora of benchmark instances might lead to sub-optimal performance. In this work, we propose an online learning approach that adapts the application of heuristics towards the single instance at hand. We replace the commonly used static heuristic handling with an adaptive framework exploiting past observations about the heuristic's behavior to make future decisions. In particular, we model the problem of controlling Large Neighborhood Search and Diving - two broad and complex classes of heuristics as a multi-armed bandit problem. Going beyond existing work in the literature, we control two different classes of heuristics simultaneously by a single learning agent. We verify our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#26550;&#26500;&#21644;&#20849;&#35774;&#35745;&#23545;&#22320;&#29699;&#31995;&#32479;&#21487;&#39044;&#27979;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#21457;&#23637;&#26032;&#30340;&#26550;&#26500;&#21644;&#31574;&#30053;&#20197;&#20419;&#36827;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#22320;&#29699;&#31995;&#32479;&#24314;&#27169;&#21644;&#39044;&#27979;&#39046;&#22495;&#30340;&#21457;&#23637;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.03748</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26550;&#26500;&#21644;&#20849;&#35774;&#35745;&#23545;&#22320;&#29699;&#31995;&#32479;&#21487;&#39044;&#27979;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Perspectives on AI Architectures and Co-design for Earth System Predictability. (arXiv:2304.03748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#26550;&#26500;&#21644;&#20849;&#35774;&#35745;&#23545;&#22320;&#29699;&#31995;&#32479;&#21487;&#39044;&#27979;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#21457;&#23637;&#26032;&#30340;&#26550;&#26500;&#21644;&#31574;&#30053;&#20197;&#20419;&#36827;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#22320;&#29699;&#31995;&#32479;&#24314;&#27169;&#21644;&#39044;&#27979;&#39046;&#22495;&#30340;&#21457;&#23637;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#33021;&#28304;&#37096;&#65288;DOE&#65289;&#31185;&#23398;&#21150;&#20844;&#23460;&#29983;&#29289;&#21644;&#29615;&#22659;&#30740;&#31350;&#65288;BER&#65289;&#21644;&#39640;&#32423;&#31185;&#23398;&#35745;&#31639;&#30740;&#31350;&#65288;ASCR&#65289;&#35745;&#21010;&#26368;&#36817;&#32452;&#32455;&#24182;&#20030;&#21150;&#20102;&#8220;&#38754;&#21521;&#22320;&#29699;&#31995;&#32479;&#21487;&#39044;&#27979;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI4ESP&#65289;&#8221;&#30740;&#35752;&#20250;&#31995;&#21015;&#12290;&#20174;&#36825;&#20010;&#30740;&#35752;&#20250;&#20013;&#65292;DOE BER&#21644;ASCR&#31038;&#21306;&#24471;&#20986;&#30340;&#19968;&#20010;&#20851;&#38190;&#32467;&#35770;&#26159;&#38656;&#35201;&#21457;&#23637;&#19968;&#20010;&#26032;&#30340;&#22320;&#29699;&#31995;&#32479;&#21487;&#39044;&#27979;&#24615;&#33539;&#24335;&#65292;&#37325;&#28857;&#26159;&#22312;&#25972;&#20010;&#39046;&#22495;&#12289;&#23454;&#39564;&#23460;&#12289;&#24314;&#27169;&#21644;&#20998;&#26512;&#27963;&#21160;&#20013;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#31216;&#20026;ModEx&#12290;BER&#30340;&#8220;&#27169;&#22411;&#23454;&#39564;&#8221;&#65292;ModEx&#65292;&#26159;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#20351;&#36807;&#31243;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20551;&#35774;&#12290;&#25152;&#24320;&#21457;&#30340;&#20551;&#35774;&#36890;&#30693;&#37319;&#38598;&#27979;&#37327;&#21644;&#35266;&#27979;&#25968;&#25454;&#30340;&#29616;&#22330;&#21644;&#23454;&#39564;&#23460;&#24037;&#20316;&#65292;&#38543;&#21518;&#29992;&#20110;&#21442;&#25968;&#21270;&#12289;&#39537;&#21160;&#21644;&#27979;&#35797;&#27169;&#22411;&#65288;&#20363;&#22914;&#22522;&#20110;&#36807;&#31243;&#30340;&#65289;&#39044;&#27979;&#12290;&#22312;&#36825;&#20010;AI4ESP&#24037;&#20316;&#22346;&#31995;&#21015;&#20013;&#20849;&#20030;&#34892;&#20102;17&#20010;&#25216;&#26415;&#20250;&#35758;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20854;&#20013;&#19968;&#27425;&#20250;&#35758;&#30340;&#20027;&#39064;&#8220;&#20154;&#24037;&#26234;&#33021;&#26550;&#26500;&#21644;&#20849;&#35774;&#35745;&#23545;&#22320;&#29699;&#31995;&#32479;&#21487;&#39044;&#27979;&#24615;&#30340;&#24433;&#21709;&#8221;&#12290;&#23427;&#25552;&#20379;&#20102;&#21457;&#23637;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#26550;&#26500;&#21644;&#20849;&#35774;&#35745;&#31574;&#30053;&#26469;&#35299;&#20915;&#22320;&#29699;&#31995;&#32479;&#24314;&#27169;&#21644;&#39044;&#27979;&#39046;&#22495;&#29305;&#23450;&#30340;&#31185;&#23398;&#21644;&#25216;&#26415;&#38656;&#27714;&#65292;&#20197;&#25512;&#36827;&#20154;&#24037;&#26234;&#33021;&#22312;&#22320;&#29699;&#31995;&#32479;&#21487;&#39044;&#27979;&#24615;&#20013;&#30340;&#20316;&#29992;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the U.S. Department of Energy (DOE), Office of Science, Biological and Environmental Research (BER), and Advanced Scientific Computing Research (ASCR) programs organized and held the Artificial Intelligence for Earth System Predictability (AI4ESP) workshop series. From this workshop, a critical conclusion that the DOE BER and ASCR community came to is the requirement to develop a new paradigm for Earth system predictability focused on enabling artificial intelligence (AI) across the field, lab, modeling, and analysis activities, called ModEx. The BER's `Model-Experimentation', ModEx, is an iterative approach that enables process models to generate hypotheses. The developed hypotheses inform field and laboratory efforts to collect measurement and observation data, which are subsequently used to parameterize, drive, and test model (e.g., process-based) predictions. A total of 17 technical sessions were held in this AI4ESP workshop series. This paper discusses the topic of the `
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#21487;&#20197;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#20197;&#21450;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2304.03724</link><description>&lt;p&gt;
&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Predicting quantum chemical property with easy-to-obtain geometry via positional denoising. (arXiv:2304.03724v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#21487;&#20197;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#20197;&#21450;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#19982;&#20854;&#20960;&#20309;&#32467;&#26500;&#26377;&#37325;&#35201;&#20851;&#32852;&#65292;&#20351;&#29992;3D&#20960;&#20309;&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#39640;&#32423;&#37327;&#23376;&#21147;&#23398;&#35745;&#31639;&#24471;&#20986;&#30340;3D&#20960;&#20309;&#32467;&#26500;&#65292;&#36825;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#29616;&#23454;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65288;&#20363;&#22914;&#26469;&#33258;&#20998;&#23376;&#21147;&#22330;&#30340;&#20248;&#21270;&#20960;&#20309;&#32467;&#26500;&#65289;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#36755;&#20837;&#20960;&#20309;&#32467;&#26500;&#36880;&#28176;&#25509;&#36817;&#27491;&#30830;&#20960;&#20309;&#32467;&#26500;&#65292;&#36890;&#36807;&#22534;&#21472;&#21435;&#22122;&#23618;&#12290;&#25105;&#20204;&#20351;&#29992;3D&#28040;&#24687;&#20256;&#36882;&#20307;&#31995;&#32467;&#26500;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#65288;&#20998;&#23376;&#24615;&#36136;&#21644;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#65289;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21435;&#22122;&#36807;&#31243;&#20943;&#23569;&#20301;&#32622;&#35823;&#24046;&#26377;&#21161;&#20110;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
As quantum chemical properties have a significant dependence on their geometries, graph neural networks (GNNs) using 3D geometric information have achieved high prediction accuracy in many tasks. However, they often require 3D geometries obtained from high-level quantum mechanical calculations, which are practically infeasible, limiting their applicability in real-world problems. To tackle this, we propose a method to accurately predict the properties with relatively easy-to-obtain geometries (e.g., optimized geometries from the molecular force field). In this method, the input geometry, regarded as the corrupted geometry of the correct one, gradually approaches the correct one as it passes through the stacked denoising layers. We investigated the performance of the proposed method using 3D message-passing architectures for two prediction tasks: molecular properties and chemical reaction property. The reduction of positional errors through the denoising process contributed to performan
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#30340;&#26032;&#30340;&#34920;&#29616;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#20197;&#19977;&#20803;&#32452;&#27604;&#36739;&#20026;&#22522;&#30784;&#30340;&#34920;&#29616;&#23450;&#29702;&#38382;&#39064;&#12290;&#36825;&#31181;&#34920;&#29616;&#23450;&#29702;&#21487;&#20197;&#29992;&#20869;&#31215;&#35825;&#23548;&#30340;&#33539;&#25968;&#26469;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.03720</link><description>&lt;p&gt;
&#24230;&#37327;&#23398;&#20064;&#19982;&#20559;&#22909;&#23398;&#20064;&#30340;&#34920;&#29616;&#23450;&#29702;&#65306;&#22522;&#20110;&#20960;&#20309;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Representer Theorems for Metric and Preference Learning: A Geometric Perspective. (arXiv:2304.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03720
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#30340;&#26032;&#30340;&#34920;&#29616;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#20197;&#19977;&#20803;&#32452;&#27604;&#36739;&#20026;&#22522;&#30784;&#30340;&#34920;&#29616;&#23450;&#29702;&#38382;&#39064;&#12290;&#36825;&#31181;&#34920;&#29616;&#23450;&#29702;&#21487;&#20197;&#29992;&#20869;&#31215;&#35825;&#23548;&#30340;&#33539;&#25968;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#34920;&#29616;&#23450;&#29702;&#21487;&#20197;&#26681;&#25454;&#38382;&#39064;&#32467;&#26500;&#20869;&#22312;&#30340;&#20869;&#31215;&#25152;&#35825;&#23548;&#30340;&#33539;&#25968;&#26469;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#19977;&#20803;&#32452;&#27604;&#36739;&#30340;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#23427;&#23548;&#33268;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#33258;&#21253;&#21547;&#30340;&#35813;&#20219;&#21153;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#21487;&#20197;&#20351;&#29992;&#31867;&#20284;&#20110;&#32463;&#20856;&#34920;&#29616;&#23450;&#29702;&#30340;&#26680;&#26415;&#35821;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the metric and preference learning problem in Hilbert spaces. We obtain a novel representer theorem for the simultaneous task of metric and preference learning. Our key observation is that the representer theorem can be formulated with respect to the norm induced by the inner product inherent in the problem structure. Additionally, we demonstrate how our framework can be applied to the task of metric learning from triplet comparisons and show that it leads to a simple and self-contained representer theorem for this task. In the case of Reproducing Kernel Hilbert Spaces (RKHS), we demonstrate that the solution to the learning problem can be expressed using kernel terms, akin to classical representer theorems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HumanLight&#30340;&#31639;&#27861;&#65292;&#37319;&#29992;&#20154;&#24615;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20013;&#28608;&#21169;&#22823;&#23478;&#25340;&#36710;&#65292;&#32531;&#35299;&#20132;&#36890;&#25317;&#22581;&#21644;&#20943;&#23569;&#27745;&#26579;&#12290;&#31639;&#27861;&#36890;&#36807;&#22870;&#21169;&#20056;&#22352;&#22823;&#23481;&#37327;&#36733;&#23458;&#24037;&#20855;&#30340;&#36890;&#21220;&#32773;&#65292;&#23454;&#29616;&#23545;&#32511;&#28783;&#26102;&#38388;&#30340;&#20844;&#24179;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2304.03697</link><description>&lt;p&gt;
&#20154;&#24615;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;&#65306;&#36890;&#36807;&#28608;&#21169;&#25340;&#36710;&#26469;&#32531;&#35299;&#20132;&#36890;&#25317;&#22581;&#21644;&#20943;&#23569;&#27745;&#26579;
&lt;/p&gt;
&lt;p&gt;
HumanLight: Incentivizing Ridesharing via Human-centric Deep Reinforcement Learning in Traffic Signal Control. (arXiv:2304.03697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HumanLight&#30340;&#31639;&#27861;&#65292;&#37319;&#29992;&#20154;&#24615;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20013;&#28608;&#21169;&#22823;&#23478;&#25340;&#36710;&#65292;&#32531;&#35299;&#20132;&#36890;&#25317;&#22581;&#21644;&#20943;&#23569;&#27745;&#26579;&#12290;&#31639;&#27861;&#36890;&#36807;&#22870;&#21169;&#20056;&#22352;&#22823;&#23481;&#37327;&#36733;&#23458;&#24037;&#20855;&#30340;&#36890;&#21220;&#32773;&#65292;&#23454;&#29616;&#23545;&#32511;&#28783;&#26102;&#38388;&#30340;&#20844;&#24179;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#20154;&#39550;&#36710;&#24050;&#25104;&#20026;&#35768;&#22810;&#36890;&#21220;&#32773;&#26368;&#21463;&#38738;&#30544;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#23548;&#33268;&#20132;&#36890;&#25317;&#22581;&#21644;&#31354;&#27668;&#27745;&#26579;&#38382;&#39064;&#21152;&#21095;&#12290;&#20449;&#24687;&#25216;&#26415;&#30340;&#36827;&#27493;&#20026;&#23454;&#29616;&#22478;&#24066;&#8220;&#36731;&#36710;&#21270;&#8221;&#25552;&#20379;&#20102;&#26426;&#36935;&#65292;&#21487;&#20197;&#36890;&#36807;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#28608;&#21169;&#25340;&#36710;&#21644;&#20999;&#25442;&#21040;&#22823;&#23481;&#37327;&#36733;&#23458;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;HumanLight&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#20998;&#25955;&#24335;&#33258;&#36866;&#24212;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31639;&#27861;&#65292;&#26088;&#22312;&#20248;&#21270;&#20132;&#21449;&#21475;&#30340;&#20154;&#21592;&#36890;&#34892;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#22120;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65292;&#22870;&#21169;&#20989;&#25968;&#23884;&#20837;&#20102;&#20197;&#20154;&#20026;&#26412;&#30340;&#20132;&#36890;&#27010;&#24565;&#12290;&#36890;&#36807;&#28608;&#21169;&#22823;&#23481;&#37327;&#36733;&#23458;&#24037;&#20855;&#30340;&#36890;&#21220;&#32773;&#21512;&#24182;&#20056;&#36710;&#20197;&#33410;&#32422;&#26102;&#38388;&#65292;HumanLight&#23454;&#29616;&#20102;&#23545;&#32511;&#28783;&#26102;&#38388;&#30340;&#20844;&#24179;&#20998;&#37197;&#12290;&#38500;&#20102;&#37319;&#29992;FRAP&#20316;&#20026;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#22806;&#65292;HumanLight&#36824;&#24341;&#20837;&#20102;&#8220;&#27963;&#36291;&#36710;&#36742;&#8221;&#36825;&#20010;&#27010;&#24565;&#65292;&#22823;&#33268;&#23450;&#20041;&#26159;&#20020;&#36817;&#20132;&#21449;&#21475;&#19988;&#21487;&#33021;&#24178;&#25200;&#20915;&#31574;&#36807;&#31243;&#30340;&#20219;&#20309;&#36710;&#36742;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single occupancy vehicles are the most attractive transportation alternative for many commuters, leading to increased traffic congestion and air pollution. Advancements in information technologies create opportunities for smart solutions that incentivize ridesharing and mode shift to higher occupancy vehicles (HOVs) to achieve the car lighter vision of cities. In this study, we present HumanLight, a novel decentralized adaptive traffic signal control algorithm designed to optimize people throughput at intersections. Our proposed controller is founded on reinforcement learning with the reward function embedding the transportation-inspired concept of pressure at the person-level. By rewarding HOV commuters with travel time savings for their efforts to merge into a single ride, HumanLight achieves equitable allocation of green times. Apart from adopting FRAP, a state-of-the-art (SOTA) base model, HumanLight introduces the concept of active vehicles, loosely defined as vehicles in proximit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#24694;&#24847;&#27969;&#37327;&#26816;&#27979;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#21152;&#23494;&#24694;&#24847;&#27969;&#37327;&#30340;&#29305;&#24449;&#21644;&#27010;&#24565;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26816;&#27979;&#26694;&#26550;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#26816;&#27979;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.03691</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21152;&#23494;&#24694;&#24847;&#27969;&#37327;&#26816;&#27979;&#20013;&#30340;&#29305;&#24449;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Feature Mining for Encrypted Malicious Traffic Detection with Deep Learning and Other Machine Learning Algorithms. (arXiv:2304.03691v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#24694;&#24847;&#27969;&#37327;&#26816;&#27979;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#21152;&#23494;&#24694;&#24847;&#27969;&#37327;&#30340;&#29305;&#24449;&#21644;&#27010;&#24565;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26816;&#27979;&#26694;&#26550;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#26816;&#27979;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#23494;&#26426;&#21046;&#30340;&#26222;&#21450;&#23545;&#24694;&#24847;&#27969;&#37327;&#26816;&#27979;&#25552;&#20986;&#20102;&#26497;&#22823;&#30340;&#25361;&#25112;&#65292;&#20256;&#32479;&#26816;&#27979;&#25216;&#26415;&#22312;&#27809;&#26377;&#23545;&#21152;&#23494;&#27969;&#37327;&#35299;&#23494;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#24037;&#20316;&#12290;&#24403;&#21069;&#65292;&#23545;&#20110;&#19981;&#35299;&#23494;&#30340;&#21152;&#23494;&#24694;&#24847;&#27969;&#37327;&#26816;&#27979;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#26426;&#22120;&#23398;&#20064;&#25110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#36873;&#25321;&#19978;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20379;&#27969;&#37327;&#29305;&#24449;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#27969;&#37327;&#29305;&#24449;&#21019;&#24314;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#21152;&#23494;&#24694;&#24847;&#27969;&#37327;&#20998;&#26512;&#35774;&#35745;&#30340;&#26032;&#27010;&#24565;&#21644;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21152;&#23494;&#24694;&#24847;&#27969;&#37327;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#20004;&#23618;&#30340;&#26816;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#65292;&#35813;&#26694;&#26550;&#22312;&#26816;&#27979;&#20934;&#30830;&#24230;&#21644;&#20551;&#38451;&#24615;&#29575;&#26041;&#38754;&#20248;&#20110;&#32463;&#20856;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;ResNet&#21644;&#38543;&#26426;&#26862;&#26519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of encryption mechanisms poses a great challenge to malicious traffic detection. The reason is traditional detection techniques cannot work without the decryption of encrypted traffic. Currently, research on encrypted malicious traffic detection without decryption has focused on feature extraction and the choice of machine learning or deep learning algorithms. In this paper, we first provide an in-depth analysis of traffic features and compare different state-of-the-art traffic feature creation approaches, while proposing a novel concept for encrypted traffic feature which is specifically designed for encrypted malicious traffic analysis. In addition, we propose a framework for encrypted malicious traffic detection. The framework is a two-layer detection framework which consists of both deep learning and traditional machine learning algorithms. Through comparative experiments, it outperforms classical deep learning and traditional machine learning algorithms, such as Res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINN)&#21644;&#22810;&#31181;&#25913;&#36827;&#24615;&#33021;&#30340;&#26032;&#25216;&#26415;&#26469;&#35299;&#20915;&#38590;&#20197;&#27714;&#35299;&#30340; Navier-Stokes&#26041;&#31243;(NSE)&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20108;&#32500;NSE&#12290;</title><link>http://arxiv.org/abs/2304.03689</link><description>&lt;p&gt;
EPINN-NSE: &#22686;&#24378;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#29992;&#20110;&#27714;&#35299;Navier-Stokes&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
EPINN-NSE: Enhanced Physics-Informed Neural Networks for Solving Navier-Stokes Equations. (arXiv:2304.03689v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINN)&#21644;&#22810;&#31181;&#25913;&#36827;&#24615;&#33021;&#30340;&#26032;&#25216;&#26415;&#26469;&#35299;&#20915;&#38590;&#20197;&#27714;&#35299;&#30340; Navier-Stokes&#26041;&#31243;(NSE)&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20108;&#32500;NSE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#20307;&#21147;&#23398;&#26159;&#24037;&#31243;&#21644;&#31185;&#23398;&#20013;&#30340;&#19968;&#39033;&#22522;&#30784;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINN)&#21644;&#22810;&#31181;&#25913;&#36827;&#24615;&#33021;&#30340;&#26032;&#25216;&#26415;&#26469;&#35299;&#20915;&#38590;&#20197;&#27714;&#35299;&#30340;Navier-Stokes&#26041;&#31243;(NSE)&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20108;&#32500;NSE&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35299;&#20915;&#19977;&#32500;NSE&#12290;
&lt;/p&gt;
&lt;p&gt;
Fluid mechanics is a fundamental field in engineering and science. Solving the Navier-Stokes equation (NSE) is critical for understanding the behavior of fluids. However, the NSE is a complex partial differential equation that is difficult to solve, and classical numerical methods can be computationally expensive. In this paper, we present an innovative approach for solving the NSE using Physics Informed Neural Networks (PINN) and several novel techniques that improve their performance. The first model is based on an assumption that involves approximating the velocity component by employing the derivative of a stream function. This assumption serves to simplify the system and guarantees that the velocity adheres to the divergence-free equation. We also developed a second more flexible model that approximates the solution without any assumptions. The proposed models can effectively solve two-dimensional NSE. Moreover, we successfully applied the second model to solve the three-dimension
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#24102;&#38656;&#27714;&#30340;&#26426;&#22120;&#23398;&#20064;&#23459;&#35328;&#65292;&#35748;&#20026;&#38656;&#27714;&#23450;&#20041;&#21644;&#28385;&#36275;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#65288;i&#65289;&#38656;&#27714;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#25110;&#21487;&#20197;&#25104;&#21151;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#65288;iii&#65289;&#24573;&#30053;&#38656;&#27714;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37329;&#23383;&#22612;&#24335;&#24320;&#21457;&#27969;&#31243;&#65292;&#22312;&#20854;&#20013;&#65292;&#38656;&#27714;&#23450;&#20041;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#27969;&#31243;&#20013;&#25152;&#26377;&#21518;&#32493;&#38454;&#27573;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;</title><link>http://arxiv.org/abs/2304.03674</link><description>&lt;p&gt;
&#24102;&#38656;&#27714;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#20221;&#23459;&#35328;
&lt;/p&gt;
&lt;p&gt;
Machine Learning with Requirements: a Manifesto. (arXiv:2304.03674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#24102;&#38656;&#27714;&#30340;&#26426;&#22120;&#23398;&#20064;&#23459;&#35328;&#65292;&#35748;&#20026;&#38656;&#27714;&#23450;&#20041;&#21644;&#28385;&#36275;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#65288;i&#65289;&#38656;&#27714;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#25110;&#21487;&#20197;&#25104;&#21151;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#65288;iii&#65289;&#24573;&#30053;&#38656;&#27714;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37329;&#23383;&#22612;&#24335;&#24320;&#21457;&#27969;&#31243;&#65292;&#22312;&#20854;&#20013;&#65292;&#38656;&#27714;&#23450;&#20041;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#27969;&#31243;&#20013;&#25152;&#26377;&#21518;&#32493;&#38454;&#27573;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#38271;&#36275;&#30340;&#36827;&#27493;&#65292;&#25104;&#20026;&#35768;&#22810;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#31361;&#30772;&#30340;&#26681;&#28304;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#23427;&#20204;&#24212;&#29992;&#21040;&#39640;&#39118;&#38505;&#25110;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#23481;&#26131;&#21464;&#24471;&#33030;&#24369;&#21644;&#19981;&#21487;&#38752;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#38656;&#27714;&#23450;&#20041;&#21644;&#28385;&#36275;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#65288;i&#65289;&#38656;&#27714;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#25110;&#21487;&#20197;&#25104;&#21151;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#65288;iii&#65289;&#24573;&#30053;&#38656;&#27714;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#38656;&#27714;&#35268;&#26684;&#35828;&#26126;&#26377;&#30410;&#22320;&#25972;&#21512;&#21040;&#26631;&#20934;&#30340;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#27969;&#31243;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37329;&#23383;&#22612;&#24335;&#24320;&#21457;&#27969;&#31243;&#65292;&#22312;&#20854;&#20013;&#65292;&#38656;&#27714;&#23450;&#20041;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#27969;&#31243;&#20013;&#25152;&#26377;&#21518;&#32493;&#38454;&#27573;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the recent years, machine learning has made great advancements that have been at the root of many breakthroughs in different application domains. However, it is still an open issue how make them applicable to high-stakes or safety-critical application domains, as they can often be brittle and unreliable. In this paper, we argue that requirements definition and satisfaction can go a long way to make machine learning models even more fitting to the real world, especially in critical domains. To this end, we present two problems in which (i) requirements arise naturally, (ii) machine learning models are or can be fruitfully deployed, and (iii) neglecting the requirements can have dramatic consequences. We show how the requirements specification can be fruitfully integrated into the standard machine learning development pipeline, proposing a novel pyramid development process in which requirements definition may impact all the subsequent phases in the pipeline, and viceversa.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ARIA&#30340;&#27169;&#22359;&#65292;&#21487;&#22312;&#20219;&#20309;QD&#31639;&#27861;&#20013;&#25552;&#39640;&#23384;&#26723;&#20013;&#23384;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#27010;&#29575;&#21644;&#36866;&#24212;&#24230;&#36827;&#34892;&#21464;&#24322;&#65292;&#20174;&#32780;&#24212;&#23545;&#19981;&#21487;&#39044;&#27979;&#30340;&#22122;&#38899;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2304.03672</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#38752;&#36816;&#27668;&#65306;&#25552;&#39640;&#36136;&#37327;&#22810;&#26679;&#24615;&#35299;&#20915;&#26041;&#26696;&#22312;&#19981;&#21487;&#39044;&#27979;&#39046;&#22495;&#20013;&#30340;&#34892;&#20026;&#37325;&#22797;&#24615;
&lt;/p&gt;
&lt;p&gt;
Don't Bet on Luck Alone: Enhancing Behavioral Reproducibility of Quality-Diversity Solutions in Uncertain Domains. (arXiv:2304.03672v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ARIA&#30340;&#27169;&#22359;&#65292;&#21487;&#22312;&#20219;&#20309;QD&#31639;&#27861;&#20013;&#25552;&#39640;&#23384;&#26723;&#20013;&#23384;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#27010;&#29575;&#21644;&#36866;&#24212;&#24230;&#36827;&#34892;&#21464;&#24322;&#65292;&#20174;&#32780;&#24212;&#23545;&#19981;&#21487;&#39044;&#27979;&#30340;&#22122;&#38899;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#31639;&#27861;&#26088;&#22312;&#22312;&#32473;&#23450;&#25551;&#36848;&#31526;&#31354;&#38388;&#20013;&#29983;&#25104;&#20248;&#31168;&#35299;&#20915;&#26041;&#26696;&#30340;&#38598;&#21512;&#24182;&#26368;&#22823;&#21270;&#23427;&#20204;&#30340;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#19981;&#21487;&#39044;&#27979;&#30340;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#19968;&#35299;&#20915;&#26041;&#26696;&#22312;&#19981;&#21516;&#35780;&#20272;&#20013;&#30340;&#36866;&#24212;&#24230;&#21644;&#25551;&#36848;&#31526;&#21487;&#33021;&#20250;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#23548;&#33268;&#20272;&#35745;&#36825;&#20123;&#20540;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#37492;&#20110;QD&#31639;&#27861;&#30340;&#31934;&#33521;&#20027;&#20041;&#26412;&#36136;&#65292;&#22312;&#36825;&#20123;&#22024;&#26434;&#30340;&#29615;&#22659;&#19979;&#65292;&#23427;&#20204;&#36890;&#24120;&#20250;&#24471;&#21040;&#35768;&#22810;&#36864;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;&#26723;&#26696;&#21487;&#37325;&#29616;&#24615;&#25913;&#36827;&#31639;&#27861;&#8221;(ARIA)&#65307;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#23384;&#26723;&#20013;&#23384;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;&#25105;&#20204;&#23558;&#20854;&#25552;&#35758;&#20026;&#19968;&#31181;&#21333;&#29420;&#30340;&#20248;&#21270;&#27169;&#22359;&#65292;&#20381;&#36182;&#20110;&#33258;&#28982;&#36827;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;QD&#31639;&#27861;&#30340;&#39030;&#37096;&#25191;&#34892;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#23545;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#21464;&#24322;&#65292;&#20197;(1)&#20248;&#21270;&#20854;&#23646;&#20110;&#33258;&#24049;&#30340;&#39046;&#22495;&#30340;&#27010;&#29575;&#65292;&#21644;(2)&#26368;&#22823;&#21270;&#23427;&#20204;&#30340;&#36866;&#24212;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity (QD) algorithms are designed to generate collections of high-performing solutions while maximizing their diversity in a given descriptor space. However, in the presence of unpredictable noise, the fitness and descriptor of the same solution can differ significantly from one evaluation to another, leading to uncertainty in the estimation of such values. Given the elitist nature of QD algorithms, they commonly end up with many degenerate solutions in such noisy settings. In this work, we introduce Archive Reproducibility Improvement Algorithm (ARIA); a plug-and-play approach that improves the reproducibility of the solutions present in an archive. We propose it as a separate optimization module, relying on natural evolution strategies, that can be executed on top of any QD algorithm. Our module mutates solutions to (1) optimize their probability of belonging to their niche, and (2) maximize their fitness. The performance of our method is evaluated on various tasks, incl
&lt;/p&gt;</description></item><item><title>RSPT&#26694;&#26550;&#36890;&#36807;&#37325;&#26500;&#29615;&#22659;&#21644;&#39044;&#27979;&#30446;&#26631;&#36712;&#36857;&#23454;&#29616;&#20102;&#19968;&#33324;&#21270;&#20027;&#21160;&#29289;&#20307;&#36319;&#36394;&#12290;</title><link>http://arxiv.org/abs/2304.03623</link><description>&lt;p&gt;
RSPT: &#29992;&#20110;&#19968;&#33324;&#21270;&#20027;&#21160;&#29289;&#20307;&#36319;&#36394;&#30340;&#37325;&#26500;&#29615;&#22659;&#21644;&#39044;&#27979;&#36712;&#36857;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RSPT: Reconstruct Surroundings and Predict Trajectories for Generalizable Active Object Tracking. (arXiv:2304.03623v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03623
&lt;/p&gt;
&lt;p&gt;
RSPT&#26694;&#26550;&#36890;&#36807;&#37325;&#26500;&#29615;&#22659;&#21644;&#39044;&#27979;&#30446;&#26631;&#36712;&#36857;&#23454;&#29616;&#20102;&#19968;&#33324;&#21270;&#20027;&#21160;&#29289;&#20307;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#29289;&#20307;&#36319;&#36394;&#65288;AOT&#65289;&#26088;&#22312;&#36890;&#36807;&#33258;&#20027;&#25511;&#21046;&#36861;&#36394;&#22120;&#30340;&#36816;&#21160;&#31995;&#32479;&#26469;&#32500;&#25345;&#36319;&#36394;&#22120;&#19982;&#29289;&#20307;&#20043;&#38388;&#30340;&#29305;&#23450;&#20851;&#31995;&#12290;&#22312;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#26434;&#20081;&#38556;&#30861;&#29289;&#21644;&#22810;&#26679;&#24067;&#23616;&#30340;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#26500;&#24314;&#33021;&#22815;&#31283;&#20581;&#22320;&#36328;&#19981;&#21516;&#22330;&#26223;&#24037;&#20316;&#30340;&#36890;&#29992;&#20027;&#21160;&#36861;&#36394;&#22120;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RSPT&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26500;&#29615;&#22659;&#21644;&#39044;&#27979;&#30446;&#26631;&#36712;&#36857;&#24418;&#25104;&#32467;&#26500;&#24863;&#30693;&#30340;&#36816;&#21160;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Object Tracking (AOT) aims to maintain a specific relation between the tracker and object(s) by autonomously controlling the motion system of a tracker given observations. AOT has wide-ranging applications, such as in mobile robots and autonomous driving. However, building a generalizable active tracker that works robustly across different scenarios remains a challenge, especially in unstructured environments with cluttered obstacles and diverse layouts. We argue that constructing a state representation capable of modeling the geometry structure of the surroundings and the dynamics of the target is crucial for achieving this goal. To address this challenge, we present RSPT, a framework that forms a structure-aware motion representation by Reconstructing the Surroundings and Predicting the target Trajectory. Additionally, we enhance the generalization of the policy network by training in an asymmetric dueling mechanism. We evaluate RSPT on various simulated scenarios and show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#33258;&#21160;&#21270;&#38750;&#30772;&#22351;&#24615;&#27979;&#37327;&#31995;&#32479;&#65292;&#21033;&#29992;Zivid 3D&#30456;&#26426;&#21019;&#24314;&#30058;&#33540;&#26893;&#26666;&#30340;3D&#34394;&#25311;&#34920;&#31034;&#65292;&#36890;&#36807;&#26816;&#27979;&#21494;&#29255;&#25513;&#27169;&#25552;&#21462;&#21494;&#29255;&#22823;&#23567;&#65292;&#23454;&#29616;&#20102;&#23545;&#26893;&#29289;&#29983;&#38271;&#30340;&#31934;&#20934;&#30417;&#27979;&#65292;&#26377;&#26395;&#24212;&#29992;&#20110;&#31934;&#20934;&#20892;&#19994;&#21644;&#28201;&#23460;&#31649;&#29702;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2304.03610</link><description>&lt;p&gt;
&#30058;&#33540;&#26893;&#26666;&#30340;&#38750;&#30772;&#22351;&#24615;&#21494;&#29255;&#26816;&#27979;&#21644;&#22823;&#23567;&#20272;&#35745;&#30340;&#19977;&#32500;&#29983;&#38271;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Look how they have grown: Non-destructive Leaf Detection and Size Estimation of Tomato Plants for 3D Growth Monitoring. (arXiv:2304.03610v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#33258;&#21160;&#21270;&#38750;&#30772;&#22351;&#24615;&#27979;&#37327;&#31995;&#32479;&#65292;&#21033;&#29992;Zivid 3D&#30456;&#26426;&#21019;&#24314;&#30058;&#33540;&#26893;&#26666;&#30340;3D&#34394;&#25311;&#34920;&#31034;&#65292;&#36890;&#36807;&#26816;&#27979;&#21494;&#29255;&#25513;&#27169;&#25552;&#21462;&#21494;&#29255;&#22823;&#23567;&#65292;&#23454;&#29616;&#20102;&#23545;&#26893;&#29289;&#29983;&#38271;&#30340;&#31934;&#20934;&#30417;&#27979;&#65292;&#26377;&#26395;&#24212;&#29992;&#20110;&#31934;&#20934;&#20892;&#19994;&#21644;&#28201;&#23460;&#31649;&#29702;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#26234;&#24935;&#20892;&#19994;&#25104;&#20026;&#20102;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#39046;&#22495;&#12290;&#26893;&#29289;&#29305;&#24449;&#26159;&#30417;&#27979;&#26893;&#29289;&#29983;&#38271;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#29992;&#20110;&#20272;&#31639;&#20687;&#21494;&#38754;&#31215;&#25351;&#25968;&#12289;&#21494;&#29255;&#30142;&#30149;&#21644;&#26893;&#26666;&#39640;&#24230;&#31561;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;&#38750;&#30772;&#22351;&#24615;&#30340;&#21494;&#29255;&#22823;&#23567;&#27979;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#38750;&#30772;&#22351;&#24615;&#22270;&#20687;&#27979;&#37327;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;Zivid 3D&#30456;&#26426;&#33719;&#21462;&#30340;2D&#21644;3D&#25968;&#25454;&#65292;&#21019;&#24314;&#30058;&#33540;&#26893;&#26666;&#30340;3D&#34394;&#25311;&#34920;&#31034;&#65288;&#25968;&#23383;&#21452;&#32990;&#32974;&#65289;&#12290;&#21494;&#23376;&#26159;&#20174;&#23545;&#24212;&#30340;2D RGB&#22270;&#20687;&#20013;&#26816;&#27979;&#21040;&#30340;&#65292;&#24182;&#19988;&#20351;&#29992;&#26816;&#27979;&#21040;&#30340;&#21494;&#29255;&#25513;&#27169;&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;&#23427;&#20204;&#30340;3D&#28857;&#20113;&#19978;&#65292;&#28982;&#21518;&#23558;&#21494;&#23376;&#28857;&#20113;&#20256;&#36865;&#21040;&#24179;&#38754;&#25311;&#21512;&#31639;&#27861;&#20013;&#25552;&#21462;&#21494;&#29255;&#22823;&#23567;&#65292;&#20026;&#29983;&#38271;&#30417;&#27979;&#25552;&#20379;&#25968;&#25454;&#12290;&#36890;&#36807;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#30058;&#33540;&#26893;&#26666;&#36827;&#34892;&#20840;&#38754;&#35797;&#39564;&#24182;&#23558;&#20854;&#24615;&#33021;&#25351;&#26631;&#19982;&#22522;&#20934;&#27979;&#37327;&#36827;&#34892;&#27604;&#36739;&#65292;&#27979;&#37327;&#24179;&#21488;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;&#20998;&#26512;&#20102;&#19977;&#20010;&#30058;&#33540;&#26893;&#26666;&#30340;&#29983;&#38271;&#38454;&#27573;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#20272;&#35745;&#21494;&#23376;&#22823;&#23567;&#21644;&#26893;&#26666;&#39640;&#24230;&#26041;&#38754;&#20855;&#26377;&#39640;&#30340;&#31934;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#20110;&#31934;&#20934;&#20892;&#19994;&#21644;&#28201;&#23460;&#31649;&#29702;&#20013;&#30340;&#38750;&#30772;&#22351;&#24615;&#26893;&#29289;&#29983;&#38271;&#30417;&#27979;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart farming is a growing field as technology advances. Plant characteristics are crucial indicators for monitoring plant growth. Research has been done to estimate characteristics like leaf area index, leaf disease, and plant height. However, few methods have been applied to non-destructive measurements of leaf size. In this paper, an automated non-destructive imaged-based measuring system is presented, which uses 2D and 3D data obtained using a Zivid 3D camera, creating 3D virtual representations (digital twins) of the tomato plants. Leaves are detected from corresponding 2D RGB images and mapped to their 3D point cloud using the detected leaf masks, which then pass the leaf point cloud to the plane fitting algorithm to extract the leaf size to provide data for growth monitoring. The performance of the measurement platform has been measured through a comprehensive trial on real-world tomato plants with quantified performance metrics compared to ground truth measurements. Three tomat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28216;&#25103;&#24341;&#25806;&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#25176;&#30424;&#20998;&#21106;&#65292;&#33258;&#20027;&#20179;&#20648;&#25216;&#26415;&#30340;&#21457;&#23637;&#23545;&#20110;&#26426;&#22120;&#35270;&#35273;&#26377;&#24456;&#22823;&#24110;&#21161;&#65292;Mask R-CNN&#27969;&#27700;&#32447;&#21487;&#20197;&#36798;&#21040;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.03602</link><description>&lt;p&gt;
&#20351;&#29992;&#28216;&#25103;&#24341;&#25806;&#20174;&#21512;&#25104;&#25968;&#25454;&#20013;&#26816;&#27979;&#25176;&#30424;
&lt;/p&gt;
&lt;p&gt;
Pallet Detection from Synthetic Data Using Game Engines. (arXiv:2304.03602v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28216;&#25103;&#24341;&#25806;&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#25176;&#30424;&#20998;&#21106;&#65292;&#33258;&#20027;&#20179;&#20648;&#25216;&#26415;&#30340;&#21457;&#23637;&#23545;&#20110;&#26426;&#22120;&#35270;&#35273;&#26377;&#24456;&#22823;&#24110;&#21161;&#65292;Mask R-CNN&#27969;&#27700;&#32447;&#21487;&#20197;&#36798;&#21040;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#20351;&#29992;&#28216;&#25103;&#24341;&#25806;&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#26816;&#27979;&#25176;&#30424;&#20998;&#21106;&#20026;&#20363;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#26159;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#34892;&#25163;&#27573;&#65292;&#24182;&#19988;&#22240;&#20026;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#22270;&#20687;&#26631;&#27880;&#30340;&#38656;&#27714;&#38477;&#20302;&#65292;&#21487;&#20197;&#33410;&#30465;&#22823;&#37327;&#25163;&#24037;&#21171;&#21160;&#26102;&#38388;&#12290;&#38543;&#30528;&#33258;&#21160;&#21270;&#20179;&#20648;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#25176;&#30424;&#26816;&#27979;&#30340;&#26426;&#22120;&#35270;&#35273;&#21487;&#20197;&#21463;&#30410;&#20110;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24037;&#20855;&#65292;&#33021;&#22815;&#20197;&#20687;&#32032;&#23436;&#32654;&#30340;&#20934;&#30830;&#29575;&#21644;&#27604;&#25163;&#21160;&#26041;&#27861;&#26356;&#24555;&#30340;&#36895;&#24230;&#20174;3D&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#22823;&#37327;&#24102;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23545;&#20110;&#22270;&#20687;&#20998;&#21106;&#65292;&#20351;&#29992;&#20102;Mask R-CNN&#27969;&#27700;&#32447;&#65292;&#23545;&#20110;&#21333;&#20010;&#25176;&#30424;&#30340;AP50&#36798;&#21040;&#20102;86&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research sets out to assess the viability of using game engines to generate synthetic training data for machine learning in the context of pallet segmentation. Using synthetic data has been proven in prior research to be a viable means of training neural networks and saves hours of manual labour due to the reduced need for manual image annotation. Machine vision for pallet detection can benefit from synthetic data as the industry increases the development of autonomous warehousing technologies. As per our methodology, we developed a tool capable of automatically generating large amounts of annotated training data from 3D models at pixel-perfect accuracy and a much faster rate than manual approaches. Regarding image segmentation, a Mask R-CNN pipeline was used, which achieved an AP50 of 86% for individual pallets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30896;&#25758;&#27010;&#29575;&#30340;&#26080;&#22270;Crowd Navigation&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26469;&#24863;&#30693;&#20154;&#32676;&#30340;&#21361;&#38505;&#31243;&#24230;&#65292;&#30830;&#20445;&#26426;&#22120;&#20154;&#22312;&#36890;&#36807;&#25317;&#25380;&#29615;&#22659;&#26102;&#30340;&#23433;&#20840;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03593</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#22270;Crowd Navigation&#19982;&#24863;&#30693;&#39118;&#38505;&#25511;&#21046;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-Based Mapless Crowd Navigation with Perceived Risk of the Moving Crowd for Mobile Robots. (arXiv:2304.03593v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30896;&#25758;&#27010;&#29575;&#30340;&#26080;&#22270;Crowd Navigation&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26469;&#24863;&#30693;&#20154;&#32676;&#30340;&#21361;&#38505;&#31243;&#24230;&#65292;&#30830;&#20445;&#26426;&#22120;&#20154;&#22312;&#36890;&#36807;&#25317;&#25380;&#29615;&#22659;&#26102;&#30340;&#23433;&#20840;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#22320;&#22270;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#26041;&#27861;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#24448;&#24448;&#20250;&#36935;&#21040;&#8220;&#20923;&#32467;&#26426;&#22120;&#20154;&#38382;&#39064;&#8221;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#27492;&#38382;&#39064;&#65292;&#20294;&#26159;&#23384;&#22312;&#27867;&#21270;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#8220;&#30896;&#25758;&#27010;&#29575;&#8221;&#26469;&#24110;&#21161;&#26426;&#22120;&#20154;&#23433;&#20840;&#36890;&#36807;&#20154;&#32676;&#30340;&#26041;&#27861;&#12290;&#23558;&#8220;&#30896;&#25758;&#27010;&#29575;&#8221;&#21253;&#25324;&#22312;&#35266;&#23519;&#31354;&#38388;&#20013;&#65292;&#32473;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#20010;&#24863;&#30693;&#31227;&#21160;&#20154;&#32676;&#30340;&#21361;&#38505;&#31243;&#24230;&#30340;&#33021;&#21147;&#12290;&#26426;&#22120;&#20154;&#20250;&#22312;&#30475;&#20284;&#23433;&#20840;&#30340;&#24773;&#20917;&#19979;&#31359;&#36807;&#20154;&#32676;&#65292;&#20294;&#22312;&#20154;&#32676;&#31227;&#21160;&#36807;&#20110;&#28608;&#28872;&#26102;&#20250;&#32469;&#36335;&#12290;&#36890;&#36807;&#20851;&#27880;&#26368;&#21361;&#38505;&#30340;&#38556;&#30861;&#29289;&#65292;&#26426;&#22120;&#20154;&#19981;&#20250;&#22312;&#20154;&#32676;&#23494;&#24230;&#36739;&#39640;&#26102;&#28151;&#28102;&#65292;&#30830;&#20445;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24320;&#21457;&#65292;&#24182;&#22312;Gazebo&#27169;&#25311;&#22120;&#20013;&#36827;&#34892;&#20102;&#38750;&#21512;&#20316;&#20154;&#32676;&#29615;&#22659;&#20013;&#30340;&#35757;&#32451;&#65292;&#20854;&#20013;&#30340;&#38556;&#30861;&#29289;&#20197;&#38543;&#26426;&#36895;&#24230;&#31227;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical map-based navigation methods are commonly used for robot navigation, but they often struggle in crowded environments due to the Frozen Robot Problem (FRP). Deep reinforcement learning-based methods address the FRP problem, however, suffer from the issues of generalization and scalability. To overcome these challenges, we propose a method that uses Collision Probability (CP) to help the robot navigate safely through crowds. The inclusion of CP in the observation space gives the robot a sense of the level of danger of the moving crowd. The robot will navigate through the crowd when it appears safe but will take a detour when the crowd is moving aggressively. By focusing on the most dangerous obstacle, the robot will not be confused when the crowd density is high, ensuring scalability of the model. Our approach was developed using deep reinforcement learning (DRL) and trained using the Gazebo simulator in a non cooperative crowd environment with obstacles moving at randomized sp
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#24050;&#26377;&#19981;&#23569;&#25104;&#26524;&#65292;&#20294;&#23578;&#32570;&#20047;&#20840;&#38754;&#24635;&#32467;&#12290;&#26412;&#25991;&#32508;&#36848;&#20998;&#20026;&#25968;&#25454;&#20013;&#24515;&#21270;&#12289;&#27169;&#22411;&#20013;&#24515;&#21270;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#21644;&#35757;&#32451;&#31574;&#30053;&#31561;&#20116;&#20010;&#26041;&#38754;&#65292;&#27604;&#36739;&#35814;&#23613;&#22320;&#22238;&#39038;&#20102;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#22522;&#26412;&#32452;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.03589</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
On Efficient Training of Large-Scale Deep Learning Models: A Literature Review. (arXiv:2304.03589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03589
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#24050;&#26377;&#19981;&#23569;&#25104;&#26524;&#65292;&#20294;&#23578;&#32570;&#20047;&#20840;&#38754;&#24635;&#32467;&#12290;&#26412;&#25991;&#32508;&#36848;&#20998;&#20026;&#25968;&#25454;&#20013;&#24515;&#21270;&#12289;&#27169;&#22411;&#20013;&#24515;&#21270;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#21644;&#35757;&#32451;&#31574;&#30053;&#31561;&#20116;&#20010;&#26041;&#38754;&#65292;&#27604;&#36739;&#35814;&#23613;&#22320;&#22238;&#39038;&#20102;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#22522;&#26412;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#37319;&#29992;&#22823;&#35268;&#27169;&#27169;&#22411;&#24182;&#22312;&#28023;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#22686;&#24378;&#24037;&#19994;&#29983;&#20135;&#21147;&#65292;&#20419;&#36827;&#31038;&#20250;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23545;&#35745;&#31639;&#33021;&#21147;&#35201;&#27714;&#30340;&#22686;&#21152;&#65292;&#23613;&#31649;&#26377;&#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#23545;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21152;&#36895;&#25216;&#26415;&#30340;&#20840;&#38754;&#24635;&#32467;&#20173;&#28982;&#22791;&#21463;&#26399;&#24453;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#22238;&#39038;&#20102;&#35757;&#32451;&#21152;&#36895;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#23558;&#22522;&#26412;&#32452;&#20214;&#20998;&#20026;&#20116;&#20010;&#20027;&#35201;&#35270;&#35282;&#65306;&#65288;1&#65289;&#25968;&#25454;&#20013;&#24515;&#21270;&#65306;&#21253;&#25324;&#25968;&#25454;&#38598;&#27491;&#21017;&#21270;&#12289;&#25968;&#25454;&#37319;&#26679;&#21644;&#25968;&#25454;&#20013;&#24515;&#21270;&#35838;&#31243;&#23398;&#20064;&#25216;&#26415;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25968;&#25454;&#26679;&#26412;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65307;&#65288;2&#65289;&#27169;&#22411;&#20013;&#24515;&#21270;&#65306;&#21253;&#25324;&#22522;&#26412;&#27169;&#22359;&#30340;&#21152;&#36895;&#12289;&#27169;&#22411;&#21387;&#32553;&#21644;&#27169;&#22411;&#33976;&#39311;&#25216;&#26415;&#65307;(&#21097;&#19979;&#21516;&#19978;&#21407;&#25991;)
&lt;/p&gt;
&lt;p&gt;
The field of deep learning has witnessed significant progress, particularly in computer vision (CV), natural language processing (NLP), and speech. The use of large-scale models trained on vast amounts of data holds immense promise for practical applications, enhancing industrial productivity and facilitating social development. With the increasing demands on computational capacity, though numerous studies have explored the efficient training, a comprehensive summarization on acceleration techniques of training deep learning models is still much anticipated. In this survey, we present a detailed review for training acceleration. We consider the fundamental update formulation and split its basic components into five main perspectives: (1) data-centric: including dataset regularization, data sampling, and data-centric curriculum learning techniques, which can significantly reduce the computational complexity of the data samples; (2) model-centric, including acceleration of basic modules,
&lt;/p&gt;</description></item><item><title>HyperTab&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#32467;&#21512;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#20248;&#28857;&#30340;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27599;&#20010;&#29305;&#23450;&#20302;&#32500;&#35270;&#22270;&#22788;&#29702;&#25968;&#25454;&#65292;&#34394;&#25311;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.03543</link><description>&lt;p&gt;
HyperTab: &#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HyperTab: Hypernetwork Approach for Deep Learning on Small Tabular Datasets. (arXiv:2304.03543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03543
&lt;/p&gt;
&lt;p&gt;
HyperTab&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#32467;&#21512;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#20248;&#28857;&#30340;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27599;&#20010;&#29305;&#23450;&#20302;&#32500;&#35270;&#22270;&#22788;&#29702;&#25968;&#25454;&#65292;&#34394;&#25311;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#34920;&#29616;&#65292;&#20363;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20294;&#23427;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20256;&#32479;&#27973;&#23618;&#26041;&#27861;&#30340;&#20248;&#21183;&#20173;&#28982;&#20540;&#24471;&#21830;&#27063;&#12290;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#65288;&#23567;&#20110;1k&#20010;&#26679;&#26412;&#65289;&#19978;&#36229;&#36807;&#26641;&#29366;&#38598;&#25104;&#65288;&#22914;XGBoost&#25110;&#38543;&#26426;&#26862;&#26519;&#65289;&#30340;&#34920;&#29616;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HyperTab&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#35299;&#20915;&#34920;&#26684;&#25968;&#25454;&#38598;&#23567;&#26679;&#26412;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;HyperTab&#29983;&#25104;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#65292;&#20854;&#20013;&#27599;&#20010;&#30446;&#26631;&#27169;&#22411;&#19987;&#38376;&#22788;&#29702;&#25968;&#25454;&#30340;&#29305;&#23450;&#20302;&#32500;&#35270;&#22270;&#12290;&#30001;&#20110;&#27599;&#20010;&#35270;&#22270;&#25198;&#28436;&#25968;&#25454;&#22686;&#24378;&#30340;&#35282;&#33394;&#65292;&#25105;&#20204;&#22312;&#20445;&#25345;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#34394;&#25311;&#22686;&#21152;&#20102;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#23545;40&#22810;&#20010;&#22823;&#23567;&#19981;&#21516;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#23545;HyperTab&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has achieved impressive performance in many domains, such as computer vision and natural language processing, but its advantage over classical shallow methods on tabular datasets remains questionable. It is especially challenging to surpass the performance of tree-like ensembles, such as XGBoost or Random Forests, on small-sized datasets (less than 1k samples). To tackle this challenge, we introduce HyperTab, a hypernetwork-based approach to solving small sample problems on tabular datasets. By combining the advantages of Random Forests and neural networks, HyperTab generates an ensemble of neural networks, where each target model is specialized to process a specific lower-dimensional view of the data. Since each view plays the role of data augmentation, we virtually increase the number of training samples while keeping the number of trainable parameters unchanged, which prevents model overfitting. We evaluated HyperTab on more than 40 tabular datasets of a varying number
&lt;/p&gt;</description></item><item><title>ChatPipe &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#20248;&#21270; ChatGPT &#32534;&#25490; ML &#25968;&#25454;&#20934;&#22791;&#31243;&#24207;&#65292;&#26377;&#25928;&#25512;&#33616;&#19979;&#19968;&#20010;&#25968;&#25454;&#20934;&#22791;&#25805;&#20316;&#65292;&#26041;&#20415;&#29992;&#25143;&#36827;&#34892;&#31243;&#24207;&#30340;&#20462;&#25913;&#21644;&#29256;&#26412;&#20999;&#25442;&#65292;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569; ML &#25968;&#25454;&#20934;&#22791;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.03540</link><description>&lt;p&gt;
ChatPipe&#65306;&#36890;&#36807;&#20248;&#21270;&#20154;-ChatGPT&#20114;&#21160;&#26469;&#32534;&#25490;&#25968;&#25454;&#20934;&#22791;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
ChatPipe: Orchestrating Data Preparation Program by Optimizing Human-ChatGPT Interactions. (arXiv:2304.03540v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03540
&lt;/p&gt;
&lt;p&gt;
ChatPipe &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#20248;&#21270; ChatGPT &#32534;&#25490; ML &#25968;&#25454;&#20934;&#22791;&#31243;&#24207;&#65292;&#26377;&#25928;&#25512;&#33616;&#19979;&#19968;&#20010;&#25968;&#25454;&#20934;&#22791;&#25805;&#20316;&#65292;&#26041;&#20415;&#29992;&#25143;&#36827;&#34892;&#31243;&#24207;&#30340;&#20462;&#25913;&#21644;&#29256;&#26412;&#20999;&#25442;&#65292;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569; ML &#25968;&#25454;&#20934;&#22791;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#25490;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#20934;&#22791;&#31243;&#24207;&#23545;&#20110;&#25104;&#21151;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#26159;&#32791;&#26102;&#36153;&#21147;&#30340;&#12290;&#23613;&#31649;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19982;&#29992;&#25143;&#20132;&#20114;&#29983;&#25104;&#31243;&#24207;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20173;&#23384;&#22312;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29992;&#25143;&#24517;&#39035;&#25552;&#20379;&#29305;&#23450;&#30340;&#25552;&#31034;&#26469;&#24341;&#23548;ChatGPT&#36845;&#20195;&#22320;&#25913;&#36827;&#25968;&#25454;&#20934;&#22791;&#31243;&#24207;&#65292;&#36825;&#38656;&#35201;&#23545;&#32534;&#31243;&#12289;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;ML&#20219;&#21153;&#26377;&#19968;&#23450;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#19968;&#26086;&#29983;&#25104;&#20102;&#31243;&#24207;&#65292;&#22312;&#19981;&#37325;&#26032;&#24320;&#22987;&#25972;&#20010;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#22238;&#39038;&#20808;&#21069;&#30340;&#29256;&#26412;&#25110;&#23545;&#31243;&#24207;&#36827;&#34892;&#26356;&#25913;&#26159;&#24456;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatPipe&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#20419;&#36827;&#29992;&#25143;&#21644;ChatGPT&#20043;&#38388;&#30340;&#26080;&#32541;&#20132;&#20114;&#12290; ChatPipe&#20026;&#29992;&#25143;&#25552;&#20379;&#20851;&#20110;&#19979;&#19968;&#20010;&#25968;&#25454;&#20934;&#22791;&#25805;&#20316;&#30340;&#26377;&#25928;&#24314;&#35758;&#65292;&#24182;&#25351;&#23548;ChatGPT&#29983;&#25104;&#25805;&#20316;&#30340;&#31243;&#24207;&#12290;&#21478;&#22806;&#65292;Chatpipe&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#20462;&#25913;&#29983;&#25104;&#30340;&#31243;&#24207;&#25110;&#20999;&#25442;&#21040;&#20808;&#21069;&#30340;&#29256;&#26412;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;ChatPipe&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20026;ML&#20219;&#21153;&#20934;&#22791;&#25968;&#25454;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orchestrating a high-quality data preparation program is essential for successful machine learning (ML), but it is known to be time and effort consuming. Despite the impressive capabilities of large language models like ChatGPT in generating programs by interacting with users through natural language prompts, there are still limitations. Specifically, a user must provide specific prompts to iteratively guide ChatGPT in improving data preparation programs, which requires a certain level of expertise in programming, the dataset used and the ML task. Moreover, once a program has been generated, it is non-trivial to revisit a previous version or make changes to the program without starting the process over again. In this paper, we present ChatPipe, a novel system designed to facilitate seamless interaction between users and ChatGPT. ChatPipe provides users with effective recommendation on next data preparation operations, and guides ChatGPT to generate program for the operations. Also, Cha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#32454;&#35843;BERT&#27169;&#22411;&#21644;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#30340;&#26041;&#27861;&#12290;&#32763;&#36716;&#26174;&#30528;&#38477;&#20302;&#20102;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#32463;&#21382;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.03518</link><description>&lt;p&gt;
SSS&#22312;SemEval-2023&#20219;&#21153;10&#20013;&#30340;&#35770;&#25991;&#65306;&#20351;&#29992;&#25237;&#31080;&#32454;&#35843;&#21464;&#21387;&#22120;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#12290; (arXiv&#65306;2304.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers. (arXiv:2304.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#32454;&#35843;BERT&#27169;&#22411;&#21644;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#30340;&#26041;&#27861;&#12290;&#32763;&#36716;&#26174;&#30528;&#38477;&#20302;&#20102;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#32463;&#21382;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval 2023&#20219;&#21153;10&#20013;&#25552;&#20132;&#30340;&#20316;&#21697;-&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#65288;EDOS&#65289;&#65292;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#12290;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#19981;&#26029;&#22686;&#38271;&#23548;&#33268;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#38754;&#20020;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#12290;&#36825;&#20351;&#24471;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#21464;&#24471;&#27604;&#20197;&#24448;&#26356;&#21152;&#37325;&#35201;&#65292;&#20197;&#20351;&#31038;&#20132;&#23186;&#20307;&#23545;&#22899;&#24615;&#26356;&#21152;&#23433;&#20840;&#21644;&#21487;&#35775;&#38382;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#23454;&#39564;&#21644;&#24494;&#35843;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22810;&#25968;&#25237;&#31080;&#38598;&#21512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#21333;&#20010;&#22522;&#32447;&#27169;&#22411;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20219;&#21153;A&#20013;&#23454;&#29616;&#20102;&#23439;F1&#20998;&#25968;0.8392&#65292;&#22312;&#20219;&#21153;B&#20013;&#20026;0.6092&#65292;&#22312;&#20219;&#21153;C&#20013;&#20026;0.4319&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to Task 10 at SemEval 2023-Explainable Detection of Online Sexism (EDOS), divided into three subtasks. The recent rise in social media platforms has seen an increase in disproportionate levels of sexism experienced by women on social media platforms. This has made detecting and explaining online sexist content more important than ever to make social media safer and more accessible for women. Our approach consists of experimenting and finetuning BERT-based models and using a Majority Voting ensemble model that outperforms individual baseline model scores. Our system achieves a macro F1 score of 0.8392 for Task A, 0.6092 for Task B, and 0.4319 for Task C.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#32993;&#33821;&#21340;&#30149;&#23475;&#26816;&#27979;&#24212;&#29992;&#31243;&#24207;&#8220;&#32993;&#33821;&#21340;&#27835;&#30103;&#8221;&#65292;&#21487;&#20197;&#35782;&#21035;&#26377;&#32570;&#38519;&#30340;&#32993;&#33821;&#21340;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.03511</link><description>&lt;p&gt;
&#32993;&#33821;&#21340;&#27835;&#30103;&#65306;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#26816;&#27979;&#32993;&#33821;&#21340;&#30149;&#23475;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Carrot Cure: A CNN based Application to Detect Carrot Disease. (arXiv:2304.03511v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#32993;&#33821;&#21340;&#30149;&#23475;&#26816;&#27979;&#24212;&#29992;&#31243;&#24207;&#8220;&#32993;&#33821;&#21340;&#27835;&#30103;&#8221;&#65292;&#21487;&#20197;&#35782;&#21035;&#26377;&#32570;&#38519;&#30340;&#32993;&#33821;&#21340;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32993;&#33821;&#21340;&#26159;&#19990;&#30028;&#21508;&#22320;&#24191;&#27867;&#31181;&#26893;&#30340;&#33829;&#20859;&#34092;&#33756;&#12290;&#19981;&#21516;&#30340;&#32993;&#33821;&#21340;&#30149;&#23475;&#24050;&#25104;&#20026;&#32993;&#33821;&#21340;&#29983;&#20135;&#39046;&#22495;&#30340;&#37325;&#22823;&#38382;&#39064;&#65292;&#36825;&#23545;&#20892;&#19994;&#32463;&#27982;&#22686;&#38271;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#12290;&#33258;&#21160;&#21270;&#30340;&#32993;&#33821;&#21340;&#30149;&#23475;&#26816;&#27979;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#26377;&#23475;&#32993;&#33821;&#21340;&#65292;&#24182;&#25552;&#20379;&#26089;&#26399;&#27835;&#30103;&#25351;&#21335;&#65292;&#20943;&#23569;&#23545;&#32993;&#33821;&#21340;&#29983;&#20135;&#31995;&#32479;&#30340;&#32463;&#27982;&#25439;&#22833;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#8220;&#32993;&#33821;&#21340;&#27835;&#30103;&#8221;&#65292;&#21487;&#35782;&#21035;&#26377;&#30149;&#23475;&#30340;&#32993;&#33821;&#21340;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;&#25910;&#38598;&#20102;&#21463;&#28342;&#27934;&#21644;&#20142;&#21494;&#24433;&#21709;&#30340;&#32993;&#33821;&#21340;&#30340;&#22270;&#20687;&#20197;&#21450;&#20581;&#24247;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20986;&#29983;&#31070;&#32463;&#30446;&#30340;&#30340;&#25506;&#35752;&#21644;&#23436;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;FCNN&#65289;&#36827;&#34892;&#24863;&#26579;&#39034;&#24207;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Carrot is a famous nutritional vegetable and developed all over the world. Different diseases of Carrot has become a massive issue in the carrot production circle which leads to a tremendous effect on the economic growth in the agricultural sector. An automatic carrot disease detection system can help to identify malicious carrots and can provide a guide to cure carrot disease in an earlier stage, resulting in a less economical loss in the carrot production system. The proposed research study has developed a web application Carrot Cure based on Convolutional Neural Network (CNN), which can identify a defective carrot and provide a proper curative solution. Images of carrots affected by cavity spot and leaf bright as well as healthy images were collected. Further, this research work has employed Convolutional Neural Network to include birth neural purposes and a Fully Convolutional Neural Network model (FCNN) for infection order. Different avenues regarding different convolutional model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#29611;&#29808;&#21697;&#31181;&#26816;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;1939&#24352;&#22270;&#20687;&#25968;&#25454;&#35757;&#32451;&#65292;&#26816;&#27979;&#20845;&#31181;&#19981;&#21516;&#21697;&#31181;&#30340;&#29611;&#29808;&#65292;&#20934;&#30830;&#24230;&#36798;&#21040;97.89&#65285;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20026;&#33457;&#21321;&#34892;&#19994;&#35782;&#21035;&#21644;&#26685;&#22521;&#19981;&#21516;&#21697;&#31181;&#30340;&#29611;&#29808;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2304.03509</link><description>&lt;p&gt;
&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#26412;&#22320;&#29611;&#29808;&#21697;&#31181;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Local Rose Breeds Detection System Using Transfer Learning Techniques. (arXiv:2304.03509v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03509
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#29611;&#29808;&#21697;&#31181;&#26816;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;1939&#24352;&#22270;&#20687;&#25968;&#25454;&#35757;&#32451;&#65292;&#26816;&#27979;&#20845;&#31181;&#19981;&#21516;&#21697;&#31181;&#30340;&#29611;&#29808;&#65292;&#20934;&#30830;&#24230;&#36798;&#21040;97.89&#65285;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20026;&#33457;&#21321;&#34892;&#19994;&#35782;&#21035;&#21644;&#26685;&#22521;&#19981;&#21516;&#21697;&#31181;&#30340;&#29611;&#29808;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33457;&#21321;&#21697;&#31181;&#26816;&#27979;&#24182;&#32473;&#20986;&#21697;&#31181;&#35814;&#32454;&#20449;&#24687;&#65292;&#21253;&#25324;&#26685;&#22521;&#36807;&#31243;&#21644;&#29031;&#39038;&#26041;&#24335;&#65292;&#23545;&#20110;&#33457;&#21321;&#26685;&#22521;&#12289;&#21697;&#31181;&#21457;&#26126;&#21644;&#33457;&#21321;&#19994;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#23391;&#21152;&#25289;&#22269;&#25152;&#26377;&#30340;&#26412;&#22320;&#33457;&#21321;&#20013;&#65292;&#29611;&#29808;&#26159;&#26368;&#21463;&#27426;&#36814;&#21644;&#38656;&#27714;&#37327;&#26368;&#22823;&#30340;&#33457;&#21321;&#20043;&#19968;&#12290;&#29611;&#29808;&#19981;&#20165;&#22312;&#23391;&#21152;&#25289;&#22269;&#65292;&#32780;&#19988;&#22312;&#19990;&#30028;&#33539;&#22260;&#20869;&#37117;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#33457;&#21321;&#20043;&#19968;&#12290;&#29611;&#29808;&#38500;&#20102;&#29992;&#20110;&#35013;&#39280;&#22806;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#35768;&#22810;&#20854;&#20182;&#29992;&#36884;&#12290;&#30001;&#20110;&#29611;&#29808;&#22312;&#33457;&#21321;&#34892;&#19994;&#20855;&#26377;&#24040;&#22823;&#30340;&#38656;&#27714;&#65292;&#22240;&#27492;&#29611;&#29808;&#21697;&#31181;&#26816;&#27979;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19982;&#19981;&#21516;&#33457;&#21321;&#30340;&#20998;&#31867;&#19981;&#21516;&#65292;&#30446;&#21069;&#27809;&#26377;&#20851;&#20110;&#29305;&#23450;&#33457;&#21321;&#21697;&#31181;&#26816;&#27979;&#30340;&#26174;&#33879;&#24037;&#20316;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#20174;&#22270;&#20687;&#20013;&#26816;&#27979;&#29611;&#29808;&#21697;&#31181;&#30340;&#27169;&#22411;&#12290;&#23545;&#20110;&#33457;&#21321;&#26041;&#38754;&#30340;&#36825;&#39033;&#24037;&#20316;&#65292;&#22270;&#20687;&#22788;&#29702;&#21644;&#20998;&#31867;&#30340;&#36164;&#28304;&#19981;&#36275;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#22270;&#20687;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;1939&#24352;&#19981;&#21516;&#29611;&#29808;&#21697;&#31181;&#30340;&#21407;&#22987;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26816;&#27979;&#20845;&#31181;&#19981;&#21516;&#21697;&#31181;&#30340;&#29611;&#29808;&#26041;&#38754;&#33719;&#24471;&#20102;97.89&#65285;&#30340;&#20934;&#30830;&#24230;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22823;&#22823;&#24110;&#21161;&#33457;&#21321;&#34892;&#19994;&#35782;&#21035;&#21644;&#26685;&#22521;&#19981;&#21516;&#21697;&#31181;&#30340;&#29611;&#29808;&#65292;&#24182;&#20026;&#27599;&#20010;&#21697;&#31181;&#25552;&#20986;&#26368;&#20339;&#30340;&#26685;&#22521;&#21644;&#29031;&#39038;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flower breed detection and giving details of that breed with the suggestion of cultivation processes and the way of taking care is important for flower cultivation, breed invention, and the flower business. Among all the local flowers in Bangladesh, the rose is one of the most popular and demanded flowers. Roses are the most desirable flower not only in Bangladesh but also throughout the world. Roses can be used for many other purposes apart from decoration. As roses have a great demand in the flower business so rose breed detection will be very essential. However, there is no remarkable work for breed detection of a particular flower unlike the classification of different flowers. In this research, we have proposed a model to detect rose breeds from images using transfer learning techniques. For such work in flowers, resources are not enough in image processing and classification, so we needed a large dataset of the massive number of images to train our model. we have used 1939 raw im
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#20256;&#32479;mini-batch&#31639;&#27861;&#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#37325;&#28857;&#35757;&#32451;&#20855;&#26377;&#39640;&#25439;&#22833;&#30340;mini-batch&#32593;&#32476;&#65292;&#20854;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#24615;&#24182;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.03486</link><description>&lt;p&gt;
&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#38590;&#26679;&#26412;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can we learn better with hard samples?. (arXiv:2304.03486v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03486
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#20256;&#32479;mini-batch&#31639;&#27861;&#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#37325;&#28857;&#35757;&#32451;&#20855;&#26377;&#39640;&#25439;&#22833;&#30340;mini-batch&#32593;&#32476;&#65292;&#20854;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#24615;&#24182;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#24120;&#29992;mini-batch&#35757;&#32451;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;mini-batch&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#23398;&#20064;&#25968;&#25454;&#20013;&#30340;&#27425;&#25968;&#36739;&#23569;&#30340;&#26679;&#26412;&#21644;&#22797;&#26434;&#30340;&#27169;&#24335;&#65292;&#23548;&#33268;&#27867;&#21270;&#26102;&#38388;&#26356;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#20256;&#32479;&#31639;&#27861;&#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#37325;&#28857;&#35757;&#32451;&#20855;&#26377;&#39640;&#25439;&#22833;&#30340;mini-batch&#32593;&#32476;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;CIFAR-10&#65292;CIFAR-100&#21644;STL-10&#65289;&#19978;&#35757;&#32451;&#30340;&#21508;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;ResNet-18&#65292;ResNet-50&#65292;Efficient Net B4&#65292;EfficientNetV2-S&#21644;MobilenetV3-S&#65289;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;mini-batch&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#24615;&#24182;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36229;&#21442;&#25968;delta ({\delta})&#65292;&#23427;&#20915;&#23450;&#26377;&#22810;&#23569;&#20010;mini-batch&#34987;&#32771;&#34385;&#29992;&#20110;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning, mini-batch training is commonly used to optimize network parameters. However, the traditional mini-batch method may not learn the under-represented samples and complex patterns in the data, leading to a longer time for generalization. To address this problem, a variant of the traditional algorithm has been proposed, which trains the network focusing on mini-batches with high loss. The study evaluates the effectiveness of the proposed training using various deep neural networks trained on three benchmark datasets (CIFAR-10, CIFAR-100, and STL-10). The deep neural networks used in the study are ResNet-18, ResNet-50, Efficient Net B4, EfficientNetV2-S, and MobilenetV3-S. The experimental results showed that the proposed method can significantly improve the test accuracy and speed up the convergence compared to the traditional mini-batch training method. Furthermore, we introduce a hyper-parameter delta ({\delta}) that decides how many mini-batches are considered for trai
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#12290;&#20026;&#25506;&#31350;EA&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26356;&#25509;&#36817;&#29616;&#23454;&#30340;&#39640;&#24230;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03468</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#65306;&#26032;&#25968;&#25454;&#38598;&#21644;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking GNN-based Entity Alignment on Heterogeneous Knowledge Graphs: New Datasets and A New Method. (arXiv:2304.03468v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03468
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#12290;&#20026;&#25506;&#31350;EA&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26356;&#25509;&#36817;&#29616;&#23454;&#30340;&#39640;&#24230;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#24212;&#29992;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#38656;&#35201;&#20174;&#21508;&#31181;&#26469;&#28304;&#25552;&#21462;&#30340;&#24322;&#26500;KG&#20043;&#38388;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#12290;&#36817;&#26469;&#65292;&#30001;&#20110;GNN&#30340;&#20986;&#33394;&#32467;&#26500;&#20449;&#24687;&#25429;&#25417;&#33021;&#21147;&#65292;&#22312;EA&#20219;&#21153;&#20013;&#24191;&#27867;&#37319;&#29992;GNN&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#24120;&#35265;EA&#25968;&#25454;&#38598;&#30340;&#36807;&#20110;&#31616;&#21333;&#21270;&#30340;&#35774;&#32622;&#19982;&#29616;&#23454;&#22330;&#26223;&#30456;&#36317;&#29978;&#36828;&#65292;&#36825;&#22952;&#30861;&#20102;&#23545;&#26368;&#36817;&#26041;&#27861;&#25152;&#21462;&#24471;&#36827;&#23637;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#36825;&#31181;&#29616;&#35937;&#20351;&#25105;&#20204;&#28145;&#24605;&#65306;&#29616;&#26377;&#22522;&#20110;GNN&#30340;EA&#26041;&#27861;&#26159;&#21542;&#30495;&#30340;&#21462;&#24471;&#20102;&#20255;&#22823;&#36827;&#23637;&#65311;&#20026;&#20102;&#30740;&#31350;EA&#26041;&#27861;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#32858;&#28966;&#20110;&#39640;&#24230;&#24322;&#26500;&#30340;KG&#65288;HHKG&#65289;&#65288;&#20363;&#22914;&#65292;&#20107;&#20214;KG&#21644;&#36890;&#29992;KG&#65289;&#30340;&#23545;&#40784;&#65292;&#36825;&#20123;KG&#22312;&#35268;&#27169;&#21644;&#32467;&#26500;&#19978;&#19981;&#21516;&#65292;&#24182;&#20849;&#20139;&#26356;&#23569;&#30340;&#37325;&#21472;&#23454;&#20307;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28165;&#29702;&#20102;&#19981;&#21512;&#29702;&#30340;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HHKG&#25968;&#25454;&#38598;&#65292;&#20854;&#23494;&#20999;&#22320;&#27169;&#25311;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of knowledge graph (KG) applications has led to a rising need for entity alignment (EA) between heterogeneous KGs that are extracted from various sources. Recently, graph neural networks (GNNs) have been widely adopted in EA tasks due to GNNs' impressive ability to capture structure information. However, we have observed that the oversimplified settings of the existing common EA datasets are distant from real-world scenarios, which obstructs a full understanding of the advancements achieved by recent methods. This phenomenon makes us ponder: Do existing GNN-based EA methods really make great progress?  In this paper, to study the performance of EA methods in realistic settings, we focus on the alignment of highly heterogeneous KGs (HHKGs) (e.g., event KGs and general KGs) which are different with regard to the scale and structure, and share fewer overlapping entities. First, we sweep the unreasonable settings, and propose two new HHKG datasets that closely mimic real-wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AMS-DRL&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#23545;&#25239;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#23398;&#20064;&#21644;&#24555;&#36895;&#36866;&#24212;&#22810;&#20010;&#25915;&#20987;&#32773;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#23433;&#20840;&#23548;&#33322;&#21644;&#21040;&#36798;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.03443</link><description>&lt;p&gt;
AMS-DRL: &#23398;&#20064;&#22810;&#30446;&#26631;&#36867;&#36991;&#20197;&#23454;&#29616;&#26080;&#20154;&#26426;&#23433;&#20840;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
AMS-DRL: Learning Multi-Pursuit Evasion for Safe Targeted Navigation of Drones. (arXiv:2304.03443v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AMS-DRL&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#23545;&#25239;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#23398;&#20064;&#21644;&#24555;&#36895;&#36866;&#24212;&#22810;&#20010;&#25915;&#20987;&#32773;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#23433;&#20840;&#23548;&#33322;&#21644;&#21040;&#36798;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#34989;&#20987;&#32773;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#20154;&#26426;&#30340;&#23433;&#20840;&#23548;&#33322;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#24322;&#27493;&#22810;&#38454;&#27573;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(AMS-DRL)&#65292;&#26469;&#35757;&#32451;&#23545;&#25239;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#20174;&#22810;&#20010;&#25915;&#20987;&#32773;&#30340;&#34892;&#21160;&#20013;&#23398;&#20064;&#21644;&#24555;&#36895;&#36866;&#24212;&#23427;&#20204;&#30340;&#34892;&#20026;&#65292;&#20351;&#26080;&#20154;&#26426;&#33021;&#22815;&#36991;&#20813;&#25915;&#20987;&#24182;&#21040;&#36798;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#30830;&#20445;&#21338;&#24328;&#35770;&#20998;&#26512;&#20013;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;Nash&#22343;&#34913;&#26469;&#20445;&#35777;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#27169;&#25311;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#27604;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#23548;&#33322;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#19968;&#20123;&#21442;&#25968;&#22914;&#30456;&#23545;&#26368;&#22823;&#36895;&#24230;&#22914;&#20309;&#24433;&#21709;&#23548;&#33322;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29289;&#29702;&#23454;&#39564;&#65292;&#24182;&#39564;&#35777;&#20102;&#23454;&#26102;&#39134;&#34892;&#20013;&#21463;&#35757;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#20171;&#32461;&#20102;&#25104;&#21151;&#29575;&#28909;&#22270;&#65292;&#20197;&#35828;&#26126;&#31354;&#38388;&#20960;&#20309;&#23545;&#23548;&#33322;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#39033;&#30446;&#32593;&#31449;&#65306;https://gi
&lt;/p&gt;
&lt;p&gt;
Safe navigation of drones in the presence of adversarial physical attacks from multiple pursuers is a challenging task. This paper proposes a novel approach, asynchronous multi-stage deep reinforcement learning (AMS-DRL), to train an adversarial neural network that can learn from the actions of multiple pursuers and adapt quickly to their behavior, enabling the drone to avoid attacks and reach its target. Our approach guarantees convergence by ensuring Nash Equilibrium among agents from the game-theory analysis. We evaluate our method in extensive simulations and show that it outperforms baselines with higher navigation success rates. We also analyze how parameters such as the relative maximum speed affect navigation performance. Furthermore, we have conducted physical experiments and validated the effectiveness of the trained policies in real-time flights. A success rate heatmap is introduced to elucidate how spatial geometry influences navigation outcomes. Project website: https://gi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#20195;&#29702;&#30340;&#26550;&#26500;&#65292;&#23427;&#33021;&#22815;&#20223;&#30495;&#20986;&#20855;&#26377;&#21487;&#20449;&#24230;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#22635;&#20805;&#20132;&#20114;&#24335;&#27801;&#30418;&#29615;&#22659;&#65292;&#20026;&#21019;&#36896;&#26356;&#21152;&#30495;&#23454;&#30340;&#20154;&#26426;&#20132;&#20114;&#20307;&#39564;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.03442</link><description>&lt;p&gt;
&#29983;&#25104;&#20195;&#29702;: &#20154;&#31867;&#34892;&#20026;&#30340;&#20132;&#20114;&#20223;&#30495;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generative Agents: Interactive Simulacra of Human Behavior. (arXiv:2304.03442v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#20195;&#29702;&#30340;&#26550;&#26500;&#65292;&#23427;&#33021;&#22815;&#20223;&#30495;&#20986;&#20855;&#26377;&#21487;&#20449;&#24230;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#22635;&#20805;&#20132;&#20114;&#24335;&#27801;&#30418;&#29615;&#22659;&#65292;&#20026;&#21019;&#36896;&#26356;&#21152;&#30495;&#23454;&#30340;&#20154;&#26426;&#20132;&#20114;&#20307;&#39564;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#30340;&#20154;&#31867;&#34892;&#20026;&#20223;&#30495;&#21487;&#36171;&#33021;&#20110;&#20174;&#27785;&#28024;&#24335;&#29615;&#22659;&#21040;&#20154;&#38469;&#20132;&#27969;&#25490;&#32451;&#31354;&#38388;&#21040;&#21407;&#22411;&#24037;&#20855;&#30340;&#20132;&#20114;&#24335;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#25104;&#20195;&#29702;&#8212;&#8212;&#20855;&#26377;&#21487;&#20449;&#24230;&#30340;&#20154;&#31867;&#34892;&#20026;&#20223;&#30495;&#30340;&#35745;&#31639;&#26426;&#36719;&#20214;&#20195;&#29702;&#12290;&#29983;&#25104;&#20195;&#29702;&#20250;&#36215;&#24202;&#65292;&#20570;&#26089;&#39184;&#65292;&#21435;&#24037;&#20316;&#65307;&#33402;&#26415;&#23478;&#30011;&#30011;&#65292;&#20316;&#23478;&#20889;&#20316;&#65307;&#20182;&#20204;&#24418;&#25104;&#35266;&#28857;&#65292;&#20114;&#30456;&#27880;&#24847;&#65292;&#24182;&#24320;&#22987;&#20132;&#35848;&#65307;&#20182;&#20204;&#22238;&#24518;&#36807;&#21435;&#30340;&#26085;&#23376;&#24182;&#35745;&#21010;&#26410;&#26469;&#12290;&#20026;&#20102;&#20351;&#29983;&#25104;&#20195;&#29702;&#33021;&#22815;&#23454;&#29616;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23384;&#20648;&#20195;&#29702;&#30340;&#32463;&#21382;&#30340;&#23436;&#25972;&#35760;&#24405;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32508;&#21512;&#36825;&#20123;&#35760;&#24518;&#21040;&#26356;&#39640;&#23618;&#27425;&#30340;&#21453;&#24605;&#65292;&#20197;&#21450;&#21160;&#24577;&#26816;&#32034;&#36825;&#20123;&#35760;&#24518;&#20197;&#35268;&#21010;&#34892;&#20026;&#12290;&#25105;&#20204;&#23454;&#20363;&#21270;&#29983;&#25104;&#20195;&#29702;&#20197;&#22635;&#20805;&#21463;&#12298;&#27169;&#25311;&#20154;&#29983;&#12299;&#21551;&#21457;&#30340;&#20132;&#20114;&#24335;&#27801;&#30418;&#29615;&#22659;&#65292;&#26368;&#32456;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#31995;&#32479;&#19982;25&#20010;&#20195;&#29702;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#65292;&#32780;GPT-4&#30340;&#34920;&#29616;&#21017;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.03439</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#21644;GPT-4&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. (arXiv:2304.03439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#65292;&#32780;GPT-4&#30340;&#34920;&#29616;&#21017;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;4&#65288;GPT-4&#65289;&#30340;&#21457;&#24067;&#65292;&#25105;&#20204;&#28212;&#26395;&#20102;&#35299;GPT-4&#22312;&#21508;&#31181;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;LogiQA&#21644;ReClor&#31561;&#24120;&#29992;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#21450;&#20687;AR-LSAT&#36825;&#26679;&#30340;&#26032;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#38656;&#35201;&#36923;&#36753;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#27979;&#35797;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;ChatGPT&#21644;GPT-4&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;ChatGPT&#21644;GPT-4&#20043;&#38388;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#30340;&#34920;&#29616;&#36828;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#12290;GPT-4&#22312;&#25105;&#20204;&#30340;&#25163;&#21160;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#39640;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#21644;GPT-4&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#20026;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing logical reasoning ability is a comprehensive natural language understanding endeavor. With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks. This report analyses multiple logical reasoning datasets, with popular benchmarks like LogiQA and ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice reading comprehension and natural language inference tasks with benchmarks requiring logical reasoning. We further construct a logical reasoning out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4. We also make a performance comparison between ChatGPT and GPT-4. Experiment results show that ChatGPT performs significantly better than the RoBERTa fine-tuning method on most logical reasoning benchmarks. GPT-4 shows even higher performance on our manual tests. Among benchmarks, ChatGPT and GPT-4 do relatively w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35777;&#26126;&#20855;&#26377;&#19981;&#21464;&#34920;&#31034;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#38750;&#32467;&#26500;&#21270;&#28508;&#22312;&#34920;&#31034;&#65292;&#22240;&#27492;&#20351;&#19981;&#21464;&#24615;&#25104;&#20026;&#22495;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.03431</link><description>&lt;p&gt;
&#40065;&#26834;&#19981;&#21464;&#34920;&#31034;&#20013;&#30340;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization In Robust Invariant Representation. (arXiv:2304.03431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35777;&#26126;&#20855;&#26377;&#19981;&#21464;&#34920;&#31034;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#38750;&#32467;&#26500;&#21270;&#28508;&#22312;&#34920;&#31034;&#65292;&#22240;&#27492;&#20351;&#19981;&#21464;&#24615;&#25104;&#20026;&#22495;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#24120;&#35265;&#21464;&#25442;&#30340;&#19981;&#21464;&#34920;&#31034;&#26041;&#27861;&#24120;&#29992;&#20110;&#30446;&#26631;&#35782;&#21035;&#12290;&#23398;&#20064;&#19981;&#21464;&#24615;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#26356;&#23481;&#26131;&#24212;&#29992;&#12290;&#30001;&#20110;&#19981;&#25913;&#21464;&#23545;&#35937;&#22266;&#26377;&#23646;&#24615;&#30340;&#25968;&#25454;&#21464;&#25442;&#26159;&#35782;&#21035;&#20219;&#21153;&#20013;&#20027;&#35201;&#30340;&#22797;&#26434;&#24615;&#26469;&#28304;&#65292;&#23545;&#36825;&#20123;&#21464;&#25442;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#24182;&#31616;&#21270;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#35797;&#22270;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#20855;&#26377;&#26576;&#20123;&#21464;&#25442;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#22312;&#20808;&#21069;&#26410;&#35265;&#22495;&#20013;&#26159;&#21542;&#20173;&#20855;&#26377;&#19981;&#21464;&#24615;&#65311;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#19981;&#21464;&#34920;&#31034;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#38750;&#32467;&#26500;&#21270;&#28508;&#22312;&#34920;&#31034;&#65292;&#22240;&#27492;&#20351;&#19981;&#21464;&#24615;&#25104;&#20026;&#22495;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised approaches for learning representations invariant to common transformations are used quite often for object recognition. Learning invariances makes models more robust and practical to use in real-world scenarios. Since data transformations that do not change the intrinsic properties of the object cause the majority of the complexity in recognition tasks, models that are invariant to these transformations help reduce the amount of training data required. This further increases the model's efficiency and simplifies training. In this paper, we investigate the generalization of invariant representations on out-of-distribution data and try to answer the question: Do model representations invariant to some transformations in a particular seen domain also remain invariant in previously unseen domains? Through extensive experiments, we demonstrate that the invariant model learns unstructured latent representations that are robust to distribution shifts, thus making invariance a de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#36755;&#20986;&#20013;&#30340;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2304.03427</link><description>&lt;p&gt;
&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cleansing Jewel: A Neural Spelling Correction Model Built On Google OCR-ed Tibetan Manuscripts. (arXiv:2304.03427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#36755;&#20986;&#20013;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#25991;&#23398;&#32773;&#22312;&#30740;&#31350;&#21382;&#21490;&#12289;&#23447;&#25945;&#21644;&#31038;&#20250;&#25919;&#27835;&#32467;&#26500;&#31561;&#26041;&#38754;&#32463;&#24120;&#20381;&#36182;&#20110;&#21476;&#20195;&#25163;&#31295;&#12290;&#34429;&#28982;OCR&#25216;&#26415;&#21487;&#20197;&#23558;&#36825;&#20123;&#23453;&#36149;&#25163;&#31295;&#25968;&#23383;&#21270;&#65292;&#20294;&#22810;&#25968;&#25163;&#31295;&#22240;&#30952;&#25439;&#32780;&#36807;&#26102;&#65292;OCR&#31243;&#24207;&#27809;&#21150;&#27861;&#35782;&#21035;&#32763;&#39029;&#30340;&#34394;&#28129;&#25110;&#27745;&#28173;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#36755;&#20986;&#20013;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#20998;&#20026;&#22235;&#20010;&#37096;&#20998;&#65306;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#35757;&#32451;&#21644;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#21407;&#22987;&#34255;&#25991;&#30005;&#23376;&#25991;&#26412;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#29305;&#24449;&#24037;&#31243;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#20004;&#32452;&#32467;&#26500;&#21270;&#25968;&#25454;&#26694;&#8212;&#8212;&#19968;&#32452;&#21305;&#37197;&#30340;&#29609;&#20855;&#25968;&#25454;&#21644;&#19968;&#32452;&#21305;&#37197;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;Transformer&#26550;&#26500;&#20013;&#23454;&#29616;&#20102;&#32622;&#20449;&#24230;&#24471;&#20998;&#26426;&#21046;&#26469;&#25191;&#34892;&#25340;&#20889;&#26657;&#27491;&#20219;&#21153;&#12290;&#26681;&#25454;&#25439;&#22833;&#21644;&#23383;&#31526;&#38169;&#35823;&#29575;&#65292;&#25105;&#20204;&#30340;Transformer + &#32622;&#20449;&#24230;&#24471;&#20998;&#26426;&#21046;&#27604;&#20854;&#20182;&#24120;&#29992;&#30340;&#25340;&#20889;&#26657;&#27491;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scholars in the humanities rely heavily on ancient manuscripts to study history, religion, and socio-political structures in the past. Many efforts have been devoted to digitizing these precious manuscripts using OCR technology, but most manuscripts were blemished over the centuries so that an Optical Character Recognition (OCR) program cannot be expected to capture faded graphs and stains on pages. This work presents a neural spelling correction model built on Google OCR-ed Tibetan Manuscripts to auto-correct OCR-ed noisy output. This paper is divided into four sections: dataset, model architecture, training and analysis. First, we feature-engineered our raw Tibetan etext corpus into two sets of structured data frames -- a set of paired toy data and a set of paired real data. Then, we implemented a Confidence Score mechanism into the Transformer architecture to perform spelling correction tasks. According to the Loss and Character Error Rate, our Transformer + Confidence score mechani
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#38899;&#20048;&#28151;&#38899;&#24037;&#20316;&#27969;&#20013;AI&#25216;&#26415;&#30340;&#24212;&#29992;&#24773;&#20917;&#21450;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#30340;&#37319;&#29992;&#24773;&#20917;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;AI&#28151;&#38899;&#24037;&#20855;&#33021;&#22815;&#20026;&#19994;&#20313;&#29233;&#22909;&#32773;&#25552;&#20379;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;&#32844;&#19994;&#19994;&#20313;&#29233;&#22909;&#32773;&#21644;&#19987;&#19994;&#20154;&#21592;&#38656;&#35201;&#26356;&#20026;&#31934;&#30830;&#30340;&#25511;&#21046;&#21644;&#33258;&#23450;&#20041;&#36873;&#39033;&#65292;&#20197;&#21450;&#36741;&#21161;&#21644;&#21327;&#20316;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.03407</link><description>&lt;p&gt;
&#38899;&#20048;&#28151;&#38899;&#24037;&#20316;&#27969;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#24212;&#29992;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adoption of AI Technology in the Music Mixing Workflow: An Investigation. (arXiv:2304.03407v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#38899;&#20048;&#28151;&#38899;&#24037;&#20316;&#27969;&#20013;AI&#25216;&#26415;&#30340;&#24212;&#29992;&#24773;&#20917;&#21450;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#30340;&#37319;&#29992;&#24773;&#20917;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;AI&#28151;&#38899;&#24037;&#20855;&#33021;&#22815;&#20026;&#19994;&#20313;&#29233;&#22909;&#32773;&#25552;&#20379;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;&#32844;&#19994;&#19994;&#20313;&#29233;&#22909;&#32773;&#21644;&#19987;&#19994;&#20154;&#21592;&#38656;&#35201;&#26356;&#20026;&#31934;&#30830;&#30340;&#25511;&#21046;&#21644;&#33258;&#23450;&#20041;&#36873;&#39033;&#65292;&#20197;&#21450;&#36741;&#21161;&#21644;&#21327;&#20316;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#38899;&#20048;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#27491;&#22312;&#25512;&#21160;&#38899;&#20048;&#21019;&#20316;&#12289;&#21046;&#20316;&#21644;&#28151;&#38899;&#30340;&#37325;&#22823;&#21464;&#21270;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#28151;&#38899;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#24403;&#21069;&#29366;&#24577;&#20197;&#21450;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#23545;&#20854;&#37319;&#32435;&#24773;&#20917;&#12290;&#36890;&#36807;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#12289;&#22522;&#20110;&#38382;&#21367;&#30340;&#30740;&#31350;&#21644;&#20998;&#26512;&#32593;&#32476;&#35770;&#22363;&#65292;&#26412;&#30740;&#31350;&#30830;&#35748;&#20102;&#21253;&#25324;&#19994;&#20313;&#29233;&#22909;&#32773;&#12289;&#32844;&#19994;&#19994;&#20313;&#29233;&#22909;&#32773;&#21644;&#19987;&#19994;&#20154;&#22763;&#22312;&#20869;&#30340;&#19977;&#20010;&#29992;&#25143;&#32676;&#20307;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;AI&#28151;&#38899;&#24037;&#20855;&#33021;&#22815;&#31616;&#21270;&#36807;&#31243;&#24182;&#20026;&#19994;&#20313;&#29233;&#22909;&#32773;&#25552;&#20379;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;&#32844;&#19994;&#19994;&#20313;&#29233;&#22909;&#32773;&#38656;&#35201;&#31934;&#30830;&#30340;&#25511;&#21046;&#21644;&#33258;&#23450;&#20041;&#36873;&#39033;&#65292;&#32780;&#19987;&#19994;&#20154;&#21592;&#38500;&#20102;&#38656;&#35201;&#25511;&#21046;&#21644;&#33258;&#23450;&#20041;&#36873;&#39033;&#22806;&#65292;&#36824;&#38656;&#35201;&#36741;&#21161;&#21644;&#21327;&#20316;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#20026;&#35774;&#35745;&#38024;&#23545;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#30340;&#26377;&#25928;&#30340;AI&#28151;&#38899;&#24037;&#20855;&#25552;&#20379;&#20102;&#31574;&#30053;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of artificial intelligence (AI) technology in the music industry is driving a significant change in the way music is being composed, produced and mixed. This study investigates the current state of AI in the mixing workflows and its adoption by different user groups. Through semi-structured interviews, a questionnaire-based study, and analyzing web forums, the study confirms three user groups comprising amateurs, pro-ams, and professionals. Our findings show that while AI mixing tools can simplify the process and provide decent results for amateurs, pro-ams seek precise control and customization options, while professionals desire control and customization options in addition to assistive and collaborative technologies. The study provides strategies for designing effective AI mixing tools for different user groups and outlines future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#21306;&#22495;&#23545;&#27604;&#30340;&#21307;&#23398;&#22270;&#20687;&#33258;&#30417;&#30563;&#23398;&#20064;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#22810;&#22120;&#23448;&#20998;&#21106;&#31561;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03406</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#21306;&#22495;&#23545;&#27604;&#30340;&#21307;&#23398;&#22270;&#20687;&#33258;&#30417;&#30563;&#23398;&#20064;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Localized Region Contrast for Enhancing Self-Supervised Learning in Medical Image Segmentation. (arXiv:2304.03406v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#21306;&#22495;&#23545;&#27604;&#30340;&#21307;&#23398;&#22270;&#20687;&#33258;&#30417;&#30563;&#23398;&#20064;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#22810;&#22120;&#23448;&#20998;&#21106;&#31561;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#34920;&#26126;&#21487;&#20197;&#20174;&#26080;&#26631;&#31614;&#22270;&#20687;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#36825;&#23548;&#33268;&#20154;&#20204;&#23545;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#20852;&#36259;&#22686;&#21152;&#20102;&#65292;&#22240;&#20026;&#26080;&#26631;&#31614;&#22270;&#20687;&#20016;&#23500;&#32780;&#26377;&#26631;&#31614;&#22270;&#20687;&#24456;&#38590;&#33719;&#24471;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#34987;&#24314;&#27169;&#20026;&#22270;&#20687;&#32423;&#21035;&#30340;&#21028;&#21035;&#25110;&#29983;&#25104;&#20195;&#29702;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#22810;&#22120;&#23448;&#20998;&#21106;&#31561;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#25152;&#38656;&#30340;&#26356;&#32454;&#33268;&#32423;&#21035;&#30340;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#38598;&#25104;&#20102;&#23616;&#37096;&#21306;&#22495;&#23545;&#27604;&#65288;LRC&#65289;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;Felzenszwalb&#31639;&#27861;&#35782;&#21035;&#36229;&#20687;&#32032;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#23545;&#27604;&#37319;&#26679;&#25439;&#22833;&#36827;&#34892;&#23616;&#37096;&#23545;&#27604;&#23398;&#20064;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#22810;&#22120;&#23448;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#24182;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in self-supervised learning have demonstrated that effective visual representations can be learned from unlabeled images. This has led to increased interest in applying self-supervised learning to the medical domain, where unlabeled images are abundant and labeled images are difficult to obtain. However, most self-supervised learning approaches are modeled as image level discriminative or generative proxy tasks, which may not capture the finer level representations necessary for dense prediction tasks like multi-organ segmentation. In this paper, we propose a novel contrastive learning framework that integrates Localized Region Contrast (LRC) to enhance existing self-supervised pre-training methods for medical image segmentation. Our approach involves identifying Super-pixels by Felzenszwalb's algorithm and performing local contrastive learning using a novel contrastive sampling loss. Through extensive experiments on three multi-organ segmentation datasets, we demon
&lt;/p&gt;</description></item><item><title>CAPOT&#20351;&#29992;&#21518;&#35757;&#32451;&#23545;&#27604;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#22122;&#22768;&#26597;&#35810;&#30340;&#20581;&#22766;&#24615;&#65292;&#34920;&#29616;&#31867;&#20284;&#20110;&#25968;&#25454;&#22686;&#24378;&#20294;&#27809;&#26377;&#20854;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2304.03401</link><description>&lt;p&gt;
CAPOT: &#20351;&#29992;&#21518;&#35757;&#32451;&#23545;&#27604;&#23545;&#40784;&#21019;&#24314;&#24378;&#20581;&#30340;&#23494;&#38598;&#26597;&#35810;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
CAPOT: Creating Robust Dense Query Encoders using Post Training Contrastive Alignment. (arXiv:2304.03401v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03401
&lt;/p&gt;
&lt;p&gt;
CAPOT&#20351;&#29992;&#21518;&#35757;&#32451;&#23545;&#27604;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#22122;&#22768;&#26597;&#35810;&#30340;&#20581;&#22766;&#24615;&#65292;&#34920;&#29616;&#31867;&#20284;&#20110;&#25968;&#25454;&#22686;&#24378;&#20294;&#27809;&#26377;&#20854;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#30340;&#25104;&#21151;&#21644;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#30340;&#36827;&#27493;&#20351;&#24471;&#22522;&#20110;&#23494;&#38598;&#21521;&#37327;&#30340;&#26816;&#32034;&#25104;&#20026;&#27573;&#33853;&#21644;&#25991;&#26723;&#25490;&#21517;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#21452;&#32534;&#30721;&#22120;&#34429;&#28982;&#26377;&#25928;&#21644;&#39640;&#25928;&#65292;&#20294;&#23545;&#26597;&#35810;&#20998;&#24067;&#21644;&#22024;&#26434;&#26597;&#35810;&#21464;&#21270;&#24456;&#33030;&#24369;&#12290;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#21152;&#20581;&#22766;&#65292;&#20294;&#20250;&#24341;&#20837;&#35757;&#32451;&#38598;&#29983;&#25104;&#30340;&#24320;&#38144;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#21644;&#32034;&#24341;&#37325;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Contrastive Alignment POst Training (CAPOT)&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#25991;&#26723;&#32534;&#30721;&#22120;&#65292;&#35753;&#26597;&#35810;&#32534;&#30721;&#22120;&#23398;&#20064;&#23558;&#22024;&#26434;&#26597;&#35810;&#19982;&#20854;&#26410;&#26356;&#25913;&#30340;&#26681;&#23545;&#40784;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102; CAPOT &#22312; MSMARCO&#12289;&#33258;&#28982;&#38382;&#39064;&#21644; Trivia QA &#27573;&#33853;&#26816;&#32034;&#30340;&#22024;&#26434;&#21464;&#20307;&#19978;&#65292;&#21457;&#29616; CAPOT &#20855;&#26377;&#19982;&#25968;&#25454;&#22686;&#24378;&#31867;&#20284;&#30340;&#24433;&#21709;&#65292;&#20294;&#27809;&#26377;&#23427;&#30340;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of contextual word representations and advances in neural information retrieval have made dense vector-based retrieval a standard approach for passage and document ranking. While effective and efficient, dual-encoders are brittle to variations in query distributions and noisy queries. Data augmentation can make models more robust but introduces overhead to training set generation and requires retraining and index regeneration. We present Contrastive Alignment POst Training (CAPOT), a highly efficient finetuning method that improves model robustness without requiring index regeneration, the training set optimization, or alteration. CAPOT enables robust retrieval by freezing the document encoder while the query encoder learns to align noisy queries with their unaltered root. We evaluate CAPOT noisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval, finding CAPOT has a similar impact as data augmentation with none of its overhead.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03392</link><description>&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge. (arXiv:2304.03392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;&#34394;&#25311;&#25945;&#32451;&#31995;&#32479;&#65292;&#24110;&#21161;&#24739;&#32773;&#22362;&#25345;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#65288;BCI&#65289;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#39044;&#27979;&#24739;&#32773;&#26159;&#21542;&#20250;&#25191;&#34892;&#30446;&#26631;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#65292;&#20197;&#25351;&#23548;&#20010;&#24615;&#21270;BCI&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#21709;&#24212;&#27700;&#24179;&#30340;&#27169;&#25311;&#24739;&#32773;&#25968;&#25454;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are developing a virtual coaching system that helps patients adhere to behavior change interventions (BCI). Our proposed system predicts whether a patient will perform the targeted behavior and uses counterfactual examples with feature control to guide personalizsation of BCI. We evaluated our prediction model using simulated patient data with varying levels of receptivity to intervention.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22788;&#29702;Wikidata&#38480;&#23450;&#35789;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#38480;&#23450;&#35789;&#36827;&#34892;&#20998;&#31867;&#21644;&#20351;&#29992;&#22810;&#25490;&#24207;&#36923;&#36753;&#35821;&#35328;&#24418;&#24335;&#21270;Wikidata&#27169;&#22411;&#65292;&#20197;&#24212;&#29992;&#20110;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.03375</link><description>&lt;p&gt;
&#25512;&#29702;&#20013;&#22788;&#29702;Wikidata&#38480;&#23450;&#35789;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Handling Wikidata Qualifiers in Reasoning. (arXiv:2304.03375v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22788;&#29702;Wikidata&#38480;&#23450;&#35789;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#38480;&#23450;&#35789;&#36827;&#34892;&#20998;&#31867;&#21644;&#20351;&#29992;&#22810;&#25490;&#24207;&#36923;&#36753;&#35821;&#35328;&#24418;&#24335;&#21270;Wikidata&#27169;&#22411;&#65292;&#20197;&#24212;&#29992;&#20110;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wikidata&#26159;&#19968;&#20010;&#34987;&#35768;&#22810;&#31038;&#21306;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;Wikidata&#35821;&#21477;&#24102;&#26377;&#38480;&#23450;&#35789;&#20540;&#23545;&#65292;&#29992;&#20110;&#25551;&#36848;&#20449;&#24687;&#65292;&#20363;&#22914;&#35821;&#21477;&#30340;&#26377;&#25928;&#19978;&#19979;&#25991;&#65292;&#20854;&#22240;&#26524;&#20851;&#31995;&#65292;&#26469;&#28304;&#31561;&#12290;&#22312;&#25512;&#29702;&#20013;&#22788;&#29702;&#38480;&#23450;&#35789;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23545;&#38480;&#23450;&#31526;&#36827;&#34892;&#20998;&#31867;&#21644;&#23558;Wikidata&#27169;&#22411;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#22810;&#25490;&#24207;&#30340;&#36923;&#36753;&#35821;&#35328;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#35813;&#36923;&#36753;&#19982;&#35821;&#20041;Web&#35268;&#21017;&#35821;&#35328;&#65288;SWRL&#65289;&#30456;&#32467;&#21512;&#65292;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wikidata is a knowledge graph increasingly adopted by many communities for diverse applications. Wikidata statements are annotated with qualifier-value pairs that are used to depict information, such as the validity context of the statement, its causality, provenances, etc. Handling the qualifiers in reasoning is a challenging problem. When defining inference rules (in particular, rules on ontological properties (x subclass of y, z instance of x, etc.)), one must consider the qualifiers, as most of them participate in the semantics of the statements. This poses a complex problem because a) there is a massive number of qualifiers, and b) the qualifiers of the inferred statement are often a combination of the qualifiers in the rule condition. In this work, we propose to address this problem by a) defining a categorization of the qualifiers b) formalizing the Wikidata model with a many-sorted logical language; the sorts of this language are the qualifier categories. We couple this logic w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#35782;&#21035;&#24615;&#30340;DF&#35299;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2304.03365</link><description>&lt;p&gt;
&#22870;&#21169;&#36716;&#31227;&#30340;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Decision-Focused Learning for Reward Transfer. (arXiv:2304.03365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#35782;&#21035;&#24615;&#30340;DF&#35299;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20915;&#31574;&#37325;&#28857;&#65288;Decision-focused&#65292;DF&#65289;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#34987;&#20171;&#32461;&#20026;&#19968;&#31181;&#24378;&#26377;&#21147;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#19987;&#27880;&#20110;&#23398;&#20064;&#26368;&#26377;&#21033;&#20110;&#33719;&#24471;&#39640;&#25253;&#37228;&#30340;MDP&#21160;&#24577;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#19987;&#27880;&#20110;&#30452;&#25509;&#20248;&#21270;&#25253;&#37228;&#26469;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#65292;&#20294;&#20174;MLE&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#19981;&#22815;&#20934;&#30830;&#65292;&#22240;&#27492;&#21487;&#33021;&#23545;&#22870;&#21169;&#20989;&#25968;&#30340;&#21464;&#21270;&#24456;&#33030;&#24369;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;DF&#35299;&#30340;&#38750;&#35782;&#21035;&#24615;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#29609;&#20855;&#31034;&#20363;&#21644;&#21307;&#30103;&#27169;&#25311;&#22120;&#19978;&#23637;&#31034;&#20102;RDF&#26174;&#30528;&#22686;&#21152;&#20102;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused (DF) model-based reinforcement learning has recently been introduced as a powerful algorithm which can focus on learning the MDP dynamics which are most relevant for obtaining high rewards. While this approach increases the performance of agents by focusing the learning towards optimizing for the reward directly, it does so by learning less accurate dynamics (from a MLE standpoint), and may thus be brittle to changes in the reward function. In this work, we develop the robust decision-focused (RDF) algorithm which leverages the non-identifiability of DF solutions to learn models which maximize expected returns while simultaneously learning models which are robust to changes in the reward function. We demonstrate on a variety of toy example and healthcare simulators that RDF significantly increases the robustness of DF to changes in the reward function, without decreasing the overall return the agent obtains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#65292;&#23427;&#21253;&#25324;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;top-K&#37319;&#26679;&#22686;&#24378;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25152;&#26377;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.03344</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#22270;&#21327;&#20316;&#20449;&#21495;&#21435;&#22122;&#19982;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Graph Collaborative Signals Denoising and Augmentation for Recommendation. (arXiv:2304.03344v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#65292;&#23427;&#21253;&#25324;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;top-K&#37319;&#26679;&#22686;&#24378;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25152;&#26377;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21327;&#20316;&#36807;&#28388;&#65288;GCF&#65289;&#26159;&#25429;&#25417;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#38454;&#21327;&#21516;&#20449;&#21495;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;GCF&#30340;&#21452;&#21521;&#37051;&#25509;&#30697;&#38453;&#65292;&#20854;&#23450;&#20041;&#20102;&#22522;&#20110;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#36827;&#34892;&#32858;&#21512;&#30340;&#37051;&#23621;&#65292;&#23545;&#20110;&#26377;&#22823;&#37327;&#20132;&#20114;&#20294;&#19981;&#36275;&#30340;&#29992;&#25143;/&#39033;&#30446;&#26469;&#35828;&#21487;&#33021;&#26159;&#22024;&#26434;&#30340;&#12290;&#27492;&#22806;&#65292;&#37051;&#25509;&#30697;&#38453;&#24573;&#30053;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#32858;&#21512;&#30340;&#26377;&#30410;&#37051;&#23621;&#30340;&#33539;&#22260;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#65292;&#23427;&#21253;&#25324;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#20197;&#24179;&#34913;&#25152;&#26377;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#25968;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#26469;&#33719;&#24471;&#29992;&#25143;/&#39033;&#30446;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;top-K&#37319;&#26679;&#22686;&#24378;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#12290;&#25105;&#20204;&#36824;&#22686;&#24378;&#20102;&#23545;&#31216;&#30340;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30456;&#20851;&#32452;&#20214;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25152;&#26377;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph collaborative filtering (GCF) is a popular technique for capturing high-order collaborative signals in recommendation systems. However, GCF's bipartite adjacency matrix, which defines the neighbors being aggregated based on user-item interactions, can be noisy for users/items with abundant interactions and insufficient for users/items with scarce interactions. Additionally, the adjacency matrix ignores user-user and item-item correlations, which can limit the scope of beneficial neighbors being aggregated.  In this work, we propose a new graph adjacency matrix that incorporates user-user and item-item correlations, as well as a properly designed user-item interaction matrix that balances the number of interactions across all users. To achieve this, we pre-train a graph-based recommendation method to obtain users/items embeddings, and then enhance the user-item interaction matrix via top-K sampling. We also augment the symmetric user-user and item-item correlation components to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#22823;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#21028;&#23450;&#26159;&#21542;&#23384;&#22312;&#26159;&#19968;&#20010;NP&#23436;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;&#35745;&#31639;&#26368;&#22823;&#22240;&#23376;&#20998;&#35299;&#30340;&#31639;&#27861;Ord2Factor&#12290;</title><link>http://arxiv.org/abs/2304.03338</link><description>&lt;p&gt;
&#26368;&#22823;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Maximal Ordinal Two-Factorizations. (arXiv:2304.03338v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#22823;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#21028;&#23450;&#26159;&#21542;&#23384;&#22312;&#26159;&#19968;&#20010;NP&#23436;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;&#35745;&#31639;&#26368;&#22823;&#22240;&#23376;&#20998;&#35299;&#30340;&#31639;&#27861;Ord2Factor&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#24418;&#24335;&#32972;&#26223;&#20013;&#65292;&#24207;&#25968;&#22240;&#23376;&#26159;&#20854;&#20851;&#31995;&#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#27010;&#24565;&#26684;&#20013;&#30340;&#38142;&#65292;&#21363;&#23545;&#24212;&#20110;&#32447;&#24615;&#39034;&#24207;&#30340;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#12290;&#20026;&#20102;&#21487;&#35270;&#21270;&#24418;&#24335;&#19978;&#19979;&#25991;&#20013;&#30340;&#25968;&#25454;&#65292;Ganter&#21644;Glodeanu&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#20010;&#24207;&#25968;&#22240;&#23376;&#30340;&#21452;&#22270;&#12290;&#20026;&#20102;&#20351;&#21452;&#22270;&#26377;&#29992;&#65292;&#37325;&#35201;&#30340;&#26159;&#36825;&#20123;&#22240;&#23376;&#23613;&#21487;&#33021;&#21253;&#21547;&#26356;&#22810;&#25968;&#25454;&#28857;&#65292;&#21363;&#35206;&#30422;&#23613;&#21487;&#33021;&#22810;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#30740;&#31350;&#36825;&#26679;&#30340;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30465;&#30053;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#30340;&#24418;&#24335;&#32972;&#26223;&#20013;&#20004;&#20010;&#22240;&#23376;&#30340;&#19981;&#30456;&#20132;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21028;&#23450;&#32473;&#23450;&#22823;&#23567;&#30340;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#26159;&#21542;&#23384;&#22312;&#26159;&#19968;&#20010;NP&#23436;&#20840;&#38382;&#39064;&#65292;&#36825;&#20351;&#24471;&#35745;&#31639;&#26368;&#22823;&#22240;&#23376;&#20998;&#35299;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31639;&#27861;Ord2Factor&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#35745;&#31639;&#22823;&#30340;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a formal context, an ordinal factor is a subset of its incidence relation that forms a chain in the concept lattice, i.e., a part of the dataset that corresponds to a linear order. To visualize the data in a formal context, Ganter and Glodeanu proposed a biplot based on two ordinal factors. For the biplot to be useful, it is important that these factors comprise as much data points as possible, i.e., that they cover a large part of the incidence relation. In this work, we investigate such ordinal two-factorizations. First, we investigate for formal contexts that omit ordinal two-factorizations the disjointness of the two factors. Then, we show that deciding on the existence of two-factorizations of a given size is an NP-complete problem which makes computing maximal factorizations computationally expensive. Finally, we provide the algorithm Ord2Factor that allows us to compute large ordinal two-factorizations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24403;&#21069;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#25918;&#23556;&#23398;&#24212;&#29992;&#20013;&#30340;&#38590;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#25512;&#29702;&#21644;&#35777;&#26126;&#30340;&#35299;&#37322;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25918;&#23556;&#23398;&#26696;&#20363;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2304.03318</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21644;&#35270;&#35273;&#25512;&#29702;&#65306;&#26469;&#33258;&#25918;&#23556;&#23398;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Explainable AI And Visual Reasoning: Insights From Radiology. (arXiv:2304.03318v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24403;&#21069;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#25918;&#23556;&#23398;&#24212;&#29992;&#20013;&#30340;&#38590;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#25512;&#29702;&#21644;&#35777;&#26126;&#30340;&#35299;&#37322;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25918;&#23556;&#23398;&#26696;&#20363;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20160;&#20040;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#25918;&#23556;&#23398;&#20013;&#30340;&#35299;&#37322;&#65292;&#23613;&#31649;&#25215;&#35834;&#36879;&#26126;&#24230;&#65292;&#20173;&#28982;&#26080;&#27861;&#33719;&#24471;&#20154;&#31867;&#30340;&#20449;&#20219;&#65311;&#24403;&#21069;&#30340;XAI&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#20851;&#39044;&#27979;&#30340;&#35777;&#26126;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#28385;&#36275;&#20174;&#19994;&#32773;&#30340;&#38656;&#27714;&#12290;&#36825;&#20123;XAI&#35299;&#37322;&#32570;&#20047;&#30452;&#35266;&#30340;&#35777;&#25454;&#22522;&#30784;&#35206;&#30422;&#65292;&#36825;&#26159;&#37319;&#29992;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19982;&#20256;&#32479;&#30340;&#28909;&#22270;&#31561;&#35270;&#35273;&#35299;&#37322;&#19981;&#21516;&#65292;&#27169;&#20223;&#20154;&#31867;&#25512;&#29702;&#21644;&#35777;&#26126;&#30340;XAI&#35299;&#37322;&#21487;&#33021;&#27604;&#36739;&#26377;&#29992;&#21644;&#21487;&#38752;&#12290;&#36890;&#36807;&#25918;&#23556;&#23398;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25918;&#23556;&#23398;&#20174;&#19994;&#32773;&#22914;&#20309;&#35753;&#20854;&#20182;&#20174;&#19994;&#32773;&#30475;&#21040;&#35786;&#26029;&#32467;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#32570;&#20047;&#36825;&#31181;&#35777;&#25454;&#22522;&#30784;&#65292;&#22240;&#27492;&#26410;&#33021;&#24341;&#36215;&#28508;&#22312;&#29992;&#25143;&#30340;&#20449;&#20219;&#21644;&#37319;&#29992;&#12290;&#26412;&#30740;&#31350;&#30340;&#27934;&#35265;&#21487;&#33021;&#36866;&#29992;&#20110;&#22522;&#20110;&#20154;&#31867;&#25512;&#29702;&#21644;&#35777;&#26126;&#30340;&#38754;&#21521;&#20154;&#31867;&#35299;&#37322;&#35774;&#35745;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why do explainable AI (XAI) explanations in radiology, despite their promise of transparency, still fail to gain human trust? Current XAI approaches provide justification for predictions, however, these do not meet practitioners' needs. These XAI explanations lack intuitive coverage of the evidentiary basis for a given classification, posing a significant barrier to adoption. We posit that XAI explanations that mirror human processes of reasoning and justification with evidence may be more useful and trustworthy than traditional visual explanations like heat maps. Using a radiology case study, we demonstrate how radiology practitioners get other practitioners to see a diagnostic conclusion's validity. Machine-learned classifications lack this evidentiary grounding and consequently fail to elicit trust and adoption by potential users. Insights from this study may generalize to guiding principles for human-centered explanation design based on human reasoning and justification of evidence
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;NARS&#21644;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#24207;&#21015;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;NARS&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#37117;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.03291</link><description>&lt;p&gt;
&#27604;&#36739;NARS&#21644;&#24378;&#21270;&#23398;&#20064;&#65306;&#23545;ONA&#21644;$Q$-Learning&#31639;&#27861;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparing NARS and Reinforcement Learning: An Analysis of ONA and $Q$-Learning Algorithms. (arXiv:2304.03291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;NARS&#21644;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#24207;&#21015;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;NARS&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#37117;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#22522;&#20110;&#24207;&#21015;&#20219;&#21153;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;RL&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#20173;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#21644;&#21019;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20854;&#20013;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#26367;&#20195;&#26041;&#26696;&#26159;&#38750;&#20844;&#29702;&#25512;&#29702;&#31995;&#32479;&#65288;NARS&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35748;&#30693;&#25512;&#29702;&#26694;&#26550;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;NARS&#20316;&#20026;RL&#26367;&#20195;&#26041;&#26696;&#22312;&#35299;&#20915;&#22522;&#20110;&#24207;&#21015;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#20351;&#29992;Open AI gym&#21019;&#24314;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#23545;ONA&#20316;&#20026;NARS&#23454;&#29616;&#21644;$Q$-Learning&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#36825;&#20123;&#29615;&#22659;&#20855;&#26377;&#19981;&#21516;&#30340;&#38590;&#24230;&#32423;&#21035;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#19981;&#31561;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#65292;NARS&#26159;&#19968;&#20010;&#26377;&#31454;&#20105;&#21147;&#30340;RL&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, reinforcement learning (RL) has emerged as a popular approach for solving sequence-based tasks in machine learning. However, finding suitable alternatives to RL remains an exciting and innovative research area. One such alternative that has garnered attention is the Non-Axiomatic Reasoning System (NARS), which is a general-purpose cognitive reasoning framework. In this paper, we delve into the potential of NARS as a substitute for RL in solving sequence-based tasks. To investigate this, we conduct a comparative analysis of the performance of ONA as an implementation of NARS and $Q$-Learning in various environments that were created using the Open AI gym. The environments have different difficulty levels, ranging from simple to complex. Our results demonstrate that NARS is a promising alternative to RL, with competitive performance in diverse environments, particularly in non-deterministic ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#29305;&#24449;&#34701;&#21512;&#65288;AFF&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#29305;&#24449;&#34701;&#21512;&#36807;&#31243;&#26469;&#22686;&#24378;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#33021;&#22815;&#26681;&#25454;&#25968;&#25454;&#29305;&#24449;&#21644;&#27169;&#22411;&#35201;&#27714;&#33258;&#36866;&#24212;&#34701;&#21512;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.03290</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#29305;&#24449;&#34701;&#21512;&#65306;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Adaptive Feature Fusion: Enhancing Generalization in Deep Learning Models. (arXiv:2304.03290v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#29305;&#24449;&#34701;&#21512;&#65288;AFF&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#29305;&#24449;&#34701;&#21512;&#36807;&#31243;&#26469;&#22686;&#24378;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#33021;&#22815;&#26681;&#25454;&#25968;&#25454;&#29305;&#24449;&#21644;&#27169;&#22411;&#35201;&#27714;&#33258;&#36866;&#24212;&#34701;&#21512;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#33021;&#20250;&#21463;&#21040;&#29305;&#24449;&#34701;&#21512;&#25216;&#26415;&#30340;&#38480;&#21046;&#32780;&#21463;&#21040;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#24615;&#30340;&#26041;&#27861;&#65292;&#33258;&#36866;&#24212;&#29305;&#24449;&#34701;&#21512;&#65288;AFF&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#29305;&#24449;&#34920;&#31034;&#30340;&#34701;&#21512;&#36807;&#31243;&#26469;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;AFF&#26694;&#26550;&#26088;&#22312;&#23558;&#34701;&#21512;&#23618;&#34701;&#20837;&#21040;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#65292;&#23454;&#29616;&#26080;&#32541;&#38598;&#25104;&#21644;&#25913;&#36827;&#24615;&#33021;&#12290;&#36890;&#36807;&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;AFF&#33021;&#22815;&#26681;&#25454;&#25968;&#25454;&#29305;&#24449;&#21644;&#27169;&#22411;&#35201;&#27714;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#29305;&#24449;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;AFF&#26694;&#26550;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#21253;&#25324;&#36866;&#29992;&#20110;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34701;&#21512;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning models have demonstrated remarkable success in various domains, such as computer vision, natural language processing, and speech recognition. However, the generalization capabilities of these models can be negatively impacted by the limitations of their feature fusion techniques. This paper introduces an innovative approach, Adaptive Feature Fusion (AFF), to enhance the generalization of deep learning models by dynamically adapting the fusion process of feature representations.  The proposed AFF framework is designed to incorporate fusion layers into existing deep learning architectures, enabling seamless integration and improved performance. By leveraging a combination of data-driven and model-based fusion strategies, AFF is able to adaptively fuse features based on the underlying data characteristics and model requirements. This paper presents a detailed description of the AFF framework, including the design and implementation of fusion layers for vario
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20851;&#27880;&#20110;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20013;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#21512;&#25104;&#25968;&#23398;&#31243;&#24207;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;CodeT5&#21644;&#20351;&#29992;GPT-3&#26469;&#29983;&#25104;&#38656;&#35201;&#30340;&#31034;&#20363;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2304.03287</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20013;&#30340;&#25968;&#23398;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Synthesis of Mathematical programs from Natural Language Specifications. (arXiv:2304.03287v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20851;&#27880;&#20110;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20013;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#21512;&#25104;&#25968;&#23398;&#31243;&#24207;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;CodeT5&#21644;&#20351;&#29992;GPT-3&#26469;&#29983;&#25104;&#38656;&#35201;&#30340;&#31034;&#20363;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#21830;&#19994;&#39046;&#22495;&#20013;&#36935;&#21040;&#30340;&#20960;&#20010;&#20915;&#31574;&#38382;&#39064;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#25968;&#23398;&#31243;&#24207;&#65292;&#21363;&#20248;&#21270;&#38382;&#39064;&#12290;&#36827;&#34892;&#36825;&#31181;&#24314;&#27169;&#30340;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#28041;&#21450;&#21040;&#21463;&#36807;&#36816;&#31609;&#23398;&#21644;&#39640;&#32423;&#31639;&#27861;&#22521;&#35757;&#30340;&#19987;&#23478;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;&#31243;&#24207;&#21644;&#20195;&#30721;&#21512;&#25104;&#65292;AutoML&#65292;&#23398;&#20064;&#20248;&#21270;&#31561;&#26041;&#38754;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#20154;&#20851;&#27880;&#33258;&#21160;&#21270;&#21512;&#25104;&#25968;&#23398;&#31243;&#24207;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24819;&#35937;&#19968;&#31181;&#24773;&#26223;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24314;&#27169;&#30340;&#35268;&#33539;&#65292;&#21363;&#30446;&#26631;&#21644;&#32422;&#26463;&#20197;&#33258;&#28982;&#35821;&#35328;&#30340;&#24418;&#24335;&#34920;&#36798;&#65292;&#24182;&#19988;&#24517;&#39035;&#20174;&#36825;&#26679;&#30340;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20013;&#21512;&#25104;&#25968;&#23398;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20351;&#29992;&#24102;&#26377;&#25968;&#25454;&#22686;&#24378;&#21644;&#26463;&#21518;&#22788;&#29702;&#30340;CodeT5&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#21033;&#29992;GPT-3&#36827;&#34892;&#32972;&#32763;&#35793;&#20197;&#29983;&#25104;&#21512;&#25104;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#32447;&#24615;&#35268;&#21010;&#35268;&#21017;&#26469;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several decision problems that are encountered in various business domains can be modeled as mathematical programs, i.e. optimization problems. The process of conducting such modeling often requires the involvement of experts trained in operations research and advanced algorithms. Surprisingly, despite the significant advances in the methods for program and code synthesis, AutoML, learning to optimize etc., there has been little or no attention paid to automating the task of synthesizing mathematical programs. We imagine a scenario where the specifications for modeling, i.e. the objective and constraints are expressed in an unstructured form in natural language (NL) and the mathematical program has to be synthesized from such an NL specification. In this work we evaluate the efficacy of employing CodeT5 with data augmentation and post-processing of beams. We utilize GPT-3 with back translation for generation of synthetic examples. Further we apply rules of linear programming to score b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#21452;&#37325;&#24130;&#24459;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#29420;&#29305;&#30340;&#24615;&#33021;&#26435;&#34913;&#21069;&#27839;&#65292;&#24182;&#24314;&#31435;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#26679;&#26412;&#27604;&#20363;&#36873;&#25321;&#20248;&#21270;&#38382;&#39064;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03216</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;Pareto&#21069;&#27839;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Pareto Front of Multilingual Neural Machine Translation. (arXiv:2304.03216v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#21452;&#37325;&#24130;&#24459;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#29420;&#29305;&#30340;&#24615;&#33021;&#26435;&#34913;&#21069;&#27839;&#65292;&#24182;&#24314;&#31435;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#26679;&#26412;&#27604;&#20363;&#36873;&#25321;&#20248;&#21270;&#38382;&#39064;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#32473;&#23450;&#26041;&#21521;&#30340;&#27867;&#21270;&#24615;&#33021;&#22914;&#20309;&#38543;&#20854;&#37319;&#26679;&#27604;&#20363;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;&#36890;&#36807;&#35757;&#32451;200&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#12289;&#26041;&#21521;&#21644;&#24635;&#20219;&#21153;&#25968;&#37327;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#23384;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#26102;&#65292;&#26631;&#37327;&#21270;&#23548;&#33268;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26435;&#34913;&#21069;&#27839;&#65292;&#35813;&#21069;&#27839;&#20559;&#31163;&#20102;&#20256;&#32479;&#30340;Pareto&#21069;&#27839;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#37325;&#24130;&#24459;&#26469;&#39044;&#27979;MNMT&#20013;&#29420;&#29305;&#30340;&#24615;&#33021;&#26435;&#34913;&#21069;&#27839;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#35821;&#35328;&#12289;&#25968;&#25454;&#20805;&#36275;&#24615;&#21644;&#20219;&#21153;&#25968;&#37327;&#26041;&#38754;&#37117;&#24456;&#40065;&#26834;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;MNMT&#20013;&#30340;&#26679;&#26412;&#27604;&#20363;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#22522;&#20110;&#21452;&#37325;&#24130;&#24459;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study how the generalization performance of a given direction changes with its sampling ratio in Multilingual Neural Machine Translation (MNMT). By training over 200 multilingual models with various model sizes, directions, and total numbers of tasks, we find that scalarization leads to a multitask trade-off front that deviates from the traditional Pareto front when there exists data imbalance in the training corpus. That is, the performance of certain translation directions does not improve with the increase of its weight in the multi-task optimization objective, which poses greater challenge to improve the overall performance of all directions. Based on our observations, we propose the Double Power Law to predict the unique performance trade-off front in MNMT, which is robust across various languages, data adequacy and number of tasks. Finally, we formulate sample ratio selection in MNMT as an optimization problem based on the Double Power Law, which achieves better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;ACTION++&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#35299;&#21078;&#23545;&#27604;&#26469;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2304.02689</link><description>&lt;p&gt;
ACTION++&#65306;&#20351;&#29992;&#33258;&#36866;&#24212;&#35299;&#21078;&#23545;&#27604;&#24230;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast. (arXiv:2304.02689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;ACTION++&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#35299;&#21078;&#23545;&#27604;&#26469;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25968;&#25454;&#36890;&#24120;&#34920;&#29616;&#20026;&#38271;&#23614;&#20998;&#24067;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#36825;&#33258;&#28982;&#23548;&#33268;&#23569;&#25968;&#31867;&#21035;&#65288;&#21363;&#36793;&#30028;&#21306;&#22495;&#25110;&#32597;&#35265;&#29289;&#20307;&#65289;&#30340;&#20998;&#31867;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#37197;&#22791;&#26080;&#30417;&#30563;&#23545;&#27604;&#26631;&#20934;&#65292;&#22312;&#38271;&#23614;&#22330;&#26223;&#20013;&#26174;&#30528;&#25913;&#36827;&#20102;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#22312;&#31867;&#21035;&#20998;&#24067;&#20063;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#26631;&#35760;&#25968;&#25454;&#37096;&#20998;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACTION++&#65292;&#19968;&#31181;&#25913;&#36827;&#30340;&#20855;&#26377;&#33258;&#36866;&#24212;&#35299;&#21078;&#23545;&#27604;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#21307;&#23398;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical data often exhibits long-tail distributions with heavy class imbalance, which naturally leads to difficulty in classifying the minority classes (i.e., boundary regions or rare objects). Recent work has significantly improved semi-supervised medical image segmentation in long-tailed scenarios by equipping them with unsupervised contrastive criteria. However, it remains unclear how well they will perform in the labeled portion of data where class distribution is also highly imbalanced. In this work, we present ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. Specifically, we propose an adaptive supervised contrastive loss, where we first compute the optimal locations of class centers uniformly distributed on the embedding space (i.e., off-line), and then perform online contrastive matching training by encouraging different class features to adaptively match these distinct and uniformly distributed cla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#31561;&#21464;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#31561;&#21464;&#24615;&#27979;&#37327;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#12289;&#20943;&#23569;&#27169;&#22411;&#23481;&#37327;&#21644;&#21367;&#31215;&#25805;&#20316;&#21487;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#31561;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02628</link><description>&lt;p&gt;
&#24433;&#21709;&#28145;&#24230;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#20013;&#23398;&#20064;&#31561;&#21464;&#24615;&#30340;&#22240;&#32032;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Affects Learned Equivariance in Deep Image Recognition Models?. (arXiv:2304.02628v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#31561;&#21464;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#31561;&#21464;&#24615;&#27979;&#37327;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#12289;&#20943;&#23569;&#27169;&#22411;&#23481;&#37327;&#21644;&#21367;&#31215;&#25805;&#20316;&#21487;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#19982;&#20960;&#20309;&#21464;&#25442;&#30456;&#20851;&#30340;&#31561;&#21464;&#24615;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12289;&#21442;&#25968;&#25928;&#29575;&#21644;&#23545;&#22495;&#22806;&#36879;&#35270;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#12290;&#24403;&#31561;&#21464;&#24615;&#27809;&#26377;&#34987;&#35774;&#35745;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#26102;&#65292;&#32593;&#32476;&#20173;&#28982;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#31561;&#21464;&#20989;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#31561;&#21464;&#24615;&#27979;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#23398;&#20064;&#21040;&#30340;&#31561;&#21464;&#24615;&#65292;&#24182;&#21457;&#29616;&#23398;&#20064;&#21040;&#30340;&#32763;&#35793;&#31561;&#21464;&#24615;&#19982;&#22312;ImageNet&#19978;&#30340;&#39564;&#35777;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#31561;&#21464;&#24615;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#12289;&#20943;&#23569;&#27169;&#22411;&#23481;&#37327;&#20197;&#21450;&#21367;&#31215;&#24418;&#24335;&#30340;&#24402;&#32435;&#20559;&#24046;&#21487;&#20197;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#26356;&#39640;&#30340;&#23398;&#20064;&#21040;&#30340;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equivariance w.r.t. geometric transformations in neural networks improves data efficiency, parameter efficiency and robustness to out-of-domain perspective shifts. When equivariance is not designed into a neural network, the network can still learn equivariant functions from the data. We quantify this learned equivariance, by proposing an improved measure for equivariance. We find evidence for a correlation between learned translation equivariance and validation accuracy on ImageNet. We therefore investigate what can increase the learned equivariance in neural networks, and find that data augmentation, reduced model capacity and inductive bias in the form of convolutions induce higher learned equivariance in neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GPTEval&#65292;&#19968;&#20010;&#21033;&#29992;&#38142;&#24335;&#24605;&#32771;&#21644;&#24418;&#24335;&#22635;&#20805;&#35780;&#20215;NLG&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;GPTEval&#32467;&#21512;GPT-4&#21462;&#24471;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16634</link><description>&lt;p&gt;
GPTEval&#65306;&#20351;&#29992;GPT-4&#21644;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#26469;&#35780;&#20272;NLG
&lt;/p&gt;
&lt;p&gt;
GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment. (arXiv:2303.16634v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GPTEval&#65292;&#19968;&#20010;&#21033;&#29992;&#38142;&#24335;&#24605;&#32771;&#21644;&#24418;&#24335;&#22635;&#20805;&#35780;&#20215;NLG&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;GPTEval&#32467;&#21512;GPT-4&#21462;&#24471;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31995;&#32479;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#24456;&#38590;&#36827;&#34892;&#33258;&#21160;&#27979;&#37327;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;BLEU&#21644;ROUGE&#24050;&#34987;&#35777;&#26126;&#22312;&#38656;&#35201;&#21019;&#36896;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#30456;&#23545;&#36739;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#26080;&#21442;&#32771;&#30340;NLG&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#20123;&#27169;&#22411;&#36866;&#29992;&#20110;&#32570;&#20047;&#20154;&#31867;&#21442;&#32771;&#30340;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#20173;&#28982;&#27604;&#20013;&#31561;&#35268;&#27169;&#30340;&#31070;&#32463;&#35780;&#20272;&#22120;&#30340;&#20154;&#31867;&#23545;&#24212;&#24230;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPTEval&#65292;&#19968;&#20010;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#21644;&#24418;&#24335;&#22635;&#20805;&#33539;&#24335;&#26469;&#35780;&#20272;NLG&#36755;&#20986;&#36136;&#37327;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#20010;&#29983;&#25104;&#20219;&#21153;&#65292;&#25991;&#26412;&#25688;&#35201;&#21644;&#23545;&#35805;&#29983;&#25104;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;GPTEval&#32467;&#21512;GPT-4&#20316;&#20026;&#39592;&#24178;&#27169;&#22411;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present GPTEval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that GPTEval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#20960;&#20309;&#23398;&#20064;&#35270;&#28857;&#31561;&#21464;&#24615;&#20197;&#25552;&#39640;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#23450;&#20301;&#31934;&#24230;&#30340;&#26694;&#26550;VEDet&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;transformer&#26550;&#26500;&#21644;&#35270;&#35282;&#26465;&#20214;&#30340;&#26597;&#35810;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.14548</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#35266;&#28857;&#31561;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Viewpoint Equivariance for Multi-View 3D Object Detection. (arXiv:2303.14548v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#20960;&#20309;&#23398;&#20064;&#35270;&#28857;&#31561;&#21464;&#24615;&#20197;&#25552;&#39640;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#23450;&#20301;&#31934;&#24230;&#30340;&#26694;&#26550;VEDet&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;transformer&#26550;&#26500;&#21644;&#35270;&#35282;&#26465;&#20214;&#30340;&#26597;&#35810;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#26159;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#29616;&#20195;&#26041;&#27861;&#20391;&#37325;&#20110;&#20174;&#22810;&#35270;&#35282;&#30456;&#26426;&#36755;&#20837;&#25512;&#29702;&#21644;&#35299;&#30721;&#29289;&#20307;&#36793;&#30028;&#26694;&#12290;&#26412;&#25991;&#21033;&#29992;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#22312;&#19977;&#32500;&#22330;&#26223;&#29702;&#35299;&#21644;&#20960;&#20309;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#20171;&#32461;&#20102;VEDet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#28857;&#24863;&#30693;&#21644;&#31561;&#21464;&#24615;&#21033;&#29992;&#19977;&#32500;&#22810;&#35270;&#35282;&#20960;&#20309;&#26469;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#12290;VEDet&#21033;&#29992;&#22522;&#20110;&#26597;&#35810;&#30340;transformer&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#23558;&#22270;&#20687;&#29305;&#24449;&#21644;&#26469;&#33258;&#23427;&#20204;&#30340;&#19977;&#32500;&#36879;&#35270;&#20960;&#20309;&#30340;&#20301;&#32622;&#32534;&#30721;&#30456;&#32467;&#21512;&#26469;&#32534;&#30721;&#19977;&#32500;&#22330;&#26223;&#12290;&#25105;&#20204;&#22312;&#36755;&#20986;&#23618;&#35774;&#35745;&#20102;&#35270;&#35282;&#26465;&#20214;&#30340;&#26597;&#35810;&#65292;&#36825;&#20351;&#24471;&#22312;&#35757;&#32451;&#26399;&#38388;&#29983;&#25104;&#22810;&#20010;&#34394;&#25311;&#24103;&#65292;&#36890;&#36807;&#24378;&#21046;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#26469;&#23398;&#20064;&#35270;&#28857;&#31561;&#21464;&#24615;&#12290;&#22312;&#36755;&#20837;&#23618;&#27880;&#20837;&#30340;&#22810;&#35270;&#35282;&#20960;&#20309;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#24182;&#22312;&#25439;&#22833;&#23618;&#20013;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#22320;&#29702;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D object detection from visual sensors is a cornerstone capability of robotic systems. State-of-the-art methods focus on reasoning and decoding object bounding boxes from multi-view camera input. In this work we gain intuition from the integral role of multi-view consistency in 3D scene understanding and geometric learning. To this end, we introduce VEDet, a novel 3D object detection framework that exploits 3D multi-view geometry to improve localization through viewpoint awareness and equivariance. VEDet leverages a query-based transformer architecture and encodes the 3D scene by augmenting image features with positional encodings from their 3D perspective geometry. We design view-conditioned queries at the output level, which enables the generation of multiple virtual frames during training to learn viewpoint equivariance by enforcing multi-view consistency. The multi-view geometry injected at the input level as positional encodings and regularized at the loss level provides rich geo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23398;&#29983;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#26102;&#65292;&#23436;&#32654;&#27867;&#21270;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#36755;&#20837;&#32500;&#24230;&#27604;&#65292;&#23398;&#29983;&#37117;&#26080;&#27861;&#23436;&#32654;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.14083</link><description>&lt;p&gt;
&#23398;&#29983;-&#25945;&#24072;&#26694;&#26550;&#19979;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning for the Random Feature Model in the Student-Teacher Framework. (arXiv:2303.14083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23398;&#29983;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#26102;&#65292;&#23436;&#32654;&#27867;&#21270;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#36755;&#20837;&#32500;&#24230;&#27604;&#65292;&#23398;&#29983;&#37117;&#26080;&#27861;&#23436;&#32654;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#38543;&#30528;&#26435;&#37325;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#20854;&#24615;&#33021;&#36890;&#24120;&#20250;&#25552;&#39640;&#65292;&#23548;&#33268;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#31532;&#19968;&#23618;&#26159;&#20923;&#32467;&#30340;&#65292;&#32780;&#26368;&#21518;&#19968;&#23618;&#26159;&#21487;&#35757;&#32451;&#30340;&#65292;&#31216;&#20026;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#23398;&#29983;-&#25945;&#24072;&#26694;&#26550;&#19979;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#36890;&#36807;&#23548;&#20986;&#19968;&#32452;&#23398;&#20064;&#21160;&#24577;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#36755;&#20837;&#32500;&#24230;&#27604;&#65292;&#23398;&#29983;&#37117;&#26080;&#27861;&#23436;&#32654;&#27867;&#21270;&#65292;&#24182;&#35745;&#31639;&#38750;&#38646;&#28176;&#36817;&#27867;&#21270;&#35823;&#24046;&#12290;&#21482;&#26377;&#24403;&#23398;&#29983;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#26102;&#65292;&#25165;&#26377;&#21487;&#33021;&#23454;&#29616;&#23436;&#32654;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are widely used prediction algorithms whose performance often improves as the number of weights increases, leading to over-parametrization. We consider a two-layered neural network whose first layer is frozen while the last layer is trainable, known as the random feature model. We study over-parametrization in the context of a student-teacher framework by deriving a set of differential equations for the learning dynamics. For any finite ratio of hidden layer size and input dimension, the student cannot generalize perfectly, and we compute the non-zero asymptotic generalization error. Only when the student's hidden layer size is exponentially larger than the input dimension, an approach to perfect generalization is possible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#22312;&#38750;&#20984;&#26465;&#20214;&#19979;&#20351;&#29992;&#28145;&#23618;&#27169;&#22411;&#21644;&#26356;&#22797;&#26434;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12922</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Fragility of Influence Functions. (arXiv:2303.12922v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#22312;&#38750;&#20984;&#26465;&#20214;&#19979;&#20351;&#29992;&#28145;&#23618;&#27169;&#22411;&#21644;&#26356;&#22797;&#26434;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26377;&#24456;&#22810;&#35770;&#25991;&#33268;&#21147;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#39564;&#35777;&#36825;&#20123;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#25110;&#21487;&#20449;&#24230;&#12290;&#26368;&#36817;&#65292;&#24433;&#21709;&#20989;&#25968;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21333;&#20010;&#26679;&#26412;&#19978;&#30340;&#28789;&#25935;&#24230;&#30340;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#24433;&#21709;&#20989;&#25968;&#26131;&#21463;&#22122;&#22768;&#21644;&#25968;&#25454;&#20998;&#24067;&#19981;&#23545;&#31216;&#24615;&#24433;&#21709;&#65292;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25506;&#31350;&#24433;&#21709;&#20989;&#25968;&#32972;&#21518;&#30340;&#26426;&#29702;&#65292;&#20174;&#32780;&#20026;&#22686;&#24378;&#24433;&#21709;&#20989;&#25968;&#30340;&#40065;&#26834;&#24615;&#25552;&#20379;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last few years, many works have tried to explain the predictions of deep learning models. Few methods, however, have been proposed to verify the accuracy or faithfulness of these explanations. Recently, influence functions, which is a method that approximates the effect that leave-one-out training has on the loss function, has been shown to be fragile. The proposed reason for their fragility remains unclear. Although previous work suggests the use of regularization to increase robustness, this does not hold in all cases. In this work, we seek to investigate the experiments performed in the prior work in an effort to understand the underlying mechanisms of influence function fragility. First, we verify influence functions using procedures from the literature under conditions where the convexity assumptions of influence functions are met. Then, we relax these assumptions and study the effects of non-convexity by using deeper models and more complex datasets. Here, we analyze the k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32593;&#32476;&#32467;&#21512;&#30340;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807; LoRA &#35757;&#32451;&#26041;&#27861;&#24494;&#35843;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#21644; ControlNet &#27169;&#22411;&#30340;&#28155;&#21152;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20026;&#21518;&#32493;&#24314;&#31569;&#22270;&#20687;&#29983;&#25104;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.12755</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;: &#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#24314;&#31569;&#31435;&#38754;&#35774;&#35745;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Text Semantics to Image Generation: A method of building facades design base on Stable Diffusion model. (arXiv:2303.12755v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32593;&#32476;&#32467;&#21512;&#30340;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807; LoRA &#35757;&#32451;&#26041;&#27861;&#24494;&#35843;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#21644; ControlNet &#27169;&#22411;&#30340;&#28155;&#21152;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20026;&#21518;&#32493;&#24314;&#31569;&#22270;&#20687;&#29983;&#25104;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#24314;&#31569;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#20013;&#65292;&#20294;&#30446;&#21069;&#20173;&#26377;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#20869;&#23481;&#21487;&#25511;&#24615;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32593;&#32476;&#32467;&#21512;&#30340;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807; LoRA&#65288;&#20302;&#31209;&#33258;&#36866;&#24212;&#65289;&#26041;&#27861;&#22312; CMP Fa-cades &#25968;&#25454;&#38598;&#19978;&#23545;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#28982;&#21518;&#24212;&#29992; ControlNet &#27169;&#22411;&#36827;&#19968;&#27493;&#25511;&#21046;&#36755;&#20986;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#24314;&#31569;&#39118;&#26684;&#25991;&#26412;&#20869;&#23481;&#21644;&#25511;&#21046;&#31574;&#30053;&#19979;&#30340;&#31435;&#38754;&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LoRA &#35757;&#32451;&#26041;&#27861;&#26174;&#30528;&#38477;&#20302;&#20102;&#24494;&#35843;&#31283;&#23450;&#25193;&#25955;&#22823;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#32780; ControlNet &#27169;&#22411;&#30340;&#28155;&#21152;&#22686;&#21152;&#20102;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#20026;&#21518;&#32493;&#20851;&#20110;&#24314;&#31569;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stable Diffusion model has been extensively employed in the study of archi-tectural image generation, but there is still an opportunity to enhance in terms of the controllability of the generated image content. A multi-network combined text-to-building facade image generating method is proposed in this work. We first fine-tuned the Stable Diffusion model on the CMP Fa-cades dataset using the LoRA (Low-Rank Adaptation) approach, then we ap-ply the ControlNet model to further control the output. Finally, we contrast-ed the facade generating outcomes under various architectural style text con-tents and control strategies. The results demonstrate that the LoRA training approach significantly decreases the possibility of fine-tuning the Stable Dif-fusion large model, and the addition of the ControlNet model increases the controllability of the creation of text to building facade images. This pro-vides a foundation for subsequent studies on the generation of architectural images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550; RegionLight&#65292;&#22522;&#20110;&#20132;&#21449;&#21475;&#20043;&#38388;&#30340;&#37051;&#25509;&#20851;&#31995;&#23558;&#26234;&#33021;&#20307;&#20998;&#37197;&#21040;&#27599;&#20010;&#21306;&#22495;&#20013;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#25193;&#23637;&#20102;BDQ&#26041;&#27861;&#20026;DBDQ&#65292;&#20197;&#38480;&#21046;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#38271;&#24182;&#32531;&#35299;&#26234;&#33021;&#20307;&#35757;&#32451;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.11899</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22823;&#35268;&#27169;&#26684;&#32593;&#20132;&#36890;&#32593;&#32476;&#21306;&#22495;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Reinforcement Learning for Regional Signal control in Large-scale Grid Traffic network. (arXiv:2303.11899v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550; RegionLight&#65292;&#22522;&#20110;&#20132;&#21449;&#21475;&#20043;&#38388;&#30340;&#37051;&#25509;&#20851;&#31995;&#23558;&#26234;&#33021;&#20307;&#20998;&#37197;&#21040;&#27599;&#20010;&#21306;&#22495;&#20013;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#25193;&#23637;&#20102;BDQ&#26041;&#27861;&#20026;DBDQ&#65292;&#20197;&#38480;&#21046;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#38271;&#24182;&#32531;&#35299;&#26234;&#33021;&#20307;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#33258;&#36866;&#24212;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26159;&#24403;&#21069;&#38750;&#24120;&#27969;&#34892;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#25511;&#21046;&#21333;&#20010;&#36335;&#21475;&#65292;&#36825;&#20123;&#26041;&#27861;&#20391;&#37325;&#20110;&#36335;&#21475;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;MARL&#30340;&#38750;&#31283;&#24577;&#24615;&#36136;&#38543;&#30528;&#20132;&#36890;&#32593;&#32476;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20173;&#28982;&#38480;&#21046;&#30528;&#19978;&#36848;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#19968;&#31181;&#22949;&#21327;&#30340;&#31574;&#30053;&#26159;&#23558;&#19968;&#21517;&#26234;&#33021;&#20307;&#20998;&#37197;&#21040;&#19968;&#32452;&#36335;&#21475;&#20013;&#65292;&#20197;&#20943;&#23569;&#26234;&#33021;&#20307;&#25968;&#37327;&#12290;&#36825;&#31181;&#31574;&#30053;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#65292;&#19968;&#20010;&#26159;&#22914;&#20309;&#23558;&#20132;&#36890;&#32593;&#32476;&#21010;&#20998;&#25104;&#23567;&#21306;&#22495;&#65292;&#21478;&#19968;&#20010;&#26159;&#22914;&#20309;&#25628;&#32034;&#21306;&#22495;&#20869;&#30340;&#26368;&#20248;&#32852;&#21512;&#21160;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;RegionLight&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#21306;&#22495;&#21010;&#20998;&#35268;&#21017;&#22522;&#20110;&#20132;&#21449;&#21475;&#20043;&#38388;&#30340;&#37051;&#25509;&#20851;&#31995;&#65292;&#24182;&#25193;&#23637;&#20102;Branching Dueling Q-Network(BDQ)&#12290;&#35813;&#26041;&#27861;&#23558;BDQ&#36827;&#19968;&#27493;&#20248;&#21270;&#20026;Dynamic Branching Dueling Q-Network(DBDQ)&#65292;&#20197;&#38480;&#21046;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#38271;&#24182;&#32531;&#35299;&#26234;&#33021;&#20307;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive traffic signal control with Multi-agent Reinforcement Learning(MARL) is a very popular topic nowadays. In most existing novel methods, one agent controls single intersections and these methods focus on the cooperation between intersections. However, the non-stationary property of MARL still limits the performance of the above methods as the size of traffic networks grows. One compromised strategy is to assign one agent with a region of intersections to reduce the number of agents. There are two challenges in this strategy, one is how to partition a traffic network into small regions and the other is how to search for the optimal joint actions for a region of intersections. In this paper, we propose a novel training framework RegionLight where our region partition rule is based on the adjacency between the intersection and extended Branching Dueling Q-Network(BDQ) to Dynamic Branching Dueling Q-Network(DBDQ) to bound the growth of the size of joint action space and alleviate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;LQR&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#36882;&#25512;-&#35270;&#35282;&#31574;&#30053;&#26799;&#24230;&#65288;RHPG&#65289;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#37319;&#26679;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#36890;&#36807;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;RHPG&#22312;&#32447;&#24615;&#25511;&#21046;&#21644;&#20272;&#35745;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.13144</link><description>&lt;p&gt;
&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;LQR&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Revisiting LQR Control from the Perspective of Receding-Horizon Policy Gradient. (arXiv:2302.13144v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;LQR&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#36882;&#25512;-&#35270;&#35282;&#31574;&#30053;&#26799;&#24230;&#65288;RHPG&#65289;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#37319;&#26679;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#36890;&#36807;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;RHPG&#22312;&#32447;&#24615;&#25511;&#21046;&#21644;&#20272;&#35745;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#12290;&#32467;&#21512;&#36882;&#25512;-&#35270;&#35282;&#31574;&#30053;&#26799;&#24230;&#65288;RHPG&#65289;&#27169;&#22411;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#37319;&#26679;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#22312;&#949;-&#33539;&#25968;&#24847;&#20041;&#19979;&#25509;&#36817;LQR&#26368;&#20248;&#35299;&#30340;&#20248;&#21270;&#25511;&#21046;&#31574;&#30053;&#12290;&#22312;&#26368;&#36817;&#23558;RHPG&#24212;&#29992;&#20110;&#23398;&#20064;&#21345;&#23572;&#26364;&#28388;&#27874;&#20013;&#36827;&#34892;&#25299;&#23637;&#20998;&#26512;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RHPG&#22312;&#32447;&#24615;&#25511;&#21046;&#21644;&#20272;&#35745;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit in this paper the discrete-time linear quadratic regulator (LQR) problem from the perspective of receding-horizon policy gradient (RHPG), a newly developed model-free learning framework for control applications. We provide a fine-grained sample complexity analysis for RHPG to learn a control policy that is both stabilizing and $\epsilon$-close to the optimal LQR solution, and our algorithm does not require knowing a stabilizing control policy for initialization. Combined with the recent application of RHPG in learning the Kalman filter, we demonstrate the general applicability of RHPG in linear control and estimation with streamlined analyses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36793;&#32536;Transformer&#21644;&#39044;&#35757;&#32451;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#28216;&#25103;&#20013;&#36827;&#34892;&#22909;&#21451;&#25490;&#21517;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.10043</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#36793;&#32536;Transformer&#22312;&#22312;&#32447;&#28216;&#25103;&#20013;&#36827;&#34892;&#22909;&#21451;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Friend Ranking in Online Games via Pre-training Edge Transformers. (arXiv:2302.10043v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36793;&#32536;Transformer&#21644;&#39044;&#35757;&#32451;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#28216;&#25103;&#20013;&#36827;&#34892;&#22909;&#21451;&#25490;&#21517;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#28216;&#25103;&#20013;&#65292;&#22909;&#21451;&#22238;&#24518;&#26159;&#25552;&#39640;&#27599;&#26085;&#27963;&#36291;&#29992;&#25143;&#25968;&#37327;&#30340;&#37325;&#35201;&#36884;&#24452;&#12290;&#26412;&#25991;&#23558;&#22909;&#21451;&#22238;&#24518;&#38382;&#39064;&#35270;&#20026;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#21487;&#20197;&#20351;&#29992;&#65288;&#27963;&#36291;&#30340;&#21644;&#22833;&#33853;&#30340;&#65289;&#29609;&#23478;&#29305;&#24449;&#20197;&#21450;&#21382;&#21490;&#20107;&#20214;&#30340;&#20960;&#31181;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36793;&#32536;Transformer&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#27454;&#33150;&#35759;&#28216;&#25103;&#30340;&#31163;&#32447;&#23454;&#39564;&#21644;&#22312;&#32447;A/B&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Friend recall is an important way to improve Daily Active Users (DAU) in online games. The problem is to generate a proper lost friend ranking list essentially. Traditional friend recall methods focus on rules like friend intimacy or training a classifier for predicting lost players' return probability, but ignore feature information of (active) players and historical friend recall events. In this work, we treat friend recall as a link prediction problem and explore several link prediction methods which can use features of both active and lost players, as well as historical events. Furthermore, we propose a novel Edge Transformer model and pre-train the model via masked auto-encoders. Our method achieves state-of-the-art results in the offline experiments and online A/B Tests of three Tencent games.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20123;&#20855;&#26377;&#22823;&#22495;&#21644;&#38271;&#23376;&#21477;&#30340;&#26497;&#38590;&#20363;&#23376;&#65292;&#35201;&#27714;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#25165;&#33021;&#35299;&#20915;&#65292;&#36825;&#24847;&#21619;&#30528;P $\neq$ NP&#12290;</title><link>http://arxiv.org/abs/2302.09512</link><description>&lt;p&gt;
SAT&#38656;&#35201;&#24443;&#24213;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
SAT Requires Exhaustive Search. (arXiv:2302.09512v4 [cs.CC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20123;&#20855;&#26377;&#22823;&#22495;&#21644;&#38271;&#23376;&#21477;&#30340;&#26497;&#38590;&#20363;&#23376;&#65292;&#35201;&#27714;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#25165;&#33021;&#35299;&#20915;&#65292;&#36825;&#24847;&#21619;&#30528;P $\neq$ NP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;&#20855;&#26377;&#22823;&#22495;&#21644;&#38271;&#23376;&#21477;&#30340;CSP&#21644;SAT&#30340;&#26497;&#38590;&#20363;&#23376;&#65292;&#35777;&#26126;&#36825;&#20123;&#20363;&#23376;&#26080;&#27861;&#22312;&#19981;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#65292;&#36825;&#24847;&#21619;&#30528;&#19968;&#20010;&#36739;&#24369;&#30340;&#32467;&#35770;P $\neq$ NP&#12290;&#26412;&#25991;&#37319;&#29992;&#30340;&#26159;&#19968;&#31181;&#35777;&#26126;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#30340;&#24314;&#35774;&#24615;&#26041;&#27861;&#65292;&#19982;&#30446;&#21069;&#35745;&#31639;&#22797;&#26434;&#24615;&#29702;&#35770;&#20013;&#20351;&#29992;&#30340;&#26041;&#27861;&#38750;&#24120;&#19981;&#21516;&#65292;&#20294;&#19982;Kurt G\"{o}del&#22312;&#35777;&#26126;&#20182;&#33879;&#21517;&#30340;&#36923;&#36753;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#26102;&#20351;&#29992;&#30340;&#26041;&#27861;&#30456;&#20284;&#12290;&#27491;&#22914;G\"{o}del&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25968;&#23398;&#20013;&#35777;&#26126;&#24418;&#24335;&#19978;&#30340;&#19981;&#21487;&#35777;&#26126;&#24615;&#26159;&#21487;&#34892;&#30340;&#19968;&#26679;&#65292;&#26412;&#25991;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25968;&#23398;&#20013;&#35777;&#26126;&#35745;&#31639;&#19978;&#30340;&#38590;&#24230;&#19981;&#26159;&#24456;&#38590;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#35768;&#22810;&#38382;&#39064;&#65292;&#22914;3-SAT&#65292;&#35777;&#26126;&#19979;&#30028;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#26377;&#21508;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#21487;&#29992;&#20110;&#36991;&#20813;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#26497;&#38590;&#30340;&#20363;&#23376;&#20013;&#65292;&#24443;&#24213;&#25628;&#32034;&#21487;&#33021;&#26159;&#21807;&#19968;&#21487;&#34892;&#30340;&#36873;&#25321;&#65292;&#35777;&#26126;&#20854;&#24517;&#35201;&#24615;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, by constructing extremely hard examples of CSP (with large domains) and SAT (with long clauses), we prove that such examples cannot be solved without exhaustive search, which implies a weaker conclusion P $\neq$ NP. This constructive approach for proving impossibility results is very different (and missing) from those currently used in computational complexity theory, but is similar to that used by Kurt G\"{o}del in proving his famous logical impossibility results. Just as shown by G\"{o}del's results that proving formal unprovability is feasible in mathematics, the results of this paper show that proving computational hardness is not hard in mathematics. Specifically, proving lower bounds for many problems, such as 3-SAT, can be challenging because these problems have various effective strategies available for avoiding exhaustive search. However, in cases of extremely hard examples, exhaustive search may be the only viable option, and proving its necessity becomes more 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#21644;&#31574;&#30053;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#28151;&#21512;&#25216;&#26415;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.09051</link><description>&lt;p&gt;
&#22797;&#26434;&#38382;&#31572;&#21644;&#35821;&#35328;&#27169;&#22411;&#28151;&#21512;&#26550;&#26500;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Complex QA and language models hybrid architectures, Survey. (arXiv:2302.09051v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#21644;&#31574;&#30053;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#28151;&#21512;&#25216;&#26415;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#21644;&#31574;&#30053;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#28151;&#21512;&#25216;&#26415;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#26631;&#20934;&#38382;&#39064;&#19978;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#65292;&#20294;&#22312;&#35299;&#20915;&#26356;&#20855;&#20307;&#30340;&#22797;&#26434;&#38382;&#39064;&#26102;&#65288;&#22914;&#22312;&#19981;&#21516;&#25991;&#21270;&#20013;&#20010;&#20154;&#33258;&#30001;&#27010;&#24565;&#30340;&#21464;&#21270;&#22914;&#20309;&#65311;&#20160;&#20040;&#26159;&#20026;&#20943;&#23569;&#27668;&#20505;&#21464;&#21270;&#32780;&#23454;&#29616;&#30340;&#26368;&#20339;&#21457;&#30005;&#26041;&#27861;&#32452;&#21512;&#65311;&#65289;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#26550;&#26500;&#12289;&#30693;&#35782;&#12289;&#25216;&#33021;&#12289;&#26041;&#27861;&#12289;&#25935;&#24863;&#25968;&#25454;&#20445;&#25252;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#31867;&#23457;&#25209;&#21644;&#22810;&#21151;&#33021;&#21453;&#39304;&#12290;&#26368;&#36817;&#30340;&#39033;&#30446;&#22914;ChatGPT&#21644;GALACTICA&#20801;&#35768;&#38750;&#19987;&#19994;&#20154;&#21592;&#20102;&#35299;LLM&#22312;&#22797;&#26434;QA&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#20197;&#21450;&#21516;&#31561;&#24378;&#22823;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23457;&#26597;&#25152;&#38656;&#30340;&#25216;&#33021;&#21644;&#35780;&#20272;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#28151;&#21512;&#26550;&#26500;&#65292;&#23558;LLM&#19982;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;&#20854;&#20182;AI/ML&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#36825;&#20123;CQA&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#33021;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reviews the state-of-the-art of language models architectures and strategies for "complex" question-answering (QA, CQA, CPS) with a focus on hybridization. Large Language Models (LLM) are good at leveraging public data on standard problems but once you want to tackle more specific complex questions or problems (e.g. How does the concept of personal freedom vary between different cultures ? What is the best mix of power generation methods to reduce climate change ?) you may need specific architecture, knowledge, skills, methods, sensitive data protection, explainability, human approval and versatile feedback... Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA. In this paper, we start by reviewing required skills and evaluation techniques. We integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;10&#31181;&#24403;&#20195;&#35266;&#27979;&#24615;&#26102;&#38388;&#22240;&#26524;&#21457;&#29616;&#25216;&#26415;&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#25913;&#36827;&#30340;&#22320;&#26041;&#65292;&#20197;&#20419;&#36827;&#36825;&#20123;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.00064</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#36947;&#36335;&#39550;&#39542;&#34892;&#20026;&#30340;&#26102;&#38388;&#35266;&#27979;&#22522;&#22240;&#22240;&#26524;&#21457;&#29616;&#25216;&#26415;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Temporal Observation-Based Causal Discovery Techniques Applied to Road Driver Behaviour. (arXiv:2302.00064v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;10&#31181;&#24403;&#20195;&#35266;&#27979;&#24615;&#26102;&#38388;&#22240;&#26524;&#21457;&#29616;&#25216;&#26415;&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#25913;&#36827;&#30340;&#22320;&#26041;&#65292;&#20197;&#20419;&#36827;&#36825;&#20123;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#38656;&#35201;&#25512;&#29702;&#20854;&#29615;&#22659;&#20013;&#21160;&#24577;&#23454;&#20307;&#30340;&#34892;&#20026;&#12290;&#25551;&#36848;&#36825;&#20123;&#20851;&#31995;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#36890;&#36807;&#24212;&#29992;&#22522;&#22240;&#22240;&#26524;&#21457;&#29616;&#25216;&#26415;&#26469;&#23436;&#25104;&#30340;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#35266;&#27979;&#24615;&#22240;&#26524;&#21457;&#29616;&#25216;&#26415;&#24448;&#24448;&#38590;&#20197;&#24212;&#23545;&#33258;&#20027;&#20307;&#39046;&#22495;&#20013;&#22312;&#32447;&#20351;&#29992;&#26399;&#38388;&#36890;&#24120;&#20986;&#29616;&#30340;&#22240;&#26524;&#31232;&#30095;&#21644;&#38750;&#31283;&#24577;&#31561;&#26465;&#20214;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#39046;&#22495;&#38480;&#21046;&#65292;&#24178;&#39044;&#25216;&#26415;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25506;&#32034;&#35266;&#23519;&#25216;&#26415;&#38754;&#20020;&#30340;&#38382;&#39064;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#35752;&#35770;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#23545;10&#31181;&#24403;&#20195;&#35266;&#27979;&#24615;&#26102;&#38388;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#23545;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20197;&#21450;&#29983;&#25104;&#30340;&#22330;&#26223;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#38656;&#35201;&#25913;&#36827;&#30340;&#22320;&#26041;&#65292;&#20197;&#20419;&#36827;&#36825;&#20123;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous robots are required to reason about the behaviour of dynamic agents in their environment. The creation of models to describe these relationships is typically accomplished through the application of causal discovery techniques. However, as it stands observational causal discovery techniques struggle to adequately cope with conditions such as causal sparsity and non-stationarity typically seen during online usage in autonomous agent domains. Meanwhile, interventional techniques are not always feasible due to domain restrictions. In order to better explore the issues facing observational techniques and promote further discussion of these topics we carry out a benchmark across 10 contemporary observational temporal causal discovery methods in the domain of autonomous driving. By evaluating these methods upon causal scenes drawn from real world datasets in addition to those generated synthetically we highlight where improvements need to be made in order to facilitate the applicat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#24577;&#20559;&#35265;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21464;&#20998;&#21387;&#32553;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#27169;&#24577;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.09479</link><description>&lt;p&gt;
&#26080;&#27169;&#24577;&#20559;&#35265;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21464;&#20998;&#21387;&#32553;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modality-Agnostic Variational Compression of Implicit Neural Representations. (arXiv:2301.09479v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09479
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#24577;&#20559;&#35265;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21464;&#20998;&#21387;&#32553;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#27169;&#24577;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#20989;&#25968;&#35270;&#22270;&#65292;&#24182;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#21442;&#25968;&#21270;&#30340;&#26080;&#27169;&#24577;&#31070;&#32463;&#21387;&#32553;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#36719;&#38376;&#25511;&#26426;&#21046;&#23558;&#38750;&#32447;&#24615;&#26144;&#23556;&#21040;&#32039;&#20945;&#30340;&#28508;&#22312;&#34920;&#31034;&#20013;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#28508;&#22312;&#32534;&#30721;&#21644;&#31232;&#30095;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36825;&#20801;&#35768;&#27599;&#20010;&#25968;&#25454;&#39033;&#36890;&#36807;&#23376;&#32593;&#32476;&#36873;&#25321;&#26469;&#23450;&#21046;&#20849;&#20139;&#30340;INR&#32593;&#32476;&#30340;&#19987;&#19994;&#21270;&#12290;&#22312;&#33719;&#21462;&#36825;&#31181;&#28508;&#22312;&#34920;&#31034;&#30340;&#25968;&#25454;&#38598;&#21518;&#65292;&#25105;&#20204;&#22312;&#26080;&#27169;&#24577;&#31354;&#38388;&#20013;&#30452;&#25509;&#20248;&#21270;&#36895;&#29575;/&#22833;&#30495;&#30340;&#25240;&#34935;&#26041;&#26696;&#65292;&#20351;&#29992;&#31070;&#32463;&#21387;&#32553;&#12290;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#21464;&#20998;&#21387;&#32553;&#65288;VC-INR&#65289;&#22312;&#20855;&#26377;&#30456;&#21516;&#34920;&#31034;&#23481;&#37327;&#30340;&#37327;&#21270;&#20043;&#21069;&#26174;&#31034;&#20986;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20248;&#20110;&#20854;&#20182;INR&#25216;&#26415;&#25152;&#20351;&#29992;&#30340;&#20808;&#21069;&#37327;&#21270;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#31639;&#27861;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#23450;&#20110;&#27169;&#24577;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#19978;&#21462;&#24471;&#21331;&#36234;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22270;&#20687;&#12289;&#27668;&#20505;&#25968;&#25454;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#25968;&#25454;&#19978;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a modality-agnostic neural compression algorithm based on a functional view of data and parameterised as an Implicit Neural Representation (INR). Bridging the gap between latent coding and sparsity, we obtain compact latent representations non-linearly mapped to a soft gating mechanism. This allows the specialisation of a shared INR network to each data item through subnetwork selection. After obtaining a dataset of such latent representations, we directly optimise the rate/distortion trade-off in a modality-agnostic space using neural compression. Variational Compression of Implicit Neural Representations (VC-INR) shows improved performance given the same representational capacity pre quantisation while also outperforming previous quantisation schemes used for other INR techniques. Our experiments demonstrate strong results over a large set of diverse modalities using the same algorithm without any modality-specific inductive biases. We show results on images, climate dat
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#31070;&#32463;&#24418;&#29366;&#32534;&#35793;&#22120;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#19977;&#32500;&#24418;&#29366;&#30340;&#19981;&#21516;&#25277;&#35937;&#31867;&#22411;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25552;&#20986;&#30340;&#24418;&#29366;&#20195;&#30721;&#36716;&#25442;&#22120;&#23558;&#19977;&#32500;&#24418;&#29366;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#31163;&#25955;&#24418;&#29366;&#20195;&#30721;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#20102;&#36739;&#24378;&#30340;&#36716;&#25442;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.12952</link><description>&lt;p&gt;
&#31070;&#32463;&#24418;&#29366;&#32534;&#35793;&#22120;&#65306;&#22312;&#25991;&#26412;&#12289;&#28857;&#20113;&#21644;&#31243;&#24207;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program. (arXiv:2212.12952v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#31070;&#32463;&#24418;&#29366;&#32534;&#35793;&#22120;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#19977;&#32500;&#24418;&#29366;&#30340;&#19981;&#21516;&#25277;&#35937;&#31867;&#22411;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25552;&#20986;&#30340;&#24418;&#29366;&#20195;&#30721;&#36716;&#25442;&#22120;&#23558;&#19977;&#32500;&#24418;&#29366;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#31163;&#25955;&#24418;&#29366;&#20195;&#30721;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#20102;&#36739;&#24378;&#30340;&#36716;&#25442;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#24418;&#29366;&#26377;&#30528;&#20114;&#34917;&#30340;&#25277;&#35937;&#32423;&#21035;&#65292;&#20174;&#20302;&#32423;&#30340;&#20960;&#20309;&#24418;&#29366;&#21040;&#22522;&#20110;&#37096;&#20214;&#30340;&#23618;&#27425;&#32467;&#26500;&#20877;&#21040;&#35821;&#35328;&#65292;&#20256;&#36882;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24418;&#29366;&#25277;&#35937;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65306;$\textit{&#25991;&#26412;}$ $\Longleftrightarrow$ $\textit{&#28857;&#20113;}$ $\Longleftrightarrow$ $\textit{&#31243;&#24207;}$&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;$\textbf{&#31070;&#32463;&#24418;&#29366;&#32534;&#35793;&#22120;}$&#26469;&#23558;&#25277;&#35937;&#36716;&#25442;&#24314;&#27169;&#20026;&#19968;&#31181;&#26465;&#20214;&#29983;&#25104;&#36807;&#31243;&#12290;&#23427;&#23558;&#19977;&#31181;&#25277;&#35937;&#31867;&#22411;&#30340;&#19977;&#32500;&#24418;&#29366;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#31163;&#25955;&#24418;&#29366;&#20195;&#30721;&#65292;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;$\textit{&#24418;&#29366;&#20195;&#30721;&#36716;&#25442;&#22120;}$&#23558;&#27599;&#20010;&#24418;&#29366;&#20195;&#30721;&#36716;&#25442;&#20026;&#20854;&#20182;&#25277;&#35937;&#31867;&#22411;&#30340;&#20195;&#30721;&#65292;&#24182;&#23558;&#20854;&#35299;&#30721;&#20197;&#36755;&#20986;&#30446;&#26631;&#24418;&#29366;&#25277;&#35937;&#12290;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;$\textit{Point}$VQVAE&#65292;&#20197;&#31867;&#19981;&#30830;&#23450;&#26041;&#24335;&#33719;&#24471;&#28857;&#20113;&#20195;&#30721;&#12290;&#22312;Text2Shape&#12289;ShapeGlot&#12289;ABO&#12289;Genre&#21644;Program Synthetic&#25968;&#25454;&#38598;&#19978;&#65292;&#31070;&#32463;&#24418;&#29366;&#32534;&#35793;&#22120;&#22312;$\textit{&#25991;&#26412;}$ $\Longrightarrow$ $\textit{&#28857;&#20113;}$&#12289;$\textit{&#28857;&#20113;}$ $\Longrightarrow$ $\textit{&#31243;&#24207;}$&#21644;$\textit{&#31243;&#24207;}$ $\Longrightarrow$ $\textit{&#28857;&#20113;}$&#36716;&#25442;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D shapes have complementary abstractions from low-level geometry to part-based hierarchies to languages, which convey different levels of information. This paper presents a unified framework to translate between pairs of shape abstractions: $\textit{Text}$ $\Longleftrightarrow$ $\textit{Point Cloud}$ $\Longleftrightarrow$ $\textit{Program}$. We propose $\textbf{Neural Shape Compiler}$ to model the abstraction transformation as a conditional generation process. It converts 3D shapes of three abstract types into unified discrete shape code, transforms each shape code into code of other abstract types through the proposed $\textit{ShapeCode Transformer}$, and decodes them to output the target shape abstraction. Point Cloud code is obtained in a class-agnostic way by the proposed $\textit{Point}$VQVAE. On Text2Shape, ShapeGlot, ABO, Genre, and Program Synthetic datasets, Neural Shape Compiler shows strengths in $\textit{Text}$ $\Longrightarrow$ $\textit{Point Cloud}$, $\textit{Point Cloud
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21644;&#21487;&#35299;&#37322;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#20998;&#31867;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#24573;&#30053;&#36855;&#22240;&#35821;&#20041;&#21644;&#21019;&#24314;&#19978;&#19979;&#25991;&#23548;&#33268;&#20844;&#27491;&#20869;&#23481;&#31649;&#29702;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#37319;&#29992;&#31034;&#20363;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#25512;&#29702;&#24182;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;SOTA&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#25104;&#21151;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#26816;&#27979;&#20102;&#26377;&#23475;&#30340;&#36855;&#22240;&#12290;</title><link>http://arxiv.org/abs/2212.05612</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21644;&#21487;&#35299;&#37322;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multimodal and Explainable Internet Meme Classification. (arXiv:2212.05612v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21644;&#21487;&#35299;&#37322;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#20998;&#31867;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#24573;&#30053;&#36855;&#22240;&#35821;&#20041;&#21644;&#21019;&#24314;&#19978;&#19979;&#25991;&#23548;&#33268;&#20844;&#27491;&#20869;&#23481;&#31649;&#29702;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#37319;&#29992;&#31034;&#20363;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#25512;&#29702;&#24182;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;SOTA&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#25104;&#21151;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#26816;&#27979;&#20102;&#26377;&#23475;&#30340;&#36855;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#29615;&#22659;&#20013;&#65292;&#32593;&#32476;&#24179;&#21488;&#24050;&#32463;&#34987;&#26377;&#25928;&#22320;&#27494;&#22120;&#21270;&#65292;&#34987;&#29992;&#20110;&#21508;&#31181;&#22320;&#32536;&#25919;&#27835;&#20107;&#20214;&#21644;&#31038;&#20250;&#38382;&#39064;&#20013;&#65292;&#20114;&#32852;&#32593;&#36855;&#22240;&#20351;&#24471;&#22823;&#35268;&#27169;&#30340;&#20844;&#27491;&#20869;&#23481;&#31649;&#29702;&#26356;&#21152;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#36855;&#22240;&#20998;&#31867;&#21644;&#36319;&#36394;&#24037;&#20316;&#20027;&#35201;&#37319;&#29992;&#40657;&#30418;&#26041;&#27861;&#65292;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#36855;&#22240;&#30340;&#35821;&#20041;&#25110;&#20854;&#21019;&#24314;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36861;&#27714;&#19968;&#31181;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#29702;&#35299;&#26550;&#26500;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#20998;&#31867;&#26041;&#27861;&#65292;&#23545;&#35757;&#32451;&#26696;&#20363;&#36827;&#34892;&#31034;&#20363;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#25512;&#29702;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;SOTA&#27169;&#22411;&#26469;&#34920;&#31034;&#21508;&#20010;&#26696;&#20363;&#12290; &#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#27169;&#22411;&#22312;&#26816;&#27979;&#20004;&#20010;&#29616;&#26377;&#20219;&#21153;&#20013;&#26377;&#23475;&#36855;&#22240;&#30340;&#30456;&#20851;&#24615;&#65306;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#21388;&#22899;&#30151;&#20998;&#31867;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#31034;&#20363;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#20197;&#21450;&#25991;&#26412;&#65292;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current context where online platforms have been effectively weaponized in a variety of geo-political events and social issues, Internet memes make fair content moderation at scale even more difficult. Existing work on meme classification and tracking has focused on black-box methods that do not explicitly consider the semantics of the memes or the context of their creation. In this paper, we pursue a modular and explainable architecture for Internet meme understanding. We design and implement multimodal classification methods that perform example- and prototype-based reasoning over training cases, while leveraging both textual and visual SOTA models to represent the individual cases. We study the relevance of our modular and explainable models in detecting harmful memes on two existing tasks: Hate Speech Detection and Misogyny Classification. We compare the performance between example- and prototype-based methods, and between text, vision, and multimodal models, across differen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36830;&#32493;&#21160;&#20316;IMDPs (caIMDPs)&#65292;&#30740;&#31350;&#20102;&#35813;&#36807;&#31243;&#19978;&#26368;&#22823;&#21270;&#26399;&#26395;&#32047;&#31215;&#22870;&#21169;&#30340;&#20540;&#36845;&#20195;&#31639;&#27861;&#65292;&#20854;&#20013;&#36716;&#31227;&#27010;&#29575;&#30340;&#19978;&#19979;&#30028;&#26159;&#21160;&#20316;&#21464;&#37327;&#30340;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2211.01231</link><description>&lt;p&gt;
&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#21306;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(IMDPs)
&lt;/p&gt;
&lt;p&gt;
Interval Markov Decision Processes with Continuous Action-Spaces. (arXiv:2211.01231v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36830;&#32493;&#21160;&#20316;IMDPs (caIMDPs)&#65292;&#30740;&#31350;&#20102;&#35813;&#36807;&#31243;&#19978;&#26368;&#22823;&#21270;&#26399;&#26395;&#32047;&#31215;&#22870;&#21169;&#30340;&#20540;&#36845;&#20195;&#31639;&#27861;&#65292;&#20854;&#20013;&#36716;&#31227;&#27010;&#29575;&#30340;&#19978;&#19979;&#30028;&#26159;&#21160;&#20316;&#21464;&#37327;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(IMDPs)&#26159;&#26377;&#38480;&#29366;&#24577;&#19981;&#30830;&#23450;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#20854;&#36716;&#31227;&#27010;&#29575;&#23646;&#20110;&#21306;&#38388;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#30740;&#31350;&#20102;&#23558;IMDPs&#20316;&#20026;&#38543;&#26426;&#31995;&#32479;&#30340;&#25277;&#35937;&#20197;&#36827;&#34892;&#25511;&#21046;&#21512;&#25104;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#29992;&#20110;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;IMDPs&#21512;&#25104;&#31639;&#27861;&#65292;&#22240;&#27492;&#20808;&#21069;&#19968;&#30452;&#20551;&#35774;&#21160;&#20316;&#31354;&#38388;&#26159;&#31163;&#25955;&#30340;&#65292;&#36825;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#32780;&#35328;&#26159;&#19968;&#31181;&#38480;&#21046;&#24615;&#20551;&#35774;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36830;&#32493;&#21160;&#20316;IMDPs (caIMDPs)&#65292;&#20854;&#20013;&#36716;&#31227;&#27010;&#29575;&#30340;&#19978;&#19979;&#30028;&#26159;&#21160;&#20316;&#21464;&#37327;&#30340;&#20989;&#25968;&#65292;&#24182;&#30740;&#31350;&#20102;&#26368;&#22823;&#21270;&#26399;&#26395;&#32047;&#31215;&#22870;&#21169;&#30340;&#20540;&#36845;&#20195;&#31639;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#19982;&#20540;&#36845;&#20195;&#30456;&#20851;&#30340;max-min&#38382;&#39064;&#20998;&#35299;&#25104;$|\mathcal{Q}|$ &#20010;max&#38382;&#39064;&#65292;&#20854;&#20013;$|\mathcal{Q}|$&#26159;caIMDP&#30340;&#29366;&#24577;&#25968;&#37327;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;max&#38382;&#39064;&#30340;&#31616;&#21333;&#24418;&#24335;&#65292;&#30830;&#23450;&#20102;caIMDPs&#20540;&#36845;&#20195;&#33021;&#22815;&#26377;&#25928;&#27714;&#35299;&#30340;&#24773;&#20917;&#65288;&#20363;&#22914;...&#65289;
&lt;/p&gt;
&lt;p&gt;
Interval Markov Decision Processes (IMDPs) are finite-state uncertain Markov models, where the transition probabilities belong to intervals. Recently, there has been a surge of research on employing IMDPs as abstractions of stochastic systems for control synthesis. However, due to the absence of algorithms for synthesis over IMDPs with continuous action-spaces, the action-space is assumed discrete a-priori, which is a restrictive assumption for many applications. Motivated by this, we introduce continuous-action IMDPs (caIMDPs), where the bounds on transition probabilities are functions of the action variables, and study value iteration for maximizing expected cumulative rewards. Specifically, we decompose the max-min problem associated to value iteration to $|\mathcal{Q}|$ max problems, where $|\mathcal{Q}|$ is the number of states of the caIMDP. Then, exploiting the simple form of these max problems, we identify cases where value iteration over caIMDPs can be solved efficiently (e.g.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;SPIDR&#65292;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#31070;&#32463;&#28857;&#22330;&#32593;&#32476;&#65292;&#24182;&#32467;&#21512;&#28857;&#20113;&#21644;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#36136;&#37327;&#30340;&#23545;&#35937;&#34920;&#38754;&#37325;&#24314;&#21644;&#20809;&#29031;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2210.08398</link><description>&lt;p&gt;
SPIDR&#65306;&#22522;&#20110;SDF&#30340;&#31070;&#32463;&#28857;&#22330;&#32593;&#32476;&#23454;&#29616;&#20809;&#29031;&#21644;&#21464;&#24418;
&lt;/p&gt;
&lt;p&gt;
SPIDR: SDF-based Neural Point Fields for Illumination and Deformation. (arXiv:2210.08398v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;SPIDR&#65292;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#31070;&#32463;&#28857;&#22330;&#32593;&#32476;&#65292;&#24182;&#32467;&#21512;&#28857;&#20113;&#21644;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#36136;&#37327;&#30340;&#23545;&#35937;&#34920;&#38754;&#37325;&#24314;&#21644;&#20809;&#29031;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#24050;&#25104;&#20026;3D&#37325;&#24314;&#21644;&#26032;&#35270;&#35282;&#21512;&#25104;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;NeRF&#30340;&#26041;&#27861;&#38544;&#24335;&#22320;&#32534;&#30721;&#20102;&#24418;&#29366;&#12289;&#21453;&#23556;&#21644;&#20809;&#29031;&#65292;&#36825;&#20351;&#24471;&#29992;&#25143;&#38590;&#20197;&#26174;&#24335;&#22320;&#25805;&#20316;&#36825;&#20123;&#23646;&#24615;&#22312;&#28210;&#26579;&#22270;&#20687;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#33021;&#22312;&#22330;&#26223;&#32534;&#36753;&#21644;&#20960;&#20309;&#21464;&#24418;&#26041;&#38754;&#36827;&#34892;&#26377;&#38480;&#30340;&#32534;&#36753;&#12290;&#27492;&#22806;&#65292;&#27809;&#26377;&#29616;&#26377;&#30340;&#24037;&#20316;&#22312;&#23545;&#35937;&#21464;&#24418;&#21518;&#33021;&#22815;&#23454;&#29616;&#20934;&#30830;&#30340;&#22330;&#26223;&#29031;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SPIDR&#65292;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#31070;&#32463;&#22330;&#34920;&#31034;&#12290;SPIDR&#32467;&#21512;&#20102;&#28857;&#20113;&#21644;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#36136;&#37327;&#30340;&#23545;&#35937;&#34920;&#38754;&#37325;&#24314;&#21644;&#20809;&#29031;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural radiance fields (NeRFs) have recently emerged as a promising approach for 3D reconstruction and novel view synthesis. However, NeRF-based methods encode shape, reflectance, and illumination implicitly and this makes it challenging for users to manipulate these properties in the rendered images explicitly. Existing approaches only enable limited editing of the scene and deformation of the geometry. Furthermore, no existing work enables accurate scene illumination after object deformation. In this work, we introduce SPIDR, a new hybrid neural SDF representation. SPIDR combines point cloud and neural implicit representations to enable the reconstruction of higher quality object surfaces for geometry deformation and lighting estimation. meshes and surfaces for object deformation and lighting estimation. To more accurately capture environment illumination for scene relighting, we propose a novel neural implicit model to learn environment light. To enable more accurate illumination up
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#33521;&#35821;&#23398;&#20064;&#32773;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#21477;&#23376;&#22635;&#31354;&#38382;&#39064;&#25968;&#25454;&#38598;\textsc{SC-Ques}&#65292;&#24182;&#19988;&#24050;&#32463;&#26500;&#24314;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#26469;&#33258;&#21160;&#35299;&#20915; SC &#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.12036</link><description>&lt;p&gt;
SC-Ques&#65306;&#20026;&#33521;&#35821;&#23398;&#20064;&#32773;&#35774;&#35745;&#30340;&#21477;&#23376;&#22635;&#31354;&#38382;&#39064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SC-Ques: A Sentence Completion Question Dataset for English as a Second Language Learners. (arXiv:2206.12036v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#33521;&#35821;&#23398;&#20064;&#32773;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#21477;&#23376;&#22635;&#31354;&#38382;&#39064;&#25968;&#25454;&#38598;\textsc{SC-Ques}&#65292;&#24182;&#19988;&#24050;&#32463;&#26500;&#24314;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#26469;&#33258;&#21160;&#35299;&#20915; SC &#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#22635;&#31354;&#65288;SC&#65289;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#24102;&#26377;&#19968;&#20010;&#25110;&#22810;&#20010;&#31354;&#30333;&#38656;&#35201;&#22635;&#20889;&#30340;&#21477;&#23376;&#65292;&#24182;&#25552;&#20379;&#19977;&#33267;&#20116;&#20010;&#21487;&#33021;&#30340;&#21333;&#35789;&#25110;&#30701;&#35821;&#20316;&#20026;&#36873;&#39033;&#12290; SC&#38382;&#39064;&#24191;&#27867;&#29992;&#20110;&#23398;&#20064;&#33521;&#35821;&#20316;&#20026;&#31532;&#20108;&#35821;&#35328;&#65288;ESL&#65289;&#30340;&#23398;&#29983;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;SC&#25968;&#25454;&#38598;\textsc{SC-Ques}&#65292;&#30001;&#26469;&#33258;&#30495;&#23454;&#26631;&#20934;&#33521;&#35821;&#32771;&#35797;&#30340;289,148&#20010;ESL SC&#38382;&#39064;&#32452;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#25552;&#20986;&#30340;\textsc{SC-Ques}&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#35299;&#20915;SC&#38382;&#39064;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;&#25105;&#20204;&#23545;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#12289;&#38480;&#21046;&#21644;&#26435;&#34913;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#29992;&#20110;&#30740;&#31350;&#30446;&#30340;&#65306;\url{https://github.com/ai4ed/SC-Ques}&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence completion (SC) questions present a sentence with one or more blanks that need to be filled in, three to five possible words or phrases as options. SC questions are widely used for students learning English as a Second Language (ESL). In this paper, we present a large-scale SC dataset, \textsc{SC-Ques}, which is made up of 289,148 ESL SC questions from real-world standardized English examinations. Furthermore, we build a comprehensive benchmark of automatically solving the SC questions by training the large-scale pre-trained language models on the proposed \textsc{SC-Ques} dataset. We conduct detailed analysis of the baseline models performance, limitations and trade-offs. The data and our code are available for research purposes from: \url{https://github.com/ai4ed/SC-Ques}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#20998;&#25903;&#35821;&#20041;&#20998;&#21106;&#31070;&#32463;&#32593;&#32476; PIDNet&#65292;&#37319;&#29992;&#36793;&#30028;&#27880;&#24847;&#21147;&#24341;&#23548;&#35814;&#32454;&#21644;&#19978;&#19979;&#25991;&#20998;&#25903;&#30340;&#34701;&#21512;&#65292;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#20004;&#20998;&#25903;&#27169;&#22411;&#20013;&#22240;&#35814;&#32454;&#29305;&#24449;&#34987;&#21608;&#22260;&#32972;&#26223;&#20449;&#24687;&#28153;&#27809;&#23548;&#33268;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.02066</link><description>&lt;p&gt;
PIDNet: &#19968;&#31181;&#21463; PID &#25511;&#21046;&#22120;&#21551;&#21457;&#30340;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers. (arXiv:2206.02066v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#20998;&#25903;&#35821;&#20041;&#20998;&#21106;&#31070;&#32463;&#32593;&#32476; PIDNet&#65292;&#37319;&#29992;&#36793;&#30028;&#27880;&#24847;&#21147;&#24341;&#23548;&#35814;&#32454;&#21644;&#19978;&#19979;&#25991;&#20998;&#25903;&#30340;&#34701;&#21512;&#65292;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#20004;&#20998;&#25903;&#27169;&#22411;&#20013;&#22240;&#35814;&#32454;&#29305;&#24449;&#34987;&#21608;&#22260;&#32972;&#26223;&#20449;&#24687;&#28153;&#27809;&#23548;&#33268;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#20010;&#20998;&#25903;&#30340;&#32593;&#32476;&#32467;&#26500;&#24050;&#32463;&#22312;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#39640;&#20998;&#36776;&#29575;&#32454;&#33410;&#21644;&#20302;&#39057;&#29575;&#32972;&#26223;&#20449;&#24687;&#30340;&#30452;&#25509;&#34701;&#21512;&#23384;&#22312;&#32454;&#33410;&#29305;&#24449;&#23481;&#26131;&#34987;&#21608;&#22260;&#32972;&#26223;&#20449;&#24687;&#28153;&#27809;&#30340;&#32570;&#28857;&#65292;&#36825;&#31181;&#36229;&#35843;&#29616;&#35937;&#38480;&#21046;&#20102;&#29616;&#26377;&#20004;&#20010;&#20998;&#25903;&#27169;&#22411;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#25913;&#21892;&#12290;&#26412;&#25991;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#21644; &#27604;&#20363;&#31215;&#20998;&#24494;&#20998; (PID) &#25511;&#21046;&#22120;&#24314;&#31435;&#36215;&#32852;&#31995;&#65292;&#24182;&#25581;&#31034;&#20986;&#20004;&#20010;&#20998;&#25903;&#32593;&#32476;&#30456;&#24403;&#20110;&#19968;&#20010;&#27604;&#20363;&#31215;&#20998; (PI) &#25511;&#21046;&#22120;&#65292;&#20854;&#26412;&#36136;&#19978;&#20063;&#23384;&#22312;&#31867;&#20284;&#30340;&#36229;&#35843;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#20998;&#25903;&#32593;&#32476;&#32467;&#26500;&#65306;PIDNet&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#20010;&#20998;&#25903;&#65292;&#20998;&#21035;&#35299;&#26512;&#35814;&#32454;&#12289;&#19978;&#19979;&#25991;&#21644;&#36793;&#30028;&#20449;&#24687;&#65292;&#37319;&#29992;&#36793;&#30028;&#27880;&#24847;&#21147;&#24341;&#23548;&#35814;&#32454;&#21644;&#19978;&#19979;&#25991;&#20998;&#25903;&#30340;&#34701;&#21512;&#12290;&#25105;&#20204;&#30340; PIDNet &#31995;&#21015;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#34920;&#29616;&#65292;&#21253;&#25324; Cityscapes&#12289;CamVid &#21644; COCO-Stuff&#12290;
&lt;/p&gt;
&lt;p&gt;
Two-branch network architecture has shown its efficiency and effectiveness in real-time semantic segmentation tasks. However, direct fusion of high-resolution details and low-frequency context has the drawback of detailed features being easily overwhelmed by surrounding contextual information. This overshoot phenomenon limits the improvement of the segmentation accuracy of existing two-branch models. In this paper, we make a connection between Convolutional Neural Networks (CNN) and Proportional-Integral-Derivative (PID) controllers and reveal that a two-branch network is equivalent to a Proportional-Integral (PI) controller, which inherently suffers from similar overshoot issues. To alleviate this problem, we propose a novel three-branch network architecture: PIDNet, which contains three branches to parse detailed, context and boundary information, respectively, and employs boundary attention to guide the fusion of detailed and context branches. Our family of PIDNets achieve the best 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#24182;&#26681;&#25454;&#35745;&#31639;&#38656;&#27714;&#29983;&#25104;&#25968;&#25454;&#30456;&#20851;&#30340;&#21069;&#21521;&#36335;&#24452;&#65292;&#22312;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#20860;&#39038;&#20934;&#30830;&#24230;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.00102</link><description>&lt;p&gt;
&#21160;&#24577;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Dynamic Multimodal Fusion. (arXiv:2204.00102v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.00102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#24182;&#26681;&#25454;&#35745;&#31639;&#38656;&#27714;&#29983;&#25104;&#25968;&#25454;&#30456;&#20851;&#30340;&#21069;&#21521;&#36335;&#24452;&#65292;&#22312;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#20860;&#39038;&#20934;&#30830;&#24230;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22810;&#27169;&#24577;&#23398;&#20064;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#38271;&#36275;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#34701;&#21512;&#26041;&#27861;&#22312;&#26412;&#36136;&#19978;&#26159;&#38745;&#24577;&#30340;&#65292;&#21363;&#23427;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#35745;&#31639;&#22788;&#29702;&#21644;&#34701;&#21512;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#32780;&#19981;&#32771;&#34385;&#19981;&#21516;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#19981;&#21516;&#35745;&#31639;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#22810;&#27169;&#24577;&#34701;&#21512;&#65288;DynMM&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#24182;&#22312;&#25512;&#29702;&#26399;&#38388;&#29983;&#25104;&#25968;&#25454;&#30456;&#20851;&#30340;&#21069;&#21521;&#36335;&#24452;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38376;&#25511;&#20989;&#25968;&#65292;&#26681;&#25454;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#20379;&#27169;&#24577;&#32423;&#25110;&#34701;&#21512;&#32423;&#20915;&#31574;&#65292;&#20197;&#21450;&#19968;&#20010;&#36164;&#28304;&#24863;&#30693;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#40723;&#21169;&#35745;&#31639;&#25928;&#29575;&#12290;&#23545;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#20363;&#22914;&#65292;DynMM&#21487;&#20197;&#22312;&#21482;&#26377;&#24494;&#19981;&#36275;&#36947;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23558;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;46.5%&#65288;CMU-MOSEI&#24773;&#24863;&#20998;&#26512;&#65289;&#65292;&#24182;&#22312;&#35745;&#31639;&#20013;&#33410;&#30465;&#36229;&#36807;21%&#30340;&#24320;&#38144;&#20197;&#25552;&#39640;&#20998;&#27573;&#24615;&#33021;&#65288;NYU Depth V2&#35821;&#20041;&#20998;&#21106;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep multimodal learning has achieved great progress in recent years. However, current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data. In this work, we propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses multimodal data and generates data-dependent forward paths during inference. To this end, we propose a gating function to provide modality-level or fusion-level decisions on-the-fly based on multimodal features and a resource-aware loss function that encourages computational efficiency. Results on various multimodal tasks demonstrate the efficiency and wide applicability of our approach. For instance, DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic s
&lt;/p&gt;</description></item></channel></rss>