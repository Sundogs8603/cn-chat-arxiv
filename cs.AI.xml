<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#35745;&#31639;&#30149;&#29702;&#23398;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#21508;&#31181;&#32452;&#32455;&#31867;&#22411;&#19978;&#36827;&#34892;&#24191;&#27867;&#24320;&#21457;&#21644;&#35780;&#20272;&#65292;&#20026;&#32452;&#32455;&#34920;&#22411;&#20998;&#22411;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.15474</link><description>&lt;p&gt;
&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#35745;&#31639;&#30149;&#29702;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A General-Purpose Self-Supervised Model for Computational Pathology. (arXiv:2308.15474v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#35745;&#31639;&#30149;&#29702;&#23398;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#21508;&#31181;&#32452;&#32455;&#31867;&#22411;&#19978;&#36827;&#34892;&#24191;&#27867;&#24320;&#21457;&#21644;&#35780;&#20272;&#65292;&#20026;&#32452;&#32455;&#34920;&#22411;&#20998;&#22411;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#34920;&#22411;&#20998;&#22411;&#26159;&#23398;&#20064;&#35299;&#21078;&#30149;&#29702;&#26631;&#24535;&#29289;&#23458;&#35266;&#29305;&#24449;&#30340;&#22522;&#26412;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25972;&#20010;&#20999;&#29255;&#26174;&#24494;&#38236;&#22270;&#20687;(WSI)&#23384;&#22312;&#19968;&#20010;&#22797;&#26434;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#21363;WSI&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#24418;&#24577;&#34920;&#22411;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#20351;&#24471;&#22823;&#35268;&#27169;&#25968;&#25454;&#27880;&#37322;&#25104;&#20026;&#19981;&#21487;&#33021;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#24037;&#20316;&#25552;&#20986;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#20174;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#36801;&#31227;&#23398;&#20064;&#25110;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20294;&#23578;&#26410;&#22312;&#21508;&#31181;&#32452;&#32455;&#31867;&#22411;&#19978;&#36827;&#34892;&#24191;&#27867;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;UNI&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#35745;&#31639;&#30149;&#29702;&#23398;&#27169;&#22411;&#65292;&#20351;&#29992;&#26469;&#33258;20&#31181;&#20027;&#35201;&#32452;&#32455;&#31867;&#22411;&#30340;100,000&#20010;&#35786;&#26029;&#24615;&#34880;&#32418;&#34507;&#30333;&#21644;&#20234;&#32418;&#22266;&#37255;&#26579;&#33394;&#20999;&#29255;&#30340;&#19968;&#20159;&#22810;&#20010;&#32452;&#32455;&#22359;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;33&#20010;&#20195;&#34920;&#24615;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#20020;&#24202;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tissue phenotyping is a fundamental computational pathology (CPath) task in learning objective characterizations of histopathologic biomarkers in anatomic pathology. However, whole-slide imaging (WSI) poses a complex computer vision problem in which the large-scale image resolutions of WSIs and the enormous diversity of morphological phenotypes preclude large-scale data annotation. Current efforts have proposed using pretrained image encoders with either transfer learning from natural image datasets or self-supervised pretraining on publicly-available histopathology datasets, but have not been extensively developed and evaluated across diverse tissue types at scale. We introduce UNI, a general-purpose self-supervised model for pathology, pretrained using over 100 million tissue patches from over 100,000 diagnostic haematoxylin and eosin-stained WSIs across 20 major tissue types, and evaluated on 33 representative CPath clinical tasks in CPath of varying diagnostic difficulties. In addi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22270;&#20687;&#25968;&#25454;&#21644;&#34920;&#26684;&#25968;&#25454;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#26684;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.15469</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#21644;&#34920;&#26684;&#27880;&#24847;&#21147;&#22312;&#33258;&#21160;&#21270;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multimodal Contrastive Learning and Tabular Attention for Automated Alzheimer's Disease Prediction. (arXiv:2308.15469v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22270;&#20687;&#25968;&#25454;&#21644;&#34920;&#26684;&#25968;&#25454;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#26684;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#24433;&#20687;&#23398;&#25968;&#25454;&#22914;MRI&#25195;&#25551;&#21644;PET&#65292;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#25968;&#25454;&#38598;&#20013;&#36824;&#21253;&#21547;&#26377;&#20215;&#20540;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#21253;&#25324;AD&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#20020;&#24202;&#35780;&#20272;&#12290;&#29616;&#26377;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#38590;&#20197;&#21033;&#29992;&#36825;&#20123;&#39069;&#22806;&#20449;&#24687;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#22270;&#20687;&#25968;&#25454;&#21644;&#34920;&#26684;&#25968;&#25454;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#19968;&#31181;&#29992;&#20110;&#25918;&#22823;&#21644;&#25490;&#24207;&#34920;&#26684;&#20013;&#26174;&#33879;&#29305;&#24449;&#30340;&#26032;&#39062;&#34920;&#26684;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#39044;&#27979;&#12290;&#23454;&#39564;&#35780;&#20272;&#36890;&#36807;&#20174;ADNI&#25968;&#25454;&#24211;&#20013;&#30340;882&#20010;MR&#22270;&#20687;&#20999;&#29255;&#20013;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#30340;&#39640;&#21487;&#35299;&#37322;&#24615;&#21644;&#25105;&#20204;&#30340;&#26032;&#39062;&#34920;&#26684;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27599;&#34892;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#36827;&#34892;&#24402;&#22240;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#24182;&#25490;&#21517;&#26368;&#20027;&#35201;&#30340;&#29305;&#24449;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#36229;&#36807;83.8%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alongside neuroimaging such as MRI scans and PET, Alzheimer's disease (AD) datasets contain valuable tabular data including AD biomarkers and clinical assessments. Existing computer vision approaches struggle to utilize this additional information. To address these needs, we propose a generalizable framework for multimodal contrastive learning of image data and tabular data, a novel tabular attention module for amplifying and ranking salient features in tables, and the application of these techniques onto Alzheimer's disease prediction. Experimental evaulations demonstrate the strength of our framework by detecting Alzheimer's disease (AD) from over 882 MR image slices from the ADNI database. We take advantage of the high interpretability of tabular data and our novel tabular attention approach and through attribution of the attention scores for each row of the table, we note and rank the most predominant features. Results show that the model is capable of an accuracy of over 83.8%, al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22810;&#31181;&#21463;&#37325;&#23614;&#20998;&#26512;&#21644;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#21551;&#21457;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#20132;&#36890;&#39044;&#27979;&#20013;&#25317;&#22581;&#24773;&#20917;&#30340;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#21035;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#20248;&#21270;&#30446;&#26631;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.15464</link><description>&lt;p&gt;
&#25439;&#22833;&#20989;&#25968;&#30340;&#27604;&#36739;&#30740;&#31350;&#65306;&#24120;&#35268;&#21644;&#25317;&#22581;&#24773;&#26223;&#19979;&#30340;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Loss Functions: Traffic Predictions in Regular and Congestion Scenarios. (arXiv:2308.15464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22810;&#31181;&#21463;&#37325;&#23614;&#20998;&#26512;&#21644;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#21551;&#21457;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#20132;&#36890;&#39044;&#27979;&#20013;&#25317;&#22581;&#24773;&#20917;&#30340;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#21035;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#20248;&#21270;&#30446;&#26631;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20132;&#36890;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20256;&#32479;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#24448;&#24448;&#38590;&#20197;&#20934;&#30830;&#39044;&#27979;&#25317;&#22581;&#24773;&#20917;&#12290;&#20934;&#30830;&#39044;&#27979;&#24120;&#35268;&#20132;&#36890;&#26465;&#20214;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36824;&#24517;&#39035;&#20934;&#30830;&#39044;&#27979;&#25317;&#22581;&#24773;&#26223;&#65292;&#20197;&#32500;&#25345;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#20132;&#36890;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21463;&#37325;&#23614;&#20998;&#26512;&#21644;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#21551;&#21457;&#30340;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#26041;&#38754;&#35780;&#20272;&#20102;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#25317;&#22581;&#24773;&#20917;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20248;&#21270;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#26102;&#65292;MAE-Focal Loss&#20989;&#25968;&#34920;&#29616;&#26368;&#20026;&#26377;&#25928;&#12290;&#22312;&#20248;&#21270;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#26102;&#65292;Gumbel Loss&#34987;&#35777;&#26126;&#26159;&#26356;&#20248;&#30340;&#36873;&#25321;&#12290;&#36825;&#20123;&#36873;&#25321;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;&#20132;&#36890;&#25317;&#22581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal graph neural networks have achieved state-of-the-art performance in traffic forecasting. However, they often struggle to forecast congestion accurately due to the limitations of traditional loss functions. While accurate forecasting of regular traffic conditions is crucial, a reliable AI system must also accurately forecast congestion scenarios to maintain safe and efficient transportation. In this paper, we explore various loss functions inspired by heavy tail analysis and imbalanced classification problems to address this issue. We evaluate the efficacy of these loss functions in forecasting traffic speed, with an emphasis on congestion scenarios. Through extensive experiments on real-world traffic datasets, we discovered that when optimizing for Mean Absolute Error (MAE), the MAE-Focal Loss function stands out as the most effective. When optimizing Mean Squared Error (MSE), Gumbel Loss proves to be the superior choice. These choices effectively forecast traffic conges
&lt;/p&gt;</description></item><item><title>ParaGuide&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;&#65292;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#65292;&#36890;&#36807;&#26799;&#24230;&#24341;&#23548;&#21644;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#30340;&#39118;&#26684;&#36716;&#21464;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.15459</link><description>&lt;p&gt;
ParaGuide: &#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;
&lt;/p&gt;
&lt;p&gt;
ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer. (arXiv:2308.15459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15459
&lt;/p&gt;
&lt;p&gt;
ParaGuide&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;&#65292;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#65292;&#36890;&#36807;&#26799;&#24230;&#24341;&#23548;&#21644;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#30340;&#39118;&#26684;&#36716;&#21464;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#26159;&#22312;&#20445;&#30041;&#24847;&#20041;&#30340;&#21516;&#26102;&#36716;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#23646;&#24615;&#30340;&#20219;&#21153;&#12290;&#30446;&#26631;&#39118;&#26684;&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#23450;&#20041;&#65292;&#20174;&#21333;&#19968;&#23646;&#24615;&#65288;&#20363;&#22914;&#27491;&#24335;&#24615;&#65289;&#21040;&#20316;&#32773;&#65288;&#20363;&#22914;&#33678;&#22763;&#27604;&#20122;&#65289;&#12290;&#20808;&#21069;&#30340;&#26080;&#30417;&#30563;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#20165;&#36866;&#29992;&#20110;&#22266;&#23450;&#30340;&#39118;&#26684;&#38598;&#65292;&#25110;&#38656;&#35201;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;ParaGuide&#21033;&#29992;&#20102;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#26469;&#33258;&#29616;&#25104;&#30340;&#20998;&#31867;&#22120;&#21644;&#24378;&#22823;&#30340;&#39118;&#26684;&#23884;&#20837;&#22120;&#30340;&#26799;&#24230;&#24341;&#23548;&#65292;&#20197;&#36716;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;Enron&#37038;&#20214;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#20154;&#24037;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27491;&#24335;&#24615;&#21644;... (&#20869;&#23481;&#22826;&#22810;&#65292;&#35831;&#21442;&#32771;&#33521;&#25991;&#25688;&#35201;)
&lt;/p&gt;
&lt;p&gt;
Textual style transfer is the task of transforming stylistic properties of text while preserving meaning. Target "styles" can be defined in numerous ways, ranging from single attributes (e.g, formality) to authorship (e.g, Shakespeare). Previous unsupervised style-transfer approaches generally rely on significant amounts of labeled data for only a fixed set of styles or require large language models. In contrast, we introduce a novel diffusion-based framework for general-purpose style transfer that can be flexibly adapted to arbitrary target styles at inference time. Our parameter-efficient approach, ParaGuide, leverages paraphrase-conditioned diffusion models alongside gradient-based guidance from both off-the-shelf classifiers and strong existing style embedders to transform the style of text while preserving semantic information. We validate the method on the Enron Email Corpus, with both human and automatic evaluations, and find that it outperforms strong baselines on formality, se
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;SMOTE&#21040;Mixup&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;&#12290;&#36890;&#36807;&#23545;SMOTE&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#32467;&#21512;Mixup&#25216;&#26415;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;Mixup&#25216;&#26415;&#36890;&#36807;&#23454;&#29616;&#22810;&#25968;&#31867;&#21644;&#23569;&#25968;&#31867;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38388;&#38553;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36793;&#30028;&#30340;Mixup&#25216;&#26415;&#65292;&#26356;&#26126;&#30830;&#22320;&#23454;&#29616;&#20102;&#19981;&#24179;&#34913;&#38388;&#38553;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15457</link><description>&lt;p&gt;
&#20174;SMOTE&#21040;Mixup&#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
From SMOTE to Mixup for Deep Imbalanced Classification. (arXiv:2308.15457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;SMOTE&#21040;Mixup&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;&#12290;&#36890;&#36807;&#23545;SMOTE&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#32467;&#21512;Mixup&#25216;&#26415;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;Mixup&#25216;&#26415;&#36890;&#36807;&#23454;&#29616;&#22810;&#25968;&#31867;&#21644;&#23569;&#25968;&#31867;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38388;&#38553;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36793;&#30028;&#30340;Mixup&#25216;&#26415;&#65292;&#26356;&#26126;&#30830;&#22320;&#23454;&#29616;&#20102;&#19981;&#24179;&#34913;&#38388;&#38553;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#22909;&#30340;&#20998;&#31867;&#22120;&#22240;&#20026;&#23569;&#25968;&#31867;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#32780;&#22256;&#38590;&#37325;&#37325;&#12290;&#20256;&#32479;&#19978;&#65292;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#30693;&#21517;&#23569;&#25968;&#31867;&#21512;&#25104;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#38754;&#21521;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#34987;&#29992;&#26469;&#25913;&#21892;&#36825;&#31181;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;SMOTE&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#26159;&#21542;&#20063;&#26377;&#30410;&#22788;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#21407;&#22987;&#30340;SMOTE&#23545;&#28145;&#24230;&#23398;&#20064;&#26469;&#35828;&#26159;&#19981;&#36275;&#30340;&#65292;&#24182;&#20351;&#29992;&#36719;&#26631;&#31614;&#22686;&#24378;&#20102;SMOTE&#12290;&#23558;&#24471;&#21040;&#30340;&#36719;SMOTE&#19982;Mixup&#65292;&#19968;&#31181;&#29616;&#20195;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#20256;&#32479;&#21644;&#29616;&#20195;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#32435;&#20837;&#21516;&#19968;&#20010;&#33539;&#30068;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#34920;&#26126;&#65292;Mixup&#36890;&#36807;&#38544;&#24335;&#22320;&#23454;&#29616;&#22810;&#25968;&#31867;&#21644;&#23569;&#25968;&#31867;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38388;&#38553;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36793;&#30028;&#30340;Mixup&#25216;&#26415;&#65292;&#26356;&#26126;&#30830;&#22320;&#23454;&#29616;&#20102;&#19981;&#24179;&#34913;&#38388;&#38553;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#37117;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given imbalanced data, it is hard to train a good classifier using deep learning because of the poor generalization of minority classes. Traditionally, the well-known synthetic minority oversampling technique (SMOTE) for data augmentation, a data mining approach for imbalanced learning, has been used to improve this generalization. However, it is unclear whether SMOTE also benefits deep learning. In this work, we study why the original SMOTE is insufficient for deep learning, and enhance SMOTE using soft labels. Connecting the resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to a unified framework that puts traditional and modern data augmentation techniques under the same umbrella. A careful study within this framework shows that Mixup improves generalization by implicitly achieving uneven margins between majority and minority classes. We then propose a novel margin-aware Mixup technique that more explicitly achieves uneven margins. Extensive experimental r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34917;&#20805;&#21355;&#26143;&#22320;&#22270;&#65292;&#22686;&#24378;&#20102;&#36710;&#36733;&#20256;&#24863;&#22120;&#26500;&#24314;&#39640;&#31934;&#24230;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21355;&#26143;&#22320;&#22270;&#30340;&#24191;&#38420;&#35206;&#30422;&#33021;&#21147;&#12290;&#25105;&#20204;&#37322;&#25918;&#20102;&#21355;&#26143;&#22320;&#22270;&#29926;&#29255;&#20316;&#20026;nuScenes&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#34701;&#21512;&#27169;&#22359;&#26469;&#26356;&#22909;&#22320;&#34701;&#21512;&#36710;&#36733;&#20256;&#24863;&#22120;&#19982;&#21355;&#26143;&#22320;&#22270;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.15427</link><description>&lt;p&gt;
&#36890;&#36807;&#21355;&#26143;&#22320;&#22270;&#34917;&#20805;&#36710;&#36733;&#20256;&#24863;&#22120;&#65306;&#39640;&#31934;&#24230;&#22320;&#22270;&#26500;&#24314;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction. (arXiv:2308.15427v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34917;&#20805;&#21355;&#26143;&#22320;&#22270;&#65292;&#22686;&#24378;&#20102;&#36710;&#36733;&#20256;&#24863;&#22120;&#26500;&#24314;&#39640;&#31934;&#24230;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21355;&#26143;&#22320;&#22270;&#30340;&#24191;&#38420;&#35206;&#30422;&#33021;&#21147;&#12290;&#25105;&#20204;&#37322;&#25918;&#20102;&#21355;&#26143;&#22320;&#22270;&#29926;&#29255;&#20316;&#20026;nuScenes&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#34701;&#21512;&#27169;&#22359;&#26469;&#26356;&#22909;&#22320;&#34701;&#21512;&#36710;&#36733;&#20256;&#24863;&#22120;&#19982;&#21355;&#26143;&#22320;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#31934;&#24230;&#65288;HD&#65289;&#22320;&#22270;&#23545;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#22522;&#20110;&#36710;&#36733;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#20449;&#24687;&#23454;&#26102;&#26500;&#24314;HD&#22320;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#36710;&#36742;&#21608;&#22260;&#29615;&#22659;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#36825;&#26159;&#30001;&#20110;&#36710;&#36733;&#20256;&#24863;&#22120;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#22914;&#23545;&#36828;&#31243;&#25506;&#27979;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#34917;&#20805;&#21355;&#26143;&#22320;&#22270;&#21487;&#20197;&#22686;&#24378;HD&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21033;&#29992;&#21355;&#26143;&#22320;&#22270;&#30340;&#24191;&#27867;&#35206;&#30422;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#21355;&#26143;&#22320;&#22270;&#29926;&#29255;&#20316;&#20026;nuScenes&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#25968;&#25454;&#38598;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#34701;&#21512;&#27169;&#22359;&#65292;&#20351;&#21355;&#26143;&#22320;&#22270;&#20449;&#24687;&#19982;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#34701;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#20998;&#21106;&#21644;&#36317;&#31163;&#30340;&#27880;&#24847;&#21147;&#25513;&#30721;&#65292;&#24212;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#34701;&#21512;&#36710;&#36733;&#20256;&#24863;&#22120;&#19982;&#21355;&#26143;&#22320;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-Definition (HD) maps play a crucial role in autonomous driving systems. Recent methods have attempted to construct HD maps in real-time based on information obtained from vehicle onboard sensors. However, the performance of these methods is significantly susceptible to the environment surrounding the vehicle due to the inherent limitation of onboard sensors, such as weak capacity for long-range detection. In this study, we demonstrate that supplementing onboard sensors with satellite maps can enhance the performance of HD map construction methods, leveraging the broad coverage capability of satellite maps. For the purpose of further research, we release the satellite map tiles as a complementary dataset of nuScenes dataset. Meanwhile, we propose a hierarchical fusion module that enables better fusion of satellite maps information with existing methods. Specifically, we design an attention mask based on segmentation and distance, applying the cross-attention mechanism to fuse onboa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#29992;&#25143;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#21327;&#35843;&#21644;&#20559;&#22909;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#26412;&#39068;&#33394;&#30340;&#20559;&#22909;&#21644;&#33394;&#24425;&#21327;&#35843;&#30340;&#35780;&#20998;&#26469;&#39044;&#27979;&#23545;&#33394;&#24425;&#26041;&#26696;&#30340;&#20559;&#22909;&#65292;&#22312;&#21508;&#31181;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#20013;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.15397</link><description>&lt;p&gt;
&#33394;&#24425;&#32654;&#23398;&#65306;&#22522;&#20110;&#27169;&#31946;&#29992;&#25143;&#39537;&#21160;&#30340;&#21327;&#35843;&#19982;&#20559;&#22909;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Color Aesthetics: Fuzzy based User-driven Method for Harmony and Preference Prediction. (arXiv:2308.15397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15397
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#29992;&#25143;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#21327;&#35843;&#21644;&#20559;&#22909;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#26412;&#39068;&#33394;&#30340;&#20559;&#22909;&#21644;&#33394;&#24425;&#21327;&#35843;&#30340;&#35780;&#20998;&#26469;&#39044;&#27979;&#23545;&#33394;&#24425;&#26041;&#26696;&#30340;&#20559;&#22909;&#65292;&#22312;&#21508;&#31181;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#20013;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33394;&#24425;&#26159;&#23545;&#20135;&#21697;&#38144;&#21806;&#20135;&#29983;&#24378;&#22823;&#24433;&#21709;&#30340;&#26368;&#37325;&#35201;&#30340;&#20869;&#22312;&#24863;&#35273;&#29305;&#24449;&#12290;&#33394;&#24425;&#29978;&#33267;&#23545;&#25105;&#20204;&#22823;&#33041;&#20013;&#30340;&#23457;&#32654;&#24863;&#30693;&#36215;&#21040;&#25552;&#39640;&#30340;&#20316;&#29992;&#12290;&#32771;&#34385;&#21040;&#20010;&#20307;&#24046;&#24322;&#22312;&#33394;&#24425;&#32654;&#23398;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#21508;&#31181;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#65292;&#38656;&#35201;&#29992;&#25143;&#39537;&#21160;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23450;&#37327;&#35780;&#20272;&#23545;&#39068;&#33394;&#30340;&#25152;&#26377;&#31867;&#22411;&#30340;&#30693;&#35273;&#21453;&#24212;&#30340;&#26041;&#27861;&#65306;&#26126;&#30830;&#30340;&#39068;&#33394;&#20559;&#22909;&#12289;&#33394;&#24425;&#21327;&#35843;&#21644;&#33394;&#24425;&#32452;&#21512;&#20559;&#22909;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#26412;&#39068;&#33394;&#30340;&#20559;&#22909;&#21644;&#33394;&#24425;&#21327;&#35843;&#30340;&#35780;&#20998;&#65292;&#21487;&#20197;&#39044;&#27979;&#23545;&#33394;&#24425;&#26041;&#26696;&#30340;&#20559;&#22909;&#12290;&#20351;&#29992;&#22522;&#20110;&#27169;&#31946;&#30456;&#20284;&#24615;&#21644;&#20998;&#32452;&#30340;&#27604;&#36739;&#31639;&#27861;&#20174;&#22823;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#21327;&#35843;&#35843;&#33394;&#26495;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#23545;&#22810;&#33394;&#22270;&#20687;&#30340;&#21327;&#35843;&#21644;&#20559;&#22909;&#36827;&#34892;&#26377;&#29992;&#30340;&#39044;&#27979;&#12290;&#20363;&#22914;&#65292;&#22312;&#26381;&#35013;&#21327;&#35843;&#30340;&#32972;&#26223;&#19979;&#65292;&#23427;&#20801;&#35768;&#26681;&#25454;&#26381;&#35013;&#39068;&#33394;&#39044;&#27979;&#22806;&#35266;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26631;&#20934;&#30340;&#23457;&#32654;&#27169;&#22411;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Color is the most important intrinsic sensory feature that has a powerful impact on product sales. Color is even responsible for raising the aesthetic senses in our brains. Account for individual differences is crucial in color aesthetics. It requires user-driven mechanisms for various e-commerce applications. We propose a method for quantitative evaluation of all types of perceptual responses to color(s): distinct color preference, color harmony, and color combination preference. Preference for color schemes can be predicted by combining preferences for the basic colors and ratings of color harmony. Harmonious pallets are extracted from big data set using comparison algorithms based on fuzzy similarity and grouping. The proposed model results in useful predictions of harmony and preference of multicolored images. For example, in the context of apparel coordination, it allows predicting a preference for a look based on clothing colors. Our approach differs from standard aesthetic model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#30005;&#33655;&#24179;&#34913;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#20013;&#30340;&#30005;&#33655;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24179;&#22343;&#19968;&#33268;&#24615;&#31639;&#27861;&#26469;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.15394</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30005;&#33655;&#24179;&#34913;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-agent Reinforcement Learning based State-of-Charge Balancing Strategy for Distributed Energy Storage System. (arXiv:2308.15394v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#30005;&#33655;&#24179;&#34913;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#20013;&#30340;&#30005;&#33655;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24179;&#22343;&#19968;&#33268;&#24615;&#31639;&#27861;&#26469;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;Dec-MARL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#24335;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#20013;&#30340;&#30005;&#33655;&#24179;&#34913;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#23558;&#30005;&#33655;&#24179;&#34913;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#26377;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#26681;&#25454;&#38656;&#27714;&#24179;&#34913;&#24471;&#21040;&#30340;&#21160;&#20316;&#32422;&#26463;&#65292;&#37319;&#29992;Dec-MARL&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21033;&#29992;&#19968;&#38454;&#24179;&#22343;&#19968;&#33268;&#24615;&#31639;&#27861;&#20197;&#23436;&#20840;&#20998;&#25955;&#30340;&#26041;&#24335;&#25193;&#23637;&#20998;&#24067;&#24335;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#29366;&#24577;&#30340;&#35266;&#27979;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#35266;&#27979;&#20915;&#23450;&#21021;&#22987;&#21160;&#20316;&#65288;&#21363;&#36755;&#20986;&#21151;&#29575;&#65289;&#12290;&#20026;&#20102;&#24471;&#21040;&#20801;&#35768;&#33539;&#22260;&#20869;&#30340;&#26368;&#32456;&#21160;&#20316;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#38656;&#27714;&#24179;&#34913;&#31639;&#27861;&#26469;&#24179;&#34913;&#24635;&#38656;&#27714;&#21644;&#21021;&#22987;&#21160;&#20316;&#12290;&#28982;&#21518;&#65292;&#26234;&#33021;&#20307;&#25191;&#34892;&#26368;&#32456;&#21160;&#20316;&#24182;&#20174;&#29615;&#22659;&#20013;&#33719;&#24471;&#23616;&#37096;&#22870;&#21169;&#65292;&#20351;&#31995;&#32479;&#36827;&#20837;&#19979;&#19968;&#20010;&#29366;&#24577;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#19968;&#38454;&#24179;&#22343;&#19968;&#33268;&#24615;&#31639;&#27861;&#65292;&#26234;&#33021;&#20307;&#33719;&#24471;&#24179;&#22343;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#31574;&#30053;&#26469;&#19981;&#26029;&#20248;&#21270;&#30005;&#33655;&#24179;&#34913;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops a Decentralized Multi-Agent Reinforcement Learning (Dec-MARL) method to solve the SoC balancing problem in the distributed energy storage system (DESS). First, the SoC balancing problem is formulated into a finite Markov decision process with action constraints derived from demand balance, which can be solved by Dec-MARL. Specifically, the first-order average consensus algorithm is utilized to expand the observations of the DESS state in a fully-decentralized way, and the initial actions (i.e., output power) are decided by the agents (i.e., energy storage units) according to these observations. In order to get the final actions in the allowable range, a counterfactual demand balance algorithm is proposed to balance the total demand and the initial actions. Next, the agents execute the final actions and get local rewards from the environment, and the DESS steps into the next state. Finally, through the first-order average consensus algorithm, the agents get the avera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#19978;&#32780;&#19979;&#35843;&#33410;&#30340;WTA&#32593;&#32476;&#22312;&#36125;&#21494;&#26031;&#25512;&#29702;&#20013;&#25972;&#21512;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#33258;&#19978;&#32780;&#19979;&#30340;&#36807;&#31243;&#22312;&#25552;&#39640;WTA&#32593;&#32476;&#24615;&#33021;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.15390</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#19978;&#21521;&#19979;&#35843;&#33410;&#30340;WTA&#32593;&#32476;&#22312;&#36125;&#21494;&#26031;&#25512;&#29702;&#20013;&#25972;&#21512;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bayesian Integration of Information Using Top-Down Modulated WTA Networks. (arXiv:2308.15390v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#19978;&#32780;&#19979;&#35843;&#33410;&#30340;WTA&#32593;&#32476;&#22312;&#36125;&#21494;&#26031;&#25512;&#29702;&#20013;&#25972;&#21512;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#33258;&#19978;&#32780;&#19979;&#30340;&#36807;&#31243;&#22312;&#25552;&#39640;WTA&#32593;&#32476;&#24615;&#33021;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32988;&#32773;&#36890;&#21507;&#65288;WTA&#65289;&#30005;&#36335;&#20316;&#20026;&#19968;&#31181;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#34987;&#24314;&#35758;&#20316;&#20026;&#22823;&#33041;&#20197;&#36125;&#21494;&#26031;&#26041;&#24335;&#22788;&#29702;&#20449;&#24687;&#30340;&#33021;&#21147;&#30340;&#20419;&#36827;&#22240;&#32032;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;WTA&#30005;&#36335;&#33021;&#22815;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#36924;&#36817;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20010;&#30740;&#31350;&#26041;&#21521;&#20027;&#35201;&#20851;&#27880;&#33258;&#19979;&#32780;&#19978;&#30340;&#36807;&#31243;&#65292;&#36825;&#19982;&#31070;&#32463;&#31185;&#23398;&#35777;&#25454;&#30456;&#30683;&#30462;&#65292;&#21518;&#32773;&#26174;&#31034;&#38500;&#20102;&#33258;&#19979;&#32780;&#19978;&#30340;&#36807;&#31243;&#22806;&#65292;&#33258;&#19978;&#32780;&#19979;&#30340;&#36807;&#31243;&#20063;&#22312;&#20154;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#33258;&#19978;&#32780;&#19979;&#30340;&#36807;&#31243;&#21253;&#25324;&#27880;&#24847;&#21147;&#30340;&#25351;&#21521;&#12289;&#23545;&#26399;&#26395;&#30340;&#35843;&#25972;&#12289;&#23398;&#20064;&#20449;&#24687;&#30340;&#32534;&#30721;&#21644;&#22238;&#24518;&#30340;&#20419;&#36827;&#20197;&#21450;&#24819;&#35937;&#31561;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;WTA&#30005;&#36335;&#26159;&#21542;&#36866;&#21512;&#36827;&#19968;&#27493;&#25972;&#21512;&#22312;&#19981;&#21516;WTA&#32593;&#32476;&#20013;&#34920;&#31034;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25506;&#35752;&#20102;&#22312;&#20160;&#20040;&#24773;&#20917;&#19979;&#33258;&#19978;&#32780;&#19979;&#30340;&#36807;&#31243;&#33021;&#22815;&#25552;&#39640;WTA&#32593;&#32476;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Winner Take All (WTA) circuits a type of Spiking Neural Networks (SNN) have been suggested as facilitating the brain's ability to process information in a Bayesian manner. Research has shown that WTA circuits are capable of approximating hierarchical Bayesian models via Expectation Maximization (EM). So far, research in this direction has focused on bottom up processes. This is contrary to neuroscientific evidence that shows that, besides bottom up processes, top down processes too play a key role in information processing by the human brain. Several functions ascribed to top down processes include direction of attention, adjusting for expectations, facilitation of encoding and recall of learned information, and imagery. This paper explores whether WTA circuits are suitable for further integrating information represented in separate WTA networks. Furthermore, it explores whether, and under what circumstances, top down processes can improve WTA network performance with respect to infere
&lt;/p&gt;</description></item><item><title>pFedPG is a novel personalized FL framework that generates client-specific prompts to adapt frozen backbones to local data distributions, enabling efficient model personalization for heterogeneous clients in FL.</title><link>http://arxiv.org/abs/2308.15367</link><description>&lt;p&gt;
&#36890;&#36807;&#23458;&#25143;&#31471;&#29305;&#23450;&#30340;&#25552;&#31034;&#29983;&#25104;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation. (arXiv:2308.15367v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15367
&lt;/p&gt;
&lt;p&gt;
pFedPG is a novel personalized FL framework that generates client-specific prompts to adapt frozen backbones to local data distributions, enabling efficient model personalization for heterogeneous clients in FL.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#20998;&#25955;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#20197;&#20445;&#25252;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#22810;&#20010;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#35757;&#32451;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;Vision Transformer&#65289;&#23637;&#31034;&#20102;&#20174;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#20013;&#33719;&#24471;&#31283;&#20581;&#34920;&#31034;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12289;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#36890;&#20449;&#24102;&#23485;&#38480;&#21046;&#20102;&#22823;&#22411;&#27169;&#22411;&#22312;FL&#26694;&#26550;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#21033;&#29992;&#22823;&#22411;&#27169;&#22411;&#30340;&#31283;&#20581;&#34920;&#31034;&#65292;&#21516;&#26102;&#23454;&#29616;&#23545;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#39640;&#25928;&#27169;&#22411;&#20010;&#24615;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;FL&#26694;&#26550;&#65292;&#21363;&#23458;&#25143;&#31471;&#29305;&#23450;&#30340;&#25552;&#31034;&#29983;&#25104;&#65288;pFedPG&#65289;&#65292;&#23427;&#23398;&#20064;&#22312;&#26381;&#21153;&#22120;&#31471;&#37096;&#32626;&#20010;&#24615;&#21270;&#25552;&#31034;&#29983;&#25104;&#22120;&#65292;&#29992;&#20197;&#20135;&#29983;&#36866;&#24212;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#29305;&#23450;&#35270;&#35273;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23558;&#20923;&#32467;&#30340;&#39592;&#24178;&#32593;&#32476;&#36866;&#24212;&#21040;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) emerges as a decentralized learning framework which trains models from multiple distributed clients without sharing their data to preserve privacy. Recently, large-scale pre-trained models (e.g., Vision Transformer) have shown a strong capability of deriving robust representations. However, the data heterogeneity among clients, the limited computation resources, and the communication bandwidth restrict the deployment of large-scale models in FL frameworks. To leverage robust representations from large-scale models while enabling efficient model personalization for heterogeneous clients, we propose a novel personalized FL framework of client-specific Prompt Generation (pFedPG), which learns to deploy a personalized prompt generator at the server for producing client-specific visual prompts that efficiently adapts frozen backbones to local data distributions. Our proposed framework jointly optimizes the stages of personalized prompt adaptation locally and personal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#39640;&#20998;&#36776;&#29575;&#38647;&#36798;&#20256;&#24863;&#22120;&#22312;&#31215;&#32047;&#38647;&#36798;&#28857;&#20113;&#21644;&#25913;&#21892;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#21644;&#21160;&#24577;&#36816;&#21160;&#26657;&#27491;&#26041;&#27861;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15357</link><description>&lt;p&gt;
&#20174;&#19977;&#32500;&#28857;&#20113;&#20013;&#20272;&#35745;&#33258;&#25105;&#36816;&#21160;&#21644;&#21160;&#24577;&#36816;&#21160;&#30340;&#20998;&#31163;&#20197;&#31215;&#32047;&#25968;&#25454;&#21644;&#25913;&#21892;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ego-Motion Estimation and Dynamic Motion Separation from 3D Point Clouds for Accumulating Data and Improving 3D Object Detection. (arXiv:2308.15357v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#39640;&#20998;&#36776;&#29575;&#38647;&#36798;&#20256;&#24863;&#22120;&#22312;&#31215;&#32047;&#38647;&#36798;&#28857;&#20113;&#21644;&#25913;&#21892;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#21644;&#21160;&#24577;&#36816;&#21160;&#26657;&#27491;&#26041;&#27861;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#30340;&#19977;&#32500;&#39640;&#20998;&#36776;&#29575;&#38647;&#36798;&#20256;&#24863;&#22120;&#22312;&#27773;&#36710;&#39046;&#22495;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#30456;&#23545;&#20415;&#23452;&#19988;&#26816;&#27979;&#33021;&#21147;&#27604;&#20256;&#32479;&#20302;&#20998;&#36776;&#29575;&#38647;&#36798;&#20256;&#24863;&#22120;&#26356;&#22909;&#12290;&#19982;&#28608;&#20809;&#38647;&#36798;&#20256;&#24863;&#22120;&#30456;&#27604;&#65292;&#39640;&#20998;&#36776;&#29575;&#38647;&#36798;&#20256;&#24863;&#22120;&#30340;&#19968;&#20010;&#38480;&#21046;&#26159;&#29983;&#25104;&#30340;&#28857;&#20113;&#31232;&#30095;&#12290;&#36890;&#36807;&#31215;&#32047;&#36830;&#32493;&#26102;&#38388;&#27493;&#38271;&#30340;&#38647;&#36798;&#28857;&#20113;&#65292;&#21487;&#20197;&#37096;&#20998;&#20811;&#26381;&#36825;&#31181;&#31232;&#30095;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#19981;&#21516;&#30340;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;View-of-Delft&#25968;&#25454;&#38598;&#19978;&#31215;&#32047;&#38647;&#36798;&#28857;&#20113;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#23454;&#20363;&#36816;&#21160;&#20272;&#35745;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#21160;&#24577;&#36816;&#21160;&#23545;&#32047;&#31215;&#28857;&#20113;&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24212;&#29992;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#21644;&#21160;&#24577;&#36816;&#21160;&#26657;&#27491;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
New 3+1D high-resolution radar sensors are gaining importance for 3D object detection in the automotive domain due to their relative affordability and improved detection compared to classic low-resolution radar sensors. One limitation of high-resolution radar sensors, compared to lidar sensors, is the sparsity of the generated point cloud. This sparsity could be partially overcome by accumulating radar point clouds of subsequent time steps. This contribution analyzes limitations of accumulating radar point clouds on the View-of-Delft dataset. By employing different ego-motion estimation approaches, the dataset's inherent constraints, and possible solutions are analyzed. Additionally, a learning-based instance motion estimation approach is deployed to investigate the influence of dynamic motion on the accumulated point cloud for object detection. Experiments document an improved object detection performance by applying an ego-motion estimation and dynamic motion correction approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23631;&#24149;&#38378;&#20809;&#19979;&#23545;&#22810;&#26679;&#25915;&#20987;&#31867;&#22411;&#40065;&#26834;&#30340;&#20154;&#33080;&#38450;&#20266;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#20010;&#32593;&#32476;&#21644;&#21452;&#38376;&#27169;&#22359;&#65292;&#26377;&#25928;&#20943;&#23567;&#20102;&#27450;&#39575;&#20154;&#33080;&#30340;&#31867;&#20869;&#36317;&#31163;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#38450;&#20266;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15346</link><description>&lt;p&gt;
&#25552;&#21319;&#31227;&#21160;&#20154;&#33080;&#38450;&#20266;&#65306;&#22312;&#23631;&#24149;&#38378;&#20809;&#19979;&#23545;&#22810;&#26679;&#25915;&#20987;&#31867;&#22411;&#30340;&#40065;&#26834;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Enhancing Mobile Face Anti-Spoofing: A Robust Framework for Diverse Attack Types under Screen Flash. (arXiv:2308.15346v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23631;&#24149;&#38378;&#20809;&#19979;&#23545;&#22810;&#26679;&#25915;&#20987;&#31867;&#22411;&#40065;&#26834;&#30340;&#20154;&#33080;&#38450;&#20266;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#20010;&#32593;&#32476;&#21644;&#21452;&#38376;&#27169;&#22359;&#65292;&#26377;&#25928;&#20943;&#23567;&#20102;&#27450;&#39575;&#20154;&#33080;&#30340;&#31867;&#20869;&#36317;&#31163;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#38450;&#20266;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#38450;&#20266;&#65288;FAS&#65289;&#23545;&#20110;&#20445;&#25252;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#20108;&#36827;&#21046;&#25110;&#20687;&#32032;&#32423;&#26631;&#31614;&#30340;FAS&#26041;&#27861;&#30001;&#20110;&#21508;&#31181;&#23637;&#31034;&#25915;&#20987;&#65288;PA&#65289;&#30340;&#38480;&#21046;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20809;&#38378;&#20809;&#19979;&#25915;&#20987;&#31867;&#22411;&#40065;&#26834;&#30340;&#20154;&#33080;&#38450;&#20266;&#26694;&#26550;&#65292;&#31216;&#20026;ATR-FAS&#12290;&#30001;&#20110;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#23548;&#33268;&#30340;&#22270;&#20687;&#24046;&#24322;&#65292;&#22522;&#20110;&#21333;&#19968;&#20108;&#36827;&#21046;&#20998;&#31867;&#32593;&#32476;&#30340;&#20256;&#32479;FAS&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27450;&#39575;&#20154;&#33080;&#30340;&#36807;&#24230;&#31867;&#20869;&#36317;&#31163;&#65292;&#20174;&#32780;&#23548;&#33268;&#20915;&#31574;&#36793;&#30028;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#20010;&#32593;&#32476;&#26469;&#37325;&#24314;&#22810;&#24103;&#28145;&#24230;&#22270;&#20316;&#20026;&#36741;&#21161;&#30417;&#30563;&#65292;&#24182;&#19988;&#27599;&#20010;&#32593;&#32476;&#19987;&#27880;&#20110;&#19968;&#31181;&#25915;&#20987;&#31867;&#22411;&#12290;&#24341;&#20837;&#20102;&#30001;&#31867;&#22411;&#38376;&#21644;&#24103;&#27880;&#24847;&#21147;&#38376;&#32452;&#25104;&#30340;&#21452;&#38376;&#27169;&#22359;&#65288;DGM&#65289;&#65292;&#20998;&#21035;&#29992;&#20110;&#25915;&#20987;&#31867;&#22411;&#35782;&#21035;&#21644;&#22810;&#24103;&#27880;&#24847;&#21147;&#29983;&#25104;&#12290;DGM&#30340;&#36755;&#20986;&#34987;&#29992;&#20316;&#21152;&#26435;&#28151;&#21512;&#22810;&#20010;&#19987;&#23478;&#32593;&#32476;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face anti-spoofing (FAS) is crucial for securing face recognition systems. However, existing FAS methods with handcrafted binary or pixel-wise labels have limitations due to diverse presentation attacks (PAs). In this paper, we propose an attack type robust face anti-spoofing framework under light flash, called ATR-FAS. Due to imaging differences caused by various attack types, traditional FAS methods based on single binary classification network may result in excessive intra-class distance of spoof faces, leading to a challenge of decision boundary learning. Therefore, we employed multiple networks to reconstruct multi-frame depth maps as auxiliary supervision, and each network experts in one type of attack. A dual gate module (DGM) consisting of a type gate and a frame-attention gate is introduced, which perform attack type recognition and multi-frame attention generation, respectively. The outputs of DGM are utilized as weight to mix the result of multiple expert networks. The multi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#24179;&#34913;&#21644;&#22686;&#24378;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;&#35813;&#26041;&#27861;&#23545;&#20110;&#20854;&#20182;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#39640;&#21644;&#26679;&#26412;&#37327;&#23567;&#30340;&#24773;&#20917;&#20063;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.15339</link><description>&lt;p&gt;
&#26089;&#26399;&#35786;&#26029;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65306;&#36793;&#30028;SMOTE&#12289;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AI Framework for Early Diagnosis of Coronary Artery Disease: An Integration of Borderline SMOTE, Autoencoders and Convolutional Neural Networks Approach. (arXiv:2308.15339v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#24179;&#34913;&#21644;&#22686;&#24378;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;&#35813;&#26041;&#27861;&#23545;&#20110;&#20854;&#20182;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#39640;&#21644;&#26679;&#26412;&#37327;&#23567;&#30340;&#24773;&#20917;&#20063;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;&#22810;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#30151;&#29366;&#12289;&#21307;&#23398;&#26816;&#26597;&#12289;&#24515;&#30005;&#22270;&#21644;&#36229;&#22768;&#24515;&#21160;&#22270;&#31561;&#25968;&#25454;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#22312;&#35786;&#26029;&#36807;&#31243;&#30340;&#26089;&#26399;&#38454;&#27573;&#35782;&#21035;&#39640;&#39118;&#38505;&#24739;&#32773;&#65292;&#36890;&#36807;&#32508;&#21512;&#22810;&#20010;&#22240;&#32032;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26681;&#25454;CAD&#30142;&#30149;&#39118;&#38505;&#23545;&#24739;&#32773;&#36827;&#34892;&#20998;&#31867;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#29992;&#20110;&#24179;&#34913;&#21644;&#22686;&#24378;&#25968;&#25454;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#22312;&#30149;&#20363;&#25968;&#25454;&#19981;&#24179;&#34913;&#19988;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#20026;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#20351;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#39640;&#19988;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;CAD&#39044;&#27979;&#26041;&#27861;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;95.36&#65292;&#39640;&#20110;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#12289;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accuracy of coronary artery disease (CAD) diagnosis is dependent on a variety of factors, including demographic, symptom, and medical examination, ECG, and echocardiography data, among others. In this context, artificial intelligence (AI) can help clinicians identify high-risk patients early in the diagnostic process, by synthesizing information from multiple factors. To this aim, Machine Learning algorithms are used to classify patients based on their CAD disease risk. In this study, we contribute to this research filed by developing a methodology for balancing and augmenting data for more accurate prediction when the data is imbalanced and the sample size is small. The methodology can be used in a variety of other situations, particularly when data collection is expensive and the sample size is small. The experimental results revealed that the average accuracy of our proposed method for CAD prediction was 95.36, and was higher than random forest (RF), decision tree (DT), support 
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#33258;&#21160;&#23398;&#29983;&#21453;&#39304;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#20016;&#23500;&#30340;&#21453;&#39304;&#65292;&#20294;&#24341;&#20837;&#20102;&#20262;&#29702;&#38382;&#39064;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#8220;&#22810;&#25968;&#20154;&#30340;&#26292;&#25919;&#8221;&#21644;&#24573;&#35270;&#38271;&#23614;&#20013;&#23569;&#25968;&#32676;&#20307;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.15334</link><description>&lt;p&gt;
&#19968;&#31181;&#36127;&#36131;&#20219;&#24320;&#21457;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#33258;&#21160;&#23398;&#29983;&#21453;&#39304;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Responsible Development of Automated Student Feedback with Generative AI. (arXiv:2308.15334v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15334
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#33258;&#21160;&#23398;&#29983;&#21453;&#39304;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#20016;&#23500;&#30340;&#21453;&#39304;&#65292;&#20294;&#24341;&#20837;&#20102;&#20262;&#29702;&#38382;&#39064;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#8220;&#22810;&#25968;&#20154;&#30340;&#26292;&#25919;&#8221;&#21644;&#24573;&#35270;&#38271;&#23614;&#20013;&#23569;&#25968;&#32676;&#20307;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20016;&#23500;&#30340;&#21453;&#39304;&#23545;&#20110;&#25903;&#25345;&#23398;&#29983;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#29983;&#25104;AI&#23588;&#20854;&#26159;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#20026;&#21521;&#23398;&#29983;&#25552;&#20379;&#21487;&#37325;&#22797;&#12289;&#21487;&#25193;&#23637;&#21644;&#21363;&#26102;&#29983;&#25104;&#30340;&#33258;&#21160;&#21453;&#39304;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#20351;&#24471;&#20043;&#21069;&#31232;&#32570;&#19988;&#26114;&#36149;&#30340;&#23398;&#20064;&#36164;&#28304;&#21464;&#24471;&#20016;&#23500;&#36215;&#26469;&#12290;&#20174;&#25216;&#26415;&#35282;&#24230;&#32780;&#35328;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#21487;&#34892;&#30340;&#65292;&#24471;&#30410;&#20110;&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#27493;&#65307;&#28982;&#32780;&#65292;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#20063;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#28508;&#22312;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#38656;&#35201;&#35748;&#30495;&#32771;&#34385;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21560;&#24341;&#21147;&#22312;&#20110;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#33258;&#21160;&#21270;&#26368;&#20047;&#21619;&#30340;&#20219;&#21153;&#65307;&#20294;&#26159;&#36825;&#20063;&#21487;&#33021;&#23548;&#33268;&#8220;&#22810;&#25968;&#20154;&#30340;&#26292;&#25919;&#8221;&#65292;&#21363;&#24573;&#35270;&#20102;&#38271;&#23614;&#20013;&#23569;&#25968;&#32676;&#20307;&#30340;&#38656;&#27714;&#65292;&#22240;&#20026;&#36825;&#20123;&#38656;&#27714;&#24456;&#38590;&#33258;&#21160;&#21270;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#33021;&#22815;&#20135;&#29983;&#26377;&#20215;&#20540;&#21644;&#30495;&#23454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing rich feedback to students is essential for supporting student learning. Recent advances in generative AI, particularly within large language modelling (LLM), provide the opportunity to deliver repeatable, scalable and instant automatically generated feedback to students, making abundant a previously scarce and expensive learning resource. Such an approach is feasible from a technical perspective due to these recent advances in Artificial Intelligence (AI) and Natural Language Processing (NLP); while the potential upside is a strong motivator, doing so introduces a range of potential ethical issues that must be considered as we apply these technologies. The attractiveness of AI systems is that they can effectively automate the most mundane tasks; but this risks introducing a "tyranny of the majority", where the needs of minorities in the long tail are overlooked because they are difficult to automate.  Developing machine learning models that can generate valuable and authentic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLogic&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39046;&#22495;&#24605;&#32500;&#38142;&#36873;&#25321;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24179;&#34913;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15324</link><description>&lt;p&gt;
FedLogic: &#21487;&#35299;&#37322;&#21270;&#30340;&#32852;&#37030;&#22810;&#39046;&#22495;&#24605;&#32500;&#38142;&#36873;&#25321;&#26041;&#27861;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FedLogic: Interpretable Federated Multi-Domain Chain-of-Thought Prompt Selection for Large Language Models. (arXiv:2308.15324v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLogic&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39046;&#22495;&#24605;&#32500;&#38142;&#36873;&#25321;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24179;&#34913;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#8220;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#8221;&#25512;&#29702;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#33719;&#21462;&#24555;&#36895;&#31934;&#30830;&#30340;&#22238;&#31572;&#27491;&#36805;&#36895;&#24341;&#36215;&#30740;&#31350;&#30028;&#30340;&#20852;&#36259;&#12290;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#35774;&#35745;&#25110;&#36873;&#25321;&#26368;&#20339;&#25552;&#31034;&#12290;&#25552;&#31034;&#36873;&#25321;&#30340;&#36807;&#31243;&#20381;&#36182;&#20110;&#29992;&#25143;&#26681;&#25454;LLM&#29983;&#25104;&#30340;&#30456;&#24212;&#26032;&#21453;&#24212;&#19981;&#26029;&#35843;&#25972;&#21644;&#32452;&#21512;&#36755;&#20837;&#25552;&#31034;&#30340;&#35797;&#38169;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#30740;&#31350;&#25506;&#35752;LLM&#22914;&#20309;&#21033;&#29992;&#20174;&#29992;&#25143;&#20132;&#20114;&#20013;&#23398;&#20064;&#21040;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#33021;&#21147;&#26469;&#35299;&#20915;&#21465;&#36848;&#20889;&#20316;&#20013;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25913;&#36827;&#21487;&#35299;&#37322;&#24615;&#24182;&#22312;&#22810;&#39046;&#22495;CoT&#25552;&#31034;&#36873;&#25321;&#22330;&#26223;&#19979;&#25506;&#32034;&#36890;&#29992;&#24615;&#21644;&#20010;&#24615;&#21270;&#20043;&#38388;&#30340;&#24179;&#34913;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#36923;&#36753;&#35268;&#21017;&#23398;&#20064;&#26041;&#27861;&#65288;FedLogic&#65289;&#12290;&#25105;&#20204;&#22312;&#32852;&#37030;LLM&#30340;&#32972;&#26223;&#19979;&#24341;&#20837;&#20102;&#22810;&#39046;&#22495;CoT&#25552;&#31034;&#36873;&#25321;&#22256;&#22659;&#30340;&#29702;&#35770;&#24418;&#24335;&#21270;&#21644;&#20132;&#20114;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging ``chain-of-thought (CoT)'' reasoning to elicit rapid and precise responses from large language models (LLMs) is rapidly attracting research interest. A notable challenge here is how to design or select optimal prompts. The process of prompt selection relies on trial and error, involving continuous adjustments and combinations of input prompts by users based on the corresponding new responses generated from LLMs. Furthermore, minimal research has been conducted to explore how LLMs employ the mathematical problem-solving capabilities learned from user interactions to address issues in narrative writing. To improve interpretability and explore the balance principle between generality and personalization under a multi-domain CoT prompt selection scenario, we propose the Federated Logic rule learning approach (FedLogic). We introduce a theoretical formalization and interactive emulation of the multi-domain CoT prompt selection dilemma in the context of federated LLMs. We cast the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15321</link><description>&lt;p&gt;
&#38416;&#26126;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#8220;&#26333;&#20809;&#20559;&#24046;&#8221;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#65292;&#32570;&#20047;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#20808;&#23545;&#37319;&#26679;&#20998;&#24067;&#36827;&#34892;&#20998;&#26512;&#24314;&#27169;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#30340;&#39044;&#27979;&#35823;&#24046;&#24402;&#22240;&#20026;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#38500;&#20102;&#38416;&#26126;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;Epsilon Scaling&#65292;&#20197;&#20943;&#36731;&#26333;&#20809;&#20559;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Epsilon Scaling&#36890;&#36807;&#32553;&#23567;&#32593;&#32476;&#36755;&#20986;&#65288;Epsilon&#65289;&#26126;&#30830;&#22320;&#23558;&#37319;&#26679;&#36712;&#36857;&#31227;&#36817;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21040;&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#12290;&#22312;&#21508;&#31181;&#25193;&#25955;&#26694;&#26550;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#20351;&#29992;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35774;&#22791;&#31471;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26368;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15308</link><description>&lt;p&gt;
&#20351;&#29992;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35774;&#22791;&#19978;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On-Device Learning with Binary Neural Networks. (arXiv:2308.15308v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#20351;&#29992;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35774;&#22791;&#31471;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26368;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#22312;&#20302;&#21151;&#32791;&#23884;&#20837;&#24335;CPU&#19978;&#37096;&#32626;&#26102;&#65292;&#20165;&#37096;&#20998;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21151;&#32791;&#12289;&#20869;&#23384;&#21644;&#35745;&#31639;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;(BNN)&#30340;&#39640;&#25928;&#24615;&#65292;BNN&#20351;&#29992;1&#20301;&#29992;&#20110;&#26435;&#37325;&#21644;&#28608;&#27963;&#20197;&#39640;&#25928;&#25191;&#34892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;CWR*&#30340;&#28151;&#21512;&#37327;&#21270;&#26041;&#27861;&#65288;&#19968;&#31181;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#21069;&#21521;&#20256;&#36882;&#21644;&#21453;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#20998;&#21035;&#32771;&#34385;&#65292;&#20197;&#22312;&#26799;&#24230;&#26356;&#26032;&#27493;&#39588;&#20013;&#20445;&#25345;&#26356;&#39640;&#30340;&#31934;&#24230;&#65292;&#24182;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#24310;&#36831;&#24320;&#38144;&#12290;&#36873;&#25321;&#20108;&#36827;&#21046;&#32593;&#32476;&#20316;&#20026;&#39592;&#24178;&#32593;&#32476;&#23545;&#20110;&#28385;&#36275;&#20302;&#21151;&#32791;&#35774;&#22791;&#30340;&#38480;&#21046;&#33267;&#20851;&#37325;&#35201;&#65292;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23581;&#35797;&#35777;&#26126;&#20351;&#29992;BNN&#36827;&#34892;&#35774;&#22791;&#19978;&#30340;&#23398;&#20064;&#12290;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing Continual Learning (CL) solutions only partially address the constraints on power, memory and computation of the deep learning models when deployed on low-power embedded CPUs. In this paper, we propose a CL solution that embraces the recent advancements in CL field and the efficiency of the Binary Neural Networks (BNN), that use 1-bit for weights and activations to efficiently execute deep learning models. We propose a hybrid quantization of CWR* (an effective CL approach) that considers differently forward and backward pass in order to retain more precision during gradient update step and at the same time minimizing the latency overhead. The choice of a binary network as backbone is essential to meet the constraints of low power devices and, to the best of authors' knowledge, this is the first attempt to prove on-device learning with BNN. The experimental validation carried out confirms the validity and the suitability of the proposed method.
&lt;/p&gt;</description></item><item><title>KGConv&#26159;&#19968;&#20010;&#22522;&#20110;Wikidata&#30340;&#22823;&#22411;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#27599;&#20010;&#23545;&#35805;&#37117;&#22522;&#20110;&#19968;&#20010;&#20107;&#23454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#20010;&#21464;&#20307;&#30340;&#38382;&#39064;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#30693;&#35782;&#23545;&#35805;&#38382;&#39064;&#29983;&#25104;&#21644;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.15298</link><description>&lt;p&gt;
KGConv&#65292;&#22522;&#20110;Wikidata&#30340;&#23545;&#35805;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
KGConv, a Conversational Corpus grounded in Wikidata. (arXiv:2308.15298v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15298
&lt;/p&gt;
&lt;p&gt;
KGConv&#26159;&#19968;&#20010;&#22522;&#20110;Wikidata&#30340;&#22823;&#22411;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#27599;&#20010;&#23545;&#35805;&#37117;&#22522;&#20110;&#19968;&#20010;&#20107;&#23454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#20010;&#21464;&#20307;&#30340;&#38382;&#39064;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#30693;&#35782;&#23545;&#35805;&#38382;&#39064;&#29983;&#25104;&#21644;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;KGConv&#65292;&#19968;&#20010;&#21253;&#21547;71k&#20010;&#23545;&#35805;&#30340;&#22823;&#22411;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#27599;&#20010;&#38382;&#39064;-&#22238;&#31572;&#23545;&#37117;&#22522;&#20110;Wikidata&#20013;&#30340;&#19968;&#20010;&#20107;&#23454;&#12290;&#27599;&#20010;&#23545;&#35805;&#24179;&#22343;&#21547;&#26377;8.6&#20010;&#38382;&#39064;&#65292;&#24182;&#20026;&#27599;&#20010;Wikidata&#20107;&#23454;&#25552;&#20379;&#22810;&#20010;&#21464;&#20307;(&#24179;&#22343;12&#20010;)&#65292;&#36825;&#20123;&#21464;&#20307;&#20351;&#29992;&#27169;&#26495;&#12289;&#20154;&#24037;&#27880;&#37322;&#12289;&#25163;&#24037;&#35268;&#21017;&#21644;&#38382;&#39064;&#37325;&#20889;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#12290;&#25105;&#20204;&#20026;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#25552;&#20379;&#20102;&#22522;&#32447;&#12290;KGConv&#36824;&#21487;&#29992;&#20110;&#20854;&#20182;&#29983;&#25104;&#21644;&#20998;&#26512;&#20219;&#21153;&#65292;&#20363;&#22914;&#20174;Wikidata&#19977;&#20803;&#32452;&#29983;&#25104;&#21333;&#36718;&#38382;&#39064;&#12289;&#38382;&#39064;&#37325;&#20889;&#12289;&#20174;&#23545;&#35805;&#25110;&#30693;&#35782;&#22270;&#20013;&#22238;&#31572;&#38382;&#39064;&#20197;&#21450;&#29983;&#25104;&#27979;&#39564;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present KGConv, a large, conversational corpus of 71k conversations where each question-answer pair is grounded in a Wikidata fact. Conversations contain on average 8.6 questions and for each Wikidata fact, we provide multiple variants (12 on average) of the corresponding question using templates, human annotations, hand-crafted rules and a question rewriting neural model. We provide baselines for the task of Knowledge-Based, Conversational Question Generation. KGConv can further be used for other generation and analysis tasks such as single-turn question generation from Wikidata triples, question rewriting, question answering from conversation or from knowledge graphs and quiz generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25104;&#21592;&#28508;&#22312;&#36317;&#31163;&#27169;&#22411;&#65288;HM-LDM&#65289;&#21644;&#26377;&#31526;&#21495;&#28151;&#21512;&#25104;&#21592;-&#28508;&#22312;&#36317;&#31163;&#27169;&#22411;&#65288;sHM-LDM&#65289;&#65292;&#36890;&#36807;&#25511;&#21046;&#28508;&#22312;&#31354;&#38388;&#30340;&#20307;&#31215;&#65292;&#25581;&#31034;&#20102;&#32593;&#32476;&#20013;&#30340;&#31038;&#21306;&#32467;&#26500;&#65292;&#24182;&#24341;&#23548;&#33410;&#28857;&#20043;&#38388;&#30340;&#38142;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.15293</link><description>&lt;p&gt;
&#19968;&#31181;&#28151;&#21512;&#25104;&#21592;&#28508;&#22312;&#36317;&#31163;&#27169;&#22411;&#29992;&#20110;&#26080;&#31526;&#21495;&#21644;&#26377;&#31526;&#21495;&#25972;&#25968;&#21152;&#26435;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Membership Latent Distance Model for Unsigned and Signed Integer Weighted Networks. (arXiv:2308.15293v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25104;&#21592;&#28508;&#22312;&#36317;&#31163;&#27169;&#22411;&#65288;HM-LDM&#65289;&#21644;&#26377;&#31526;&#21495;&#28151;&#21512;&#25104;&#21592;-&#28508;&#22312;&#36317;&#31163;&#27169;&#22411;&#65288;sHM-LDM&#65289;&#65292;&#36890;&#36807;&#25511;&#21046;&#28508;&#22312;&#31354;&#38388;&#30340;&#20307;&#31215;&#65292;&#25581;&#31034;&#20102;&#32593;&#32476;&#20013;&#30340;&#31038;&#21306;&#32467;&#26500;&#65292;&#24182;&#24341;&#23548;&#33410;&#28857;&#20043;&#38388;&#30340;&#38142;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#24050;&#25104;&#20026;&#36827;&#19968;&#27493;&#29702;&#35299;&#22797;&#26434;&#32593;&#32476;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#23884;&#20837;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#30340;&#25216;&#24039;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#25104;&#21592;-&#28508;&#22312;&#36317;&#31163;&#27169;&#22411;&#65288;HM-LDM&#65289;&#65292;&#36890;&#36807;&#25506;&#32034;&#22914;&#20309;&#23558;&#28508;&#22312;&#36317;&#31163;&#27169;&#22411;&#65288;LDM&#65289;&#38480;&#21046;&#22312;&#19968;&#20010;&#28508;&#22312;&#30340;&#21333;&#32431;&#24418;&#19978;&#12290;&#36890;&#36807;&#25511;&#21046;&#21333;&#32431;&#24418;&#35282;&#28857;&#30340;&#36793;&#38271;&#65292;&#21487;&#20197;&#31995;&#32479;&#22320;&#25511;&#21046;&#28508;&#22312;&#31354;&#38388;&#30340;&#20307;&#31215;&#12290;&#38543;&#30528;&#31354;&#38388;&#36234;&#26469;&#36234;&#21463;&#38480;&#65292;&#31038;&#21306;&#23558;&#34987;&#25581;&#31034;&#20986;&#26469;&#65292;&#24403;&#21333;&#32431;&#24418;&#20307;&#31215;&#36235;&#36817;&#20110;&#38646;&#26102;&#65292;&#20063;&#21487;&#20197;&#24674;&#22797;&#30828;&#25104;&#21592;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#19968;&#31181;&#26368;&#36817;&#38024;&#23545;&#26377;&#31526;&#21495;&#32593;&#32476;&#30340;&#20284;&#28982;&#20989;&#25968;&#20844;&#24335;&#65292;&#21033;&#29992;Skellam&#20998;&#24067;&#32771;&#34385;&#20102;&#26377;&#31526;&#21495;&#21152;&#26435;&#32593;&#32476;&#65292;&#24182;&#23558;HM-LDM&#25193;&#23637;&#21040;&#26377;&#31526;&#21495;&#28151;&#21512;&#25104;&#21592;-&#28508;&#22312;&#36317;&#31163;&#27169;&#22411;&#65288;sHM-LDM&#65289;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#24341;&#23548;&#20284;&#28982;&#20989;&#25968;&#26126;&#30830;&#22320;&#21560;&#24341;&#37027;&#20123;&#20855;&#26377;&#27491;&#38142;&#25509;&#30340;&#33410;&#28857;&#65292;&#24182;&#38459;&#27490;&#33410;&#28857;&#20855;&#26377;&#36127;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning (GRL) has become a prominent tool for furthering the understanding of complex networks providing tools for network embedding, link prediction, and node classification. In this paper, we propose the Hybrid Membership-Latent Distance Model (HM-LDM) by exploring how a Latent Distance Model (LDM) can be constrained to a latent simplex. By controlling the edge lengths of the corners of the simplex, the volume of the latent space can be systematically controlled. Thereby communities are revealed as the space becomes more constrained, with hard memberships being recovered as the simplex volume goes to zero. We further explore a recent likelihood formulation for signed networks utilizing the Skellam distribution to account for signed weighted networks and extend the HM-LDM to the signed Hybrid Membership-Latent Distance Model (sHM-LDM). Importantly, the induced likelihood function explicitly attracts nodes with positive links and deters nodes from having negative 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AutoDroid&#65292;&#19968;&#20010;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;Android&#24212;&#29992;&#31243;&#24207;&#19978;&#33258;&#21160;&#22788;&#29702;&#20219;&#24847;&#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#32467;&#21512;LLMs&#30340;&#24120;&#35782;&#30693;&#35782;&#21644;&#24212;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#23454;&#29616;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#21160;&#24577;&#20998;&#26512;&#26469;&#23454;&#29616;&#21151;&#33021;&#24847;&#35782;&#30340;UI&#34920;&#31034;&#26041;&#27861;&#21644;&#22522;&#20110;&#25506;&#32034;&#30340;&#20869;&#23384;&#27880;&#20837;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2308.15272</link><description>&lt;p&gt;
&#35753;LLM&#33021;&#22815;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#36827;&#34892;&#26234;&#33021;&#20219;&#21153;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
Empowering LLM to use Smartphone for Intelligent Task Automation. (arXiv:2308.15272v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AutoDroid&#65292;&#19968;&#20010;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;Android&#24212;&#29992;&#31243;&#24207;&#19978;&#33258;&#21160;&#22788;&#29702;&#20219;&#24847;&#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#32467;&#21512;LLMs&#30340;&#24120;&#35782;&#30693;&#35782;&#21644;&#24212;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#23454;&#29616;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#21160;&#24577;&#20998;&#26512;&#26469;&#23454;&#29616;&#21151;&#33021;&#24847;&#35782;&#30340;UI&#34920;&#31034;&#26041;&#27861;&#21644;&#22522;&#20110;&#25506;&#32034;&#30340;&#20869;&#23384;&#27880;&#20837;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#23454;&#29616;&#22522;&#20110;&#35821;&#38899;&#30340;&#20813;&#25552;&#29992;&#25143;&#19982;&#26234;&#33021;&#25163;&#26426;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#30001;&#20110;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#26377;&#38480;&#65292;&#20197;&#21450;&#24320;&#21457;&#20154;&#21592;&#25110;&#32456;&#31471;&#29992;&#25143;&#38656;&#35201;&#20184;&#20986;&#38750;&#24120;&#21162;&#21147;&#30340;&#25163;&#21160;&#24037;&#20316;&#32780;&#23548;&#33268;&#21487;&#25193;&#23637;&#24615;&#24046;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#36827;&#23637;&#28608;&#21457;&#20102;&#25105;&#20204;&#20174;&#27169;&#22411;&#20013;&#24515;&#21270;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#32479;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#20219;&#21153;&#20934;&#22791;&#12289;&#29702;&#35299;&#21644;&#25191;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoDroid&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#20219;&#20309;Android&#24212;&#29992;&#31243;&#24207;&#19978;&#26080;&#38656;&#25163;&#21160;&#24037;&#20316;&#22788;&#29702;&#20219;&#24847;&#20219;&#21153;&#30340;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#31995;&#32479;&#12290;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#21160;&#24577;&#20998;&#26512;&#23558;LLMs&#30340;&#24120;&#35782;&#30693;&#35782;&#19982;&#24212;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30456;&#32467;&#21512;&#12290;&#20027;&#35201;&#32452;&#20214;&#21253;&#25324;&#21151;&#33021;&#24847;&#35782;&#30340;UI&#34920;&#31034;&#26041;&#27861;&#65292;&#26725;&#25509;&#20102;UI&#21644;LLM&#65292;&#22522;&#20110;&#25506;&#32034;&#30340;&#20869;&#23384;&#27880;&#20837;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or end-users. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system that can handle arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#24314;&#39640;&#36136;&#37327;&#35821;&#38899;&#30340;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#23545;&#22810;&#26144;&#23556;&#38382;&#39064;&#21644;&#32454;&#33410;&#31934;&#28860;&#26469;&#26174;&#33879;&#25913;&#36827;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.15256</link><description>&lt;p&gt;
&#35753;&#22768;&#38899;&#23384;&#22312;&#65306;&#20174;&#26080;&#22768;&#35270;&#39057;&#20013;&#37325;&#24314;&#39640;&#36136;&#37327;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Let There Be Sound: Reconstructing High Quality Speech from Silent Videos. (arXiv:2308.15256v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#24314;&#39640;&#36136;&#37327;&#35821;&#38899;&#30340;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#23545;&#22810;&#26144;&#23556;&#38382;&#39064;&#21644;&#32454;&#33410;&#31934;&#28860;&#26469;&#26174;&#33879;&#25913;&#36827;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20165;&#36890;&#36807;&#21767;&#36816;&#21160;&#37325;&#24314;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#65292;&#20063;&#34987;&#31216;&#20026;&#21767;&#35821;&#36716;&#35821;&#38899;&#12290;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30001;&#20110;&#21516;&#24418;&#24322;&#38899;&#21644;&#22810;&#26679;&#21270;&#35821;&#38899;&#21464;&#21270;&#32780;&#36896;&#25104;&#30340;&#19968;&#23545;&#22810;&#26144;&#23556;&#65292;&#23548;&#33268;&#21457;&#38899;&#38169;&#35823;&#21644;&#36807;&#24230;&#24179;&#28369;&#30340;&#35821;&#38899;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#32531;&#35299;&#19968;&#23545;&#22810;&#26144;&#23556;&#38382;&#39064;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#65288;1&#65289;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#34920;&#31034;&#26469;&#28040;&#38500;&#21516;&#24418;&#24322;&#38899;&#65292;&#21644;&#65288;2&#65289;&#22768;&#23398;&#21464;&#24322;&#20449;&#24687;&#26469;&#24314;&#27169;&#22810;&#26679;&#21270;&#30340;&#35821;&#38899;&#39118;&#26684;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#27969;&#30340;&#21518;&#22788;&#29702;&#32593;&#32476;&#65292;&#25429;&#25417;&#21644;&#31934;&#28860;&#25152;&#29983;&#25104;&#35821;&#38899;&#30340;&#32454;&#33410;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#25509;&#36817;&#30495;&#23454;&#20154;&#31867;&#35821;&#38899;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this work is to reconstruct high quality speech from lip motions alone, a task also known as lip-to-speech. A key challenge of lip-to-speech systems is the one-to-many mapping caused by (1) the existence of homophenes and (2) multiple speech variations, resulting in a mispronounced and over-smoothed speech. In this paper, we propose a novel lip-to-speech system that significantly improves the generation quality by alleviating the one-to-many mapping problem from multiple perspectives. Specifically, we incorporate (1) self-supervised speech representations to disambiguate homophenes, and (2) acoustic variance information to model diverse speech styles. Additionally, to better solve the aforementioned problem, we employ a flow based post-net which captures and refines the details of the generated speech. We perform extensive experiments and demonstrate that our method achieves the generation quality close to that of real human utterance, outperforming existing methods in term
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#37325;&#33258;&#36866;&#24212;&#31354;&#38388;&#34701;&#21512;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#31354;&#38388;&#26469;&#34701;&#21512;&#21452;&#26354;&#12289;&#27431;&#20960;&#37324;&#24471;&#21644;&#29699;&#38754;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#20102;&#30693;&#35782;&#20256;&#25773;&#30340;&#23884;&#20837;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.15244</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#37325;&#33258;&#36866;&#24212;&#31354;&#38388;&#34701;&#21512;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based Multiple Adaptive Spaces Fusion for Recommendation. (arXiv:2308.15244v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15244
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#37325;&#33258;&#36866;&#24212;&#31354;&#38388;&#34701;&#21512;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#31354;&#38388;&#26469;&#34701;&#21512;&#21452;&#26354;&#12289;&#27431;&#20960;&#37324;&#24471;&#21644;&#29699;&#38754;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#20102;&#30693;&#35782;&#20256;&#25773;&#30340;&#23884;&#20837;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#30693;&#35782;&#22270;&#35889;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#36817;&#24180;&#26469;&#19968;&#20123;KG-enhanced&#25512;&#33616;&#26041;&#27861;&#23618;&#20986;&#19981;&#31351;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#26159;&#22522;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#35774;&#35745;&#30340;&#65292;&#27809;&#26377;&#32771;&#34385;&#26354;&#29575;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24040;&#22823;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#34920;&#29616;&#20986;&#39640;&#24230;&#38750;&#27431;&#20960;&#37324;&#24471;&#30340;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#37325;&#33258;&#36866;&#24212;&#31354;&#38388;&#34701;&#21512;&#25512;&#33616;&#26041;&#27861;&#65292;&#31216;&#20026;MCKG&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20165;&#37319;&#29992;&#29305;&#23450;&#27969;&#24418;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19982;&#21452;&#26354;&#12289;&#27431;&#20960;&#37324;&#24471;&#21644;&#29699;&#38754;&#31354;&#38388;&#20860;&#23481;&#30340;&#32479;&#19968;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#22312;&#27880;&#24847;&#21147;&#26041;&#24335;&#19979;&#65292;&#25105;&#20204;&#34701;&#21512;&#20102;&#22810;&#20010;&#32479;&#19968;&#31354;&#38388;&#65292;&#20197;&#33719;&#21462;&#26356;&#22909;&#30340;&#30693;&#35782;&#20256;&#25773;&#30340;&#39640;&#36136;&#37327;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#24863;&#30693;&#20248;&#21270;&#31574;&#30053;&#65292;&#20351;&#24471;&#25289;&#21644;&#25512;&#36807;&#31243;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#21452;&#26354;&#21644;&#29699;&#38754;&#31354;&#38388;&#30340;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Since Knowledge Graphs (KGs) contain rich semantic information, recently there has been an influx of KG-enhanced recommendation methods. Most of existing methods are entirely designed based on euclidean space without considering curvature. However, recent studies have revealed that a tremendous graph-structured data exhibits highly non-euclidean properties. Motivated by these observations, in this work, we propose a knowledge-based multiple adaptive spaces fusion method for recommendation, namely MCKG. Unlike existing methods that solely adopt a specific manifold, we introduce the unified space that is compatible with hyperbolic, euclidean and spherical spaces. Furthermore, we fuse the multiple unified spaces in an attention manner to obtain the high-quality embeddings for better knowledge propagation. In addition, we propose a geometry-aware optimization strategy which enables the pull and push processes benefited from both hyperbolic and spherical spaces. Specifically, in hyperbolic 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#36716;SQL&#30340;&#31649;&#36947;&#65292;&#20801;&#35768;&#20302;&#20195;&#30721;&#24179;&#21488;&#24320;&#21457;&#20154;&#21592;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26816;&#32034;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#22823;&#37327;&#25968;&#25454;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;NL&#27169;&#22411;&#26469;&#29983;&#25104;SQL&#65292;&#24182;&#20351;&#29992;&#21453;&#39304;&#24490;&#29615;&#19981;&#26029;&#20248;&#21270;&#27169;&#22411;&#12290;&#36890;&#36807;A/B&#27979;&#35797;&#65292;&#35266;&#23519;&#21040;&#29305;&#24615;&#37319;&#29992;&#29575;&#25552;&#39640;&#20102;240%&#65292;&#21442;&#19982;&#29575;&#25552;&#39640;&#20102;220%&#12290;</title><link>http://arxiv.org/abs/2308.15239</link><description>&lt;p&gt;
&#20302;&#20195;&#30721;&#24179;&#21488;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;SQL
&lt;/p&gt;
&lt;p&gt;
Natural language to SQL in low-code platforms. (arXiv:2308.15239v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#36716;SQL&#30340;&#31649;&#36947;&#65292;&#20801;&#35768;&#20302;&#20195;&#30721;&#24179;&#21488;&#24320;&#21457;&#20154;&#21592;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26816;&#32034;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#22823;&#37327;&#25968;&#25454;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;NL&#27169;&#22411;&#26469;&#29983;&#25104;SQL&#65292;&#24182;&#20351;&#29992;&#21453;&#39304;&#24490;&#29615;&#19981;&#26029;&#20248;&#21270;&#27169;&#22411;&#12290;&#36890;&#36807;A/B&#27979;&#35797;&#65292;&#35266;&#23519;&#21040;&#29305;&#24615;&#37319;&#29992;&#29575;&#25552;&#39640;&#20102;240%&#65292;&#21442;&#19982;&#29575;&#25552;&#39640;&#20102;220%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20195;&#30721;&#24179;&#21488;&#24320;&#21457;&#20154;&#21592;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#20351;&#29992;SQL&#26597;&#35810;&#20174;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31649;&#36947;&#65292;&#20801;&#35768;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#26469;&#26816;&#32034;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#12289;&#26631;&#27880;&#21644;&#39564;&#35777;&#20102;OutSystems&#29992;&#25143;&#26368;&#24120;&#20351;&#29992;&#30340;SQL&#26597;&#35810;&#25152;&#28041;&#21450;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#35757;&#32451;&#19968;&#20010;NL&#27169;&#22411;&#26469;&#29983;&#25104;SQL&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25972;&#20010;&#31649;&#36947;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#21453;&#39304;&#24490;&#29615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24555;&#36895;&#25910;&#38598;&#29983;&#20135;&#25968;&#25454;&#65292;&#24182;&#29992;&#23427;&#26469;&#37325;&#26032;&#35757;&#32451;&#25105;&#20204;&#30340;SQL&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#20247;&#21253;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;26k&#20010;NL&#21644;SQL&#23545;&#65292;&#24182;&#20174;&#29983;&#20135;&#25968;&#25454;&#20013;&#33719;&#24471;&#20102;&#39069;&#22806;&#30340;1k&#23545;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#25143;&#30028;&#38754;&#65292;&#20801;&#35768;&#24320;&#21457;&#20154;&#21592;&#22312;&#25552;&#31034;&#20013;&#36755;&#20837;NL&#26597;&#35810;&#65292;&#24182;&#33719;&#24471;&#23545;&#24212;SQL&#26597;&#35810;&#30340;&#29992;&#25143;&#21451;&#22909;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;A/B&#27979;&#35797;&#26469;&#27604;&#36739;&#29983;&#20135;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#27169;&#22411;&#65292;&#35266;&#23519;&#21040;&#29305;&#24615;&#37319;&#29992;&#29575;&#25552;&#39640;&#20102;240%&#65292;&#21442;&#19982;&#29575;&#25552;&#39640;&#20102;220%&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the developers' biggest challenges in low-code platforms is retrieving data from a database using SQL queries. Here, we propose a pipeline allowing developers to write natural language (NL) to retrieve data. In this study, we collect, label, and validate data covering the SQL queries most often performed by OutSystems users. We use that data to train a NL model that generates SQL. Alongside this, we describe the entire pipeline, which comprises a feedback loop that allows us to quickly collect production data and use it to retrain our SQL generation model. Using crowd-sourcing, we collect 26k NL and SQL pairs and obtain an additional 1k pairs from production data. Finally, we develop a UI that allows developers to input a NL query in a prompt and receive a user-friendly representation of the resulting SQL query. We use A/B testing to compare four different models in production and observe a 240% improvement in terms of adoption of the feature, 220% in terms of engagement rate, a
&lt;/p&gt;</description></item><item><title>PronounFlow&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#26657;&#20934;&#21477;&#23376;&#20013;&#30340;&#20195;&#35789;&#65292;&#20197;&#28040;&#38500;&#27495;&#20041;&#12290;&#36825;&#23545;&#20110;&#20351;&#26426;&#22120;&#20855;&#22791;&#24120;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.15235</link><description>&lt;p&gt;
PronounFlow:&#19968;&#31181;&#29992;&#20110;&#26657;&#20934;&#21477;&#23376;&#20013;&#20195;&#35789;&#30340;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PronounFlow: A Hybrid Approach for Calibrating Pronouns in Sentences. (arXiv:2308.15235v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15235
&lt;/p&gt;
&lt;p&gt;
PronounFlow&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#26657;&#20934;&#21477;&#23376;&#20013;&#30340;&#20195;&#35789;&#65292;&#20197;&#28040;&#38500;&#27495;&#20041;&#12290;&#36825;&#23545;&#20110;&#20351;&#26426;&#22120;&#20855;&#22791;&#24120;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#38405;&#20219;&#20309;&#19968;&#26412;&#20070;&#25110;&#21548;&#20219;&#20309;&#19968;&#39318;&#27468;&#35789;&#65292;&#20320;&#20250;&#36935;&#21040;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20250;&#38459;&#30861;&#29702;&#35299;&#30340;&#20195;&#35789;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26426;&#22120;&#26469;&#35828;&#12290;&#38543;&#30528;&#35748;&#30693;&#26426;&#22120;&#22312;&#25105;&#20204;&#29983;&#27963;&#20013;&#30340;&#26222;&#21450;&#65292;&#35768;&#22810;&#31995;&#32479;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#20197;&#35299;&#20915;&#21508;&#31181;&#25361;&#25112;&#19979;&#30340;&#20195;&#35789;&#27495;&#20041;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#35748;&#20026;&#33021;&#22815;&#28040;&#38500;&#21477;&#23376;&#20013;&#30340;&#20195;&#35789;&#27495;&#20041;&#30340;&#31995;&#32479;&#23558;&#26377;&#21161;&#20110;&#20351;&#26426;&#22120;&#20855;&#22791;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#24120;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#22312;&#29616;&#20195;&#33521;&#35821;&#20013;&#38754;&#20020;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#32570;&#20047;&#24615;&#21035;&#20195;&#35789;&#65292;&#20154;&#20204;&#35797;&#22270;&#36890;&#36807;&#20351;&#29992;&#30007;&#24615;&#12289;&#22899;&#24615;&#25110;&#22797;&#25968;&#26469;&#36991;&#20813;&#25972;&#20010;&#38382;&#39064;&#30340;&#20986;&#29616;&#12290;&#30001;&#20110;&#20154;&#31867;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#23436;&#25972;&#24847;&#20041;&#19978;&#30340;&#31995;&#32479;&#65292;&#37027;&#20040;&#24403;&#20070;&#38754;&#25991;&#26412;&#20013;&#30340;&#20195;&#35789;(&#22914;&#22797;&#25968;&#25110;&#20013;&#24615;&#20195;&#35789;)&#25351;&#30340;&#26159;&#24615;&#21035;&#19981;&#19968;&#23450;&#24050;&#30693;&#30340;&#26410;&#25351;&#23450;&#23454;&#20307;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#21602;&#65311;
&lt;/p&gt;
&lt;p&gt;
Flip through any book or listen to any song lyrics, and you will come across pronouns that, in certain cases, can hinder meaning comprehension, especially for machines. As the role of having cognitive machines becomes pervasive in our lives, numerous systems have been developed to resolve pronouns under various challenges. Commensurate with this, it is believed that having systems able to disambiguate pronouns in sentences will help towards the endowment of machines with commonsense and reasoning abilities like those found in humans. However, one problem these systems face with modern English is the lack of gender pronouns, where people try to alternate by using masculine, feminine, or plural to avoid the whole issue. Since humanity aims to the building of systems in the full-bodied sense we usually reserve for people, what happens when pronouns in written text, like plural or epicene ones, refer to unspecified entities whose gender is not necessarily known? Wouldn't that put extra bar
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#32534;&#30721;&#26469;&#20943;&#23569;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27495;&#35270;&#65292;&#20174;&#32780;&#20026;&#20197;&#21069;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#25552;&#20379;&#20844;&#24179;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2308.15230</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20026;&#20197;&#21069;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#25552;&#20379;&#20844;&#24179;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Providing Previously Unseen Users Fair Recommendations Using Variational Autoencoders. (arXiv:2308.15230v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#32534;&#30721;&#26469;&#20943;&#23569;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27495;&#35270;&#65292;&#20174;&#32780;&#20026;&#20197;&#21069;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#25552;&#20379;&#20844;&#24179;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20851;&#20110;&#20844;&#24179;&#24615;&#30340;&#26032;&#23450;&#20041;&#35201;&#27714;&#27169;&#22411;&#23545;&#29992;&#25143;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#19981;&#21487;&#35265;&#65292;&#20363;&#22914;&#65292;&#29992;&#25143;&#30340;&#24615;&#21035;&#25110;&#24180;&#40836;&#19981;&#24212;&#24433;&#21709;&#27169;&#22411;&#12290;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#29305;&#21035;&#23481;&#26131;&#36890;&#36807;&#20854;&#26174;&#24335;&#30340;&#29992;&#25143;&#20851;&#27880;&#21644;&#29992;&#25143;&#24314;&#27169;&#26469;&#36829;&#21453;&#36825;&#20010;&#23450;&#20041;&#12290;&#26174;&#24335;&#30340;&#29992;&#25143;&#24314;&#27169;&#20063;&#26159;&#35768;&#22810;&#25512;&#33616;&#31995;&#32479;&#26080;&#27861;&#20026;&#20197;&#21069;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#25552;&#20379;&#25512;&#33616;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38480;&#21046;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#32534;&#30721;&#30340;&#26032;&#26041;&#27861;&#26469;&#20943;&#23569;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27495;&#35270;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#35780;&#20272;&#20013;&#20026;&#26410;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#29992;&#25143;&#25552;&#20379;&#20844;&#24179;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emerging definition of fairness in machine learning requires that models are oblivious to demographic user information, e.g., a user's gender or age should not influence the model. Personalized recommender systems are particularly prone to violating this definition through their explicit user focus and user modelling. Explicit user modelling is also an aspect that makes many recommender systems incapable of providing hitherto unseen users with recommendations. We propose novel approaches for mitigating discrimination in Variational Autoencoder-based recommender systems by limiting the encoding of demographic information. The approaches are capable of, and evaluated on, providing users that are not represented in the training data with fair recommendations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CLIPTrans&#65292;&#23427;&#36890;&#36807;&#31616;&#21333;&#22320;&#36866;&#24212;&#29420;&#31435;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20013;&#35270;&#35273;&#30693;&#35782;&#30340;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2308.15226</link><description>&lt;p&gt;
CLIPTrans&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36716;&#31227;&#35270;&#35273;&#30693;&#35782;&#36827;&#34892;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation. (arXiv:2308.15226v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CLIPTrans&#65292;&#23427;&#36890;&#36807;&#31616;&#21333;&#22320;&#36866;&#24212;&#29420;&#31435;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20013;&#35270;&#35273;&#30693;&#35782;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24320;&#21457;&#22686;&#24378;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#31995;&#32479;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#30340;&#20852;&#36259;&#36880;&#28176;&#22686;&#38271;&#12290;&#36825;&#19968;&#38382;&#39064;&#35774;&#32622;&#28041;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22270;&#20687;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#65292;&#24182;&#19988;&#26368;&#36817;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#28040;&#38500;&#23427;&#20204;&#30340;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#22312;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#24378;&#22823;&#30340;MMT&#27169;&#22411;&#26102;&#38754;&#20020;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#22810;&#35821;&#35328;&#35270;&#35273;&#35821;&#35328;&#25968;&#25454;&#30340;&#26631;&#27880;&#31232;&#32570;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#38024;&#23545;NMT&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#22823;&#37327;&#28044;&#29616;&#65292;&#20027;&#35201;&#38024;&#23545;&#33521;&#25991;&#65292;&#23427;&#20204;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;MMT&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#20026;&#29983;&#25104;&#20219;&#21153;&#25552;&#20379;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLIPTrans&#65292;&#23427;&#19981;&#20687;&#35774;&#35745;&#22797;&#26434;&#30340;MMT&#27169;&#22359;&#65292;&#32780;&#26159;&#31616;&#21333;&#22320;&#36866;&#24212;&#29420;&#31435;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
There has been a growing interest in developing multimodal machine translation (MMT) systems that enhance neural machine translation (NMT) with visual knowledge. This problem setup involves using images as auxiliary information during training, and more recently, eliminating their use during inference. Towards this end, previous works face a challenge in training powerful MMT models from scratch due to the scarcity of annotated multilingual vision-language data, especially for low-resource languages. Simultaneously, there has been an influx of multilingual pre-trained models for NMT and multimodal pre-trained models for vision-language tasks, primarily in English, which have shown exceptional generalisation ability. However, these are not directly applicable to MMT since they do not provide aligned multimodal multilingual features for generative tasks. To alleviate this issue, instead of designing complex modules for MMT, we propose CLIPTrans, which simply adapts the independently pre-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;&#20915;&#31574;&#36807;&#31243;&#25968;&#25454;&#21644;&#27169;&#22411;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#20915;&#31574;&#36807;&#31243;&#21644;&#24314;&#31435;&#20915;&#31574;&#28436;&#21464;&#27169;&#22411;&#65292;&#21487;&#20197;&#25581;&#31034;&#28508;&#22312;&#30340;&#20559;&#22909;&#65292;&#21516;&#26102;&#36861;&#36394;&#20915;&#31574;&#36807;&#31243;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#20379;&#37325;&#35201;&#20449;&#24687;&#65292;&#20174;&#32780;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15225</link><description>&lt;p&gt;
&#20174;DDMs&#21040;DNNs&#65306;&#21033;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#26469;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
From DDMs to DNNs: Using process data and models of decision-making to improve human-AI interactions. (arXiv:2308.15225v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;&#20915;&#31574;&#36807;&#31243;&#25968;&#25454;&#21644;&#27169;&#22411;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#20915;&#31574;&#36807;&#31243;&#21644;&#24314;&#31435;&#20915;&#31574;&#28436;&#21464;&#27169;&#22411;&#65292;&#21487;&#20197;&#25581;&#31034;&#28508;&#22312;&#30340;&#20559;&#22909;&#65292;&#21516;&#26102;&#36861;&#36394;&#20915;&#31574;&#36807;&#31243;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#20379;&#37325;&#35201;&#20449;&#24687;&#65292;&#20174;&#32780;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#23478;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#23478;&#24050;&#32463;&#35748;&#35782;&#21040;&#35814;&#32454;&#25551;&#36848;&#20915;&#31574;&#36807;&#31243;&#21644;&#24314;&#31435;&#20915;&#31574;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#27169;&#22411;&#30340;&#20215;&#20540;&#12290;&#20363;&#22914;&#65292;&#20915;&#31574;&#25152;&#38656;&#30340;&#26102;&#38388;&#21487;&#20197;&#25581;&#31034;&#19968;&#20010;&#20010;&#20307;&#30495;&#27491;&#30340;&#28508;&#22312;&#20559;&#22909;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20915;&#31574;&#26412;&#36523;&#12290;&#31867;&#20284;&#22320;&#65292;&#36861;&#36394;&#20915;&#31574;&#36807;&#31243;&#30340;&#25968;&#25454;&#65292;&#22914;&#30524;&#21160;&#25110;&#31070;&#32463;&#35760;&#24405;&#65292;&#21253;&#21547;&#20102;&#20851;&#38190;&#30340;&#20449;&#24687;&#65292;&#21363;&#20351;&#27809;&#26377;&#36798;&#25104;&#20915;&#31574;&#20063;&#21487;&#20197;&#34987;&#21033;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35748;&#20026;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24212;&#26356;&#21152;&#20851;&#27880;&#20915;&#31574;&#22914;&#20309;&#38543;&#26102;&#38388;&#28436;&#21464;&#20197;&#21450;&#22914;&#20309;&#34701;&#20837;&#30456;&#20851;&#30340;&#36807;&#31243;&#25968;&#25454;&#26469;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20132;&#20114;&#20013;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#38750;&#24120;&#25104;&#29087;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35748;&#20026;&#20915;&#31574;&#26159;&#20174;&#26434;&#38899;&#32047;&#31215;&#30340;&#35777;&#25454;&#20013;&#20135;&#29983;&#30340;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#24515;&#29702;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#32463;&#27982;&#23398;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decades, cognitive neuroscientists and behavioral economists have recognized the value of describing the process of decision making in detail and modeling the emergence of decisions over time. For example, the time it takes to decide can reveal more about an agents true hidden preferences than only the decision itself. Similarly, data that track the ongoing decision process such as eye movements or neural recordings contain critical information that can be exploited, even if no decision is made. Here, we argue that artificial intelligence (AI) research would benefit from a stronger focus on insights about how decisions emerge over time and incorporate related process data to improve AI predictions in general and human-AI interactions in particular. First, we introduce a highly established computational framework that assumes decisions to emerge from the noisy accumulation of evidence, and we present related empirical work in psychology, neuroscience, and economics. Next, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#23558;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;&#12289;&#33080;&#37096;&#34920;&#24773;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;GPT-3.5&#27169;&#22411;&#26469;&#29983;&#25104;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#65292;&#20197;&#25552;&#20379;&#20449;&#24687;&#24182;&#19982;&#35775;&#23458;&#36827;&#34892;&#33258;&#28982;&#20132;&#27969;&#12290;</title><link>http://arxiv.org/abs/2308.15214</link><description>&lt;p&gt;
FurChat: &#20351;&#29992;LLMs&#30340;&#20855;&#26377;&#33080;&#37096;&#34920;&#24773;&#30340;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#32467;&#21512;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain Dialogue with Facial Expressions. (arXiv:2308.15214v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#23558;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;&#12289;&#33080;&#37096;&#34920;&#24773;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;GPT-3.5&#27169;&#22411;&#26469;&#29983;&#25104;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#65292;&#20197;&#25552;&#20379;&#20449;&#24687;&#24182;&#19982;&#35775;&#23458;&#36827;&#34892;&#33258;&#28982;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#21487;&#20197;&#20316;&#20026;&#25509;&#24453;&#21592;&#65292;&#29983;&#25104;&#32467;&#21512;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;&#20197;&#21450;&#33080;&#37096;&#34920;&#24773;&#30340;&#28151;&#21512;&#23545;&#35805;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#24320;&#21457;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#23558;&#35813;&#31995;&#32479;&#37096;&#32626;&#21040;&#20102;&#19968;&#20010;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;Furhat&#26426;&#22120;&#20154;&#19978;&#65292;&#22312;&#20114;&#21160;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#21475;&#22836;&#21644;&#38750;&#35821;&#35328;&#25552;&#31034;&#12290;&#35813;&#31995;&#32479;&#19987;&#38376;&#20026;&#22269;&#23478;&#26426;&#22120;&#20154;&#23454;&#39564;&#23460;&#35774;&#35745;&#65292;&#36890;&#36807;&#33258;&#28982;&#23545;&#35805;&#19982;&#35775;&#23458;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#21521;&#20182;&#20204;&#25552;&#20379;&#26377;&#20851;&#35774;&#26045;&#12289;&#30740;&#31350;&#12289;&#26032;&#38395;&#12289;&#21363;&#23558;&#20030;&#34892;&#30340;&#27963;&#21160;&#31561;&#26041;&#38754;&#30340;&#20449;&#24687;&#12290;&#31995;&#32479;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;GPT-3.5&#27169;&#22411;&#26681;&#25454;&#25552;&#31034;&#29983;&#25104;&#36825;&#20123;&#20449;&#24687;&#65292;&#21516;&#26102;&#29983;&#25104;&#39046;&#22495;&#36890;&#29992;&#30340;&#23545;&#35805;&#21644;&#38754;&#37096;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate an embodied conversational agent that can function as a receptionist and generate a mixture of open and closed-domain dialogue along with facial expressions, by using a large language model (LLM) to develop an engaging conversation. We deployed the system onto a Furhat robot, which is highly expressive and capable of using both verbal and nonverbal cues during interaction. The system was designed specifically for the National Robotarium to interact with visitors through natural conversations, providing them with information about the facilities, research, news, upcoming events, etc. The system utilises the state-of-the-art GPT-3.5 model to generate such information along with domain-general conversations and facial expressions based on prompt engineering.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19977;&#31181;&#35821;&#35328;&#23545;&#30340;&#20116;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#23545;&#20849;&#20139;&#35789;&#27719;&#20316;&#20026;&#20195;&#30721;&#20999;&#25442;&#30340;&#35302;&#21457;&#22120;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#32034;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20849;&#20139;&#35789;&#27719;&#30830;&#23454;&#20250;&#35302;&#21457;&#20195;&#30721;&#20999;&#25442;&#65292;&#20999;&#25442;&#30340;&#20542;&#21521;&#21462;&#20915;&#20110;&#35302;&#21457;&#22120;&#19982;&#20999;&#25442;&#28857;&#30340;&#36317;&#31163;&#21644;&#20301;&#32622;&#65292;&#32780;&#19981;&#21462;&#20915;&#20110;&#35302;&#21457;&#35789;&#30340;&#35789;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.15209</link><description>&lt;p&gt;
&#20849;&#20139;&#35789;&#27719;&#20316;&#20026;&#20195;&#30721;&#20999;&#25442;&#30340;&#35302;&#21457;&#22120;
&lt;/p&gt;
&lt;p&gt;
Shared Lexical Items as Triggers of Code Switching. (arXiv:2308.15209v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19977;&#31181;&#35821;&#35328;&#23545;&#30340;&#20116;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#23545;&#20849;&#20139;&#35789;&#27719;&#20316;&#20026;&#20195;&#30721;&#20999;&#25442;&#30340;&#35302;&#21457;&#22120;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#32034;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20849;&#20139;&#35789;&#27719;&#30830;&#23454;&#20250;&#35302;&#21457;&#20195;&#30721;&#20999;&#25442;&#65292;&#20999;&#25442;&#30340;&#20542;&#21521;&#21462;&#20915;&#20110;&#35302;&#21457;&#22120;&#19982;&#20999;&#25442;&#28857;&#30340;&#36317;&#31163;&#21644;&#20301;&#32622;&#65292;&#32780;&#19981;&#21462;&#20915;&#20110;&#35302;&#21457;&#35789;&#30340;&#35789;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20160;&#20040;&#21452;&#35821;&#32773;&#35201;&#36827;&#34892;&#20195;&#30721;&#20999;&#25442;&#65288;&#28151;&#21512;&#20004;&#31181;&#35821;&#35328;&#65289;&#65311;&#22312;&#35299;&#37322;&#36825;&#31181;&#33258;&#28982;&#21644;&#26222;&#36941;&#29616;&#35937;&#30340;&#20960;&#31181;&#29702;&#35770;&#20013;&#65292;&#35302;&#21457;&#20551;&#35828;&#23558;&#20195;&#30721;&#20999;&#25442;&#19982;&#20999;&#25442;&#28857;&#38468;&#36817;&#30340;&#35789;&#27719;&#35302;&#21457;&#22120;&#65288;&#29305;&#21035;&#26159;&#21516;&#28304;&#35789;&#21644;&#19987;&#26377;&#21517;&#35789;&#65289;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#22522;&#20110;&#19977;&#31181;&#35821;&#35328;&#23545;&#30340;&#20116;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21453;&#26144;&#20102;&#21475;&#35821;&#21644;&#20070;&#38754;&#21452;&#35821;&#20132;&#27969;&#65292;&#23545;&#35302;&#21457;&#20551;&#35828;&#36827;&#34892;&#20102;&#26356;&#20840;&#38754;&#12289;&#26356;&#32454;&#33268;&#12289;&#26356;&#31934;&#32454;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#34987;&#35748;&#20026;&#21516;&#26102;&#23384;&#22312;&#20110;&#20004;&#31181;&#35821;&#35328;&#30340;&#20849;&#20139;&#24515;&#29702;&#35789;&#27719;&#20013;&#30340;&#35789;&#30830;&#23454;&#35302;&#21457;&#20195;&#30721;&#20999;&#25442;&#65307;&#20999;&#25442;&#30340;&#20542;&#21521;&#21462;&#20915;&#20110;&#35302;&#21457;&#22120;&#19982;&#20999;&#25442;&#28857;&#30340;&#36317;&#31163;&#65307;&#20197;&#21450;&#35302;&#21457;&#22120;&#26159;&#22312;&#20999;&#25442;&#20043;&#21069;&#36824;&#26159;&#20043;&#21518;&#65307;&#20294;&#19981;&#21462;&#20915;&#20110;&#35302;&#21457;&#35789;&#30340;&#35789;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#35789;&#27719;&#35302;&#21457;&#22120;&#19982;&#20195;&#30721;&#20999;&#25442;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#12289;&#31283;&#23450;&#12289;&#20197;&#35777;&#25454;&#20026;&#22522;&#30784;&#30340;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why do bilingual speakers code-switch (mix their two languages)? Among the several theories that attempt to explain this natural and ubiquitous phenomenon, the Triggering Hypothesis relates code-switching to the presence of lexical triggers, specifically cognates and proper names, adjacent to the switch point. We provide a fuller, more nuanced and refined exploration of the triggering hypothesis, based on five large datasets in three language pairs, reflecting both spoken and written bilingual interactions. Our results show that words that are assumed to reside in a mental lexicon shared by both languages indeed trigger code-switching; that the tendency to switch depends on the distance of the trigger from the switch point; and on whether the trigger precedes or succeeds the switch; but not on the etymology of the trigger words. We thus provide strong, robust, evidence-based confirmation to several hypotheses on the relationships between lexical triggers and code-switching.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;LLM-Mob&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#20998;&#26512;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#21382;&#21490;&#20572;&#30041;&#21644;&#19978;&#19979;&#25991;&#20572;&#30041;&#30340;&#27010;&#24565;&#26469;&#25429;&#25417;&#38271;&#26399;&#21644;&#30701;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#26102;&#24577;&#24863;&#30693;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.15197</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#21435;&#21738;&#37324;&#65311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Where Would I Go Next? Large Language Models as Human Mobility Predictors. (arXiv:2308.15197v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;LLM-Mob&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#20998;&#26512;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#21382;&#21490;&#20572;&#30041;&#21644;&#19978;&#19979;&#25991;&#20572;&#30041;&#30340;&#27010;&#24565;&#26469;&#25429;&#25417;&#38271;&#26399;&#21644;&#30701;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#26102;&#24577;&#24863;&#30693;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#27969;&#34892;&#30149;&#24314;&#27169;&#12289;&#20132;&#36890;&#35268;&#21010;&#21644;&#24212;&#24613;&#21709;&#24212;&#12290;&#30001;&#20110;&#31227;&#21160;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#21644;&#20154;&#20204;&#26085;&#24120;&#27963;&#21160;&#30340;&#38543;&#26426;&#24615;&#65292;&#23454;&#29616;&#23545;&#20154;&#20204;&#20301;&#32622;&#30340;&#31934;&#30830;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#34429;&#28982;&#26368;&#36817;&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#19982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#20154;&#31867;&#31227;&#21160;&#30740;&#31350;&#20013;&#30340;&#36866;&#29992;&#24615;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;LLM-Mob&#65292;&#23427;&#21033;&#29992;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#26469;&#20998;&#26512;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21382;&#21490;&#20572;&#30041;&#21644;&#19978;&#19979;&#25991;&#20572;&#30041;&#30340;&#27010;&#24565;&#65292;&#20197;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#20013;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#20449;&#24687;&#26469;&#23454;&#29616;&#26102;&#24577;&#24863;&#30693;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate human mobility prediction underpins many important applications across a variety of domains, including epidemic modelling, transport planning, and emergency responses. Due to the sparsity of mobility data and the stochastic nature of people's daily activities, achieving precise predictions of people's locations remains a challenge. While recently developed large language models (LLMs) have demonstrated superior performance across numerous language-related tasks, their applicability to human mobility studies remains unexplored. Addressing this gap, this article delves into the potential of LLMs for human mobility prediction tasks. We introduce a novel method, LLM-Mob, which leverages the language understanding and reasoning capabilities of LLMs for analysing human mobility data. We present concepts of historical stays and context stays to capture both long-term and short-term dependencies in human movement and enable time-aware prediction by using time information of the predic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#22863;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#25552;&#21319;&#24369;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#23545;&#21453;&#20107;&#23454;&#23454;&#20363;&#30340;&#26368;&#23567;&#21270;&#12289;&#21487;&#25805;&#20316;&#24615;&#12289;&#31283;&#23450;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#21512;&#29702;&#24615;&#21644;&#36776;&#21035;&#21147;&#30340;&#20840;&#35206;&#30422;&#12290;</title><link>http://arxiv.org/abs/2308.15194</link><description>&lt;p&gt;
&#21512;&#22863;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Ensemble of Counterfactual Explainers. (arXiv:2308.15194v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15194
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#22863;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#25552;&#21319;&#24369;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#23545;&#21453;&#20107;&#23454;&#23454;&#20363;&#30340;&#26368;&#23567;&#21270;&#12289;&#21487;&#25805;&#20316;&#24615;&#12289;&#31283;&#23450;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#21512;&#29702;&#24615;&#21644;&#36776;&#21035;&#21147;&#30340;&#20840;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;&#65292;&#27599;&#31181;&#35299;&#37322;&#22120;&#37117;&#20851;&#27880;&#21453;&#20107;&#23454;&#23454;&#20363;&#30340;&#19968;&#20123;&#21487;&#21462;&#29305;&#24615;&#65306;&#26368;&#23567;&#21270;&#12289;&#21487;&#25805;&#20316;&#24615;&#12289;&#31283;&#23450;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#21512;&#29702;&#24615;&#12289;&#36776;&#21035;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#22863;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;&#65292;&#23427;&#33021;&#22815;&#22686;&#24378;&#24369;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#24369;&#35299;&#37322;&#22120;&#20165;&#25552;&#20379;&#36825;&#20123;&#29305;&#24615;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#28085;&#30422;&#25152;&#26377;&#29305;&#24615;&#12290;&#35813;&#21512;&#22863;&#35299;&#37322;&#22120;&#22312;&#19968;&#20123;&#23454;&#20363;&#21644;&#29305;&#24449;&#30340;&#26679;&#26412;&#19978;&#36816;&#34892;&#24369;&#35299;&#37322;&#22120;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#22810;&#26679;&#24615;&#39537;&#21160;&#30340;&#36873;&#25321;&#20989;&#25968;&#26469;&#21512;&#24182;&#20854;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#23553;&#35013;&#26041;&#27861;&#65292;&#20063;&#26159;&#25968;&#25454;&#26080;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In eXplainable Artificial Intelligence (XAI), several counterfactual explainers have been proposed, each focusing on some desirable properties of counterfactual instances: minimality, actionability, stability, diversity, plausibility, discriminative power. We propose an ensemble of counterfactual explainers that boosts weak explainers, which provide only a subset of such properties, to a powerful method covering all of them. The ensemble runs weak explainers on a sample of instances and of features, and it combines their results by exploiting a diversity-driven selection function. The method is model-agnostic and, through a wrapping approach based on autoencoders, it is also data-agnostic.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#21487;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#21592;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25552;&#20379;&#24515;&#29702;&#21672;&#35810;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#19987;&#19994;&#21672;&#35810;&#24072;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15192</link><description>&lt;p&gt;
&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#24515;&#29702;&#21672;&#35810;&#65306;&#38754;&#21521;&#38750;&#19987;&#19994;&#20154;&#21592;&#30340;&#22810;&#26041;&#38754;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Enhancing Psychological Counseling with Large Language Model: A Multifaceted Decision-Support System for Non-Professionals. (arXiv:2308.15192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#21487;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#21592;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25552;&#20379;&#24515;&#29702;&#21672;&#35810;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#19987;&#19994;&#21672;&#35810;&#24072;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#31038;&#20132;&#23186;&#20307;&#30340;&#29615;&#22659;&#19979;&#65292;&#22823;&#37327;&#29992;&#25143;&#34920;&#36798;&#36127;&#38754;&#24773;&#32490;&#65292;&#20854;&#20013;&#19968;&#20123;&#34920;&#29616;&#20026;&#24378;&#28872;&#30340;&#33258;&#26432;&#20542;&#21521;&#12290;&#36825;&#31181;&#24773;&#20917;&#20984;&#26174;&#20102;&#23545;&#35757;&#32451;&#26377;&#32032;&#30340;&#24515;&#29702;&#21672;&#35810;&#24072;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#20182;&#20204;&#21487;&#20197;&#23454;&#26045;&#26377;&#25928;&#30340;&#24515;&#29702;&#24178;&#39044;&#12290;&#28982;&#32780;&#65292;&#22521;&#20859;&#36825;&#20123;&#19987;&#19994;&#20154;&#21592;&#24120;&#24120;&#26159;&#19968;&#39033;&#24517;&#35201;&#20294;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#21160;&#21592;&#38750;&#19987;&#19994;&#20154;&#21592;&#25110;&#24535;&#24895;&#32773;&#22312;&#36825;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20026;&#27492;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#20197;&#20805;&#20998;&#36741;&#21161;&#38750;&#19987;&#19994;&#20154;&#21592;&#22312;&#22312;&#32447;&#29992;&#25143;&#20132;&#27969;&#20013;&#25552;&#20379;&#24515;&#29702;&#24178;&#39044;&#12290;&#35813;&#26694;&#26550;&#20351;&#24471;&#21512;&#29702;&#21033;&#29992;&#38750;&#19987;&#19994;&#21672;&#35810;&#24072;&#30340;&#21147;&#37327;&#21464;&#24471;&#21487;&#34892;&#12290;&#36827;&#34892;&#20102;&#19968;&#39033;&#32508;&#21512;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the contemporary landscape of social media, an alarming number of users express negative emotions, some of which manifest as strong suicidal intentions. This situation underscores a profound need for trained psychological counselors who can enact effective mental interventions. However, the development of these professionals is often an imperative but time-consuming task. Consequently, the mobilization of non-professionals or volunteers in this capacity emerges as a pressing concern. Leveraging the capabilities of artificial intelligence, and in particular, the recent advances in large language models, offers a viable solution to this challenge. This paper introduces a novel model constructed on the foundation of large language models to fully assist non-professionals in providing psychological interventions on online user discourses. This framework makes it plausible to harness the power of non-professional counselors in a meaningful way. A comprehensive study was conducted involvi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#23436;&#20840;&#21487;&#35266;&#23519;&#30340;&#38750;&#30830;&#23450;&#24615;&#35268;&#21010;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#22312;&#26377;&#38480;&#36712;&#36857;&#19978;&#34920;&#36798;&#30446;&#26631;&#30340;&#26368;&#20339;&#21162;&#21147;&#31574;&#30053;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#28216;&#25103;&#35770;&#25216;&#26415;&#26469;&#32508;&#21512;&#36825;&#20123;&#31574;&#30053;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#20102;&#38750;&#30830;&#23450;&#24615;&#35268;&#21010;&#39046;&#22495;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#22522;&#20110;&#37325;&#26032;&#34920;&#36798;&#35268;&#21010;&#39046;&#22495;&#30340;&#30452;&#25509;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15188</link><description>&lt;p&gt;
&#38750;&#30830;&#23450;&#24615;&#35268;&#21010;&#39046;&#22495;&#20013;&#30340;&#26368;&#20339;&#21162;&#21147;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LTLf Best-Effort Synthesis in Nondeterministic Planning Domains. (arXiv:2308.15188v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#23436;&#20840;&#21487;&#35266;&#23519;&#30340;&#38750;&#30830;&#23450;&#24615;&#35268;&#21010;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#22312;&#26377;&#38480;&#36712;&#36857;&#19978;&#34920;&#36798;&#30446;&#26631;&#30340;&#26368;&#20339;&#21162;&#21147;&#31574;&#30053;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#28216;&#25103;&#35770;&#25216;&#26415;&#26469;&#32508;&#21512;&#36825;&#20123;&#31574;&#30053;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#20102;&#38750;&#30830;&#23450;&#24615;&#35268;&#21010;&#39046;&#22495;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#22522;&#20110;&#37325;&#26032;&#34920;&#36798;&#35268;&#21010;&#39046;&#22495;&#30340;&#30452;&#25509;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23436;&#20840;&#21487;&#35266;&#23519;&#30340;&#38750;&#30830;&#23450;&#24615;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#22312;&#26377;&#38480;&#36712;&#36857;&#19978;&#34920;&#36798;&#30446;&#26631;&#30340;&#26368;&#20339;&#21162;&#21147;&#31574;&#30053;&#65288;&#20063;&#31216;&#20026;&#35745;&#21010;&#65289;&#12290;&#26368;&#20339;&#21162;&#21147;&#31574;&#30053;&#30340;&#27010;&#24565;&#34987;&#24341;&#20837;&#26469;&#24212;&#23545;&#24403;&#19981;&#23384;&#22312;&#19968;&#20010;&#20195;&#29702;&#31574;&#30053;&#33021;&#28385;&#36275;&#27599;&#20010;&#21487;&#33021;&#30340;&#38750;&#30830;&#23450;&#24615;&#29615;&#22659;&#21453;&#24212;&#30340;&#24773;&#20917;&#12290;&#36825;&#26679;&#30340;&#31574;&#30053;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30446;&#26631;&#65292;&#24182;&#22312;&#21542;&#21017;&#30340;&#24773;&#20917;&#19979;&#23613;&#21147;&#32780;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38750;&#30830;&#23450;&#24615;&#35268;&#21010;&#39046;&#22495;&#29305;&#24449;&#32508;&#21512;&#26368;&#20339;&#21162;&#21147;&#31574;&#30053;&#30340;&#21338;&#24328;&#35770;&#25216;&#26415;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#23558;&#35268;&#21010;&#39046;&#22495;&#37325;&#26032;&#34920;&#36798;&#20026;&#36890;&#29992;&#29615;&#22659;&#35268;&#33539;&#30340;&#30452;&#25509;&#26368;&#20339;&#21162;&#21147;&#32508;&#21512;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study best-effort strategies (aka plans) in fully observable nondeterministic domains (FOND) for goals expressed in Linear Temporal Logic on Finite Traces (LTLf). The notion of best-effort strategy has been introduced to also deal with the scenario when no agent strategy exists that fulfills the goal against every possible nondeterministic environment reaction. Such strategies fulfill the goal if possible, and do their best to do so otherwise. We present a game-theoretic technique for synthesizing best-effort strategies that exploit the specificity of nondeterministic planning domains. We formally show its correctness and demonstrate its effectiveness experimentally, exhibiting a much greater scalability with respect to a direct best-effort synthesis approach based on re-expressing the planning domain as generic environment specifications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LTLf&#32508;&#21512;&#22312;&#21487;&#36798;&#24615;&#21644;&#23433;&#20840;&#24615;&#23646;&#24615;&#30340;&#29615;&#22659;&#35268;&#33539;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#32508;&#21512;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#24773;&#20917;&#26159;&#39318;&#27425;&#30740;&#31350;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.15184</link><description>&lt;p&gt;
LTLf&#32508;&#21512;&#22312;&#29615;&#22659;&#35268;&#33539;&#19979;&#30340;&#21487;&#36798;&#24615;&#21644;&#23433;&#20840;&#24615;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
LTLf Synthesis Under Environment Specifications for Reachability and Safety Properties. (arXiv:2308.15184v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LTLf&#32508;&#21512;&#22312;&#21487;&#36798;&#24615;&#21644;&#23433;&#20840;&#24615;&#23646;&#24615;&#30340;&#29615;&#22659;&#35268;&#33539;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#32508;&#21512;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#24773;&#20917;&#26159;&#39318;&#27425;&#30740;&#31350;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LTLf&#32508;&#21512;&#22312;&#21487;&#36798;&#24615;&#21644;&#23433;&#20840;&#24615;&#23646;&#24615;&#30340;&#29615;&#22659;&#35268;&#33539;&#19979;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20195;&#29702;&#20219;&#21153;&#21644;&#29615;&#22659;&#35268;&#33539;&#30340;&#20004;&#31181;&#23646;&#24615;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#32508;&#21512;&#31639;&#27861;&#12290;&#38024;&#23545;&#27599;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#31639;&#27861;&#65288;&#38382;&#39064;&#22797;&#26434;&#24230;&#26368;&#20248;&#65289;&#24182;&#35777;&#26126;&#20102;&#20854;&#27491;&#30830;&#24615;&#12290;&#36825;&#20123;&#31639;&#27861;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#32452;&#21512;&#20102;&#24120;&#35265;&#30340;&#26500;&#20214;&#12290;&#34429;&#28982;&#26377;&#20123;&#24773;&#20917;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#30740;&#31350;&#36807;&#65292;&#20294;&#20854;&#20182;&#24773;&#20917;&#22312;&#26412;&#25991;&#20013;&#39318;&#27425;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study LTLf synthesis under environment specifications for arbitrary reachability and safety properties. We consider both kinds of properties for both agent tasks and environment specifications, providing a complete landscape of synthesis algorithms. For each case, we devise a specific algorithm (optimal wrt complexity of the problem) and prove its correctness. The algorithms combine common building blocks in different ways. While some cases are already studied in literature others are studied here for the first time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#29992;&#20110;&#26377;&#38480;&#36712;&#36857;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTLf&#65289;&#30340;&#26368;&#22823;&#21162;&#21147;&#21512;&#25104;&#30340;&#19981;&#21516;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#32452;&#20214;&#30340;&#32452;&#21512;&#26041;&#24335;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#23545;&#26041;&#27861;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.15178</link><description>&lt;p&gt;
&#31526;&#21495;&#21270;&#30340;LTLf&#26368;&#22823;&#21162;&#21147;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Symbolic LTLf Best-Effort Synthesis. (arXiv:2308.15178v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#29992;&#20110;&#26377;&#38480;&#36712;&#36857;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTLf&#65289;&#30340;&#26368;&#22823;&#21162;&#21147;&#21512;&#25104;&#30340;&#19981;&#21516;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#32452;&#20214;&#30340;&#32452;&#21512;&#26041;&#24335;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#23545;&#26041;&#27861;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#23436;&#25104;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#24403;&#19981;&#23384;&#22312;&#19968;&#31181;&#26080;&#35770;&#29615;&#22659;&#22914;&#20309;&#34892;&#21160;&#37117;&#33021;&#23436;&#25104;&#20219;&#21153;&#30340;&#31574;&#30053;&#26102;&#65292;&#20195;&#29702;&#33267;&#23569;&#24212;&#35813;&#36991;&#20813;&#37319;&#29992;&#38459;&#27490;&#23436;&#25104;&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;&#26368;&#22823;&#21162;&#21147;&#21512;&#25104;&#25429;&#25417;&#20102;&#36825;&#31181;&#30452;&#35273;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#22522;&#20110;&#26377;&#38480;&#36712;&#36857;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTLf&#65289;&#30340;&#26368;&#22823;&#21162;&#21147;&#21512;&#25104;&#30340;&#31526;&#21495;&#21270;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#30456;&#21516;&#30340;&#22522;&#26412;&#32452;&#20214;&#65292;&#20294;&#23427;&#20204;&#22312;&#22914;&#20309;&#32452;&#21512;&#36825;&#20123;&#32452;&#20214;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#32780;&#36825;&#23545;&#26041;&#27861;&#30340;&#24615;&#33021;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#65292;&#36825;&#20063;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#32463;&#39564;&#35780;&#20272;&#25152;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider an agent acting to fulfil tasks in a nondeterministic environment. When a strategy that fulfills the task regardless of how the environment acts does not exist, the agent should at least avoid adopting strategies that prevent from fulfilling its task. Best-effort synthesis captures this intuition. In this paper, we devise and compare various symbolic approaches for best-effort synthesis in Linear Temporal Logic on finite traces (LTLf). These approaches are based on the same basic components, however they change in how these components are combined, and this has a significant impact on the performance of the approaches as confirmed by our empirical evaluations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20301;&#32622;&#22320;&#22270;&#25968;&#25454;&#30340;&#36731;&#37327;&#32423;3D&#23494;&#38598;&#38754;&#37096;&#20851;&#38190;&#28857;&#20272;&#35745;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#25972;&#20010;&#33080;&#37096;&#30340;&#23494;&#38598;3D&#20851;&#38190;&#28857;&#26469;&#35299;&#20915;&#26080;&#38656;3D&#25968;&#25454;&#30340;&#38754;&#37096;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15170</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20301;&#32622;&#22320;&#22270;&#25968;&#25454;&#30340;&#36731;&#37327;&#32423;3D&#23494;&#38598;&#38754;&#37096;&#20851;&#38190;&#28857;&#20272;&#35745;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A lightweight 3D dense facial landmark estimation model from position map data. (arXiv:2308.15170v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15170
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20301;&#32622;&#22320;&#22270;&#25968;&#25454;&#30340;&#36731;&#37327;&#32423;3D&#23494;&#38598;&#38754;&#37096;&#20851;&#38190;&#28857;&#20272;&#35745;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#25972;&#20010;&#33080;&#37096;&#30340;&#23494;&#38598;3D&#20851;&#38190;&#28857;&#26469;&#35299;&#20915;&#26080;&#38656;3D&#25968;&#25454;&#30340;&#38754;&#37096;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;3D&#25968;&#25454;&#25972;&#21512;&#21040;&#38754;&#37096;&#20998;&#26512;&#20219;&#21153;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#23613;&#31649;&#23427;&#25552;&#20379;&#20102;&#20154;&#33080;&#26356;&#20934;&#30830;&#12289;&#35814;&#32454;&#30340;&#34920;&#31034;&#65292;&#20294;&#33719;&#21462;3D&#20154;&#33080;&#25968;&#25454;&#27604;2D&#20154;&#33080;&#22270;&#20687;&#26356;&#22797;&#26434;&#12289;&#26356;&#26114;&#36149;&#12290;&#20154;&#20204;&#35201;&#20040;&#20381;&#36182;&#26114;&#36149;&#30340;3D&#25195;&#25551;&#20202;&#25110;&#28145;&#24230;&#20256;&#24863;&#22120;&#65292;&#36825;&#20123;&#35774;&#22791;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#21478;&#19968;&#31181;&#36873;&#25321;&#26159;&#22312;&#27809;&#26377;&#20219;&#20309;3D&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20174;&#38750;&#26631;&#23450;&#30340;2D&#22270;&#20687;&#37325;&#24314;3D&#20154;&#33080;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#20250;&#24456;&#26114;&#36149;&#65292;&#32780;&#19988;&#25152;&#23398;&#20064;&#30340;&#27169;&#22411;&#23610;&#23544;&#19981;&#36866;&#21512;&#31227;&#21160;&#35774;&#22791;&#25110;&#20854;&#20182;&#36793;&#32536;&#35774;&#22791;&#24212;&#29992;&#12290;&#36890;&#36807;&#39044;&#27979;&#25972;&#20010;&#33080;&#37096;&#30340;&#23494;&#38598;3D&#20851;&#38190;&#28857;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#27809;&#26377;&#21253;&#21547;&#23494;&#38598;&#20851;&#38190;&#28857;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21487;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#20174;&#29616;&#26377;&#30340;&#38754;&#37096;&#20301;&#32622;&#22320;&#22270;&#25968;&#25454;&#20013;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;520&#20010;&#20851;&#38190;&#28857;&#30340;&#23494;&#38598;&#20851;&#38190;&#28857;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;MobileNet&#30340;&#36731;&#37327;&#32423;&#22238;&#24402;&#22120;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The incorporation of 3D data in facial analysis tasks has gained popularity in recent years. Though it provides a more accurate and detailed representation of the human face, accruing 3D face data is more complex and expensive than 2D face images. Either one has to rely on expensive 3D scanners or depth sensors which are prone to noise. An alternative option is the reconstruction of 3D faces from uncalibrated 2D images in an unsupervised way without any ground truth 3D data. However, such approaches are computationally expensive and the learned model size is not suitable for mobile or other edge device applications. Predicting dense 3D landmarks over the whole face can overcome this issue. As there is no public dataset available containing dense landmarks, we propose a pipeline to create a dense keypoint training dataset containing 520 key points across the whole face from an existing facial position map data. We train a lightweight MobileNet-based regressor model with the generated da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#39033;&#23545;&#25968;&#23383;&#23402;&#29983;&#20013;&#26412;&#20307;&#35770;&#30340;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65292;&#36890;&#36807;&#20998;&#26512;82&#31687;&#30740;&#31350;&#35770;&#25991;&#30340;&#36884;&#24452;&#65292;&#25506;&#35752;&#20102;&#26412;&#20307;&#35770;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#24212;&#29992;&#21644;&#36129;&#29486;&#65292;&#21253;&#25324;&#30693;&#35782;&#34920;&#31034;&#12289;&#20114;&#25805;&#20316;&#24615;&#21644;&#33258;&#21160;&#25512;&#29702;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2308.15168</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#26412;&#20307;&#35770;: &#19968;&#39033;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Ontologies in Digital Twins: A Systematic Literature Review. (arXiv:2308.15168v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#39033;&#23545;&#25968;&#23383;&#23402;&#29983;&#20013;&#26412;&#20307;&#35770;&#30340;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65292;&#36890;&#36807;&#20998;&#26512;82&#31687;&#30740;&#31350;&#35770;&#25991;&#30340;&#36884;&#24452;&#65292;&#25506;&#35752;&#20102;&#26412;&#20307;&#35770;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#24212;&#29992;&#21644;&#36129;&#29486;&#65292;&#21253;&#25324;&#30693;&#35782;&#34920;&#31034;&#12289;&#20114;&#25805;&#20316;&#24615;&#21644;&#33258;&#21160;&#25512;&#29702;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;(DT)&#22312;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#20419;&#36827;&#30417;&#25511;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;&#30001;&#20110;&#23494;&#38598;&#30340;&#30740;&#31350;&#27963;&#21160;&#21644;&#24037;&#19994;&#36827;&#23637;&#65292;&#23427;&#20204;&#22312;&#36817;&#24180;&#26469;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;DT&#20013;&#65292;&#26412;&#20307;&#21644;&#30693;&#35782;&#22270;&#22312;&#30693;&#35782;&#34920;&#31034;&#12289;&#20114;&#25805;&#20316;&#24615;&#21644;&#33258;&#21160;&#25512;&#29702;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#21463;&#21040;&#20102;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23545;&#35821;&#20041;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#26412;&#20307;&#35770;&#65292;&#22312;DT&#20013;&#30340;&#20855;&#20307;&#24212;&#29992;&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#22522;&#20110;&#23545;82&#31687;&#19982;DT&#30456;&#20851;&#30340;&#30740;&#31350;&#25991;&#31456;&#30340;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#39033;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#12290;&#35770;&#25991;&#37319;&#29992;&#19981;&#21516;&#30340;&#20998;&#26512;&#35282;&#24230;&#65292;&#21253;&#25324;&#22522;&#20110;&#21442;&#32771;DT&#26550;&#26500;&#30340;&#32467;&#26500;&#20998;&#26512;&#21644;&#22522;&#20110;&#24212;&#29992;&#39046;&#22495;&#30340;&#29305;&#23450;&#20998;&#26512;&#65292;&#20197;&#20855;&#20307;&#30340;&#26041;&#24335;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital Twins (DT) facilitate monitoring and reasoning processes in cyber-physical systems. They have progressively gained popularity over the past years because of intense research activity and industrial advancements. Cognitive Twins is a novel concept, recently coined to refer to the involvement of Semantic Web technology in DTs. Recent studies address the relevance of ontologies and knowledge graphs in the context of DTs, in terms of knowledge representation, interoperability and automatic reasoning. However, there is no comprehensive analysis of how semantic technologies, and specifically ontologies, are utilized within DTs. This Systematic Literature Review (SLR) is based on the analysis of 82 research articles, that either propose or benefit from ontologies with respect to DT. The paper uses different analysis perspectives, including a structural analysis based on a reference DT architecture, and an application-specific analysis to specifically address the different domains, suc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20687;&#30495;&#23454;&#21160;&#29289;&#19968;&#26679;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#31574;&#30053;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#20445;&#30041;&#20102;&#21160;&#29289;&#34892;&#20026;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36866;&#24212;&#29615;&#22659;&#65292;&#20811;&#26381;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2308.15143</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#29983;&#21160;&#30340;&#28789;&#27963;&#24615;&#21644;&#28216;&#25103;&#24615;
&lt;/p&gt;
&lt;p&gt;
Lifelike Agility and Play on Quadrupedal Robots using Reinforcement Learning and Generative Pre-trained Models. (arXiv:2308.15143v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20687;&#30495;&#23454;&#21160;&#29289;&#19968;&#26679;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#31574;&#30053;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#20445;&#30041;&#20102;&#21160;&#29289;&#34892;&#20026;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36866;&#24212;&#29615;&#22659;&#65292;&#20811;&#26381;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24635;&#32467;&#21160;&#29289;&#21644;&#20154;&#31867;&#30340;&#30693;&#35782;&#21551;&#21457;&#20102;&#26426;&#22120;&#20154;&#21019;&#26032;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20687;&#30495;&#23454;&#21160;&#29289;&#19968;&#26679;&#25317;&#26377;&#29983;&#21160;&#30340;&#28789;&#27963;&#24615;&#21644;&#31574;&#30053;&#12290;&#21463;&#21040;&#22312;&#35821;&#35328;&#21644;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20197;&#29983;&#25104;&#27169;&#25311;&#30495;&#23454;&#21160;&#29289;&#21160;&#20316;&#30340;&#36816;&#21160;&#25511;&#21046;&#20449;&#21495;&#12290;&#19982;&#20256;&#32479;&#25511;&#21046;&#22120;&#21644;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21482;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#21160;&#29289;&#36816;&#21160;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#20445;&#30041;&#26377;&#34920;&#36798;&#21147;&#30340;&#21160;&#29289;&#34892;&#20026;&#30693;&#35782;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#25317;&#26377;&#36275;&#22815;&#30340;&#21407;&#22987;&#32423;&#30693;&#35782;&#65292;&#20294;&#19982;&#29615;&#22659;&#26080;&#20851;&#12290;&#28982;&#21518;&#65292;&#22312;&#23398;&#20064;&#30340;&#21518;&#32493;&#38454;&#27573;&#65292;&#36890;&#36807;&#31359;&#36234;&#19968;&#20123;&#20197;&#21069;&#30340;&#26041;&#27861;&#24456;&#23569;&#32771;&#34385;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#65292;&#22914;&#31359;&#36807;&#29421;&#31364;&#30340;&#31354;&#38388;&#31561;&#65292;&#20351;&#20854;&#36866;&#24212;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing knowledge from animals and human beings inspires robotic innovations. In this work, we propose a framework for driving legged robots act like real animals with lifelike agility and strategy in complex environments. Inspired by large pre-trained models witnessed with impressive performance in language and image understanding, we introduce the power of advanced deep generative models to produce motor control signals stimulating legged robots to act like real animals. Unlike conventional controllers and end-to-end RL methods that are task-specific, we propose to pre-train generative models over animal motion datasets to preserve expressive knowledge of animal behavior. The pre-trained model holds sufficient primitive-level knowledge yet is environment-agnostic. It is then reused for a successive stage of learning to align with the environments by traversing a number of challenging obstacles that are rarely considered in previous approaches, including creeping through narrow sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#35270;&#35273;&#20449;&#24687;&#32534;&#30721;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#23383;&#35821;&#20041;&#20449;&#24687;&#21644;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#20449;&#24687;&#30340;&#32534;&#30721;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;Transformer&#32593;&#32476;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#21367;&#31215;&#32593;&#32476;&#23558;&#29305;&#24449;&#26144;&#23556;&#21040;&#20307;&#32032;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#35270;&#35273;&#20449;&#24687;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2308.15142</link><description>&lt;p&gt;
&#21463;&#25991;&#23383;&#35821;&#20041;&#20449;&#24687;&#36741;&#21161;&#30340;&#22810;&#27169;&#24577;&#35270;&#35273;&#32534;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Visual Encoding Model Aided by Introducing Verbal Semantic Information. (arXiv:2308.15142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#35270;&#35273;&#20449;&#24687;&#32534;&#30721;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#23383;&#35821;&#20041;&#20449;&#24687;&#21644;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#20449;&#24687;&#30340;&#32534;&#30721;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;Transformer&#32593;&#32476;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#21367;&#31215;&#32593;&#32476;&#23558;&#29305;&#24449;&#26144;&#23556;&#21040;&#20307;&#32032;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#35270;&#35273;&#20449;&#24687;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22823;&#33041;&#30382;&#23618;&#20013;&#65292;&#25991;&#23383;&#35821;&#20041;&#20449;&#24687;&#20316;&#20026;&#19968;&#31181;&#39069;&#22806;&#30340;&#26469;&#28304;&#21442;&#19982;&#38750;&#35821;&#35328;&#35821;&#20041;&#20219;&#21153;&#65292;&#27604;&#22914;&#35270;&#35273;&#32534;&#30721;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#35270;&#35273;&#32534;&#30721;&#27169;&#22411;&#27809;&#26377;&#25972;&#21512;&#25991;&#23383;&#35821;&#20041;&#20449;&#24687;&#65292;&#19982;&#36825;&#19968;&#29983;&#29289;&#23398;&#21457;&#29616;&#30456;&#30683;&#30462;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21050;&#28608;&#22270;&#20687;&#21644;&#30456;&#20851;&#25991;&#26412;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#35270;&#35273;&#20449;&#24687;&#32534;&#30721;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35270;&#35273;&#20449;&#24687;&#32534;&#30721;&#32593;&#32476;&#27169;&#22411;&#20197;&#21050;&#28608;&#22270;&#20687;&#20026;&#36755;&#20837;&#65292;&#24182;&#21033;&#29992;&#30001;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#20449;&#24687;&#20316;&#20026;&#25991;&#23383;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#26032;&#30340;&#20449;&#24687;&#27880;&#20837;&#21040;&#35270;&#35273;&#32534;&#30721;&#27169;&#22411;&#20013;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;Transformer&#32593;&#32476;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#20449;&#24687;&#36827;&#34892;&#23545;&#40784;&#65292;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#29305;&#24449;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#21367;&#31215;&#32593;&#32476;&#23558;&#20174;&#36825;&#20010;&#22810;&#27169;&#24577;&#29305;&#24449;&#31354;&#38388;&#26144;&#23556;&#21040;&#20307;&#32032;&#31354;&#38388;&#65292;&#26500;&#24314;&#22810;&#27169;&#24577;&#35270;&#35273;&#20449;&#24687;&#32534;&#30721;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological research has revealed that the verbal semantic information in the brain cortex, as an additional source, participates in nonverbal semantic tasks, such as visual encoding. However, previous visual encoding models did not incorporate verbal semantic information, contradicting this biological finding. This paper proposes a multimodal visual information encoding network model based on stimulus images and associated textual information in response to this issue. Our visual information encoding network model takes stimulus images as input and leverages textual information generated by a text-image generation model as verbal semantic information. This approach injects new information into the visual encoding model. Subsequently, a Transformer network aligns image and text feature information, creating a multimodal feature space. A convolutional network then maps from this multimodal feature space to voxel space, constructing the multimodal visual information encoding network model
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33145;&#37096;&#22810;&#22120;&#23448;&#20998;&#21106;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;FPN&#65289;&#21644;&#31354;&#38388;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;SRNN&#65289;&#65292;&#20197;&#21152;&#36895;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#24182;&#20943;&#36731;&#36229;&#22768;&#26816;&#26597;&#24072;&#30340;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2308.15137</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#31354;&#38388;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#33145;&#37096;&#22810;&#22120;&#23448;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Abdominal Multi-Organ Segmentation Based on Feature Pyramid Network and Spatial Recurrent Neural Network. (arXiv:2308.15137v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15137
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33145;&#37096;&#22810;&#22120;&#23448;&#20998;&#21106;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;FPN&#65289;&#21644;&#31354;&#38388;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;SRNN&#65289;&#65292;&#20197;&#21152;&#36895;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#24182;&#20943;&#36731;&#36229;&#22768;&#26816;&#26597;&#24072;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#23548;&#33268;&#20256;&#32479;&#35786;&#26029;&#26041;&#27861;&#30340;&#34928;&#36864;&#65292;&#31471;&#21040;&#31471;&#35786;&#26029;&#30340;&#23454;&#29616;&#27491;&#22312;&#36805;&#36895;&#25509;&#36817;&#12290;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#26159;&#35786;&#26029;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#20998;&#21106;&#27169;&#22411;&#21487;&#20197;&#21152;&#36895;&#36807;&#31243;&#24182;&#20943;&#36731;&#36229;&#22768;&#26816;&#26597;&#24072;&#30340;&#36127;&#25285;&#12290;&#19982;&#20197;&#24448;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36229;&#22768;&#22270;&#20687;&#30340;&#20004;&#20010;&#20869;&#22312;&#29305;&#24449;&#65306;&#65288;1&#65289;&#19981;&#21516;&#22120;&#23448;&#21644;&#32452;&#32455;&#30340;&#31354;&#38388;&#23610;&#23544;&#21464;&#21270;&#65292;&#65288;2&#65289;&#20154;&#20307;&#20869;&#37096;&#30340;&#35299;&#21078;&#32467;&#26500;&#24418;&#25104;&#30456;&#23545;&#24658;&#23450;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;FPN&#65289;&#21644;&#31354;&#38388;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;SRNN&#65289;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20026;&#20160;&#20040;&#20351;&#29992;FPN&#25552;&#21462;&#19981;&#21516;&#23610;&#24230;&#30340;&#35299;&#21078;&#32467;&#26500;&#20197;&#21450;&#22914;&#20309;&#23454;&#29616;SRNN&#26469;&#25552;&#21462;&#33145;&#37096;&#36229;&#22768;&#22270;&#20687;&#20013;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
As recent advances in AI are causing the decline of conventional diagnostic methods, the realization of end-to-end diagnosis is fast approaching. Ultrasound image segmentation is an important step in the diagnostic process. An accurate and robust segmentation model accelerates the process and reduces the burden of sonographers. In contrast to previous research, we take two inherent features of ultrasound images into consideration: (1) different organs and tissues vary in spatial sizes, (2) the anatomical structures inside human body form a relatively constant spatial relationship. Based on those two ideas, we propose a new image segmentation model combining Feature Pyramid Network (FPN) and Spatial Recurrent Neural Network (SRNN). We discuss why we use FPN to extract anatomical structures of different scales and how SRNN is implemented to extract the spatial context features in abdominal ultrasound images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.15126</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#35780;&#20272;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;LVLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#24187;&#35273;&#25351;&#30340;&#26159;LVLMs&#21709;&#24212;&#20013;&#19981;&#23384;&#22312;&#20110;&#35270;&#35273;&#36755;&#20837;&#20013;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#37325;&#22823;&#21518;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#30446;&#21069;&#23545;LVLMs&#20013;&#30340;&#24187;&#35273;&#35780;&#20272;&#30340;&#30740;&#31350;&#24037;&#20316;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#12290;HaELM&#30340;&#24615;&#33021;&#36817;&#20284;&#20110;ChatGPT&#30340;95%&#65292;&#24182;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#21487;&#22797;&#29616;&#12289;&#20445;&#25252;&#38544;&#31169;&#21644;&#26412;&#22320;&#37096;&#32626;&#31561;&#39069;&#22806;&#20248;&#21183;&#12290;&#21033;&#29992;HaELM&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;LVLMs&#20013;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23548;&#33268;LVLMs&#20013;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#26377;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation halluci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#25945;&#32946;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#25945;&#24072;&#29992;&#25143;&#20998;&#20026;&#23450;&#21521;&#12289;&#29366;&#20917;&#21644;&#20559;&#22909;&#19977;&#31867;&#65292;&#24182;&#25972;&#29702;&#20102;FER&#35299;&#20915;&#26041;&#26696;&#30340;&#25216;&#26415;&#21644;&#24212;&#29992;&#20004;&#31867;&#65292;&#23545;&#25945;&#32946;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.15119</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#25945;&#32946;&#35299;&#20915;&#26041;&#26696;&#65306;&#25945;&#24072;&#29992;&#25143;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AI-Based Facial Emotion Recognition Solutions for Education: A Study of Teacher-User and Other Categories. (arXiv:2308.15119v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#25945;&#32946;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#25945;&#24072;&#29992;&#25143;&#20998;&#20026;&#23450;&#21521;&#12289;&#29366;&#20917;&#21644;&#20559;&#22909;&#19977;&#31867;&#65292;&#24182;&#25972;&#29702;&#20102;FER&#35299;&#20915;&#26041;&#26696;&#30340;&#25216;&#26415;&#21644;&#24212;&#29992;&#20004;&#31867;&#65292;&#23545;&#25945;&#32946;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20851;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#65288;FER&#65289;&#30340;&#20449;&#24687;&#23545;&#20110;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#20197;&#22806;&#30340;&#20154;&#26469;&#35828;&#19981;&#26131;&#29702;&#35299;&#65292;&#38656;&#35201;&#36328;&#23398;&#31185;&#30340;&#21162;&#21147;&#26469;&#30830;&#23450;&#19968;&#20010;&#20419;&#36827;&#23545;&#36825;&#39033;&#25216;&#26415;&#21450;&#20854;&#23545;&#29992;&#25143;&#30340;&#24433;&#21709;&#30340;&#29702;&#35299;&#30340;&#20998;&#31867;&#26694;&#26550;&#12290;&#22823;&#37096;&#20998;&#25903;&#25345;&#32773;&#23558;FER&#25353;&#29031;&#26041;&#27861;&#35770;&#12289;&#23454;&#26045;&#21644;&#20998;&#26512;&#30340;&#35282;&#24230;&#36827;&#34892;&#20998;&#31867;&#65292;&#30456;&#23545;&#36739;&#23569;&#23558;&#20854;&#24212;&#29992;&#20110;&#25945;&#32946;&#39046;&#22495;&#65292;&#32780;&#27809;&#26377;&#23558;&#20854;&#29992;&#25143;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#25945;&#32946;&#20013;FER&#24037;&#20855;&#30340;&#65288;&#28508;&#22312;&#65289;&#25945;&#24072;&#29992;&#25143;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24773;&#24863;&#25945;&#32946;&#30446;&#26631;&#21644;&#30456;&#20851;&#29702;&#35770;&#30340;&#32463;&#20856;&#20998;&#31867;&#27861;&#65292;&#23558;&#36825;&#20123;&#25945;&#24072;&#29992;&#25143;&#20998;&#20026;&#23450;&#21521;&#12289;&#29366;&#20917;&#21644;&#20559;&#22909;&#19977;&#31867;&#65292;&#24182;&#23558;&#25991;&#29486;&#20013;&#21457;&#29616;&#25110;&#25512;&#23548;&#20986;&#30340;FER&#35299;&#20915;&#26041;&#26696;&#31867;&#22411;&#36827;&#34892;&#25972;&#29702;&#21644;&#24402;&#31867;&#20026;&#8220;&#25216;&#26415;&#8221;&#21644;&#8220;&#24212;&#29992;&#8221;&#20004;&#31867;&#65292;&#20316;&#20026;&#26500;&#24314;&#25552;&#20986;&#30340;&#8220;&#25945;&#24072;&#29992;&#25143;&#8221;&#31867;&#21035;&#30340;&#21069;&#25552;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;&#25903;&#25345;&#32773;&#65292;&#35780;&#35770;&#32773;&#20197;&#21450;&#25945;&#32946;&#25919;&#31574;&#21046;&#23450;&#32773;&#37117;&#26377;&#30528;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing information on AI-based facial emotion recognition (FER) is not easily comprehensible by those outside the field of computer science, requiring cross-disciplinary effort to determine a categorisation framework that promotes the understanding of this technology, and its impact on users. Most proponents classify FER in terms of methodology, implementation and analysis; relatively few by its application in education; and none by its users. This paper is concerned primarily with (potential) teacher-users of FER tools for education. It proposes a three-part classification of these teachers, by orientation, condition and preference, based on a classical taxonomy of affective educational objectives, and related theories. It also compiles and organises the types of FER solutions found in or inferred from the literature into "technology" and "applications" categories, as a prerequisite for structuring the proposed "teacher-user" category. This work has implications for proponents', cri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#25311;&#36830;&#32493;&#21160;&#24577;&#26465;&#20214;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15116</link><description>&lt;p&gt;
&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators. (arXiv:2308.15116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#25311;&#36830;&#32493;&#21160;&#24577;&#26465;&#20214;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#29983;&#29289;&#20998;&#23376;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#20998;&#23376;&#33021;&#22815;&#27874;&#21160;&#30340;&#21508;&#31181;&#26465;&#20214;&#19979;&#23545;&#19968;&#32452;&#31890;&#23376;&#36827;&#34892;&#27169;&#25311;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36719;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#20998;&#23376;&#21160;&#21147;&#23398;&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#36866;&#24212;&#24615;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#38750;&#24120;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#22330;&#26223;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#24037;&#20316;&#20197;&#28201;&#24230;&#20026;&#27979;&#35797;&#26696;&#20363;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#20351;&#20854;&#21487;&#20197;&#36890;&#36807;&#20219;&#20309;&#36830;&#32493;&#30340;&#21160;&#24577;&#26465;&#20214;&#65288;&#22914;&#21387;&#21147;&#21644;&#20307;&#31215;&#65289;&#36827;&#34892;&#26377;&#25928;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#20004;&#20010;&#38454;&#27573;&#65306;1&#65289;&#20351;&#29992;&#25968;&#25454;&#28151;&#21512;&#25216;&#26415;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22686;&#24378;&#20998;&#23376;&#32467;&#26500;&#25968;&#25454;&#21644;&#28201;&#24230;&#25552;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#27604;&#20363;&#30340;&#26041;&#24335;&#24212;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;2&#65289;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#24494;&#35843;&#26694;&#26550;&#25552;&#39640;&#20102;&#24494;&#35843;&#36807;&#31243;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#20026;&#36719;&#25552;&#31034;&#24494;&#35843;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular dynamics simulations have emerged as a fundamental instrument for studying biomolecules. At the same time, it is desirable to perform simulations of a collection of particles under various conditions in which the molecules can fluctuate. In this paper, we explore and adapt the soft prompt-based learning method to molecular dynamics tasks. Our model can remarkably generalize to unseen and out-of-distribution scenarios with limited training data. While our work focuses on temperature as a test case, the versatility of our approach allows for efficient simulation through any continuous dynamic conditions, such as pressure and volumes. Our framework has two stages: 1) Pre-trains with data mixing technique, augments molecular structure data and temperature prompts, then applies a curriculum learning method by increasing the ratio of them smoothly. 2) Meta-learning-based fine-tuning framework improves sample-efficiency of fine-tuning process and gives the soft prompt-tuning better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20174;&#21487;&#35299;&#37322;&#27169;&#22411;&#20013;&#27010;&#29575;&#37325;&#24314;&#25968;&#25454;&#38598;&#12290;&#22312;&#29616;&#23454;&#20551;&#35774;&#19979;&#65292;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#37325;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#37327;&#21270;&#20449;&#24687;&#27844;&#28431;&#30340;&#38544;&#31169;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.15099</link><description>&lt;p&gt;
&#20174;&#21487;&#35299;&#37322;&#27169;&#22411;&#20013;&#23454;&#29616;&#27010;&#29575;&#25968;&#25454;&#38598;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Dataset Reconstruction from Interpretable Models. (arXiv:2308.15099v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20174;&#21487;&#35299;&#37322;&#27169;&#22411;&#20013;&#27010;&#29575;&#37325;&#24314;&#25968;&#25454;&#38598;&#12290;&#22312;&#29616;&#23454;&#20551;&#35774;&#19979;&#65292;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#37325;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#37327;&#21270;&#20449;&#24687;&#27844;&#28431;&#30340;&#38544;&#31169;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#32463;&#24120;&#34987;&#35748;&#20026;&#26159;&#21487;&#20449;&#36182;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#38190;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#21644;&#21457;&#24067;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#20250;&#27844;&#38706;&#26377;&#20851;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#30001;&#20110;&#36825;&#31181;&#25259;&#38706;&#21487;&#33021;&#30452;&#25509;&#19982;&#38544;&#31169;&#20914;&#31361;&#65292;&#22240;&#27492;&#20934;&#30830;&#37327;&#21270;&#36825;&#31181;&#27844;&#28431;&#24102;&#26469;&#30340;&#38544;&#31169;&#24433;&#21709;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#20363;&#22914;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#21033;&#29992;&#20915;&#31574;&#26641;&#30340;&#32467;&#26500;&#26500;&#24314;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#27010;&#29575;&#37325;&#24314;&#65292;&#37325;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#20449;&#24687;&#27844;&#28431;&#30340;&#19968;&#20010;&#30456;&#20851;&#24230;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#25512;&#24191;&#20102;&#36825;&#20123;&#27010;&#29575;&#37325;&#24314;&#65292;&#20351;&#20854;&#21487;&#20197;&#22788;&#29702;&#20854;&#20182;&#24418;&#24335;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#21644;&#26356;&#19968;&#33324;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#32467;&#26500;&#36827;&#34892;&#23454;&#38469;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#37325;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability is often pointed out as a key requirement for trustworthy machine learning. However, learning and releasing models that are inherently interpretable leaks information regarding the underlying training data. As such disclosure may directly conflict with privacy, a precise quantification of the privacy impact of such breach is a fundamental problem. For instance, previous work have shown that the structure of a decision tree can be leveraged to build a probabilistic reconstruction of its training dataset, with the uncertainty of the reconstruction being a relevant metric for the information leak. In this paper, we propose of a novel framework generalizing these probabilistic reconstructions in the sense that it can handle other forms of interpretable models and more generic types of knowledge. In addition, we demonstrate that under realistic assumptions regarding the interpretable models' structure, the uncertainty of the reconstruction can be computed efficiently. Final
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#23884;&#20837;&#24335;&#23545;&#35805;&#26234;&#33021;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#36164;&#28304;&#20351;&#29992;&#29702;&#35770;&#22522;&#30784;&#30340;&#27880;&#37322;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2308.15097</link><description>&lt;p&gt;
&#33258;&#28982;&#21457;&#29983;&#30340;&#20154;&#26426;&#20132;&#20114;&#30340;&#39034;&#24207;&#27880;&#37322;:&#21021;&#27493;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Sequential annotations for naturally-occurring HRI: first insights. (arXiv:2308.15097v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15097
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#23884;&#20837;&#24335;&#23545;&#35805;&#26234;&#33021;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#36164;&#28304;&#20351;&#29992;&#29702;&#35770;&#22522;&#30784;&#30340;&#27880;&#37322;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#37322;&#20102;&#25105;&#20204;&#24320;&#21457;&#30340;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#23545;&#35805;&#20998;&#26512;&#30340;&#39034;&#24207;&#21644;&#22810;&#27169;&#24577;&#20998;&#26512;&#26469;&#25913;&#21892;&#23884;&#20837;&#24335;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#12290;&#20351;&#29992;&#26696;&#20363;&#26159;&#19968;&#20010;Pepper&#26426;&#22120;&#20154;&#65292;&#39044;&#26399;&#22312;&#22270;&#20070;&#39302;&#20013;&#21521;&#29992;&#25143;&#25552;&#20379;&#20449;&#24687;&#21644;&#23548;&#33322;&#12290;&#20026;&#20102;&#25552;&#20986;&#21644;&#23398;&#20064;&#26356;&#22909;&#30340;&#20132;&#20114;&#27169;&#24335;&#65292;&#25105;&#20204;&#27491;&#22312;&#21019;&#24314;&#19968;&#20010;&#33258;&#28982;&#21457;&#29983;&#30340;&#20132;&#20114;&#35821;&#26009;&#24211;&#65292;&#24182;&#23558;&#20854;&#25552;&#20379;&#32473;&#31038;&#21306;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26377;&#20851;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#36164;&#28304;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#20351;&#29992;&#30340;&#19968;&#20123;&#29702;&#35770;&#22522;&#30784;&#30340;&#27880;&#37322;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explain the methodology we developed for improving the interactions accomplished by an embedded conversational agent, drawing from Conversation Analytic sequential and multimodal analysis. The use case is a Pepper robot that is expected to inform and orient users in a library. In order to propose and learn better interactive schema, we are creating a corpus of naturally-occurring interactions that will be made available to the community. To do so, we propose an annotation practice based on some theoretical underpinnings about the use of language and multimodal resources in human-robot interaction. CCS CONCEPTS $\bullet$ Computing methodologies $\rightarrow$ Discourse, dialogue and pragmatics; $\bullet$ Human-centered computing $\rightarrow$ Text input; HCI theory, concepts and models; Field studies.
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#31639;&#27861;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#24341;&#21457;&#20102;&#23433;&#20840;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#35813;&#20027;&#39064;&#30340;&#27010;&#36848;&#65292;&#20851;&#27880;&#23545;&#24212;&#29992;&#21644;&#35745;&#31639;&#25968;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21487;&#33021;&#24863;&#20852;&#36259;&#30340;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2308.15092</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#20381;&#36182;&#20154;&#24037;&#26234;&#33021;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Rely on AI?. (arXiv:2308.15092v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15092
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#31639;&#27861;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#24341;&#21457;&#20102;&#23433;&#20840;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#35813;&#20027;&#39064;&#30340;&#27010;&#36848;&#65292;&#20851;&#27880;&#23545;&#24212;&#29992;&#21644;&#35745;&#31639;&#25968;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21487;&#33021;&#24863;&#20852;&#36259;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#31639;&#27861;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#36825;&#20123;&#31639;&#27861;&#24341;&#21457;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#12290;&#20174;&#23454;&#38469;&#35282;&#24230;&#26469;&#30475;&#65292;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#21457;&#23637;&#20043;&#38388;&#23384;&#22312;&#30528;&#19968;&#31181;&#21319;&#32423;&#30340;&#25112;&#20105;&#12290;&#22312;&#26356;&#21152;&#29702;&#35770;&#30340;&#23618;&#38754;&#19978;&#65292;&#30740;&#31350;&#20154;&#21592;&#20063;&#30740;&#31350;&#20102;&#25915;&#20987;&#30340;&#23384;&#22312;&#24615;&#21644;&#21487;&#35745;&#31639;&#24615;&#31561;&#26356;&#22823;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#31616;&#35201;&#27010;&#36848;&#20102;&#36825;&#20010;&#20027;&#39064;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#23545;&#24212;&#29992;&#21644;&#35745;&#31639;&#25968;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21487;&#33021;&#24863;&#20852;&#36259;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, adversarial attack algorithms have revealed instabilities in deep learning tools. These algorithms raise issues regarding safety, reliability and interpretability in artificial intelligence; especially in high risk settings. From a practical perspective, there has been a war of escalation between those developing attack and defence strategies. At a more theoretical level, researchers have also studied bigger picture questions concerning the existence and computability of attacks. Here we give a brief overview of the topic, focusing on aspects that are likely to be of interest to researchers in applied and computational mathematics.
&lt;/p&gt;</description></item><item><title>LAMBO&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36793;&#32536;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#12290;&#23427;&#35299;&#20915;&#20102;&#20256;&#32479;&#28145;&#24230;&#21368;&#36733;&#26550;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#24615;&#33021;&#30340;&#20915;&#31574;&#27169;&#22359;&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2308.15078</link><description>&lt;p&gt;
LAMBO: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#36793;&#32536;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
LAMBO: Large Language Model Empowered Edge Intelligence. (arXiv:2308.15078v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15078
&lt;/p&gt;
&lt;p&gt;
LAMBO&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36793;&#32536;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#12290;&#23427;&#35299;&#20915;&#20102;&#20256;&#32479;&#28145;&#24230;&#21368;&#36733;&#26550;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#24615;&#33021;&#30340;&#20915;&#31574;&#27169;&#22359;&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35745;&#19979;&#19968;&#20195;&#36793;&#32536;&#26234;&#33021;&#23558;&#20026;&#21508;&#31181;&#24212;&#29992;&#24102;&#26469;&#24040;&#22823;&#30340;&#22909;&#22788;&#65292;&#20363;&#22914;&#21368;&#36733;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#28145;&#24230;&#21368;&#36733;&#26550;&#26500;&#38754;&#20020;&#22810;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#24322;&#26500;&#38480;&#21046;&#12289;&#37096;&#20998;&#24863;&#30693;&#12289;&#19981;&#30830;&#23450;&#30340;&#27867;&#21270;&#21644;&#32570;&#20047;&#21487;&#36861;&#28335;&#24615;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#23558;&#21368;&#36733;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#22312;&#19968;&#36215;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#21368;&#36733;&#65288;LAMBO&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#65292;&#23427;&#30001;&#22235;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;i&#65289;&#36755;&#20837;&#23884;&#20837;&#65288;IE&#65289;&#65292;&#29992;&#20110;&#29992;&#39640;&#36136;&#37327;&#30340;&#21487;&#23398;&#20064;&#21521;&#37327;&#34920;&#31034;&#20855;&#26377;&#32422;&#26463;&#21644;&#25552;&#31034;&#30340;&#21368;&#36733;&#31995;&#32479;&#30340;&#20449;&#24687;&#65307;&#65288;ii&#65289;&#38750;&#23545;&#31216;&#32534;&#30721;&#35299;&#30721;&#65288;AED&#65289;&#27169;&#22411;&#65292;&#26159;&#19968;&#20010;&#20915;&#31574;&#27169;&#22359;&#65292;&#20855;&#26377;&#28145;&#24230;&#32534;&#30721;&#22120;&#21644;&#27973;&#23618;&#35299;&#30721;&#22120;&#12290;&#23427;&#21487;&#20197;&#22522;&#20110;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#39640;&#24615;&#33021;&#65307;&#65288;iii&#65289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#65288;ACRL&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next-generation edge intelligence is anticipated to bring huge benefits to various applications, e.g., offloading systems. However, traditional deep offloading architectures face several issues, including heterogeneous constraints, partial perception, uncertain generalization, and lack of tractability. In this context, the integration of offloading with large language models (LLMs) presents numerous advantages. Therefore, we propose an LLM-Based Offloading (LAMBO) framework for mobile edge computing (MEC), which comprises four components: (i) Input embedding (IE), which is used to represent the information of the offloading system with constraints and prompts through learnable vectors with high quality; (ii) Asymmetric encoderdecoder (AED) model, which is a decision-making module with a deep encoder and a shallow decoder. It can achieve high performance based on multi-head self-attention schemes; (iii) Actor-critic reinforcement learning (ACRL) module, which is employed to pre-train th
&lt;/p&gt;</description></item><item><title>MadSGM&#26159;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#32771;&#34385;&#20102;&#37325;&#26500;&#12289;&#23494;&#24230;&#21644;&#26799;&#24230;&#31561;&#20840;&#38754;&#30340;&#24322;&#24120;&#27979;&#37327;&#22240;&#32032;&#12290;&#23454;&#39564;&#35777;&#26126;MadSGM&#20855;&#26377;&#26368;&#24378;&#22823;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15069</link><description>&lt;p&gt;
MadSGM&#65306;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#21464;&#37327;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MadSGM: Multivariate Anomaly Detection with Score-based Generative Models. (arXiv:2308.15069v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15069
&lt;/p&gt;
&lt;p&gt;
MadSGM&#26159;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#32771;&#34385;&#20102;&#37325;&#26500;&#12289;&#23494;&#24230;&#21644;&#26799;&#24230;&#31561;&#20840;&#38754;&#30340;&#24322;&#24120;&#27979;&#37327;&#22240;&#32032;&#12290;&#23454;&#39564;&#35777;&#26126;MadSGM&#20855;&#26377;&#26368;&#24378;&#22823;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#20013;&#26368;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#19982;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21644;&#20998;&#31867;&#19981;&#21516;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#36890;&#24120;&#38656;&#35201;&#26080;&#30417;&#30563;&#65288;&#25110;&#33258;&#30417;&#30563;&#65289;&#35757;&#32451;&#65292;&#22240;&#20026;&#25910;&#38598;&#21644;&#26631;&#35760;&#24322;&#24120;&#35266;&#27979;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#26377;&#38480;&#24418;&#24335;&#30340;&#24322;&#24120;&#27979;&#37327;&#65292;&#22240;&#27492;&#19981;&#28165;&#26970;&#23427;&#20204;&#26159;&#21542;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#37117;&#26159;&#26368;&#20248;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#31216;&#20026;MadSGM&#65292;&#23427;&#32771;&#34385;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#24191;&#27867;&#30340;&#24322;&#24120;&#27979;&#37327;&#22240;&#32032;&#65306;i&#65289;&#22522;&#20110;&#37325;&#26500;&#30340;&#12289;ii&#65289;&#22522;&#20110;&#23494;&#24230;&#30340;&#21644;iii&#65289;&#22522;&#20110;&#26799;&#24230;&#30340;&#24322;&#24120;&#27979;&#37327;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26465;&#20214;&#35780;&#20998;&#32593;&#32476;&#21450;&#20854;&#21435;&#22122;&#35780;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23545;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MadSGM&#23454;&#29616;&#20102;&#26368;&#24378;&#22823;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The time-series anomaly detection is one of the most fundamental tasks for time-series. Unlike the time-series forecasting and classification, the time-series anomaly detection typically requires unsupervised (or self-supervised) training since collecting and labeling anomalous observations are difficult. In addition, most existing methods resort to limited forms of anomaly measurements and therefore, it is not clear whether they are optimal in all circumstances. To this end, we present a multivariate time-series anomaly detector based on score-based generative models, called MadSGM, which considers the broadest ever set of anomaly measurement factors: i) reconstruction-based, ii) density-based, and iii) gradient-based anomaly measurements. We also design a conditional score network and its denoising score matching loss for the time-series anomaly detection. Experiments on five real-world benchmark datasets illustrate that MadSGM achieves the most robust and accurate predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#22686;&#24378;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#36866;&#24403;&#30340;&#32452;&#21512;&#65292;&#20998;&#26512;&#24182;&#21387;&#32553;&#27169;&#25311;&#24322;&#24120;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#19982;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#20998;&#21106;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15068</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Augmentation Framework for Anomaly Detection. (arXiv:2308.15068v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#22686;&#24378;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#36866;&#24403;&#30340;&#32452;&#21512;&#65292;&#20998;&#26512;&#24182;&#21387;&#32553;&#27169;&#25311;&#24322;&#24120;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#19982;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#20998;&#21106;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36890;&#24120;&#34987;&#25972;&#21512;&#21040;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22797;&#21046;&#30495;&#23454;&#19990;&#30028;&#30340;&#24322;&#24120;&#25110;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21040;&#24322;&#24120;&#30340;&#26631;&#20934;&#22312;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#20998;&#24067;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#37325;&#26500;&#32593;&#32476;&#35757;&#32451;&#26377;&#36129;&#29486;&#30340;&#27169;&#25311;&#24322;&#24120;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#21387;&#32553;&#25104;&#20960;&#31181;&#26041;&#27861;&#65292;&#20174;&#32780;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#36866;&#24403;&#30340;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#23558;&#36825;&#20010;&#26694;&#26550;&#19982;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#21106;&#35757;&#32451;&#31574;&#30053;&#65292;&#26082;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#21448;&#36991;&#20813;&#23545;&#37325;&#26500;&#36807;&#31243;&#24341;&#20837;&#24178;&#25200;&#12290;&#22312;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#30446;&#26631;&#30456;&#20851;&#25351;&#26631;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation methods are commonly integrated into the training of anomaly detection models. Previous approaches have primarily focused on replicating real-world anomalies or enhancing diversity, without considering that the standard of anomaly varies across different classes, potentially leading to a biased training distribution.This paper analyzes crucial traits of simulated anomalies that contribute to the training of reconstructive networks and condenses them into several methods, thus creating a comprehensive framework by selectively utilizing appropriate combinations.Furthermore, we integrate this framework with a reconstruction-based approach and concurrently propose a split training strategy that alleviates the issue of overfitting while avoiding introducing interference to the reconstruction process. The evaluations conducted on the MVTec anomaly detection dataset demonstrate that our method outperforms the previous state-of-the-art approach, particularly in terms of objec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CMInfoNet&#30340;&#26032;&#22411;&#32593;&#32476;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#30456;&#20114;&#20449;&#24687;&#29942;&#39048;&#25240;&#34935;&#65292;&#25552;&#21462;&#20855;&#26377;&#26368;&#20855;&#20195;&#34920;&#24615;&#20449;&#24687;&#30340;&#27169;&#24577;&#19981;&#21464;&#36523;&#20221;&#29305;&#24449;&#65292;&#24182;&#20943;&#23569;&#20449;&#24687;&#20887;&#20313;&#21644;&#27169;&#24577;&#34917;&#20805;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15063</link><description>&lt;p&gt;
&#23398;&#20064;&#24322;&#26500;&#20154;&#21592;&#20877;&#35782;&#21035;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#29942;&#39048;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Cross-modality Information Bottleneck Representation for Heterogeneous Person Re-Identification. (arXiv:2308.15063v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CMInfoNet&#30340;&#26032;&#22411;&#32593;&#32476;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#30456;&#20114;&#20449;&#24687;&#29942;&#39048;&#25240;&#34935;&#65292;&#25552;&#21462;&#20855;&#26377;&#26368;&#20855;&#20195;&#34920;&#24615;&#20449;&#24687;&#30340;&#27169;&#24577;&#19981;&#21464;&#36523;&#20221;&#29305;&#24449;&#65292;&#24182;&#20943;&#23569;&#20449;&#24687;&#20887;&#20313;&#21644;&#27169;&#24577;&#34917;&#20805;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35265;&#20809;&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;&#65288;VI-ReID&#65289;&#26159;&#26234;&#33021;&#35270;&#39057;&#30417;&#25511;&#20013;&#19968;&#39033;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#23398;&#20064;&#19968;&#20010;&#20849;&#20139;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#20943;&#23569;&#21487;&#35265;&#20809;&#21644;&#32418;&#22806;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#28982;&#32780;&#20173;&#26377;&#20004;&#20010;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65306;&#20449;&#24687;&#20887;&#20313;&#21644;&#27169;&#24577;&#34917;&#20805;&#12290;&#20026;&#27492;&#65292;&#21512;&#29702;&#22320;&#28040;&#38500;&#19982;&#36523;&#20221;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#34917;&#20805;&#27169;&#24577;&#29305;&#23450;&#30340;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#65292;&#19988;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30456;&#20114;&#20449;&#24687;&#21644;&#27169;&#24577;&#19968;&#33268;&#24615;&#32593;&#32476;&#65292;&#21363;CMInfoNet&#65292;&#20197;&#25552;&#21462;&#20855;&#26377;&#26368;&#20855;&#20195;&#34920;&#24615;&#20449;&#24687;&#30340;&#27169;&#24577;&#19981;&#21464;&#36523;&#20221;&#29305;&#24449;&#24182;&#20943;&#23569;&#20887;&#20313;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#25214;&#21040;&#19968;&#31181;&#26368;&#20339;&#34920;&#31034;&#26469;&#25429;&#25417;&#26356;&#22810;&#19982;&#36523;&#20221;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#30456;&#20114;&#20449;&#24687;&#29942;&#39048;&#25240;&#34935;&#26469;&#21387;&#32553;&#26080;&#20851;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#27169;&#24577;&#34917;&#20805;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visible-Infrared person re-identification (VI-ReID) is an important and challenging task in intelligent video surveillance. Existing methods mainly focus on learning a shared feature space to reduce the modality discrepancy between visible and infrared modalities, which still leave two problems underexplored: information redundancy and modality complementarity. To this end, properly eliminating the identity-irrelevant information as well as making up for the modality-specific information are critical and remains a challenging endeavor. To tackle the above problems, we present a novel mutual information and modality consensus network, namely CMInfoNet, to extract modality-invariant identity features with the most representative information and reduce the redundancies. The key insight of our method is to find an optimal representation to capture more identity-relevant information and compress the irrelevant parts by optimizing a mutual information bottleneck trade-off. Besides, we propos
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#26500;&#24314;&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;&#36827;&#34892;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#21644;&#25991;&#26412;&#23545;&#35805;&#31995;&#32479;&#23454;&#29616;&#20102;&#25554;&#27133;&#21644;&#20540;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.15053</link><description>&lt;p&gt;
&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;
&lt;/p&gt;
&lt;p&gt;
Adapting text-based dialogue state tracker for spoken dialogues. (arXiv:2308.15053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#26500;&#24314;&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;&#36827;&#34892;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#21644;&#25991;&#26412;&#23545;&#35805;&#31995;&#32479;&#23454;&#29616;&#20102;&#25554;&#27133;&#21644;&#20540;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36890;&#36807;&#23545;&#35805;&#31995;&#32479;&#25216;&#26415;&#31454;&#36187;&#65288;DSTC&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#35821;&#38899;&#30028;&#38754;&#30340;&#31283;&#20581;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22823;&#37096;&#20998;&#36827;&#23637;&#37117;&#26159;&#38024;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#22240;&#20026;&#26377;&#20016;&#23500;&#30340;&#20070;&#38754;&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#65292;&#32780;&#20855;&#26377;&#21475;&#35821;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#38750;&#24120;&#31232;&#32570;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;Siri&#21644;Alexa&#31561;&#35821;&#38899;&#21161;&#25163;&#31995;&#32479;&#25152;&#23637;&#31034;&#30340;&#65292;&#23558;&#36825;&#31181;&#25104;&#21151;&#36716;&#31227;&#21040;&#21475;&#35821;&#23545;&#35805;&#20013;&#20855;&#26377;&#23454;&#38469;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;DSTC11&#30340;&#20855;&#26377;&#35821;&#38899;&#24863;&#30693;&#30340;&#23545;&#35805;&#31995;&#32479;&#25216;&#26415;&#25361;&#25112;&#36187;&#20013;&#30340;&#39640;&#24230;&#25104;&#21151;&#27169;&#22411;&#30340;&#24037;&#31243;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65306;&#65288;1&#65289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#65292;&#20197;&#24357;&#21512;&#21475;&#35821;&#21644;&#25991;&#26412;&#35805;&#35821;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#65288;2&#65289;&#29992;&#20110;&#20272;&#35745;&#25554;&#27133;&#21644;&#20540;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#23545;&#35805;&#31995;&#32479;&#65288;D3ST&#65289;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#25554;&#27133;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although there have been remarkable advances in dialogue systems through the dialogue systems technology competition (DSTC), it remains one of the key challenges to building a robust task-oriented dialogue system with a speech interface. Most of the progress has been made for text-based dialogue systems since there are abundant datasets with written corpora while those with spoken dialogues are very scarce. However, as can be seen from voice assistant systems such as Siri and Alexa, it is of practical importance to transfer the success to spoken dialogues. In this paper, we describe our engineering effort in building a highly successful model that participated in the speech-aware dialogue systems technology challenge track in DSTC11. Our model consists of three major modules: (1) automatic speech recognition error correction to bridge the gap between the spoken and the text utterances, (2) text-based dialogue system (D3ST) for estimating the slots and values using slot descriptions, an
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#24179;&#34913;&#24863;&#30693;&#24335;&#25151;&#38388;&#24067;&#23616;&#20272;&#35745;&#65288;iBARLE&#65289;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#22806;&#35266;&#21464;&#21270;&#29983;&#25104;&#27169;&#22359;&#12289;&#22797;&#26434;&#32467;&#26500;&#28151;&#21512;&#27169;&#22359;&#21644;&#26799;&#24230;-based&#24067;&#23616;&#30446;&#26631;&#20989;&#25968;&#65292;&#26088;&#22312;&#35299;&#20915;&#25151;&#38388;&#24067;&#23616;&#20272;&#35745;&#20013;&#30340;&#19981;&#24179;&#34913;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15050</link><description>&lt;p&gt;
iBARLE&#65306;&#24179;&#34913;&#24863;&#30693;&#24335;&#25151;&#38388;&#24067;&#23616;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
iBARLE: imBalance-Aware Room Layout Estimation. (arXiv:2308.15050v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15050
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#24179;&#34913;&#24863;&#30693;&#24335;&#25151;&#38388;&#24067;&#23616;&#20272;&#35745;&#65288;iBARLE&#65289;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#22806;&#35266;&#21464;&#21270;&#29983;&#25104;&#27169;&#22359;&#12289;&#22797;&#26434;&#32467;&#26500;&#28151;&#21512;&#27169;&#22359;&#21644;&#26799;&#24230;-based&#24067;&#23616;&#30446;&#26631;&#20989;&#25968;&#65292;&#26088;&#22312;&#35299;&#20915;&#25151;&#38388;&#24067;&#23616;&#20272;&#35745;&#20013;&#30340;&#19981;&#24179;&#34913;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25151;&#38388;&#24067;&#23616;&#20272;&#35745;&#26159;&#20174;&#21333;&#20010;&#20840;&#26223;&#22270;&#39044;&#27979;&#24067;&#23616;&#12290;&#23427;&#38656;&#35201;&#20855;&#26377;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#30340;&#25151;&#38388;&#24418;&#29366;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#24179;&#34913;&#65292;&#21253;&#25324;&#24067;&#23616;&#22797;&#26434;&#24230;&#30340;&#23610;&#23544;&#12289;&#30456;&#26426;&#20301;&#32622;&#21644;&#22330;&#26223;&#22806;&#35266;&#30340;&#21464;&#21270;&#12290;&#36825;&#20123;&#38382;&#39064;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24179;&#34913;&#24863;&#30693;&#24335;&#25151;&#38388;&#24067;&#23616;&#20272;&#35745;&#65288;iBARLE&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;iBARLE&#21253;&#25324;&#65288;1&#65289;&#22806;&#35266;&#21464;&#21270;&#29983;&#25104;&#65288;AVG&#65289;&#27169;&#22359;&#65292;&#20419;&#36827;&#35270;&#35273;&#22806;&#35266;&#39046;&#22495;&#30340;&#27867;&#21270;&#65292;&#65288;2&#65289;&#22797;&#26434;&#32467;&#26500;&#28151;&#21512;&#65288;CSMix&#65289;&#27169;&#22359;&#65292;&#25552;&#39640;&#23545;&#25151;&#38388;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21644;&#65288;3&#65289;&#22522;&#20110;&#26799;&#24230;&#30340;&#24067;&#23616;&#30446;&#26631;&#20989;&#25968;&#65292;&#26356;&#26377;&#25928;&#22320;&#32771;&#34385;&#22797;&#26434;&#24067;&#23616;&#20013;&#30340;&#36974;&#25377;&#12290;&#25152;&#26377;&#27169;&#22359;&#37117;&#26159;&#32852;&#21512;&#35757;&#32451;&#30340;&#65292;&#24444;&#27492;&#24110;&#21161;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#22522;&#20110;ZInD~\cite{cruz2021zillow}&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Room layout estimation predicts layouts from a single panorama. It requires datasets with large-scale and diverse room shapes to train the models. However, there are significant imbalances in real-world datasets including the dimensions of layout complexity, camera locations, and variation in scene appearance. These issues considerably influence the model training performance. In this work, we propose the imBalance-Aware Room Layout Estimation (iBARLE) framework to address these issues. iBARLE consists of (1) Appearance Variation Generation (AVG) module, which promotes visual appearance domain generalization, (2) Complex Structure Mix-up (CSMix) module, which enhances generalizability w.r.t. room structure, and (3) a gradient-based layout objective function, which allows more effective accounting for occlusions in complex layouts. All modules are jointly trained and help each other to achieve the best performance. Experiments and ablation studies based on ZInD~\cite{cruz2021zillow} dat
&lt;/p&gt;</description></item><item><title>R^3&#26159;&#19968;&#31181;&#22312;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;&#35774;&#22791;&#30340;&#23454;&#26102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21160;&#24577;&#25209;&#37327;&#22823;&#23567;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#22823;&#23567;&#30340;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#26102;&#38388;&#21644;&#31639;&#27861;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#26377;&#25928;&#22320;&#31649;&#29702;&#20102;&#20869;&#23384;&#21644;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15039</link><description>&lt;p&gt;
R^3: &#22522;&#20110;&#35774;&#22791;&#30340;&#23454;&#26102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
R^3: On-device Real-Time Deep Reinforcement Learning for Autonomous Robotics. (arXiv:2308.15039v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15039
&lt;/p&gt;
&lt;p&gt;
R^3&#26159;&#19968;&#31181;&#22312;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;&#35774;&#22791;&#30340;&#23454;&#26102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21160;&#24577;&#25209;&#37327;&#22823;&#23567;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#22823;&#23567;&#30340;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#26102;&#38388;&#21644;&#31639;&#27861;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#26377;&#25928;&#22320;&#31649;&#29702;&#20102;&#20869;&#23384;&#21644;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#26426;&#22120;&#20154;&#25628;&#25937;&#31995;&#32479;&#65292;&#38656;&#35201;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36830;&#32493;&#36866;&#24212;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#27169;&#22411;&#30340;&#39640;&#25928;&#35774;&#22791;&#19978;&#35757;&#32451;&#12290;&#26412;&#30740;&#31350;&#30340;&#22522;&#26412;&#21160;&#26426;&#26159;&#29702;&#35299;&#21644;&#24212;&#23545;&#22522;&#20110;&#35774;&#22791;&#30340;&#23454;&#26102;DRL&#30340;&#25361;&#25112;&#65292;&#36825;&#28041;&#21450;&#22312;&#20869;&#23384;&#32422;&#26463;&#19979;&#24179;&#34913;&#26102;&#38388;&#21644;&#31639;&#27861;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#12290;&#36825;&#31181;&#24494;&#22937;&#30340;&#24179;&#34913;&#38656;&#35201;&#20849;&#21516;&#20248;&#21270;DRL&#35757;&#32451;&#30340;&#20004;&#20010;&#20851;&#38190;&#21442;&#25968;--&#25209;&#37327;&#22823;&#23567;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#22823;&#23567;&#12290;&#37197;&#32622;&#36825;&#20123;&#21442;&#25968;&#23545;&#26102;&#38388;&#21644;&#31639;&#27861;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#28982;&#32780;&#20004;&#32773;&#37117;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#20869;&#23384;&#20998;&#37197;&#25165;&#33021;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;R^3&#65292;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#31649;&#29702;&#26102;&#38388;&#12289;&#20869;&#23384;&#21644;&#31639;&#27861;&#24615;&#33021;&#30340;&#25972;&#20307;&#35299;&#20915;&#26041;&#26696;&#12290;R^3&#37319;&#29992;&#65288;i&#65289;&#19968;&#20010;&#20197;&#25130;&#27490;&#26085;&#26399;&#20026;&#39537;&#21160;&#30340;&#21453;&#39304;&#24490;&#29615;&#65292;&#24102;&#26377;&#21160;&#24577;&#25209;&#37327;&#22823;&#23567;&#65292;
&lt;/p&gt;
&lt;p&gt;
Autonomous robotic systems, like autonomous vehicles and robotic search and rescue, require efficient on-device training for continuous adaptation of Deep Reinforcement Learning (DRL) models in dynamic environments. This research is fundamentally motivated by the need to understand and address the challenges of on-device real-time DRL, which involves balancing timing and algorithm performance under memory constraints, as exposed through our extensive empirical studies. This intricate balance requires co-optimizing two pivotal parameters of DRL training -- batch size and replay buffer size. Configuring these parameters significantly affects timing and algorithm performance, while both (unfortunately) require substantial memory allocation to achieve near-optimal performance.  This paper presents R^3, a holistic solution for managing timing, memory, and algorithm performance in on-device real-time DRL training. R^3 employs (i) a deadline-driven feedback loop with dynamic batch sizing for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36890;&#36807;&#21160;&#24577;&#19987;&#23478;&#20132;&#25442;&#20026;MoE&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20998;&#26512;MoE&#27169;&#22411;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#26469;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#20998;&#26512;&#20248;&#21270;&#21442;&#25968;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2308.15030</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36890;&#36807;&#21160;&#24577;&#19987;&#23478;&#20132;&#25442;&#20026;MoE&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Serving MoE Models on Resource-constrained Edge Devices via Dynamic Expert Swapping. (arXiv:2308.15030v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36890;&#36807;&#21160;&#24577;&#19987;&#23478;&#20132;&#25442;&#20026;MoE&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20998;&#26512;MoE&#27169;&#22411;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#26469;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#20998;&#26512;&#20248;&#21270;&#21442;&#25968;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;(MoE)&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26377;&#26465;&#20214;&#28608;&#27963;&#30340;&#24182;&#34892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;(&#19987;&#23478;)&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24310;&#36831;&#20851;&#38190;&#30340;&#36793;&#32536;&#22330;&#26223;&#20013;&#25552;&#20379;MoE&#27169;&#22411;&#30340;&#26381;&#21153;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#26174;&#33879;&#22686;&#21152;&#12290;&#26412;&#25991;&#39318;&#20808;&#20998;&#26512;&#20102;MoE&#27169;&#22411;&#22312;&#36830;&#32493;&#25512;&#29702;&#22330;&#26223;&#20013;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#19987;&#23478;&#28608;&#27963;&#30340;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#65292;&#21253;&#25324;&#26102;&#38388;&#23616;&#37096;&#24615;&#12289;&#21487;&#20132;&#25442;&#24615;&#21644;&#21487;&#36339;&#36807;&#35745;&#31639;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PC-MoE&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#36830;&#32493;MoE&#27169;&#22411;&#26381;&#21153;&#30340;&#25512;&#29702;&#26694;&#26550;&#12290;PC-MoE&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#31216;&#20026;&#8220;&#21442;&#25968;&#22996;&#21592;&#20250;&#8221;&#65292;&#23427;&#26234;&#33021;&#22320;&#32500;&#25252;&#19968;&#37096;&#20998;&#37325;&#35201;&#30340;&#27491;&#22312;&#20351;&#29992;&#30340;&#19987;&#23478;&#65292;&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#22522;&#20110;&#24615;&#33021;&#20998;&#26512;&#30340;&#22996;&#21592;&#20250;&#35268;&#21010;&#22120;&#65292;&#22312;&#32447;&#25214;&#21040;&#21442;&#25968;&#22996;&#21592;&#20250;&#30340;&#26368;&#20339;&#37197;&#32622;&#65292;&#24182;&#36827;&#34892;&#19987;&#23478;&#20132;&#25442;&#21644;&#35831;&#27714;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture of experts (MoE) is a popular technique in deep learning that improves model capacity with conditionally-activated parallel neural network modules (experts). However, serving MoE models in resource-constrained latency-critical edge scenarios is challenging due to the significantly increased model size and complexity. In this paper, we first analyze the behavior pattern of MoE models in continuous inference scenarios, which leads to three key observations about the expert activations, including temporal locality, exchangeability, and skippable computation. Based on these observations, we introduce PC-MoE, an inference framework for resource-constrained continuous MoE model serving. The core of PC-MoE is a new data structure, Parameter Committee, that intelligently maintains a subset of important experts in use to reduce resource consumption. The optimal configuration of Parameter Committee is found offline by a profiling-guided committee planner, and expert swapping and request 
&lt;/p&gt;</description></item><item><title>&#36882;&#24402;&#24635;&#32467;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#22312;&#38271;&#23545;&#35805;&#20013;&#35760;&#24518;&#37325;&#35201;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15022</link><description>&lt;p&gt;
&#36882;&#24402;&#24635;&#32467;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models. (arXiv:2308.15022v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15022
&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#24635;&#32467;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#22312;&#38271;&#23545;&#35805;&#20013;&#35760;&#24518;&#37325;&#35201;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#31995;&#32479;&#22312;&#38271;&#26399;&#23545;&#35805;&#20013;&#23481;&#26131;&#36951;&#24536;&#37325;&#35201;&#20449;&#24687;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#29305;&#23450;&#30340;&#26816;&#32034;&#22120;&#25110;&#24635;&#32467;&#22120;&#20174;&#36807;&#21435;&#33719;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#36825;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#19988;&#39640;&#24230;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36882;&#24402;&#29983;&#25104;&#24635;&#32467;/&#35760;&#24518;&#65292;&#20197;&#22686;&#24378;&#38271;&#26399;&#35760;&#24518;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#21050;&#28608;LLMs&#35760;&#20303;&#23567;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#36882;&#24402;&#22320;&#20351;&#29992;&#20043;&#21069;&#30340;&#35760;&#24518;&#21644;&#38543;&#21518;&#30340;&#23545;&#35805;&#20869;&#23481;&#20135;&#29983;&#26032;&#30340;&#35760;&#24518;&#12290;&#26368;&#21518;&#65292;LLM&#21487;&#20197;&#22312;&#26368;&#26032;&#35760;&#24518;&#30340;&#24110;&#21161;&#19979;&#36731;&#26494;&#29983;&#25104;&#39640;&#24230;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#21644;text-davinci-003&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#23545;&#35805;&#20013;&#21487;&#20197;&#29983;&#25104;&#26356;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23454;&#29616;LLM&#24314;&#27169;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most open-domain dialogue systems suffer from forgetting important information, especially in a long-term conversation. Existing works usually train the specific retriever or summarizer to obtain key information from the past, which is time-consuming and highly depends on the quality of labeled data. To alleviate this problem, we propose to recursively generate summaries/ memory using large language models (LLMs) to enhance long-term memory ability. Specifically, our method first stimulates LLMs to memorize small dialogue contexts and then recursively produce new memory using previous memory and following contexts. Finally, the LLM can easily generate a highly consistent response with the help of the latest memory. We evaluate our method using ChatGPT and text-davinci-003, and the experiments on the widely-used public dataset show that our method can generate more consistent responses in a long-context conversation. Notably, our method is a potential solution to enable the LLM to model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPU&#30340;&#22823;&#35268;&#27169;&#24182;&#34892;&#36830;&#32493;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#26469;&#21152;&#36895;&#28151;&#21512;SAT&#27714;&#35299;&#22120;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#26032;&#22411;&#24182;&#34892;&#31639;&#27861;&#26469;&#35745;&#31639;&#22522;&#26412;&#23545;&#31216;&#22810;&#39033;&#24335;&#65292;&#24182;&#22312;&#25628;&#32034;&#20013;&#20351;&#29992;&#37325;&#21551;&#21551;&#21457;&#24335;&#31639;&#27861;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.15020</link><description>&lt;p&gt;
&#22522;&#20110;GPU&#30340;&#28151;&#21512;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#27714;&#35299;&#22120;&#30340;&#22823;&#35268;&#27169;&#24182;&#34892;&#36830;&#32493;&#23616;&#37096;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Massively Parallel Continuous Local Search for Hybrid SAT Solving on GPUs. (arXiv:2308.15020v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15020
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPU&#30340;&#22823;&#35268;&#27169;&#24182;&#34892;&#36830;&#32493;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#26469;&#21152;&#36895;&#28151;&#21512;SAT&#27714;&#35299;&#22120;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#26032;&#22411;&#24182;&#34892;&#31639;&#27861;&#26469;&#35745;&#31639;&#22522;&#26412;&#23545;&#31216;&#22810;&#39033;&#24335;&#65292;&#24182;&#22312;&#25628;&#32034;&#20013;&#20351;&#29992;&#37325;&#21551;&#21551;&#21457;&#24335;&#31639;&#27861;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22522;&#20110;&#20914;&#31361;&#39537;&#21160;&#23376;&#21477;&#23398;&#20064;&#65288;CDCL&#65289;&#30340;&#26368;&#20808;&#36827;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65288;SAT&#65289;&#27714;&#35299;&#22120;&#22312;&#24037;&#31243;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#20854;&#39034;&#24207;&#24615;&#36136;&#38480;&#21046;&#20102;&#22312;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#65288;GPU&#65289;&#31561;&#24179;&#21488;&#19978;&#36827;&#34892;&#21152;&#36895;&#30340;&#24182;&#34892;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FastFourierSAT&#65292;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#39537;&#21160;&#36830;&#32493;&#23616;&#37096;&#25628;&#32034;&#65288;CLS&#65289;&#30340;&#39640;&#24230;&#24182;&#34892;&#28151;&#21512;SAT&#27714;&#35299;&#22120;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#21463;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#21367;&#31215;&#21551;&#21457;&#30340;&#26032;&#22411;&#24182;&#34892;&#31639;&#27861;&#26469;&#23454;&#29616;&#30340;&#65292;&#29992;&#20110;&#35745;&#31639;&#20197;&#21069;&#30340;CLS&#26041;&#27861;&#20013;&#30340;&#20027;&#35201;&#35745;&#31639;&#20219;&#21153;&#8212;&#8212;&#22522;&#26412;&#23545;&#31216;&#22810;&#39033;&#24335;&#65288;ESP&#65289;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#19982;&#26368;&#20339;&#30340;&#20197;&#21069;&#32467;&#26524;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31639;&#27861;&#22266;&#26377;&#30340;&#22823;&#35268;&#27169;&#24182;&#34892;&#24615;&#33021;&#22815;&#21033;&#29992;GPU&#36827;&#34892;&#21152;&#36895;&#65292;&#30456;&#27604;&#20197;&#21069;&#30340;CLS&#26041;&#27861;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23558;&#37325;&#21551;&#21551;&#21457;&#24335;&#31639;&#27861;&#34701;&#20837;CLS&#20197;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#24212;&#29992;&#19982;&#20854;&#20182;SAT&#27714;&#35299;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although state-of-the-art (SOTA) SAT solvers based on conflict-driven clause learning (CDCL) have achieved remarkable engineering success, their sequential nature limits the parallelism that may be extracted for acceleration on platforms such as the graphics processing unit (GPU). In this work, we propose FastFourierSAT, a highly parallel hybrid SAT solver based on gradient-driven continuous local search (CLS). This is realized by a novel parallel algorithm inspired by the Fast Fourier Transform (FFT)-based convolution for computing the elementary symmetric polynomials (ESPs), which is the major computational task in previous CLS methods. The complexity of our algorithm matches the best previous result. Furthermore, the substantial parallelism inherent in our algorithm can leverage the GPU for acceleration, demonstrating significant improvement over the previous CLS approaches. We also propose to incorporate the restart heuristics in CLS to improve search efficiency. We compare our app
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#31449;&#24335;&#26694;&#26550;NN-Factory&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#23450;&#21046;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#38024;&#23545;&#21508;&#31181;&#36793;&#32536;&#22330;&#26223;&#30340;&#36164;&#28304;&#23450;&#21046;&#21270;&#21644;&#20219;&#21153;&#23450;&#21046;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.15003</link><description>&lt;p&gt;
&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65306;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21644;&#36164;&#28304;&#38480;&#21046;&#30340;&#24555;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Generative Model for Models: Rapid DNN Customization for Diverse Tasks and Resource Constraints. (arXiv:2308.15003v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15003
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#31449;&#24335;&#26694;&#26550;NN-Factory&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#23450;&#21046;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#38024;&#23545;&#21508;&#31181;&#36793;&#32536;&#22330;&#26223;&#30340;&#36164;&#28304;&#23450;&#21046;&#21270;&#21644;&#20219;&#21153;&#23450;&#21046;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#36890;&#24120;&#22823;&#32780;&#32479;&#19968;&#30340;&#22522;&#20110;&#20113;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19981;&#21516;&#65292;&#37096;&#32626;&#22312;&#36793;&#32536;&#30340;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#36164;&#28304;&#26377;&#38480;&#29615;&#22659;&#30340;&#23450;&#21046;&#21270;&#12290;&#30001;&#20110;&#36793;&#32536;&#22330;&#26223;&#30340;&#22810;&#26679;&#24615;&#21644;&#27599;&#20010;&#22330;&#26223;&#30340;&#35757;&#32451;&#36127;&#36733;&#65292;&#36825;&#31181;&#23450;&#21046;&#21270;&#36807;&#31243;&#21487;&#33021;&#26159;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#20998;&#21035;&#23454;&#29616;&#24555;&#36895;&#38754;&#21521;&#36164;&#28304;&#30340;&#23450;&#21046;&#21270;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#23450;&#21046;&#21270;&#65292;&#20294;&#21516;&#26102;&#23454;&#29616;&#36825;&#20004;&#32773;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21463;&#21040;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#21487;&#32452;&#21512;&#24615;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NN-Factory&#65292;&#19968;&#20010;&#19968;&#31449;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#36793;&#32536;&#22330;&#26223;&#29983;&#25104;&#23450;&#21046;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#23450;&#21046;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#36827;&#34892;&#35757;&#32451;&#12290;NN-Factory&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#19968;&#20010;&#27169;&#22359;&#21270;&#36229;&#32593;&#32476;&#21644;&#21487;&#20197;&#26377;&#26465;&#20214;&#28608;&#27963;&#20197;&#23436;&#25104;&#19981;&#21516;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlike cloud-based deep learning models that are often large and uniform, edge-deployed models usually demand customization for domain-specific tasks and resource-limited environments. Such customization processes can be costly and time-consuming due to the diversity of edge scenarios and the training load for each scenario. Although various approaches have been proposed for rapid resource-oriented customization and task-oriented customization respectively, achieving both of them at the same time is challenging. Drawing inspiration from the generative AI and the modular composability of neural networks, we introduce NN-Factory, an one-for-all framework to generate customized lightweight models for diverse edge scenarios. The key idea is to use a generative model to directly produce the customized models, instead of training them. The main components of NN-Factory include a modular supernet with pretrained modules that can be conditionally activated to accomplish different tasks and a g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#26029;&#20013;&#21382;&#21490;&#20449;&#24687;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#20214;&#39044;&#27979;&#27169;&#22411;CENET&#65292;&#36890;&#36807;&#21382;&#21490;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#23398;&#20064;&#21382;&#21490;&#21644;&#38750;&#21382;&#21490;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21306;&#20998;&#26368;&#26377;&#28508;&#21147;&#30340;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2308.15002</link><description>&lt;p&gt;
&#25506;&#32034;&#21382;&#21490;&#20449;&#24687;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#26029;&#20013;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limits of Historical Information for Temporal Knowledge Graph Extrapolation. (arXiv:2308.15002v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#26029;&#20013;&#21382;&#21490;&#20449;&#24687;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#20214;&#39044;&#27979;&#27169;&#22411;CENET&#65292;&#36890;&#36807;&#21382;&#21490;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#23398;&#20064;&#21382;&#21490;&#21644;&#38750;&#21382;&#21490;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21306;&#20998;&#26368;&#26377;&#28508;&#21147;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#26159;&#34920;&#31034;&#23454;&#20307;&#20043;&#38388;&#21160;&#24577;&#20851;&#31995;&#21644;&#20132;&#20114;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#34987;&#35748;&#20026;&#26159;&#20107;&#20214;&#39044;&#27979;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#26041;&#27861;&#23384;&#22312;&#19968;&#20010;&#23616;&#38480;&#24615;&#65292;&#23601;&#26159;&#36807;&#20110;&#20381;&#36182;&#20107;&#20214;&#30340;&#37325;&#22797;&#24615;&#25110;&#21608;&#26399;&#24615;&#65292;&#36825;&#32473;&#25512;&#26029;&#19982;&#32570;&#20047;&#21382;&#21490;&#20132;&#20114;&#30340;&#23454;&#20307;&#30456;&#20851;&#30340;&#26410;&#26469;&#20107;&#20214;&#24102;&#26469;&#25361;&#25112;&#12290;&#20107;&#23454;&#19978;&#65292;&#24403;&#21069;&#20107;&#24577;&#24448;&#24448;&#26159;&#21382;&#21490;&#20449;&#24687;&#21644;&#19981;&#30452;&#25509;&#21487;&#35266;&#23519;&#30340;&#28508;&#22312;&#22240;&#32032;&#30340;&#32452;&#21512;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21382;&#21490;&#20449;&#24687;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#26029;&#20013;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#20214;&#39044;&#27979;&#27169;&#22411;&#65292;&#31216;&#20026;&#23545;&#27604;&#20107;&#20214;&#32593;&#32476;&#65288;CENET&#65289;&#65292;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#21382;&#21490;&#23545;&#27604;&#23398;&#20064;&#30340;&#35757;&#32451;&#26694;&#26550;&#12290;CENET&#36890;&#36807;&#23398;&#20064;&#21382;&#21490;&#21644;&#38750;&#21382;&#21490;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21306;&#20998;&#26368;&#26377;&#28508;&#21147;&#30340;&#23454;&#20307;&#65292;&#20197;&#26368;&#20339;&#21305;&#37197;&#32473;&#23450;&#30340;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal knowledge graphs, representing the dynamic relationships and interactions between entities over time, have been identified as a promising approach for event forecasting. However, a limitation of most temporal knowledge graph reasoning methods is their heavy reliance on the recurrence or periodicity of events, which brings challenges to inferring future events related to entities that lack historical interaction. In fact, the current state of affairs is often the result of a combination of historical information and underlying factors that are not directly observable. To this end, we investigate the limits of historical information for temporal knowledge graph extrapolation and propose a new event forecasting model called Contrastive Event Network (CENET) based on a novel training framework of historical contrastive learning. CENET learns both the historical and non-historical dependency to distinguish the most potential entities that best match the given query. Simultaneously,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31070;&#32463;&#21551;&#21457;&#30340;&#36866;&#24212;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#27169;&#25311;&#26524;&#34631;&#23398;&#20064;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#21464;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#23398;&#20064;&#21487;&#22609;&#24615;&#65292;&#24182;&#30830;&#20445;&#35299;&#20915;&#26041;&#26696;&#30340;&#20860;&#23481;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14991</link><description>&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#24341;&#20837;&#31070;&#32463;&#21551;&#21457;&#30340;&#36866;&#24212;&#24615;&#65292;&#20197;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incorporating Neuro-Inspired Adaptability for Continual Learning in Artificial Intelligence. (arXiv:2308.14991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31070;&#32463;&#21551;&#21457;&#30340;&#36866;&#24212;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#27169;&#25311;&#26524;&#34631;&#23398;&#20064;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#21464;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#23398;&#20064;&#21487;&#22609;&#24615;&#65292;&#24182;&#30830;&#20445;&#35299;&#20915;&#26041;&#26696;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#36171;&#20104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#24378;&#22823;&#36866;&#24212;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#19968;&#20010;&#29702;&#24819;&#30340;&#35299;&#20915;&#26041;&#26696;&#24212;&#35813;&#22312;&#35760;&#24518;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21487;&#22609;&#24615;&#20043;&#38388;&#20445;&#25345;&#36866;&#24403;&#24179;&#34913;&#65292;&#24182;&#33719;&#24471;&#36275;&#22815;&#30340;&#20860;&#23481;&#24615;&#26469;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#20998;&#24067;&#12290;&#29616;&#26377;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#20445;&#25345;&#35760;&#24518;&#31283;&#23450;&#24615;&#20197;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20294;&#20173;&#38590;&#20197;&#20687;&#29983;&#29289;&#26234;&#33021;&#65288;BI&#65289;&#37027;&#26679;&#28789;&#27963;&#22320;&#36866;&#24212;&#22686;&#37327;&#21464;&#21270;&#12290;&#36890;&#36807;&#24314;&#27169;&#19968;&#20010;&#33021;&#22815;&#20027;&#21160;&#35843;&#33410;&#36951;&#24536;&#30340;&#31283;&#20581;&#26524;&#34631;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#22810;&#20010;&#23398;&#20064;&#27169;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21442;&#25968;&#20998;&#24067;&#20013;&#36866;&#24403;&#34928;&#20943;&#26087;&#35760;&#24518;&#26469;&#25913;&#21892;&#23398;&#20064;&#21487;&#22609;&#24615;&#65292;&#24182;&#30456;&#24212;&#22320;&#21327;&#35843;&#22810;&#23398;&#20064;&#32773;&#26550;&#26500;&#26469;&#30830;&#20445;&#35299;&#20915;&#26041;&#26696;&#30340;&#20860;&#23481;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#26126;&#26174;&#25552;&#39640;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#31361;&#35302;&#35843;&#33410;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning aims to empower artificial intelligence (AI) with strong adaptability to the real world. For this purpose, a desirable solution should properly balance memory stability with learning plasticity, and acquire sufficient compatibility to capture the observed distributions. Existing advances mainly focus on preserving memory stability to overcome catastrophic forgetting, but remain difficult to flexibly accommodate incremental changes as biological intelligence (BI) does. By modeling a robust Drosophila learning system that actively regulates forgetting with multiple learning modules, here we propose a generic approach that appropriately attenuates old memories in parameter distributions to improve learning plasticity, and accordingly coordinates a multi-learner architecture to ensure solution compatibility. Through extensive theoretical and empirical validation, our approach not only clearly enhances the performance of continual learning, especially over synaptic regula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#37327;&#26500;&#24314;&#23398;&#20064;&#21644;&#38598;&#25104;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28378;&#21160;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#19978;&#23454;&#26045;&#22686;&#37327;&#23398;&#20064;&#65292;&#32467;&#21512;&#20113;&#29305;&#24449;&#25552;&#21462;&#21644;&#23567;&#27874;&#21253;&#20998;&#35299;&#65292;&#25552;&#39640;&#20102;&#25925;&#38556;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14983</link><description>&lt;p&gt;
&#22522;&#20110;&#22686;&#37327;&#26500;&#24314;&#23398;&#20064;&#21644;&#38598;&#25104;&#22495;&#36866;&#24212;&#30340;&#28378;&#21160;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Constructive Incremental Learning for Fault Diagnosis of Rolling Bearings with Ensemble Domain Adaptation. (arXiv:2308.14983v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#37327;&#26500;&#24314;&#23398;&#20064;&#21644;&#38598;&#25104;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28378;&#21160;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#19978;&#23454;&#26045;&#22686;&#37327;&#23398;&#20064;&#65292;&#32467;&#21512;&#20113;&#29305;&#24449;&#25552;&#21462;&#21644;&#23567;&#27874;&#21253;&#20998;&#35299;&#65292;&#25552;&#39640;&#20102;&#25925;&#38556;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#28378;&#21160;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#22312;&#21508;&#31181;&#24037;&#20917;&#19979;&#30340;&#23454;&#38469;&#38382;&#39064;&#26222;&#36941;&#23384;&#22312;&#65292;&#26679;&#26412;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#22686;&#21152;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#22806;&#37096;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21644;&#28378;&#21160;&#36724;&#25215;&#30340;&#32467;&#26500;&#32463;&#24120;&#34920;&#29616;&#20986;&#20855;&#26377;&#38543;&#26426;&#24615;&#21644;&#27169;&#31946;&#24615;&#30340;&#25925;&#38556;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#25925;&#38556;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#65292;&#24182;&#38480;&#21046;&#20102;&#25925;&#38556;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26500;&#36896;&#22686;&#37327;&#23398;&#20064;&#30340;&#38598;&#25104;&#22495;&#36866;&#24212;&#65288;CIL-EDA&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#22312;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCN&#65289;&#19978;&#23454;&#26045;&#65292;&#20197;&#22312;&#22810;&#20010;&#22495;&#20013;&#26500;&#24314;&#20854;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#37319;&#29992;&#20113;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#32467;&#21512;&#23567;&#27874;&#21253;&#20998;&#35299;&#65288;WPD&#65289;&#26469;&#25429;&#25417;&#26469;&#33258;&#22810;&#20010;&#20998;&#36776;&#29575;&#26041;&#38754;&#30340;&#25925;&#38556;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#38543;&#21518;&#65292;&#37319;&#29992;&#22686;&#37327;&#26500;&#24314;&#23398;&#20064;&#30340;&#22495;&#36866;&#24212;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the prevalence of rolling bearing fault diagnosis as a practical issue across various working conditions, the limited availability of samples compounds the challenge. Additionally, the complexity of the external environment and the structure of rolling bearings often manifests faults characterized by randomness and fuzziness, hindering the effective extraction of fault characteristics and restricting the accuracy of fault diagnosis. To overcome these problems, this paper presents a novel approach termed constructive Incremental learning-based ensemble domain adaptation (CIL-EDA) approach. Specifically, it is implemented on stochastic configuration networks (SCN) to constructively improve its adaptive performance in multi-domains. Concretely, a cloud feature extraction method is employed in conjunction with wavelet packet decomposition (WPD) to capture the uncertainty of fault information from multiple resolution aspects. Subsequently, constructive Incremental learning-based domai
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31895;&#30053;&#26631;&#27880;&#30340;&#22825;&#25991;&#35270;&#39057;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#26631;&#27880;&#36136;&#37327;&#21644;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#20943;&#23569;&#26631;&#27880;&#36807;&#31243;&#20013;&#30340;&#26102;&#38388;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2308.14976</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#25928;&#23545;&#22826;&#38451;&#36890;&#37327;&#28436;&#21270;&#35270;&#39057;&#36827;&#34892;&#26631;&#27880;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient labeling of solar flux evolution videos by a deep learning model. (arXiv:2308.14976v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14976
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31895;&#30053;&#26631;&#27880;&#30340;&#22825;&#25991;&#35270;&#39057;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#26631;&#27880;&#36136;&#37327;&#21644;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#20943;&#23569;&#26631;&#27880;&#36807;&#31243;&#20013;&#30340;&#26102;&#38388;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27491;&#25104;&#20026;&#23545;&#22823;&#22411;&#22797;&#26434;&#25968;&#25454;&#36827;&#34892;&#38382;&#35810;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#26631;&#27880;&#65292;&#21363;&#28155;&#21152;&#26377;&#24847;&#20041;&#30340;&#27880;&#37322;&#30340;&#36807;&#31243;&#65292;&#26159;&#30417;&#30563;&#24335;ML&#30340;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#25968;&#25454;&#38598;&#38750;&#24120;&#32791;&#26102;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21487;&#20197;&#21033;&#29992;&#31895;&#30053;&#26631;&#27880;&#30340;&#22825;&#25991;&#35270;&#39057;&#26469;&#25552;&#39640;&#25968;&#25454;&#26631;&#27880;&#36136;&#37327;&#65292;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#20351;&#29992;&#22826;&#38451;&#30913;&#22330;&#30340;&#35270;&#39057;&#65292;&#26681;&#25454;&#23427;&#20204;&#22312;&#22826;&#38451;&#30424;&#19978;&#39318;&#27425;&#26816;&#27979;&#21040;&#30340;&#21452;&#26497;&#30913;&#21306;&#65288;BMRs&#65289;&#30340;&#20986;&#29616;&#25110;&#38750;&#20986;&#29616;&#23558;&#20854;&#31895;&#30053;&#26631;&#35760;&#25104;&#20004;&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#20351;&#29992;&#31895;&#30053;&#26631;&#31614;&#35757;&#32451;CNN&#65292;&#25163;&#21160;&#39564;&#35777;&#21644;&#32416;&#27491;CNN&#19982;&#26631;&#27880;&#30340;&#19981;&#19968;&#33268;&#20043;&#22788;&#65292;&#24182;&#37325;&#22797;&#27492;&#36807;&#31243;&#30452;&#33267;&#25910;&#25947;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#37327;&#20986;&#29616;&#30340;&#26631;&#27880;&#26159;&#25163;&#21160;&#23436;&#25104;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#24471;&#21040;&#30340;&#39640;&#36136;&#37327;&#26631;&#27880;&#25968;&#25454;&#38598;&#21487;&#20197;&#23558;&#20154;&#24037;&#39564;&#35777;&#30340;&#38656;&#27714;&#38477;&#20302;50%&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#36880;&#28176;&#23631;&#34109;&#25481;&#26631;&#31614;&#65292;&#25105;&#20204;&#21487;&#20197;&#36880;&#27493;&#25552;&#39640;CNN&#22312;&#20934;&#30830;&#26631;&#27880;&#36807;&#31243;&#20013;&#30340;&#33258;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) is becoming a critical tool for interrogation of large complex data. Labeling, defined as the process of adding meaningful annotations, is a crucial step of supervised ML. However, labeling datasets is time consuming. Here we show that convolutional neural networks (CNNs), trained on crudely labeled astronomical videos, can be leveraged to improve the quality of data labeling and reduce the need for human intervention. We use videos of the solar magnetic field, crudely labeled into two classes: emergence or non-emergence of bipolar magnetic regions (BMRs), based on their first detection on the solar disk. We train CNNs using crude labels, manually verify, correct labeling vs. CNN disagreements, and repeat this process until convergence. Traditionally, flux emergence labelling is done manually. We find that a high-quality labeled dataset, derived through this iterative process, reduces the necessary manual verification by 50%. Furthermore, by gradually masking the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20154;&#26426;&#21512;&#20316;&#25805;&#20316;&#26694;&#26550;&#65292;&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#21644;&#29615;&#22659;&#24863;&#30693;&#65292;&#23558;&#39640;&#32423;&#35821;&#35328;&#21629;&#20196;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#30340;&#36816;&#21160;&#20989;&#25968;&#12290;&#37319;&#29992;&#36828;&#31243;&#25805;&#20316;&#21644;&#21160;&#24577;&#36816;&#21160;&#21407;&#29702;&#36827;&#34892;&#21160;&#20316;&#30699;&#27491;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#23454;&#29992;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14972</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#20154;&#26426;&#21512;&#20316;&#25805;&#20316;&#26694;&#26550;&#29992;&#20110;&#25805;&#32437;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
LLM-Based Human-Robot Collaboration Framework for Manipulation Tasks. (arXiv:2308.14972v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20154;&#26426;&#21512;&#20316;&#25805;&#20316;&#26694;&#26550;&#65292;&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#21644;&#29615;&#22659;&#24863;&#30693;&#65292;&#23558;&#39640;&#32423;&#35821;&#35328;&#21629;&#20196;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#30340;&#36816;&#21160;&#20989;&#25968;&#12290;&#37319;&#29992;&#36828;&#31243;&#25805;&#20316;&#21644;&#21160;&#24577;&#36816;&#21160;&#21407;&#29702;&#36827;&#34892;&#21160;&#20316;&#30699;&#27491;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#23454;&#29992;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#33258;&#20027;&#26426;&#22120;&#20154;&#30340;&#25805;&#32437;&#33021;&#21147;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#23558;&#39640;&#32423;&#35821;&#35328;&#21629;&#20196;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#30340;&#36816;&#21160;&#20989;&#25968;&#24207;&#21015;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#23558;LLM&#30340;&#20248;&#21183;&#19982;&#22522;&#20110;YOLO&#30340;&#29615;&#22659;&#24863;&#30693;&#30456;&#32467;&#21512;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26681;&#25454;&#32473;&#23450;&#30340;&#21629;&#20196;&#33258;&#20027;&#36827;&#34892;&#21512;&#29702;&#30340;&#20915;&#31574;&#21644;&#20219;&#21153;&#35268;&#21010;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;LLM&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#20934;&#30830;&#24615;&#25110;&#19981;&#21512;&#36923;&#36753;&#30340;&#34892;&#20026;&#65292;&#37319;&#29992;&#20102;&#36828;&#31243;&#25805;&#20316;&#21644;&#21160;&#24577;&#36816;&#21160;&#21407;&#29702;&#65288;DMP&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#36827;&#34892;&#21160;&#20316;&#30699;&#27491;&#12290;&#36825;&#31181;&#38598;&#25104;&#26088;&#22312;&#25552;&#39640;&#22522;&#20110;LLM&#30340;&#20154;&#26426;&#21512;&#20316;&#31995;&#32479;&#30340;&#23454;&#29992;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to enhance autonomous robotic manipulation using the Large Language Model (LLM) for logical inference, converting high-level language commands into sequences of executable motion functions. The proposed system combines the advantage of LLM with YOLO-based environmental perception to enable robots to autonomously make reasonable decisions and task planning based on the given commands. Additionally, to address the potential inaccuracies or illogical actions arising from LLM, a combination of teleoperation and Dynamic Movement Primitives (DMP) is employed for action correction. This integration aims to improve the practicality and generalizability of the LLM-based human-robot collaboration system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#24320;&#25918;&#38598;&#35328;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;MFCC&#21644;&#38899;&#39640;&#29305;&#24449;&#65292;TDNN&#27169;&#22411;&#25552;&#21462;&#29305;&#24449;&#23884;&#20837;&#65292;&#35774;&#32622;&#32622;&#20449;&#24230;&#38408;&#20540;&#65292;&#20197;&#21450;&#20351;&#29992;LDA&#21644;pLDA&#23398;&#20064;&#26032;&#30340;&#26410;&#30693;&#35821;&#35328;&#20998;&#31867;&#26469;&#23454;&#29616;&#12290;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#35328;&#19978;&#65292;&#31995;&#32479;&#20934;&#30830;&#29575;&#36798;&#21040;91.76%&#65292;&#24182;&#19988;&#20855;&#22791;&#23454;&#26102;&#36866;&#24212;&#26410;&#30693;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.14951</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#24320;&#25918;&#38598;&#35328;&#35821;&#35782;&#21035;&#21644;CU MultiLang&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Robust Open-Set Spoken Language Identification and the CU MultiLang Dataset. (arXiv:2308.14951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#24320;&#25918;&#38598;&#35328;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;MFCC&#21644;&#38899;&#39640;&#29305;&#24449;&#65292;TDNN&#27169;&#22411;&#25552;&#21462;&#29305;&#24449;&#23884;&#20837;&#65292;&#35774;&#32622;&#32622;&#20449;&#24230;&#38408;&#20540;&#65292;&#20197;&#21450;&#20351;&#29992;LDA&#21644;pLDA&#23398;&#20064;&#26032;&#30340;&#26410;&#30693;&#35821;&#35328;&#20998;&#31867;&#26469;&#23454;&#29616;&#12290;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#35328;&#19978;&#65292;&#31995;&#32479;&#20934;&#30830;&#29575;&#36798;&#21040;91.76%&#65292;&#24182;&#19988;&#20855;&#22791;&#23454;&#26102;&#36866;&#24212;&#26410;&#30693;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#35328;&#35821;&#35782;&#21035;&#27169;&#22411;&#26159;&#38381;&#38598;&#30340;&#65292;&#21363;&#23427;&#20204;&#21482;&#33021;&#36755;&#20986;&#23427;&#20204;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#31867;&#21035;&#38598;&#21512;&#20013;&#30340;&#35821;&#35328;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#24320;&#25918;&#38598;&#35328;&#35821;&#35782;&#21035;&#31995;&#32479;&#20855;&#22791;&#26816;&#27979;&#36755;&#20837;&#26159;&#21542;&#19981;&#23646;&#20110;&#21407;&#22987;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#38598;&#35328;&#35821;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;MFCC&#21644;&#38899;&#39640;&#29305;&#24449;&#65292; TDNN&#27169;&#22411;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#36890;&#36807;&#23545;softmax&#36755;&#20986;&#36827;&#34892;&#32622;&#20449;&#24230;&#38408;&#20540;&#22788;&#29702;&#65292;&#20197;&#21450;&#20351;&#29992;LDA&#21644;pLDA&#23398;&#20064;&#23545;&#26032;&#30340;&#26410;&#30693;&#35821;&#35328;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35328;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#20854;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#35328;&#19978;&#23454;&#29616;&#20102;91.76%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#20855;&#22791;&#23454;&#26102;&#36866;&#24212;&#26410;&#30693;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;CU MultiLang&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most state-of-the-art spoken language identification models are closed-set; in other words, they can only output a language label from the set of classes they were trained on. Open-set spoken language identification systems, however, gain the ability to detect when an input exhibits none of the original languages. In this paper, we implement a novel approach to open-set spoken language identification that uses MFCC and pitch features, a TDNN model to extract meaningful feature embeddings, confidence thresholding on softmax outputs, and LDA and pLDA for learning to classify new unknown languages. We present a spoken language identification system that achieves 91.76% accuracy on trained languages and has the capability to adapt to unknown languages on the fly. To that end, we also built the CU MultiLang Dataset, a large and diverse multilingual speech corpus which was used to train and evaluate our system.
&lt;/p&gt;</description></item><item><title>Transfusor&#27169;&#22411;&#26159;&#19968;&#20010;&#20351;&#29992;Transformer&#21644;Diffusor&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#39640;&#36895;&#20844;&#36335;&#22330;&#26223;&#20013;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#19988;&#21487;&#25511;&#30340;&#31867;&#20154;&#25442;&#36947;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2308.14943</link><description>&lt;p&gt;
Transfusor&#65306;&#29992;&#20110;&#21487;&#25511;&#20154;&#31867;&#33324;&#29983;&#25104;&#36710;&#36742;&#25442;&#36947;&#36712;&#36857;&#30340;Transformer Diffusor
&lt;/p&gt;
&lt;p&gt;
Transfusor: Transformer Diffusor for Controllable Human-like Generation of Vehicle Lane Changing Trajectories. (arXiv:2308.14943v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14943
&lt;/p&gt;
&lt;p&gt;
Transfusor&#27169;&#22411;&#26159;&#19968;&#20010;&#20351;&#29992;Transformer&#21644;Diffusor&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#39640;&#36895;&#20844;&#36335;&#22330;&#26223;&#20013;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#19988;&#21487;&#25511;&#30340;&#31867;&#20154;&#25442;&#36947;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#19981;&#26029;&#21457;&#23637;&#21644;&#37096;&#32626;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#30740;&#31350;&#20154;&#21592;&#19981;&#26029;&#23547;&#27714;&#21487;&#38752;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#34394;&#25311;&#27169;&#25311;&#27979;&#35797;&#65288;VST&#65289;&#24050;&#32463;&#25104;&#20026;&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65288;ADS&#65289;&#21644;&#39640;&#32423;&#39550;&#39542;&#21592;&#36741;&#21161;&#31995;&#32479;&#65288;ADAS&#65289;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#24555;&#36895;&#25191;&#34892;&#12289;&#20302;&#25104;&#26412;&#21644;&#39640;&#37325;&#22797;&#24615;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#27169;&#25311;&#30340;&#23454;&#39564;&#30340;&#25104;&#21151;&#31243;&#24230;&#20005;&#37325;&#20381;&#36182;&#20110;&#27979;&#35797;&#22330;&#26223;&#30340;&#36924;&#30495;&#31243;&#24230;&#12290;&#20026;&#20102;&#22686;&#21152;ADS&#21644;ADAS&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#38656;&#35201;&#22312;VST&#20013;&#21019;&#24314;&#26356;&#28789;&#27963;&#21644;&#39640;&#20445;&#30495;&#24230;&#30340;&#27979;&#35797;&#22330;&#26223;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;Transfusor&#8221;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;Transformer&#21644;Diffusor&#27169;&#22411;&#65288;&#20004;&#31181;&#21069;&#27839;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#25216;&#26415;&#65289;&#12290;Transfusor&#27169;&#22411;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#39640;&#36895;&#20844;&#36335;&#22330;&#26223;&#20013;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#19988;&#21487;&#25511;&#30340;&#31867;&#20154;&#25442;&#36947;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
With ongoing development of autonomous driving systems and increasing desire for deployment, researchers continue to seek reliable approaches for ADS systems. The virtual simulation test (VST) has become a prominent approach for testing autonomous driving systems (ADS) and advanced driver assistance systems (ADAS) due to its advantages of fast execution, low cost, and high repeatability. However, the success of these simulation-based experiments heavily relies on the realism of the testing scenarios. It is needed to create more flexible and high-fidelity testing scenarios in VST in order to increase the safety and reliabilityof ADS and ADAS.To address this challenge, this paper introduces the "Transfusor" model, which leverages the transformer and diffusor models (two cutting-edge deep learning generative technologies). The primary objective of the Transfusor model is to generate highly realistic and controllable human-like lane-changing trajectories in highway scenarios. Extensive exp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoSAM Adapter&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#23545;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.14936</link><description>&lt;p&gt;
&#20026;&#31227;&#21160;&#21451;&#22909;&#30340;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#33258;&#21160;&#25552;&#31034;SAM
&lt;/p&gt;
&lt;p&gt;
Auto-Prompting SAM for Mobile Friendly 3D Medical Image Segmentation. (arXiv:2308.14936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14936
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoSAM Adapter&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#23545;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#24050;&#32463;&#34987;&#36805;&#36895;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#38500;&#20102;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#22806;&#65292;2D&#21644;3D&#22270;&#20687;&#20043;&#38388;&#30340;&#31354;&#38388;&#24067;&#23616;&#24046;&#24322;&#65292;&#24378;&#22823;&#30340;GPU&#26381;&#21153;&#22120;&#25152;&#24102;&#26469;&#30340;&#22823;&#37327;&#35745;&#31639;&#36127;&#25285;&#65292;&#20197;&#21450;&#32791;&#26102;&#30340;&#25163;&#21160;&#25552;&#31034;&#29983;&#25104;&#20351;&#24471;SAM&#26080;&#27861;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;AutoSAM Adapter&#65292;&#19987;&#20026;3D&#22810;&#22120;&#23448;CT&#20998;&#21106;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#20197;&#20419;&#36827;&#23558;SAM&#27169;&#22411;&#30340;&#33021;&#21147;&#36716;&#21270;&#20026;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) has rapidly been adopted for segmenting a wide range of natural images. However, recent studies have indicated that SAM exhibits subpar performance on 3D medical image segmentation tasks. In addition to the domain gaps between natural and medical images, disparities in the spatial arrangement between 2D and 3D images, the substantial computational burden imposed by powerful GPU servers, and the time-consuming manual prompt generation impede the extension of SAM to a broader spectrum of medical image segmentation applications. To address these challenges, in this work, we introduce a novel method, AutoSAM Adapter, designed specifically for 3D multi-organ CT-based segmentation. We employ parameter-efficient adaptation techniques in developing an automatic prompt learning paradigm to facilitate the transformation of the SAM model's capabilities to 3D medical image segmentation, eliminating the need for manually generated prompts. Furthermore, we effectivel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#32959;&#30244;&#29983;&#38271;&#21644;&#27835;&#30103;&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#8220;&#22823;&#25968;&#25454;&#8221;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26426;&#26800;&#27169;&#22411;&#20197;&#21450;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#12290;&#21516;&#26102;&#25351;&#20986;&#20102;&#24403;&#21069;&#32959;&#30244;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14925</link><description>&lt;p&gt;
&#24739;&#32773;&#29305;&#24322;&#24615;&#30340;&#26426;&#26800;&#27169;&#22411;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#25968;&#25454;&#30340;&#32959;&#30244;&#29983;&#38271;
&lt;/p&gt;
&lt;p&gt;
Patient-specific, mechanistic models of tumor growth incorporating artificial intelligence and big data. (arXiv:2308.14925v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#32959;&#30244;&#29983;&#38271;&#21644;&#27835;&#30103;&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#8220;&#22823;&#25968;&#25454;&#8221;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26426;&#26800;&#27169;&#22411;&#20197;&#21450;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#12290;&#21516;&#26102;&#25351;&#20986;&#20102;&#24403;&#21069;&#32959;&#30244;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36807;&#21435;&#21313;&#24180;&#20013;&#30284;&#30151;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#31649;&#29702;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24694;&#24615;&#32959;&#30244;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#20844;&#20849;&#20581;&#24247;&#38382;&#39064;&#12290;&#20010;&#24615;&#21270;&#27835;&#30103;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#21487;&#33021;&#36890;&#36807;&#26681;&#25454;&#27599;&#20010;&#24739;&#32773;&#30340;&#39044;&#27979;&#21453;&#24212;&#26469;&#20010;&#24615;&#21270;&#27835;&#30103;&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;&#20010;&#24615;&#21270;&#27835;&#30103;&#30340;&#35774;&#35745;&#38656;&#35201;&#23558;&#24739;&#32773;&#29305;&#24322;&#24615;&#30340;&#20449;&#24687;&#25972;&#21512;&#21040;&#36866;&#24403;&#30340;&#32959;&#30244;&#21453;&#24212;&#30340;&#25968;&#23398;&#27169;&#22411;&#20013;&#12290;&#23454;&#29616;&#36825;&#19968;&#33539; Paradigm&#30340;&#19968;&#20010;&#22522;&#26412;&#38556;&#30861;&#26159;&#30446;&#21069;&#32570;&#20047;&#20005;&#26684;&#20294;&#23454;&#29992;&#30340;&#32959;&#30244;&#21457;&#29983;&#12289;&#21457;&#23637;&#12289;&#20405;&#34989;&#21644;&#27835;&#30103;&#21453;&#24212;&#30340;&#25968;&#23398;&#29702;&#35770;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#19981;&#21516;&#30340;&#32959;&#30244;&#29983;&#38271;&#21644;&#27835;&#30103;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#8220;&#22823;&#25968;&#25454;&#8221;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26426;&#26800;&#27169;&#22411;&#20197;&#21450;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#23398;&#27169;&#22411;&#30340;&#23454;&#20363;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#24182;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable advances in cancer diagnosis, treatment, and management that have occurred over the past decade, malignant tumors remain a major public health problem. Further progress in combating cancer may be enabled by personalizing the delivery of therapies according to the predicted response for each individual patient. The design of personalized therapies requires patient-specific information integrated into an appropriate mathematical model of tumor response. A fundamental barrier to realizing this paradigm is the current lack of a rigorous, yet practical, mathematical theory of tumor initiation, development, invasion, and response to therapy. In this review, we begin by providing an overview of different approaches to modeling tumor growth and treatment, including mechanistic as well as data-driven models based on ``big data" and artificial intelligence. Next, we present illustrative examples of mathematical models manifesting their utility and discussing the limitation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#35199;&#38376;&#23376;&#33021;&#28304;&#25552;&#20379;&#30340;&#28909;&#21147;&#23398;&#36719;&#20214;&#32435;&#20837;&#29615;&#22659;&#27169;&#22411;&#65292;&#24182;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#65292;&#25581;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#32463;&#27982;&#29123;&#27668;&#36718;&#26426;&#35843;&#24230;&#20248;&#21270;&#30340;&#22909;&#22788;&#65292;&#24182;&#21457;&#29616;Deep Q-Networks (DQN) &#22312;&#31639;&#27861;&#21644;&#22522;&#20934;&#26041;&#27861;&#20013;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2308.14924</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#32463;&#27982;&#29123;&#27668;&#36718;&#26426;&#35843;&#24230;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimal Economic Gas Turbine Dispatch with Deep Reinforcement Learning. (arXiv:2308.14924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#35199;&#38376;&#23376;&#33021;&#28304;&#25552;&#20379;&#30340;&#28909;&#21147;&#23398;&#36719;&#20214;&#32435;&#20837;&#29615;&#22659;&#27169;&#22411;&#65292;&#24182;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#65292;&#25581;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#32463;&#27982;&#29123;&#27668;&#36718;&#26426;&#35843;&#24230;&#20248;&#21270;&#30340;&#22909;&#22788;&#65292;&#24182;&#21457;&#29616;Deep Q-Networks (DQN) &#22312;&#31639;&#27861;&#21644;&#22522;&#20934;&#26041;&#27861;&#20013;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#30005;&#21147;&#32593;&#32476;&#20013;&#65292;&#29123;&#27668;&#36718;&#26426;&#30340;&#35843;&#24230;&#31574;&#30053;&#27491;&#22312;&#21457;&#29983;&#21464;&#21270;&#12290;&#19982;&#38388;&#27463;&#24615;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#26085;&#30410;&#34701;&#20837;&#30456;&#27604;&#65292;&#29123;&#27668;&#36718;&#26426;&#38656;&#35201;&#26356;&#39057;&#32321;&#22320;&#20197;&#26356;&#30701;&#21608;&#26399;&#21644;&#37096;&#20998;&#36127;&#36733;&#36816;&#34892;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#21487;&#20197;&#24212;&#23545;&#36825;&#31181;&#21457;&#23637;&#24182;&#22312;&#32463;&#27982;&#19978;&#35843;&#24230;&#29123;&#27668;&#36718;&#26426;&#30340;&#24037;&#20855;&#12290;DRL&#30340;&#20027;&#35201;&#20248;&#21183;&#26159;&#26080;&#27169;&#22411;&#20248;&#21270;&#21644;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#65292;&#27604;&#22914;&#30001;&#19981;&#21516;&#36127;&#36733;&#25110;&#21487;&#20877;&#29983;&#33021;&#28304;&#20135;&#29983;&#30340;&#21464;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;DRL&#31639;&#27861;&#26469;&#35299;&#20915;&#21152;&#25343;&#22823;&#38463;&#23572;&#20271;&#22612;&#30465;&#30340;&#32463;&#27982;&#29123;&#27668;&#36718;&#26426;&#35843;&#24230;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#35199;&#38376;&#23376;&#33021;&#28304;&#25552;&#20379;&#30340;&#29616;&#26377;&#28909;&#21147;&#23398;&#36719;&#20214;&#32435;&#20837;&#29615;&#22659;&#27169;&#22411;&#24182;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#65288;&#22914;&#30005;&#20215;&#12289;&#36127;&#36733;&#21644;&#29615;&#22659;&#26465;&#20214;&#30340;&#21464;&#21270;&#65289;&#26469;&#20984;&#26174;DRL&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dispatching strategies for gas turbines (GTs) are changing in modern electricity grids. A growing incorporation of intermittent renewable energy requires GTs to operate more but shorter cycles and more frequently on partial loads. Deep reinforcement learning (DRL) has recently emerged as a tool that can cope with this development and dispatch GTs economically. The key advantages of DRL are a model-free optimization and the ability to handle uncertainties, such as those introduced by varying loads or renewable energy production. In this study, three popular DRL algorithms are implemented for an economic GT dispatch problem on a case study in Alberta, Canada. We highlight the benefits of DRL by incorporating an existing thermodynamic software provided by Siemens Energy into the environment model and by simulating uncertainty via varying electricity prices, loads, and ambient conditions. Among the tested algorithms and baseline methods, Deep Q-Networks (DQN) obtained the highest rewards w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22120;&#29992;&#20110;&#20272;&#35745;&#21333;&#20010;&#29366;&#24577;&#20540;&#65292;&#24182;&#36890;&#36807;&#26681;&#25454;&#22870;&#21169;&#20195;&#26367;&#24120;&#29992;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#24120;&#25968;&#65292;&#25552;&#20379;&#20102;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#25216;&#24039;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.14919</link><description>&lt;p&gt;
&#35770;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22870;&#21169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
On Reward Structures of Markov Decision Processes. (arXiv:2308.14919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22120;&#29992;&#20110;&#20272;&#35745;&#21333;&#20010;&#29366;&#24577;&#20540;&#65292;&#24182;&#36890;&#36807;&#26681;&#25454;&#22870;&#21169;&#20195;&#26367;&#24120;&#29992;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#24120;&#25968;&#65292;&#25552;&#20379;&#20102;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#25216;&#24039;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#36716;&#31227;&#26680;&#19982;&#22870;&#21169;&#20989;&#25968;&#21442;&#25968;&#21270;&#12290;&#36825;&#20004;&#20010;&#22240;&#32032;&#22312;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#27491;&#22914;&#23427;&#20204;&#22312;&#36125;&#23572;&#26364;&#26041;&#31243;&#20013;&#30340;&#23384;&#22312;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#38024;&#23545;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#20851;&#30340;&#21508;&#31181;"&#25104;&#26412;"&#65292;&#22870;&#21169;&#26159;&#29702;&#35299;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#32467;&#26500;&#30340;&#26680;&#24515;&#65292;&#22870;&#21169;&#20013;&#24515;&#27010;&#24565;&#21487;&#20197;&#38416;&#26126;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#27010;&#24565;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31574;&#30053;&#35780;&#20272;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#20854;&#23454;&#20363;&#29305;&#23450;&#30340;&#35823;&#24046;&#30028;&#20026;$\tilde{O}(\sqrt{\frac{\tau_s}{n}})$&#65292;&#29992;&#20110;&#20272;&#35745;&#21333;&#20010;&#29366;&#24577;&#20540;&#12290;&#22312;&#22312;&#32447;&#36951;&#25022;&#26368;&#23567;&#21270;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#36716;&#31227;&#30340;MDP&#24120;&#25968;&#65292;&#30452;&#24452;&#65292;&#25913;&#36827;&#20026;&#22522;&#20110;&#22870;&#21169;&#30340;&#24120;&#25968;&#65292;&#26368;&#22823;&#39044;&#26399;&#21040;&#36798;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#35813;&#24120;&#25968;&#20026;&#19968;&#31181;&#24191;&#20026;&#20154;&#30693;&#30340;&#25216;&#26415;&#65292;&#22522;&#20110;&#28508;&#21147;&#30340;&#22870;&#21169;&#24418;&#29366;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Markov decision process can be parameterized by a transition kernel and a reward function. Both play essential roles in the study of reinforcement learning as evidenced by their presence in the Bellman equations. In our inquiry of various kinds of ``costs'' associated with reinforcement learning inspired by the demands in robotic applications, rewards are central to understanding the structure of a Markov decision process and reward-centric notions can elucidate important concepts in reinforcement learning. Specifically, we studied the sample complexity of policy evaluation and developed a novel estimator with an instance-specific error bound of $\tilde{O}(\sqrt{\frac{\tau_s}{n}})$ for estimating a single state value. Under the online regret minimization setting, we refined the transition-based MDP constant, diameter, into a reward-based constant, maximum expected hitting cost, and with it, provided a theoretical explanation for how a well-known technique, potential-based reward shap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#24615;&#34917;&#25937;&#26694;&#26550;&#65292;&#29992;&#20110;&#24110;&#21161;&#29702;&#35299;&#25512;&#33616;&#31995;&#32479;&#30340;&#27169;&#22411;&#24182;&#20462;&#25913;&#25512;&#33616;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.14916</link><description>&lt;p&gt;
RecRec: &#25512;&#33616;&#31995;&#32479;&#30340;&#31639;&#27861;&#24615;&#34917;&#25937;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
RecRec: Algorithmic Recourse for Recommender Systems. (arXiv:2308.14916v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#24615;&#34917;&#25937;&#26694;&#26550;&#65292;&#29992;&#20110;&#24110;&#21161;&#29702;&#35299;&#25512;&#33616;&#31995;&#32479;&#30340;&#27169;&#22411;&#24182;&#20462;&#25913;&#25512;&#33616;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#23089;&#20048;&#12289;&#36141;&#29289;&#12289;&#39135;&#29289;&#12289;&#26032;&#38395;&#12289;&#23601;&#19994;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#30340;&#20915;&#31574;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36825;&#20123;&#25512;&#33616;&#31995;&#32479;&#32972;&#21518;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20110;&#29992;&#25143;&#12289;&#20869;&#23481;&#25552;&#20379;&#32773;&#21644;&#31995;&#32479;&#24320;&#21457;&#32773;&#26469;&#35828;&#36890;&#24120;&#37117;&#26159;&#24040;&#22823;&#19988;&#19981;&#36879;&#26126;&#30340;&#12290;&#23545;&#20110;&#25152;&#26377;&#21033;&#30410;&#30456;&#20851;&#32773;&#26469;&#35828;&#65292;&#29702;&#35299;&#27169;&#22411;&#22312;&#36827;&#34892;&#26576;&#20123;&#39044;&#27979;&#21644;&#25512;&#33616;&#26102;&#30340;&#21407;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#37027;&#20123;&#29983;&#35745;&#20381;&#36182;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#20869;&#23481;&#25552;&#20379;&#32773;&#26469;&#35828;&#23588;&#20854;&#22914;&#27492;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#20174;&#23454;&#38469;&#38656;&#27714;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#20869;&#23481;&#25552;&#20379;&#32773;&#30340;&#25512;&#33616;&#31995;&#32479;&#34917;&#25937;&#26694;&#26550;&#12290;&#25512;&#33616;&#35774;&#32622;&#20013;&#30340;&#31639;&#27861;&#24615;&#34917;&#25937;&#26159;&#19968;&#32452;&#25805;&#20316;&#65292;&#22914;&#26524;&#25191;&#34892;&#65292;&#23558;&#20197;&#26399;&#26395;&#30340;&#26041;&#24335;&#20462;&#25913;&#39033;&#30446;&#30340;&#25512;&#33616;&#65288;&#25110;&#25490;&#24207;&#65289;&#12290;&#34917;&#25937;&#25514;&#26045;&#25552;&#20379;&#30340;&#25805;&#20316;&#24418;&#24335;&#20026;&#65306;&#8220;&#22914;&#26524;&#19968;&#20010;&#29305;&#24449;&#20174;X&#21464;&#20026;Y&#65292;&#37027;&#20040;&#35813;&#39033;&#30446;&#30340;&#25490;&#21517;&#20063;&#20250;&#30456;&#24212;&#21464;&#21270;&#12290;&#8221;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play an essential role in the choices people make in domains such as entertainment, shopping, food, news, employment, and education. The machine learning models underlying these recommender systems are often enormously large and black-box in nature for users, content providers, and system developers alike. It is often crucial for all stakeholders to understand the model's rationale behind making certain predictions and recommendations. This is especially true for the content providers whose livelihoods depend on the recommender system. Drawing motivation from the practitioners' need, in this work, we propose a recourse framework for recommender systems, targeted towards the content providers. Algorithmic recourse in the recommendation setting is a set of actions that, if executed, would modify the recommendations (or ranking) of an item in the desired manner. A recourse suggests actions of the form: "if a feature changes X to Y, then the ranking of that item for a s
&lt;/p&gt;</description></item><item><title>RobustCLEVR&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#20013;&#24515;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#26694;&#26550;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#35268;&#23450;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#33021;&#22815;&#20135;&#29983;&#22810;&#31181;&#26080;&#27861;&#22312;&#20854;&#20182;&#35780;&#20272;&#20013;&#20986;&#29616;&#30340;&#22270;&#20687;&#30772;&#22351;&#65292;&#20197;&#35780;&#20272;&#30446;&#26631;&#20013;&#24515;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14899</link><description>&lt;p&gt;
RobustCLEVR&#65306;&#19968;&#31181;&#35780;&#20272;&#30446;&#26631;&#20013;&#24515;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#21644;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RobustCLEVR: A Benchmark and Framework for Evaluating Robustness in Object-centric Learning. (arXiv:2308.14899v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14899
&lt;/p&gt;
&lt;p&gt;
RobustCLEVR&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#20013;&#24515;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#26694;&#26550;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#35268;&#23450;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#33021;&#22815;&#20135;&#29983;&#22810;&#31181;&#26080;&#27861;&#22312;&#20854;&#20182;&#35780;&#20272;&#20013;&#20986;&#29616;&#30340;&#22270;&#20687;&#30772;&#22351;&#65292;&#20197;&#35780;&#20272;&#30446;&#26631;&#20013;&#24515;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#20013;&#24515;&#34920;&#31034;&#23398;&#20064;&#36890;&#36807;&#26126;&#30830;&#22320;&#23558;&#22270;&#20687;&#22330;&#26223;&#35299;&#26512;&#20026;&#20854;&#32452;&#25104;&#37096;&#20998;&#65292;&#20811;&#26381;&#20102;&#22270;&#20687;&#32423;&#34920;&#31034;&#30340;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#32423;&#34920;&#31034;&#36890;&#24120;&#23545;&#33258;&#28982;&#22270;&#20687;&#30772;&#22351;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#32780;&#30446;&#26631;&#20013;&#24515;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#27979;&#35797;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RobustCLEVR&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#22312;&#19987;&#23478;&#30693;&#35782;&#22522;&#30784;&#19978;&#21551;&#29992;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#22240;&#26524;&#20381;&#36182;&#24615;&#30340;&#35268;&#23450;&#65292;&#24182;&#33021;&#22815;&#20135;&#29983;&#19968;&#31995;&#21015;&#22312;&#29616;&#26377;&#40065;&#26834;&#24615;&#35780;&#20272;&#20013;&#26080;&#27861;&#33719;&#24471;&#30340;&#22270;&#20687;&#30772;&#22351;&#65292;&#20197;&#19968;&#31181;&#20840;&#26032;&#30340;&#26041;&#24335;&#35780;&#20272;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20960;&#31181;&#22270;&#20687;&#30772;&#22351;&#36807;&#31243;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26126;&#30830;&#32534;&#30721;&#20102;&#27599;&#31181;&#30772;&#22351;&#31867;&#22411;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#20998;&#24067;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#22240;&#26524;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#38598;&#21464;&#20307;&#65292;&#24182;&#22312;&#19978;&#38754;&#35780;&#20272;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object-centric representation learning offers the potential to overcome limitations of image-level representations by explicitly parsing image scenes into their constituent components. While image-level representations typically lack robustness to natural image corruptions, the robustness of object-centric methods remains largely untested. To address this gap, we present the RobustCLEVR benchmark dataset and evaluation framework. Our framework takes a novel approach to evaluating robustness by enabling the specification of causal dependencies in the image generation process grounded in expert knowledge and capable of producing a wide range of image corruptions unattainable in existing robustness evaluations. Using our framework, we define several causal models of the image corruption process which explicitly encode assumptions about the causal relationships and distributions of each corruption type. We generate dataset variants for each causal model on which we evaluate state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#31532;39&#23626;&#22269;&#38469;&#36923;&#36753;&#32534;&#31243;&#20250;&#35758;&#35770;&#25991;&#38598;&#21253;&#21547;&#20102;&#22312;&#20262;&#25958;&#24093;&#22269;&#29702;&#24037;&#23398;&#38498;&#20030;&#34892;&#30340;&#25216;&#26415;&#20132;&#27969;&#25253;&#21578;&#65292;&#28041;&#21450;&#22810;&#20010;&#19987;&#39064;&#65292;&#21253;&#25324;&#36923;&#36753;&#32534;&#31243;&#21644;&#26426;&#22120;&#23398;&#20064;&#12289;&#36923;&#36753;&#32534;&#31243;&#21644;&#21487;&#35299;&#37322;&#24615;&#12289;&#20262;&#29702;&#21644;&#21487;&#20449;&#24230;&#31561;&#12290;</title><link>http://arxiv.org/abs/2308.14898</link><description>&lt;p&gt;
&#31532;39&#23626;&#22269;&#38469;&#36923;&#36753;&#32534;&#31243;&#20250;&#35758;&#35770;&#25991;&#38598;
&lt;/p&gt;
&lt;p&gt;
Proceedings 39th International Conference on Logic Programming. (arXiv:2308.14898v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14898
&lt;/p&gt;
&lt;p&gt;
&#31532;39&#23626;&#22269;&#38469;&#36923;&#36753;&#32534;&#31243;&#20250;&#35758;&#35770;&#25991;&#38598;&#21253;&#21547;&#20102;&#22312;&#20262;&#25958;&#24093;&#22269;&#29702;&#24037;&#23398;&#38498;&#20030;&#34892;&#30340;&#25216;&#26415;&#20132;&#27969;&#25253;&#21578;&#65292;&#28041;&#21450;&#22810;&#20010;&#19987;&#39064;&#65292;&#21253;&#25324;&#36923;&#36753;&#32534;&#31243;&#21644;&#26426;&#22120;&#23398;&#20064;&#12289;&#36923;&#36753;&#32534;&#31243;&#21644;&#21487;&#35299;&#37322;&#24615;&#12289;&#20262;&#29702;&#21644;&#21487;&#20449;&#24230;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#21367;&#21253;&#21547;&#20102;&#22312;2023&#24180;7&#26376;9&#26085;&#33267;15&#26085;&#22312;&#33521;&#22269;&#20262;&#25958;&#24093;&#22269;&#29702;&#24037;&#23398;&#38498;&#20030;&#34892;&#30340;&#31532;39&#23626;&#22269;&#38469;&#36923;&#36753;&#32534;&#31243;&#20250;&#35758;&#65288;ICLP 2023&#65289;&#19978;&#21457;&#34920;&#30340;&#25216;&#26415;&#20132;&#27969;&#25253;&#21578;&#12290;&#36825;&#20123;&#25216;&#26415;&#20132;&#27969;&#25253;&#21578;&#28041;&#21450;&#20027;&#39064;&#35762;&#24231;&#12289;&#21338;&#22763;&#29983;&#35770;&#22363;&#12289;&#24212;&#29992;&#19982;&#31995;&#32479;/&#28436;&#31034;&#19987;&#39064;&#12289;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#19987;&#39064;&#12289;&#20852;&#36259;&#23567;&#32452;&#20132;&#27969;&#12289;&#36923;&#36753;&#32534;&#31243;&#19982;&#26426;&#22120;&#23398;&#20064;&#19987;&#39064;&#20197;&#21450;&#36923;&#36753;&#32534;&#31243;&#19982;&#21487;&#35299;&#37322;&#24615;&#12289;&#20262;&#29702;&#21644;&#21487;&#20449;&#24230;&#19987;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This volume contains the Technical Communications presented at the 39th International Conference on Logic Programming (ICLP 2023), held at Imperial College London, UK from July 9 to July 15, 2023. Technical Communications included here concern the Main Track, the Doctoral Consortium, the Application and Systems/Demo track, the Recently Published Research Track, the Birds-of-a-Feather track, the Thematic Tracks on Logic Programming and Machine Learning, and Logic Programming and Explainability, Ethics, and Trustworthiness.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#31574;&#30053;&#35780;&#20272;&#30340;&#31163;&#32447;&#24207;&#21015;&#24314;&#27169;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#35777;&#26126;&#20855;&#26377;&#26041;&#24046;&#32553;&#20943;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14897</link><description>&lt;p&gt;
&#39640;&#25928;&#32479;&#35745;&#26041;&#24046;&#32553;&#20943;&#30340;&#21452;&#31574;&#30053;&#35780;&#20272;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Statistically Efficient Variance Reduction with Double Policy Estimation for Off-Policy Evaluation in Sequence-Modeled Reinforcement Learning. (arXiv:2308.14897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#31574;&#30053;&#35780;&#20272;&#30340;&#31163;&#32447;&#24207;&#21015;&#24314;&#27169;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#35777;&#26126;&#20855;&#26377;&#26041;&#24046;&#32553;&#20943;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#20808;&#21069;&#25910;&#38598;&#30340;&#29615;&#22659;-&#21160;&#20316;&#20132;&#20114;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#26469;&#23398;&#20064;&#19968;&#20010;&#26080;&#38656;&#35775;&#38382;&#30495;&#23454;&#29615;&#22659;&#30340;&#31574;&#30053;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20915;&#31574;&#36716;&#25442;&#22120;&#31561;&#26041;&#27861;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#12290;&#23613;&#31649;&#36825;&#20123;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#22312;&#22238;&#25253;&#29575;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#22312;&#38656;&#35201;&#36739;&#38271;&#30340;&#22238;&#21512;&#25110;&#22238;&#25253;&#31232;&#32570;&#30340;&#20219;&#21153;&#19978;&#65292;&#20294;&#22312;&#22788;&#29702;&#31163;&#32447;&#25968;&#25454;&#26102;&#24182;&#26410;&#32771;&#34385;&#37325;&#35201;&#24615;&#37319;&#26679;&#26469;&#26657;&#27491;&#31574;&#30053;&#20559;&#24046;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#32570;&#20047;&#34892;&#20026;&#31574;&#30053;&#21644;&#20351;&#29992;&#30830;&#23450;&#24615;&#35780;&#20272;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DPE&#65306;&#19968;&#31181;&#23558;&#31163;&#32447;&#24207;&#21015;&#24314;&#27169;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#32479;&#35745;&#19978;&#35777;&#26126;&#20855;&#26377;&#26041;&#24046;&#32553;&#20943;&#24615;&#36136;&#30340;&#21452;&#31574;&#30053;&#35780;&#20272;&#65288;DPE&#65289;&#34701;&#21512;&#22312;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#30340;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning aims to utilize datasets of previously gathered environment-action interaction records to learn a policy without access to the real environment. Recent work has shown that offline reinforcement learning can be formulated as a sequence modeling problem and solved via supervised learning with approaches such as decision transformer. While these sequence-based methods achieve competitive results over return-to-go methods, especially on tasks that require longer episodes or with scarce rewards, importance sampling is not considered to correct the policy bias when dealing with off-policy data, mainly due to the absence of behavior policy and the use of deterministic evaluation policies. To this end, we propose DPE: an RL algorithm that blends offline sequence modeling and offline reinforcement learning with Double Policy Estimation (DPE) in a unified framework with statistically proven properties on variance reduction. We validate our method in multiple tasks 
&lt;/p&gt;</description></item><item><title>NAS-X&#26159;&#19968;&#31181;&#22522;&#20110;&#25197;&#26354;&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;&#21796;&#37266;-&#30561;&#30496;&#31639;&#27861;&#26469;&#23398;&#20064;&#21644;&#25512;&#26029;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#25512;&#26029;&#21644;&#21442;&#25968;&#24674;&#22797;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.14864</link><description>&lt;p&gt;
NAS-X: &#22522;&#20110;&#25197;&#26354;&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NAS-X: Neural Adaptive Smoothing via Twisting. (arXiv:2308.14864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14864
&lt;/p&gt;
&lt;p&gt;
NAS-X&#26159;&#19968;&#31181;&#22522;&#20110;&#25197;&#26354;&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;&#21796;&#37266;-&#30561;&#30496;&#31639;&#27861;&#26469;&#23398;&#20064;&#21644;&#25512;&#26029;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#25512;&#26029;&#21644;&#21442;&#25968;&#24674;&#22797;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NAS-X&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#37325;&#26032;&#21152;&#26435;&#30340;&#21796;&#37266;-&#30561;&#30496;&#31639;&#27861;&#36827;&#34892;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#23398;&#20064;&#21644;&#25512;&#26029;&#12290;NAS-X&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#28508;&#21464;&#37327;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;SMC&#26041;&#27861;&#26469;&#25311;&#21512;&#27604;&#20256;&#32479;&#30340;&#37325;&#26032;&#21152;&#26435;&#21796;&#37266;-&#30561;&#30496;&#26041;&#27861;&#26356;&#24191;&#27867;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;NAS-X&#65292;&#24182;&#21457;&#29616;&#22312;&#25512;&#26029;&#21644;&#21442;&#25968;&#24674;&#22797;&#26041;&#38754;&#65292;&#23427;&#26126;&#26174;&#20248;&#20110;&#20808;&#21069;&#30340;&#21464;&#20998;&#21644;&#22522;&#20110;&#37325;&#26032;&#21152;&#26435;&#21796;&#37266;-&#30561;&#30496;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Neural Adaptive Smoothing via Twisting (NAS-X), a method for learning and inference in sequential latent variable models based on reweighted wake-sleep (RWS). NAS-X works with both discrete and continuous latent variables, and leverages smoothing SMC to fit a broader range of models than traditional RWS methods. We test NAS-X on discrete and continuous tasks and find that it substantially outperforms previous variational and RWS-based methods in inference and parameter recovery.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#29076;&#27744;&#22270;&#20687;&#27969;&#36827;&#34892;&#21360;&#21047;&#36712;&#36857;&#24322;&#24120;&#20998;&#31867;&#30340;&#20851;&#38190;&#26102;&#31354;&#23398;&#20064;&#22120;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#39046;&#20808;&#30340;&#28145;&#24230;&#26102;&#31354;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#36341;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.14861</link><description>&lt;p&gt;
&#35780;&#20272;&#20351;&#29992;&#29076;&#27744;&#22270;&#20687;&#27969;&#36827;&#34892;&#21360;&#21047;&#36712;&#36857;&#24322;&#24120;&#20998;&#31867;&#30340;&#20851;&#38190;&#26102;&#31354;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Key Spatiotemporal Learners for Print Track Anomaly Classification Using Melt Pool Image Streams. (arXiv:2308.14861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#29076;&#27744;&#22270;&#20687;&#27969;&#36827;&#34892;&#21360;&#21047;&#36712;&#36857;&#24322;&#24120;&#20998;&#31867;&#30340;&#20851;&#38190;&#26102;&#31354;&#23398;&#20064;&#22120;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#39046;&#20808;&#30340;&#28145;&#24230;&#26102;&#31354;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#36341;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#65288;MAM&#65289;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#35299;&#20915;&#26222;&#36941;&#37319;&#29992;MAM&#25216;&#26415;&#30340;&#20851;&#38190;&#38556;&#30861;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#24378;&#35843;&#20102;&#21033;&#29992;&#29076;&#27744;&#29305;&#24449;&#36827;&#34892;&#23454;&#26102;&#32570;&#38519;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#39640;&#36136;&#37327;&#30340;&#29076;&#27744;&#22270;&#20687;&#25968;&#25454;&#20855;&#26377;&#23454;&#29616;&#31934;&#30830;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#20294;&#23545;&#20110;&#33021;&#22815;&#21033;&#29992;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#30340;&#30636;&#24577;&#21644;&#26102;&#24207;&#29305;&#24449;&#30340;&#23574;&#31471;&#26102;&#31354;&#27169;&#22411;&#30340;&#21033;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#24182;&#23454;&#36341;&#20102;&#19968;&#20123;&#39046;&#20808;&#30340;&#28145;&#24230;&#26102;&#31354;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20998;&#31867;&#26469;&#33258;&#19981;&#21516;&#26448;&#26009;&#12289;&#31995;&#32479;&#21644;&#24212;&#29992;&#30340;&#29076;&#27744;&#22270;&#20687;&#27969;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#30740;&#31350;&#20102;&#30001;&#31354;&#38388;&#21644;&#26102;&#38388;&#27969;&#32452;&#25104;&#30340;&#20004;&#27969;&#32593;&#32476;&#12289;&#24490;&#29615;&#31354;&#38388;&#32593;&#32476;&#21644;&#22240;&#24335;&#20998;&#35299;3D&#21367;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent applications of machine learning in metal additive manufacturing (MAM) have demonstrated significant potential in addressing critical barriers to the widespread adoption of MAM technology. Recent research in this field emphasizes the importance of utilizing melt pool signatures for real-time defect prediction. While high-quality melt pool image data holds the promise of enabling precise predictions, there has been limited exploration into the utilization of cutting-edge spatiotemporal models that can harness the inherent transient and sequential characteristics of the additive manufacturing process. This research introduces and puts into practice some of the leading deep spatiotemporal learning models that can be adapted for the classification of melt pool image streams originating from various materials, systems, and applications. Specifically, it investigates two-stream networks comprising spatial and temporal streams, a recurrent spatial network, and a factorized 3D convoluti
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;Attention Visualizer&#21253;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#21333;&#35789;&#22312;&#32534;&#30721;&#22120;-&#21482;&#26377;&#30340;Transformer&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#39640;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14850</link><description>&lt;p&gt;
Attention Visualizer Package:&#25581;&#31034;&#32534;&#30721;&#22120;-&#21482;&#26377;&#30340;Transformer&#27169;&#22411;&#20013;&#21333;&#35789;&#37325;&#35201;&#24615;&#30340;&#27880;&#24847;&#21147;&#21487;&#35270;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Attention Visualizer Package: Revealing Word Importance for Deeper Insight into Encoder-Only Transformer Models. (arXiv:2308.14850v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14850
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;Attention Visualizer&#21253;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#21333;&#35789;&#22312;&#32534;&#30721;&#22120;-&#21482;&#26377;&#30340;Transformer&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#39640;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Attention Visualizer&#21253;&#65292;&#35813;&#21253;&#29992;&#20110;&#35270;&#35273;&#21270;&#23637;&#31034;&#32534;&#30721;&#22120;-&#21482;&#26377;&#30340;Transformer&#27169;&#22411;&#20013;&#20010;&#21035;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#12290;&#19982;&#20854;&#20182;&#20851;&#27880;&#26631;&#35760;&#21644;&#33258;&#27880;&#24847;&#21147;&#20998;&#25968;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#30740;&#31350;&#21333;&#35789;&#21450;&#20854;&#23545;&#26368;&#32456;&#23884;&#20837;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;&#36825;&#26679;&#30340;&#24211;&#22312;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23427;&#20204;&#25552;&#20379;&#20102;&#20102;&#35299;&#20854;&#20869;&#37096;&#26426;&#21046;&#12289;&#25552;&#39640;&#20854;&#24615;&#33021;&#30340;&#26356;&#22909;&#29702;&#35299;&#30340;&#26426;&#20250;&#12290;&#24744;&#21487;&#20197;&#35775;&#38382;&#20197;&#19979;GitHub&#23384;&#20648;&#24211;&#33719;&#21462;&#20195;&#30721;&#24182;&#26597;&#30475;&#31034;&#20363;&#65306;https://github.com/AlaFalaki/AttentionVisualizer&#12290;
&lt;/p&gt;
&lt;p&gt;
This report introduces the Attention Visualizer package, which is crafted to visually illustrate the significance of individual words in encoder-only transformer-based models. In contrast to other methods that center on tokens and self-attention scores, our approach will examine the words and their impact on the final embedding representation. Libraries like this play a crucial role in enhancing the interpretability and explainability of neural networks. They offer the opportunity to illuminate their internal mechanisms, providing a better understanding of how they operate and can be enhanced. You can access the code and review examples on the following GitHub repository: https://github.com/AlaFalaki/AttentionVisualizer.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#24314;&#31569;&#20174;&#19994;&#32773;&#30340;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#32467;&#26524;&#65292;&#30740;&#31350;&#20102;&#24314;&#31569;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;AI&#21160;&#21147;&#21327;&#20316;&#26426;&#22120;&#20154;&#30340;&#29305;&#24449;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38500;&#20102;&#20043;&#21069;&#24050;&#37492;&#23450;&#20986;&#30340;&#20851;&#38190;&#20449;&#20219;&#22240;&#32032;&#22806;&#65292;&#36130;&#21153;&#32771;&#34385;&#21644;&#19981;&#30830;&#23450;&#24615;&#31561;&#20854;&#20182;&#22240;&#32032;&#20063;&#26159;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.14846</link><description>&lt;p&gt;
&#24314;&#31569;AI&#21160;&#21147;&#21327;&#20316;&#26426;&#22120;&#20154;&#30340;&#20449;&#20219;&#38382;&#39064;&#65306;&#19968;&#39033;&#23450;&#24615;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Trust in Construction AI-Powered Collaborative Robots: A Qualitative Empirical Analysis. (arXiv:2308.14846v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14846
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#24314;&#31569;&#20174;&#19994;&#32773;&#30340;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#32467;&#26524;&#65292;&#30740;&#31350;&#20102;&#24314;&#31569;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;AI&#21160;&#21147;&#21327;&#20316;&#26426;&#22120;&#20154;&#30340;&#29305;&#24449;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38500;&#20102;&#20043;&#21069;&#24050;&#37492;&#23450;&#20986;&#30340;&#20851;&#38190;&#20449;&#20219;&#22240;&#32032;&#22806;&#65292;&#36130;&#21153;&#32771;&#34385;&#21644;&#19981;&#30830;&#23450;&#24615;&#31561;&#20854;&#20182;&#22240;&#32032;&#20063;&#26159;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#25216;&#26415;&#30740;&#31350;&#20154;&#21592;&#21644;&#20855;&#26377;&#21069;&#30651;&#24605;&#32500;&#30340;&#20844;&#21496;&#27491;&#22312;&#23581;&#35797;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#21327;&#20316;&#26426;&#22120;&#20154;&#65288;&#21363;cobots&#65289;&#26469;&#25506;&#32034;&#24314;&#31569;&#34892;&#19994;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#21508;&#31181;&#33258;&#21160;&#21270;&#22330;&#26223;&#12290;&#26234;&#33021;&#21327;&#20316;&#26426;&#22120;&#20154;&#34987;&#26399;&#26395;&#25104;&#20026;&#26410;&#26469;&#24314;&#31569;&#24037;&#20316;&#30340;&#20027;&#23548;&#22411;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;AI&#39537;&#21160;&#30340;cobots&#30340;&#40657;&#31665;&#24615;&#36136;&#20197;&#21450;&#24341;&#20837;&#24037;&#20316;&#29616;&#22330;&#30340;&#25216;&#26415;&#21644;&#24515;&#29702;&#26041;&#38754;&#30340;&#26410;&#30693;&#24615;&#65292;&#26159;&#20449;&#20219;&#25361;&#25112;&#30340;&#21069;&#20806;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20351;&#29992;&#22522;&#20110;&#22320;&#29702;&#29702;&#35770;&#30340;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#30340;&#32467;&#26524;&#65292;&#30740;&#31350;&#20102;&#24314;&#31569;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;AI&#21160;&#21147;&#21327;&#20316;&#26426;&#22120;&#20154;&#30340;&#29305;&#24449;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#20043;&#21069;&#20316;&#32773;&#36827;&#34892;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#20013;&#37492;&#23450;&#20986;&#30340;&#20851;&#38190;&#20449;&#20219;&#22240;&#32032;&#19982;&#39046;&#22495;&#19987;&#23478;&#21644;&#26368;&#32456;&#29992;&#25143;&#20135;&#29983;&#20102;&#20849;&#40483;&#65292;&#20294;&#36130;&#21153;&#32771;&#34385;&#21644;&#19981;&#30830;&#23450;&#24615;&#31561;&#20854;&#20182;&#22240;&#32032;&#20063;&#26159;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Construction technology researchers and forward-thinking companies are experimenting with collaborative robots (aka cobots), powered by artificial intelligence (AI), to explore various automation scenarios as part of the digital transformation of the industry. Intelligent cobots are expected to be the dominant type of robots in the future of work in construction. However, the black-box nature of AI-powered cobots and unknown technical and psychological aspects of introducing them to job sites are precursors to trust challenges. By analyzing the results of semi-structured interviews with construction practitioners using grounded theory, this paper investigates the characteristics of trustworthy AI-powered cobots in construction. The study found that while the key trust factors identified in a systematic literature review -- conducted previously by the authors -- resonated with the field experts and end users, other factors such as financial considerations and the uncertainty associated 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#40065;&#26834;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#23545;&#20110;&#24314;&#31569;&#24037;&#20154;&#30340;&#27963;&#21160;&#35782;&#21035;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#35201;&#27714;&#26356;&#23569;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.14843</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#24037;&#20154;-&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#40065;&#26834;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Robust Activity Recognition for Adaptive Worker-Robot Interaction using Transfer Learning. (arXiv:2308.14843v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#40065;&#26834;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#23545;&#20110;&#24314;&#31569;&#24037;&#20154;&#30340;&#27963;&#21160;&#35782;&#21035;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#35201;&#27714;&#26356;&#23569;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#22312;&#26816;&#27979;&#24314;&#31569;&#24037;&#20154;&#30340;&#27963;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#27963;&#21160;&#35782;&#21035;&#22312;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#29702;&#35299;&#20154;&#31867;&#21516;&#20107;&#30340;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#32570;&#20047;&#40065;&#26834;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#31569;&#24037;&#20154;&#30340;&#27963;&#21160;&#35782;&#21035;&#65292;&#35813;&#26041;&#27861;&#22312;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#19979;&#65292;&#38656;&#35201;&#27604;&#20043;&#21069;&#26356;&#23569;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#26102;&#38388;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20174;&#21407;&#20316;&#32773;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#36716;&#31227;&#29305;&#24449;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#20197;&#36827;&#34892;&#24314;&#31569;&#27963;&#21160;&#35782;&#21035;&#30340;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#22312;Kinetics-400&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;Kinetics-400&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;400&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;&#35813;&#27169;&#22411;&#32463;&#36807;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#20174;YouTube&#19978;&#25429;&#33719;&#30340;&#25163;&#21160;&#26448;&#26009;&#22788;&#29702;&#27963;&#21160;&#30340;&#35270;&#39057;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition (HAR) using machine learning has shown tremendous promise in detecting construction workers' activities. HAR has many applications in human-robot interaction research to enable robots' understanding of human counterparts' activities. However, many existing HAR approaches lack robustness, generalizability, and adaptability. This paper proposes a transfer learning methodology for activity recognition of construction workers that requires orders of magnitude less data and compute time for comparable or better classification accuracy. The developed algorithm transfers features from a model pre-trained by the original authors and fine-tunes them for the downstream task of activity recognition in construction. The model was pre-trained on Kinetics-400, a large-scale video-based human activity recognition dataset with 400 distinct classes. The model was fine-tuned and tested using videos captured from manual material handling (MMH) activities found on YouTube. Resul
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#32908;&#30005;&#22270;&#35774;&#22791;&#27979;&#37327;&#12289;&#24314;&#27169;&#21644;&#39044;&#27979;&#20102;VR&#29992;&#25143;&#22312;&#19982;&#34394;&#25311;&#29615;&#22659;&#20132;&#20114;&#26102;&#30340;&#39048;&#37096;&#32908;&#32905;&#25910;&#32553;&#27700;&#24179;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#29289;&#29289;&#29702;&#21551;&#21457;&#24335;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#22836;&#37096;&#36816;&#21160;&#29366;&#24577;&#19979;&#30340;&#39048;&#37096;&#25910;&#32553;&#27700;&#24179;&#65292;&#24182;&#19988;&#21487;&#20197;&#39044;&#27979;&#20165;&#36890;&#36807;&#30446;&#26631;&#22836;&#37096;&#23039;&#21183;&#30340;&#28508;&#22312;&#25910;&#32553;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.14841</link><description>&lt;p&gt;
&#20248;&#21270;&#34394;&#25311;&#29616;&#23454;/&#22686;&#24378;&#29616;&#23454;&#20154;&#26426;&#24037;&#31243;&#23398;&#65306;&#24314;&#27169;&#21644;&#39044;&#27979;&#29992;&#25143;&#39048;&#37096;&#32908;&#32905;&#25910;&#32553;
&lt;/p&gt;
&lt;p&gt;
Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck Muscle Contraction. (arXiv:2308.14841v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14841
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#32908;&#30005;&#22270;&#35774;&#22791;&#27979;&#37327;&#12289;&#24314;&#27169;&#21644;&#39044;&#27979;&#20102;VR&#29992;&#25143;&#22312;&#19982;&#34394;&#25311;&#29615;&#22659;&#20132;&#20114;&#26102;&#30340;&#39048;&#37096;&#32908;&#32905;&#25910;&#32553;&#27700;&#24179;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#29289;&#29289;&#29702;&#21551;&#21457;&#24335;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#22836;&#37096;&#36816;&#21160;&#29366;&#24577;&#19979;&#30340;&#39048;&#37096;&#25910;&#32553;&#27700;&#24179;&#65292;&#24182;&#19988;&#21487;&#20197;&#39044;&#27979;&#20165;&#36890;&#36807;&#30446;&#26631;&#22836;&#37096;&#23039;&#21183;&#30340;&#28508;&#22312;&#25910;&#32553;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#35268;&#27169;&#21644;&#38271;&#26399;&#37319;&#29992;VR/AR&#20307;&#39564;&#26469;&#35828;&#65292;&#20154;&#20307;&#24037;&#31243;&#23398;&#25928;&#29575;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#34429;&#28982;VR/AR&#22836;&#25140;&#26174;&#31034;&#22120;&#22312;&#35266;&#30475;&#26102;&#35299;&#38145;&#20102;&#29992;&#25143;&#33258;&#28982;&#30340;&#24191;&#27867;&#22836;&#37096;&#36816;&#21160;&#65292;&#20294;&#30001;&#20110;&#22686;&#21152;&#30340;&#30828;&#20214;&#37325;&#37327;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#24433;&#21709;&#20102;&#20182;&#20204;&#30340;&#39048;&#37096;&#32908;&#32905;&#33298;&#36866;&#24230;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#22810;&#23569;&#37327;&#21270;&#30340;&#30693;&#35782;&#26469;&#29702;&#35299;&#21644;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21033;&#29992;&#32908;&#30005;&#22270;&#35774;&#22791;&#65292;&#25105;&#20204;&#27979;&#37327;&#12289;&#24314;&#27169;&#21644;&#39044;&#27979;VR&#29992;&#25143;&#22312;&#19982;&#34394;&#25311;&#29615;&#22659;&#20132;&#20114;&#26102;&#22836;&#37096;&#36816;&#21160;&#26102;&#30340;&#39048;&#37096;&#32908;&#32905;&#25910;&#32553;&#27700;&#24179;&#65288;MCL&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#23398;&#20064;&#25910;&#38598;&#30340;&#29983;&#29702;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#29289;&#29289;&#29702;&#21551;&#21457;&#24335;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#22312;&#19981;&#21516;&#22836;&#37096;&#36816;&#21160;&#29366;&#24577;&#19979;&#30340;&#39048;&#37096;MCL&#12290;&#38500;&#20102;&#37327;&#21270;&#23436;&#25104;&#30340;&#22836;&#37096;&#36816;&#21160;&#30340;&#32047;&#31215;MCL&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#39044;&#27979;&#20165;&#36890;&#36807;&#30446;&#26631;&#22836;&#37096;&#23039;&#21183;&#30340;&#28508;&#22312;MCL&#38656;&#27714;&#12290;&#19968;&#31995;&#21015;&#23458;&#35266;&#35780;&#20272;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ergonomic efficiency is essential to the mass and prolonged adoption of VR/AR experiences. While VR/AR head-mounted displays unlock users' natural wide-range head movements during viewing, their neck muscle comfort is inevitably compromised by the added hardware weight. Unfortunately, little quantitative knowledge for understanding and addressing such an issue is available so far.  Leveraging electromyography devices, we measure, model, and predict VR users' neck muscle contraction levels (MCL) while they move their heads to interact with the virtual environment. Specifically, by learning from collected physiological data, we develop a bio-physically inspired computational model to predict neck MCL under diverse head kinematic states. Beyond quantifying the cumulative MCL of completed head movements, our model can also predict potential MCL requirements with target head poses only. A series of objective evaluations and user studies demonstrate its prediction accuracy and generality, as
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#20010;&#30740;&#35752;&#20250;&#30340;&#32508;&#21512;&#25253;&#36947;&#65292;&#35752;&#35770;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#24102;&#26469;&#30340;&#21452;&#37325;&#29992;&#36884;&#22256;&#22659;&#65292;&#25552;&#20986;&#20102;&#31038;&#21306;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.14840</link><description>&lt;p&gt;
&#35782;&#21035;&#21644;&#20943;&#36731;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Identifying and Mitigating the Security Risks of Generative AI. (arXiv:2308.14840v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14840
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#20010;&#30740;&#35752;&#20250;&#30340;&#32508;&#21512;&#25253;&#36947;&#65292;&#35752;&#35770;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#24102;&#26469;&#30340;&#21452;&#37325;&#29992;&#36884;&#22256;&#22659;&#65292;&#25552;&#20986;&#20102;&#31038;&#21306;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#19968;&#39033;&#37325;&#22823;&#25216;&#26415;&#21457;&#26126;&#37117;&#20250;&#24102;&#26469;&#21452;&#37325;&#29992;&#36884;&#30340;&#22256;&#22659; - &#26032;&#25216;&#26415;&#26082;&#26377;&#21487;&#33021;&#34987;&#29992;&#20110;&#21892;&#33391;&#65292;&#20063;&#21487;&#33021;&#34987;&#29992;&#20110;&#24694;&#24847;&#34892;&#20026;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#25216;&#26415;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65288;&#20363;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20195;&#30721;&#34917;&#20840;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#21644;&#32534;&#36753;&#65289;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#21516;&#26679;&#21487;&#20197;&#21033;&#29992;GenAI&#29983;&#25104;&#26032;&#30340;&#25915;&#20987;&#65292;&#24182;&#22686;&#21152;&#29616;&#26377;&#25915;&#20987;&#30340;&#36895;&#24230;&#21644;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#22312;Google&#20030;&#21150;&#30340;&#19968;&#20010;&#30740;&#35752;&#20250;&#30340;&#21457;&#29616;&#65288;&#30001;&#26031;&#22374;&#31119;&#22823;&#23398;&#21644;&#23041;&#26031;&#24247;&#26143;&#22823;&#23398;&#40614;&#36842;&#36874;&#20998;&#26657;&#20849;&#21516;&#32452;&#32455;&#65289;&#12290;&#26412;&#25991;&#24182;&#19981;&#24847;&#21619;&#30528;&#20840;&#38754;&#65292;&#32780;&#26159;&#35797;&#22270;&#32508;&#21512;&#19968;&#20123;&#26377;&#36259;&#30340;&#30740;&#35752;&#20250;&#21457;&#29616;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#20027;&#39064;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#35770;&#25991;&#26082;&#20026;&#36825;&#20010;&#37325;&#35201;&#20027;&#39064;&#30340;&#35752;&#35770;&#25552;&#20379;&#19968;&#20010;&#36215;&#28857;&#65292;&#20063;&#24341;&#36215;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Every major technical invention resurfaces the dual-use dilemma -- the new technology has the potential to be used for good as well as for harm. Generative AI (GenAI) techniques, such as large language models (LLMs) and diffusion models, have shown remarkable capabilities (e.g., in-context learning, code-completion, and text-to-image generation and editing). However, GenAI can be used just as well by attackers to generate new attacks and increase the velocity and efficacy of existing attacks.  This paper reports the findings of a workshop held at Google (co-organized by Stanford University and the University of Wisconsin-Madison) on the dual-use dilemma posed by GenAI. This paper is not meant to be comprehensive, but is rather an attempt to synthesize some of the interesting findings from the workshop. We discuss short-term and long-term goals for the community on this topic. We hope this paper provides both a launching point for a discussion on this important topic as well as interest
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#22823;&#37327;&#30340;&#20998;&#24067;&#19978;&#25552;&#20379;&#23545;&#40657;&#30418;&#31995;&#32479;&#34892;&#20026;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.14815</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Statistical Verification with Imprecise Neural Networks. (arXiv:2308.14815v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#22823;&#37327;&#30340;&#20998;&#24067;&#19978;&#25552;&#20379;&#23545;&#40657;&#30418;&#31995;&#32479;&#34892;&#20026;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#23433;&#20840;&#39046;&#22495;&#65292;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26159;&#22312;&#39640;&#32500;&#33258;&#20027;&#31995;&#32479;&#30340;&#34892;&#20026;&#19978;&#25552;&#20379;&#20445;&#35777;&#12290;&#20197;&#21487;&#36798;&#24615;&#20998;&#26512;&#20026;&#20013;&#24515;&#30340;&#39564;&#35777;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#65292;&#32780;&#32431;&#31929;&#30340;&#32479;&#35745;&#26041;&#27861;&#21463;&#21040;&#23545;&#37319;&#26679;&#36807;&#31243;&#30340;&#20998;&#24067;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#40657;&#30418;&#31995;&#32479;&#30340;&#20998;&#24067;&#40065;&#26834;&#29256;&#26412;&#30340;&#32479;&#35745;&#39564;&#35777;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#24615;&#33021;&#20445;&#35777;&#36866;&#29992;&#20110;&#22823;&#37327;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26680;&#24515;&#37096;&#20998;&#26159;&#19968;&#31181;&#31216;&#20026;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#23427;&#25552;&#20379;&#20102;&#19981;&#30830;&#23450;&#24615;&#20197;&#25351;&#23548;&#20027;&#21160;&#23398;&#20064;&#12290;&#20027;&#21160;&#23398;&#20064;&#20351;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026;Sherlock&#30340;&#20840;&#38754;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#24037;&#20855;&#26469;&#25910;&#38598;&#26679;&#26412;&#12290;&#22312;openAI gym Mujoco&#29615;&#22659;&#20013;&#20351;&#29992;&#22810;&#20010;&#29289;&#29702;&#27169;&#25311;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A particularly challenging problem in AI safety is providing guarantees on the behavior of high-dimensional autonomous systems. Verification approaches centered around reachability analysis fail to scale, and purely statistical approaches are constrained by the distributional assumptions about the sampling process. Instead, we pose a distributionally robust version of the statistical verification problem for black-box systems, where our performance guarantees hold over a large family of distributions. This paper proposes a novel approach based on a combination of active learning, uncertainty quantification, and neural network verification. A central piece of our approach is an ensemble technique called Imprecise Neural Networks, which provides the uncertainty to guide active learning. The active learning uses an exhaustive neural-network verification tool Sherlock to collect samples. An evaluation on multiple physical simulators in the openAI gym Mujoco environments with reinforcement-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#32422;&#26463;&#19979;&#29983;&#25104;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#37325;&#22797;&#21644;&#38544;&#31169;&#27844;&#38706;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.14784</link><description>&lt;p&gt;
&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#29983;&#25104;&#34920;&#26684;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Generating tabular datasets under differential privacy. (arXiv:2308.14784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14784
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#32422;&#26463;&#19979;&#29983;&#25104;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#37325;&#22797;&#21644;&#38544;&#31169;&#27844;&#38706;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#34892;&#19994;&#20013;&#25512;&#21160;&#20102;&#36827;&#23637;&#65292;&#20294;&#20854;&#20381;&#36182;&#20110;&#21487;&#35775;&#38382;&#21644;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#19968;&#20123;&#26368;&#37325;&#35201;&#30340;&#25968;&#25454;&#38598;&#20197;&#34920;&#26684;&#21644;&#20851;&#31995;&#25968;&#25454;&#24211;&#30340;&#24418;&#24335;&#20986;&#29616;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#37329;&#34701;&#39046;&#22495;&#12290;&#20294;&#36825;&#20123;&#34920;&#26684;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#25935;&#24863;&#24615;&#36136;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25581;&#31034;&#25935;&#24863;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#20294;&#29983;&#25104;&#27169;&#22411;&#24448;&#24448;&#20250;&#35760;&#24518;&#21644;&#37325;&#22797;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#30772;&#22351;&#38544;&#31169;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#25968;&#23398;&#26694;&#26550;&#34701;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#20294;&#36825;&#20250;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#38544;&#31169;&#20043;&#38388;&#20135;&#29983;&#26435;&#34913;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#30340;&#20027;&#35201;&#33539;&#24335;&#65292;&#20294;&#21463;&#21040;&#19981;&#31283;&#23450;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#27169;&#24335;&#22349;&#22604;&#30340;&#22256;&#25200;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#38544;&#31169;&#32422;&#26463;&#21644;&#22797;&#26434;&#30340;&#34920;&#26684;&#25968;&#25454;&#27169;&#24577;&#19979;&#26356;&#21152;&#20005;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) is accelerating progress across fields and industries, but relies on accessible and high-quality training data. Some of the most important datasets are found in biomedical and financial domains in the form of spreadsheets and relational databases. But this tabular data is often sensitive in nature. Synthetic data generation offers the potential to unlock sensitive data, but generative models tend to memorise and regurgitate training data, which undermines the privacy goal. To remedy this, researchers have incorporated the mathematical framework of Differential Privacy (DP) into the training process of deep neural networks. But this creates a trade-off between the quality and privacy of the resulting data. Generative Adversarial Networks (GANs) are the dominant paradigm for synthesising tabular data under DP, but suffer from unstable adversarial training and mode collapse, which are exacerbated by the privacy constraints and challenging tabular data modality. This 
&lt;/p&gt;</description></item><item><title>C3AL&#26159;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65292;&#36890;&#36807;&#23558;&#35266;&#27979;&#26641;&#20316;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#24182;&#26368;&#23567;&#21270;&#27979;&#35797;&#27425;&#25968;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.14781</link><description>&lt;p&gt;
&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conflict-Aware Active Automata Learning. (arXiv:2308.14781v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14781
&lt;/p&gt;
&lt;p&gt;
C3AL&#26159;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65292;&#36890;&#36807;&#23558;&#35266;&#27979;&#26641;&#20316;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#24182;&#26368;&#23567;&#21270;&#27979;&#35797;&#27425;&#25968;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#31639;&#27861;&#22312;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65288;&#21516;&#19968;&#36755;&#20837;&#23545;&#24212;&#19981;&#21516;&#36755;&#20986;&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#31181;&#22266;&#26377;&#30340;&#20914;&#31361;&#24674;&#22797;&#33021;&#21147;&#19981;&#36275;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#23384;&#22312;&#22122;&#22768;&#25110;&#23398;&#20064;&#20013;&#30340;&#31995;&#32479;&#21464;&#21270;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#65288;C3AL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#22788;&#29702;&#20914;&#31361;&#20449;&#24687;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#25152;&#35859;&#30340;&#35266;&#27979;&#26641;&#35270;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#12290;&#23613;&#31649;&#36825;&#20010;&#24819;&#27861;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#25506;&#32034;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#65292;&#24182;&#22312;&#38754;&#23545;&#20914;&#31361;&#26102;&#26368;&#23567;&#21270;&#23545;&#27491;&#22312;&#23398;&#20064;&#30340;&#31995;&#32479;&#25191;&#34892;&#30340;&#27979;&#35797;&#27425;&#25968;&#65292;&#20805;&#20998;&#21457;&#25381;&#20102;&#23427;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;C3AL&#65292;&#28085;&#30422;&#20102;30&#22810;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#30446;&#26631;&#21644;18,000&#22810;&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;C3AL&#26159;&#19968;&#20010;&#21512;&#36866;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active automata learning algorithms cannot easily handle \emph{conflict} in the observation data (different outputs observed for the same inputs). This inherent inability to recover after a conflict impairs their effective applicability in scenarios where noise is present or the system under learning is mutating.  We propose the Conflict-Aware Active Automata Learning (C3AL) framework to enable handling conflicting information during the learning process. The core idea is to consider the so-called observation tree as a first-class citizen in the learning process. Though this idea is explored in recent work, we take it to its full effect by enabling its use with any existing learner and minimizing the number of tests performed on the system under learning, specially in the face of conflicts. We evaluate C3AL in a large set of benchmarks, covering over 30 different realistic targets, and over 18,000 different scenarios. The results of the evaluation show that C3AL is a suitable alternati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#21147;&#23548;&#21521;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;3D&#20998;&#23376;&#26500;&#22411;&#65292;&#28085;&#30422;&#20102;&#24179;&#34913;&#21644;&#38750;&#24179;&#34913;&#25968;&#25454;&#12290;&#36890;&#36807;&#30452;&#25509;&#20174;&#21407;&#23376;&#21147;&#20013;&#23398;&#20064;&#38750;&#24179;&#34913;&#25968;&#25454;&#21644;&#20351;&#29992;&#38646;&#21147;&#27491;&#21017;&#21270;&#21644;&#22522;&#20110;&#21147;&#30340;&#21435;&#22122;&#25216;&#26415;&#36817;&#20284;&#36817;&#24179;&#34913;&#21147;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;1500&#19975;&#20010;&#22810;&#26679;&#26500;&#22411;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#26410;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#21147;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#22823;&#32422;3&#20493;&#12290;</title><link>http://arxiv.org/abs/2308.14759</link><description>&lt;p&gt;
&#24895;&#21407;&#21147;&#19982;&#20320;&#21516;&#22312;&#65306;&#32479;&#19968;&#30340;&#21147;&#23548;&#21521;&#39044;&#35757;&#32451;&#29992;&#20110;3D&#20998;&#23376;&#26500;&#22411;
&lt;/p&gt;
&lt;p&gt;
May the Force be with You: Unified Force-Centric Pre-Training for 3D Molecular Conformations. (arXiv:2308.14759v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#21147;&#23548;&#21521;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;3D&#20998;&#23376;&#26500;&#22411;&#65292;&#28085;&#30422;&#20102;&#24179;&#34913;&#21644;&#38750;&#24179;&#34913;&#25968;&#25454;&#12290;&#36890;&#36807;&#30452;&#25509;&#20174;&#21407;&#23376;&#21147;&#20013;&#23398;&#20064;&#38750;&#24179;&#34913;&#25968;&#25454;&#21644;&#20351;&#29992;&#38646;&#21147;&#27491;&#21017;&#21270;&#21644;&#22522;&#20110;&#21147;&#30340;&#21435;&#22122;&#25216;&#26415;&#36817;&#20284;&#36817;&#24179;&#34913;&#21147;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;1500&#19975;&#20010;&#22810;&#26679;&#26500;&#22411;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#26410;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#21147;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#22823;&#32422;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#23398;&#20064;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;3D&#20998;&#23376;&#34920;&#31034;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#24179;&#34913;&#25968;&#25454;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#38750;&#24179;&#34913;&#26500;&#22411;&#12290;&#23558;&#36825;&#20123;&#26041;&#27861;&#25193;&#23637;&#21040;&#38750;&#24179;&#34913;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#35757;&#32451;&#30446;&#26631;&#20381;&#36182;&#20110;&#26500;&#22411;&#26159;&#23616;&#37096;&#33021;&#37327;&#26497;&#23567;&#20540;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21147;&#23548;&#21521;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#29992;&#20110;3D&#20998;&#23376;&#26500;&#22411;&#30340;&#24179;&#34913;&#21644;&#38750;&#24179;&#34913;&#25968;&#25454;&#12290;&#23545;&#20110;&#38750;&#24179;&#34913;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30452;&#25509;&#20174;&#21407;&#23376;&#21147;&#20013;&#23398;&#20064;&#12290;&#23545;&#20110;&#24179;&#34913;&#25968;&#25454;&#65292;&#25105;&#20204;&#24341;&#20837;&#38646;&#21147;&#27491;&#21017;&#21270;&#21644;&#22522;&#20110;&#21147;&#30340;&#21435;&#22122;&#25216;&#26415;&#26469;&#36817;&#20284;&#36817;&#24179;&#34913;&#21147;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#36229;&#36807;1500&#19975;&#20010;&#22810;&#26679;&#30340;&#26500;&#22411;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#26410;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#19979;&#65292;&#25105;&#20204;&#23558;&#21147;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#22823;&#32422;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown the promise of learning pre-trained models for 3D molecular representation. However, existing pre-training models focus predominantly on equilibrium data and largely overlook off-equilibrium conformations. It is challenging to extend these methods to off-equilibrium data because their training objective relies on assumptions of conformations being the local energy minima. We address this gap by proposing a force-centric pretraining model for 3D molecular conformations covering both equilibrium and off-equilibrium data. For off-equilibrium data, our model learns directly from their atomic forces. For equilibrium data, we introduce zero-force regularization and forced-based denoising techniques to approximate near-equilibrium forces. We obtain a unified pre-trained model for 3D molecular representation with over 15 million diverse conformations. Experiments show that, with our pre-training objective, we increase forces accuracy by around 3 times compared to the un
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#25512;&#26029;&#31639;&#27861;&#26469;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#32531;&#35299;&#26102;&#21464;&#37327;&#23376;&#22122;&#22768;&#65292;&#24182;&#23637;&#31034;&#20854;&#20248;&#20110;&#38750;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.14756</link><description>&lt;p&gt;
&#21487;&#33258;&#36866;&#24212;&#32531;&#35299;&#26102;&#21464;&#37327;&#23376;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Adaptive mitigation of time-varying quantum noise. (arXiv:2308.14756v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14756
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#25512;&#26029;&#31639;&#27861;&#26469;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#32531;&#35299;&#26102;&#21464;&#37327;&#23376;&#22122;&#22768;&#65292;&#24182;&#23637;&#31034;&#20854;&#20248;&#20110;&#38750;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#21463;&#21040;&#38750;&#31283;&#23450;&#30340;&#39640;&#35823;&#24046;&#29575;&#22122;&#22768;&#36890;&#36947;&#30340;&#24433;&#21709;&#65292;&#36825;&#21066;&#24369;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21644;&#32531;&#35299;&#38543;&#30528;&#36890;&#36947;&#26465;&#20214;&#21464;&#21270;&#30340;&#37327;&#23376;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#38656;&#35201;&#21160;&#24577;&#25512;&#26029;&#20851;&#38190;&#36890;&#36947;&#21442;&#25968;&#26469;&#25552;&#39640;&#31243;&#24207;&#31934;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#26469;&#24314;&#27169;&#27850;&#26494;&#22122;&#22768;&#30340;&#38543;&#26426;&#24615;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20174;&#32780;&#21487;&#20197;&#25913;&#21892;&#22312;&#26102;&#21464;&#22122;&#22768;&#19979;&#27010;&#29575;&#24615;&#35823;&#24046;&#25269;&#28040;&#65288;PEC&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#34920;&#24449;&#21644;&#32531;&#35299;&#37327;&#23376;&#22122;&#22768;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#23545;&#20110;&#24320;&#21457;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#37327;&#23376;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36125;&#21494;&#26031;PEC&#22312;&#20351;&#29992;Hellinger&#36317;&#31163;&#20174;&#29702;&#24819;&#20998;&#24067;&#36827;&#34892;&#27979;&#37327;&#26102;&#21487;&#20197;&#27604;&#38750;&#33258;&#36866;&#24212;&#26041;&#27861;&#25552;&#39640;4.5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current quantum computers suffer from non-stationary noise channels with high error rates, which undermines their reliability and reproducibility. We propose a Bayesian inference-based adaptive algorithm that can learn and mitigate quantum noise in response to changing channel conditions. Our study emphasizes the need for dynamic inference of critical channel parameters to improve program accuracy. We use the Dirichlet distribution to model the stochasticity of the Pauli channel. This allows us to perform Bayesian inference, which can improve the performance of probabilistic error cancellation (PEC) under time-varying noise. Our work demonstrates the importance of characterizing and mitigating temporal variations in quantum noise, which is crucial for developing more accurate and reliable quantum technologies. Our results show that Bayesian PEC can outperform non-adaptive approaches by a factor of 4.5x when measured using Hellinger distance from the ideal distribution.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#24378;&#21270;&#23398;&#20064;&#23637;&#31034;&#20102;&#20854;&#20174;&#22810;&#20010;&#35282;&#24230;&#24341;&#20837;&#20154;&#31867;&#24402;&#32435;&#20559;&#22909;&#30340;&#24378;&#22823;&#21644;&#28789;&#27963;&#24615;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#24615;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.14328</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Generative AI: A Survey. (arXiv:2308.14328v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14328
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#24378;&#21270;&#23398;&#20064;&#23637;&#31034;&#20102;&#20854;&#20174;&#22810;&#20010;&#35282;&#24230;&#24341;&#20837;&#20154;&#31867;&#24402;&#32435;&#20559;&#22909;&#30340;&#24378;&#22823;&#21644;&#28789;&#27963;&#24615;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#24615;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#26159;&#26426;&#22120;&#23398;&#20064;&#30028;&#20013;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#37325;&#35201;&#20027;&#39064;&#65292;&#21487;&#20197;&#24433;&#21709;&#21040;&#35832;&#22810;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#25991;&#26412;&#29983;&#25104;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#12290;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#33539;&#24335;&#26159;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#36890;&#36807;&#20943;&#23567;&#27169;&#22411;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25512;&#21160;&#23398;&#20064;&#22120;&#25429;&#25417;&#24182;&#36924;&#36817;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#12290;&#36825;&#31181;&#20844;&#24335;&#25104;&#21151;&#22320;&#24314;&#31435;&#20102;&#29983;&#25104;&#20219;&#21153;&#30340;&#30446;&#26631;&#65292;&#28982;&#32780;&#21364;&#26080;&#27861;&#28385;&#36275;&#20351;&#29992;&#32773;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25152;&#26377;&#38656;&#27714;&#12290;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#31454;&#20105;&#24615;&#36873;&#25321;&#65292;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#30446;&#26631;&#26469;&#27880;&#20837;&#26032;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#24378;&#22823;&#21644;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#24341;&#20837;&#20154;&#31867;&#24402;&#32435;&#20559;&#22909;&#65292;&#22914;&#23545;&#25239;&#23398;&#20064;&#12289;&#25163;&#21160;&#35774;&#35745;&#35268;&#21017;&#21644;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#24615;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Generative AI has been a long-standing essential topic in the machine learning community, which can impact a number of application areas like text generation and computer vision. The major paradigm to train a generative model is maximum likelihood estimation, which pushes the learner to capture and approximate the target data distribution by decreasing the divergence between the model distribution and the target distribution. This formulation successfully establishes the objective of generative tasks, while it is incapable of satisfying all the requirements that a user might expect from a generative model. Reinforcement learning, serving as a competitive option to inject new training signals by creating new objectives that exploit novel signals, has demonstrated its power and flexibility to incorporate human inductive bias from multiple angles, such as adversarial learning, hand-designed rules and learned reward model to build a performant model. Thereby, reinforcement learning ha
&lt;/p&gt;</description></item><item><title>chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.14120</link><description>&lt;p&gt;
&#25480;&#26435;&#20020;&#24202;&#21307;&#29983;&#24182;&#27665;&#20027;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#20020;&#24202;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290; (arXiv:2308.14120v2 [cs.LG] &#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14120
&lt;/p&gt;
&lt;p&gt;
chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24320;&#21457;&#32773;&#65288;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#65289;&#21644;&#20174;&#19994;&#32773;&#65288;&#22914;&#20020;&#24202;&#21307;&#29983;&#65289;&#20043;&#38388;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#65292;&#38459;&#30861;&#20102;ML&#22312;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;chatGPT Advanced Data Analysis&#65288;ADA&#65289;&#65292;&#21363;GPT-4&#30340;&#25193;&#23637;&#65292;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#24182;&#39640;&#25928;&#25191;&#34892;ML&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21521;chatGPT ADA&#25552;&#20379;&#20102;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#30340;&#22823;&#22411;&#35797;&#39564;&#30340;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21644;&#30740;&#31350;&#35814;&#32454;&#20449;&#24687;&#65292;&#27809;&#26377;&#32473;&#20986;&#20855;&#20307;&#25351;&#23548;&#12290;ChatGPT ADA&#22522;&#20110;&#21407;&#22987;&#30740;&#31350;&#30340;&#35757;&#32451;&#25968;&#25454;&#33258;&#20027;&#24320;&#21457;&#20102;&#26368;&#20808;&#36827;&#30340;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#65292;&#22914;&#30284;&#30151;&#21457;&#23637;&#12289;&#30284;&#30151;&#36827;&#23637;&#12289;&#30142;&#30149;&#24182;&#21457;&#30151;&#25110;&#33268;&#30149;&#22522;&#22240;&#24207;&#21015;&#31561;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;ML&#27169;&#22411;&#19982;&#20854;&#24050;&#21457;&#34920;&#30340;&#23545;&#24212;&#29289;&#30456;&#21305;&#37197;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;chatGPT ADA&#20026;&#27665;&#20027;&#21270;&#21307;&#23398;&#20013;&#30340;ML&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#20351;&#38750;ML&#19987;&#23478;&#33021;&#22815;&#33719;&#24471;&#20808;&#36827;&#30340;&#20998;&#26512;&#24037;&#20855;&#24182;&#25512;&#21160;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21160;&#24577;&#22270;&#65288;UniDG&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#34920;&#31034;&#32593;&#32476;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#24182;&#20445;&#30041;&#27169;&#24577;&#30340;&#29420;&#29305;&#24615;&#12290;&#37319;&#29992;&#21160;&#24577;&#22270;&#32858;&#31867;&#26041;&#27861;&#26500;&#24314;&#21487;&#38752;&#30340;&#20146;&#21644;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#30340;&#20851;&#32852;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.14105</link><description>&lt;p&gt;
&#38271;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#24615;&#35282;&#33394;&#20998;&#32452;&#30340;&#32479;&#19968;&#21160;&#24577;&#22270;
&lt;/p&gt;
&lt;p&gt;
Unified and Dynamic Graph for Temporal Character Grouping in Long Videos. (arXiv:2308.14105v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21160;&#24577;&#22270;&#65288;UniDG&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#34920;&#31034;&#32593;&#32476;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#24182;&#20445;&#30041;&#27169;&#24577;&#30340;&#29420;&#29305;&#24615;&#12290;&#37319;&#29992;&#21160;&#24577;&#22270;&#32858;&#31867;&#26041;&#27861;&#26500;&#24314;&#21487;&#38752;&#30340;&#20146;&#21644;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#30340;&#20851;&#32852;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#26102;&#38388;&#24615;&#35282;&#33394;&#20998;&#32452;&#26681;&#25454;&#35282;&#33394;&#30340;&#36523;&#20221;&#22312;&#35270;&#39057;&#20013;&#23450;&#20301;&#20986;&#29616;&#30340;&#26102;&#21051;&#12290; &#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20174;&#26080;&#30417;&#30563;&#32858;&#31867;&#21457;&#23637;&#21040;&#22522;&#20110;&#22270;&#30340;&#26377;&#30417;&#30563;&#32858;&#31867;&#12290; &#28982;&#32780;&#65292;&#22270;&#26041;&#27861;&#24314;&#31435;&#22312;&#22266;&#23450;&#30340;&#20146;&#21644;&#22270;&#21069;&#25552;&#19979;&#65292;&#24102;&#26469;&#20102;&#35768;&#22810;&#19981;&#31934;&#30830;&#30340;&#36830;&#25509;&#12290; &#27492;&#22806;&#65292;&#23427;&#20204;&#20351;&#29992;&#21508;&#31181;&#27169;&#22411;&#25552;&#21462;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#23545;&#37096;&#32626;&#19981;&#21451;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24615;&#35282;&#33394;&#20998;&#32452;&#30340;&#32479;&#19968;&#21160;&#24577;&#22270;&#65288;UniDG&#65289;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#34920;&#31034;&#32593;&#32476;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#21516;&#19968;&#31354;&#38388;&#20013;&#22810;&#20010;&#27169;&#24577;&#30340;&#34920;&#31034;&#65292;&#24182;&#21516;&#26102;&#20445;&#30041;&#20102;&#27169;&#24577;&#30340;&#29420;&#29305;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#21305;&#37197;&#31574;&#30053;&#20026;&#27599;&#20010;&#33410;&#28857;&#21160;&#24577;&#26500;&#24314;&#19981;&#21516;&#25968;&#37327;&#30340;&#37051;&#23621;&#65292;&#20197;&#33719;&#24471;&#26356;&#21487;&#38752;&#30340;&#20146;&#21644;&#22270;&#12290; &#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28176;&#36827;&#24335;&#30340;&#20851;&#32852;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Video temporal character grouping locates appearing moments of major characters within a video according to their identities. To this end, recent works have evolved from unsupervised clustering to graph-based supervised clustering. However, graph methods are built upon the premise of fixed affinity graphs, bringing many inexact connections. Besides, they extract multi-modal features with kinds of models, which are unfriendly to deployment. In this paper, we present a unified and dynamic graph (UniDG) framework for temporal character grouping. This is accomplished firstly by a unified representation network that learns representations of multiple modalities within the same space and still preserves the modality's uniqueness simultaneously. Secondly, we present a dynamic graph clustering where the neighbors of different quantities are dynamically constructed for each node via a cyclic matching strategy, leading to a more reliable affinity graph. Thirdly, a progressive association method 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23457;&#35270;&#65292;&#26088;&#22312;&#32416;&#27491;&#25968;&#25454;&#20998;&#24067;&#20559;&#24046;&#65292;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#21644;&#20195;&#34920;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13821</link><description>&lt;p&gt;
&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#32508;&#36848;&#65306;&#38382;&#39064;&#12289;&#25216;&#26415;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey of Imbalanced Learning on Graphs: Problems, Techniques, and Future Directions. (arXiv:2308.13821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23457;&#35270;&#65292;&#26088;&#22312;&#32416;&#27491;&#25968;&#25454;&#20998;&#24067;&#20559;&#24046;&#65292;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#21644;&#20195;&#34920;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#19982;&#19990;&#30028;&#21508;&#31181;&#22330;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#30456;&#20114;&#36830;&#25509;&#30340;&#32467;&#26500;&#12290;&#26377;&#25928;&#30340;&#22270;&#20998;&#26512;&#25216;&#26415;&#65292;&#22914;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#20174;&#22270;&#25968;&#25454;&#20013;&#33719;&#24471;&#28145;&#21051;&#30340;&#27934;&#23519;&#21147;&#65292;&#20026;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#36335;&#39044;&#27979;&#31561;&#21508;&#31181;&#20219;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#22270;&#25968;&#25454;&#20013;&#26576;&#20123;&#29255;&#27573;&#25317;&#26377;&#22823;&#37327;&#25968;&#25454;&#32780;&#20854;&#20182;&#25968;&#25454;&#31232;&#32570;&#65292;&#20174;&#32780;&#23548;&#33268;&#20559;&#20506;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#36825;&#23601;&#38656;&#35201;&#20986;&#29616;&#20102;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#26088;&#22312;&#32416;&#27491;&#36825;&#20123;&#25968;&#25454;&#20998;&#24067;&#20559;&#24046;&#65292;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#21644;&#20195;&#34920;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23457;&#35270;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#23545;&#35813;&#27010;&#24565;&#21644;&#30456;&#20851;&#26415;&#35821;&#30340;&#26126;&#30830;&#29702;&#35299;&#65292;&#20026;&#35835;&#32773;&#24314;&#31435;&#20102;&#25166;&#23454;&#30340;&#22522;&#30784;&#30693;&#35782;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65306;&#65288;1&#65289;&#38382;&#39064;&#20998;&#31867;&#27861;&#65288;Problem Taxonomy&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs represent interconnected structures prevalent in a myriad of real-world scenarios. Effective graph analytics, such as graph learning methods, enables users to gain profound insights from graph data, underpinning various tasks including node classification and link prediction. However, these methods often suffer from data imbalance, a common issue in graph data where certain segments possess abundant data while others are scarce, thereby leading to biased learning outcomes. This necessitates the emerging field of imbalanced learning on graphs, which aims to correct these data distribution skews for more accurate and representative learning outcomes. In this survey, we embark on a comprehensive review of the literature on imbalanced learning on graphs. We begin by providing a definitive understanding of the concept and related terminologies, establishing a strong foundational understanding for readers. Following this, we propose two comprehensive taxonomies: (1) the problem taxono
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20223;&#21046;&#35895;&#27468;&#30524;&#21160;&#35770;&#25991;&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#23454;&#29616;&#19982;&#35895;&#27468;&#35770;&#25991;&#30456;&#24403;&#30340;&#20934;&#30830;&#30524;&#21160;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.13495</link><description>&lt;p&gt;
&#24320;&#25918;&#27880;&#35270;&#65306;&#19968;&#20010;&#20223;&#21046;&#35895;&#27468;&#30524;&#21160;&#35770;&#25991;&#30340;&#24320;&#28304;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Open Gaze: An Open-Source Implementation Replicating Google's Eye Tracking Paper. (arXiv:2308.13495v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20223;&#21046;&#35895;&#27468;&#30524;&#21160;&#35770;&#25991;&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#23454;&#29616;&#19982;&#35895;&#27468;&#35770;&#25991;&#30456;&#24403;&#30340;&#20934;&#30830;&#30524;&#21160;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30524;&#21160;&#24050;&#32463;&#25104;&#20026;&#35270;&#35273;&#30740;&#31350;&#12289;&#35821;&#35328;&#20998;&#26512;&#21644;&#21487;&#29992;&#24615;&#35780;&#20272;&#31561;&#19981;&#21516;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;&#19987;&#38376;&#30340;&#12289;&#26114;&#36149;&#30340;&#30524;&#21160;&#36861;&#36394;&#30828;&#20214;&#30340;&#25193;&#23637;&#24335;&#26700;&#38754;&#26174;&#31034;&#22120;&#19978;&#12290;&#23613;&#31649;&#26234;&#33021;&#25163;&#26426;&#30340;&#26222;&#21450;&#29575;&#21644;&#20351;&#29992;&#39057;&#29575;&#24456;&#39640;&#65292;&#20294;&#23545;&#20110;&#26234;&#33021;&#25163;&#26426;&#19978;&#30340;&#30524;&#29699;&#31227;&#21160;&#27169;&#24335;&#21364;&#40092;&#26377;&#35265;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#24320;&#28304;&#27880;&#35270;&#36861;&#36394;&#23454;&#29616;&#65292;&#27169;&#25311;&#20102;&#35895;&#27468;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#65288;&#20854;&#28304;&#20195;&#30721;&#20173;&#28982;&#26159;&#19987;&#26377;&#30340;&#65289;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#30828;&#20214;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#19982;&#35895;&#27468;&#35770;&#25991;&#26041;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#31181;&#26412;&#22320;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#20934;&#30830;&#30524;&#21160;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#31227;&#21160;&#30524;&#21160;&#36861;&#36394;&#22120;&#30456;&#24403;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eye tracking has been a pivotal tool in diverse fields such as vision research, language analysis, and usability assessment. The majority of prior investigations, however, have concentrated on expansive desktop displays employing specialized, costly eye tracking hardware that lacks scalability. Remarkably little insight exists into ocular movement patterns on smartphones, despite their widespread adoption and significant usage. In this manuscript, we present an open-source implementation of a smartphone-based gaze tracker that emulates the methodology proposed by a GooglePaper (whose source code remains proprietary). Our focus is on attaining accuracy comparable to that attained through the GooglePaper's methodology, without the necessity for supplementary hardware. Through the integration of machine learning techniques, we unveil an accurate eye tracking solution that is native to smartphones. Our approach demonstrates precision akin to the state-of-the-art mobile eye trackers, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;RL-EA&#65289;&#65292;&#35813;&#31639;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#23545;&#21508;&#31181;RL-EA&#30340;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#21644;&#25628;&#32034;&#27169;&#24335;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.13420</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65306;&#35843;&#26597;&#21644;&#30740;&#31350;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities. (arXiv:2308.13420v2 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;RL-EA&#65289;&#65292;&#35813;&#31639;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#23545;&#21508;&#31181;RL-EA&#30340;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#21644;&#25628;&#32034;&#27169;&#24335;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#26159;&#19968;&#31867;&#22522;&#20110;&#33258;&#28982;&#36827;&#21270;&#21407;&#29702;&#30340;&#38543;&#26426;&#25628;&#32034;&#26041;&#27861;&#65292;&#22240;&#20854;&#22312;&#21508;&#31181;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#32780;&#24191;&#21463;&#36190;&#35465;&#12290;&#23613;&#31649;&#20840;&#29699;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#36827;&#21270;&#31639;&#27861;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#23398;&#32773;&#31215;&#26497;&#25506;&#32034;&#25913;&#36827;&#31639;&#27861;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#12289;&#25628;&#32034;&#27169;&#24335;&#31561;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#20248;&#21270;&#24615;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#36827;&#21270;&#31639;&#27861;&#26694;&#26550;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#36229;&#36234;&#24615;&#33021;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#21040;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#34987;&#31216;&#20026;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;RL-EA&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;RL-EA&#20013;&#19981;&#21516;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#21644;&#25628;&#32034;&#27169;&#24335;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary algorithms (EA), a class of stochastic search methods based on the principles of natural evolution, have received widespread acclaim for their exceptional performance in various real-world optimization problems. While researchers worldwide have proposed a wide variety of EAs, certain limitations remain, such as slow convergence speed and poor generalization capabilities. Consequently, numerous scholars actively explore improvements to algorithmic structures, operators, search patterns, etc., to enhance their optimization performance. Reinforcement learning (RL) integrated as a component in the EA framework has demonstrated superior performance in recent years. This paper presents a comprehensive survey on integrating reinforcement learning into the evolutionary algorithm, referred to as reinforcement learning-assisted evolutionary algorithm (RL-EA). We begin with the conceptual outlines of reinforcement learning and the evolutionary algorithm. We then provide a taxonomy of
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#20840;&#31216;&#37327;&#21270;&#27010;&#24565;&#25193;&#23637;&#20102;&#25551;&#36848;&#36923;&#36753;$\mathcal{EL}$&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#27169;&#24335;&#35821;&#20041;&#21644;&#20108;&#38454;&#35821;&#20041;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#24615;&#36136;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#26377;&#29992;&#29255;&#27573;&#20013;&#30340;&#32467;&#35770;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2308.08252</link><description>&lt;p&gt;
&#25551;&#36848;&#36923;&#36753;&#36827;&#20837;&#20108;&#38454;--&#29992;&#20840;&#31216;&#37327;&#21270;&#27010;&#24565;&#25193;&#23637;EL
&lt;/p&gt;
&lt;p&gt;
Description Logics Go Second-Order -- Extending EL with Universally Quantified Concepts. (arXiv:2308.08252v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08252
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#20840;&#31216;&#37327;&#21270;&#27010;&#24565;&#25193;&#23637;&#20102;&#25551;&#36848;&#36923;&#36753;$\mathcal{EL}$&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#27169;&#24335;&#35821;&#20041;&#21644;&#20108;&#38454;&#35821;&#20041;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#24615;&#36136;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#26377;&#29992;&#29255;&#27573;&#20013;&#30340;&#32467;&#35770;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21382;&#21490;&#19978;&#65292;&#25551;&#36848;&#36923;&#36753;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21487;&#20197;&#32763;&#35793;&#25104;&#21487;&#21028;&#23450;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#30340;&#29305;&#24449;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25918;&#24323;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#23547;&#25214;&#26377;&#29992;&#19988;&#21487;&#21028;&#23450;&#30340;&#19968;&#38454;&#36923;&#36753;&#20197;&#22806;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#31216;&#37327;&#21270;&#27010;&#24565;&#65292;&#23427;&#20204;&#20197;&#21487;&#20197;&#34987;&#20219;&#24847;&#27010;&#24565;&#26367;&#25442;&#30340;&#21464;&#37327;&#24418;&#24335;&#20986;&#29616;&#65292;&#24182;&#23450;&#20041;&#20102;&#36825;&#20010;&#25193;&#23637;&#30340;&#20004;&#31181;&#35821;&#20041;&#12290;&#27169;&#24335;&#35821;&#20041;&#21482;&#20801;&#35768;&#27010;&#24565;&#21464;&#37327;&#34987;&#29305;&#23450;&#35821;&#35328;&#30340;&#27010;&#24565;&#26367;&#25442;&#65292;&#20135;&#29983;&#31867;&#20284;&#27169;&#24577;&#36923;&#36753;&#30340;&#20844;&#29702;&#27169;&#24335;&#12290;&#20108;&#38454;&#35821;&#20041;&#20801;&#35768;&#27010;&#24565;&#21464;&#37327;&#34987;&#22495;&#30340;&#20219;&#24847;&#23376;&#38598;&#26367;&#25442;&#65292;&#31867;&#20284;&#20110;&#20108;&#38454;&#36923;&#36753;&#20013;&#30340;&#37327;&#21270;&#35859;&#35789;&#12290;&#20026;&#20102;&#30740;&#31350;&#25152;&#25552;&#20986;&#30340;&#35821;&#20041;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#25551;&#36848;&#36923;&#36753;$\mathcal{EL}$&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25193;&#23637;&#30340;&#26377;&#29992;&#29255;&#27573;&#20013;&#65292;&#19981;&#21516;&#35821;&#20041;&#25152;&#34164;&#21547;&#30340;&#32467;&#35770;&#26159;&#30456;&#21516;&#30340;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#32463;&#20856;&#36923;&#36753;&#30340;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of Description Logics have been historically mostly focused on features that can be translated to decidable fragments of first-order logic. In this paper, we leave this restriction behind and look for useful and decidable extensions outside first-order logic. We introduce universally quantified concepts, which take the form of variables that can be replaced with arbitrary concepts, and define two semantics of this extension. A schema semantics allows replacements of concept variables only by concepts from a particular language, giving us axiom schemata similar to modal logics. A second-order semantics allows replacement of concept variables with arbitrary subsets of the domain, which is similar to quantified predicates in second-order logic.  To study the proposed semantics, we focus on the extension of the description logic $\mathcal{EL}$. We show that for a useful fragment of the extension, the conclusions entailed by the different semantics coincide, allowing us to use cla
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NBIAS&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#31561;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#26469;&#35782;&#21035;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01681</link><description>&lt;p&gt;
NBIAS: &#29992;&#20110;&#25991;&#26412;&#20013;&#20559;&#35265;&#35782;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NBIAS: A Natural Language Processing Framework for Bias Identification in Text. (arXiv:2308.01681v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NBIAS&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#31561;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#26469;&#35782;&#21035;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#20013;&#23384;&#22312;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#20351;&#29992;&#26102;&#20135;&#29983;&#20542;&#26012;&#30340;&#35299;&#37322;&#21644;&#32467;&#26524;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#20250;&#25345;&#32493;&#24378;&#21270;&#21051;&#26495;&#21360;&#35937;&#12289;&#27495;&#35270;&#25110;&#20854;&#20182;&#24418;&#24335;&#30340;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;&#22312;&#26377;&#20559;&#35265;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31639;&#27861;&#26368;&#32456;&#20250;&#20570;&#20986;&#19981;&#24179;&#31561;&#24433;&#21709;&#26576;&#20010;&#32676;&#20307;&#30340;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#21644;&#28040;&#38500;&#36825;&#20123;&#20559;&#35265;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#23545;&#25968;&#25454;&#30340;&#20844;&#24179;&#21644;&#36947;&#24503;&#20351;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#32780;&#24378;&#22823;&#30340;&#26694;&#26550;"NBIAS"&#65292;&#23427;&#21253;&#25324;&#25968;&#25454;&#23618;&#12289;&#35821;&#26009;&#24211;&#26500;&#24314;&#12289;&#27169;&#22411;&#24320;&#21457;&#23618;&#21644;&#35780;&#20272;&#23618;&#12290;&#25968;&#25454;&#38598;&#30001;&#20174;&#21508;&#20010;&#39046;&#22495;&#25910;&#38598;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#65292;&#21253;&#25324;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#38376;&#25143;&#32593;&#31449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#20010;&#21807;&#19968;&#30340;&#21629;&#21517;&#23454;&#20307;&#33021;&#22815;&#35782;&#21035;&#20986;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias in textual data can lead to skewed interpretations and outcomes when the data is used. These biases could perpetuate stereotypes, discrimination, or other forms of unfair treatment. An algorithm trained on biased data ends up making decisions that disproportionately impact a certain group of people. Therefore, it is crucial to detect and remove these biases to ensure the fair and ethical use of data. To this end, we develop a comprehensive and robust framework \textsc{Nbias} that consists of a data layer, corpus contruction, model development layer and an evaluation layer. The dataset is constructed by collecting diverse data from various fields, including social media, healthcare, and job hiring portals. As such, we applied a transformer-based token classification model that is able to identify bias words/ phrases through a unique named entity. In the assessment procedure, we incorporate a blend of quantitative and qualitative evaluations to gauge the effectiveness of our models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;ChatGPT&#22312;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#30740;&#31350;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;ChatGPT&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.12488</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;&#36719;&#20214;&#23433;&#20840;&#65306;&#25506;&#32034;ChatGPT&#22312;&#23433;&#20840;&#24212;&#29992;&#20013;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications. (arXiv:2307.12488v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;ChatGPT&#22312;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#30740;&#31350;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;ChatGPT&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#22810;&#25165;&#22810;&#33402;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;ChatGPT&#22312;&#21508;&#20010;&#39046;&#22495;&#24212;&#23545;&#38382;&#39064;&#30340;&#28508;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#23637;&#31034;&#12290;&#23427;&#33021;&#22815;&#20998;&#26512;&#12289;&#29702;&#35299;&#21644;&#32508;&#21512;&#26469;&#33258;&#22312;&#32447;&#36164;&#28304;&#21644;&#29992;&#25143;&#36755;&#20837;&#30340;&#20449;&#24687;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;ChatGPT&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#23457;&#26597;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;ChatGPT&#22312;&#38754;&#21521;&#23433;&#20840;&#30340;&#31243;&#24207;&#20998;&#26512;&#20013;&#30340;&#33021;&#21147;&#65292;&#20174;&#25915;&#20987;&#32773;&#21644;&#23433;&#20840;&#20998;&#26512;&#24072;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#35780;&#20272;ChatGPT&#22312;&#20960;&#20010;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22238;&#31572;&#36136;&#37327;&#65292;&#24182;&#26377;&#24847;&#22320;&#24341;&#20837;&#25361;&#25112;&#26469;&#35780;&#20272;&#20854;&#21709;&#24212;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;ChatGPT&#25552;&#20379;&#30340;&#31572;&#26696;&#36136;&#37327;&#30340;&#32771;&#23519;&#65292;&#25105;&#20204;&#23545;&#20854;&#22312;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#39046;&#22495;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#26377;&#20102;&#26356;&#28165;&#26224;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, as a versatile large language model, has demonstrated remarkable potential in addressing inquiries across various domains. Its ability to analyze, comprehend, and synthesize information from both online sources and user inputs has garnered significant attention. Previous research has explored ChatGPT's competence in code generation and code reviews. In this paper, we delve into ChatGPT's capabilities in security-oriented program analysis, focusing on perspectives from both attackers and security analysts. We present a case study involving several security-oriented program analysis tasks while deliberately introducing challenges to assess ChatGPT's responses. Through an examination of the quality of answers provided by ChatGPT, we gain a clearer understanding of its strengths and limitations in the realm of security-oriented program analysis.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22788;&#29702;&#35270;&#39057;&#24103;&#21644;&#28145;&#24230;&#32454;&#33410;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#31995;&#32479;&#26469;&#39044;&#27979;&#39550;&#39542;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#27604;&#21333;&#29420;&#20351;&#29992;&#35270;&#39057;&#24103;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.11058</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31574;&#30053;&#39044;&#27979;&#26469;&#39044;&#27979;&#39550;&#39542;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Anticipating Driving Behavior through Deep Learning-Based Policy Prediction. (arXiv:2307.11058v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11058
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22788;&#29702;&#35270;&#39057;&#24103;&#21644;&#28145;&#24230;&#32454;&#33410;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#31995;&#32479;&#26469;&#39044;&#27979;&#39550;&#39542;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#27604;&#21333;&#29420;&#20351;&#29992;&#35270;&#39057;&#24103;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#31995;&#32479;&#65292;&#36890;&#36807;&#22788;&#29702;&#30001;&#26222;&#36890;&#25668;&#20687;&#22836;&#25293;&#25668;&#30340;&#35270;&#39057;&#24103;&#34893;&#29983;&#20986;&#30340;&#32508;&#21512;&#35270;&#35273;&#29305;&#24449;&#20197;&#21450;&#20174;&#28857;&#20113;&#25195;&#25551;&#20202;&#33719;&#24471;&#30340;&#28145;&#24230;&#32454;&#33410;&#12290;&#35813;&#31995;&#32479;&#26088;&#22312;&#39044;&#27979;&#39550;&#39542;&#34892;&#20026;&#65292;&#21253;&#25324;&#36710;&#36742;&#36895;&#24230;&#21644;&#36716;&#21521;&#35282;&#24230;&#12290;&#20026;&#20102;&#30830;&#20445;&#20854;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23558;&#39044;&#27979;&#32467;&#26524;&#19982;&#29087;&#32451;&#30340;&#30495;&#23454;&#39550;&#39542;&#21592;&#36981;&#24490;&#30340;&#26082;&#23450;&#35268;&#33539;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#27979;&#22312;&#33267;&#23569;&#19968;&#21322;&#30340;&#27979;&#35797;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#65288;&#26681;&#25454;&#20855;&#20307;&#27169;&#22411;&#65292;&#22312;50-80%&#20043;&#38388;&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20351;&#29992;&#32508;&#21512;&#29305;&#24449;&#30456;&#27604;&#20110;&#21482;&#20351;&#29992;&#35270;&#39057;&#24103;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this endeavor, we developed a comprehensive system that processes integrated visual features derived from video frames captured by a regular camera, along with depth details obtained from a point cloud scanner. This system is designed to anticipate driving actions, encompassing both vehicle speed and steering angle. To ensure its reliability, we conducted assessments where we juxtaposed the projected outcomes with the established norms adhered to by skilled real-world drivers. Our evaluation outcomes indicate that the forecasts achieve a noteworthy level of accuracy in a minimum of half the test scenarios (ranging around 50-80%, contingent on the specific model). Notably, the utilization of amalgamated features yielded superior performance in comparison to using video frames in isolation, as demonstrated by most of the cases.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#33539;&#22260;&#23457;&#26597;&#26041;&#27861;&#35843;&#26597;&#20102;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24182;&#37319;&#29992;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#30340;&#35770;&#25991;&#65292;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#27169;&#22411;&#36825;&#19968;&#26415;&#35821;&#30340;&#21547;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.09673</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#21487;&#35299;&#37322;&#27169;&#22411;&#65306;&#19968;&#39033;&#33539;&#22260;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
What's meant by explainable model: A Scoping Review. (arXiv:2307.09673v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09673
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#33539;&#22260;&#23457;&#26597;&#26041;&#27861;&#35843;&#26597;&#20102;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24182;&#37319;&#29992;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#30340;&#35770;&#25991;&#65292;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#27169;&#22411;&#36825;&#19968;&#26415;&#35821;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32463;&#24120;&#22312;&#25551;&#36848;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24212;&#29992;&#30340;&#35770;&#25991;&#26631;&#39064;&#20013;&#30475;&#21040;&#21487;&#35299;&#37322;&#36825;&#20010;&#26415;&#35821;&#12290;&#28982;&#32780;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;XAI&#20013;&#30340;&#35299;&#37322;&#26159;&#29305;&#23450;&#24212;&#29992;&#21644;&#39046;&#22495;&#30340;&#65292;&#22240;&#27492;&#22312;&#29992;&#20110;&#35299;&#37322;&#29305;&#23450;&#24212;&#29992;&#38382;&#39064;&#30340;&#27169;&#22411;&#26102;&#38656;&#35201;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25991;&#29486;&#25581;&#31034;&#20102;&#20107;&#21518;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#24615;&#33021;&#23384;&#22312;&#24456;&#22823;&#24046;&#24322;&#65292;&#26263;&#31034;&#23427;&#20204;&#24182;&#19981;&#33021;&#25104;&#20026;AI&#21487;&#35299;&#37322;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#22312;&#20351;&#29992;XAI&#26041;&#27861;&#26102;&#65292;&#24212;&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#35780;&#20272;&#20854;&#20449;&#24687;&#36755;&#20986;&#30340;&#36136;&#37327;&#21644;&#36866;&#29992;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#22240;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#33539;&#22260;&#23457;&#26597;&#26041;&#27861;&#26469;&#30740;&#31350;&#24212;&#29992;AI&#27169;&#22411;&#21644;&#37319;&#29992;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#30340;&#35770;&#25991;&#65292;&#21516;&#26102;&#23558;&#36825;&#20123;&#27169;&#22411;&#31216;&#20026;&#21487;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We often see the term explainable in the titles of papers that describe applications based on artificial intelligence (AI). However, the literature in explainable artificial intelligence (XAI) indicates that explanations in XAI are application- and domain-specific, hence requiring evaluation whenever they are employed to explain a model that makes decisions for a specific application problem. Additionally, the literature reveals that the performance of post-hoc methods, particularly feature attribution methods, varies substantially hinting that they do not represent a solution to AI explainability. Therefore, when using XAI methods, the quality and suitability of their information outputs should be evaluated within the specific application. For these reasons, we used a scoping review methodology to investigate papers that apply AI models and adopt methods to generate post-hoc explanations while referring to said models as explainable. This paper investigates whether the term explainabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.16740</link><description>&lt;p&gt;
&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Principles and Guidelines for Evaluating Social Robot Navigation Algorithms. (arXiv:2306.16740v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#26159;&#37096;&#32626;&#26426;&#22120;&#20154;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#34429;&#28982;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#35780;&#20272;&#35299;&#20915;&#31038;&#20132;&#23548;&#33322;&#30340;&#31639;&#27861;&#20173;&#28982;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#28041;&#21450;&#26426;&#22120;&#20154;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#31227;&#21160;&#65292;&#36824;&#28041;&#21450;&#21040;&#21160;&#24577;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#21450;&#20854;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24863;&#30693;&#36866;&#24212;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#28165;&#26224;&#12289;&#21487;&#37325;&#22797;&#12289;&#26131;&#20110;&#33719;&#24471;&#30340;&#22522;&#20934;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20256;&#32479;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#39046;&#22495;&#21152;&#36895;&#20102;&#36827;&#23637;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20844;&#24179;&#27604;&#36739;&#31639;&#27861;&#65292;&#25581;&#31034;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21576;&#29616;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#21521;&#12290;&#25105;&#20204;&#30456;&#20449;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#21161;&#20110;&#31038;&#20132;&#23548;&#33322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#24314;&#31435;&#20102;&#20849;&#21516;&#12289;&#24191;&#27867;&#21487;&#29992;&#19988;&#21487;&#37325;&#22797;&#30340;&#22522;&#20934;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#24049;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributio
&lt;/p&gt;</description></item><item><title>DR-HAI&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20114;&#21160;&#35843;&#21644;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#24322;&#65292;&#20026;&#20419;&#36827;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.14694</link><description>&lt;p&gt;
DR-HAI: &#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#20013;&#22522;&#20110;&#35770;&#35777;&#30340;&#36777;&#35777;&#35843;&#21644;
&lt;/p&gt;
&lt;p&gt;
DR-HAI: Argumentation-based Dialectical Reconciliation in Human-AI Interactions. (arXiv:2306.14694v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14694
&lt;/p&gt;
&lt;p&gt;
DR-HAI&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20114;&#21160;&#35843;&#21644;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#24322;&#65292;&#20026;&#20419;&#36827;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DR-HAI&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25193;&#23637;&#20154;&#31867;&#24863;&#30693;&#35268;&#21010;&#20013;&#24120;&#29992;&#30340;&#27169;&#22411;&#35843;&#21644;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#35770;&#35777;&#30340;&#23545;&#35805;&#33539;&#24335;&#65292;DR-HAI&#33021;&#22815;&#36827;&#34892;&#20114;&#21160;&#35843;&#21644;&#65292;&#35299;&#20915;&#35299;&#37322;&#32773;&#21644;&#34987;&#35299;&#37322;&#32773;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#24322;&#12290;&#25105;&#20204;&#23545;DR-HAI&#30340;&#25805;&#20316;&#35821;&#20041;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#25551;&#36848;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#23545;&#20854;&#25928;&#26524;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DR-HAI&#20026;&#20419;&#36827;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#28508;&#21147;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DR-HAI -- a novel argumentation-based framework designed to extend model reconciliation approaches, commonly used in human-aware planning, for enhanced human-AI interaction. By adopting an argumentation-based dialogue paradigm, DR-HAI enables interactive reconciliation to address knowledge discrepancies between an explainer and an explainee. We formally describe the operational semantics of DR-HAI, provide theoretical guarantees, and empirically evaluate its efficacy. Our findings suggest that DR-HAI offers a promising direction for fostering effective human-AI interactions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20986;&#29616;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.11167</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35823;&#23548;&#65306;&#20351;&#29992;Only Connect Wall&#25968;&#25454;&#38598;&#25506;&#32034;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#21644;Einstellung&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. (arXiv:2306.11167v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20986;&#29616;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#20154;&#24037;&#26234;&#33021;&#35806;&#29983;&#20197;&#26469;&#65292;&#23545;&#20154;&#31867;&#20223;&#30495;&#26234;&#33021;&#30340;&#36861;&#27714;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#25345;&#20037;&#35805;&#39064;&#12290;&#26368;&#26032;&#19968;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25216;&#26415;&#28436;&#36827;&#21644;&#26032;&#20852;&#33021;&#21147;&#23558;&#36825;&#20010;&#20027;&#39064;&#20174;&#23398;&#26415;&#30028;&#24102;&#21040;&#20102;&#25991;&#21270;&#26102;&#20195;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;NLP&#35780;&#20272;&#22522;&#20934;&#20219;&#21153;&#27979;&#35797;&#20102;&#20154;&#31867;&#20223;&#30495;&#34892;&#20026;&#30340;&#19968;&#20123;&#26041;&#38754;&#65288;&#20363;&#22914;BIG-bench&#30340;&#8220;&#31867;&#20154;&#34892;&#20026;&#8221;&#20219;&#21153;&#65289;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#19968;&#20010;&#20219;&#21153;&#32771;&#23519;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20154;&#31867;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#26159;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#20013;&#30740;&#31350;&#36739;&#20026;&#28145;&#20837;&#30340;&#20027;&#39064;&#65292;&#26631;&#20934;&#21270;&#27979;&#35797;&#20027;&#35201;&#20351;&#29992;&#23558;&#32447;&#32034;&#35789;&#20043;&#38388;&#30340;&#65288;&#24322;&#26500;&#65289;&#36830;&#25509;&#33021;&#21147;&#20316;&#20026;&#21019;&#36896;&#24615;&#30340;&#24230;&#37327;&#12290;&#22312;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#65292;&#26263;&#31034;&#24615;&#30340;&#35823;&#23548;&#24615;&#21050;&#28608;-&#34987;&#31216;&#20026;&#8220;&#35825;&#23548;&#35823;&#35299;&#8221;&#30340;&#24178;&#25200;&#22240;&#32032;-&#36890;&#36807;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#38459;&#30861;&#20102;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#22312;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20107;&#20808;&#35753;&#21442;&#19982;&#32773;&#25509;&#35302;&#21040;&#26377;&#30456;&#20284;&#25340;&#20889;&#30340;&#38169;&#35823;&#22240;&#32032;&#26469;&#23454;&#39564;&#24615;&#22320;&#35825;&#23548;&#36825;&#26679;&#30340;&#22266;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quest for human imitative AI has been an enduring topic in AI research since its inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to the cultural zeitgeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behaviour (e.g., BIG-bench's 'human-like behavior' tasks), few, if not none, examine creative problem solving abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use the ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incor
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#23545;&#25968;&#25454;&#26631;&#27880;&#30340;&#24433;&#21709;&#24456;&#37325;&#35201;&#12290;&#36890;&#36807;POPQUORN&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#22312;&#20182;&#20204;&#30340;&#21028;&#26029;&#20013;&#36215;&#21040;&#20102;&#26174;&#33879;&#20316;&#29992;&#65292;&#24182;&#19988;&#24212;&#35813;&#32771;&#34385;&#20197;&#21069;&#26410;&#32771;&#34385;&#30340;&#32972;&#26223;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24314;&#35758;&#29702;&#35299;&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#65292;&#20174;&#20855;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#34913;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#20013;&#25910;&#38598;&#26631;&#31614;&#65292;&#20197;&#20943;&#23569;&#25968;&#25454;&#38598;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.06826</link><description>&lt;p&gt;
Annotator Demographics Matter - Measuring the Influence of Annotator Demographics with the POPQUORN Dataset
&lt;/p&gt;
&lt;p&gt;
When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the POPQUORN Dataset. (arXiv:2306.06826v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06826
&lt;/p&gt;
&lt;p&gt;
&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#23545;&#25968;&#25454;&#26631;&#27880;&#30340;&#24433;&#21709;&#24456;&#37325;&#35201;&#12290;&#36890;&#36807;POPQUORN&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#22312;&#20182;&#20204;&#30340;&#21028;&#26029;&#20013;&#36215;&#21040;&#20102;&#26174;&#33879;&#20316;&#29992;&#65292;&#24182;&#19988;&#24212;&#35813;&#32771;&#34385;&#20197;&#21069;&#26410;&#32771;&#34385;&#30340;&#32972;&#26223;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24314;&#35758;&#29702;&#35299;&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#65292;&#20174;&#20855;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#34913;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#20013;&#25910;&#38598;&#26631;&#31614;&#65292;&#20197;&#20943;&#23569;&#25968;&#25454;&#38598;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#21644;&#32463;&#21382;&#20250;&#24433;&#21709;&#20182;&#20204;&#23545;&#25968;&#25454;&#30340;&#26631;&#27880;&#65292;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21482;&#36817;&#26399;&#24320;&#22987;&#32771;&#34385;&#26631;&#27880;&#32773;&#36523;&#20221;&#23545;&#20182;&#20204;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;POPQUORN&#65288;POtato-Prolific &#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#12289;&#20882;&#29359;&#24615;&#12289;&#25991;&#26412;&#25913;&#20889;&#21644;&#31036;&#35980;&#35780;&#20998;&#65292;&#21253;&#21547;&#20154;&#21475;&#32479;&#35745;&#23398;&#32454;&#24494;&#24046;&#24322;&#65289;&#12290;POPQUORN&#21253;&#21547;1,484&#20010;&#26631;&#27880;&#32773;&#30340;45,000&#20010;&#26631;&#27880;&#65292;&#37319;&#29992;&#20102;&#32654;&#22269;&#20154;&#21475;&#20013;&#24615;&#21035;&#12289;&#24180;&#40836;&#21644;&#31181;&#26063;&#30340;&#20195;&#34920;&#26679;&#26412;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#27880;&#32773;&#32972;&#26223;&#22312;&#20182;&#20204;&#30340;&#21028;&#26029;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#34920;&#26126;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20197;&#21069;&#26410;&#32771;&#34385;&#30340;&#32972;&#26223;&#22240;&#32032;&#65288;&#20363;&#22914;&#25945;&#32946;&#65289;&#26159;&#26377;&#24847;&#20041;&#19988;&#24212;&#35813;&#34987;&#32771;&#34385;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29702;&#35299;&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#65292;&#24182;&#20174;&#20855;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#34913;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#20013;&#25910;&#38598;&#26631;&#31614;&#65292;&#23545;&#20943;&#23569;&#25968;&#25454;&#38598;&#20559;&#24046;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotators are not fungible. Their demographics, life experiences, and backgrounds all contribute to how they label data. However, NLP has only recently considered how annotator identity might influence their decisions. Here, we present POPQUORN (the POtato-Prolific dataset for QUestion-Answering, Offensiveness, text Rewriting, and politeness rating with demographic Nuance). POPQUORN contains 45,000 annotations from 1,484 annotators, drawn from a representative sample regarding sex, age, and race as the US population. Through a series of analyses, we show that annotators' background plays a significant role in their judgments. Further, our work shows that backgrounds not previously considered in NLP (e.g., education), are meaningful and should be considered. Our study suggests that understanding the background of annotators and collecting labels from a demographically balanced pool of crowd workers is important to reduce the bias of datasets. The dataset, annotator background, and anno
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#20248;&#32531;&#23384;&#19982;&#27169;&#22411;&#22797;&#29992;&#20004;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#22823;&#22411;&#27169;&#22411;&#25512;&#29702;&#20013;&#36164;&#28304;&#28040;&#32791;&#21644;&#24310;&#36831;&#25361;&#25112;&#65292;&#32463;&#36807;&#23454;&#35777;&#27169;&#25311;&#21457;&#29616;&#36825;&#31181;&#32452;&#21512;&#22823;&#22823;&#25552;&#39640;&#20102;&#20256;&#32479;&#27169;&#22411;&#25512;&#29702;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02003</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#26368;&#20248;&#32531;&#23384;&#19982;&#27169;&#22411;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Optimal Caching and Model Multiplexing for Large Model Inference. (arXiv:2306.02003v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#20248;&#32531;&#23384;&#19982;&#27169;&#22411;&#22797;&#29992;&#20004;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#22823;&#22411;&#27169;&#22411;&#25512;&#29702;&#20013;&#36164;&#28304;&#28040;&#32791;&#21644;&#24310;&#36831;&#25361;&#25112;&#65292;&#32463;&#36807;&#23454;&#35777;&#27169;&#25311;&#21457;&#29616;&#36825;&#31181;&#32452;&#21512;&#22823;&#22823;&#25552;&#39640;&#20102;&#20256;&#32479;&#27169;&#22411;&#25512;&#29702;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20854;&#20182;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#23610;&#23544;&#21152;&#21095;&#20102;&#29616;&#26377;&#30340;&#36164;&#28304;&#28040;&#32791;&#21644;&#24310;&#36831;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65306;&#21033;&#29992;&#32531;&#23384;&#23384;&#20648;&#20808;&#21069;&#30340;&#26597;&#35810;&#21644;&#23398;&#20064;&#27169;&#22411;&#22797;&#29992;&#22120;&#26469;&#36873;&#25321;&#29992;&#20110;&#26597;&#35810;&#22788;&#29702;&#30340;&#27169;&#22411;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26368;&#20248;&#31639;&#27861;&#26469;&#32852;&#21512;&#20248;&#21270;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#31163;&#32447;&#21644;&#22312;&#32447;&#21046;&#34920;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#36890;&#36807;&#23558;&#32531;&#23384;&#31639;&#27861;&#21644;&#27169;&#22411;&#22797;&#29992;&#22120;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#37117;&#23454;&#29616;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;&#23454;&#35777;&#27169;&#25311;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32531;&#23384;&#21644;&#27169;&#22411;&#22797;&#29992;&#31639;&#27861;&#30340;&#32452;&#21512;&#22823;&#22823;&#25552;&#39640;&#20102;&#20256;&#32479;&#27169;&#22411;&#25512;&#29702;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) and other large foundation models have achieved noteworthy success, but their size exacerbates existing resource consumption and latency challenges. In particular, the large-scale deployment of these models is hindered by the significant resource requirements during inference. In this paper, we study two approaches for mitigating these challenges: employing a cache to store previous queries and learning a model multiplexer to choose from an ensemble of models for query processing.  Theoretically, we provide an optimal algorithm for jointly optimizing both approaches to reduce the inference cost in both offline and online tabular settings. By combining a caching algorithm, namely Greedy Dual Size with Frequency (GDSF) or Least Expected Cost (LEC), with a model multiplexer, we achieve optimal rates in both offline and online settings. Empirically, simulations show that the combination of our caching and model multiplexing algorithms greatly improves over the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#20219;&#21153;&#20013;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22238;&#25253;&#20989;&#25968;&#30340;&#38543;&#26426;&#21338;&#24328;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#26426;&#21046;&#30340;&#31639;&#27861;&#65292;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#30340;Q&#20989;&#25968;&#23558;&#20250;&#25910;&#25947;&#20110;&#19968;&#20010;&#32435;&#20160;&#22343;&#34913;&#30340;Q&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#38454;&#27573;&#21338;&#24328;&#20855;&#26377;&#20840;&#23616;&#26368;&#20248;&#28857;&#25110;&#32773;&#38797;&#28857;&#65292;&#24182;&#19988;&#26234;&#33021;&#20307;&#22522;&#20110;&#36825;&#19968;&#28857;&#36827;&#34892;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2305.17372</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#20351;&#29992;&#22870;&#21169;&#26426;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning With Reward Machines in Stochastic Games. (arXiv:2305.17372v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17372
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#20219;&#21153;&#20013;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22238;&#25253;&#20989;&#25968;&#30340;&#38543;&#26426;&#21338;&#24328;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#26426;&#21046;&#30340;&#31639;&#27861;&#65292;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#30340;Q&#20989;&#25968;&#23558;&#20250;&#25910;&#25947;&#20110;&#19968;&#20010;&#32435;&#20160;&#22343;&#34913;&#30340;Q&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#38454;&#27573;&#21338;&#24328;&#20855;&#26377;&#20840;&#23616;&#26368;&#20248;&#28857;&#25110;&#32773;&#38797;&#28857;&#65292;&#24182;&#19988;&#26234;&#33021;&#20307;&#22522;&#20110;&#36825;&#19968;&#28857;&#36827;&#34892;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22797;&#26434;&#20219;&#21153;&#20013;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22238;&#25253;&#20989;&#25968;&#30340;&#38543;&#26426;&#21338;&#24328;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;&#22870;&#21169;&#26426;&#21046;&#26469;&#25972;&#21512;&#39640;&#23618;&#27425;&#30340;&#22797;&#26434;&#20219;&#21153;&#30693;&#35782;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;QRM-SG&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#12290;&#22312;QRM-SG&#20013;&#65292;&#25105;&#20204;&#22312;&#22686;&#24191;&#29366;&#24577;&#31354;&#38388;&#20013;&#23450;&#20041;&#20102;&#32435;&#20160;&#22343;&#34913;&#19979;&#30340;Q&#20989;&#25968;&#12290;&#22686;&#24191;&#29366;&#24577;&#31354;&#38388;&#25972;&#21512;&#20102;&#38543;&#26426;&#21338;&#24328;&#30340;&#29366;&#24577;&#21644;&#22870;&#21169;&#26426;&#21046;&#30340;&#29366;&#24577;&#12290;&#27599;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#20102;&#31995;&#32479;&#20013;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;Q&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;QRM-SG&#20013;&#23398;&#20064;&#30340;Q&#20989;&#25968;&#23558;&#20250;&#25910;&#25947;&#20110;&#19968;&#20010;&#32435;&#20160;&#22343;&#34913;&#30340;Q&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#38454;&#27573;&#21338;&#24328;&#20855;&#26377;&#20840;&#23616;&#26368;&#20248;&#28857;&#25110;&#32773;&#38797;&#28857;&#65292;&#24182;&#19988;&#26234;&#33021;&#20307;&#22522;&#20110;&#36825;&#19968;&#28857;&#36827;&#34892;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#26356;&#26032;&#12290;&#25105;&#20204;&#20351;&#29992;Lemke-Howson&#26041;&#27861;&#26469;&#24471;&#20986;&#32473;&#23450;&#24403;&#21069;Q&#20989;&#25968;&#26102;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate multi-agent reinforcement learning for stochastic games with complex tasks, where the reward functions are non-Markovian. We utilize reward machines to incorporate high-level knowledge of complex tasks. We develop an algorithm called Q-learning with reward machines for stochastic games (QRM-SG), to learn the best-response strategy at Nash equilibrium for each agent. In QRM-SG, we define the Q-function at a Nash equilibrium in augmented state space. The augmented state space integrates the state of the stochastic game and the state of reward machines. Each agent learns the Q-functions of all agents in the system. We prove that Q-functions learned in QRM-SG converge to the Q-functions at a Nash equilibrium if the stage game at each time step during learning has a global optimum point or a saddle point, and the agents update Q-functions based on the best-response strategy at this point. We use the Lemke-Howson method to derive the best-response strategy given current Q-func
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#26102;&#26816;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#21160;&#24577;&#21644;&#38745;&#24577;&#27969;&#21452;&#27969;&#24863;&#30693;&#27169;&#22359;&#65292;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#24182;&#20943;&#36731;&#26102;&#24310;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#40060;&#30524;&#25668;&#20687;&#22836;&#30446;&#26631;&#26816;&#27979;&#20013;&#26102;&#24310;&#24046;&#24322;&#24102;&#26469;&#30340;&#23433;&#20840;&#38544;&#24739;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14713</link><description>&lt;p&gt;
&#33258;&#21160;&#27850;&#36710;&#30340;&#40060;&#30524;&#25668;&#20687;&#22836;&#27969;&#24335;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Streaming Object Detection on Fisheye Cameras for Automatic Parking. (arXiv:2305.14713v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14713
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#26102;&#26816;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#21160;&#24577;&#21644;&#38745;&#24577;&#27969;&#21452;&#27969;&#24863;&#30693;&#27169;&#22359;&#65292;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#24182;&#20943;&#36731;&#26102;&#24310;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#40060;&#30524;&#25668;&#20687;&#22836;&#30446;&#26631;&#26816;&#27979;&#20013;&#26102;&#24310;&#24046;&#24322;&#24102;&#26469;&#30340;&#23433;&#20840;&#38544;&#24739;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40060;&#30524;&#25668;&#20687;&#22836;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#21160;&#27850;&#36710;&#65292;&#32780;&#20854;&#35270;&#39057;&#27969;&#30446;&#26631;&#26816;&#27979;&#26159;&#30830;&#20445;&#36710;&#36742;&#23433;&#20840;&#36816;&#34892;&#30340;&#22522;&#26412;&#24863;&#30693;&#21151;&#33021;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#19982;&#24403;&#21069;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#30340;&#26102;&#24310;&#24046;&#24322;&#65292;&#28982;&#32780;&#29615;&#22659;&#22312;&#26102;&#24310;&#26102;&#38388;&#20869;&#19981;&#21487;&#36991;&#20813;&#30340;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#21487;&#33021;&#36896;&#25104;&#28508;&#22312;&#30340;&#23433;&#20840;&#38544;&#24739;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#37197;&#22791;&#21160;&#24577;&#21644;&#38745;&#24577;&#27969;&#21452;&#27969;&#24863;&#30693;&#27169;&#22359;&#65292;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#24182;&#20943;&#36731;&#26102;&#24310;&#38382;&#39064;&#30340;&#23454;&#26102;&#26816;&#27979;&#26694;&#26550;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#35780;&#20272;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#12290;&#26631;&#20934;&#36793;&#30028;&#26694;&#23545;&#20110;&#40060;&#30524;&#25668;&#20687;&#22836;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#40060;&#30524;&#25668;&#20687;&#22836;&#30340;&#24378;&#28872;&#24452;&#21521;&#30072;&#21464;&#65292;&#32780;&#20572;&#36710;&#24863;&#30693;&#30340;&#20027;&#35201;&#26816;&#27979;&#23545;&#35937;&#26159;&#36710;&#36742;&#21644;&#34892;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fisheye cameras are widely employed in automatic parking, and the video stream object detection (VSOD) of the fisheye camera is a fundamental perception function to ensure the safe operation of vehicles. In past research work, the difference between the output of the deep learning model and the actual situation at the current moment due to the existence of delay of the perception system is generally ignored. But the environment will inevitably change within the delay time which may cause a potential safety hazard. In this paper, we propose a real-time detection framework equipped with a dual-flow perception module (dynamic and static flows) that can predict the future and alleviate the time-lag problem. Meanwhile, we use a new scheme to evaluate latency and accuracy. The standard bounding box is unsuitable for the object in fisheye camera images due to the strong radial distortion of the fisheye camera and the primary detection objects of parking perception are vehicles and pedestrians
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#31532;&#19968;&#20010;&#23454;&#39564;&#35774;&#32622;&#65292;&#25506;&#35752;&#20102;LiDAR&#35821;&#20041;&#20998;&#21106;&#22312;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#36328;&#39046;&#22495;&#35774;&#32622;&#20013;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31232;&#30095;-&#23494;&#38598;&#21367;&#31215;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#30340;&#26174;&#33879;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.11705</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;LiDAR&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#26041;&#27861;&#30740;&#31350;&#21644;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation. (arXiv:2304.11705v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#31532;&#19968;&#20010;&#23454;&#39564;&#35774;&#32622;&#65292;&#25506;&#35752;&#20102;LiDAR&#35821;&#20041;&#20998;&#21106;&#22312;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#36328;&#39046;&#22495;&#35774;&#32622;&#20013;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31232;&#30095;-&#23494;&#38598;&#21367;&#31215;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#30340;&#26174;&#33879;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#36807;&#31243;&#20013;&#65292;&#33021;&#22815;&#23433;&#20840;&#22320;&#22312;&#22810;&#26679;&#21270;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#22312;&#21516;&#19968;&#39046;&#22495;&#30340;LiDAR&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#27867;&#21270;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#39046;&#22495;&#27867;&#21270;&#65288;DG-LSS&#65289;&#30340;&#23454;&#39564;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36328;&#39046;&#22495;&#30340;&#35774;&#32622;&#20013;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65306;&#20363;&#22914;&#65292;&#22312;&#28304;&#25968;&#25454;&#38598;&#65288;SemanticKITTI&#65289;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30446;&#26631;&#25968;&#25454;&#19978;&#33719;&#24471;&#20102;26.53&#30340;mIoU&#65292;&#32780;&#22312;&#30446;&#26631;&#22495;&#65288;nuScenes&#65289;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21017;&#33719;&#24471;&#20102;48.49&#30340;mIoU&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;DG-LSS&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#33719;&#24471;&#20102;34.88&#30340;mIoU&#65292;&#36229;&#36234;&#20102;&#25152;&#26377;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#31232;&#30095;&#21367;&#31215;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;3D&#20998;&#21106;&#32593;&#32476;&#19982;&#39069;&#22806;&#30340;&#23494;&#38598;2D&#21367;&#31215;&#30456;&#32467;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;
The ability to deploy robots that can operate safely in diverse environments is crucial for developing embodied intelligent agents. As a community, we have made tremendous progress in within-domain LiDAR semantic segmentation. However, do these methods generalize across domains? To answer this question, we design the first experimental setup for studying domain generalization (DG) for LiDAR semantic segmentation (DG-LSS). Our results confirm a significant gap between methods, evaluated in a cross-domain setting: for example, a model trained on the source dataset (SemanticKITTI) obtains $26.53$ mIoU on the target data, compared to $48.49$ mIoU obtained by the model trained on the target domain (nuScenes). To tackle this gap, we propose the first method specifically designed for DG-LSS, which obtains $34.88$ mIoU on the target domain, outperforming all baselines. Our method augments a sparse-convolutional encoder-decoder 3D segmentation network with an additional, dense 2D convolutional 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07122</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#37327;&#21270;&#21271;&#26497;&#25918;&#22823;&#30340;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference. (arXiv:2303.07122v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21271;&#26497;&#21464;&#26262;&#65292;&#20063;&#31216;&#21271;&#26497;&#25918;&#22823;&#65292;&#30001;&#22810;&#31181;&#22823;&#27668;&#21644;&#28023;&#27915;&#22240;&#32032;&#23548;&#33268;&#65292;&#20294;&#20854;&#22522;&#30784;&#28909;&#21147;&#22240;&#32032;&#30340;&#35814;&#32454;&#24773;&#20917;&#20173;&#19981;&#28165;&#26970;&#12290;&#20351;&#29992;&#22266;&#23450;&#27835;&#30103;&#25928;&#24212;&#31574;&#30053;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#20250;&#23548;&#33268;&#19981;&#29616;&#23454;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#26102;&#38388;&#21464;&#21270;&#30340;&#28151;&#28102;&#30340;&#24433;&#21709;&#32780;&#24341;&#36215;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TCINet - &#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#65292;&#20197;&#36830;&#32493;&#27835;&#30103;&#26041;&#24335;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#35266;&#27979;&#25968;&#25454;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#22914;&#20309;&#22823;&#22823;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The warming of the Arctic, also known as Arctic amplification, is led by several atmospheric and oceanic drivers, however, the details of its underlying thermodynamic causes are still unknown. Inferring the causal effects of atmospheric processes on sea ice melt using fixed treatment effect strategies leads to unrealistic counterfactual estimations. Such models are also prone to bias due to time-varying confoundedness. In order to tackle these challenges, we propose TCINet - time-series causal inference model to infer causation under continuous treatment using recurrent neural networks. Through experiments on synthetic and observational data, we show how our research can substantially improve the ability to quantify the leading causes of Arctic sea ice melt.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#26377;&#19968;&#33268;&#30340;&#20248;&#21183;&#65292;&#20294;&#32477;&#23545;&#34920;&#29616;&#20173;&#26377;&#25552;&#39640;&#31354;&#38388;&#65292;&#40065;&#26834;&#24615;&#20173;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.12095</link><description>&lt;p&gt;
&#35770;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65306;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (arXiv:2302.12095v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#26377;&#19968;&#33268;&#30340;&#20248;&#21183;&#65292;&#20294;&#32477;&#23545;&#34920;&#29616;&#20173;&#26377;&#25552;&#39640;&#31354;&#38388;&#65292;&#40065;&#26834;&#24615;&#20173;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#26368;&#36817;&#21457;&#24067;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26381;&#21153;&#65292;&#24182;&#22312;&#36807;&#21435;&#20960;&#20010;&#26376;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#23545;ChatGPT&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20294;&#20854;&#40065;&#26834;&#24615;&#65292;&#21363;&#23545;&#20110;&#26410;&#39044;&#26399;&#36755;&#20837;&#30340;&#34920;&#29616;&#65292;&#20173;&#19981;&#28165;&#26970;&#12290;&#40065;&#26834;&#24615;&#22312;&#36127;&#36131;&#20219;&#30340;AI&#20013;&#29305;&#21035;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#35282;&#24230;&#23545;ChatGPT&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;AdvGLUE&#21644;ANLI&#22522;&#20934;&#26469;&#35780;&#20272;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#37319;&#29992;Flipkart&#35780;&#35770;&#21644;DDXPlus&#21307;&#23398;&#35786;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;OOD&#35780;&#20272;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20960;&#20010;&#27969;&#34892;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#22522;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#23545;&#25239;&#24615;&#21644;OOD&#20998;&#31867;&#21644;&#32763;&#35793;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#20248;&#21183;&#12290;&#20294;&#26159;&#65292;&#32477;&#23545;&#30340;&#34920;&#29616;&#36828;&#38750;&#23436;&#32654;&#65292;&#36825;&#34920;&#26126;&#23545;&#25239;&#24615;&#21644;OOD&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Unreliable Partial Label Learning with Recursive Separation (UPLLRS)&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#26469;&#35299;&#20915;&#19981;&#21487;&#38752;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#37319;&#29992;&#33258;&#36866;&#24212;&#36882;&#24402;&#20998;&#31163;&#31574;&#30053;&#23558;&#35757;&#32451;&#38598;&#20998;&#20026;&#21487;&#38752;&#23376;&#38598;&#21644;&#19981;&#21487;&#38752;&#23376;&#38598;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#21033;&#29992;&#36825;&#20123;&#23376;&#38598;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.09891</link><description>&lt;p&gt;
&#19981;&#21487;&#38752;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#19982;&#36882;&#24402;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Unreliable Partial Label Learning with Recursive Separation. (arXiv:2302.09891v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Unreliable Partial Label Learning with Recursive Separation (UPLLRS)&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#26469;&#35299;&#20915;&#19981;&#21487;&#38752;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#37319;&#29992;&#33258;&#36866;&#24212;&#36882;&#24402;&#20998;&#31163;&#31574;&#30053;&#23558;&#35757;&#32451;&#38598;&#20998;&#20026;&#21487;&#38752;&#23376;&#38598;&#21644;&#19981;&#21487;&#38752;&#23376;&#38598;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#21033;&#29992;&#36825;&#20123;&#23376;&#38598;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#65288;PLL&#65289;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#27599;&#20010;&#23454;&#20363;&#37117;&#19982;&#19968;&#20010;&#20505;&#36873;&#26631;&#31614;&#38598;&#30456;&#20851;&#65292;&#20854;&#20013;&#21482;&#26377;&#19968;&#20010;&#26159;&#30495;&#23454;&#30340;&#12290;&#28982;&#32780;&#65292;&#20551;&#35774;&#30495;&#23454;&#26631;&#31614;&#24635;&#26159;&#22312;&#20505;&#36873;&#26631;&#31614;&#38598;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#20505;&#36873;&#26631;&#31614;&#38598;&#30340;&#21487;&#38752;&#24615;&#19981;&#33021;&#30001;&#27880;&#37322;&#32773;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Unreliable Partial Label Learning&#65288;UPLL&#65289;&#30340;&#24191;&#20041;PLL&#26041;&#27861;&#65292;&#20854;&#20013;&#30495;&#23454;&#26631;&#31614;&#21487;&#33021;&#19981;&#22312;&#20505;&#36873;&#26631;&#31614;&#38598;&#20013;&#12290;&#30001;&#20110;&#19981;&#21487;&#38752;&#26631;&#27880;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;PLL&#26041;&#27861;&#22312;&#24212;&#29992;&#21040;UPLL&#26102;&#20250;&#36935;&#21040;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Unreliable Partial Label Learning with Recursive Separation&#65288;UPLLRS&#65289;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#36882;&#24402;&#20998;&#31163;&#31574;&#30053;&#65292;&#23558;&#35757;&#32451;&#38598;&#20998;&#20026;&#21487;&#38752;&#23376;&#38598;&#21644;&#19981;&#21487;&#38752;&#23376;&#38598;&#12290;&#22312;&#31532;&#20108;&#20010;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#20998;&#31163;&#30340;UPLL&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#23376;&#38598;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#19981;&#21487;&#38752;&#26631;&#27880;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial label learning (PLL) is a typical weakly supervised learning problem in which each instance is associated with a candidate label set, and among which only one is true. However, the assumption that the ground-truth label is always among the candidate label set would be unrealistic, as the reliability of the candidate label sets in real-world applications cannot be guaranteed by annotators. Therefore, a generalized PLL named Unreliable Partial Label Learning (UPLL) is proposed, in which the true label may not be in the candidate label set. Due to the challenges posed by unreliable labeling, previous PLL methods will experience a marked decline in performance when applied to UPLL. To address the issue, we propose a two-stage framework named Unreliable Partial Label Learning with Recursive Separation (UPLLRS). In the first stage, the self-adaptive recursive separation strategy is proposed to separate the training set into a reliable subset and an unreliable subset. In the second st
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#20559;&#33258;&#27880;&#24847;&#21147;&#30340;&#20844;&#24179;&#24863;&#30693;&#35270;&#35273;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#23450;&#20301;&#21644;&#23631;&#34109;&#36825;&#20123;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2301.13803</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20559;&#33258;&#27880;&#24847;&#21147;&#30340;&#20844;&#24179;&#24863;&#30693;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fairness-aware Vision Transformer via Debiased Self-Attention. (arXiv:2301.13803v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13803
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#20559;&#33258;&#27880;&#24847;&#21147;&#30340;&#20844;&#24179;&#24863;&#30693;&#35270;&#35273;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#23450;&#20301;&#21644;&#23631;&#34109;&#36825;&#20123;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#25552;&#21462;&#20449;&#24687;&#29305;&#24449;&#21644;&#36890;&#36807;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#24314;&#27169;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#22312;&#35299;&#20915;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#38382;&#39064;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;ViT&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;ViT&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21253;&#25324;&#20854;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#21478;&#19968;&#20010;&#38656;&#27714;&#65292;&#20844;&#24179;&#24615;&#65292;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#20844;&#24179;&#24863;&#30693;&#31639;&#27861;&#65288;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;CNN&#65289;&#22312;ViT&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#23601;&#38656;&#35201;&#25105;&#20204;&#36890;&#36807;&#21435;&#20559;&#33258;&#27880;&#24847;&#65288;DSA&#65289;&#24320;&#21457;&#25105;&#20204;&#30340;&#26032;&#26694;&#26550;&#12290;DSA&#26159;&#19968;&#31181;&#36890;&#36807;&#30450;&#30446;&#26041;&#27861;&#26469;&#24378;&#21046;ViT&#28040;&#38500;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#20197;&#20943;&#36731;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#34987;&#29992;&#26469;&#23450;&#20301;&#21644;&#23631;&#34109;&#36755;&#20837;&#22270;&#20687;&#22359;&#20013;&#30340;&#34394;&#20551;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has recently gained significant interest in solving computer vision (CV) problems due to its capability of extracting informative features and modeling long-range dependencies through the self-attention mechanism. To fully realize the advantages of ViT in real-world applications, recent works have explored the trustworthiness of ViT, including its robustness and explainability. However, another desiderata, fairness has not yet been adequately addressed in the literature. We establish that the existing fairness-aware algorithms (primarily designed for CNNs) do not perform well on ViT. This necessitates the need for developing our novel framework via Debiased Self-Attention (DSA). DSA is a fairness-through-blindness approach that enforces ViT to eliminate spurious features correlated with the sensitive attributes for bias mitigation. Notably, adversarial examples are leveraged to locate and mask the spurious features in the input image patches. In addition, DSA u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2301.03364</link><description>&lt;p&gt;
&#36208;&#21521;AI-enabled&#36830;&#25509;&#20135;&#19994;: AGV&#36890;&#20449;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Towards an AI-enabled Connected Industry: AGV Communication and Sensor Measurement Datasets. (arXiv:2301.03364v3 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#24037;&#19994;&#27979;&#35797;&#24179;&#21488;&#19978;&#30340;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;: &#24037;&#19994;&#36710;&#36742;&#38388;&#36890;&#20449;(iV2V)&#21644;&#24037;&#19994;&#36710;&#36742;&#21040;&#22522;&#30784;&#35774;&#26045;&#21152;&#20256;&#24863;&#22120;(iV2I+)&#12290;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20004;&#20010;&#25429;&#33719;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;iV2V&#28085;&#30422;&#20102;&#33258;&#21160;&#24341;&#23548;&#36710;(AGVs)&#20043;&#38388;&#30340;&#20391;&#21521;&#38142;&#36335;&#36890;&#20449;&#22330;&#26223;&#65292;&#32780;iV2I+&#21017;&#26159;&#22312;&#24037;&#19994;&#35774;&#32622;&#20013;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#33258;&#20027;&#28165;&#27905;&#26426;&#22120;&#20154;&#36830;&#25509;&#21040;&#31169;&#26377;&#34562;&#31389;&#32593;&#32476;&#12290;&#19981;&#21516;&#36890;&#20449;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#36830;&#21516;&#20849;&#21516;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;(ML)&#21487;&#20197;&#21033;&#29992;&#30340;&#27934;&#23519;&#21147;&#65292;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#24050;&#26631;&#35760;&#21644;&#39044;&#36807;&#28388;&#65292;&#20197;&#20415;&#24555;&#36895;&#21551;&#21160;&#21644;&#24212;&#29992;&#12290;&#23545;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#36824;&#35814;&#32454;&#20171;&#32461;&#20102;&#30456;&#24212;&#30340;&#27979;&#35797;&#24179;&#21488;&#21644;&#27979;&#37327;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two wireless measurement campaigns in industrial testbeds: industrial Vehicle-to-vehicle (iV2V) and industrial Vehicle-to-infrastructure plus Sensor (iV2I+). Detailed information about the two captured datasets is provided as well. iV2V covers sidelink communication scenarios between Automated Guided Vehicles (AGVs), while iV2I+ is conducted at an industrial setting where an autonomous cleaning robot is connected to a private cellular network. The combination of different communication technologies, together with a common measurement methodology, provides insights that can be exploited by Machine Learning (ML) for tasks such as fingerprinting, line-of-sight detection, prediction of quality of service or link selection. Moreover, the datasets are labelled and pre-filtered for fast on-boarding and applicability. The corresponding testbeds and measurements are also presented in detail for both datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#21457;&#29616;&#36890;&#29992;&#30340;&#21069;&#26399;&#35757;&#32451;&#21487;&#20197;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2112.09153</link><description>&lt;p&gt;
&#21069;&#26399;&#35757;&#32451;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Investigation of the Role of Pre-training in Lifelong Learning. (arXiv:2112.09153v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09153
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#21457;&#29616;&#36890;&#29992;&#30340;&#21069;&#26399;&#35757;&#32451;&#21487;&#20197;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#32456;&#36523;&#23398;&#20064;&#33539;&#24335;&#19981;&#20165;&#22240;&#20854;&#31867;&#20284;&#29983;&#29289;&#23398;&#20064;&#30340;&#29305;&#24615;&#32780;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#32780;&#19988;&#22240;&#20854;&#36890;&#36807;&#36991;&#20813;&#36807;&#22810;&#30340;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#32780;&#20943;&#23569;&#33021;&#28304;&#28010;&#36153;&#30340;&#28508;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#19968;&#33539;&#24335;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26085;&#30410;&#27969;&#34892;&#21644;&#25104;&#21151;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#65292;&#21069;&#26399;&#35757;&#32451;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#26041;&#38754;&#25198;&#28436;&#20309;&#31181;&#35282;&#33394;&#65311;&#25105;&#20204;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#30740;&#31350;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#25991;&#26412;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20351;&#29992;&#19968;&#20010;&#26032;&#39062;&#30340;&#21253;&#21547;15&#20010;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#30456;&#27604;&#65292;&#36890;&#29992;&#30340;&#21069;&#26399;&#35757;&#32451;&#22312;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#26102;&#38544;&#21547;&#22320;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning but also its potential to reduce energy waste by obviating excessive model re-training. A key challenge to this paradigm is the phenomenon of catastrophic forgetting. With the increasing popularity and success of pre-trained models in machine learning, we pose the question: What role does pre-training play in lifelong learning, specifically with respect to catastrophic forgetting? We investigate existing methods in the context of large, pre-trained models and evaluate their performance on a variety of text and image classification tasks, including a large-scale study using a novel data set of 15 diverse NLP tasks. Across all settings, we observe that generic pre-training implicitly alleviates the effects of catastrophic forgetting when learning multiple tasks sequentially compared to randomly initialized mo
&lt;/p&gt;</description></item></channel></rss>