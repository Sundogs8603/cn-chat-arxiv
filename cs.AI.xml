<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;Physion++&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#38656;&#35201;&#20934;&#30830;&#20272;&#35745;&#22330;&#26223;&#20013;&#29289;&#20307;&#28508;&#22312;&#29289;&#29702;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#30340;&#35270;&#35273;&#29289;&#29702;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.15668</link><description>&lt;p&gt;
Physion++&#65306;&#23545;&#38656;&#35201;&#22312;&#32447;&#25512;&#29702;&#19981;&#21516;&#29289;&#29702;&#23646;&#24615;&#30340;&#29289;&#29702;&#22330;&#26223;&#29702;&#35299;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Physion++: Evaluating Physical Scene Understanding that Requires Online Inference of Different Physical Properties. (arXiv:2306.15668v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15668
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;Physion++&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#38656;&#35201;&#20934;&#30830;&#20272;&#35745;&#22330;&#26223;&#20013;&#29289;&#20307;&#28508;&#22312;&#29289;&#29702;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#30340;&#35270;&#35273;&#29289;&#29702;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#30340;&#29289;&#29702;&#22330;&#26223;&#29702;&#35299;&#38656;&#35201;&#30340;&#19981;&#20165;&#20165;&#26159;&#23450;&#20301;&#21644;&#35782;&#21035;&#29289;&#20307;&#65292;&#36824;&#38656;&#35201;&#20102;&#35299;&#29289;&#20307;&#21487;&#20197;&#20855;&#26377;&#19981;&#21516;&#30340;&#28508;&#22312;&#23646;&#24615;&#65288;&#20363;&#22914;&#36136;&#37327;&#25110;&#24377;&#24615;&#65289;&#65292;&#24182;&#19988;&#36825;&#20123;&#23646;&#24615;&#20250;&#24433;&#21709;&#29289;&#29702;&#20107;&#20214;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#29289;&#29702;&#21644;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#35780;&#20272;&#23427;&#20204;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#36890;&#24120;&#19981;&#35201;&#27714;&#29702;&#35299;&#29289;&#20307;&#20855;&#26377;&#20010;&#20307;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;&#25110;&#32773;&#26368;&#22810;&#21482;&#27979;&#35797;&#21487;&#20197;&#30452;&#25509;&#35266;&#23519;&#21040;&#30340;&#23646;&#24615;&#65288;&#20363;&#22914;&#22823;&#23567;&#25110;&#39068;&#33394;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;Physion++&#65292;&#22312;&#36825;&#20010;&#20154;&#24037;&#31995;&#32479;&#19979;&#20005;&#26684;&#35780;&#20272;&#20102;&#35270;&#35273;&#29289;&#29702;&#39044;&#27979;&#65292;&#20854;&#20013;&#39044;&#27979;&#20381;&#36182;&#20110;&#22330;&#26223;&#20013;&#29289;&#20307;&#28508;&#22312;&#29289;&#29702;&#23646;&#24615;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#20934;&#30830;&#39044;&#27979;&#20381;&#36182;&#20110;&#36136;&#37327;&#12289;&#25705;&#25830;&#21147;&#12289;&#24377;&#24615;&#21644;&#21487;&#21464;&#24615;&#31561;&#23646;&#24615;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
General physical scene understanding requires more than simply localizing and recognizing objects -- it requires knowledge that objects can have different latent properties (e.g., mass or elasticity), and that those properties affect the outcome of physical events. While there has been great progress in physical and video prediction models in recent years, benchmarks to test their performance typically do not require an understanding that objects have individual physical properties, or at best test only those properties that are directly observable (e.g., size or color). This work proposes a novel dataset and benchmark, termed Physion++, that rigorously evaluates visual physical prediction in artificial systems under circumstances where those predictions rely on accurate estimates of the latent physical properties of objects in the scene. Specifically, we test scenarios where accurate prediction relies on estimates of properties such as mass, friction, elasticity, and deformability, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24037;&#20855;&#30340;&#21151;&#33021;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26082;&#19981;&#20934;&#30830;&#20063;&#19981;&#21487;&#38752;&#12290;</title><link>http://arxiv.org/abs/2306.15666</link><description>&lt;p&gt;
AI-&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Testing of Detection Tools for AI-Generated Text. (arXiv:2306.15666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24037;&#20855;&#30340;&#21151;&#33021;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26082;&#19981;&#20934;&#30830;&#20063;&#19981;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24378;&#35843;&#20102;&#22312;&#23398;&#26415;&#29615;&#22659;&#20013;&#19981;&#20844;&#24179;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21152;&#24378;&#20102;&#23547;&#25214;&#26816;&#27979;&#27492;&#31867;&#20869;&#23481;&#35299;&#20915;&#26041;&#26696;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24037;&#20855;&#30340;&#19968;&#33324;&#21151;&#33021;&#65292;&#24182;&#26681;&#25454;&#20934;&#30830;&#24615;&#21644;&#38169;&#35823;&#31867;&#22411;&#20998;&#26512;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#20197;&#19979;&#30740;&#31350;&#38382;&#39064;&#65306;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26159;&#21542;&#33021;&#21487;&#38752;&#22320;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#21644;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#20197;&#21450;&#26426;&#22120;&#32763;&#35793;&#21644;&#20869;&#23481;&#28151;&#28102;&#25216;&#26415;&#26159;&#21542;&#24433;&#21709;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#12290;&#30740;&#31350;&#28085;&#30422;&#20102;12&#31181;&#20844;&#24320;&#21487;&#29992;&#30340;&#24037;&#20855;&#21644;&#20004;&#20010;&#24191;&#27867;&#24212;&#29992;&#20110;&#23398;&#26415;&#29615;&#22659;&#20013;&#30340;&#21830;&#19994;&#31995;&#32479;&#65288;Turnitin&#21644;PlagiarismCheck&#65289;&#12290;&#30740;&#31350;&#20154;&#21592;&#24471;&#20986;&#32467;&#35770;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26082;&#19981;&#20934;&#30830;&#20063;&#19981;&#21487;&#38752;&#65292;&#24182;&#19988;&#23384;&#22312;&#20027;&#35201;&#30340;&#21487;&#36776;&#35782;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative pre-trained transformer large language models have emphasised the potential risks of unfair use of artificial intelligence (AI) generated content in an academic environment and intensified efforts in searching for solutions to detect such content. The paper examines the general functionality of detection tools for artificial intelligence generated text and evaluates them based on accuracy and error type analysis. Specifically, the study seeks to answer research questions about whether existing detection tools can reliably differentiate between human-written text and ChatGPT-generated text, and whether machine translation and content obfuscation techniques affect the detection of AIgenerated text. The research covers 12 publicly available tools and two commercial systems (Turnitin and PlagiarismCheck) that are widely used in the academic setting. The researchers conclude that the available detection tools are neither accurate nor reliable and have a main bi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#25910;&#38598;&#33258;2022&#24180;&#39640;&#25490;&#21517;&#27604;&#36187;&#30340;&#32701;&#27611;&#29699;&#21333;&#25171;&#25968;&#25454;&#38598;ShuttleSet22&#65292;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25293;&#29699;&#39044;&#27979;&#26041;&#27861;ShuttleNet&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2306.15664</link><description>&lt;p&gt;
ShuttleSet22: &#29992;&#32701;&#27611;&#29699;&#21333;&#25171;&#25968;&#25454;&#38598;&#23545;&#20013;&#39118;&#39044;&#27979;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ShuttleSet22: Benchmarking Stroke Forecasting with Stroke-Level Badminton Dataset. (arXiv:2306.15664v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#25910;&#38598;&#33258;2022&#24180;&#39640;&#25490;&#21517;&#27604;&#36187;&#30340;&#32701;&#27611;&#29699;&#21333;&#25171;&#25968;&#25454;&#38598;ShuttleSet22&#65292;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25293;&#29699;&#39044;&#27979;&#26041;&#27861;ShuttleNet&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#21644;&#25968;&#25454;&#37319;&#38598;&#30340;&#25928;&#29575;&#65292;&#32701;&#27611;&#29699;&#20998;&#26512;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#31995;&#21015;&#26377;&#25928;&#30340;&#24212;&#29992;&#26469;&#25913;&#21892;&#21644;&#30740;&#31350;&#36873;&#25163;&#34920;&#29616;&#65292;&#20294;&#21482;&#26377;&#24456;&#23569;&#20960;&#20010;&#21487;&#20197;&#20379;&#32701;&#27611;&#29699;&#39046;&#22495;&#22806;&#30340;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#30340;&#20844;&#24320;&#32701;&#27611;&#29699;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#30340;&#32701;&#27611;&#29699;&#21333;&#25171;&#25968;&#25454;&#38598;&#38598;&#20013;&#20110;&#29305;&#23450;&#30340;&#27604;&#36187;&#23545;&#20915;&#65292;&#28982;&#32780;&#23427;&#20204;&#26080;&#27861;&#23545;&#19981;&#21516;&#36873;&#25163;&#21644;&#21508;&#31181;&#27604;&#36187;&#23545;&#20915;&#36827;&#34892;&#32508;&#21512;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32701;&#27611;&#29699;&#21333;&#25171;&#25968;&#25454;&#38598;ShuttleSet22&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;2022&#24180;&#39640;&#25490;&#21517;&#27604;&#36187;&#20013;&#25910;&#38598;&#30340;&#12290;ShuttleSet22&#35757;&#32451;&#38598;&#21253;&#25324;30,172&#20010;&#22238;&#21512;&#20013;&#30340;2,888&#20010;&#25293;&#29699;&#65292;&#39564;&#35777;&#38598;&#21253;&#25324;450&#20010;&#22238;&#21512;&#20013;&#30340;1,400&#20010;&#25293;&#29699;&#65292;&#27979;&#35797;&#38598;&#21253;&#25324;654&#20010;&#22238;&#21512;&#20013;&#30340;2,040&#20010;&#25293;&#29699;&#65292;&#24182;&#19988;&#20855;&#26377;&#27599;&#20010;&#22238;&#21512;&#20013;&#35814;&#32454;&#30340;&#25293;&#29699;&#32423;&#20803;&#25968;&#25454;&#12290;&#20026;&#20102;&#19982;ShuttleSet22&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#20351;&#29992;ShuttleNet&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#25293;&#29699;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, badminton analytics has drawn attention due to the advancement of artificial intelligence and the efficiency of data collection. While there is a line of effective applications to improve and investigate player performance, there are only a few public badminton datasets that can be used for researchers outside the badminton domain. Existing badminton singles datasets focus on specific matchups; however, they cannot provide comprehensive studies on different players and various matchups. In this paper, we provide a badminton singles dataset, ShuttleSet22, which is collected from high-ranking matches in 2022. ShuttleSet22 consists of 30,172 strokes in 2,888 rallies in the training set, 1,400 strokes in 450 rallies in the validation set, and 2,040 strokes in 654 rallies in the testing set with detailed stroke-level metadata within a rally. To benchmark existing work with ShuttleSet22, we test the state-of-the-art stroke forecasting approach, ShuttleNet, with the correspon
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#22522;&#20110;&#38598;&#25104;VAE&#30340;&#20998;&#32780;&#27835;&#20043;&#26041;&#27861;&#65292;&#22312;&#39640;&#32500;&#24230;&#12289;&#20302;&#26679;&#26412;&#25968;&#37327;&#30340;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#22312;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15661</link><description>&lt;p&gt;
&#22312;&#39640;&#32500;&#24230;&#12289;&#23567;&#26679;&#26412;&#34920;&#26684;&#25968;&#25454;&#19978;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;VAE&#30340;&#20998;&#32780;&#27835;&#20043;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Representation Learning on High-Dimensional, Small-Size Tabular Data: A Divide and Conquer Method with Ensembled VAEs. (arXiv:2306.15661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15661
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#22522;&#20110;&#38598;&#25104;VAE&#30340;&#20998;&#32780;&#27835;&#20043;&#26041;&#27861;&#65292;&#22312;&#39640;&#32500;&#24230;&#12289;&#20302;&#26679;&#26412;&#25968;&#37327;&#30340;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#22312;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21450;&#20854;&#21508;&#31181;&#21464;&#20307;&#23637;&#29616;&#20102;&#22312;&#38477;&#32500;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#36890;&#24120;&#33021;&#36798;&#21040;&#26368;&#26032;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#24230;&#12289;&#20302;&#26679;&#26412;&#25968;&#37327;(HDLSS)&#20219;&#21153;&#20013;&#65292;&#35768;&#22810;&#24403;&#21069;&#26041;&#27861;&#22312;&#23398;&#20064;&#33391;&#22909;&#30340;&#34920;&#31034;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#19968;&#20010;&#22266;&#26377;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;VAE&#30340;&#38598;&#25104;&#26469;&#23398;&#20064;&#29305;&#24449;&#31354;&#38388;&#23376;&#38598;&#19978;&#30340;&#21518;&#39564;&#27010;&#29575;&#65292;&#20174;&#32780;&#22312;&#26032;&#39062;&#30340;&#20998;&#32780;&#27835;&#20043;&#26041;&#27861;&#20013;&#32858;&#21512;&#21040;&#19968;&#20010;&#32852;&#21512;&#21518;&#39564;&#20013;&#65292;&#20174;&#32780;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#21518;&#39564;&#30340;&#19968;&#31181;&#26367;&#20195;&#20998;&#35299;&#26041;&#24335;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38544;&#24335;&#25968;&#25454;&#22686;&#24378;&#24418;&#24335;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#36890;&#36807;&#23545;&#20843;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;HDLSS&#35774;&#32622;&#20013;&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#36825;&#23548;&#33268;&#20102;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#35299;&#32544;&#32538;&#25928;&#26524;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Autoencoders and their many variants have displayed impressive ability to perform dimensionality reduction, often achieving state-of-the-art performance. Many current methods however, struggle to learn good representations in High Dimensional, Low Sample Size (HDLSS) tasks, which is an inherently challenging setting. We address this challenge by using an ensemble of lightweight VAEs to learn posteriors over subsets of the feature-space, which get aggregated into a joint posterior in a novel divide-and-conquer approach. Specifically, we present an alternative factorisation of the joint posterior that induces a form of implicit data augmentation that yields greater sample efficiency. Through a series of experiments on eight real-world datasets, we show that our method learns better latent representations in HDLSS settings, which leads to higher accuracy in a downstream classification task. Furthermore, we verify that our approach has a positive effect on disentanglement and a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25237;&#31080;&#35268;&#21017;&#30340;&#26399;&#26395;&#21464;&#24418;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#30452;&#35266;&#30340;&#35268;&#21017;-&#20108;&#39033;&#24335;&#25237;&#31080;&#65292;&#24182;&#20026;&#25152;&#26377;&#20998;&#24067;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26399;&#26395;&#21464;&#24418;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.15657</link><description>&lt;p&gt;
&#25237;&#31080;&#20013;&#20108;&#39033;&#24335;&#21464;&#24418;&#20986;&#20046;&#24847;&#26009;
&lt;/p&gt;
&lt;p&gt;
The Distortion of Binomial Voting Defies Expectation. (arXiv:2306.15657v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25237;&#31080;&#35268;&#21017;&#30340;&#26399;&#26395;&#21464;&#24418;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#30452;&#35266;&#30340;&#35268;&#21017;-&#20108;&#39033;&#24335;&#25237;&#31080;&#65292;&#24182;&#20026;&#25152;&#26377;&#20998;&#24067;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26399;&#26395;&#21464;&#24418;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#20013;&#65292;&#25237;&#31080;&#35268;&#21017;&#30340;&#21464;&#24418;&#24230;&#37327;&#20102;&#35268;&#21017;&#22312;&#20811;&#26381;&#26377;&#38480;&#30340;&#20559;&#22909;&#20449;&#24687;&#20197;&#36873;&#25321;&#31038;&#20250;&#19978;&#29702;&#24819;&#32467;&#26524;&#30340;&#31243;&#24230;&#12290;&#36825;&#20010;&#27010;&#24565;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20165;&#20165;&#36890;&#36807;&#26368;&#22351;&#24773;&#20917;&#30340;&#35270;&#35282;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25237;&#31080;&#35268;&#21017;&#30456;&#23545;&#20110;&#24213;&#23618;&#36164;&#20135;&#32773;&#25928;&#29992;&#20998;&#24067;&#30340;&#26399;&#26395;&#21464;&#24418;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#30452;&#35266;&#30340;&#35268;&#21017;-&#20108;&#39033;&#24335;&#25237;&#31080;&#65292;&#23427;&#20026;&#25152;&#26377;&#20998;&#24067;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26399;&#26395;&#21464;&#24418;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computational social choice, the distortion of a voting rule quantifies the degree to which the rule overcomes limited preference information to select a socially desirable outcome. This concept has been investigated extensively, but only through a worst-case lens. Instead, we study the expected distortion of voting rules with respect to an underlying distribution over voter utilities. Our main contribution is the design and analysis of a novel and intuitive rule, binomial voting, which provides strong expected distortion guarantees for all distributions.
&lt;/p&gt;</description></item><item><title>SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15656</link><description>&lt;p&gt;
SparseOptimizer: &#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#26469;&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#32534;&#35793;&#22120;&#20849;&#21516;&#35774;&#35745;&#26469;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design. (arXiv:2306.15656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15656
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SparseOptimizer&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65292;ALBERT&#21644;GPT&#65289;&#20013;&#33258;&#28982;&#22320;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;SparseOptimizer&#35774;&#35745;&#30340;&#20851;&#38190;&#26159;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30452;&#25509;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#36825;&#20010;&#25805;&#20316;&#31526;&#36890;&#36807;&#22362;&#23454;&#30340;&#29702;&#35770;&#26694;&#26550;&#25903;&#25345;&#65292;&#24182;&#21253;&#21547;&#20102;&#19968;&#20010;&#20998;&#26512;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20248;&#21270;&#22120;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;SparseOptimizer&#30340;&#21363;&#25554;&#21363;&#29992;&#21151;&#33021;&#28040;&#38500;&#20102;&#23545;&#20195;&#30721;&#20462;&#25913;&#30340;&#38656;&#27714;&#65292;&#20351;&#20854;&#25104;&#20026;&#36866;&#29992;&#20110;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#36866;&#24212;&#24037;&#20855;&#12290;&#22312;GLUE&#12289;RACE&#12289;SQuAD1&#21644;SQuAD2&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;SparseOptimizer&#31232;&#30095;&#21270;&#21518;&#30340;SparseBERT&#21644;SparseALBERT&#22312;&#24615;&#33021;&#19978;&#19982;&#23494;&#38598;&#22411;&#30340;BERT&#21644;ALBERT&#30456;&#24403;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#20998;&#31163;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24322;&#27493;&#35745;&#31639;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#22522;&#30784;&#65292;&#36827;&#34892;&#20102;&#24322;&#27493;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2306.15632</link><description>&lt;p&gt;
&#24322;&#27493;&#31639;&#27861;&#19982;Cocycles&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Algorithmic Alignment with Cocycles. (arXiv:2306.15632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#20998;&#31163;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24322;&#27493;&#35745;&#31639;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#22522;&#30784;&#65292;&#36827;&#34892;&#20102;&#24322;&#27493;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#12290;&#20294;&#26159;&#65292;&#20856;&#22411;&#30340;GNN&#22312;&#23450;&#20041;&#21644;&#35843;&#29992;&#28040;&#24687;&#20989;&#25968;&#20043;&#38388;&#27169;&#31946;&#20102;&#21306;&#21035;&#65292;&#36843;&#20351;&#33410;&#28857;&#22312;&#27599;&#19968;&#23618;&#37117;&#21521;&#20854;&#37051;&#23621;&#21457;&#36865;&#28040;&#24687;&#65292;&#21516;&#27493;&#22320;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;GNN&#24212;&#29992;&#20110;&#23398;&#20064;&#25191;&#34892;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26102;&#65292;&#22823;&#22810;&#25968;&#27493;&#39588;&#21482;&#26377;&#23569;&#25968;&#20960;&#20010;&#33410;&#28857;&#20250;&#26377;&#26377;&#24847;&#20041;&#30340;&#26356;&#26032;&#35201;&#21457;&#36865;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#22312;&#22270;&#20013;&#21457;&#36865;&#22826;&#22810;&#26080;&#20851;&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#23548;&#33268;&#20302;&#25928;&#29575;&#65292;&#32780;&#35768;&#22810;&#20013;&#38388;&#30340;GNN&#27493;&#39588;&#24517;&#39035;&#23398;&#20064;&#36523;&#20221;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#20998;&#31163;&#20102;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#36825;&#31181;&#20998;&#31163;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#25968;&#23398;&#34920;&#36798;&#65292;&#21487;&#20197;&#35753;&#25105;&#20204;&#24605;&#32771;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24322;&#27493;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art neural algorithmic reasoners make use of message passing in graph neural networks (GNNs). But typical GNNs blur the distinction between the definition and invocation of the message function, forcing a node to send messages to its neighbours at every layer, synchronously. When applying GNNs to learn to execute dynamic programming algorithms, however, on most steps only a handful of the nodes would have meaningful updates to send. One, hence, runs the risk of inefficiencies by sending too much irrelevant data across the graph -- with many intermediate GNN steps having to learn identity functions. In this work, we explicitly separate the concepts of node state update and message function invocation. With this separation, we obtain a mathematical formulation that allows us to reason about asynchronous computation in both algorithms and neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#34920;&#24449;&#21644;&#26657;&#27491;&#20013;&#24615;&#21407;&#23376;NISQ&#35774;&#22791;&#19978;&#30340;&#22122;&#22768;&#21442;&#25968;&#65292;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#35745;&#31639;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2306.15628</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20013;&#24615;&#21407;&#23376;NISQ&#35774;&#22791;&#22122;&#22768;&#29305;&#24615;&#21644;&#26657;&#27491;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine-learning based noise characterization and correction on neutral atoms NISQ devices. (arXiv:2306.15628v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#34920;&#24449;&#21644;&#26657;&#27491;&#20013;&#24615;&#21407;&#23376;NISQ&#35774;&#22791;&#19978;&#30340;&#22122;&#22768;&#21442;&#25968;&#65292;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#35745;&#31639;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#24615;&#21407;&#23376;&#22120;&#20214;&#21033;&#29992;&#20809;&#38218;&#25490;&#21015;&#21407;&#23376;&#21644;&#35843;&#21046;&#28608;&#20809;&#33033;&#20914;&#25511;&#21046;&#37327;&#23376;&#24577;&#65292;&#20195;&#34920;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;Pasqal&#21457;&#23637;&#20102;&#19968;&#31181;&#20351;&#29992;&#38135;&#21407;&#23376;&#30340;&#20013;&#24615;&#21407;&#23376;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#65292;&#21487;&#22788;&#29702;&#22810;&#36798;100&#20010;&#37327;&#23376;&#27604;&#29305;&#12290;&#25152;&#26377;NISQ&#35774;&#22791;&#37117;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#36825;&#23545;&#35745;&#31639;&#32467;&#26524;&#26377;&#30528;&#19968;&#23450;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26356;&#22909;&#22320;&#20102;&#35299;&#21644;&#34920;&#24449;&#22122;&#22768;&#28304;&#24182;&#21487;&#33021;&#32416;&#27491;&#23427;&#20204;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#34920;&#24449;&#21644;&#26657;&#27491;&#20013;&#24615;&#21407;&#23376;NISQ&#35774;&#22791;&#19978;&#30340;&#22122;&#22768;&#21442;&#25968;&#12290;&#29305;&#21035;&#20851;&#27880;Pasqal&#35774;&#22791;&#65292;&#24182;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#26469;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#12290;&#20026;&#20102;&#34920;&#24449;&#22122;&#22768;&#21442;&#25968;&#65292;&#35757;&#32451;&#20102;&#22810;&#20010;ML&#27169;&#22411;&#65292;&#21482;&#20351;&#29992;&#21407;&#23376;&#26368;&#32456;&#37327;&#23376;&#24577;&#30340;&#27979;&#37327;&#32467;&#26524;&#20316;&#20026;&#36755;&#20837;&#65292;&#26469;&#39044;&#27979;&#28608;&#20809;&#24378;&#24230;&#27874;&#21160;&#21644;&#33136;&#22260;&#12289;&#28201;&#24230;&#20197;&#21450;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#27979;&#37327;&#20540;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neutral atoms devices represent a promising technology that uses optical tweezers to geometrically arrange atoms and modulated laser pulses to control the quantum states. A neutral atoms Noisy Intermediate Scale Quantum (NISQ) device is developed by Pasqal with rubidium atoms that will allow to work with up to 100 qubits. All NISQ devices are affected by noise that have an impact on the computations results. Therefore it is important to better understand and characterize the noise sources and possibly to correct them. Here, two approaches are proposed to characterize and correct noise parameters on neutral atoms NISQ devices. In particular the focus is on Pasqal devices and Machine Learning (ML) techniques are adopted to pursue those objectives. To characterize the noise parameters, several ML models are trained, using as input only the measurements of the final quantum state of the atoms, to predict laser intensity fluctuation and waist, temperature and false positive and negative mea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;LeanDojo&#65292;&#35813;&#24037;&#20855;&#36890;&#36807;&#25552;&#21462;Lean&#30340;&#25968;&#25454;&#65292;&#20026;&#23450;&#29702;&#35777;&#26126;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#24179;&#21488;&#12290;&#21033;&#29992;LeanDojo&#30340;&#25968;&#25454;&#65292;&#24320;&#21457;&#20102;ReProver&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#26126;&#22120;&#65292;&#21487;&#20197;&#20174;&#24222;&#22823;&#30340;&#25968;&#23398;&#24211;&#20013;&#36873;&#25321;&#21629;&#39064;&#65292;&#35757;&#32451;&#25104;&#26412;&#20302;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#19968;&#21608;&#30340;GPU&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.15626</link><description>&lt;p&gt;
LeanDojo: &#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#29702;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
LeanDojo: Theorem Proving with Retrieval-Augmented Language Models. (arXiv:2306.15626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;LeanDojo&#65292;&#35813;&#24037;&#20855;&#36890;&#36807;&#25552;&#21462;Lean&#30340;&#25968;&#25454;&#65292;&#20026;&#23450;&#29702;&#35777;&#26126;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#24179;&#21488;&#12290;&#21033;&#29992;LeanDojo&#30340;&#25968;&#25454;&#65292;&#24320;&#21457;&#20102;ReProver&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#26126;&#22120;&#65292;&#21487;&#20197;&#20174;&#24222;&#22823;&#30340;&#25968;&#23398;&#24211;&#20013;&#36873;&#25321;&#21629;&#39064;&#65292;&#35757;&#32451;&#25104;&#26412;&#20302;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#19968;&#21608;&#30340;GPU&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20351;&#29992;Lean&#31561;&#35777;&#26126;&#21161;&#25163;&#35777;&#26126;&#24418;&#24335;&#23450;&#29702;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31169;&#26377;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#22823;&#37327;&#35745;&#31639;&#35201;&#27714;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24456;&#38590;&#22797;&#21046;&#25110;&#24314;&#31435;&#22312;&#20854;&#22522;&#30784;&#19978;&#65292;&#36825;&#32473;&#23450;&#29702;&#35777;&#26126;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#30740;&#31350;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;LeanDojo&#26469;&#28040;&#38500;&#36825;&#20123;&#38556;&#30861;&#65306;&#19968;&#20010;&#21253;&#21547;&#24037;&#20855;&#21253;&#12289;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;Lean&#28216;&#20048;&#22330;&#12290;LeanDojo&#20174;Lean&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#24182;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#32534;&#31243;&#19982;&#35777;&#26126;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#23427;&#21253;&#21547;&#35777;&#26126;&#20013;&#21629;&#39064;&#30340;&#32454;&#31890;&#24230;&#27880;&#37322;&#65292;&#20026;&#21629;&#39064;&#36873;&#25321;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#65306;&#36825;&#26159;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;ReProver&#65288;&#26816;&#32034;&#22686;&#24378;&#30340;&#35777;&#26126;&#22120;&#65289;&#65306;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;LLM&#30340;&#35777;&#26126;&#22120;&#65292;&#36890;&#36807;&#26816;&#32034;&#20174;&#24222;&#22823;&#30340;&#25968;&#23398;&#24211;&#20013;&#36873;&#25321;&#21629;&#39064;&#12290;&#23427;&#25104;&#26412;&#20302;&#24265;&#65292;&#21482;&#38656;&#35201;&#19968;&#21608;&#30340;GPU&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26816;&#32034;&#22120;&#21033;&#29992;&#20102;LeanDojo&#30340;pro&#30456;&#20851;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection: a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): the first LLM-based prover that is augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's prog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20215;&#20540;&#24863;&#30693;&#37325;&#35201;&#24615;&#26435;&#37325;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#27491;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#30340;&#26041;&#24046;&#24182;&#20445;&#25345;&#26080;&#20559;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#23454;&#36341;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15625</link><description>&lt;p&gt;
&#38024;&#23545;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#30340;&#20215;&#20540;&#24863;&#30693;&#37325;&#35201;&#24615;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
Value-aware Importance Weighting for Off-policy Reinforcement Learning. (arXiv:2306.15625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20215;&#20540;&#24863;&#30693;&#37325;&#35201;&#24615;&#26435;&#37325;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#27491;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#30340;&#26041;&#24046;&#24182;&#20445;&#25345;&#26080;&#20559;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#23454;&#36341;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#37319;&#26679;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#31163;&#31574;&#30053;&#39044;&#27979;&#30340;&#19968;&#20010;&#26680;&#24515;&#24605;&#24819;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#19968;&#20010;&#20998;&#24067;&#20013;&#30340;&#26679;&#26412;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#65292;&#20174;&#32780;&#22312;&#21478;&#19968;&#20010;&#20998;&#24067;&#19979;&#33719;&#24471;&#26080;&#20559;&#30340;&#20272;&#35745;&#20540;&#12290;&#28982;&#32780;&#65292;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#24448;&#24448;&#20250;&#34920;&#29616;&#20986;&#26497;&#31471;&#30340;&#26041;&#24046;&#65292;&#24120;&#24120;&#23548;&#33268;&#23454;&#36341;&#20013;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#26356;&#24191;&#27867;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#26469;&#26657;&#27491;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#8220;&#20215;&#20540;&#24863;&#30693;&#37325;&#35201;&#24615;&#26435;&#37325;&#8221;&#65292;&#23427;&#32771;&#34385;&#20102;&#26679;&#26412;&#31354;&#38388;&#65292;&#20174;&#32780;&#22312;&#30446;&#26631;&#20998;&#24067;&#19979;&#25552;&#20379;&#26356;&#20302;&#30340;&#26041;&#24046;&#65292;&#20294;&#20173;&#28982;&#26159;&#26080;&#20559;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#22914;&#20309;&#35745;&#31639;&#36825;&#26679;&#30340;&#26435;&#37325;&#65292;&#24182;&#35814;&#32454;&#35828;&#26126;&#20102;&#32467;&#26524;&#37325;&#35201;&#24615;&#26435;&#37325;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20960;&#20010;&#24378;&#21270;&#23398;&#20064;&#39044;&#27979;&#31639;&#27861;&#25193;&#23637;&#21040;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#30340;&#31163;&#31574;&#30053;&#35774;&#32622;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Importance sampling is a central idea underlying off-policy prediction in reinforcement learning. It provides a strategy for re-weighting samples from a distribution to obtain unbiased estimates under another distribution. However, importance sampling weights tend to exhibit extreme variance, often leading to stability issues in practice. In this work, we consider a broader class of importance weights to correct samples in off-policy learning. We propose the use of $\textit{value-aware importance weights}$ which take into account the sample space to provide lower variance, but still unbiased, estimates under a target distribution. We derive how such weights can be computed, and detail key properties of the resulting importance weights. We then extend several reinforcement learning prediction algorithms to the off-policy setting with these weights, and evaluate them empirically.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#25105;&#20204;&#20445;&#25345;&#20102;&#25193;&#23637;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#20219;&#21153;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15595</link><description>&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;
&lt;/p&gt;
&lt;p&gt;
Extending Context Window of Large Language Models via Positional Interpolation. (arXiv:2306.15595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15595
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#25105;&#20204;&#20445;&#25345;&#20102;&#25193;&#23637;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#20219;&#21153;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20301;&#32622;&#25554;&#20540;&#65288;PI&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;LLaMA&#27169;&#22411;&#65289;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#19988;&#22312;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#30340;&#21508;&#31181;&#20219;&#21153;&#65288;&#21253;&#25324;&#23494;&#38053;&#26816;&#32034;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#31687;&#25991;&#26723;&#25688;&#35201;&#31561;&#65289;&#19978;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#25193;&#23637;&#30340;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#30340;&#20219;&#21153;&#20013;&#30456;&#23545;&#20445;&#25345;&#33391;&#22909;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20301;&#32622;&#25554;&#20540;&#32447;&#24615;&#22320;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#20197;&#21305;&#37197;&#21407;&#22987;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#65292;&#32780;&#19981;&#26159;&#36229;&#36807;&#35757;&#32451;&#26102;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#39640;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#23436;&#20840;&#30772;&#22351;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#25554;&#20540;&#30340;&#19978;&#30028;&#33267;&#23569;&#26159;&#25512;&#26029;&#30340;&#19978;&#30028;&#30340;$\sim 600 \times$&#35201;&#23567;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least $\sim 600 \times$ smaller than that of extrapolation, further demonstrating its stability. Models extend
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RansomAI&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#38544;&#34109;&#21152;&#23494;&#21202;&#32034;&#36719;&#20214;&#65292;&#23427;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36866;&#24212;&#21152;&#23494;&#34892;&#20026;&#65292;&#26368;&#23567;&#21270;&#34987;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#21152;&#23494;&#26102;&#26368;&#22823;&#21270;&#25439;&#23475;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15559</link><description>&lt;p&gt;
RansomAI: &#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#38544;&#34109;&#21152;&#23494;&#21202;&#32034;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
RansomAI: AI-powered Ransomware for Stealthy Encryption. (arXiv:2306.15559v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RansomAI&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#38544;&#34109;&#21152;&#23494;&#21202;&#32034;&#36719;&#20214;&#65292;&#23427;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36866;&#24212;&#21152;&#23494;&#34892;&#20026;&#65292;&#26368;&#23567;&#21270;&#34987;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#21152;&#23494;&#26102;&#26368;&#22823;&#21270;&#25439;&#23475;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#29190;&#21457;, &#21202;&#32034;&#36719;&#20214;&#65288;&#21253;&#25324;&#24694;&#24847;&#36719;&#20214;&#65289;&#23558;&#20250;&#36816;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#26234;&#33021;&#32780;&#21160;&#24577;&#22320;&#36866;&#24212;&#20854;&#21152;&#23494;&#34892;&#20026;&#20197;&#36991;&#20813;&#34987;&#26816;&#27979;&#12290;&#36825;&#23558;&#23548;&#33268;&#29616;&#26377;&#30340;&#32593;&#32476;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#21464;&#24471;&#26080;&#25928;&#21644;&#36807;&#26102;&#65292;&#20294;&#26159;&#25991;&#29486;&#20013;&#32570;&#20047;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21202;&#32034;&#36719;&#20214;&#26469;&#39564;&#35777;&#27492;&#35266;&#28857;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RansomAI&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#21202;&#32034;&#36719;&#20214;&#26679;&#26412;&#20013;&#65292;&#20351;&#20854;&#21152;&#23494;&#34892;&#20026;&#20855;&#26377;&#36866;&#24212;&#24615;&#65292;&#24182;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;RansomAI&#24341;&#20837;&#20102;&#19968;&#20010;&#26234;&#33021;&#20195;&#29702;&#65292;&#36890;&#36807;&#22870;&#21169;&#26426;&#21046;&#21644;&#25351;&#32441;&#26234;&#33021;&#26816;&#27979;&#31995;&#32479;&#26469;&#23398;&#20064;&#26368;&#20339;&#30340;&#21152;&#23494;&#31639;&#27861;&#12289;&#36895;&#29575;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#20197;&#38477;&#20302;&#34987;&#26816;&#27979;&#30340;&#27010;&#29575;&#21516;&#26102;&#26368;&#22823;&#21270;&#20854;&#25439;&#23475;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cybersecurity solutions have shown promising performance when detecting ransomware samples that use fixed algorithms and encryption rates. However, due to the current explosion of Artificial Intelligence (AI), sooner than later, ransomware (and malware in general) will incorporate AI techniques to intelligently and dynamically adapt its encryption behavior to be undetected. It might result in ineffective and obsolete cybersecurity solutions, but the literature lacks AI-powered ransomware to verify it. Thus, this work proposes RansomAI, a Reinforcement Learning-based framework that can be integrated into existing ransomware samples to adapt their encryption behavior and stay stealthy while encrypting files. RansomAI presents an agent that learns the best encryption algorithm, rate, and duration that minimizes its detection (using a reward mechanism and a fingerprinting intelligent detection system) while maximizing its damage function. The proposed framework was validated in a ransomwar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CamemBERT-bio&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#36890;&#29992;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15550</link><description>&lt;p&gt;
CamemBERT-bio&#65306;&#19968;&#31181;&#26356;&#20581;&#24247;&#30340;&#27861;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CamemBERT-bio: a Tasty French Language Model Better for your Health. (arXiv:2306.15550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CamemBERT-bio&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#36890;&#29992;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20020;&#24202;&#25968;&#25454;&#20179;&#24211;&#65292;&#21307;&#38498;&#20013;&#30340;&#20020;&#24202;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#23481;&#26131;&#29992;&#20110;&#30740;&#31350;&#65292;&#28982;&#32780;&#36825;&#20123;&#25991;&#20214;&#37117;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#20174;&#21307;&#30103;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#36827;&#34892;&#20020;&#24202;&#30740;&#31350;&#12290;&#20351;&#29992;CamemBERT&#31561;BERT-like&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#20026;&#36890;&#29992;&#35821;&#35328;&#35757;&#32451;&#30340;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#25928;&#26524;&#36739;&#24369;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27861;&#35821;&#20844;&#20849;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#23545;CamemBERT&#36827;&#34892;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CamemBERT-bio&#30340;&#31532;&#19968;&#20010;&#29256;&#26412;&#65292;&#23427;&#26159;&#19968;&#31181;&#20026;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#20844;&#20849;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical data in hospitals are increasingly accessible for research through clinical data warehouses, however these documents are unstructured. It is therefore necessary to extract information from medical reports to conduct clinical studies. Transfer learning with BERT-like models such as CamemBERT has allowed major advances, especially for named entity recognition. However, these models are trained for plain language and are less efficient on biomedical data. This is why we propose a new French public biomedical dataset on which we have continued the pre-training of CamemBERT. Thus, we introduce a first version of CamemBERT-bio, a specialized public model for the French biomedical domain that shows 2.54 points of F1 score improvement on average on different biomedical named entity recognition tasks.
&lt;/p&gt;</description></item><item><title>&#22522;&#30784;&#27169;&#22411;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#25193;&#23637;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;&#27169;&#22411;&#21457;&#23637;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2306.15546</link><description>&lt;p&gt;
&#24403;&#22522;&#30784;&#27169;&#22411;&#36935;&#21040;&#32852;&#37030;&#23398;&#20064;&#65306;&#21160;&#26426;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions. (arXiv:2306.15546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15546
&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#25193;&#23637;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;&#27169;&#22411;&#21457;&#23637;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#30456;&#20114;&#30340;&#22909;&#22788;&#65292;&#22312;AI&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#35299;&#20915;&#20102;AI&#21644;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;FL&#25193;&#23637;&#20102;FM&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#35745;&#31639;&#20849;&#20139;&#65292;&#20998;&#25955;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20943;&#36731;&#20102;FL&#21442;&#19982;&#32773;&#30340;&#36127;&#25285;&#12290;&#23427;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;FM&#21457;&#23637;&#65292;&#27665;&#20027;&#21270;&#20102;&#36825;&#19968;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#21253;&#23481;&#24615;&#21644;&#21019;&#26032;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;FM&#20197;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20026;FL&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#36215;&#28857;&#65292;&#20419;&#36827;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19979;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;FM&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#20016;&#23500;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#20943;&#23569;&#36807;&#25311;&#21512;&#65292;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#30740;&#31350;FL&#21644;FM&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26412;&#25991;&#26088;&#22312;&#21152;&#28145;&#23545;&#23427;&#20204;&#21327;&#21516;&#20851;&#31995;&#30340;&#29702;&#35299;&#65292;&#24378;&#35843;&#21160;&#26426;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intersection of the Foundation Model (FM) and Federated Learning (FL) provides mutual benefits, presents a unique opportunity to unlock new possibilities in AI research, and address critical challenges in AI and real-world applications. FL expands the availability of data for FMs and enables computation sharing, distributing the training process and reducing the burden on FL participants. It promotes collaborative FM development, democratizing the process and fostering inclusivity and innovation. On the other hand, FM, with its enormous size, pre-trained knowledge, and exceptional performance, serves as a robust starting point for FL, facilitating faster convergence and better performance under non-iid data. Additionally, leveraging FM to generate synthetic data enriches data diversity, reduces overfitting, and preserves privacy. By examining the interplay between FL and FM, this paper aims to deepen the understanding of their synergistic relationship, highlighting the motivations,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26032;&#24037;&#20855;&#65292;&#25506;&#35752;&#20102;&#31038;&#20250;&#24433;&#21709;&#26426;&#21046;&#19982;&#33322;&#31354;&#20844;&#21496;&#36873;&#25321;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#23545;&#29992;&#25143;&#35780;&#35770;&#30340;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#21345;&#22612;&#23612;&#20122;&#26426;&#22330;&#33322;&#31354;&#29983;&#24577;&#31995;&#32479;&#20013;&#33322;&#31354;&#20844;&#21496;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.15541</link><description>&lt;p&gt;
&#35299;&#25918;&#29992;&#25143;&#35780;&#35770;&#30340;&#21147;&#37327;&#65306;&#25506;&#32034;&#24847;&#22823;&#21033;&#21345;&#22612;&#23612;&#20122;&#26426;&#22330;&#30340;&#33322;&#31354;&#20844;&#21496;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of User Reviews: Exploring Airline Choices at Catania Airport, Italy. (arXiv:2306.15541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26032;&#24037;&#20855;&#65292;&#25506;&#35752;&#20102;&#31038;&#20250;&#24433;&#21709;&#26426;&#21046;&#19982;&#33322;&#31354;&#20844;&#21496;&#36873;&#25321;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#23545;&#29992;&#25143;&#35780;&#35770;&#30340;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#21345;&#22612;&#23612;&#20122;&#26426;&#22330;&#33322;&#31354;&#29983;&#24577;&#31995;&#32479;&#20013;&#33322;&#31354;&#20844;&#21496;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26032;&#24037;&#20855;&#65292;&#25506;&#35752;&#31038;&#20250;&#24433;&#21709;&#26426;&#21046;&#19982;&#33322;&#31354;&#20844;&#21496;&#36873;&#25321;&#20043;&#38388;&#30340;&#21487;&#33021;&#20851;&#31995;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;&#24433;&#21709;&#28040;&#36153;&#32773;&#22312;&#33322;&#31354;&#39046;&#22495;&#20915;&#31574;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#36873;&#25321;&#20174;&#30693;&#21517;&#24179;&#21488;Trustpilot&#12289;Google&#21644;Twitter&#20013;&#25552;&#21462;&#29992;&#25143;&#35780;&#35770;&#12290;&#36890;&#36807;&#32467;&#21512;&#32593;&#32476;&#29228;&#21462;&#25216;&#26415;&#65292;&#25105;&#20204;&#33021;&#22815;&#25910;&#38598;&#21040;&#21253;&#21547;&#21508;&#31181;&#29992;&#25143;&#24847;&#35265;&#12289;&#21453;&#39304;&#21644;&#35780;&#20998;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;BERT&#27169;&#22411;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#32858;&#28966;&#33322;&#31354;&#20844;&#21496;&#35780;&#35770;&#20013;&#30340;&#26377;&#35265;&#22320;&#30340;&#24773;&#24863;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#33322;&#31354;&#20844;&#21496;&#24179;&#22343;&#36127;&#38754;&#24773;&#24863;&#24471;&#20998;&#30340;&#26377;&#36259;&#36235;&#21183;&#65292;&#36825;&#20351;&#25105;&#20204;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#20102;&#33322;&#31354;&#20844;&#21496;&#20043;&#38388;&#30340;&#21160;&#24577;&#65292;&#24110;&#21161;&#25105;&#20204;&#35782;&#21035;&#21345;&#22612;&#23612;&#20122;&#26426;&#22330;&#33322;&#31354;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#21512;&#20316;&#20249;&#20276;&#12289;&#28909;&#38376;&#33322;&#32447;&#21644;&#25198;&#28436;&#26680;&#24515;&#35282;&#33394;&#30340;&#33322;&#31354;&#20844;&#21496;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to investigate the possible relationship between the mechanisms of social influence and the choice of airline, through the use of new tools, with the aim of understanding whether they can contribute to a better understanding of the factors influencing the decisions of consumers in the aviation sector. We have chosen to extract user reviews from well-known platforms: Trustpilot, Google, and Twitter. By combining web scraping techniques, we have been able to collect a comprehensive dataset comprising a wide range of user opinions, feedback, and ratings. We then refined the BERT model to focus on insightful sentiment in the context of airline reviews. Through our analysis, we observed an intriguing trend of average negative sentiment scores across various airlines, giving us deeper insight into the dynamics between airlines and helping us identify key partnerships, popular routes, and airlines that play a central role in the aeronautical ecosystem of Catania airport during
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#34394;&#25311;&#22330;&#26223;&#38598;&#21512;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#21644;&#24555;&#36895;&#35780;&#20272;&#23548;&#33322;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#21442;&#25968;&#26041;&#27861;&#29992;&#20110;&#25506;&#32034;&#19981;&#21516;&#30340;&#30000;&#22320;&#24418;&#29366;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.15517</link><description>&lt;p&gt;
&#22686;&#24378;&#34892;&#20316;&#29289;&#27169;&#25311;&#20013;&#30340;&#23548;&#33322;&#22522;&#20934;&#27979;&#35797;&#21644;&#24863;&#30693;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Navigation Benchmarking and Perception Data Generation for Row-based Crops in Simulation. (arXiv:2306.15517v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#34394;&#25311;&#22330;&#26223;&#38598;&#21512;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#21644;&#24555;&#36895;&#35780;&#20272;&#23548;&#33322;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#21442;&#25968;&#26041;&#27861;&#29992;&#20110;&#25506;&#32034;&#19981;&#21516;&#30340;&#30000;&#22320;&#24418;&#29366;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26381;&#21153;&#26426;&#22120;&#20154;&#25216;&#26415;&#22312;&#31934;&#20934;&#20892;&#19994;&#20013;&#30340;&#24212;&#29992;&#19981;&#26029;&#25552;&#21319;&#65292;&#20351;&#24471;&#24456;&#22810;&#33258;&#21160;&#21270;&#36807;&#31243;&#24471;&#20197;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#29983;&#25104;&#21644;&#29616;&#22330;&#39564;&#35777;&#27963;&#21160;&#38480;&#21046;&#20102;&#22823;&#35268;&#27169;&#33258;&#20027;&#24179;&#21488;&#30340;&#36827;&#23637;&#12290;&#27169;&#25311;&#29615;&#22659;&#21644;&#28145;&#24230;&#35270;&#35273;&#24863;&#30693;&#24050;&#25104;&#20026;&#21152;&#36895;&#24320;&#21457;&#20855;&#26377;&#20302;&#25104;&#26412;RGB-D&#30456;&#26426;&#30340;&#40065;&#26834;&#23548;&#33322;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#22871;&#29992;&#20110;&#24555;&#36895;&#35780;&#20272;&#23548;&#33322;&#31639;&#27861;&#30340;&#34394;&#25311;&#22330;&#26223;&#38598;&#21512;&#12290;&#27492;&#22806;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25506;&#32034;&#19981;&#21516;&#30340;&#30000;&#22320;&#20960;&#20309;&#24418;&#29366;&#21644;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#20316;&#29289;&#19978;&#35757;&#32451;&#28145;&#24230;&#20998;&#21106;&#32593;&#32476;&#24182;&#23545;&#20854;&#23548;&#33322;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#27169;&#25311;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Service robotics is recently enhancing precision agriculture enabling many automated processes based on efficient autonomous navigation solutions. However, data generation and infield validation campaigns hinder the progress of large-scale autonomous platforms. Simulated environments and deep visual perception are spreading as successful tools to speed up the development of robust navigation with low-cost RGB-D cameras. In this context, the contribution of this work is twofold: a synthetic dataset to train deep semantic segmentation networks together with a collection of virtual scenarios for a fast evaluation of navigation algorithms. Moreover, an automatic parametric approach is developed to explore different field geometries and features. The simulation framework and the dataset have been evaluated by training a deep segmentation network on different crops and benchmarking the resulting navigation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#30340;&#22238;&#25918;&#35760;&#24518;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#37319;&#26679;&#30340;&#35270;&#35282;&#25193;&#23637;&#21040;&#36712;&#36857;&#20013;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#21453;&#21521;&#37319;&#26679;&#36712;&#36857;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#21152;&#26435;&#35780;&#35770;&#30446;&#26631;&#36991;&#20813;&#37319;&#26679;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#12290;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#36824;&#33021;&#26681;&#25454;&#19981;&#21516;&#30340;&#20248;&#20808;&#24230;&#25351;&#26631;&#20248;&#20808;&#37319;&#26679;&#25928;&#29575;&#26356;&#39640;&#30340;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2306.15503</link><description>&lt;p&gt;
&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#65306;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#30340;&#22238;&#25918;&#35760;&#24518;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prioritized Trajectory Replay: A Replay Memory for Data-driven Reinforcement Learning. (arXiv:2306.15503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#30340;&#22238;&#25918;&#35760;&#24518;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#37319;&#26679;&#30340;&#35270;&#35282;&#25193;&#23637;&#21040;&#36712;&#36857;&#20013;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#21453;&#21521;&#37319;&#26679;&#36712;&#36857;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#21152;&#26435;&#35780;&#35770;&#30446;&#26631;&#36991;&#20813;&#37319;&#26679;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#12290;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#36824;&#33021;&#26681;&#25454;&#19981;&#21516;&#30340;&#20248;&#20808;&#24230;&#25351;&#26631;&#20248;&#20808;&#37319;&#26679;&#25928;&#29575;&#26356;&#39640;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#20063;&#31216;&#20026;&#31163;&#32447;RL&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#20855;&#26377;&#25552;&#21319;&#22312;&#32447;RL&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20294;&#31163;&#32447;RL&#20013;&#30340;&#25968;&#25454;&#37319;&#26679;&#25216;&#26415;&#30340;&#20316;&#29992;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30452;&#25509;&#23558;&#37319;&#26679;&#25216;&#26415;&#24212;&#29992;&#20110;&#29366;&#24577;&#36716;&#25442;&#24182;&#19981;&#33021;&#22987;&#32456;&#25552;&#39640;&#31163;&#32447;RL&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#25216;&#26415;&#8212;&#8212;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#65288;TR/PTR&#65289;&#65292;&#23427;&#23558;&#37319;&#26679;&#30340;&#35270;&#35282;&#25193;&#23637;&#21040;&#36712;&#36857;&#20013;&#65292;&#20197;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;TR&#36890;&#36807;&#21453;&#21521;&#37319;&#26679;&#36712;&#36857;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#20248;&#21270;&#21518;&#32493;&#29366;&#24577;&#20449;&#24687;&#30340;&#20351;&#29992;&#12290;&#22312;TR&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21152;&#26435;&#35780;&#35770;&#30446;&#26631;&#65292;&#20197;&#36991;&#20813;&#22312;&#31163;&#32447;&#35757;&#32451;&#20013;&#37319;&#26679;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#65288;PTR&#65289;&#26469;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#36712;&#36857;&#37319;&#26679;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#36712;&#36857;&#20248;&#20808;&#24230;&#25351;&#26631;&#36827;&#34892;&#20248;&#20808;&#35774;&#32622;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
In recent years, data-driven reinforcement learning (RL), also known as offline RL, have gained significant attention. However, the role of data sampling techniques in offline RL has been overlooked despite its potential to enhance online RL performance. Recent research suggests applying sampling techniques directly to state-transitions does not consistently improve performance in offline RL. Therefore, in this study, we propose a memory technique, (Prioritized) Trajectory Replay (TR/PTR), which extends the sampling perspective to trajectories for more comprehensive information extraction from limited data. TR enhances learning efficiency by backward sampling of trajectories that optimizes the use of subsequent state information. Building on TR, we build the weighted critic target to avoid sampling unseen actions in offline training, and Prioritized Trajectory Replay (PTR) that enables more efficient trajectory sampling, prioritized by various trajectory priority metrics. We demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#35770;&#35777;&#26694;&#26550;$xADG$&#65292;&#36890;&#36807;&#20351;&#29992;&#24067;&#23572;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#22810;&#20010;&#25903;&#25345;&#26469;&#26500;&#24314;&#31616;&#27905;&#19988;&#21487;&#29702;&#35299;&#30340;&#35770;&#35777;&#22270;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20998;&#31867;&#20219;&#21153;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.15500</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#20998;&#31867;&#20219;&#21153;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#22411;&#32467;&#26500;&#21270;&#35770;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A novel structured argumentation framework for improved explainability of classification tasks. (arXiv:2306.15500v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#35770;&#35777;&#26694;&#26550;$xADG$&#65292;&#36890;&#36807;&#20351;&#29992;&#24067;&#23572;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#22810;&#20010;&#25903;&#25345;&#26469;&#26500;&#24314;&#31616;&#27905;&#19988;&#21487;&#29702;&#35299;&#30340;&#35770;&#35777;&#22270;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20998;&#31867;&#20219;&#21153;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#23637;&#35770;&#35777;&#20915;&#31574;&#22270;&#65288;$xADG$&#65289;&#30340;&#26032;&#22411;&#32467;&#26500;&#21270;&#35770;&#35777;&#26694;&#26550;&#12290;&#23427;&#26159;&#22522;&#20110;Dung&#30340;&#25277;&#35937;&#35770;&#35777;&#22270;&#26500;&#24314;&#30340;&#35770;&#35777;&#20915;&#31574;&#22270;&#30340;&#25299;&#23637;&#12290;$xADG$&#26694;&#26550;&#20801;&#35768;&#35770;&#35777;&#20351;&#29992;&#24067;&#23572;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#22810;&#20010;&#21069;&#25552;&#65288;&#25903;&#25345;&#65289;&#22312;&#20869;&#37096;&#32467;&#26500;&#20013;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#31616;&#27905;&#30340;&#35770;&#35777;&#22270;&#65292;&#20351;&#29992;&#25143;&#26356;&#23481;&#26131;&#29702;&#35299;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;$xADG$&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#35268;&#27169;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#22823;&#23567;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24471;&#21040;&#30340;$xADG$&#20855;&#26377;&#24378;&#65288;&#24179;&#34913;&#30340;&#65289;&#20934;&#30830;&#24615;&#65292;&#36825;&#26159;&#36890;&#36807;&#36755;&#20837;&#20915;&#31574;&#26641;&#23454;&#29616;&#30340;&#65292;&#21516;&#26102;&#36824;&#20943;&#23569;&#20102;&#36798;&#21040;&#32467;&#35770;&#25152;&#38656;&#30340;&#24179;&#22343;&#25903;&#25345;&#25968;&#12290;&#32467;&#26524;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#21487;&#20197;&#26500;&#24314;&#20986;&#22312;&#39044;&#27979;&#33021;&#21147;&#21644;&#24635;&#20307;&#22823;&#23567;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26500;&#24314;$ADG$&#25216;&#26415;&#30340;&#21487;&#29702;&#35299;&#30340;$xADG$&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel framework for structured argumentation, named extend argumentative decision graph ($xADG$). It is an extension of argumentative decision graphs built upon Dung's abstract argumentation graphs. The $xADG$ framework allows for arguments to use boolean logic operators and multiple premises (supports) within their internal structure, resulting in more concise argumentation graphs that may be easier for users to understand. The study presents a methodology for construction of $xADGs$ and evaluates their size and predictive capacity for classification tasks of varying magnitudes. Resulting $xADGs$ achieved strong (balanced) accuracy, which was accomplished through an input decision tree, while also reducing the average number of supports needed to reach a conclusion. The results further indicated that it is possible to construct plausibly understandable $xADGs$ that outperform other techniques for building $ADGs$ in terms of predictive capacity and overall size. I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20154;&#31867;&#23548;&#24072;&#25552;&#20379;&#35299;&#37322;&#24615;&#21453;&#39304;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#20004;&#31181;&#26041;&#27861;&#65292;&#22312;&#22312;&#32447;&#35838;&#31243;&#20013;&#23454;&#26102;&#20026;&#23548;&#24072;&#25552;&#20379;&#26377;&#20851;&#22914;&#20309;&#32473;&#23398;&#29983;&#26377;&#25928;&#36190;&#25196;&#30340;&#21453;&#39304;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#20351;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25552;&#20379;&#35299;&#37322;&#24615;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2306.15498</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20154;&#31867;&#23548;&#24072;&#25552;&#20379;&#35299;&#37322;&#24615;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Provide Explanatory Feedback to Human Tutors. (arXiv:2306.15498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20154;&#31867;&#23548;&#24072;&#25552;&#20379;&#35299;&#37322;&#24615;&#21453;&#39304;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#20004;&#31181;&#26041;&#27861;&#65292;&#22312;&#22312;&#32447;&#35838;&#31243;&#20013;&#23454;&#26102;&#20026;&#23548;&#24072;&#25552;&#20379;&#26377;&#20851;&#22914;&#20309;&#32473;&#23398;&#29983;&#26377;&#25928;&#36190;&#25196;&#30340;&#21453;&#39304;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#20351;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25552;&#20379;&#35299;&#37322;&#24615;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#32773;&#22312;&#20135;&#29983;&#35299;&#37322;&#20197;&#25903;&#25345;&#20182;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#26102;&#65292;&#23545;&#23398;&#20064;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25552;&#20379;&#23398;&#20064;&#32773;&#23454;&#26102;&#30340;&#35299;&#37322;&#24615;&#21453;&#39304;&#24120;&#24120;&#38754;&#20020;&#19982;&#20998;&#31867;&#20934;&#30830;&#24615;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#21253;&#21547;&#22797;&#26434;&#21644;&#24494;&#22937;&#30340;&#24773;&#22659;&#21709;&#24212;&#30340;&#39046;&#22495;&#19987;&#29992;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#22312;&#22312;&#32447;&#35838;&#31243;&#20013;&#20026;&#23548;&#24072;&#25552;&#20379;&#26377;&#20851;&#22914;&#20309;&#32473;&#23398;&#29983;&#26377;&#25928;&#36190;&#25196;&#30340;&#23454;&#26102;&#21453;&#39304;&#12290;&#36825;&#39033;&#36827;&#34892;&#20013;&#30340;&#24037;&#20316;&#22312;&#32416;&#27491;&#24615;&#21453;&#39304;&#65288;F1&#20998;&#25968;=0.811&#65289;&#21644;&#25104;&#26524;&#23548;&#21521;&#21453;&#39304;&#65288;F1&#20998;&#25968;=0.350&#65289;&#30340;&#20108;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26469;&#25552;&#20379;&#35299;&#37322;&#24615;&#21453;&#39304;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#36825;&#19981;&#20165;&#21487;&#20197;&#22312;&#35838;&#31243;&#20013;&#20026;&#23548;&#24072;&#25552;&#20379;&#21453;&#39304;&#65292;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#20986;&#23454;&#26102;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research demonstrates learners engaging in the process of producing explanations to support their reasoning, can have a positive impact on learning. However, providing learners real-time explanatory feedback often presents challenges related to classification accuracy, particularly in domain-specific environments, containing situationally complex and nuanced responses. We present two approaches for supplying tutors real-time feedback within an online lesson on how to give students effective praise. This work-in-progress demonstrates considerable accuracy in binary classification for corrective feedback of effective, or effort-based (F1 score = 0.811), and ineffective, or outcome-based (F1 score = 0.350), praise responses. More notably, we introduce progress towards an enhanced approach of providing explanatory feedback using large language model-facilitated named entity recognition, which can provide tutors feedback, not only while engaging in lessons, but can potentially suggest real-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;&#21069;&#20307;-&#24322;&#24120;&#26816;&#27979;&#65288;PoA&#26816;&#27979;&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#19981;&#21516;&#65292;PoA&#26816;&#27979;&#26088;&#22312;&#22312;&#24322;&#24120;&#21457;&#29983;&#20043;&#21069;&#26816;&#27979;&#21040;&#26410;&#26469;&#30340;&#24322;&#24120;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;17&#20010;&#22522;&#20934;&#32447;&#21644;3&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15489</link><description>&lt;p&gt;
&#38024;&#23545;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#21069;&#20307;
&lt;/p&gt;
&lt;p&gt;
Precursor-of-Anomaly Detection for Irregular Time Series. (arXiv:2306.15489v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;&#21069;&#20307;-&#24322;&#24120;&#26816;&#27979;&#65288;PoA&#26816;&#27979;&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#19981;&#21516;&#65292;PoA&#26816;&#27979;&#26088;&#22312;&#22312;&#24322;&#24120;&#21457;&#29983;&#20043;&#21069;&#26816;&#27979;&#21040;&#26410;&#26469;&#30340;&#24322;&#24120;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;17&#20010;&#22522;&#20934;&#32447;&#21644;3&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#26088;&#22312;&#35782;&#21035;&#24847;&#22806;&#30340;&#27169;&#24335;&#25110;&#25968;&#25454;&#28857;&#65292;&#24182;&#19982;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#23494;&#20999;&#30456;&#20851;&#65292;&#23588;&#20854;&#26159;&#22312;&#37329;&#34701;&#12289;&#21046;&#36896;&#12289;&#32593;&#32476;&#23433;&#20840;&#31561;&#24212;&#29992;&#20013;&#12290;&#34429;&#28982;&#24322;&#24120;&#26816;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#24322;&#24120;&#21457;&#29983;&#20043;&#21069;&#26816;&#27979;&#21040;&#26410;&#26469;&#30340;&#24322;&#24120;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#21069;&#20307;-&#24322;&#24120;&#8221;&#65288;PoA&#65289;&#26816;&#27979;&#12290;&#19982;&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#19981;&#21516;&#65292;&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#20391;&#37325;&#20110;&#30830;&#23450;&#32473;&#23450;&#26102;&#38388;&#24207;&#21015;&#35266;&#27979;&#20540;&#26159;&#21542;&#20026;&#24322;&#24120;&#65292;&#32780;PoA&#26816;&#27979;&#26088;&#22312;&#22312;&#24322;&#24120;&#21457;&#29983;&#20043;&#21069;&#26816;&#27979;&#21040;&#26410;&#26469;&#30340;&#24322;&#24120;&#12290;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#22810;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;17&#20010;&#22522;&#20934;&#32447;&#21644;3&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#35268;&#21017;&#21644;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is an important field that aims to identify unexpected patterns or data points, and it is closely related to many real-world problems, particularly to applications in finance, manufacturing, cyber security, and so on. While anomaly detection has been studied extensively in various fields, detecting future anomalies before they occur remains an unexplored territory. In this paper, we present a novel type of anomaly detection, called \emph{\textbf{P}recursor-of-\textbf{A}nomaly} (PoA) detection. Unlike conventional anomaly detection, which focuses on determining whether a given time series observation is an anomaly or not, PoA detection aims to detect future anomalies before they happen. To solve both problems at the same time, we present a neural controlled differential equation-based neural network and its multi-task learning algorithm. We conduct experiments using 17 baselines and 3 datasets, including regular and irregular time series, and demonstrate that our prese
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#40065;&#26834;&#24615;&#20013;&#30340;&#21512;&#20316;&#19982;&#31454;&#20105;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#23545;&#25163;&#39044;&#31639;&#26469;&#36991;&#20813;&#29609;&#23478;&#20027;&#23548;&#30340;&#26032;&#26694;&#26550;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#30446;&#26631;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15482</link><description>&lt;p&gt;
&#21512;&#20316;&#36824;&#26159;&#31454;&#20105;&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#39044;&#31639;&#36991;&#20813;&#22810;&#30446;&#26631;&#40065;&#26834;&#24615;&#20013;&#30340;&#29609;&#23478;&#20027;&#23548;
&lt;/p&gt;
&lt;p&gt;
Cooperation or Competition: Avoiding Player Domination for Multi-Target Robustness via Adaptive Budgets. (arXiv:2306.15482v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#40065;&#26834;&#24615;&#20013;&#30340;&#21512;&#20316;&#19982;&#31454;&#20105;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#23545;&#25163;&#39044;&#31639;&#26469;&#36991;&#20813;&#29609;&#23478;&#20027;&#23548;&#30340;&#26032;&#26694;&#26550;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#30446;&#26631;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#30340;&#36827;&#23637;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#20197;&#32463;&#39564;&#21644;&#21487;&#35777;&#26126;&#30340;&#26041;&#24335;&#35757;&#32451;&#40065;&#26834;&#32593;&#32476;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#33021;&#38450;&#24481;&#19968;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65292;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#38450;&#24481;&#22810;&#31181;&#25915;&#20987;&#26041;&#38754;&#26377;&#25152;&#36827;&#23637;&#12290;&#26412;&#25991;&#23558;&#22810;&#30446;&#26631;&#40065;&#26834;&#24615;&#38382;&#39064;&#35270;&#20026;&#19968;&#20010;&#21338;&#24328;&#36807;&#31243;&#65292;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#19981;&#21516;&#30340;&#29609;&#23478;&#65288;&#23545;&#25163;&#65289;&#36890;&#36807;&#21327;&#21830;&#22312;&#21442;&#25968;&#26356;&#26032;&#30340;&#26041;&#21521;&#19978;&#36798;&#25104;&#19968;&#33268;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#29609;&#23478;&#20027;&#23548;&#30340;&#29616;&#35937;&#65292;&#22312;&#36825;&#20010;&#21338;&#24328;&#20013;&#65292;&#29616;&#26377;&#30340;&#26368;&#22823;&#20540;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#22914;MAX&#21644;MSD&#65292;&#26080;&#27861;&#25910;&#25947;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#25972;&#19981;&#21516;&#23545;&#25163;&#30340;&#39044;&#31639;&#26469;&#36991;&#20813;&#20219;&#20309;&#29609;&#23478;&#30340;&#20027;&#23548;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#29616;&#26377;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#30446;&#26631;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite incredible advances, deep learning has been shown to be susceptible to adversarial attacks. Numerous approaches have been proposed to train robust networks both empirically and certifiably. However, most of them defend against only a single type of attack, while recent work takes steps forward in defending against multiple attacks. In this paper, to understand multi-target robustness, we view this problem as a bargaining game in which different players (adversaries) negotiate to reach an agreement on a joint direction of parameter updating. We identify a phenomenon named player domination in the bargaining game, namely that the existing max-based approaches, such as MAX and MSD, do not converge. Based on our theoretical analysis, we design a novel framework that adjusts the budgets of different adversaries to avoid any player dominance. Experiments on standard benchmarks show that employing the proposed framework to the existing approaches significantly advances multi-target ro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#20195;&#29702;&#23398;&#20064;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#26174;&#24335;&#23398;&#20064;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#31034;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#31867;&#21035;&#40065;&#26834;&#25200;&#21160;&#29983;&#25104;&#20195;&#34920;&#31867;&#21035;&#30340;&#40065;&#26834;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#40065;&#26834;&#20195;&#29702;&#26469;&#23398;&#20064;&#23545;&#25239;&#24615;&#40065;&#26834;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.15457</link><description>&lt;p&gt;
&#40065;&#26834;&#20195;&#29702;&#23398;&#20064;: &#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Proxy: Improving Adversarial Robustness by Robust Proxy Learning. (arXiv:2306.15457v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#20195;&#29702;&#23398;&#20064;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#26174;&#24335;&#23398;&#20064;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#31034;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#31867;&#21035;&#40065;&#26834;&#25200;&#21160;&#29983;&#25104;&#20195;&#34920;&#31867;&#21035;&#30340;&#40065;&#26834;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#40065;&#26834;&#20195;&#29702;&#26469;&#23398;&#20064;&#23545;&#25239;&#24615;&#40065;&#26834;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#32780;&#23481;&#26131;&#34987;&#30772;&#22351;&#65292;&#20026;&#20102;&#20943;&#36731;&#23545;&#25239;&#24615;&#30340;&#33030;&#24369;&#24615;&#65292;&#24456;&#22810;&#38450;&#24481;&#31639;&#27861;&#34987;&#25552;&#20986;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#35768;&#22810;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#23545;&#21028;&#21035;&#29305;&#24449;&#36827;&#34892;&#26356;&#22810;&#30452;&#25509;&#30417;&#30563;&#26469;&#22686;&#24378;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#23545;&#25239;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#31216;&#20026;&#40065;&#26834;&#20195;&#29702;&#23398;&#20064;&#12290;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#40065;&#26834;&#20195;&#29702;&#26174;&#24335;&#23398;&#20064;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#31867;&#21035;&#40065;&#26834;&#25200;&#21160;&#26469;&#29983;&#25104;&#20195;&#34920;&#31867;&#21035;&#30340;&#40065;&#26834;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#31867;&#21035;&#20195;&#34920;&#29305;&#24449;&#20316;&#20026;&#40065;&#26834;&#20195;&#29702;&#12290;&#36890;&#36807;&#31867;&#21035;&#40065;&#26834;&#29305;&#24449;&#65292;&#27169;&#22411;&#26174;&#24335;&#22320;&#23398;&#20064;&#23545;&#25239;&#24615;&#40065;&#26834;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been widely known that deep neural networks are highly vulnerable and easily broken by adversarial attacks. To mitigate the adversarial vulnerability, many defense algorithms have been proposed. Recently, to improve adversarial robustness, many works try to enhance feature representation by imposing more direct supervision on the discriminative feature. However, existing approaches lack an understanding of learning adversarially robust feature representation. In this paper, we propose a novel training framework called Robust Proxy Learning. In the proposed method, the model explicitly learns robust feature representations with robust proxies. To this end, firstly, we demonstrate that we can generate class-representative robust features by adding class-wise robust perturbations. Then, we use the class representative features as robust proxies. With the class-wise robust features, the model explicitly learns adversarially robust features through the proposed robust proxy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27880;&#20837;&#22686;&#24378;&#20449;&#21495;&#26469;&#25552;&#39640;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;&#22806;&#37096;&#20449;&#21495;&#32780;&#19981;&#26159;&#27169;&#22411;&#21442;&#25968;&#26469;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#21644;&#33258;&#28982;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15451</link><description>&lt;p&gt;
&#36890;&#36807;&#27880;&#20837;&#22686;&#24378;&#20449;&#21495;&#26469;&#25512;&#36827;&#23545;&#25239;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Advancing Adversarial Training by Injecting Booster Signal. (arXiv:2306.15451v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27880;&#20837;&#22686;&#24378;&#20449;&#21495;&#26469;&#25552;&#39640;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;&#22806;&#37096;&#20449;&#21495;&#32780;&#19981;&#26159;&#27169;&#22411;&#21442;&#25968;&#26469;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#21644;&#33258;&#28982;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23545;&#20110;&#23545;&#25239;&#25915;&#20987;&#38750;&#24120;&#33030;&#24369;&#12290;&#20026;&#20102;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#38450;&#24481;&#31574;&#30053;&#65292;&#20854;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#34987;&#35777;&#26126;&#26159;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#26377;&#26102;&#20250;&#25439;&#23475;&#33258;&#28982;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#24037;&#20316;&#38598;&#20013;&#22312;&#20248;&#21270;&#27169;&#22411;&#21442;&#25968;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#20449;&#21495;&#32780;&#19981;&#26159;&#27169;&#22411;&#21442;&#25968;&#26469;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#27880;&#20837;&#20102;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;&#36890;&#29992;&#22806;&#37096;&#20449;&#21495;&#65292;&#31216;&#20026;&#22686;&#24378;&#20449;&#21495;&#65292;&#35813;&#20449;&#21495;&#34987;&#27880;&#20837;&#21040;&#19982;&#21407;&#22987;&#20869;&#23481;&#19981;&#37325;&#21472;&#30340;&#22270;&#20687;&#22806;&#37096;&#12290;&#28982;&#21518;&#65292;&#23427;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#21644;&#33258;&#28982;&#20934;&#30830;&#24615;&#12290;&#22686;&#24378;&#20449;&#21495;&#19982;&#27169;&#22411;&#21442;&#25968;&#24182;&#34892;&#36880;&#27493;&#36827;&#34892;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarial attacks. To defend against adversarial attacks, many defense strategies have been proposed, among which adversarial training has been demonstrated to be the most effective strategy. However, it has been known that adversarial training sometimes hurts natural accuracy. Then, many works focus on optimizing model parameters to handle the problem. Different from the previous approaches, in this paper, we propose a new approach to improve the adversarial robustness by using an external signal rather than model parameters. In the proposed method, a well-optimized universal external signal called a booster signal is injected into the outside of the image which does not overlap with the original content. Then, it boosts both adversarial robustness and natural accuracy. The booster signal is optimized in parallel to model parameters step by step collaboratively. Experimental results show that th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22635;&#20805;&#22240;&#26524;&#27169;&#26495;&#26469;&#29983;&#25104;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20043;&#21069;&#35780;&#20272;&#32467;&#26524;&#19981;&#19968;&#33268;&#21644;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23384;&#22312;&#30097;&#34385;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#20182;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31038;&#20132;&#25512;&#29702;&#22522;&#20934;&#65288;BigToM&#65289;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#21442;&#19982;&#32773;&#35780;&#20215;&#36825;&#20010;&#22522;&#20934;&#30340;&#36136;&#37327;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.15448</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20132;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Understanding Social Reasoning in Language Models with Language Models. (arXiv:2306.15448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15448
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22635;&#20805;&#22240;&#26524;&#27169;&#26495;&#26469;&#29983;&#25104;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20043;&#21069;&#35780;&#20272;&#32467;&#26524;&#19981;&#19968;&#33268;&#21644;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23384;&#22312;&#30097;&#34385;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#20182;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31038;&#20132;&#25512;&#29702;&#22522;&#20934;&#65288;BigToM&#65289;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#21442;&#19982;&#32773;&#35780;&#20215;&#36825;&#20010;&#22522;&#20934;&#30340;&#36136;&#37327;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20102;&#35299;&#23427;&#20204;&#29702;&#35299;&#20154;&#31867;&#24515;&#29702;&#29366;&#24577;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#26377;&#25928;&#30340;&#20132;&#20114;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#26377;&#20154;&#23581;&#35797;&#35780;&#20272;LLM&#30340;&#29702;&#35770;&#24515;&#26234;&#65288;ToM&#65289;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#31867;ToM&#30340;&#19968;&#33268;&#31243;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25506;&#32034;&#20027;&#39064;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#23384;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#20043;&#21069;&#35780;&#20272;&#32467;&#26524;&#19981;&#19968;&#33268;&#65292;&#65288;2&#65289;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23384;&#22312;&#30097;&#34385;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22635;&#20805;&#22240;&#26524;&#27169;&#26495;&#26469;&#29983;&#25104;&#19982;LLM&#30340;&#35780;&#20272;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20026;LLM&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31038;&#20132;&#25512;&#29702;&#22522;&#20934;&#65288;BigToM&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;25&#20010;&#25511;&#21046;&#21644;5000&#20010;&#27169;&#22411;&#20889;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#20043;&#21069;&#20247;&#21253;&#35780;&#20272;&#30456;&#27604;&#65292;&#20154;&#31867;&#21442;&#19982;&#32773;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#30340;&#36136;&#37327;&#35780;&#20215;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evalua
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#29992;&#25143;&#26500;&#24314;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#26159;&#21542;&#20173;&#33021;&#20445;&#25345;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25915;&#20987;&#25163;&#27861;&#19981;&#36275;&#20197;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2306.15447</link><description>&lt;p&gt;
&#23545;&#40784;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#23545;&#25239;&#23545;&#40784;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are aligned neural networks adversarially aligned?. (arXiv:2306.15447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15447
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#29992;&#25143;&#26500;&#24314;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#26159;&#21542;&#20173;&#33021;&#20445;&#25345;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25915;&#20987;&#25163;&#27861;&#19981;&#36275;&#20197;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29616;&#22312;&#34987;&#35843;&#25972;&#20026;&#19982;&#20854;&#21019;&#24314;&#32773;&#30340;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#65292;&#21363;"&#26377;&#30410;&#19988;&#26080;&#23475;"&#12290;&#36825;&#20123;&#27169;&#22411;&#24212;&#35813;&#23545;&#29992;&#25143;&#30340;&#38382;&#39064;&#32473;&#20986;&#26377;&#30410;&#30340;&#22238;&#31572;&#65292;&#20294;&#25298;&#32477;&#22238;&#31572;&#21487;&#33021;&#20250;&#36896;&#25104;&#20260;&#23475;&#30340;&#35831;&#27714;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#32469;&#36807;&#23545;&#40784;&#23581;&#35797;&#30340;&#36755;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19982;&#26500;&#36896;&#26368;&#22351;&#24773;&#20917;&#36755;&#20837;&#65288;&#23545;&#25239;&#24615;&#26679;&#26412;&#65289;&#30340;&#23545;&#25239;&#29992;&#25143;&#20132;&#20114;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20445;&#25345;&#22810;&#22823;&#31243;&#24230;&#30340;&#23545;&#40784;&#12290;&#36825;&#20123;&#36755;&#20837;&#34987;&#35774;&#35745;&#25104;&#23548;&#33268;&#27169;&#22411;&#21457;&#20986;&#26412;&#24212;&#31105;&#27490;&#30340;&#26377;&#23475;&#20869;&#23481;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20248;&#21270;&#25915;&#20987;&#25163;&#27861;&#22312;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#26041;&#38754;&#30340;&#19981;&#36275;&#20043;&#22788;&#65306;&#21363;&#20351;&#22312;&#24403;&#21069;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25915;&#20987;&#22833;&#36133;&#26102;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#25915;&#20987;&#30340;&#22833;&#36133;&#19981;&#24212;&#34987;&#35270;&#20026;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#20173;&#28982;&#20445;&#25345;&#23545;&#40784;&#30340;&#35777;&#26126;&#12290;&#20294;&#26159;&#36817;&#26399;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36235;&#21183;&#26159;&#22810;&#27169;&#24577;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are now tuned to align with the goals of their creators, namely to be "helpful and harmless." These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs.  However the recent trend in large-scale ML models is multim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25299;&#25169;&#35282;&#24230;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#23433;&#20840;&#24615;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#32986;&#24615;&#36136;&#21644;&#24320;&#26144;&#23556;&#24615;&#36136;&#24314;&#31435;&#20102;&#36755;&#20837;&#38598;&#21644;&#36755;&#20986;&#38598;&#20043;&#38388;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#23433;&#20840;&#39564;&#35777;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15403</link><description>&lt;p&gt;
&#20174;&#25299;&#25169;&#35282;&#24230;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Verifying Safety of Neural Networks from Topological Perspectives. (arXiv:2306.15403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25299;&#25169;&#35282;&#24230;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#23433;&#20840;&#24615;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#32986;&#24615;&#36136;&#21644;&#24320;&#26144;&#23556;&#24615;&#36136;&#24314;&#31435;&#20102;&#36755;&#20837;&#38598;&#21644;&#36755;&#20986;&#38598;&#20043;&#38388;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#23433;&#20840;&#39564;&#35777;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#65292;&#28982;&#32780;&#23427;&#20204;&#26131;&#21463;&#25439;&#24182;&#19988;&#24120;&#24120;&#34920;&#29616;&#19981;&#33391;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#20043;&#21069;&#65292;&#23427;&#20204;&#30340;&#34892;&#20026;&#24212;&#35813;&#32463;&#36807;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25299;&#25169;&#35282;&#24230;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#23433;&#20840;&#39564;&#35777;&#38382;&#39064;&#30340;&#38598;&#36793;&#30028;&#21487;&#36798;&#24615;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#20855;&#26377;&#36755;&#20837;&#38598;&#21644;&#23433;&#20840;&#38598;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23433;&#20840;&#39564;&#35777;&#38382;&#39064;&#26159;&#30830;&#23450;&#25152;&#26377;&#26469;&#33258;&#36755;&#20837;&#38598;&#30340;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#26159;&#21542;&#33853;&#22312;&#23433;&#20840;&#38598;&#20013;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#20027;&#35201;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#32986;&#24615;&#36136;&#21644;&#24320;&#26144;&#23556;&#24615;&#36136;&#65292;&#36825;&#20123;&#24615;&#36136;&#22312;&#36755;&#20837;&#38598;&#30340;&#36793;&#30028;&#21644;&#36755;&#20986;&#38598;&#30340;&#36793;&#30028;&#20043;&#38388;&#24314;&#31435;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#21033;&#29992;&#36825;&#20004;&#20010;&#24615;&#36136;&#21487;&#20197;&#36890;&#36807;&#25552;&#21462;&#36755;&#20837;&#38598;&#30340;&#23376;&#38598;&#32780;&#19981;&#26159;&#25972;&#20010;&#36755;&#20837;&#38598;&#26469;&#36827;&#34892;&#21487;&#36798;&#24615;&#35745;&#31639;&#65292;&#20174;&#32780;&#25511;&#21046;&#21487;&#36798;&#24615;&#20998;&#26512;&#20013;&#30340;&#21253;&#35065;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) are increasingly applied in safety-critical systems such as autonomous vehicles. However, they are fragile and are often ill-behaved. Consequently, their behaviors should undergo rigorous guarantees before deployment in practice. In this paper, we propose a set-boundary reachability method to investigate the safety verification problem of NNs from a topological perspective. Given an NN with an input set and a safe set, the safety verification problem is to determine whether all outputs of the NN resulting from the input set fall within the safe set. In our method, the homeomorphism property and the open map property of NNs are mainly exploited, which establish rigorous guarantees between the boundaries of the input set and the boundaries of the output set. The exploitation of these two properties facilitates reachability computations via extracting subsets of the input set rather than the entire input set, thus controlling the wrapping effect in reachability analy
&lt;/p&gt;</description></item><item><title>AI&#22312;&#21327;&#21516;&#24037;&#20316;&#20013;&#24212;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25509;&#21463;&#24615;&#30340;&#35201;&#27714;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#24320;&#21457;&#32773;&#38656;&#35201;&#20102;&#35299;&#27169;&#22411;&#20869;&#37096;&#36816;&#20316;&#65292;&#26368;&#32456;&#29992;&#25143;&#38656;&#35201;&#20102;&#35299;AI&#30340;&#32467;&#26524;&#25110;&#34892;&#20026;&#12290;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#22240;&#19978;&#19979;&#25991;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#35748;&#30693;&#36164;&#28304;&#32780;&#26377;&#25152;&#24046;&#24322;&#12290;&#25509;&#21463;AI&#31995;&#32479;&#21462;&#20915;&#20110;&#25552;&#20379;&#30340;&#20449;&#24687;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.15394</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21327;&#21516;&#24037;&#20316;&#20013;&#24212;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25509;&#21463;&#24615;&#30340;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Requirements for Explainability and Acceptance of Artificial Intelligence in Collaborative Work. (arXiv:2306.15394v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15394
&lt;/p&gt;
&lt;p&gt;
AI&#22312;&#21327;&#21516;&#24037;&#20316;&#20013;&#24212;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25509;&#21463;&#24615;&#30340;&#35201;&#27714;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#24320;&#21457;&#32773;&#38656;&#35201;&#20102;&#35299;&#27169;&#22411;&#20869;&#37096;&#36816;&#20316;&#65292;&#26368;&#32456;&#29992;&#25143;&#38656;&#35201;&#20102;&#35299;AI&#30340;&#32467;&#26524;&#25110;&#34892;&#20026;&#12290;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#22240;&#19978;&#19979;&#25991;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#35748;&#30693;&#36164;&#28304;&#32780;&#26377;&#25152;&#24046;&#24322;&#12290;&#25509;&#21463;AI&#31995;&#32479;&#21462;&#20915;&#20110;&#25552;&#20379;&#30340;&#20449;&#24687;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#35832;&#22914;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#31561;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#38656;&#35201;&#34987;&#20154;&#31867;&#20449;&#20219;&#21644;&#25509;&#21463;&#30340;&#23454;&#29992;&#39640;&#25928;&#19988;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#23545;&#20851;&#20110;AI&#21487;&#35299;&#37322;&#24615;&#21644;&#25509;&#21463;&#24615;&#35201;&#27714;&#30340;236&#31687;&#25991;&#31456;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#25991;&#29486;&#20998;&#26512;&#12290;&#32467;&#26524;&#21253;&#25324;&#20851;&#20110;&#20154;&#20204;&#38656;&#35201;&#30693;&#36947;&#30340;&#20351;AI&#21487;&#35299;&#37322;&#30340;&#20449;&#24687;&#12289;&#25509;&#21463;AI&#25152;&#38656;&#30340;&#20449;&#24687;&#20197;&#21450;&#20419;&#36827;&#23545;AI&#20449;&#20219;&#30340;&#34920;&#31034;&#21644;&#20132;&#20114;&#26041;&#27861;&#30340;48&#31687;&#25991;&#31456;&#30340;&#32508;&#21512;&#22238;&#39038;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20027;&#35201;&#30340;&#29992;&#25143;&#32676;&#20307;&#26159;&#38656;&#35201;&#20102;&#35299;&#27169;&#22411;&#20869;&#37096;&#36816;&#20316;&#20449;&#24687;&#30340;&#24320;&#21457;&#32773;&#21644;&#38656;&#35201;&#20102;&#35299;AI&#32467;&#26524;&#25110;&#34892;&#20026;&#20449;&#24687;&#30340;&#26368;&#32456;&#29992;&#25143;&#12290;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#22312;&#20855;&#20307;&#24615;&#12289;&#22797;&#26434;&#24615;&#21644;&#32039;&#36843;&#24615;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#24517;&#39035;&#32771;&#34385;&#21040;&#19978;&#19979;&#25991;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#29992;&#25143;&#30340;&#35748;&#30693;&#36164;&#28304;&#12290;AI&#31995;&#32479;&#30340;&#25509;&#21463;&#31243;&#24230;&#21462;&#20915;&#20110;&#20449;&#24687;&#30340;&#25552;&#20379;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing prevalence of Artificial Intelligence (AI) in safety-critical contexts such as air-traffic control leads to systems that are practical and efficient, and to some extent explainable to humans to be trusted and accepted. The present structured literature analysis examines n = 236 articles on the requirements for the explainability and acceptance of AI. Results include a comprehensive review of n = 48 articles on information people need to perceive an AI as explainable, the information needed to accept an AI, and representation and interaction methods promoting trust in an AI. Results indicate that the two main groups of users are developers who require information about the internal operations of the model and end users who require information about AI results or behavior. Users' information needs vary in specificity, complexity, and urgency and must consider context, domain knowledge, and the user's cognitive resources. The acceptance of AI systems depends on information 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#20013;&#25968;&#25454;&#38598;&#36136;&#37327;&#35780;&#20272;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#36136;&#37327;&#23545;&#27169;&#22411;&#35757;&#32451;&#21644;&#24615;&#33021;&#30340;&#28145;&#36828;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.15392</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#26641;&#29305;&#24449;&#22312;&#33258;&#21160;&#32534;&#30721;&#22120;&#22788;&#29702;&#31354;&#38388;&#20013;&#35780;&#20272;&#25968;&#25454;&#38598;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Assessing Dataset Quality Through Decision Tree Characteristics in Autoencoder-Processed Spaces. (arXiv:2306.15392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#20013;&#25968;&#25454;&#38598;&#36136;&#37327;&#35780;&#20272;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#36136;&#37327;&#23545;&#27169;&#22411;&#35757;&#32451;&#21644;&#24615;&#33021;&#30340;&#28145;&#36828;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#20013;&#25968;&#25454;&#38598;&#36136;&#37327;&#35780;&#20272;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#20511;&#21161;&#20061;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#32463;&#36807;&#20998;&#31867;&#20219;&#21153;&#38656;&#27714;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#36136;&#37327;&#23545;&#27169;&#22411;&#35757;&#32451;&#21644;&#24615;&#33021;&#30340;&#28145;&#36828;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#29992;&#20110;&#34920;&#31034;&#20855;&#20307;&#30340;&#25968;&#25454;&#26465;&#20214; - &#19968;&#20010;&#26368;&#22823;&#21270;&#29109;&#65292;&#21478;&#19968;&#20010;&#23637;&#31034;&#39640;&#20887;&#20313;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#36866;&#24403;&#30340;&#29305;&#24449;&#36873;&#25321;&#12289;&#20805;&#36275;&#30340;&#25968;&#25454;&#37327;&#21644;&#25968;&#25454;&#36136;&#37327;&#22312;&#23454;&#29616;&#39640;&#24615;&#33021;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#35780;&#20272;&#25163;&#22836;&#30340;&#25968;&#25454;&#38598;&#26159;&#21542;&#36275;&#22815;&#19988;&#20855;&#22791;&#29305;&#23450;&#20219;&#21153;&#25152;&#38656;&#30340;&#36136;&#37327;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#25968;&#25454;&#35780;&#20272;&#23454;&#36341;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#24182;&#26377;&#21161;&#20110;&#24320;&#21457;&#26356;&#20934;&#30830;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we delve into the critical aspect of dataset quality assessment in machine learning classification tasks. Leveraging a variety of nine distinct datasets, each crafted for classification tasks with varying complexity levels, we illustrate the profound impact of dataset quality on model training and performance. We further introduce two additional datasets designed to represent specific data conditions - one maximizing entropy and the other demonstrating high redundancy. Our findings underscore the importance of appropriate feature selection, adequate data volume, and data quality in achieving high-performing machine learning models. To aid researchers and practitioners, we propose a comprehensive framework for dataset quality assessment, which can help evaluate if the dataset at hand is sufficient and of the required quality for specific tasks. This research offers valuable insights into data assessment practices, contributing to the development of more accurate and robus
&lt;/p&gt;</description></item><item><title>DCP-NAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#24322;&#24615;&#23376;&#29238;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23454;&#20540;&#27169;&#22411;&#30340;&#30417;&#30563;&#19979;&#25628;&#32034;1&#20301;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2306.15390</link><description>&lt;p&gt;
DCP-NAS: &#38024;&#23545;1&#20301;CNN&#30340;&#24046;&#24322;&#24615;&#23376;&#29238;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
DCP-NAS: Discrepant Child-Parent Neural Architecture Search for 1-bit CNNs. (arXiv:2306.15390v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15390
&lt;/p&gt;
&lt;p&gt;
DCP-NAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#24322;&#24615;&#23376;&#29238;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23454;&#20540;&#27169;&#22411;&#30340;&#30417;&#30563;&#19979;&#25628;&#32034;1&#20301;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#36890;&#36807;&#29983;&#25104;&#24212;&#29992;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#34987;&#35777;&#26126;&#26159;&#35768;&#22810;&#20219;&#21153;&#20013;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#20173;&#38754;&#20020;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#30340;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20855;&#26377;&#20108;&#36827;&#21046;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;1&#20301;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#23637;&#29616;&#20102;&#28508;&#21147;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#21033;&#29992;1&#20301;CNN&#26469;&#20943;&#23569;NAS&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#30001;&#20110;&#28041;&#21450;&#26356;&#22797;&#26434;&#30340;&#36807;&#31243;&#65292;&#25628;&#32034;1&#20301;CNN&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24046;&#24322;&#24615;&#23376;&#29238;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(DCP-NAS)&#26469;&#39640;&#25928;&#25628;&#32034;1&#20301;CNN&#65292;&#22522;&#20110;&#22312;&#23454;&#20540;&#27169;&#22411;(&#29238;&#27169;&#22411;)&#30340;&#30417;&#30563;&#19979;&#25628;&#32034;1&#20301;&#27169;&#22411;(&#23376;&#27169;&#22411;)&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#29238;&#27169;&#22411;&#35745;&#31639;&#20999;&#32447;&#26041;&#21521;&#65292;&#22522;&#20110;&#20999;&#32447;&#26041;&#21521;&#36827;&#34892;&#20999;&#32447;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural architecture search (NAS) proves to be among the effective approaches for many tasks by generating an application-adaptive neural architecture, which is still challenged by high computational cost and memory consumption. At the same time, 1-bit convolutional neural networks (CNNs) with binary weights and activations show their potential for resource-limited embedded devices. One natural approach is to use 1-bit CNNs to reduce the computation and memory cost of NAS by taking advantage of the strengths of each in a unified framework, while searching the 1-bit CNNs is more challenging due to the more complicated processes involved. In this paper, we introduce Discrepant Child-Parent Neural Architecture Search (DCP-NAS) to efficiently search 1-bit CNNs, based on a new framework of searching the 1-bit model (Child) under the supervision of a real-valued model (Parent). Particularly, we first utilize a Parent model to calculate a tangent direction, based on which the tangent propagati
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#35782;&#21035;&#33609;&#33647;&#19982;&#33647;&#29289;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20351;&#33647;&#25151;&#31038;&#21306;&#22312;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#20013;&#33021;&#22815;&#20570;&#20986;&#26356;&#22909;&#12289;&#26356;&#20934;&#30830;&#30340;&#27835;&#30103;&#20915;&#31574;&#65292;&#24182;&#20943;&#36731;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#33391;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2306.15365</link><description>&lt;p&gt;
&#33609;&#33647;&#19982;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65306;&#32508;&#21512;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Herb-Drug Interactions: A Holistic Decision Support System in Healthcare. (arXiv:2306.15365v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15365
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#35782;&#21035;&#33609;&#33647;&#19982;&#33647;&#29289;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20351;&#33647;&#25151;&#31038;&#21306;&#22312;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#20013;&#33021;&#22815;&#20570;&#20986;&#26356;&#22909;&#12289;&#26356;&#20934;&#30830;&#30340;&#27835;&#30103;&#20915;&#31574;&#65292;&#24182;&#20943;&#36731;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#33391;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22791;&#21463;&#24191;&#27867;&#24212;&#29992;&#30340;&#34917;&#20805;&#19982;&#26367;&#20195;&#21307;&#23398;&#24120;&#19982;&#20256;&#32479;&#33647;&#29289;&#21516;&#26102;&#20351;&#29992;&#65292;&#20174;&#32780;&#23548;&#33268;&#33647;&#29289;&#19981;&#33391;&#21453;&#24212;&#29978;&#33267;&#26377;&#26102;&#33268;&#21629;&#12290;&#27492;&#22806;&#65292;&#33609;&#33647;&#19982;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#30340;&#24040;&#22823;&#21487;&#33021;&#24615;&#20351;&#24471;&#20581;&#24247;&#19987;&#19994;&#20154;&#21592;&#38590;&#20197;&#35760;&#20303;&#25110;&#25163;&#21160;&#25628;&#32034;&#25968;&#25454;&#24211;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#20449;&#24687;&#12290;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#22312;&#24739;&#32773;&#25252;&#29702;&#20013;&#20570;&#20986;&#35786;&#26029;&#21644;&#27835;&#30103;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21407;&#21019;&#30340;&#28151;&#21512;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#35782;&#21035;&#33609;&#33647;&#19982;&#33647;&#29289;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#35782;&#21035;&#26032;&#30340;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#12290;&#23558;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#21152;&#24378;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#20351;&#29992;&#30340;&#20856;&#22411;&#35268;&#21017;&#24341;&#25806;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#25552;&#20986;&#30340;&#31995;&#32479;&#65292;&#33647;&#25151;&#31038;&#21306;&#12289;&#20154;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#20013;&#30340;&#31532;&#19968;&#32447;&#25509;&#35302;&#28857;&#65292;&#23558;&#33021;&#22815;&#20570;&#20986;&#26356;&#22909;&#12289;&#26356;&#20934;&#30830;&#30340;&#27835;&#30103;&#20915;&#31574;&#65292;&#24182;&#20943;&#36731;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#33391;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complementary and alternative medicine are commonly used concomitantly with conventional medications leading to adverse drug reactions and even fatality in some cases. Furthermore, the vast possibility of herb-drug interactions prevents health professionals from remembering or manually searching them in a database. Decision support systems are a powerful tool that can be used to assist clinicians in making diagnostic and therapeutic decisions in patient care. Therefore, an original and hybrid decision support system was designed to identify herb-drug interactions, applying artificial intelligence techniques to identify new possible interactions. Different machine learning models will be used to strengthen the typical rules engine used in these cases. Thus, using the proposed system, the pharmacy community, people's first line of contact within the Healthcare System, will be able to make better and more accurate therapeutic decisions and mitigate possible adverse events.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20197;&#22320;&#26631;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#36827;&#34892;&#20877;&#35752;&#35770;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#35813;&#26041;&#27861;&#20013;&#20351;&#29992;&#21021;&#22987;&#29366;&#24577;&#30340;&#22320;&#26631;&#27809;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.15362</link><description>&lt;p&gt;
&#20197;&#22320;&#26631;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#35782;&#21035;&#20877;&#35752;&#35770;&#65306;&#20351;&#29992;&#21021;&#22987;&#29366;&#24577;&#30340;&#22320;&#26631;&#26159;&#21542;&#26377;&#24847;&#20041;&#65311;
&lt;/p&gt;
&lt;p&gt;
Planning Landmark Based Goal Recognition Revisited: Does Using Initial State Landmarks Make Sense?. (arXiv:2306.15362v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20197;&#22320;&#26631;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#36827;&#34892;&#20877;&#35752;&#35770;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#35813;&#26041;&#27861;&#20013;&#20351;&#29992;&#21021;&#22987;&#29366;&#24577;&#30340;&#22320;&#26631;&#27809;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#35782;&#21035;&#26159;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#65288;&#22914;&#26222;&#36866;&#35745;&#31639;&#12289;&#20837;&#20405;&#26816;&#27979;&#12289;&#30005;&#33041;&#28216;&#25103;&#31561;&#65289;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#30446;&#26631;&#35782;&#21035;&#31639;&#27861;&#38656;&#35201;&#33021;&#22815;&#23613;&#24555;&#22320;&#35782;&#21035;&#35266;&#23519;&#21040;&#30340;&#20027;&#20307;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#35745;&#21010;&#35782;&#21035;&#21363;&#35745;&#21010;&#20013;&#65292;&#35768;&#22810;&#26089;&#26399;&#26041;&#27861;&#38656;&#35201;&#30456;&#24403;&#22823;&#37327;&#30340;&#35745;&#31639;&#26102;&#38388;&#26469;&#35745;&#31639;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;Pereira&#31561;&#20154;&#26368;&#36817;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21010;&#22320;&#26631;&#30340;&#26041;&#27861;&#65292;&#23427;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#35201;&#39640;&#24471;&#22810;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;Pereira&#31561;&#20154;&#25552;&#20986;&#30340;&#26041;&#27861;&#19968;&#26679;&#65292;&#23427;&#20063;&#20351;&#29992;&#20102;&#29712;&#30862;&#30340;&#22320;&#26631;&#65288;&#21363;&#65292;&#21021;&#22987;&#29366;&#24577;&#21644;&#30446;&#26631;&#25551;&#36848;&#20013;&#30340;&#20107;&#23454;&#22312;&#23450;&#20041;&#19978;&#23601;&#26159;&#22320;&#26631;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22522;&#20110;&#35268;&#21010;&#22320;&#26631;&#30340;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#20013;&#20351;&#29992;&#21021;&#22987;&#29366;&#24577;&#30340;&#22320;&#26631;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#22909;&#22788;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#30465;&#30053;&#25481;&#21021;&#22987;&#29366;&#24577;&#30340;&#22320;&#26631;&#30340;&#26041;&#27861;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal recognition is an important problem in many application domains (e.g., pervasive computing, intrusion detection, computer games, etc.). In many application scenarios, it is important that goal recognition algorithms can recognize goals of an observed agent as fast as possible. However, many early approaches in the area of Plan Recognition As Planning, require quite large amounts of computation time to calculate a solution. Mainly to address this issue, recently, Pereira et al. developed an approach that is based on planning landmarks and is much more computationally efficient than previous approaches. However, the approach, as proposed by Pereira et al., also uses trivial landmarks (i.e., facts that are part of the initial state and goal description are landmarks by definition). In this paper, we show that it does not provide any benefit to use landmarks that are part of the initial state in a planning landmark based goal recognition approach. The empirical results show that omitt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SSC-RS&#32593;&#32476;&#65292;&#36890;&#36807;&#34920;&#31034;&#20998;&#31163;&#21644;BEV&#34701;&#21512;&#26469;&#35299;&#20915;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#38382;&#39064;&#12290;&#36890;&#36807;&#28145;&#24230;&#30417;&#30563;&#23558;&#35821;&#20041;&#21644;&#20960;&#20309;&#34920;&#31034;&#30340;&#23398;&#20064;&#36807;&#31243;&#26126;&#30830;&#22320;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#20998;&#25903;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#34920;&#31034;&#34701;&#21512;&#27169;&#22359;&#26469;&#32858;&#21512;&#22810;&#23610;&#24230;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.15349</link><description>&lt;p&gt;
SSC-RS: &#36890;&#36807;&#34920;&#31034;&#20998;&#31163;&#19982;BEV&#34701;&#21512;&#25552;&#21319;LiDAR&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
SSC-RS: Elevate LiDAR Semantic Scene Completion with Representation Separation and BEV Fusion. (arXiv:2306.15349v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SSC-RS&#32593;&#32476;&#65292;&#36890;&#36807;&#34920;&#31034;&#20998;&#31163;&#21644;BEV&#34701;&#21512;&#26469;&#35299;&#20915;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#38382;&#39064;&#12290;&#36890;&#36807;&#28145;&#24230;&#30417;&#30563;&#23558;&#35821;&#20041;&#21644;&#20960;&#20309;&#34920;&#31034;&#30340;&#23398;&#20064;&#36807;&#31243;&#26126;&#30830;&#22320;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#20998;&#25903;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#34920;&#31034;&#34701;&#21512;&#27169;&#22359;&#26469;&#32858;&#21512;&#22810;&#23610;&#24230;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;(SSC)&#32852;&#21512;&#39044;&#27979;&#25972;&#20010;3D&#22330;&#26223;&#30340;&#35821;&#20041;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;SSC&#22312;&#20998;&#21106;&#20013;&#21033;&#29992;&#35821;&#20041;&#19978;&#19979;&#25991;&#24050;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#19982;&#22330;&#26223;&#34917;&#20840;&#20013;&#30340;&#20960;&#20309;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#24453;&#25506;&#32034;&#12290;&#26412;&#25991;&#20174;&#34920;&#31034;&#20998;&#31163;&#21644;BEV&#34701;&#21512;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#23460;&#22806;SSC&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SSC-RS&#30340;&#32593;&#32476;&#65292;&#21033;&#29992;&#28145;&#24230;&#30417;&#30563;&#23558;&#35821;&#20041;&#21644;&#20960;&#20309;&#34920;&#31034;&#30340;&#23398;&#20064;&#36807;&#31243;&#26126;&#30830;&#22320;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#20998;&#25903;&#12290;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#20010;&#37197;&#22791;&#20102;&#33258;&#36866;&#24212;&#34920;&#31034;&#34701;&#21512;(ARF)&#27169;&#22359;&#30340;BEV&#34701;&#21512;&#32593;&#32476;&#65292;&#20197;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#32858;&#21512;&#22810;&#23610;&#24230;&#29305;&#24449;&#12290;&#30001;&#20110;&#20302;&#35745;&#31639;&#36127;&#25285;&#21644;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
Semantic scene completion (SSC) jointly predicts the semantics and geometry of the entire 3D scene, which plays an essential role in 3D scene understanding for autonomous driving systems. SSC has achieved rapid progress with the help of semantic context in segmentation. However, how to effectively exploit the relationships between the semantic context in semantic segmentation and geometric structure in scene completion remains under exploration. In this paper, we propose to solve outdoor SSC from the perspective of representation separation and BEV fusion. Specifically, we present the network, named SSC-RS, which uses separate branches with deep supervision to explicitly disentangle the learning procedure of the semantic and geometric representations. And a BEV fusion network equipped with the proposed Adaptive Representation Fusion (ARF) module is presented to aggregate the multi-scale features effectively and efficiently. Due to the low computational burden and powerful representatio
&lt;/p&gt;</description></item><item><title>PANet&#26159;&#19968;&#31181;&#26032;&#30340;LiDAR&#20840;&#26223;&#20998;&#21106;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#23454;&#20363;&#25552;&#20986;&#21644;&#32858;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#22823;&#29289;&#20307;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15348</link><description>&lt;p&gt;
PANet: &#24102;&#26377;&#31232;&#30095;&#23454;&#20363;&#25552;&#20986;&#21644;&#32858;&#21512;&#30340;LiDAR&#20840;&#26223;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
PANet: LiDAR Panoptic Segmentation with Sparse Instance Proposal and Aggregation. (arXiv:2306.15348v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15348
&lt;/p&gt;
&lt;p&gt;
PANet&#26159;&#19968;&#31181;&#26032;&#30340;LiDAR&#20840;&#26223;&#20998;&#21106;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#23454;&#20363;&#25552;&#20986;&#21644;&#32858;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#22823;&#29289;&#20307;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;LiDAR&#20840;&#26223;&#20998;&#21106;(LPS)&#65292;&#21253;&#25324;&#35821;&#20041;&#21644;&#23454;&#20363;&#20998;&#21106;&#65292;&#23545;&#20110;&#35768;&#22810;&#26426;&#22120;&#20154;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LPS&#26694;&#26550;PANet&#65292;&#20197;&#28040;&#38500;&#23545;&#20559;&#31227;&#20998;&#25903;&#30340;&#20381;&#36182;&#24182;&#25552;&#39640;&#22823;&#29289;&#20307;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#29289;&#20307;&#24120;&#24120;&#34987;&#32858;&#31867;&#31639;&#27861;&#36807;&#24230;&#20998;&#21106;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#23398;&#20064;&#30340;&#31232;&#30095;&#23454;&#20363;&#25552;&#20986;&#65288;SIP&#65289;&#27169;&#22359;&#65292;&#37319;&#29992;&#8220;&#37319;&#26679;-&#24179;&#31227;-&#20998;&#32452;&#8221;&#26041;&#26696;&#65292;&#20174;&#21407;&#22987;&#28857;&#20113;&#20013;&#39640;&#25928;&#22320;&#23558;&#29289;&#20307;&#28857;&#30452;&#25509;&#20998;&#32452;&#25104;&#23454;&#20363;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24341;&#20837;&#24179;&#34913;&#30340;&#28857;&#37319;&#26679;&#26469;&#29983;&#25104;&#20855;&#26377;&#26356;&#22343;&#21248;&#36317;&#31163;&#20998;&#24067;&#30340;&#31232;&#30095;&#31181;&#23376;&#28857;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#27668;&#27873;&#24179;&#31227;&#8221;&#30340;&#24179;&#31227;&#27169;&#22359;&#65292;&#23558;&#31181;&#23376;&#28857;&#25910;&#32553;&#21040;&#32858;&#31867;&#20013;&#24515;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36830;&#25509;&#32452;&#20214;&#26631;&#31614;&#31639;&#27861;&#29983;&#25104;&#23454;&#20363;&#25552;&#26696;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#20363;&#32858;&#21512;&#27169;&#22359;&#65292;&#20197;&#38598;&#25104;&#21487;&#33021;&#30340;&#30862;&#29255;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable LiDAR panoptic segmentation (LPS), including both semantic and instance segmentation, is vital for many robotic applications, such as autonomous driving. This work proposes a new LPS framework named PANet to eliminate the dependency on the offset branch and improve the performance on large objects, which are always over-segmented by clustering algorithms. Firstly, we propose a non-learning Sparse Instance Proposal (SIP) module with the ``sampling-shifting-grouping" scheme to directly group thing points into instances from the raw point cloud efficiently. More specifically, balanced point sampling is introduced to generate sparse seed points with more uniform point distribution over the distance range. And a shift module, termed bubble shifting, is proposed to shrink the seed points to the clustered centers. Then we utilize the connected component label algorithm to generate instance proposals. Furthermore, an instance aggregation module is devised to integrate potentially frag
&lt;/p&gt;</description></item><item><title>FedET&#26159;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22686;&#24378;Transformer&#21644;&#22686;&#24378;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36890;&#20449;&#25104;&#26412;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#39640;&#31934;&#24230;&#21644;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2306.15347</link><description>&lt;p&gt;
FedET: &#19968;&#31181;&#22522;&#20110;&#22686;&#24378;Transformer&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedET: A Communication-Efficient Federated Class-Incremental Learning Framework Based on Enhanced Transformer. (arXiv:2306.15347v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15347
&lt;/p&gt;
&lt;p&gt;
FedET&#26159;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22686;&#24378;Transformer&#21644;&#22686;&#24378;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36890;&#20449;&#25104;&#26412;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#39640;&#31934;&#24230;&#21644;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated Learning, FL)&#30001;&#20110;&#33021;&#22815;&#22312;&#20445;&#35777;&#25968;&#25454;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#36827;&#34892;&#20998;&#25955;&#23398;&#20064;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#19981;&#20999;&#23454;&#38469;&#22320;&#20551;&#35774;&#26412;&#22320;&#23458;&#25143;&#31471;&#36935;&#21040;&#30340;&#31867;&#21035;&#38543;&#26102;&#38388;&#22266;&#23450;&#12290;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#21518;&#65292;&#36825;&#20010;&#20551;&#35774;&#20250;&#23548;&#33268;&#27169;&#22411;&#23545;&#26087;&#31867;&#21035;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#27492;&#22806;&#65292;&#21463;&#36890;&#20449;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#22312;FL&#20013;&#20351;&#29992;&#22823;&#35268;&#27169;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#23558;&#24433;&#21709;&#39044;&#27979;&#31934;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;FedET&#65292;&#23427;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FedET&#20351;&#29992;&#22686;&#24378;&#22120;(Enhancer)&#36825;&#20010;&#23567;&#22411;&#27169;&#22359;&#26469;&#21560;&#25910;&#21644;&#20256;&#36882;&#26032;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#39044;&#35757;&#32451;&#30340;Transformer&#19982;&#19981;&#21516;&#30340;&#22686;&#24378;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#20445;&#35777;&#39640;&#31934;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#26032;&#20219;&#21153;&#30340;&#26032;&#31867;&#21035;&#24341;&#36215;&#30340;&#26412;&#22320;&#36951;&#24536;&#38382;&#39064;&#21644;&#38750;i.i.d&#65288;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65289;&#25152;&#24102;&#26469;&#30340;&#20840;&#23616;&#36951;&#24536;&#38382;&#39064;&#65292;FedET&#20351;&#29992;&#20102;&#21160;&#24577;&#25193;&#23637;&#26041;&#27861;&#21644;&#37325;&#25918;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has been widely concerned for it enables decentralized learning while ensuring data privacy. However, most existing methods unrealistically assume that the classes encountered by local clients are fixed over time. After learning new classes, this assumption will make the model's catastrophic forgetting of old classes significantly severe. Moreover, due to the limitation of communication cost, it is challenging to use large-scale models in FL, which will affect the prediction accuracy. To address these challenges, we propose a novel framework, Federated Enhanced Transformer (FedET), which simultaneously achieves high accuracy and low communication cost. Specifically, FedET uses Enhancer, a tiny module, to absorb and communicate new knowledge, and applies pre-trained Transformers combined with different Enhancers to ensure high precision on various tasks. To address local forgetting caused by new classes of new tasks and global forgetting brought by non-i.i.d (non
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#27627;&#31859;&#27874;&#25104;&#20687;&#31995;&#32479;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#20998;&#36776;&#29575;&#12289;&#23450;&#20301;&#31934;&#24230;&#21644;&#26816;&#27979;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;&#26080;&#32447;&#30005;&#39057;&#29575;&#27874;&#24418;&#30340;&#24050;&#30693;&#29305;&#24615;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.15341</link><description>&lt;p&gt;
&#25552;&#39640;&#27627;&#31859;&#27874;&#25104;&#20687;&#31995;&#32479;&#30340;&#26032;&#22411;&#28151;&#21512;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Novel Hybrid-Learning Algorithms for Improved Millimeter-Wave Imaging Systems. (arXiv:2306.15341v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15341
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#27627;&#31859;&#27874;&#25104;&#20687;&#31995;&#32479;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#20998;&#36776;&#29575;&#12289;&#23450;&#20301;&#31934;&#24230;&#21644;&#26816;&#27979;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;&#26080;&#32447;&#30005;&#39057;&#29575;&#27874;&#24418;&#30340;&#24050;&#30693;&#29305;&#24615;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#27491;&#34987;&#38598;&#20013;&#22312;&#27627;&#31859;&#27874;&#65288;30 GHz&#21040;300 GHz&#65289;&#21644;&#22826;&#36203;&#20857;&#65288;300 GHz&#21040;10 THz&#65289;&#24863;&#30693;&#24212;&#29992;&#19978;&#65292;&#21253;&#25324;&#23433;&#20840;&#24863;&#30693;&#12289;&#24037;&#19994;&#21253;&#35013;&#12289;&#21307;&#23398;&#25104;&#20687;&#21644;&#38750;&#30772;&#22351;&#24615;&#27979;&#35797;&#12290;&#20256;&#32479;&#30340;&#24863;&#30693;&#21644;&#25104;&#20687;&#26041;&#27861;&#38754;&#20020;&#30528;&#26032;&#22411;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#31639;&#27861;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#20998;&#36776;&#29575;&#12289;&#23450;&#20301;&#31934;&#24230;&#21644;&#26816;&#27979;&#29575;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24863;&#30693;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#33719;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#27969;&#34892;&#12290;&#19982;&#20256;&#32479;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#30456;&#27604;&#65292;&#28151;&#21512;&#26041;&#27861;&#23558;&#20449;&#21495;&#22788;&#29702;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#31639;&#27861;&#20132;&#26367;&#20351;&#29992;&#65292;&#25552;&#20379;&#20102;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#26377;&#24076;&#26395;&#30340;&#25240;&#34935;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#28151;&#21512;&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#26080;&#32447;&#30005;&#39057;&#29575;&#65288;RF&#65289;&#27874;&#24418;&#30340;&#24050;&#30693;&#29305;&#24615;&#26469;&#25913;&#36827;&#27169;&#22411;&#35757;&#32451;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#39640;&#25928;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increasing attention is being paid to millimeter-wave (mmWave), 30 GHz to 300 GHz, and terahertz (THz), 300 GHz to 10 THz, sensing applications including security sensing, industrial packaging, medical imaging, and non-destructive testing. Traditional methods for perception and imaging are challenged by novel data-driven algorithms that offer improved resolution, localization, and detection rates. Over the past decade, deep learning technology has garnered substantial popularity, particularly in perception and computer vision applications. Whereas conventional signal processing techniques are more easily generalized to various applications, hybrid approaches where signal processing and learning-based algorithms are interleaved pose a promising compromise between performance and generalizability. Furthermore, such hybrid algorithms improve model training by leveraging the known characteristics of radio frequency (RF) waveforms, thus yielding more efficiently trained deep learning algori
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21516;&#35843;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24212;&#29992;&#32593;&#32476;&#36807;&#28388;&#25216;&#26415;&#26500;&#24314;&#31232;&#30095;&#30340;&#39640;&#38454;&#22270;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#33021;&#28304;&#25928;&#29575;&#25361;&#25112;&#65292;&#24182;&#22312;&#34920;&#26684;&#25968;&#25454;&#21644;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15337</link><description>&lt;p&gt;
&#21516;&#35843;&#31070;&#32463;&#32593;&#32476;&#65306;&#22810;&#20803;&#22797;&#26434;&#24615;&#30340;&#31232;&#30095;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Homological Neural Networks: A Sparse Architecture for Multivariate Complexity. (arXiv:2306.15337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21516;&#35843;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24212;&#29992;&#32593;&#32476;&#36807;&#28388;&#25216;&#26415;&#26500;&#24314;&#31232;&#30095;&#30340;&#39640;&#38454;&#22270;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#33021;&#28304;&#25928;&#29575;&#25361;&#25112;&#65292;&#24182;&#22312;&#34920;&#26684;&#25968;&#25454;&#21644;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12289;&#33021;&#28304;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#26085;&#30410;&#22686;&#38271;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20808;&#36827;&#30340;&#22522;&#20110;&#32593;&#32476;&#30340;&#20449;&#24687;&#36807;&#28388;&#25216;&#26415;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#65292;&#35813;&#21333;&#20803;&#20197;&#22522;&#30784;&#25968;&#25454;&#30340;&#21516;&#35843;&#32467;&#26500;&#20026;&#22522;&#30784;&#26500;&#24314;&#20102;&#19968;&#20010;&#31232;&#30095;&#30340;&#39640;&#38454;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20256;&#32479;&#19978;&#23545;&#28145;&#24230;&#23398;&#20064;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65306;&#34920;&#26684;&#25968;&#25454;&#21644;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#39062;&#35774;&#35745;&#30340;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#20165;&#20351;&#29992;&#37096;&#20998;&#21442;&#25968;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24182;&#21015;&#25110;&#36229;&#36234;&#20854;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of Artificial Intelligence research came with the development of increasingly complex deep learning models, leading to growing challenges in terms of computational complexity, energy efficiency and interpretability. In this study, we apply advanced network-based information filtering techniques to design a novel deep neural network unit characterized by a sparse higher-order graphical architecture built over the homological structure of underlying data. We demonstrate its effectiveness in two application domains which are traditionally challenging for deep learning: tabular data and time series regression problems. Results demonstrate the advantages of this novel design which can tie or overcome the results of state-of-the-art machine learning and deep learning models using only a fraction of parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Shoggoth&#65292;&#19968;&#31181;&#38024;&#23545;&#23454;&#26102;&#35270;&#39057;&#25512;&#26029;&#30340;&#39640;&#25928;&#36793;&#32536;-&#20113;&#21327;&#21516;&#26550;&#26500;&#12290;&#36890;&#36807;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#21644;&#20113;&#31471;&#21368;&#36733;&#26631;&#31614;&#36807;&#31243;&#65292;Shoggoth&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#24182;&#20943;&#36731;&#36793;&#32536;&#35774;&#22791;&#36164;&#28304;&#38480;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20165;&#20351;&#29992;&#36793;&#32536;&#31574;&#30053;&#21644;&#20165;&#20351;&#29992;&#20113;&#31471;&#31574;&#30053;&#65292;Shoggoth&#33021;&#22815;&#25552;&#20379;15%-20%&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#25913;&#36827;&#21644;&#26356;&#20302;&#30340;&#32593;&#32476;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.15333</link><description>&lt;p&gt;
Shoggoth: &#36890;&#36807;&#33258;&#36866;&#24212;&#22312;&#32447;&#23398;&#20064;&#23454;&#29616;&#39640;&#25928;&#30340;&#36793;&#32536;-&#20113;&#21327;&#21516;&#23454;&#26102;&#35270;&#39057;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Shoggoth: Towards Efficient Edge-Cloud Collaborative Real-Time Video Inference via Adaptive Online Learning. (arXiv:2306.15333v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Shoggoth&#65292;&#19968;&#31181;&#38024;&#23545;&#23454;&#26102;&#35270;&#39057;&#25512;&#26029;&#30340;&#39640;&#25928;&#36793;&#32536;-&#20113;&#21327;&#21516;&#26550;&#26500;&#12290;&#36890;&#36807;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#21644;&#20113;&#31471;&#21368;&#36733;&#26631;&#31614;&#36807;&#31243;&#65292;Shoggoth&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#24182;&#20943;&#36731;&#36793;&#32536;&#35774;&#22791;&#36164;&#28304;&#38480;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20165;&#20351;&#29992;&#36793;&#32536;&#31574;&#30053;&#21644;&#20165;&#20351;&#29992;&#20113;&#31471;&#31574;&#30053;&#65292;Shoggoth&#33021;&#22815;&#25552;&#20379;15%-20%&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#25913;&#36827;&#21644;&#26356;&#20302;&#30340;&#32593;&#32476;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Shoggoth&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#32536;-&#20113;&#21327;&#21516;&#26550;&#26500;&#65292;&#29992;&#20110;&#25552;&#21319;&#23545;&#20110;&#23454;&#26102;&#35270;&#39057;&#20013;&#21464;&#21270;&#22330;&#26223;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;Shoggoth&#21033;&#29992;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#26469;&#25913;&#21892;&#30001;&#20110;&#25968;&#25454;&#28418;&#31227;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#23558;&#26631;&#31614;&#36807;&#31243;&#21368;&#36733;&#21040;&#20113;&#31471;&#65292;&#20943;&#36731;&#36793;&#32536;&#35774;&#22791;&#30340;&#36164;&#28304;&#38480;&#21046;&#12290;&#22312;&#36793;&#32536;&#26041;&#38754;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36866;&#24212;&#24615;&#35757;&#32451;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#35757;&#32451;&#30340;&#26041;&#24335;&#22312;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#19979;&#35843;&#25972;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#25277;&#26679;&#35757;&#32451;&#24103;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20943;&#23569;&#24102;&#23485;&#12290;&#22522;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#19982;&#20165;&#36793;&#32536;&#31574;&#30053;&#30456;&#27604;&#65292;&#27169;&#22411;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;15%-20%&#65292;&#32780;&#32593;&#32476;&#25104;&#26412;&#36739;&#20113;&#31471;&#31574;&#30053;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes Shoggoth, an efficient edge-cloud collaborative architecture, for boosting inference performance on real-time video of changing scenes. Shoggoth uses online knowledge distillation to improve the accuracy of models suffering from data drift and offloads the labeling process to the cloud, alleviating constrained resources of edge devices. At the edge, we design adaptive training using small batches to adapt models under limited computing power, and adaptive sampling of training frames for robustness and reducing bandwidth. The evaluations on the realistic dataset show 15%-20% model accuracy improvement compared to the edge-only strategy and fewer network costs than the cloud-only strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#24179;&#38754;&#22270;&#39044;&#27979;&#34892;&#20154;&#23494;&#24230;&#21644;&#24635;&#30095;&#25955;&#26102;&#38388;&#12290;&#36890;&#36807;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#24182;&#23558;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15318</link><description>&lt;p&gt;
&#20351;&#29992;Vision Transformer&#20174;&#24179;&#38754;&#22270;&#39044;&#27979;&#34892;&#20154;&#30095;&#25955;&#26102;&#38388;&#21644;&#23494;&#24230;
&lt;/p&gt;
&lt;p&gt;
Towards predicting Pedestrian Evacuation Time and Density from Floorplans using a Vision Transformer. (arXiv:2306.15318v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#24179;&#38754;&#22270;&#39044;&#27979;&#34892;&#20154;&#23494;&#24230;&#21644;&#24635;&#30095;&#25955;&#26102;&#38388;&#12290;&#36890;&#36807;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#24182;&#23558;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#34892;&#20154;&#27169;&#25311;&#22120;&#26159;&#24314;&#31569;&#35774;&#35745;&#36807;&#31243;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#65292;&#23427;&#20204;&#20351;&#39033;&#30446;&#24037;&#31243;&#24072;&#33021;&#22815;&#39044;&#38450;&#25317;&#25380;&#24773;&#20917;&#24182;&#35268;&#21010;&#30095;&#25955;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#36816;&#34892;&#26102;&#38388;&#21644;&#29983;&#25104;&#27169;&#25311;&#32467;&#26524;&#30340;&#22810;&#20010;&#32321;&#29712;&#27493;&#39588;&#21487;&#33021;&#25104;&#20026;&#24314;&#31569;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#29942;&#39048;&#12290;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#36895;&#24230;&#19978;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#24182;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#25552;&#20379;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#32467;&#26524;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;Vision Transformer&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#30340;&#24179;&#38754;&#22270;&#39044;&#27979;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#23494;&#24230;&#28909;&#21147;&#22270;&#21644;&#24635;&#30095;&#25955;&#26102;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#20256;&#32479;&#30340;&#27169;&#25311;&#22120;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#25105;&#20204;&#30340;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26080;&#32541;&#22320;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#38598;&#25104;&#21040;B&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional pedestrian simulators are inevitable tools in the design process of a building, as they enable project engineers to prevent overcrowding situations and plan escape routes for evacuation. However, simulation runtime and the multiple cumbersome steps in generating simulation results are potential bottlenecks during the building design process. Data-driven approaches have demonstrated their capability to outperform conventional methods in speed while delivering similar or even better results across many disciplines. In this work, we present a deep learning-based approach based on a Vision Transformer to predict density heatmaps over time and total evacuation time from a given floorplan. Specifically, due to limited availability of public datasets, we implement a parametric data generation pipeline including a conventional simulator. This enables us to build a large synthetic dataset that we use to train our architecture. Furthermore, we seamlessly integrate our model into a B
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20174;&#20915;&#31574;&#21512;&#29702;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#65292;&#24182;&#23450;&#20041;&#20102;&#21442;&#25968;&#24179;&#31561;&#24471;&#20998;&#26469;&#34920;&#24449;&#20844;&#24179;&#20915;&#31574;&#36807;&#31243;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#35268;&#33539;&#39033;&#26080;&#27861;&#23454;&#29616;&#20915;&#31574;&#21512;&#29702;&#24615;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2306.15299</link><description>&lt;p&gt;
FAIRER: &#20844;&#24179;&#20316;&#20026;&#20915;&#31574;&#21512;&#29702;&#24615;&#23545;&#40784;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
FAIRER: Fairness as Decision Rationale Alignment. (arXiv:2306.15299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20174;&#20915;&#31574;&#21512;&#29702;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#65292;&#24182;&#23450;&#20041;&#20102;&#21442;&#25968;&#24179;&#31561;&#24471;&#20998;&#26469;&#34920;&#24449;&#20844;&#24179;&#20915;&#31574;&#36807;&#31243;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#35268;&#33539;&#39033;&#26080;&#27861;&#23454;&#29616;&#20915;&#31574;&#21512;&#29702;&#24615;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#24448;&#24448;&#23384;&#22312;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#28145;&#24230;&#27169;&#22411;&#36890;&#24120;&#22312;&#26576;&#20123;&#23376;&#32676;&#20307;&#65288;&#20363;&#22914;&#30007;&#24615;&#21644;&#22899;&#24615;&#65289;&#20043;&#38388;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#20934;&#30830;&#24615;&#24046;&#24322;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20844;&#24179;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#26469;&#32422;&#26463;&#26368;&#21518;&#19968;&#23618;&#30340;&#36755;&#20986;&#24182;&#30452;&#25509;&#35268;&#33539;&#21270;DNNs&#26469;&#35299;&#20915;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#34429;&#28982;DNN&#30340;&#20844;&#24179;&#24615;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#20294;&#19981;&#28165;&#26970;&#32463;&#36807;&#35757;&#32451;&#30340;&#32593;&#32476;&#22914;&#20309;&#36827;&#34892;&#20844;&#24179;&#39044;&#27979;&#65292;&#36825;&#38480;&#21046;&#20102;&#26410;&#26469;&#30340;&#20844;&#24179;&#24615;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20915;&#31574;&#21512;&#29702;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20844;&#24179;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21508;&#20010;&#23376;&#32676;&#20307;&#20013;&#30340;&#31070;&#32463;&#20803;&#24433;&#21709;&#26469;&#23450;&#20041;&#21442;&#25968;&#24179;&#31561;&#24471;&#20998;&#26469;&#34920;&#24449;&#32593;&#32476;&#30340;&#20844;&#24179;&#20915;&#31574;&#36807;&#31243;&#12290;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#20844;&#24179;&#38382;&#39064;&#21487;&#33021;&#28304;&#20110;&#23376;&#32676;&#20307;&#30340;&#19981;&#23545;&#40784;&#20915;&#31574;&#21512;&#29702;&#24615;&#12290;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#35268;&#33539;&#39033;&#26080;&#27861;&#23454;&#29616;&#20915;&#31574;&#21512;&#29702;&#24615;&#30340;&#23545;&#40784;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#32422;&#26463;&#26368;&#21518;&#19968;&#23618;&#30340;&#36755;&#20986;&#65292;&#32780;&#24573;&#35270;&#20102;&#20043;&#21069;&#30340;&#23618;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have made significant progress, but often suffer from fairness issues, as deep models typically show distinct accuracy differences among certain subgroups (e.g., males and females). Existing research addresses this critical issue by employing fairness-aware loss functions to constrain the last-layer outputs and directly regularize DNNs. Although the fairness of DNNs is improved, it is unclear how the trained network makes a fair prediction, which limits future fairness improvements. In this paper, we investigate fairness from the perspective of decision rationale and define the parameter parity score to characterize the fair decision process of networks by analyzing neuron influence in various subgroups. Extensive empirical studies show that the unfair issue could arise from the unaligned decision rationales of subgroups. Existing fairness regularization terms fail to achieve decision rationale alignment because they only constrain last-layer outputs while i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#30340;IMDB&#30005;&#24433;&#20998;&#31867;&#22120;&#30340;&#20363;&#23376;&#20013;&#23545;BERT&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20844;&#24320;&#30340;BERT&#27169;&#22411;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#24378;&#35843;&#20102;&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15298</link><description>&lt;p&gt;
BERT&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#8212;&#8212;&#36890;&#36807;&#24773;&#24863;&#35780;&#20998;&#22312;&#29616;&#23454;&#30340;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#27979;&#37327;&#21644;&#20998;&#26512;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task. (arXiv:2306.15298v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15298
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#30340;IMDB&#30005;&#24433;&#20998;&#31867;&#22120;&#30340;&#20363;&#23376;&#20013;&#23545;BERT&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20844;&#24320;&#30340;BERT&#27169;&#22411;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#24378;&#35843;&#20102;&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#24212;&#29992;&#20013;&#20844;&#24320;&#21487;&#29992;&#65292;&#24182;&#19981;&#26029;&#36827;&#34892;&#24494;&#35843;&#12290;&#38543;&#30528;&#23427;&#20204;&#20855;&#22791;&#25235;&#21462;&#22797;&#26434;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#26377;&#23475;&#20559;&#35265;&#24456;&#21487;&#33021;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#36825;&#20123;&#27169;&#22411;&#20132;&#32455;&#22312;&#19968;&#36215;&#12290;&#26412;&#25991;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#26469;&#20998;&#26512;BERT&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65306;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#20559;&#35265;&#23450;&#20041;&#20026;&#22899;&#24615;&#21644;&#30007;&#24615;&#26679;&#26412;&#29256;&#26412;&#22312;&#24773;&#24863;&#35780;&#20272;&#19978;&#30340;&#24046;&#24322;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#29616;&#23454;&#30340;IMDB&#30005;&#24433;&#20998;&#31867;&#22120;&#30340;&#20363;&#23376;&#20013;BERT&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#36890;&#36807;&#31995;&#32479;&#22320;&#21464;&#21270;&#35757;&#32451;&#27969;&#31243;&#30340;&#21508;&#20010;&#20803;&#32032;&#65292;&#25105;&#20204;&#21487;&#20197;&#23545;&#26368;&#32456;&#27169;&#22411;&#20559;&#35265;&#30340;&#24433;&#21709;&#20570;&#20986;&#32467;&#35770;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19971;&#20010;&#19981;&#21516;&#30340;&#20844;&#24320;BERT&#27169;&#22411;&#30340;&#20061;&#31181;&#35757;&#32451;&#26465;&#20214;&#65292;&#21363;&#24635;&#20849;63&#20010;&#27169;&#22411;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#26465;&#20214;&#37117;&#20135;&#29983;&#20102;&#26174;&#33879;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#26144;&#30340;&#20559;&#35265;&#28304;&#20110;&#20844;&#24320;&#30340;BERT&#27169;&#22411;&#32780;&#19981;&#26159;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#65292;&#24378;&#35843;&#20102;&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models are publicly available and constantly finetuned for various real-life applications. As they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. This paper analyses gender bias in BERT models with two main contributions: First, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. Second, we comprehensively analyse BERT's biases on the example of a realistic IMDB movie classifier. By systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. Seven different public BERT models in nine training conditions, i.e. 63 models in total, are compared. Almost all conditions yield significant gender biases. Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage.
&lt;/p&gt;</description></item><item><title>IDOL&#26159;&#19968;&#31181;&#38754;&#21521;&#36923;&#36753;&#25512;&#29702;&#30340;&#25351;&#26631;&#23548;&#21521;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36923;&#36753;&#25351;&#26631;&#21644;&#36923;&#36753;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#22312;&#36923;&#36753;&#19978;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;IDOL&#22312;&#36923;&#36753;&#25512;&#29702;MRC&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32508;&#21512;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.15273</link><description>&lt;p&gt;
IDOL: &#38754;&#21521;&#36923;&#36753;&#25512;&#29702;&#30340;&#25351;&#26631;&#23548;&#21521;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning. (arXiv:2306.15273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15273
&lt;/p&gt;
&lt;p&gt;
IDOL&#26159;&#19968;&#31181;&#38754;&#21521;&#36923;&#36753;&#25512;&#29702;&#30340;&#25351;&#26631;&#23548;&#21521;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36923;&#36753;&#25351;&#26631;&#21644;&#36923;&#36753;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#22312;&#36923;&#36753;&#19978;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;IDOL&#22312;&#36923;&#36753;&#25512;&#29702;MRC&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32508;&#21512;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#39046;&#22495;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#35768;&#22810;&#20219;&#21153;&#65288;&#22914;SQuAD&#65289;&#20013;&#30340;&#34920;&#29616;&#24050;&#32463;&#36229;&#36807;&#20102;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#21040;&#36923;&#36753;&#25512;&#29702;&#26102;&#65292;&#20173;&#26377;&#24456;&#22823;&#30340;&#36827;&#27493;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IDOL&#65288;InDicator-Oriented Logic Pre-training&#65289;&#30340;&#26131;&#20110;&#29702;&#35299;&#19988;&#39640;&#25928;&#30340;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21033;&#29992;6&#31181;&#36923;&#36753;&#25351;&#26631;&#21644;&#36923;&#36753;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;LGP&#65288;LoGic Pre-training&#65289;&#22312;&#36923;&#36753;&#19978;&#24378;&#21270;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;IDOL&#22312;ReClor&#21644;LogiQA&#36825;&#20004;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#36923;&#36753;&#25512;&#29702;MRC&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#34987;&#35777;&#26126;&#33021;&#22815;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#20854;&#20182;&#31867;&#22411;&#30340;MRC&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;RACE&#21644;SQuAD 2.0&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#31454;&#20105;&#21147;&#30340;&#32508;&#21512;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of machine reading comprehension (MRC), existing systems have surpassed the average performance of human beings in many tasks like SQuAD. However, there is still a long way to go when it comes to logical reasoning. Although some methods for it have been put forward, they either are designed in a quite complicated way or rely too much on external structures. In this paper, we proposed IDOL (InDicator-Oriented Logic Pre-training), an easy-to-understand but highly effective further pre-training task which logically strengthens the pre-trained models with the help of 6 types of logical indicators and a logically rich dataset LGP (LoGic Pre-training). IDOL achieves state-of-the-art performance on ReClor and LogiQA, the two most representative benchmarks in logical reasoning MRC, and is proven to be capable of generalizing to different pre-trained models and other types of MRC benchmarks like RACE and SQuAD 2.0 while keeping competitive general language understanding ability thr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22840;&#22823;&#30340;&#35299;&#37322;&#65292;&#23427;&#26159;&#20026;&#20102;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20915;&#31574;&#21407;&#22240;&#25152;&#23450;&#20041;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#35299;&#37322;&#26041;&#27861;&#21482;&#26174;&#31034;&#36873;&#25321;&#30340;&#29305;&#24449;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#32780;&#22840;&#22823;&#30340;&#35299;&#37322;&#32771;&#34385;&#20102;&#26356;&#22810;&#29305;&#24449;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15272</link><description>&lt;p&gt;
&#25552;&#20379;&#22840;&#22823;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Delivering Inflated Explanations. (arXiv:2306.15272v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22840;&#22823;&#30340;&#35299;&#37322;&#65292;&#23427;&#26159;&#20026;&#20102;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20915;&#31574;&#21407;&#22240;&#25152;&#23450;&#20041;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#35299;&#37322;&#26041;&#27861;&#21482;&#26174;&#31034;&#36873;&#25321;&#30340;&#29305;&#24449;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#32780;&#22840;&#22823;&#30340;&#35299;&#37322;&#32771;&#34385;&#20102;&#26356;&#22810;&#29305;&#24449;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36861;&#27714;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#32463;&#24120;&#20986;&#29616;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363; AI &#31995;&#32479;&#20570;&#20986;&#20915;&#31574;&#30340;&#21407;&#22240;&#26159;&#20160;&#20040;&#12290;&#35299;&#37322;&#24615;&#30340;&#27491;&#24335;&#26041;&#27861;&#24314;&#31435;&#20102; AI &#31995;&#32479;&#30340;&#24418;&#24335;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#35813;&#27169;&#22411;&#25512;&#29702;&#31995;&#32479;&#30340;&#29305;&#24615;&#12290;&#32473;&#23450;&#19968;&#20010;&#35201;&#35299;&#37322;&#30340;&#23454;&#20363;&#30340;&#29305;&#24449;&#20540;&#38598;&#21644;&#30456;&#24212;&#30340;&#20915;&#31574;&#65292;&#19968;&#20010;&#24418;&#24335;&#25512;&#29702;&#35299;&#37322;&#26159;&#19968;&#32452;&#29305;&#24449;&#65292;&#22914;&#26524;&#23427;&#20204;&#37319;&#21462;&#32473;&#23450;&#20540;&#65292;&#23558;&#22987;&#32456;&#23548;&#33268;&#21516;&#26679;&#30340;&#20915;&#31574;&#12290;&#36825;&#31181;&#35299;&#37322;&#26159;&#26377;&#29992;&#30340;&#65292;&#23427;&#26174;&#31034;&#21482;&#26377;&#19968;&#20123;&#29305;&#24449;&#34987;&#29992;&#20110;&#20570;&#20986;&#26368;&#32456;&#20915;&#31574;&#12290;&#20294;&#23427;&#26159;&#29421;&#20041;&#30340;&#65292;&#23427;&#21482;&#26174;&#31034;&#22914;&#26524;&#36873;&#25321;&#30340;&#29305;&#24449;&#37319;&#21462;&#23427;&#20204;&#32473;&#23450;&#30340;&#20540;&#65292;&#20915;&#31574;&#23601;&#19981;&#20250;&#25913;&#21464;&#12290;&#21487;&#33021;&#26377;&#20123;&#29305;&#24449;&#30340;&#20540;&#20250;&#25913;&#21464;&#65292;&#20294;&#20173;&#28982;&#23548;&#33268;&#30456;&#21516;&#30340;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#22840;&#22823;&#30340;&#35299;&#37322;&#65292;&#23427;&#26159;&#19968;&#32452;&#29305;&#24449;&#65292;&#23545;&#20110;&#27599;&#20010;&#29305;&#24449;&#26377;&#19968;&#32452;&#20540;&#65288;&#22987;&#32456;&#21253;&#25324;&#35813;&#29305;&#24449;&#30340;&#20540;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the quest for Explainable Artificial Intelligence (XAI) one of the questions that frequently arises given a decision made by an AI system is, ``why was the decision made in this way?'' Formal approaches to explainability build a formal model of the AI system and use this to reason about the properties of the system. Given a set of feature values for an instance to be explained, and a resulting decision, a formal abductive explanation is a set of features, such that if they take the given value will always lead to the same decision. This explanation is useful, it shows that only some features were used in making the final decision. But it is narrow, it only shows that if the selected features take their given values the decision is unchanged. It's possible that some features may change values and still lead to the same decision. In this paper we formally define inflated explanations which is a set of features, and for each feature of set of values (always including the value of the i
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#24191;&#20041;&#31163;&#32676;&#25925;&#38556;&#35786;&#26029;&#65288;GOOFD&#65289;&#26694;&#26550;&#30340;&#38598;&#25104;&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#25925;&#38556;&#26816;&#27979;&#12289;&#25925;&#38556;&#20998;&#31867;&#21644;&#26032;&#39062;&#25925;&#38556;&#35786;&#26029;&#31561;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#20869;&#37096;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#26041;&#27861;&#25552;&#21462;&#29305;&#24449;&#24182;&#35782;&#21035;&#24322;&#24120;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.15266</link><description>&lt;p&gt;
&#20869;&#37096;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#24191;&#20041;&#31163;&#32676;&#25925;&#38556;&#35786;&#26029;&#65288;GOOFD&#65289;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Internal Contrastive Learning for Generalized Out-of-distribution Fault Diagnosis (GOOFD) Framework. (arXiv:2306.15266v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15266
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#24191;&#20041;&#31163;&#32676;&#25925;&#38556;&#35786;&#26029;&#65288;GOOFD&#65289;&#26694;&#26550;&#30340;&#38598;&#25104;&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#25925;&#38556;&#26816;&#27979;&#12289;&#25925;&#38556;&#20998;&#31867;&#21644;&#26032;&#39062;&#25925;&#38556;&#35786;&#26029;&#31561;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#20869;&#37096;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#26041;&#27861;&#25552;&#21462;&#29305;&#24449;&#24182;&#35782;&#21035;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#35786;&#26029;&#23545;&#20110;&#24037;&#19994;&#36807;&#31243;&#20013;&#30417;&#27979;&#37325;&#35201;&#26426;&#22120;&#30340;&#29366;&#24577;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#24037;&#20316;&#26465;&#20214;&#30340;&#22797;&#26434;&#21270;&#21644;&#29983;&#20135;&#36816;&#33829;&#36807;&#31243;&#20013;&#23545;&#23433;&#20840;&#30340;&#35201;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#19981;&#21516;&#30340;&#35786;&#26029;&#26041;&#27861;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#38656;&#35201;&#19968;&#20010;&#21487;&#20197;&#24212;&#23545;&#22810;&#20010;&#20219;&#21153;&#30340;&#38598;&#25104;&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#35786;&#26029;&#23376;&#20219;&#21153;&#36890;&#24120;&#34987;&#20998;&#24320;&#30740;&#31350;&#65292;&#32780;&#24403;&#21069;&#21487;&#29992;&#30340;&#26041;&#27861;&#22312;&#36825;&#26679;&#19968;&#20010;&#24191;&#20041;&#31995;&#32479;&#19978;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#31163;&#32676;&#25925;&#38556;&#35786;&#26029;&#65288;GOOFD&#65289;&#26694;&#26550;&#65292;&#20197;&#25972;&#21512;&#25925;&#38556;&#26816;&#27979;&#12289;&#25925;&#38556;&#20998;&#31867;&#21644;&#26032;&#39062;&#25925;&#38556;&#35786;&#26029;&#31561;&#35786;&#26029;&#23376;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#23545;&#27604;&#23398;&#20064;&#30340;&#32479;&#19968;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#20316;&#20026;&#35813;&#24191;&#20041;&#26694;&#26550;&#30340;&#22522;&#30784;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20869;&#37096;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#35782;&#21035;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fault diagnosis is essential in industrial processes for monitoring the conditions of important machines. With the ever-increasing complexity of working conditions and demand for safety during production and operation, different diagnosis methods are required, and more importantly, an integrated fault diagnosis system that can cope with multiple tasks is highly desired. However, the diagnosis subtasks are often studied separately, and the currently available methods still need improvement for such a generalized system. To address this issue, we propose the Generalized Out-of-distribution Fault Diagnosis (GOOFD) framework to integrate diagnosis subtasks, such as fault detection, fault classification, and novel fault diagnosis. Additionally, a unified fault diagnosis method based on internal contrastive learning is put forward to underpin the proposed generalized framework. The method extracts features utilizing the internal contrastive learning technique and then recognizes the outliers
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Implicit Interactive Fleet Learning (IIFL)&#65292;&#23558;&#38544;&#24615;&#31574;&#30053;&#25512;&#24191;&#21040;&#20132;&#20114;&#24335;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#27169;&#24577;&#21644;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15228</link><description>&lt;p&gt;
IIFL: &#38750;&#21516;&#36136;&#20154;&#31867;&#30417;&#30563;&#21592;&#30340;&#38544;&#24615;&#20114;&#21160;&#36710;&#38431;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
IIFL: Implicit Interactive Fleet Learning from Heterogeneous Human Supervisors. (arXiv:2306.15228v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Implicit Interactive Fleet Learning (IIFL)&#65292;&#23558;&#38544;&#24615;&#31574;&#30053;&#25512;&#24191;&#21040;&#20132;&#20114;&#24335;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#27169;&#24577;&#21644;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#24050;&#32463;&#24212;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#20294;&#22312;&#20197;&#19979;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#36935;&#21040;&#22256;&#38590;&#65306;&#65288;1&#65289;&#26426;&#22120;&#20154;&#36935;&#21040;&#35757;&#32451;&#25968;&#25454;&#20013;&#27809;&#26377;&#20195;&#34920;&#30340;&#36793;&#32536;&#26696;&#20363;&#65288;&#20998;&#24067;&#36716;&#31227;&#65289;&#25110;&#65288;2&#65289;&#20154;&#31867;&#28436;&#31034;&#26159;&#24322;&#36136;&#30340;&#65306;&#20363;&#22914;&#65292;&#22312;&#38556;&#30861;&#29289;&#21608;&#22260;&#37319;&#21462;&#19981;&#21516;&#36335;&#24452;&#65288;&#22810;&#27169;&#24577;&#65289;&#12290;&#20132;&#20114;&#24335;&#36710;&#38431;&#23398;&#20064;&#65288;IFL&#65289;&#36890;&#36807;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#25191;&#34892;&#36807;&#31243;&#20013;&#35775;&#38382;&#36828;&#31243;&#20154;&#31867;&#36828;&#31243;&#25805;&#20316;&#21592;&#24182;&#20174;&#20182;&#20204;&#37027;&#37324;&#23398;&#20064;&#26469;&#20943;&#36731;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#20294;&#19981;&#33021;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#38544;&#24615;&#34892;&#20026;&#20811;&#38534;&#65288;IBC&#65289;&#65292;&#23427;&#33021;&#22815;&#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#34920;&#31034;&#22810;&#27169;&#24577;&#28436;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#29992;&#38544;&#24615;&#20114;&#21160;&#36710;&#38431;&#23398;&#20064;&#65288;IIFL&#65289;&#35299;&#20915;&#22810;&#27169;&#24577;&#21644;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#36825;&#26159;&#38544;&#24615;&#31574;&#30053;&#22312;&#20132;&#20114;&#24335;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#31532;&#19968;&#20010;&#25193;&#23637;&#65288;&#21253;&#25324;&#21333;&#26426;&#22120;&#20154;&#12289;&#21333;&#20154;&#31867;&#30340;&#35774;&#32622;&#65289;&#12290;IIFL&#20351;&#29992;Jeffreys&#20998;&#27495;&#30340;&#26032;&#39062;&#24212;&#29992;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning has been applied to a range of robotic tasks, but can struggle when (1) robots encounter edge cases that are not represented in the training data (distribution shift) or (2) the human demonstrations are heterogeneous: taking different paths around an obstacle, for instance (multimodality). Interactive fleet learning (IFL) mitigates distribution shift by allowing robots to access remote human teleoperators during task execution and learn from them over time, but is not equipped to handle multimodality. Recent work proposes Implicit Behavior Cloning (IBC), which is able to represent multimodal demonstrations using energy-based models (EBMs). In this work, we propose addressing both multimodality and distribution shift with Implicit Interactive Fleet Learning (IIFL), the first extension of implicit policies to interactive imitation learning (including the single-robot, single-human setting). IIFL quantifies uncertainty using a novel application of Jeffreys divergence to
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#26816;&#32034;&#21644;&#32463;&#20856;&#30340;&#23398;&#20064;&#25490;&#24207;&#33539;&#20363;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27573;&#33853;&#25490;&#24207;&#25439;&#22833;&#26469;&#35757;&#32451;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#30452;&#25509;&#20248;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#26397;&#30528;&#26368;&#20248;&#35299;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.15222</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#29983;&#25104;&#24335;&#26816;&#32034;&#20013;&#36827;&#34892;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank in Generative Retrieval. (arXiv:2306.15222v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15222
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#26816;&#32034;&#21644;&#32463;&#20856;&#30340;&#23398;&#20064;&#25490;&#24207;&#33539;&#20363;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27573;&#33853;&#25490;&#24207;&#25439;&#22833;&#26469;&#35757;&#32451;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#30452;&#25509;&#20248;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#26397;&#30528;&#26368;&#20248;&#35299;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25991;&#26412;&#26816;&#32034;&#33539;&#20363;&#65292;&#23427;&#23558;&#30456;&#20851;&#27573;&#33853;&#30340;&#26631;&#35782;&#31526;&#23383;&#31526;&#20018;&#29983;&#25104;&#20026;&#26816;&#32034;&#30446;&#26631;&#12290;&#36825;&#31181;&#33539;&#20363;&#21033;&#29992;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20195;&#34920;&#20102;&#19982;&#20256;&#32479;&#30340;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#30340;&#26032;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#24555;&#36895;&#21457;&#23637;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#26041;&#27861;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#21551;&#21457;&#24335;&#20989;&#25968;&#23558;&#39044;&#27979;&#30340;&#26631;&#35782;&#31526;&#36716;&#25442;&#20026;&#27573;&#33853;&#25490;&#24207;&#21015;&#34920;&#65292;&#36825;&#22312;&#29983;&#25104;&#24335;&#26816;&#32034;&#30340;&#23398;&#20064;&#30446;&#26631;&#19982;&#26399;&#26395;&#30340;&#27573;&#33853;&#25490;&#24207;&#30446;&#26631;&#20043;&#38388;&#20135;&#29983;&#20102;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25991;&#26412;&#29983;&#25104;&#30340;&#22266;&#26377;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#22312;&#29983;&#25104;&#24335;&#26816;&#32034;&#20013;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;LTRGR&#65292;&#23427;&#23558;&#29983;&#25104;&#24335;&#26816;&#32034;&#19982;&#32463;&#20856;&#30340;&#23398;&#20064;&#25490;&#24207;&#33539;&#20363;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#27573;&#33853;&#25490;&#24207;&#25439;&#22833;&#35757;&#32451;&#19968;&#20010;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#35813;&#25439;&#22833;&#30452;&#25509;&#20248;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#26397;&#30528;&#26368;&#20248;&#35299;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generation models and represents a new paradigm distinct from traditional learning-to-rank methods. However, despite its rapid development, current generative retrieval methods are still limited. They typically rely on a heuristic function to transform predicted identifiers into a passage rank list, which creates a gap between the learning objective of generative retrieval and the desired passage ranking target. Moreover, the inherent exposure bias problem of text generation also persists in generative retrieval. To address these issues, we propose a novel framework, called LTRGR, that combines generative retrieval with the classical learning-to-rank paradigm. Our approach involves training an autoregressive model using a passage rank loss, which directly optimizes the autoregressive model toward the optimal 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#23427;&#20204;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#65292;&#24182;&#19988;&#36890;&#36807;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15217</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#29992;&#20110;&#22270;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Episode Generation for Graph Meta-learning. (arXiv:2306.15217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#23427;&#20204;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#65292;&#24182;&#19988;&#36890;&#36807;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#20027;&#27969;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#26159;&#22312;&#23384;&#22312;&#22823;&#37327;&#26377;&#26631;&#31614;&#33410;&#28857;&#29992;&#20110;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24320;&#21457;&#30340;&#65292;&#28982;&#32780;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#36825;&#26679;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#20381;&#36182;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#22270;&#20013;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#23613;&#31649;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#19978;&#24456;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#23398;&#20064;&#36890;&#29992;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#27809;&#26377;&#32771;&#34385;&#35201;&#35299;&#20915;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#20854;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#23427;&#20204;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#35299;&#20915;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#22686;&#24378;&#26041;&#27861;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate Unsupervised Episode Generation methods to solve Few-Shot Node-Classification (FSNC) problem via Meta-learning without labels. Dominant meta-learning methodologies for FSNC were developed under the existence of abundant labeled nodes for training, which however may not be possible to obtain in the real-world. Although few studies have been proposed to tackle the label-scarcity problem, they still rely on a limited amount of labeled data, which hinders the full utilization of the information of all nodes in a graph. Despite the effectiveness of Self-Supervised Learning (SSL) approaches on FSNC without labels, they mainly learn generic node embeddings without consideration on the downstream task to be solved, which may limit its performance. In this work, we propose unsupervised episode generation methods to benefit from their generalization ability for FSNC tasks while resolving label-scarcity problem. We first propose a method that utilizes graph augmentat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#33394;&#24425;&#31070;&#32463;&#34920;&#31034;&#27861;&#65288;Polyner&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;CT&#25104;&#20687;&#20013;&#23384;&#22312;&#37329;&#23646;&#20266;&#24433;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;Polyner&#36890;&#36807;&#24314;&#27169;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#20934;&#30830;&#27169;&#25311;CT&#37319;&#38598;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24674;&#22797;&#21407;&#22987;&#29289;&#20307;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;Polyner&#22312;&#37329;&#23646;&#20266;&#24433;&#20943;&#23569;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15203</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22810;&#33394;&#24425;&#31070;&#32463;&#34920;&#31034;&#27861;&#29992;&#20110;CT&#37329;&#23646;&#20266;&#24433;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Polychromatic Neural Representation for CT Metal Artifact Reduction. (arXiv:2306.15203v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#33394;&#24425;&#31070;&#32463;&#34920;&#31034;&#27861;&#65288;Polyner&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;CT&#25104;&#20687;&#20013;&#23384;&#22312;&#37329;&#23646;&#20266;&#24433;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;Polyner&#36890;&#36807;&#24314;&#27169;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#20934;&#30830;&#27169;&#25311;CT&#37319;&#38598;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24674;&#22797;&#21407;&#22987;&#29289;&#20307;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;Polyner&#22312;&#37329;&#23646;&#20266;&#24433;&#20943;&#23569;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#22522;&#20110;&#23618;&#26512;&#26415;&#30340;&#31070;&#32463;&#37325;&#24314;&#25216;&#26415;&#65288;&#22914;NeRF&#65292;NeAT&#21644;NeRP&#65289;&#22312;&#21307;&#23398;&#25104;&#20687;&#26041;&#38754;&#24050;&#32463;&#23637;&#31034;&#20986;&#29420;&#29305;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#33394;&#24425;&#31070;&#32463;&#34920;&#31034;&#27861;&#65288;Polyner&#65289;&#26469;&#35299;&#20915;CT&#25104;&#20687;&#20013;&#23384;&#22312;&#20154;&#20307;&#37329;&#23646;&#26893;&#20837;&#29289;&#26102;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#37329;&#23646;&#20266;&#24433;&#26159;&#30001;&#20110;X&#23556;&#32447;&#33021;&#35889;&#19981;&#21516;&#33021;&#37327;&#32423;&#37329;&#23646;&#30340;&#34928;&#20943;&#31995;&#25968;&#21095;&#28872;&#21464;&#21270;&#32780;&#20135;&#29983;&#30340;&#65292;&#23548;&#33268;CT&#27979;&#37327;&#20013;&#30340;&#38750;&#32447;&#24615;&#37329;&#23646;&#25928;&#24212;&#12290;&#22240;&#27492;&#65292;&#20174;&#21463;&#37329;&#23646;&#24433;&#21709;&#30340;&#27979;&#37327;&#20013;&#37325;&#24314;CT&#22270;&#20687;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#37329;&#23646;&#20266;&#24433;&#20943;&#23569;&#65288;MAR&#65289;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#32463;&#39564;&#27169;&#22411;&#23548;&#33268;&#20449;&#21495;&#25439;&#22833;&#21644;&#24378;&#28872;&#30340;&#28151;&#21472;&#37325;&#24314;&#12290;Polyner&#20174;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;&#30340;&#35282;&#24230;&#23545;MAR&#38382;&#39064;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#22810;&#33394;&#24425;&#27491;&#28436;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#27169;&#25311;&#38750;&#32447;&#24615;CT&#37319;&#38598;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25454;&#27492;&#35774;&#35745;&#19968;&#20010;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#37329;&#23646;&#20266;&#24433;&#30340;CT&#25237;&#24433;&#22270;&#20013;&#24674;&#22797;&#20986;&#21407;&#22987;&#30340;&#29289;&#20307;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#23454;&#38469;CT&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;Polyner&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging neural reconstruction techniques based on tomography (e.g., NeRF, NeAT, and NeRP) have started showing unique capabilities in medical imaging. In this work, we present a novel Polychromatic neural representation (Polyner) to tackle the challenging problem of CT imaging when metallic implants exist within the human body. The artifacts arise from the drastic variation of metal's attenuation coefficients at various energy levels of the X-ray spectrum, leading to a nonlinear metal effect in CT measurements. Reconstructing CT images from metal-affected measurements hence poses a complicated nonlinear inverse problem where empirical models adopted in previous metal artifact reduction (MAR) approaches lead to signal loss and strongly aliased reconstructions. Polyner instead models the MAR problem from a nonlinear inverse problem perspective. Specifically, we first derive a polychromatic forward model to accurately simulate the nonlinear CT acquisition process. Then, we incorporate ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21069;&#21521;&#35757;&#32451;&#26041;&#24335;&#19979;&#35757;&#32451;&#28145;&#24230;&#21333;&#31867;&#30446;&#26631;&#20989;&#25968;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#36825;&#20123;&#20989;&#25968;&#21487;&#20197;&#22788;&#29702;&#21160;&#24577;&#32593;&#32476;&#22823;&#23567;&#65292;&#20026;&#26080;&#32541;&#22312;&#32447;&#35757;&#32451;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2306.15188</link><description>&lt;p&gt;
&#19968;&#31867;&#31995;&#32479;&#23436;&#32654;&#36866;&#29992;&#20110;&#21069;&#21521;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-class systems seamlessly fit in the forward-forward algorithm. (arXiv:2306.15188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21069;&#21521;&#35757;&#32451;&#26041;&#24335;&#19979;&#35757;&#32451;&#28145;&#24230;&#21333;&#31867;&#30446;&#26631;&#20989;&#25968;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#36825;&#20123;&#20989;&#25968;&#21487;&#20197;&#22788;&#29702;&#21160;&#24577;&#32593;&#32476;&#22823;&#23567;&#65292;&#20026;&#26080;&#32541;&#22312;&#32447;&#35757;&#32451;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#21521;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26356;&#26032;&#26435;&#37325;&#65292;&#36880;&#23618;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#31435;&#21363;&#38477;&#20302;&#20102;&#20869;&#23384;&#38656;&#27714;&#65292;&#21487;&#33021;&#24102;&#26469;&#26356;&#22810;&#22909;&#22788;&#65292;&#27604;&#22914;&#26080;&#32541;&#22312;&#32447;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#25439;&#22833;&#65288;&#8220;&#22909;&#24230;&#8221;&#65289;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#21487;&#20197;&#22312;&#27599;&#20010;&#23618;&#30340;&#28608;&#27963;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#20123;&#23618;&#30340;&#21442;&#25968;&#22823;&#23567;&#21487;&#20197;&#26681;&#25454;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#21270;&#32780;&#21464;&#21270;&#12290;&#22312;&#24320;&#21019;&#24615;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22909;&#24230;&#20989;&#25968;&#26469;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65307;&#28982;&#32780;&#65292;&#22914;&#26524;&#23558;&#20854;&#32622;&#20110;&#19968;&#20010;&#21333;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#23601;&#26080;&#38656;&#24320;&#21019;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22240;&#20026;&#36825;&#20123;&#20989;&#25968;&#26412;&#36523;&#21487;&#20197;&#22788;&#29702;&#21160;&#24577;&#32593;&#32476;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21069;&#21521;&#35757;&#32451;&#26041;&#24335;&#19979;&#35757;&#32451;&#28145;&#24230;&#21333;&#31867;&#30446;&#26631;&#20989;&#25968;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/MichaelHopwood/ForwardForwardOneclass} &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The forward-forward algorithm presents a new method of training neural networks by updating weights during an inference, performing parameter updates for each layer individually. This immediately reduces memory requirements during training and may lead to many more benefits, like seamless online training. This method relies on a loss ("goodness") function that can be evaluated on the activations of each layer, of which can have a varied parameter size, depending on the hyperparamaterization of the network. In the seminal paper, a goodness function was proposed to fill this need; however, if placed in a one-class problem context, one need not pioneer a new loss because these functions can innately handle dynamic network sizes. In this paper, we investigate the performance of deep one-class objective functions when trained in a forward-forward fashion. The code is available at \url{https://github.com/MichaelHopwood/ForwardForwardOneclass}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;AutoTruss&#26694;&#26550;&#65292;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#39640;&#25928;&#29983;&#25104;&#36731;&#22411;&#21644;&#21512;&#27861;&#30340;&#26689;&#26550;&#24067;&#23616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;AutoTruss&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#25253;&#36947;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15182</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#26689;&#26550;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Automatic Truss Design with Reinforcement Learning. (arXiv:2306.15182v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;AutoTruss&#26694;&#26550;&#65292;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#39640;&#25928;&#29983;&#25104;&#36731;&#22411;&#21644;&#21512;&#27861;&#30340;&#26689;&#26550;&#24067;&#23616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;AutoTruss&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#25253;&#36947;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26689;&#26550;&#24067;&#23616;&#35774;&#35745;&#26159;&#24314;&#31569;&#34892;&#19994;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#23547;&#25214;&#19968;&#20010;&#28385;&#36275;&#25152;&#26377;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#30340;&#36731;&#22411;&#26689;&#26550;&#24067;&#23616;&#12290;&#29983;&#25104;&#26368;&#20248;&#24067;&#23616;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#31351;&#20030;&#25628;&#32034;&#27714;&#35299;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#12290;&#23558;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#26689;&#26550;&#24067;&#23616;&#35774;&#35745;&#20063;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#22312;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#19979;&#21482;&#26377;&#19968;&#20010;&#24456;&#23567;&#30340;&#24067;&#23616;&#31354;&#38388;&#26159;&#26377;&#25928;&#30340;&#65292;&#23548;&#33268;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#22870;&#21169;&#38750;&#24120;&#31232;&#30095;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AutoTruss&#65292;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#36731;&#22411;&#21644;&#21512;&#27861;&#30340;&#26689;&#26550;&#24067;&#23616;&#12290;AutoTruss&#39318;&#20808;&#37319;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26469;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#21512;&#27861;&#24067;&#23616;&#65292;&#28982;&#21518;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#36880;&#27493;&#20248;&#21270;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;2D&#21644;3D&#35774;&#32622;&#19979;&#30340;&#27969;&#34892;&#26689;&#26550;&#24067;&#23616;&#35774;&#35745;&#27979;&#35797;&#26696;&#20363;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#12290;AutoTruss&#22312;&#34920;&#29616;&#19978;&#36229;&#36807;&#20102;&#26368;&#20339;&#25253;&#36947;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Truss layout design, namely finding a lightweight truss layout satisfying all the physical constraints, is a fundamental problem in the building industry. Generating the optimal layout is a challenging combinatorial optimization problem, which can be extremely expensive to solve by exhaustive search. Directly applying end-to-end reinforcement learning (RL) methods to truss layout design is infeasible either, since only a tiny portion of the entire layout space is valid under the physical constraints, leading to particularly sparse rewards for RL training. In this paper, we develop AutoTruss, a two-stage framework to efficiently generate both lightweight and valid truss layouts. AutoTruss first adopts Monte Carlo tree search to discover a diverse collection of valid layouts. Then RL is applied to iteratively refine the valid solutions. We conduct experiments and ablation studies in popular truss layout design test cases in both 2D and 3D settings. AutoTruss outperforms the best-reported
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#24182;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15156</link><description>&lt;p&gt;
&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Learning non-Markovian Decision-Making from State-only Sequences. (arXiv:2306.15156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#24182;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#27169;&#20223;&#23398;&#20064;&#20551;&#35774;&#33021;&#22815;&#33719;&#24471;&#23637;&#31034;&#32773;&#30340;&#21160;&#20316;&#65292;&#20294;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#36825;&#20123;&#21160;&#20316;&#36890;&#24120;&#26080;&#27861;&#35266;&#27979;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#34892;&#20026;&#21487;&#33021;&#20559;&#31163;&#26631;&#20934;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;nMDP&#65289;&#20013;&#20165;&#29366;&#24577;&#24207;&#21015;&#30340;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#65292;&#20854;&#20013;&#31574;&#30053;&#26159;&#28508;&#22312;&#29366;&#24577;&#36716;&#31227;&#29983;&#25104;&#22120;&#30340;&#33021;&#37327;&#20808;&#39564;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#20808;&#39564;&#36827;&#34892;&#30701;&#26399;MCMC&#37319;&#26679;&#21644;&#23545;&#21518;&#39564;&#36827;&#34892;&#37325;&#35201;&#24615;&#37319;&#26679;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#21363;&#26080;&#27169;&#22411;&#31574;&#30053;&#25191;&#34892;&#31561;&#20215;&#20110;&#20808;&#39564;&#37319;&#26679;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#21017;&#26159;&#20174;&#31574;&#30053;&#21021;&#22987;&#21270;&#30340;&#21518;&#39564;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20855;&#26377;&#38750;&#39532;&#23572;&#31185;&#22827;&#29305;&#24449;&#30340;&#21407;&#22411;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional imitation learning assumes access to the actions of demonstrators, but these motor signals are often non-observable in naturalistic settings. Additionally, sequential decision-making behaviors in these settings can deviate from the assumptions of a standard Markov Decision Process (MDP). To address these challenges, we explore deep generative modeling of state-only sequences with non-Markov Decision Process (nMDP), where the policy is an energy-based prior in the latent space of the state transition generator. We develop maximum likelihood estimation to achieve model-based imitation, which involves short-run MCMC sampling from the prior and importance sampling for the posterior. The learned model enables \textit{decision-making as inference}: model-free policy execution is equivalent to prior sampling, model-based planning is posterior sampling initialized from the policy. We demonstrate the efficacy of the proposed method in a prototypical path planning task with non-Mark
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#22823;&#37327;&#26377;&#26631;&#35760;&#33410;&#28857;&#30340;&#31867;&#21035;&#20013;&#25277;&#21462;&#21487;&#36801;&#31227;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#33410;&#28857;&#30340;&#20854;&#20182;&#31867;&#21035;&#65292;&#23398;&#20064;&#36890;&#29992;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2306.15154</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#23545;&#27604;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Meta-Learning for Few-shot Node Classification. (arXiv:2306.15154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15154
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#22823;&#37327;&#26377;&#26631;&#35760;&#33410;&#28857;&#30340;&#31867;&#21035;&#20013;&#25277;&#21462;&#21487;&#36801;&#31227;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#33410;&#28857;&#30340;&#20854;&#20182;&#31867;&#21035;&#65292;&#23398;&#20064;&#36890;&#29992;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#26159;&#22312;&#21482;&#26377;&#26377;&#38480;&#26631;&#35760;&#33410;&#28857;&#20316;&#20026;&#21442;&#32771;&#30340;&#22270;&#19978;&#20026;&#33410;&#28857;&#39044;&#27979;&#26631;&#31614;&#30340;&#20219;&#21153;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#25366;&#25496;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#38024;&#23545;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#21033;&#29992;&#22823;&#37327;&#30340;episode&#20174;&#26377;&#22823;&#37327;&#26631;&#35760;&#33410;&#28857;&#30340;&#31867;&#21035;&#20013;&#25277;&#21462;&#21487;&#36801;&#31227;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#36825;&#20123;&#30693;&#35782;&#25512;&#24191;&#21040;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#33410;&#28857;&#30340;&#20854;&#20182;&#31867;&#21035;&#12290;&#26412;&#36136;&#19978;&#65292;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#21487;&#25512;&#24191;&#21040;&#19981;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;GNN&#32534;&#30721;&#22120;&#24517;&#39035;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#21516;&#26102;&#36824;&#35201;&#23545;&#21516;&#19968;&#31867;&#21035;&#20013;&#30340;&#33410;&#28857;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#31867;&#20869;&#21644;&#31867;&#38388;&#30340;&#33410;&#28857;&#23884;&#20837;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot node classification, which aims to predict labels for nodes on graphs with only limited labeled nodes as references, is of great significance in real-world graph mining tasks. Particularly, in this paper, we refer to the task of classifying nodes in classes with a few labeled nodes as the few-shot node classification problem. To tackle such a label shortage issue, existing works generally leverage the meta-learning framework, which utilizes a number of episodes to extract transferable knowledge from classes with abundant labeled nodes and generalizes the knowledge to other classes with limited labeled nodes. In essence, the primary aim of few-shot node classification is to learn node embeddings that are generalizable across different classes. To accomplish this, the GNN encoder must be able to distinguish node embeddings between different classes, while also aligning embeddings for nodes in the same class. Thus, in this work, we propose to consider both the intra-class and int
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#36712;&#36857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#22312;&#22266;&#23450;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#24456;&#22909;&#65292;&#20294;&#22312;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#21364;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#24573;&#35270;&#20102;&#21160;&#21147;&#23398;&#24046;&#36317;&#21644;&#35745;&#31639;&#25928;&#29575;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.15136</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#20013;&#30495;&#27491;&#37325;&#35201;&#30340;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Truly Matters in Trajectory Prediction for Autonomous Driving?. (arXiv:2306.15136v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15136
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#36712;&#36857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#22312;&#22266;&#23450;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#24456;&#22909;&#65292;&#20294;&#22312;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#21364;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#24573;&#35270;&#20102;&#21160;&#21147;&#23398;&#24046;&#36317;&#21644;&#35745;&#31639;&#25928;&#29575;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#36712;&#36857;&#39044;&#27979;&#22312;&#30830;&#20445;&#23433;&#20840;&#21644;&#20419;&#36827;&#24179;&#31283;&#23548;&#33322;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#22266;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#22120;&#20934;&#30830;&#24615;&#19982;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#39550;&#39542;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#28304;&#20110;&#24403;&#21069;&#36712;&#36857;&#39044;&#27979;&#35780;&#20272;&#21327;&#35758;&#20013;&#24573;&#35270;&#20102;&#20004;&#20010;&#22240;&#32032;&#65306;1&#65289;&#25968;&#25454;&#38598;&#19982;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20043;&#38388;&#30340;&#21160;&#21147;&#23398;&#24046;&#36317;&#65307;2&#65289;&#39044;&#27979;&#22120;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#39044;&#27979;&#31639;&#27861;&#24433;&#21709;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#65292;&#36827;&#32780;&#25913;&#21464;&#36947;&#36335;&#19978;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#20114;&#21160;&#20135;&#29983;&#20102;&#38024;&#23545;&#39044;&#27979;&#22120;&#30340;&#29305;&#23450;&#21160;&#21147;&#23398;&#65292;&#30452;&#25509;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#12290;&#30001;&#20110;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#21453;&#24212;&#22312;&#25968;&#25454;&#38598;&#19978;&#26159;&#39044;&#20808;&#30830;&#23450;&#30340;&#65292;&#22240;&#27492;&#22312;&#22266;&#23450;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#36827;&#34892;&#30340;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#21160;&#21147;&#23398;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#20165;&#20851;&#27880;&#20934;&#30830;&#24615;&#26080;&#27861;&#28385;&#36275;&#23545;&#39044;&#27979;&#22120;&#21160;&#24577;&#34892;&#20026;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the autonomous driving system, trajectory prediction plays a vital role in ensuring safety and facilitating smooth navigation. However, we observe a substantial discrepancy between the accuracy of predictors on fixed datasets and their driving performance when used in downstream tasks. This discrepancy arises from two overlooked factors in the current evaluation protocols of trajectory prediction: 1) the dynamics gap between the dataset and real driving scenario; and 2) the computational efficiency of predictors. In real-world scenarios, prediction algorithms influence the behavior of autonomous vehicles, which, in turn, alter the behaviors of other agents on the road. This interaction results in predictor-specific dynamics that directly impact prediction results. As other agents' responses are predetermined on datasets, a significant dynamics gap arises between evaluations conducted on fixed datasets and actual driving scenarios. Furthermore, focusing solely on accuracy fails to ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36719;&#20214;&#19987;&#19994;&#20154;&#21592;&#23545;&#31639;&#27861;&#31181;&#26063;&#27495;&#35270;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#19968;&#20123;&#36719;&#20214;&#31995;&#32479;&#23545;&#40657;&#20154;&#23384;&#22312;&#27495;&#35270;&#65292;&#23548;&#33268;&#40657;&#20154;&#22312;&#20351;&#29992;&#22522;&#20110;&#25216;&#26415;&#30340;&#26381;&#21153;&#26041;&#38754;&#38754;&#20020;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;</title><link>http://arxiv.org/abs/2306.15133</link><description>&lt;p&gt;
&#36719;&#20214;&#19987;&#19994;&#20154;&#21592;&#23545;&#31639;&#27861;&#31181;&#26063;&#27495;&#35270;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
The Perspective of Software Professionals on Algorithmic Racism. (arXiv:2306.15133v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36719;&#20214;&#19987;&#19994;&#20154;&#21592;&#23545;&#31639;&#27861;&#31181;&#26063;&#27495;&#35270;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#19968;&#20123;&#36719;&#20214;&#31995;&#32479;&#23545;&#40657;&#20154;&#23384;&#22312;&#27495;&#35270;&#65292;&#23548;&#33268;&#40657;&#20154;&#22312;&#20351;&#29992;&#22522;&#20110;&#25216;&#26415;&#30340;&#26381;&#21153;&#26041;&#38754;&#38754;&#20020;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#12290;&#31639;&#27861;&#31181;&#26063;&#27495;&#35270;&#26159;&#25351;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#26681;&#25454;&#29992;&#25143;&#30340;&#31181;&#26063;&#32780;&#38480;&#21046;&#20854;&#34892;&#20026;&#12290;&#36817;&#26399;&#65292;&#26377;&#25253;&#36947;&#31216;&#21508;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#36719;&#20214;&#31995;&#32479;&#23545;&#40657;&#20154;&#36827;&#34892;&#27495;&#35270;&#65292;&#21407;&#22240;&#21487;&#33021;&#26159;&#20351;&#29992;&#20102;&#24102;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#65292;&#25110;&#32773;&#26159;&#30001;&#36719;&#20214;&#19987;&#19994;&#20154;&#21592;&#22312;&#20195;&#30721;&#20013;&#20256;&#25773;&#20102;&#20559;&#35265;&#12290;&#32467;&#26524;&#65292;&#40657;&#20154;&#22312;&#20351;&#29992;&#22522;&#20110;&#25216;&#26415;&#30340;&#26381;&#21153;&#65288;&#22914;&#20303;&#25151;&#12289;&#38134;&#34892;&#21644;&#25191;&#27861;&#65289;&#26041;&#38754;&#38754;&#20020;&#19981;&#20844;&#24179;&#30340;&#24453;&#36935;&#12290;&#30446;&#26631;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#36719;&#20214;&#19987;&#19994;&#20154;&#21592;&#30340;&#35282;&#24230;&#25506;&#32034;&#31639;&#27861;&#31181;&#26063;&#27495;&#35270;&#12290;&#26041;&#27861;&#12290;&#24212;&#29992;&#35843;&#26597;&#38382;&#21367;&#26469;&#25506;&#32034;&#36719;&#20214;&#20174;&#19994;&#20154;&#21592;&#23545;&#31639;&#27861;&#31181;&#26063;&#27495;&#35270;&#30340;&#29702;&#35299;&#65292;&#20351;&#29992;&#25551;&#36848;&#24615;&#32479;&#35745;&#21644;&#32534;&#30721;&#25216;&#26415;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#12290;&#32467;&#26524;&#12290;&#25105;&#20204;&#23545;73&#21517;&#36719;&#20214;&#19987;&#19994;&#20154;&#21592;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#35752;&#35770;&#20102;&#20182;&#20204;&#23545;&#36719;&#20214;&#24320;&#21457;&#20013;&#31639;&#27861;&#31181;&#26063;&#27495;&#35270;&#30340;&#29702;&#35299;&#21644;&#35266;&#28857;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Context. Algorithmic racism is the term used to describe the behavior of technological solutions that constrains users based on their ethnicity. Lately, various data-driven software systems have been reported to discriminate against Black people, either for the use of biased data sets or due to the prejudice propagated by software professionals in their code. As a result, Black people are experiencing disadvantages in accessing technology-based services, such as housing, banking, and law enforcement. Goal. This study aims to explore algorithmic racism from the perspective of software professionals. Method. A survey questionnaire was applied to explore the understanding of software practitioners on algorithmic racism, and data analysis was conducted using descriptive statistics and coding techniques. Results. We obtained answers from a sample of 73 software professionals discussing their understanding and perspectives on algorithmic racism in software development. Our results demonstrat
&lt;/p&gt;</description></item><item><title>MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15128</link><description>&lt;p&gt;
MIMIC: &#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MIMIC: Masked Image Modeling with Image Correspondences. (arXiv:2306.15128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15128
&lt;/p&gt;
&lt;p&gt;
MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20687;&#32032;&#32423;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#8212;&#8212;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#8212;&#8212;&#22914;&#20170;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#31579;&#36873;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20165;&#36890;&#36807;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#24102;&#26377;&#27880;&#37322;&#30340;3D&#32593;&#26684;&#12289;&#28857;&#20113;&#21644;&#30456;&#26426;&#21442;&#25968;&#31579;&#36873;&#32780;&#26469;&#65292;&#24182;&#19981;&#20855;&#22791;&#22810;&#35270;&#35282;&#22330;&#26223;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#31579;&#36873;&#26426;&#21046;&#12290;&#25105;&#20204;&#20174;&#24320;&#28304;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#30340;3D&#29615;&#22659;&#20013;&#25366;&#25496;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;MIMIC-1M(&#21253;&#21547;1.3M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#21644;MIMIC-3M(&#21253;&#21547;3.1M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#30446;&#26631;&#65292;&#23637;&#31034;&#20102;&#20197;&#19979;&#21457;&#29616;&#65306;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;MIMIC-3M&#35757;&#32451;&#30340;&#34920;&#31034;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#28145;&#24230;&#20272;&#35745;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#34920;&#38754;&#27861;&#32447;&#21644;&#23039;&#24577;&#20272;&#35745;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many pixelwise dense prediction tasks-depth estimation and semantic segmentation in computer vision today rely on pretrained image representations. Therefore, curating effective pretraining datasets is vital. Unfortunately, the effective pretraining datasets are those with multi-view scenes and have only been curated using annotated 3D meshes, point clouds, and camera parameters from simulated environments. We propose a dataset-curation mechanism that does not require any annotations. We mine two datasets: MIMIC-1M with 1.3M and MIMIC-3M with 3.1M multi-view image pairs from open-sourced video datasets and from synthetic 3D environments. We train multiple self-supervised models with different masked image modeling objectives to showcase the following findings: Representations trained on MIMIC-3M outperform those mined using annotations on multiple downstream tasks, including depth estimation, semantic segmentation, surface normals, and pose estimation. They also outperform representati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#20027;&#27969;&#30340;&#36719;&#20214;&#26041;&#27861;&#35770;&#26469;&#35299;&#20915;&#25361;&#25112;&#30340;&#21442;&#32771;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#26102;&#20195;&#30340;&#38656;&#27714;&#65292;&#30830;&#23450;&#20102;23&#20010;&#20851;&#38190;&#30340;&#36136;&#37327;&#23646;&#24615;&#12290;&#36890;&#36807;&#23545;&#36817;&#26399;&#25991;&#29486;&#20013;&#30340;&#20505;&#36873;&#20307;&#31995;&#32467;&#26500;&#36827;&#34892;&#35780;&#20272;&#65292;&#26368;&#32456;&#35752;&#35770;&#20102;&#36808;&#21521;&#20840;&#38754;&#21442;&#32771;&#20307;&#31995;&#32467;&#26500;&#30340;&#19979;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.15124</link><description>&lt;p&gt;
&#12298;&#35782;&#21035;&#21644;&#24041;&#22266;&#30693;&#35782;&#24037;&#31243;&#38656;&#27714;&#12299;
&lt;/p&gt;
&lt;p&gt;
Identifying and Consolidating Knowledge Engineering Requirements. (arXiv:2306.15124v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#20027;&#27969;&#30340;&#36719;&#20214;&#26041;&#27861;&#35770;&#26469;&#35299;&#20915;&#25361;&#25112;&#30340;&#21442;&#32771;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#26102;&#20195;&#30340;&#38656;&#27714;&#65292;&#30830;&#23450;&#20102;23&#20010;&#20851;&#38190;&#30340;&#36136;&#37327;&#23646;&#24615;&#12290;&#36890;&#36807;&#23545;&#36817;&#26399;&#25991;&#29486;&#20013;&#30340;&#20505;&#36873;&#20307;&#31995;&#32467;&#26500;&#36827;&#34892;&#35780;&#20272;&#65292;&#26368;&#32456;&#35752;&#35770;&#20102;&#36808;&#21521;&#20840;&#38754;&#21442;&#32771;&#20307;&#31995;&#32467;&#26500;&#30340;&#19979;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24037;&#31243;&#26159;&#21019;&#24314;&#21644;&#32500;&#25252;&#30693;&#35782;&#20135;&#29983;&#31995;&#32479;&#30340;&#36807;&#31243;&#12290;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#21382;&#21490;&#20013;&#65292;&#30693;&#35782;&#24037;&#31243;&#24037;&#20316;&#27969;&#31243;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#34987;&#35748;&#20026;&#23545;&#21487;&#38752;&#30340;&#26234;&#33021;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#24037;&#31243;&#30340;&#26684;&#23616;&#21457;&#29983;&#20102;&#21464;&#21270;&#65292;&#20986;&#29616;&#20102;&#22235;&#20010;&#25361;&#25112;&#65306;&#26410;&#35299;&#20915;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#38656;&#27714;&#65292;&#25216;&#26415;&#19981;&#21305;&#37197;&#65292;&#26032;&#32452;&#32455;&#30340;&#37319;&#29992;&#38556;&#30861;&#65292;&#20197;&#21450;&#19982;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#30340;&#19981;&#21327;&#35843;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20351;&#29992;&#20027;&#27969;&#30340;&#36719;&#20214;&#26041;&#27861;&#35770;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#24320;&#21457;&#19968;&#20010;&#21442;&#32771;&#20307;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#26102;&#20195;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;23&#20010;&#35780;&#20272;&#21442;&#32771;&#20307;&#31995;&#32467;&#26500;&#30340;&#37325;&#35201;&#36136;&#37327;&#23646;&#24615;&#12290;&#25105;&#20204;&#26681;&#25454;&#36825;&#20123;&#23646;&#24615;&#35780;&#20272;&#20102;&#36817;&#26399;&#25991;&#29486;&#20013;&#30340;&#19977;&#31181;&#20505;&#36873;&#20307;&#31995;&#32467;&#26500;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36808;&#21521;&#19968;&#20010;&#20840;&#38754;&#21442;&#32771;&#20307;&#31995;&#32467;&#26500;&#30340;&#19979;&#19968;&#27493;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Knowledge engineering is the process of creating and maintaining knowledge-producing systems. Throughout the history of computer science and AI, knowledge engineering workflows have been widely used because high-quality knowledge is assumed to be crucial for reliable intelligent agents. However, the landscape of knowledge engineering has changed, presenting four challenges: unaddressed stakeholder requirements, mismatched technologies, adoption barriers for new organizations, and misalignment with software engineering practices. In this paper, we propose to address these challenges by developing a reference architecture using a mainstream software methodology. By studying the requirements of different stakeholders and eras, we identify 23 essential quality attributes for evaluating reference architectures. We assess three candidate architectures from recent literature based on these attributes. Finally, we discuss the next steps towards a comprehensive reference architecture, including
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#20351;&#29992;AI&#36741;&#21161;&#29983;&#25104;&#33021;&#21147;&#26469;&#29983;&#25104;&#22522;&#26412;&#25968;&#20540;&#20869;&#26680;&#30340;&#25928;&#26524;&#65292;&#24182;&#27979;&#35797;&#20102;&#22810;&#31181;&#32534;&#31243;&#27169;&#22411;&#19979;&#29983;&#25104;&#30340;&#20869;&#26680;&#20195;&#30721;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OpenAI Codex&#22312;C++&#20013;&#30340;&#36755;&#20986;&#19982;&#37319;&#32435;&#24230;&#21644;&#25104;&#29087;&#24230;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2306.15121</link><description>&lt;p&gt;
&#23545;&#20110;HPC&#24182;&#34892;&#32534;&#31243;&#27169;&#22411;&#20869;&#26680;&#29983;&#25104;&#30340;OpenAI Codex&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of OpenAI Codex for HPC Parallel Programming Models Kernel Generation. (arXiv:2306.15121v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#20351;&#29992;AI&#36741;&#21161;&#29983;&#25104;&#33021;&#21147;&#26469;&#29983;&#25104;&#22522;&#26412;&#25968;&#20540;&#20869;&#26680;&#30340;&#25928;&#26524;&#65292;&#24182;&#27979;&#35797;&#20102;&#22810;&#31181;&#32534;&#31243;&#27169;&#22411;&#19979;&#29983;&#25104;&#30340;&#20869;&#26680;&#20195;&#30721;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OpenAI Codex&#22312;C++&#20013;&#30340;&#36755;&#20986;&#19982;&#37319;&#32435;&#24230;&#21644;&#25104;&#29087;&#24230;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#20013;&#22522;&#26412;&#25968;&#20540;&#20869;&#26680;&#65288;&#21253;&#25324;AXPY&#65292;GEMV&#65292;GEMM&#65292;SpMV&#65292;Jacobi Stencil&#21644;CG&#65289;&#19978;&#30340;AI&#36741;&#21161;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#22810;&#31181;&#35821;&#35328;&#25903;&#25345;&#30340;&#32534;&#31243;&#27169;&#22411;&#65288;&#21253;&#25324;C++&#65288;&#22914;OpenMP [&#21253;&#25324;&#21368;&#36733;]&#65292;OpenACC&#65292;Kokkos&#65292;SyCL&#65292;CUDA&#21644;HIP&#65289;&#65292;Fortran&#65288;&#22914;OpenMP [&#21253;&#25324;&#21368;&#36733;]&#21644;OpenACC&#65289;&#65292;Python&#65288;&#22914;numba&#65292;Numba&#65292;cuPy&#21644;pyCUDA&#65289;&#21644;Julia&#65288;&#22914;Threads&#65292;CUDA.jl&#65292;AMDGPU.jl&#21644;KernelAbstractions.jl&#65289;&#65289;&#36827;&#34892;&#20102;&#29983;&#25104;&#20869;&#26680;&#20195;&#30721;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;OpenAI Codex&#22312;2023&#24180;4&#26376;&#20043;&#21518;&#22312;Visual Studio Code&#20013;&#25552;&#20379;&#30340;GitHub Copilot&#21151;&#33021;&#26469;&#29983;&#25104;&#22823;&#37327;&#30340;&#23454;&#29616;&#65292;&#21482;&#38656;&#32473;&#20986;&#31616;&#21333;&#30340;&lt;kernel&gt; + &lt;programming model&gt; + &lt;optional hints&gt;&#25552;&#31034;&#21464;&#20307;&#12290;&#20026;&#20102;&#23450;&#37327;&#21644;&#27604;&#36739;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27599;&#20010;&#25552;&#31034;&#30340;&#21021;&#22987;10&#20010;&#24314;&#35758;&#20026;&#22522;&#30784;&#30340;&#29087;&#32451;&#24230;&#25351;&#26631;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OpenAI Codex&#23545;&#20110;C++&#30340;&#36755;&#20986;&#19982;&#37319;&#32435;&#24230;&#21644;&#25104;&#29087;&#24230;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluate AI-assisted generative capabilities on fundamental numerical kernels in high-performance computing (HPC), including AXPY, GEMV, GEMM, SpMV, Jacobi Stencil, and CG. We test the generated kernel codes for a variety of language-supported programming models, including (1) C++ (e.g., OpenMP [including offload], OpenACC, Kokkos, SyCL, CUDA, and HIP), (2) Fortran (e.g., OpenMP [including offload] and OpenACC), (3) Python (e.g., numba, Numba, cuPy, and pyCUDA), and (4) Julia (e.g., Threads, CUDA.jl, AMDGPU.jl, and KernelAbstractions.jl). We use the GitHub Copilot capabilities powered by OpenAI Codex available in Visual Studio Code as of April 2023 to generate a vast amount of implementations given simple &lt;kernel&gt; + &lt;programming model&gt; + &lt;optional hints&gt; prompt variants. To quantify and compare the results, we propose a proficiency metric around the initial 10 suggestions given for each prompt. Results suggest that the OpenAI Codex outputs for C++ correlate with the adoption and mat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#34892;&#20154;&#26816;&#27979;&#20013;&#30340;&#36234;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#24377;&#24615;&#26435;&#37325;&#21512;&#24182;&#65292;&#20351;&#20854;&#22312;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#24182;&#20445;&#25345;&#22312;&#26087;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;9%&#21644;18%&#30340;&#35823;&#26816;&#29575;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.15117</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#29992;&#20110;&#36234;&#22495;&#34892;&#20154;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Continual Learning for Out-of-Distribution Pedestrian Detection. (arXiv:2306.15117v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#34892;&#20154;&#26816;&#27979;&#20013;&#30340;&#36234;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#24377;&#24615;&#26435;&#37325;&#21512;&#24182;&#65292;&#20351;&#20854;&#22312;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#24182;&#20445;&#25345;&#22312;&#26087;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;9%&#21644;18%&#30340;&#35823;&#26816;&#29575;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#34892;&#20154;&#26816;&#27979;&#20013;&#30340;&#36234;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#34892;&#20154;&#26816;&#27979;&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#23545;&#25512;&#29702;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#20173;&#28982;&#25935;&#24863;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#24182;&#20462;&#25913;&#20102;&#24377;&#24615;&#26435;&#37325;&#21512;&#24182;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#20027;&#24178;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#65292;&#20197;&#24809;&#32602;&#23545;&#21021;&#22987;&#23398;&#20064;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#21457;&#29983;&#21464;&#21270;&#30340;&#27169;&#22411;&#26435;&#37325;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#19968;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#24182;&#22312;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#23398;&#20064;&#26032;&#30340;&#20998;&#24067;&#24182;&#20445;&#25345;&#22312;&#20808;&#21069;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;CrowdHuman&#21644;CityPersons&#19978;&#36827;&#34892;&#20102;&#36328;&#25968;&#25454;&#38598;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20998;&#21035;&#22312;CrowdHuman&#21644;CityPersons&#25968;&#25454;&#38598;&#19978;&#38477;&#20302;&#20102;9%&#21644;18%&#30340;&#35823;&#26816;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
A continual learning solution is proposed to address the out-of-distribution generalization problem for pedestrian detection. While recent pedestrian detection models have achieved impressive performance on various datasets, they remain sensitive to shifts in the distribution of the inference data. Our method adopts and modifies Elastic Weight Consolidation to a backbone object detection network, in order to penalize the changes in the model weights based on their importance towards the initially learned task. We show that when trained with one dataset and fine-tuned on another, our solution learns the new distribution and maintains its performance on the previous one, avoiding catastrophic forgetting. We use two popular datasets, CrowdHuman and CityPersons for our cross-dataset experiments, and show considerable improvements over standard fine-tuning, with a 9% and 18% miss rate percent reduction improvement in the CrowdHuman and CityPersons datasets, respectively.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;&#20108;&#27425;&#35268;&#21010;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#36825;&#19968;&#37325;&#22823;&#31361;&#30772;&#20351;&#24471;&#25105;&#20204;&#20174;$O(\sqrt{n})$&#30340;&#20248;&#21270;&#31639;&#27861;&#36807;&#28193;&#21040;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#22312;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.15079</link><description>&lt;p&gt;
&#20174;$O(\sqrt{n})$&#21040;$O(\log(n))$&#30340;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
From $O(\sqrt n)$ to $O(\log n)$ in Quadratic Programming. (arXiv:2306.15079v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15079
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;&#20108;&#27425;&#35268;&#21010;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#36825;&#19968;&#37325;&#22823;&#31361;&#30772;&#20351;&#24471;&#25105;&#20204;&#20174;$O(\sqrt{n})$&#30340;&#20248;&#21270;&#31639;&#27861;&#36807;&#28193;&#21040;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#22312;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#25968;&#20540;&#20248;&#21270;&#29702;&#35770;&#19968;&#30452;&#23384;&#22312;&#19968;&#20010;&#22256;&#25200;&#65292;&#21363;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#20840;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#21644;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#20197;&#26377;&#30028;&#30418;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65288;Box-QP&#65289;&#20026;&#36215;&#28857;&#65292;&#35768;&#22810;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23545;&#20598;&#29702;&#35770;&#36716;&#21270;&#20026;Box-QP&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;QP&#31639;&#27861;&#65292;&#23588;&#20854;&#26159;&#20854;&#34920;&#29616;&#31867;&#20284;&#20110;&#8220;&#30452;&#25509;&#8221;&#26041;&#27861;&#65306;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#31934;&#30830;&#20540;&#20026;$\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$&#12290;&#36825;&#19968;&#37325;&#22823;&#31361;&#30772;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20174;$O(\sqrt{n})$&#30340;&#20248;&#21270;&#31639;&#27861;&#36807;&#28193;&#21040;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20986;&#33394;&#30340;&#21487;&#25193;&#23637;&#24615;&#22312;&#24403;&#20170;&#30340;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
A "dark cloud" hangs over numerical optimization theory for decades, namely, whether an optimization algorithm $O(\log(n))$ iteration complexity exists. "Yes", this paper answers, with a new optimization algorithm and strict theory proof. It starts with box-constrained quadratic programming (Box-QP), and many practical optimization problems fall into Box-QP. Smooth quadratic programming (QP) and nonsmooth Lasso can be reformulated as Box-QP via duality theory. It is the first time to present an $O(\log(n))$ iteration complexity QP algorithm, in particular, which behaves like a "direct" method: the required number of iterations is deterministic with exact value $\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$. This significant breakthrough enables us to transition from the $O(\sqrt{n})$ to the $O(\log(n))$ optimization algorithm, whose amazing scalability is particularly relevant in today's era of big data and artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#20998;&#23376;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;Mol-GDL&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#38750;&#20849;&#20215;&#38190;&#26500;&#24314;&#30340;&#20998;&#23376;&#22270;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#19982;&#22522;&#20110;&#20849;&#20215;&#38190;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#23637;&#29616;&#20102;&#36229;&#36234;&#22522;&#20110;&#20849;&#20215;&#38190;&#30340;&#20998;&#23376;&#22270;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.15065</link><description>&lt;p&gt;
&#20998;&#23376;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Molecular geometric deep learning. (arXiv:2306.15065v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15065
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;Mol-GDL&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#38750;&#20849;&#20215;&#38190;&#26500;&#24314;&#30340;&#20998;&#23376;&#22270;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#19982;&#22522;&#20110;&#20849;&#20215;&#38190;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#23637;&#29616;&#20102;&#36229;&#36234;&#22522;&#20110;&#20849;&#20215;&#38190;&#30340;&#20998;&#23376;&#22270;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#22312;&#20998;&#23376;&#25968;&#25454;&#20998;&#26512;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#23041;&#21147;&#21644;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#39640;&#25928;&#30340;&#20998;&#23376;&#34920;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;&#20849;&#20215;&#38190;&#30340;&#20998;&#23376;&#22270;&#24050;&#25104;&#20026;&#34920;&#31034;&#21407;&#23376;&#23618;&#27425;&#30340;&#20998;&#23376;&#25299;&#25169;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#65292;&#20165;&#30001;&#38750;&#20849;&#20215;&#38190;&#26500;&#24314;&#30340;&#20998;&#23376;&#22270;&#21487;&#20197;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20013;&#21462;&#24471;&#19982;&#22522;&#20110;&#20849;&#20215;&#38190;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#35777;&#26126;&#20102;&#36229;&#36234;&#22522;&#20110;&#20849;&#20215;&#38190;&#30340;&#20998;&#23376;&#22270;&#30340;&#26032;&#22411;&#20998;&#23376;&#34920;&#31034;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#23376;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;Mol-GDL&#65289;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#26356;&#36890;&#29992;&#30340;&#20998;&#23376;&#34920;&#31034;&#34701;&#20837;GDL&#27169;&#22411;&#20013;&#12290;&#22312;&#25105;&#20204;&#30340;Mol-GDL&#20013;&#65292;&#20998;&#23376;&#25299;&#25169;&#34987;&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#30340;&#20998;&#23376;&#22270;&#65292;&#27599;&#20010;&#20998;&#23376;&#22270;&#32858;&#28966;&#20110;&#19981;&#21516;&#23610;&#24230;&#30340;&#21407;&#23376;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric deep learning (GDL) has demonstrated huge power and enormous potential in molecular data analysis. However, a great challenge still remains for highly efficient molecular representations. Currently, covalent-bond-based molecular graphs are the de facto standard for representing molecular topology at the atomic level. Here we demonstrate, for the first time, that molecular graphs constructed only from non-covalent bonds can achieve similar or even better results than covalent-bond-based models in molecular property prediction. This demonstrates the great potential of novel molecular representations beyond the de facto standard of covalent-bond-based molecular graphs. Based on the finding, we propose molecular geometric deep learning (Mol-GDL). The essential idea is to incorporate a more general molecular representation into GDL models. In our Mol-GDL, molecular topology is modeled as a series of molecular graphs, each focusing on a different scale of atomic interactions. In th
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#30340;transformer&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#22312;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#20197;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#32780;&#22312;&#38408;&#20540;&#20197;&#19978;&#26126;&#26174;&#20248;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2306.15063</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#20219;&#21153;&#22810;&#26679;&#24615;&#19982;&#22238;&#24402;&#38382;&#39064;&#20013;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. (arXiv:2306.15063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15063
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;transformer&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#22312;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#20197;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#32780;&#22312;&#38408;&#20540;&#20197;&#19978;&#26126;&#26174;&#20248;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;transformer&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#38054;&#20329;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65288;ICL&#65289;&#65306;&#23427;&#20204;&#21487;&#20197;&#20174;&#20165;&#25552;&#20379;&#22312;&#25552;&#31034;&#20013;&#30340;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#26435;&#37325;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;ICL&#33021;&#22815;&#35299;&#20915;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#12289;&#22312;&#26412;&#36136;&#19978;&#19982;&#20043;&#21069;&#20219;&#21153;&#38750;&#24120;&#19981;&#21516;&#30340;&#26032;&#20219;&#21153;&#21527;&#65311;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#25913;&#21464;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#30740;&#31350;&#20102;ICL&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#20986;&#29616;ICL&#30340;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#12290;&#22312;&#36825;&#20010;&#38408;&#20540;&#20197;&#19979;&#65292;&#39044;&#35757;&#32451;&#30340;transformer&#26080;&#27861;&#35299;&#20915;&#26410;&#35265;&#30340;&#22238;&#24402;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#30340;&#34892;&#20026;&#31867;&#20284;&#20110;&#20855;&#26377;&#38750;&#22810;&#26679;&#24615;&#39044;&#35757;&#32451;&#20219;&#21153;&#20998;&#24067;&#20316;&#20026;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#12290;&#36229;&#36807;&#36825;&#20010;&#38408;&#20540;&#21518;&#65292;transformer&#26126;&#26174;&#20248;&#20110;&#36825;&#20010;&#20272;&#35745;&#22120;&#65307;&#23427;&#30340;&#34892;&#20026;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#65292;&#23545;$\textit{&#25152;&#26377;&#20219;&#21153;}$&#65292;&#21253;&#25324;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#39640;&#26031;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained transformers exhibit the remarkable ability of in-context learning (ICL): they can learn tasks from just a few examples provided in the prompt without updating any weights. This raises a foundational question: can ICL solve fundamentally $\textit{new}$ tasks that are very different from those seen during pretraining? To probe this question, we examine ICL's performance on linear regression while varying the diversity of tasks in the pretraining dataset. We empirically demonstrate a $\textit{task diversity threshold}$ for the emergence of ICL. Below this threshold, the pretrained transformer cannot solve unseen regression tasks as it behaves like a Bayesian estimator with the $\textit{non-diverse pretraining task distribution}$ as the prior. Beyond this threshold, the transformer significantly outperforms this estimator; its behavior aligns with that of ridge regression, corresponding to a Gaussian prior over $\textit{all tasks}$, including those not seen during pretraining. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21367;&#31215;&#36890;&#36947;&#20132;&#25442;&#28151;&#21512;&#31574;&#30053;&#65288;Swap&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#24314;&#31569;&#32467;&#26500;&#30340;&#21521;&#37327;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#30456;&#37051;&#25110;&#23545;&#35282;&#29305;&#24449;&#20132;&#26367;&#20132;&#25442;&#24182;&#28151;&#21512;&#19981;&#21516;&#36890;&#36947;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#38598;&#25104;&#23616;&#37096;&#29305;&#24449;&#31354;&#38388;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#32452;&#30340;&#21442;&#25968;&#20849;&#20139;&#26426;&#21046;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#27169;&#22411;&#20013;&#30340;&#20887;&#20313;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.15035</link><description>&lt;p&gt;
&#20351;&#29992;Swap&#20248;&#21270;&#24314;&#31569;&#32467;&#26500;&#30340;&#21521;&#37327;&#21270;&#65306;&#39640;&#25928;&#21367;&#31215;&#36890;&#36947;&#20132;&#25442;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Optimized Vectorizing of Building Structures with Swap: High-Efficiency Convolutional Channel-Swap Hybridization Strategy. (arXiv:2306.15035v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21367;&#31215;&#36890;&#36947;&#20132;&#25442;&#28151;&#21512;&#31574;&#30053;&#65288;Swap&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#24314;&#31569;&#32467;&#26500;&#30340;&#21521;&#37327;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#30456;&#37051;&#25110;&#23545;&#35282;&#29305;&#24449;&#20132;&#26367;&#20132;&#25442;&#24182;&#28151;&#21512;&#19981;&#21516;&#36890;&#36947;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#38598;&#25104;&#23616;&#37096;&#29305;&#24449;&#31354;&#38388;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#32452;&#30340;&#21442;&#25968;&#20849;&#20139;&#26426;&#21046;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#27169;&#22411;&#20013;&#30340;&#20887;&#20313;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#24179;&#38754;&#22270;&#30340;&#37325;&#24314;&#65292;&#20063;&#31216;&#20026;&#36275;&#36857;&#37325;&#24314;&#65292;&#23646;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22320;&#29702;&#20449;&#24687;&#23398;&#39046;&#22495;&#65292;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#20256;&#32479;&#21367;&#31215;&#27169;&#22411;&#20013;&#20887;&#20313;&#21442;&#25968;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#21644;&#33258;&#36866;&#24212;&#30340;&#31227;&#20301;&#26550;&#26500;&#65292;&#21363;Swap&#25805;&#20316;&#65292;&#23427;&#32467;&#21512;&#20102;&#38750;&#25351;&#25968;&#22686;&#38271;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#38598;&#25104;&#23616;&#37096;&#29305;&#24449;&#31354;&#38388;&#20449;&#24687;&#30340;&#31867;&#20284;&#21151;&#33021;&#65292;&#31867;&#20284;&#20110;&#39640;&#32500;&#21367;&#31215;&#25805;&#20316;&#22120;&#12290;Swap&#36328;&#36890;&#36947;&#25805;&#20316;&#26550;&#26500;&#36890;&#36807;&#24322;&#25110;&#25805;&#20316;&#20132;&#26367;&#20132;&#25442;&#30456;&#37051;&#25110;&#23545;&#35282;&#29305;&#24449;&#65292;&#28982;&#21518;&#36890;&#36807;1x1&#21367;&#31215;&#25805;&#20316;&#28151;&#21512;&#20132;&#26367;&#30340;&#36890;&#36947;&#65292;&#20197;&#25972;&#21512;&#19981;&#21516;&#36890;&#36947;&#30340;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;SwapNN&#26550;&#26500;&#37319;&#29992;&#20102;&#21463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36807;&#31243;&#21551;&#21457;&#30340;&#22522;&#20110;&#32452;&#30340;&#21442;&#25968;&#20849;&#20139;&#26426;&#21046;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The building planar graph reconstruction, a.k.a. footprint reconstruction, which lies in the domain of computer vision and geoinformatics, has been long afflicted with the challenge of redundant parameters in conventional convolutional models. Therefore, in this paper, we proposed an advanced and adaptive shift architecture, namely the Swap operation, which incorporates non-exponential growth parameters while retaining analogous functionalities to integrate local feature spatial information, resembling a high-dimensional convolution operator. The Swap, cross-channel operation, architecture implements the XOR operation to alternately exchange adjacent or diagonal features, and then blends alternating channels through a 1x1 convolution operation to consolidate information from different channels. The SwapNN architecture, on the other hand, incorporates a group-based parameter-sharing mechanism inspired by the convolutional neural network process and thereby significantly reducing the num
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#24471;&#20998;&#23551;&#21629;&#35268;&#21010;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#26041;&#27861;&#21487;&#20197;&#25628;&#32034;&#38750;&#31283;&#24577;&#31574;&#30053;&#20989;&#25968;&#65292;&#24182;&#30452;&#25509;&#35745;&#31639;&#26368;&#20248;&#26080;&#38480;&#26102;&#38388;&#21306;&#38388;&#21160;&#20316;&#24207;&#21015;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#24314;&#31435;&#21160;&#20316;&#24207;&#21015;&#21644;&#23454;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35745;&#31639;&#26368;&#20248;&#21160;&#20316;&#24207;&#21015;&#12290;&#26041;&#27861;&#22312;&#38750;&#32447;&#24615;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15029</link><description>&lt;p&gt;
&#36229;&#36234;&#21160;&#24577;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Beyond dynamic programming. (arXiv:2306.15029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15029
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#24471;&#20998;&#23551;&#21629;&#35268;&#21010;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#26041;&#27861;&#21487;&#20197;&#25628;&#32034;&#38750;&#31283;&#24577;&#31574;&#30053;&#20989;&#25968;&#65292;&#24182;&#30452;&#25509;&#35745;&#31639;&#26368;&#20248;&#26080;&#38480;&#26102;&#38388;&#21306;&#38388;&#21160;&#20316;&#24207;&#21015;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#24314;&#31435;&#21160;&#20316;&#24207;&#21015;&#21644;&#23454;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35745;&#31639;&#26368;&#20248;&#21160;&#20316;&#24207;&#21015;&#12290;&#26041;&#27861;&#22312;&#38750;&#32447;&#24615;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#24471;&#20998;&#23551;&#21629;&#35268;&#21010;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25628;&#32034;&#38750;&#31283;&#24577;&#31574;&#30053;&#20989;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#35745;&#31639;&#32473;&#23450;&#29366;&#24577;&#19979;&#30340;&#26368;&#20248;&#26080;&#38480;&#26102;&#38388;&#21306;&#38388;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#24314;&#31435;&#26080;&#38480;&#26102;&#38388;&#21306;&#38388;&#21160;&#20316;&#24207;&#21015;&#21644;&#26377;&#30028;&#21306;&#38388;&#20869;&#23454;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#31181;&#26500;&#36896;&#20351;&#25105;&#20204;&#33021;&#22815;&#21046;&#23450;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#30452;&#25509;&#35745;&#31639;&#26368;&#20248;&#26080;&#38480;&#26102;&#38388;&#21306;&#38388;&#21160;&#20316;&#24207;&#21015;&#65292;&#26080;&#38656;&#31574;&#30053;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#20026;&#21046;&#23450;&#21644;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Score-life programming, a novel theoretical approach for solving reinforcement learning problems. In contrast with classical dynamic programming-based methods, our method can search over non-stationary policy functions, and can directly compute optimal infinite horizon action sequences from a given state. The central idea in our method is the construction of a mapping between infinite horizon action sequences and real numbers in a bounded interval. This construction enables us to formulate an optimization problem for directly computing optimal infinite horizon action sequences, without requiring a policy function. We demonstrate the effectiveness of our approach by applying it to nonlinear optimal control problems. Overall, our contributions provide a novel theoretical framework for formulating and solving reinforcement learning problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DNABERT-2&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#31181;&#29289;&#31181;&#22522;&#22240;&#32452;&#30340;&#39640;&#25928;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32479;&#35745;&#30340;&#25968;&#25454;&#21387;&#32553;&#31639;&#27861;Byte Pair Encoding&#65288;BPE&#65289;&#26367;&#20195;&#20256;&#32479;&#30340;k-mer&#26631;&#35760;&#21270;&#65292;&#20811;&#26381;&#20102;k-mer&#26631;&#35760;&#21270;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.15006</link><description>&lt;p&gt;
DNABERT-2:&#22810;&#31181;&#29289;&#31181;&#22522;&#22240;&#32452;&#30340;&#39640;&#25928;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome. (arXiv:2306.15006v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DNABERT-2&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#31181;&#29289;&#31181;&#22522;&#22240;&#32452;&#30340;&#39640;&#25928;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32479;&#35745;&#30340;&#25968;&#25454;&#21387;&#32553;&#31639;&#27861;Byte Pair Encoding&#65288;BPE&#65289;&#26367;&#20195;&#20256;&#32479;&#30340;k-mer&#26631;&#35760;&#21270;&#65292;&#20811;&#26381;&#20102;k-mer&#26631;&#35760;&#21270;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#30721;&#22522;&#22240;&#32452;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#26159;&#29983;&#29289;&#23398;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#32780;DNABERT&#21644;Nucleotide Transformer&#31561;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#20381;&#36182;&#20110;k-mer&#20316;&#20026;&#22522;&#22240;&#32452;&#35821;&#35328;&#30340;&#26631;&#35760;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;k-mer&#26631;&#35760;&#21270;&#24341;&#20837;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#26159;&#21457;&#23637;&#22823;&#35268;&#27169;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#22240;&#32452;&#26631;&#35760;&#21270;&#30340;&#27010;&#24565;&#21644;&#32463;&#39564;&#35265;&#35299;&#65292;&#22522;&#20110;&#27492;&#25552;&#20986;&#29992;&#22522;&#20110;&#32479;&#35745;&#30340;&#25968;&#25454;&#21387;&#32553;&#31639;&#27861;Byte Pair Encoding&#65288;BPE&#65289;&#26367;&#20195;k-mer&#26631;&#35760;&#21270;&#65292;BPE&#36890;&#36807;&#36845;&#20195;&#21512;&#24182;&#35821;&#26009;&#24211;&#20013;&#26368;&#39057;&#32321;&#20849;&#21516;&#20986;&#29616;&#30340;&#22522;&#22240;&#32452;&#29255;&#27573;&#26469;&#26500;&#24314;&#26631;&#35760;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;BPE&#19981;&#20165;&#20811;&#26381;&#20102;k-mer&#26631;&#35760;&#21270;&#30340;&#23616;&#38480;&#24615;&#65292;&#36824;&#33021;&#20174;&#38750;&#37325;&#21472;&#26631;&#35760;&#21270;&#30340;&#35745;&#31639;&#25928;&#29575;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIMF&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#35821;&#20041;&#24863;&#30693;&#30340;&#20132;&#20114;&#24335;&#36816;&#21160;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;&#35821;&#20041;&#30340;&#34892;&#20026;&#20307;&#36873;&#25321;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20840;&#23616;&#32534;&#30721;&#65292;&#33021;&#22815;&#25429;&#25417;&#31354;&#38388;&#20449;&#24687;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20248;&#36873;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.14941</link><description>&lt;p&gt;
SIMF: &#33258;&#21160;&#39550;&#39542;&#30340;&#35821;&#20041;&#24863;&#30693;&#20132;&#20114;&#24335;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SIMF: Semantics-aware Interactive Motion Forecasting for Autonomous Driving. (arXiv:2306.14941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIMF&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#35821;&#20041;&#24863;&#30693;&#30340;&#20132;&#20114;&#24335;&#36816;&#21160;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;&#35821;&#20041;&#30340;&#34892;&#20026;&#20307;&#36873;&#25321;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20840;&#23616;&#32534;&#30721;&#65292;&#33021;&#22815;&#25429;&#25417;&#31354;&#38388;&#20449;&#24687;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20248;&#36873;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38656;&#35201;&#23545;&#21608;&#22260;&#22810;&#20010;&#34892;&#20026;&#20307;&#65288;&#34892;&#20154;&#21644;&#36710;&#36742;&#65289;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#65292;&#20197;&#20570;&#20986;&#26368;&#20248;&#23548;&#33322;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#34892;&#20026;&#20307;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#65292;&#24182;&#26410;&#33021;&#25429;&#25417;&#21040;&#22330;&#26223;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#19982;&#22330;&#26223;&#20013;&#34892;&#20026;&#20307;&#25968;&#37327;&#22686;&#21152;&#30456;&#20851;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#27431;&#27663;&#36317;&#31163;&#26469;&#21098;&#26525;&#36828;&#31163;&#30340;&#34892;&#20026;&#20307;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#26080;&#27861;&#36873;&#25321;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#24182;&#20934;&#30830;&#36827;&#34892;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SIMF&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#31354;&#38388;&#20449;&#24687;&#20197;&#21450;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20248;&#36873;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30340;&#34892;&#20026;&#20307;&#36873;&#25321;&#26041;&#27861;&#65292;&#23558;&#20854;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20256;&#36882;&#65292;&#20197;&#25552;&#21462;&#20840;&#23616;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles require motion forecasting of their surrounding multi-agents (pedestrians and vehicles) to make optimal decisions for navigation. The existing methods focus on techniques to utilize the positions and velocities of these agents and fail to capture semantic information from the scene. Moreover, to mitigate the increase in computational complexity associated with the number of agents in the scene, some works leverage Euclidean distance to prune far-away agents. However, distance-based metric alone is insufficient to select relevant agents and accurately perform their predictions. To resolve these issues, we propose Semantics-aware Interactive Motion Forecasting (SIMF) method to capture semantics along with spatial information, and optimally select relevant agents for motion prediction. Specifically, we achieve this by implementing a semantic-aware selection of relevant agents from the scene and passing them through an attention mechanism to extract global encodings. Th
&lt;/p&gt;</description></item><item><title>Sciama&#36890;&#36807;&#21306;&#20998;&#33529;&#26524;&#21644;&#27225;&#23376;&#30340;&#26041;&#24335;&#35770;&#36848;&#22312;&#38543;&#26426;&#23431;&#23449;&#20013;&#23384;&#22312;&#29983;&#21629;&#30340;&#38382;&#39064;&#65292;&#20182;&#35748;&#20026;&#29983;&#21629;&#30340;&#23384;&#22312;&#21462;&#20915;&#20110;&#35768;&#22810;&#37327;&#65292;&#20294;&#22312;&#27809;&#26377;&#23545;&#36825;&#20123;&#37327;&#26377;&#20805;&#20998;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#20182;&#30340;&#35770;&#28857;&#26263;&#31034;&#20102;&#19968;&#20010;&#30475;&#36215;&#26469;&#26159;&#8220;&#26234;&#33021;&#35774;&#35745;&#8221;&#30340;&#23431;&#23449;&#12290;</title><link>http://arxiv.org/abs/2306.14934</link><description>&lt;p&gt;
Sciama&#35770;&#36848;&#30340;&#22312;&#38543;&#26426;&#23431;&#23449;&#20013;&#23384;&#22312;&#29983;&#21629;&#30340;&#36776;&#21035;&#65306;&#21306;&#20998;&#33529;&#26524;&#21644;&#27225;&#23376;
&lt;/p&gt;
&lt;p&gt;
Sciama's argument on life in a random universe: Distinguishing apples from oranges. (arXiv:2306.14934v1 [physics.hist-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14934
&lt;/p&gt;
&lt;p&gt;
Sciama&#36890;&#36807;&#21306;&#20998;&#33529;&#26524;&#21644;&#27225;&#23376;&#30340;&#26041;&#24335;&#35770;&#36848;&#22312;&#38543;&#26426;&#23431;&#23449;&#20013;&#23384;&#22312;&#29983;&#21629;&#30340;&#38382;&#39064;&#65292;&#20182;&#35748;&#20026;&#29983;&#21629;&#30340;&#23384;&#22312;&#21462;&#20915;&#20110;&#35768;&#22810;&#37327;&#65292;&#20294;&#22312;&#27809;&#26377;&#23545;&#36825;&#20123;&#37327;&#26377;&#20805;&#20998;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#20182;&#30340;&#35770;&#28857;&#26263;&#31034;&#20102;&#19968;&#20010;&#30475;&#36215;&#26469;&#26159;&#8220;&#26234;&#33021;&#35774;&#35745;&#8221;&#30340;&#23431;&#23449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dennis Sciama&#35748;&#20026;&#29983;&#21629;&#30340;&#23384;&#22312;&#21462;&#20915;&#20110;&#35768;&#22810;&#37327;&#65292;&#20063;&#23601;&#26159;&#22522;&#26412;&#24120;&#25968;&#65292;&#22240;&#27492;&#22312;&#19968;&#20010;&#38543;&#26426;&#30340;&#23431;&#23449;&#20013;&#65292;&#29983;&#21629;&#24212;&#35813;&#26159;&#38750;&#24120;&#19981;&#22826;&#21487;&#33021;&#30340;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#23545;&#36825;&#20123;&#24120;&#25968;&#26377;&#20805;&#20998;&#30340;&#20102;&#35299;&#65292;&#20182;&#30340;&#35770;&#25454;&#24847;&#21619;&#30528;&#19968;&#20010;&#30475;&#36215;&#26469;&#26159;&#8220;&#26234;&#33021;&#35774;&#35745;&#8221;&#30340;&#23431;&#23449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dennis Sciama argued that the existence of life depended on many quantities, the fundamental constants, so in a random universe life should be highly unlikely. However, without full knowledge of these constants, his argument implies a universe that would appear to be `intelligently designed.'
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#19982;&#23376;&#35789;&#23884;&#20837;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20316;&#32773;&#24402;&#23646;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22788;&#29702;&#25991;&#26412;&#20013;&#30340;&#38544;&#21547;&#35789;&#38382;&#39064;&#30340;&#21516;&#26102;&#20445;&#30041;&#35789;&#30340;&#39034;&#24207;&#19978;&#19979;&#25991;&#12290;</title><link>http://arxiv.org/abs/2306.14933</link><description>&lt;p&gt;
&#23558;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#19982;&#23376;&#35789;&#23884;&#20837;&#30456;&#32467;&#21512;&#29992;&#20110;&#20316;&#32773;&#24402;&#23646;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Integrating Bidirectional Long Short-Term Memory with Subword Embedding for Authorship Attribution. (arXiv:2306.14933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14933
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#19982;&#23376;&#35789;&#23884;&#20837;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20316;&#32773;&#24402;&#23646;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22788;&#29702;&#25991;&#26412;&#20013;&#30340;&#38544;&#21547;&#35789;&#38382;&#39064;&#30340;&#21516;&#26102;&#20445;&#30041;&#35789;&#30340;&#39034;&#24207;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#32473;&#23450;&#25991;&#26412;&#25991;&#26723;&#30340;&#20316;&#32773;&#36523;&#20221;&#26159;&#20316;&#32773;&#24402;&#23646;&#38382;&#39064;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#25104;&#21151;&#22320;&#20351;&#29992;&#22810;&#26679;&#30340;&#22522;&#20110;&#35789;&#30340;&#39118;&#26684;&#26631;&#35760;&#26469;&#22788;&#29702;&#20316;&#32773;&#24402;&#23646;&#30340;&#20869;&#22312;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#35789;&#30340;&#20316;&#32773;&#24402;&#23646;&#31995;&#32479;&#30340;&#24615;&#33021;&#21463;&#21040;&#35757;&#32451;&#35821;&#26009;&#24211;&#35789;&#27719;&#30340;&#38480;&#21046;&#12290;&#25991;&#29486;&#25512;&#33616;&#20102;&#20197;&#23383;&#31526;&#20026;&#22522;&#30784;&#30340;&#39118;&#26684;&#26631;&#35760;&#20316;&#20026;&#20811;&#26381;&#38544;&#21547;&#35789;&#38382;&#39064;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#23383;&#31526;&#30340;&#26041;&#27861;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#25991;&#26412;&#20013;&#30340;&#35789;&#30340;&#39034;&#24207;&#20851;&#31995;&#65292;&#36825;&#26159;&#36827;&#19968;&#27493;&#25913;&#21892;&#30340;&#38590;&#39064;&#12290;&#26412;&#25991;&#35752;&#35770;&#30340;&#38382;&#39064;&#26159;&#26159;&#21542;&#21487;&#20197;&#35299;&#20915;&#25991;&#26412;&#25991;&#26723;&#20013;&#38544;&#21547;&#35789;&#30340;&#27495;&#20041;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#35789;&#30340;&#39034;&#24207;&#19978;&#19979;&#25991;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;BLSTM&#65289;&#19982;&#20108;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;&#39034;&#24207;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of unveiling the author of a given text document from multiple candidate authors is called authorship attribution. Manifold word-based stylistic markers have been successfully used in deep learning methods to deal with the intrinsic problem of authorship attribution. Unfortunately, the performance of word-based authorship attribution systems is limited by the vocabulary of the training corpus. Literature has recommended character-based stylistic markers as an alternative to overcome the hidden word problem. However, character-based methods often fail to capture the sequential relationship of words in texts which is a chasm for further improvement. The question addressed in this paper is whether it is possible to address the ambiguity of hidden words in text documents while preserving the sequential context of words. Consequently, a method based on bidirectional long short-term memory (BLSTM) with a 2-dimensional convolutional neural network (CNN) is proposed to capture sequ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20943;&#23569;&#28436;&#32462;&#32534;&#30721;&#25152;&#38656;&#26102;&#38388;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20256;&#32479;&#20869;&#23481;&#20998;&#26512;&#30340;&#28789;&#27963;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#21644;&#32463;&#39564;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#19981;&#21516;&#28436;&#32462;&#32534;&#30721;&#20219;&#21153;&#19978;&#65292;GPT-3.5&#22312;LLM&#36741;&#21161;&#20869;&#23481;&#20998;&#26512;&#65288;LACA&#65289;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14924</link><description>&lt;p&gt;
LLM&#36741;&#21161;&#20869;&#23481;&#20998;&#26512;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#28436;&#32462;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding. (arXiv:2306.14924v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20943;&#23569;&#28436;&#32462;&#32534;&#30721;&#25152;&#38656;&#26102;&#38388;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20256;&#32479;&#20869;&#23481;&#20998;&#26512;&#30340;&#28789;&#27963;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#21644;&#32463;&#39564;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#19981;&#21516;&#28436;&#32462;&#32534;&#30721;&#20219;&#21153;&#19978;&#65292;GPT-3.5&#22312;LLM&#36741;&#21161;&#20869;&#23481;&#20998;&#26512;&#65288;LACA&#65289;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#32462;&#32534;&#30721;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#25991;&#26723;&#20013;&#20027;&#39064;&#30340;&#26222;&#36941;&#24615;&#12290;&#23613;&#31649;&#26377;&#29992;&#65292;&#28436;&#32462;&#32534;&#30721;&#36890;&#24120;&#26159;&#32321;&#29712;&#19988;&#32791;&#26102;&#30340;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#30740;&#31350;&#20154;&#21592;&#38405;&#35835;&#12289;&#35299;&#37322;&#24182;&#21487;&#38752;&#22320;&#23545;&#22823;&#37327;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#26159;&#19968;&#31867;&#24555;&#36895;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#26469;&#20943;&#23569;&#28436;&#32462;&#32534;&#30721;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#30041;&#20256;&#32479;&#20869;&#23481;&#20998;&#26512;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;LLM&#36741;&#21161;&#20869;&#23481;&#20998;&#26512;&#65288;LACA&#65289;&#65292;&#24182;&#20351;&#29992;GPT-3.5&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#28436;&#32462;&#32534;&#30721;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#26696;&#20363;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20010;&#32463;&#39564;&#22522;&#20934;&#27979;&#35797;&#65292;&#20351;&#29992;LACA&#22312;4&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;GPT-3.5&#22312;&#19981;&#21516;&#28436;&#32462;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deductive coding is a widely used qualitative research method for determining the prevalence of themes across documents. While useful, deductive coding is often burdensome and time consuming since it requires researchers to read, interpret, and reliably categorize a large body of unstructured text documents. Large language models (LLMs), like ChatGPT, are a class of quickly evolving AI tools that can perform a range of natural language processing and reasoning tasks. In this study, we explore the use of LLMs to reduce the time it takes for deductive coding while retaining the flexibility of a traditional content analysis. We outline the proposed approach, called LLM-assisted content analysis (LACA), along with an in-depth case study using GPT-3.5 for LACA on a publicly available deductive coding data set. Additionally, we conduct an empirical benchmark using LACA on 4 publicly available data sets to assess the broader question of how well GPT-3.5 performs across a range of deductive co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#32454;&#21270;&#26631;&#20934;&#24471;&#20998;&#65292;&#23454;&#29616;&#23545;&#35838;&#22530;&#35752;&#35770;&#36136;&#37327;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65292;&#21516;&#26102;&#25351;&#20986;&#26631;&#20934;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;NLP&#26041;&#27861;&#23545;&#19981;&#21516;&#30340;&#26631;&#20934;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.14918</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#34892;&#35838;&#22530;&#35752;&#35770;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Utilizing Natural Language Processing for Automated Assessment of Classroom Discussion. (arXiv:2306.14918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#32454;&#21270;&#26631;&#20934;&#24471;&#20998;&#65292;&#23454;&#29616;&#23545;&#35838;&#22530;&#35752;&#35770;&#36136;&#37327;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65292;&#21516;&#26102;&#25351;&#20986;&#26631;&#20934;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;NLP&#26041;&#27861;&#23545;&#19981;&#21516;&#30340;&#26631;&#20934;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20005;&#26684;&#32780;&#20114;&#21160;&#30340;&#35838;&#22530;&#35752;&#35770;&#23545;&#20110;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#20063;&#26159;&#22823;&#22810;&#25968;&#25945;&#23398;&#24178;&#39044;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23545;&#35752;&#35770;&#36136;&#37327;&#36827;&#34892;&#35268;&#27169;&#21270;&#30340;&#27491;&#24335;&#35780;&#20272;&#23545;&#20110;&#22823;&#22810;&#25968;&#30740;&#31350;&#32773;&#26469;&#35828;&#26159;&#26114;&#36149;&#19988;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#21508;&#31181;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#65292;&#20197;&#33258;&#21160;&#29983;&#25104;&#35838;&#22530;&#25991;&#26412;&#35752;&#35770;&#36136;&#37327;&#30340;&#32454;&#21270;&#26631;&#20934;&#24471;&#20998;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21253;&#21547;&#36229;&#36807;18000&#36718;&#27425;&#12289;&#27880;&#37322;&#26377;&#35814;&#32454;&#30340;&#25945;&#23398;&#20998;&#26512;&#36816;&#21160;&#65288;ATM&#65289;&#20195;&#30721;&#30340;90&#20010;&#35838;&#22530;&#35752;&#35770;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#24182;&#32858;&#28966;&#20110;&#22235;&#20010;&#25945;&#23398;&#36136;&#37327;&#35780;&#20272;&#65288;IQA&#65289;&#26631;&#20934;&#12290;&#23613;&#31649;&#25968;&#25454;&#37327;&#26377;&#38480;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#19968;&#20123;&#26631;&#20934;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20063;&#26263;&#31034;&#20854;&#20182;&#26631;&#20934;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#26576;&#20123;NLP&#26041;&#27861;&#22312;&#26576;&#20123;&#26631;&#20934;&#19978;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rigorous and interactive class discussions that support students to engage in high-level thinking and reasoning are essential to learning and are a central component of most teaching interventions. However, formally assessing discussion quality 'at scale' is expensive and infeasible for most researchers. In this work, we experimented with various modern natural language processing (NLP) techniques to automatically generate rubric scores for individual dimensions of classroom text discussion quality. Specifically, we worked on a dataset of 90 classroom discussion transcripts consisting of over 18000 turns annotated with fine-grained Analyzing Teaching Moves (ATM) codes and focused on four Instructional Quality Assessment (IQA) rubrics. Despite the limited amount of data, our work shows encouraging results in some of the rubrics while suggesting that there is room for improvement in the others. We also found that certain NLP approaches work better for certain rubrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24341;&#23548;&#23646;&#24615;&#65288;&#38382;&#39064;&#26126;&#30830;&#24615;&#65289;&#26469;&#20016;&#23500;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#25511;&#21046;&#29983;&#25104;&#26126;&#30830;&#21644;&#38544;&#21547;wh-&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#38382;&#39064;&#26126;&#30830;&#24615;&#21644;&#21465;&#20107;&#35201;&#32032;&#21516;&#26102;&#25511;&#21046;&#38382;&#39064;&#29983;&#25104;&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.14917</link><description>&lt;p&gt;
&#36808;&#21521;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#30340;&#20016;&#23500;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Enriched Controllability for Educational Question Generation. (arXiv:2306.14917v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24341;&#23548;&#23646;&#24615;&#65288;&#38382;&#39064;&#26126;&#30830;&#24615;&#65289;&#26469;&#20016;&#23500;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#25511;&#21046;&#29983;&#25104;&#26126;&#30830;&#21644;&#38544;&#21547;wh-&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#38382;&#39064;&#26126;&#30830;&#24615;&#21644;&#21465;&#20107;&#35201;&#32032;&#21516;&#26102;&#25511;&#21046;&#38382;&#39064;&#29983;&#25104;&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#38382;&#39064;&#65288;QG&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#19968;&#20010;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#26681;&#25454;&#36755;&#20837;&#65288;&#36890;&#24120;&#30001;&#25991;&#26412;&#21644;&#30446;&#26631;&#31572;&#26696;&#32452;&#25104;&#65289;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#12290;&#36817;&#26399;&#20851;&#20110;QG&#30340;&#30740;&#31350;&#26088;&#22312;&#25511;&#21046;&#29983;&#25104;&#38382;&#39064;&#30340;&#31867;&#22411;&#65292;&#20197;&#28385;&#36275;&#25945;&#32946;&#38656;&#27714;&#12290;&#25945;&#32946;QG&#20013;&#21487;&#25511;&#24615;&#30340;&#19968;&#20010;&#26174;&#33879;&#20363;&#23376;&#26159;&#29983;&#25104;&#28041;&#21450;&#29305;&#23450;&#21465;&#20107;&#35201;&#32032;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#22240;&#26524;&#20851;&#31995;&#12289;&#32467;&#26524;&#35299;&#20915;&#25110;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24341;&#23548;&#23646;&#24615;&#65288;&#38382;&#39064;&#26126;&#30830;&#24615;&#65289;&#26469;&#20016;&#23500;QG&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#25511;&#21046;&#20174;&#36866;&#21512;&#20799;&#31461;&#30340;&#25925;&#20107;&#20013;&#29983;&#25104;&#26126;&#30830;&#21644;&#38544;&#21547;&#30340;wh-&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#36890;&#36807;&#38382;&#39064;&#26126;&#30830;&#24615;&#20197;&#21450;&#19982;&#21478;&#19968;&#20010;&#30446;&#26631;&#23646;&#24615;&#65288;&#38382;&#39064;&#30340;&#21465;&#20107;&#35201;&#32032;&#65289;&#21516;&#26102;&#25511;&#21046;QG&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;&#20195;&#30721;&#20844;&#24320;&#21487;&#22312;github.com/bernardoleite/question-generation-control&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question Generation (QG) is a task within Natural Language Processing (NLP) that involves automatically generating questions given an input, typically composed of a text and a target answer. Recent work on QG aims to control the type of generated questions so that they meet educational needs. A remarkable example of controllability in educational QG is the generation of questions underlying certain narrative elements, e.g., causal relationship, outcome resolution, or prediction. This study aims to enrich controllability in QG by introducing a new guidance attribute: question explicitness. We propose to control the generation of explicit and implicit wh-questions from children-friendly stories. We show preliminary evidence of controlling QG via question explicitness alone and simultaneously with another target attribute: the question's narrative element. The code is publicly available at github.com/bernardoleite/question-generation-control.
&lt;/p&gt;</description></item><item><title>GPT-4&#32593;&#29366;&#21270;&#23398;&#23478;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;AI&#27169;&#22411;GPT-4&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#30340;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#65292;&#33021;&#22815;&#21457;&#29616;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#31995;&#21015;&#12290;</title><link>http://arxiv.org/abs/2306.14915</link><description>&lt;p&gt;
GPT-4&#32593;&#29366;&#21270;&#23398;&#23478;&#29992;&#20110;MOF&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Reticular Chemist for MOF Discovery. (arXiv:2306.14915v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14915
&lt;/p&gt;
&lt;p&gt;
GPT-4&#32593;&#29366;&#21270;&#23398;&#23478;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;AI&#27169;&#22411;GPT-4&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#30340;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#65292;&#33021;&#22815;&#21457;&#29616;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;AI&#27169;&#22411;GPT-4&#38598;&#25104;&#21040;&#32593;&#29366;&#21270;&#23398;&#23454;&#39564;&#30340;&#36845;&#20195;&#36807;&#31243;&#20013;&#65292;&#21033;&#29992;AI&#19982;&#20154;&#31867;&#23398;&#24466;&#20043;&#38388;&#30340;&#21512;&#20316;&#24037;&#20316;&#27969;&#12290;&#36825;&#20010;GPT-4&#32593;&#29366;&#21270;&#23398;&#23478;&#26159;&#19968;&#20010;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#30340;&#32508;&#21512;&#31995;&#32479;&#12290;&#27599;&#20010;&#38454;&#27573;&#37117;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#21033;&#29992;GPT-4&#65292;&#20854;&#20013;GPT-4&#20026;&#21270;&#23398;&#23454;&#39564;&#25552;&#20379;&#35814;&#32454;&#30340;&#25351;&#23548;&#65292;&#23398;&#24466;&#21017;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#21453;&#39304;&#65292;&#21253;&#25324;&#25104;&#21151;&#21644;&#22833;&#36133;&#65292;&#20197;&#20415;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;AI&#36827;&#34892;&#20869;&#37096;&#23398;&#20064;&#12290;&#36825;&#31181;&#36845;&#20195;&#30340;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#20351;&#24471;GPT-4&#33021;&#22815;&#20687;&#32463;&#39564;&#20016;&#23500;&#30340;&#21270;&#23398;&#23478;&#19968;&#26679;&#20174;&#32467;&#26524;&#20013;&#23398;&#20064;&#65292;&#37319;&#29992;&#19968;&#31181;&#25552;&#31034;&#23398;&#20064;&#31574;&#30053;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35813;&#31995;&#32479;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#24320;&#21457;&#21644;&#25805;&#20316;&#65292;&#26080;&#38656;&#32534;&#30721;&#25216;&#33021;&#65292;&#22240;&#27492;&#23545;&#25152;&#26377;&#21270;&#23398;&#23478;&#37117;&#20855;&#26377;&#21487;&#35775;&#38382;&#24615;&#12290;&#25105;&#20204;&#30340;GPT-4&#32593;&#29366;&#21270;&#23398;&#23478;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new framework integrating the AI model GPT-4 into the iterative process of reticular chemistry experimentation, leveraging a cooperative workflow of interaction between AI and a human apprentice. This GPT-4 Reticular Chemist is an integrated system composed of three phases. Each of these utilizes GPT-4 in various capacities, wherein GPT-4 provides detailed instructions for chemical experimentation and the apprentice provides feedback on the experimental outcomes, including both success and failures, for the in-text learning of AI in the next iteration. This iterative human-AI interaction enabled GPT-4 to learn from the outcomes, much like an experienced chemist, by a prompt-learning strategy. Importantly, the system is based on natural language for both development and operation, eliminating the need for coding skills, and thus, make it accessible to all chemists. Our GPT-4 Reticular Chemist demonstrated the discovery of an isoreticular series of metal-organic frameworks (
&lt;/p&gt;</description></item><item><title>FSUIE&#26159;&#19968;&#31181;&#29992;&#20110;&#26222;&#36866;&#20449;&#24687;&#25277;&#21462;&#30340;&#26032;&#22411;&#27169;&#31946;&#36328;&#24230;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#27169;&#31946;&#36328;&#24230;&#25439;&#22833;&#21644;&#27169;&#31946;&#36328;&#24230;&#27880;&#24847;&#21147;&#65292;&#33021;&#22815;&#22312;&#24555;&#36895;&#25910;&#25947;&#21644;&#23569;&#37327;&#25968;&#25454;&#35757;&#32451;&#36718;&#25968;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20449;&#24687;&#25277;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14913</link><description>&lt;p&gt;
FSUIE:&#19968;&#31181;&#29992;&#20110;&#26222;&#36866;&#20449;&#24687;&#25277;&#21462;&#30340;&#26032;&#22411;&#27169;&#31946;&#36328;&#24230;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FSUIE: A Novel Fuzzy Span Mechanism for Universal Information Extraction. (arXiv:2306.14913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14913
&lt;/p&gt;
&lt;p&gt;
FSUIE&#26159;&#19968;&#31181;&#29992;&#20110;&#26222;&#36866;&#20449;&#24687;&#25277;&#21462;&#30340;&#26032;&#22411;&#27169;&#31946;&#36328;&#24230;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#27169;&#31946;&#36328;&#24230;&#25439;&#22833;&#21644;&#27169;&#31946;&#36328;&#24230;&#27880;&#24847;&#21147;&#65292;&#33021;&#22815;&#22312;&#24555;&#36895;&#25910;&#25947;&#21644;&#23569;&#37327;&#25968;&#25454;&#35757;&#32451;&#36718;&#25968;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20449;&#24687;&#25277;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36866;&#20449;&#24687;&#25277;&#21462;&#65288;UIE&#65289;&#20316;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#24050;&#32463;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;UIE&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23427;&#20204;&#36807;&#20110;&#20381;&#36182;&#20110;&#25968;&#25454;&#20013;&#30340;&#36328;&#24230;&#36793;&#30028;&#65292;&#36825;&#24182;&#19981;&#21453;&#26144;&#36328;&#24230;&#27880;&#37322;&#30340;&#30495;&#23454;&#25361;&#25112;&#12290;&#24494;&#23567;&#30340;&#20301;&#32622;&#35843;&#25972;&#20063;&#21487;&#20197;&#28385;&#36275;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;UIE&#27169;&#22411;&#32570;&#20047;&#23545;&#20449;&#24687;&#25277;&#21462;&#20013;&#26377;&#38480;&#30340;&#36328;&#24230;&#38271;&#24230;&#29305;&#24449;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#31946;&#36328;&#24230;&#26222;&#36866;&#20449;&#24687;&#25277;&#21462;&#65288;FSUIE&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#27169;&#31946;&#36328;&#24230;&#25439;&#22833;&#21644;&#27169;&#31946;&#36328;&#24230;&#27880;&#24847;&#21147;&#20004;&#20010;&#27010;&#24565;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#20027;&#35201;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25968;&#25454;&#37327;&#21644;&#35757;&#32451;&#36718;&#25968;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;FSUIE&#22312;&#24555;&#36895;&#25910;&#25947;&#21644;&#24378;&#22823;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;FSUIE&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Universal Information Extraction (UIE) has been introduced as a unified framework for various Information Extraction (IE) tasks and has achieved widespread success. Despite this, UIE models have limitations. For example, they rely heavily on span boundaries in the data during training, which does not reflect the reality of span annotation challenges. Slight adjustments to positions can also meet requirements. Additionally, UIE models lack attention to the limited span length feature in IE. To address these deficiencies, we propose the Fuzzy Span Universal Information Extraction (FSUIE) framework. Specifically, our contribution consists of two concepts: fuzzy span loss and fuzzy span attention. Our experimental results on a series of main IE tasks show significant improvement compared to the baseline, especially in terms of fast convergence and strong performance with small amounts of data and training epochs. These results demonstrate the effectiveness and generalization of FSUIE in di
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#21516;&#20276;&#36741;&#23548;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#21516;&#20276;&#36741;&#23548;&#20114;&#21160;&#20013;&#30340;&#25514;&#36766;&#12290;&#26368;&#20339;&#34920;&#29616;&#30340;&#26159;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#65292;&#23427;&#27604;&#29616;&#26377;&#22522;&#20934;&#26356;&#22909;&#19988;&#26356;&#23481;&#26131;&#35299;&#37322;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#26032;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.14911</link><description>&lt;p&gt;
&#22312;&#21516;&#20276;&#36741;&#23548;&#20114;&#21160;&#20013;&#35782;&#21035;&#25514;&#36766;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
"You might think about slightly revising the title": identifying hedges in peer-tutoring interactions. (arXiv:2306.14911v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#21516;&#20276;&#36741;&#23548;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#21516;&#20276;&#36741;&#23548;&#20114;&#21160;&#20013;&#30340;&#25514;&#36766;&#12290;&#26368;&#20339;&#34920;&#29616;&#30340;&#26159;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#65292;&#23427;&#27604;&#29616;&#26377;&#22522;&#20934;&#26356;&#22909;&#19988;&#26356;&#23481;&#26131;&#35299;&#37322;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#26032;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25514;&#36766;&#22312;&#23545;&#35805;&#20114;&#21160;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#21516;&#20276;&#36741;&#23548;&#20013;&#65292;&#23548;&#24072;&#22312;&#32570;&#20047;&#40664;&#22865;&#30340;&#21452;&#20154;&#23545;&#35805;&#20013;&#20351;&#29992;&#25514;&#36766;&#26469;&#20943;&#36731;&#25351;&#20196;&#21644;&#36127;&#21453;&#39304;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24314;&#31435;&#19968;&#20010;&#31649;&#29702;&#19982;&#23398;&#29983;&#30340;&#20146;&#23494;&#20851;&#31995;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#30340;&#36741;&#23548;&#20195;&#29702;&#31995;&#32479;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#27169;&#24577;&#21516;&#20276;&#36741;&#23548;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#35782;&#21035;&#25514;&#36766;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20381;&#36182;&#39044;&#35757;&#32451;&#36164;&#28304;&#21644;&#32467;&#21512;&#31038;&#20250;&#31185;&#23398;&#25991;&#29486;&#35265;&#35299;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#34920;&#29616;&#26469;&#33258;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#24182;&#19988;&#26356;&#23481;&#26131;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#25506;&#32034;&#20102;&#21516;&#20276;&#36741;&#23548;&#23545;&#35805;&#20013;&#29305;&#24449;&#30340;&#29305;&#28857;&#65292;&#24182;&#35782;&#21035;&#20102;&#19968;&#20123;&#26032;&#29305;&#24449;&#20197;&#21450;&#36825;&#31181;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hedges play an important role in the management of conversational interaction. In peer tutoring, they are notably used by tutors in dyads (pairs of interlocutors) experiencing low rapport to tone down the impact of instructions and negative feedback. Pursuing the objective of building a tutoring agent that manages rapport with students in order to improve learning, we used a multimodal peer-tutoring dataset to construct a computational framework for identifying hedges. We compared approaches relying on pre-trained resources with others that integrate insights from the social science literature. Our best performance involved a hybrid approach that outperforms the existing baseline while being easier to interpret. We employ a model explainability tool to explore the features that characterize hedges in peer-tutoring conversations, and we identify some novel features, and the benefits of such a hybrid model approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#36848;&#20102;&#22312;LLMs&#26102;&#20195;&#65292;&#20154;&#26631;&#35760;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#37325;&#35201;&#24615;&#30340;&#35770;&#25454;&#21644;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.14910</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20013;&#20154;&#26631;&#35760;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Human-Labeled Data in the Era of LLMs. (arXiv:2306.14910v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#36848;&#20102;&#22312;LLMs&#26102;&#20195;&#65292;&#20154;&#26631;&#35760;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#37325;&#35201;&#24615;&#30340;&#35770;&#25454;&#21644;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#23450;&#21046;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#26041;&#38754;&#24102;&#26469;&#20102;&#19968;&#22330;&#38761;&#21629;&#65292;&#24182;&#24341;&#21457;&#20102;&#20851;&#20110;&#37325;&#26032;&#23450;&#20041;&#25968;&#25454;&#35201;&#27714;&#30340;&#35752;&#35770;&#12290;LLMs&#30340;&#35757;&#32451;&#21644;&#23454;&#26045;&#25152;&#24102;&#26469;&#30340;&#33258;&#21160;&#21270;&#24341;&#21457;&#20102;&#23545;&#20154;&#24037;&#26631;&#35760;&#24178;&#39044;&#21487;&#33021;&#19981;&#20877;&#20855;&#26377;&#19982;&#30417;&#30563;&#23398;&#20064;&#26102;&#20195;&#30456;&#21516;&#37325;&#35201;&#24615;&#30340;&#35752;&#35770;&#21644;&#26399;&#26395;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#21147;&#30340;&#35770;&#25454;&#65292;&#25903;&#25345;&#22312;LLMs&#26102;&#20195;&#20154;&#26631;&#35760;&#25968;&#25454;&#30340;&#25345;&#32493;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has brought about a revolution in the development of tailored machine learning models and sparked debates on redefining data requirements. The automation facilitated by the training and implementation of LLMs has led to discussions and aspirations that human-level labeling interventions may no longer hold the same level of importance as in the era of supervised learning. This paper presents compelling arguments supporting the ongoing relevance of human-labeled data in the era of LLMs.
&lt;/p&gt;</description></item><item><title>PRISMA-DFLLM&#26159;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;PRISMA&#30340;&#20005;&#26684;&#25253;&#21578;&#25351;&#21335;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#22312;&#39046;&#22495;&#29305;&#23450;&#30340;&#23398;&#26415;&#35770;&#25991;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#24320;&#21551;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2306.14905</link><description>&lt;p&gt;
PRISMA-DFLLM&#65306;PRISMA&#30340;&#19968;&#31181;&#25193;&#23637;&#65292;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
PRISMA-DFLLM: An Extension of PRISMA for Systematic Literature Reviews using Domain-specific Finetuned Large Language Models. (arXiv:2306.14905v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14905
&lt;/p&gt;
&lt;p&gt;
PRISMA-DFLLM&#26159;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;PRISMA&#30340;&#20005;&#26684;&#25253;&#21578;&#25351;&#21335;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#22312;&#39046;&#22495;&#29305;&#23450;&#30340;&#23398;&#26415;&#35770;&#25991;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#24320;&#21551;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#65292;&#25105;&#20204;&#27491;&#22788;&#20110;&#28044;&#29616;&#20986;&#35768;&#22810;&#38024;&#23545;&#19987;&#19994;&#39046;&#22495;&#21644;&#24212;&#29992;&#30340;&#29305;&#23450;&#39046;&#22495;LLMs&#30340;&#38454;&#27573;&#65292;&#36825;&#20123;LLMs&#24050;&#32463;&#38024;&#23545;&#24403;&#21069;&#36890;&#29992;LLMs&#26080;&#27861;&#36866;&#24212;&#30340;&#19987;&#19994;&#39046;&#22495;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#22312;&#23398;&#26415;&#30028;&#65292;&#36825;&#39033;&#25216;&#26415;&#26377;&#28508;&#21147;&#25913;&#21464;&#25105;&#20204;&#36827;&#34892;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65288;SLRs&#65289;&#30340;&#26041;&#24335;&#65292;&#33719;&#21462;&#30693;&#35782;&#21644;&#29983;&#25104;&#26032;&#35265;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;LLMs&#24378;&#22823;&#33021;&#21147;&#21644;Preferred Reporting Items for Systematic Reviews and Meta-Analyses&#65288;PRISMA&#65289;&#30340;&#20005;&#26684;&#25253;&#21578;&#25351;&#21335;&#30340;AI-Enabled&#26041;&#27861;&#26694;&#26550;&#12290;&#36890;&#36807;&#23545;&#36890;&#36807;&#20005;&#26684;SLR&#36807;&#31243;&#36873;&#23450;&#30340;&#39046;&#22495;&#29305;&#23450;&#23398;&#26415;&#35770;&#25991;&#36827;&#34892;LLMs&#24494;&#35843;&#65292;&#25552;&#20986;&#30340;PRISMA-DFLLM&#65288;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#30340;LLMs&#24494;&#35843;&#65289;&#25253;&#21578;&#25351;&#21335;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#21516;&#26102;&#24320;&#21551;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#30740;&#31350;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of open-sourced Large Language Models (LLMs) and efficient finetuning techniques, we are on the cusp of the emergence of numerous domain-specific LLMs that have been finetuned for expertise across specialized fields and applications for which the current general-purpose LLMs are unsuitable. In academia, this technology has the potential to revolutionize the way we conduct systematic literature reviews (SLRs), access knowledge and generate new insights. This paper proposes an AI-enabled methodological framework that combines the power of LLMs with the rigorous reporting guidelines of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA). By finetuning LLMs on domain-specific academic papers that have been selected as a result of a rigorous SLR process, the proposed PRISMA-DFLLM (for Domain-specific Finetuned LLMs) reporting guidelines offer the potential to achieve greater efficiency, reusability and scalability, while also opening the po
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#24773;&#24863;&#30693;&#35782;&#20849;&#20139;&#65292;&#20174;&#31038;&#20132;&#32593;&#32476;&#28040;&#24687;&#20013;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#28508;&#22312;&#36857;&#35937;&#65292;&#26088;&#22312;&#25552;&#20379;&#26089;&#26399;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14903</link><description>&lt;p&gt;
&#20174;&#31038;&#20132;&#32593;&#32476;&#20013;&#26816;&#27979;&#25233;&#37057;&#24773;&#32490;&#24182;&#36827;&#34892;&#24773;&#24863;&#30693;&#35782;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Detect Depression from Social Networks with Sentiment Knowledge Sharing. (arXiv:2306.14903v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#24773;&#24863;&#30693;&#35782;&#20849;&#20139;&#65292;&#20174;&#31038;&#20132;&#32593;&#32476;&#28040;&#24687;&#20013;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#28508;&#22312;&#36857;&#35937;&#65292;&#26088;&#22312;&#25552;&#20379;&#26089;&#26399;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#22312;&#20256;&#25773;&#20154;&#20204;&#30340;&#35266;&#28857;&#12289;&#24773;&#32490;&#12289;&#24605;&#32500;&#21644;&#24656;&#24807;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#30340;&#23553;&#38145;&#26399;&#21518;&#65292;&#25233;&#37057;&#30151;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#35768;&#22810;&#20154;&#20511;&#21161;&#31038;&#20132;&#32593;&#32476;&#34920;&#36798;&#24773;&#32490;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#31038;&#20132;&#32593;&#32476;&#28040;&#24687;&#20013;&#36776;&#21035;&#28508;&#22312;&#30340;&#25233;&#37057;&#30151;&#36857;&#35937;&#26377;&#21161;&#20110;&#26089;&#26399;&#35782;&#21035;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#12290;&#30446;&#21069;&#65292;&#36890;&#36807;&#31038;&#20132;&#32593;&#32476;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#21162;&#21147;&#36890;&#24120;&#20165;&#20381;&#38752;&#23545;&#25991;&#26412;&#20869;&#23481;&#36827;&#34892;&#20998;&#26512;&#65292;&#24573;&#30053;&#20102;&#20854;&#20182;&#28508;&#22312;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#25233;&#37057;&#30151;&#21644;&#36127;&#38754;&#24773;&#32490;&#29366;&#24577;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#23558;&#36825;&#26679;&#30340;&#20851;&#32852;&#20316;&#20026;&#22806;&#37096;&#30693;&#35782;&#30340;&#25972;&#21512;&#21487;&#20197;&#20026;&#26816;&#27979;&#25233;&#37057;&#30151;&#25552;&#20379;&#23453;&#36149;&#30340;&#27934;&#35265;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#35757;&#32451;&#26694;&#26550;DeSK&#65292;&#21033;&#29992;&#24773;&#24863;&#30693;&#35782;&#20849;&#20139;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social network plays an important role in propagating people's viewpoints, emotions, thoughts, and fears. Notably, following lockdown periods during the COVID-19 pandemic, the issue of depression has garnered increasing attention, with a significant portion of individuals resorting to social networks as an outlet for expressing emotions. Using deep learning techniques to discern potential signs of depression from social network messages facilitates the early identification of mental health conditions. Current efforts in detecting depression through social networks typically rely solely on analyzing the textual content, overlooking other potential information. In this work, we conduct a thorough investigation that unveils a strong correlation between depression and negative emotional states. The integration of such associations as external knowledge can provide valuable insights for detecting depression. Accordingly, we propose a multi-task training framework, DeSK, which utilizes share
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24335;&#25935;&#24863;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#29305;&#23450;&#22788;&#29702;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14514</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24335;&#25935;&#24863;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65306;&#35821;&#35328;&#29305;&#23450;&#22788;&#29702;&#19982;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Approach for Formality-Sensitive Machine Translation: Language-Specific Handling and Synthetic Data Generation. (arXiv:2306.14514v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24335;&#25935;&#24863;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#29305;&#23450;&#22788;&#29702;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24335;&#25935;&#24863;&#26426;&#22120;&#32763;&#35793;&#65288;FSMT&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#22235;&#31181;&#30446;&#26631;&#35821;&#35328;&#30340;&#29420;&#29305;&#35821;&#35328;&#29305;&#24615;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#38752;&#20004;&#20010;&#26680;&#24515;&#31574;&#30053;&#65306;&#35821;&#35328;&#29305;&#23450;&#25968;&#25454;&#22788;&#29702;&#21644;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#32463;&#39564;&#24615;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#31361;&#26174;&#20102;&#25968;&#25454;&#20013;&#24515;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#25552;&#31034;&#24037;&#31243;&#31574;&#30053;&#36890;&#36807;&#20135;&#29983;&#20248;&#36136;&#30340;&#21512;&#25104;&#32763;&#35793;&#31034;&#20363;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a data-driven approach for Formality-Sensitive Machine Translation (FSMT) that caters to the unique linguistic properties of four target languages. Our methodology centers on two core strategies: 1) language-specific data handling, and 2) synthetic data generation using large-scale language models and empirical prompt engineering. This approach demonstrates a considerable improvement over the baseline, highlighting the effectiveness of data-centric techniques. Our prompt engineering strategy further improves performance by producing superior synthetic translation examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24212;&#24615;&#32422;&#26463;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Eluder Condition&#31867;&#65292;&#24182;&#38024;&#23545;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#32422;&#26463;&#20998;&#21035;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;&#27492;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#32771;&#34385;&#36890;&#29992;&#20989;&#25968;&#31867;&#21035;&#19979;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#28085;&#30422;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#22823;&#37096;&#20998;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.14468</link><description>&lt;p&gt;
&#36890;&#29992;&#26694;&#26550;&#19979;&#36866;&#24212;&#24615;&#32422;&#26463;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Sequential Decision-Making under Adaptivity Constraints. (arXiv:2306.14468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24212;&#24615;&#32422;&#26463;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Eluder Condition&#31867;&#65292;&#24182;&#38024;&#23545;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#32422;&#26463;&#20998;&#21035;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;&#27492;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#32771;&#34385;&#36890;&#29992;&#20989;&#25968;&#31867;&#21035;&#19979;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#28085;&#30422;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#22823;&#37096;&#20998;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#30740;&#31350;&#36890;&#29992;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#19979;&#23545;&#20004;&#20010;&#36866;&#24212;&#24615;&#32422;&#26463;&#36827;&#34892;&#20102;&#39318;&#27425;&#25506;&#32034;&#65306;&#31574;&#30053;&#20999;&#25442;&#31232;&#32570;&#21644;&#25209;&#27425;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31216;&#20026;Eluder Condition&#31867;&#30340;&#36890;&#29992;&#31867;&#21035;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#31574;&#30053;&#20999;&#25442;&#31232;&#32570;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#31639;&#27861;&#65292;&#22312;EC&#31867;&#21035;&#19978;&#23454;&#29616;&#20102;&#22823;&#32422;$ \widetilde{\mathcal{O}}(\log K)$&#30340;&#20999;&#25442;&#20195;&#20215;&#21644;$\widetilde{\mathcal{O}}(\sqrt{K})$&#30340;&#21518;&#24724;&#20195;&#20215;&#12290;&#23545;&#20110;&#25209;&#27425;&#23398;&#20064;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;$B$&#20010;&#25209;&#27425;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#22823;&#32422;$\widetilde{\mathcal{O}}(\sqrt{K}+K/B)$&#30340;&#21518;&#24724;&#20195;&#20215;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#31532;&#19968;&#31687;&#32771;&#34385;&#36890;&#29992;&#20989;&#25968;&#31867;&#21035;&#19979;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#28085;&#30422;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#20960;&#20046;&#25152;&#26377;&#30340;&#27169;&#22411;&#65292;&#22914;&#34920;&#26684;MDP (Bai et al. 2019; Zhang et al. 2020)&#12289;&#32447;&#24615;MDP (Wang et al. 2021; Gao et al. 2021)&#12289;&#20302;Eluder&#32500;&#24230;MDP (Kong et al. 2021; Gao et al. 2021)&#12289;&#24191;&#20041;&#32447;&#24615;&#20989;&#25968;&#31867;&#21035;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We take the first step in studying general sequential decision-making under two adaptivity constraints: rare policy switch and batch learning. First, we provide a general class called the Eluder Condition class, which includes a wide range of reinforcement learning classes. Then, for the rare policy switch constraint, we provide a generic algorithm to achieve a $\widetilde{\mathcal{O}}(\log K) $ switching cost with a $\widetilde{\mathcal{O}}(\sqrt{K})$ regret on the EC class. For the batch learning constraint, we provide an algorithm that provides a $\widetilde{\mathcal{O}}(\sqrt{K}+K/B)$ regret with the number of batches $B.$ This paper is the first work considering rare policy switch and batch learning under general function classes, which covers nearly all the models studied in the previous works such as tabular MDP (Bai et al. 2019; Zhang et al. 2020), linear MDP (Wang et al. 2021; Gao et al. 2021), low eluder dimension MDP (Kong et al. 2021; Gao et al. 2021), generalized linear fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#23545;&#25239;&#33976;&#39311;&#26041;&#27861;&#65288;FAD&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#28857;&#20113;&#20998;&#31867;&#20013;&#30693;&#35782;&#20256;&#36882;&#30340;&#20449;&#24687;&#25439;&#22833;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#21387;&#32553;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14221</link><description>&lt;p&gt;
&#29305;&#24449;&#23545;&#25239;&#33976;&#39311;&#29992;&#20110;&#28857;&#20113;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Feature Adversarial Distillation for Point Cloud Classification. (arXiv:2306.14221v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#23545;&#25239;&#33976;&#39311;&#26041;&#27861;&#65288;FAD&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#28857;&#20113;&#20998;&#31867;&#20013;&#30693;&#35782;&#20256;&#36882;&#30340;&#20449;&#24687;&#25439;&#22833;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#21387;&#32553;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28857;&#20113;&#30340;&#19981;&#35268;&#21017;&#21644;&#26080;&#24207;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#22312;&#30452;&#25509;&#24212;&#29992;&#20110;&#28857;&#20113;&#20219;&#21153;&#26102;&#20002;&#22833;&#20102;&#24456;&#22810;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#23545;&#25239;&#33976;&#39311;&#65288;Feature Adversarial Distillation&#65292;&#31616;&#31216;FAD&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#28857;&#20113;&#33976;&#39311;&#20013;&#20351;&#29992;&#30340;&#36890;&#29992;&#23545;&#25239;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20943;&#23569;&#30693;&#35782;&#20256;&#36882;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#25439;&#22833;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#38454;&#27573;&#65292;&#21033;&#29992;&#25945;&#24072;&#25552;&#21462;&#30340;&#29305;&#24449;&#20316;&#20026;&#37492;&#21035;&#22120;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#23398;&#29983;&#19981;&#26029;&#29983;&#25104;&#26032;&#29305;&#24449;&#12290;&#23398;&#29983;&#30340;&#29305;&#24449;&#36890;&#36807;&#25915;&#20987;&#25945;&#24072;&#30340;&#21453;&#39304;&#24471;&#21040;&#24471;&#20998;&#65292;&#20197;&#21028;&#26029;&#23398;&#29983;&#26159;&#21542;&#23398;&#20064;&#20102;&#30693;&#35782;&#12290;&#22312;ModelNet40&#21644;ScanObjectNN&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#26631;&#20934;&#28857;&#20113;&#20998;&#31867;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;40&#20493;&#27169;&#22411;&#21387;&#32553;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#30693;&#35782;&#20256;&#36882;&#20013;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the point cloud's irregular and unordered geometry structure, conventional knowledge distillation technology lost a lot of information when directly used on point cloud tasks. In this paper, we propose Feature Adversarial Distillation (FAD) method, a generic adversarial loss function in point cloud distillation, to reduce loss during knowledge transfer. In the feature extraction stage, the features extracted by the teacher are used as the discriminator, and the students continuously generate new features in the training stage. The feature of the student is obtained by attacking the feedback from the teacher and getting a score to judge whether the student has learned the knowledge well or not. In experiments on standard point cloud classification on ModelNet40 and ScanObjectNN datasets, our method reduced the information loss of knowledge transfer in distillation in 40x model compression while maintaining competitive performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#25805;&#32437;&#39118;&#38505;&#65292;&#21363;&#21516;&#19968;&#20915;&#31574;&#25110;&#39044;&#27979;&#21487;&#33021;&#26377;&#22810;&#31181;&#35299;&#37322;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#25915;&#20987;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25110;&#24213;&#23618;&#25968;&#25454;&#20197;&#24433;&#21709;&#35299;&#37322;&#19982;&#30452;&#25509;&#21033;&#29992;&#35299;&#37322;&#38454;&#27573;&#30340;&#31574;&#30053;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#25552;&#20379;&#32773;&#21487;&#36861;&#27714;&#30340;&#20960;&#20010;&#30446;&#26631;&#21644;&#20855;&#20307;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.13885</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#25805;&#32437;&#39118;&#38505;: &#19981;&#19968;&#33268;&#38382;&#39064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Manipulation Risks in Explainable AI: The Implications of the Disagreement Problem. (arXiv:2306.13885v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#25805;&#32437;&#39118;&#38505;&#65292;&#21363;&#21516;&#19968;&#20915;&#31574;&#25110;&#39044;&#27979;&#21487;&#33021;&#26377;&#22810;&#31181;&#35299;&#37322;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#25915;&#20987;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25110;&#24213;&#23618;&#25968;&#25454;&#20197;&#24433;&#21709;&#35299;&#37322;&#19982;&#30452;&#25509;&#21033;&#29992;&#35299;&#37322;&#38454;&#27573;&#30340;&#31574;&#30053;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#25552;&#20379;&#32773;&#21487;&#36861;&#27714;&#30340;&#20960;&#20010;&#30446;&#26631;&#21644;&#20855;&#20307;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25105;&#20204;&#29983;&#27963;&#30340;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#65292;&#36825;&#22686;&#21152;&#20102;&#35299;&#37322;&#36825;&#20123;&#20915;&#31574;&#24182;&#30830;&#20445;&#23427;&#20204;&#19982;&#25105;&#20204;&#24819;&#35201;&#30340;&#20915;&#31574;&#19968;&#33268;&#30340;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#26174;&#29616;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#65292;&#21363;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#21363;&#21516;&#19968;&#20154;&#24037;&#26234;&#33021;&#30340;&#20915;&#31574;&#25110;&#39044;&#27979;&#21487;&#33021;&#26377;&#22810;&#31181;&#35299;&#37322;&#12290;&#34429;&#28982;&#24050;&#32463;&#35748;&#35782;&#21040;&#20102;&#19981;&#19968;&#33268;&#38382;&#39064;&#30340;&#23384;&#22312;&#65292;&#20294;&#19982;&#27492;&#38382;&#39064;&#30456;&#20851;&#30340;&#28508;&#22312;&#24433;&#21709;&#23578;&#26410;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#35299;&#37322;&#25552;&#20379;&#32773;&#21487;&#20197;&#37319;&#29992;&#30340;&#19981;&#21516;&#31574;&#30053;&#65292;&#20197;&#20351;&#36820;&#22238;&#30340;&#35299;&#37322;&#31526;&#21512;&#20182;&#20204;&#30340;&#21033;&#30410;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#25915;&#20987;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25110;&#24213;&#23618;&#25968;&#25454;&#20197;&#24433;&#21709;&#35299;&#37322;&#30340;&#31574;&#30053;&#21644;&#30452;&#25509;&#21033;&#29992;&#35299;&#37322;&#38454;&#27573;&#30340;&#31574;&#30053;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35299;&#37322;&#25552;&#20379;&#32773;&#21487;&#36861;&#27714;&#30340;&#20960;&#20010;&#30446;&#26631;&#21644;&#20855;&#20307;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) systems are increasingly used in high-stakes domains of our life, increasing the need to explain these decisions and to make sure that they are aligned with how we want the decision to be made. The field of Explainable AI (XAI) has emerged in response. However, it faces a significant challenge known as the disagreement problem, where multiple explanations are possible for the same AI decision or prediction. While the existence of the disagreement problem is acknowledged, the potential implications associated with this problem have not yet been widely studied. First, we provide an overview of the different strategies explanation providers could deploy to adapt the returned explanation to their benefit. We make a distinction between strategies that attack the machine learning model or underlying data to influence the explanations, and strategies that leverage the explanation phase directly. Next, we analyse several objectives and concrete scenarios the provid
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.13596</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#36793;&#32536;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#20063;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#24778;&#20154;&#25104;&#21151;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#32972;&#21518;&#30340;&#29702;&#35770;&#21407;&#21017;&#23578;&#19981;&#28165;&#26970;&#65292;&#29305;&#21035;&#26159;&#23427;&#30340;&#38750;&#20984;&#20248;&#21270;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21019;&#24615;&#30340;softmax-attention&#27169;&#22411;$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$&#65292;&#20854;&#20013;$\boldsymbol{X}$&#26159;&#26631;&#35760;&#24207;&#21015;&#65292;$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$&#26159;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$\boldsymbol{p}$&#25110;&#31561;&#20215;&#30340;$\boldsymbol{W}$&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#20250;&#27839;&#30528;&#26041;&#21521;&#25910;&#25947;&#21040;&#20998;&#38548;&#8220;&#23616;&#37096;&#26368;&#20248;&#8221;&#26631;&#35760;&#21644;&#8220;&#38750;&#26368;&#20248;&#8221;&#26631;&#35760;&#30340;&#26368;&#22823;&#36793;&#32536;&#35299;&#12290;&#36825;&#26126;&#30830;&#22320;&#24418;&#24335;&#21270;&#20102;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#31181;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19968;&#33324;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#23884;&#20837;$\boldsymbol{Xv}$&#21644;$\texttt{softmax}(\boldsymbol{XWp})$&#31934;&#32454;&#22320;&#34920;&#24449;&#26631;&#35760;&#30340;&#8220;&#26368;&#20248;&#24615;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
&lt;/p&gt;</description></item><item><title>DiversiGATE&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#27719;&#38598;&#20102;&#22810;&#31181;LLM&#39564;&#35777;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#19968;&#33268;&#24615;&#12289;&#25968;&#23398;&#25552;&#31034;&#21644;WebGPT&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21512;&#35813;&#26694;&#26550;&#30340;&#26032;&#27169;&#22411;&#8220;SelfLearner&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#33258;&#24049;&#30340;&#36755;&#20986;&#20013;&#23398;&#20064;&#24182;&#20248;&#21270;&#24615;&#33021;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;GSM8K&#22522;&#20934;&#27979;&#35797;&#19978;&#25552;&#39640;&#20102;7%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13230</link><description>&lt;p&gt;
DiversiGATE: &#19968;&#20010;&#21487;&#38752;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20840;&#38754;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DiversiGATE: A Comprehensive Framework for Reliable Large Language Models. (arXiv:2306.13230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13230
&lt;/p&gt;
&lt;p&gt;
DiversiGATE&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#27719;&#38598;&#20102;&#22810;&#31181;LLM&#39564;&#35777;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#19968;&#33268;&#24615;&#12289;&#25968;&#23398;&#25552;&#31034;&#21644;WebGPT&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21512;&#35813;&#26694;&#26550;&#30340;&#26032;&#27169;&#22411;&#8220;SelfLearner&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#33258;&#24049;&#30340;&#36755;&#20986;&#20013;&#23398;&#20064;&#24182;&#20248;&#21270;&#24615;&#33021;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;GSM8K&#22522;&#20934;&#27979;&#35797;&#19978;&#25552;&#39640;&#20102;7%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DiversiGATE&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#27719;&#38598;LLM&#39564;&#35777;&#30340;&#22810;&#31181;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#22810;&#26679;&#21270;&#21644;&#32858;&#21512;&#65292;&#22312;&#29616;&#26377;&#30340;&#39564;&#35777;&#26041;&#27861;&#19978;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#20363;&#22914;&#33258;&#19968;&#33268;&#24615;&#12289;&#25968;&#23398;&#25552;&#31034;&#21644;WebGPT&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#8220;SelfLearner&#8221;&#27169;&#22411;&#65292;&#31526;&#21512;DiversiGATE&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#33258;&#24049;&#30340;&#36755;&#20986;&#20013;&#23398;&#20064;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#19981;&#26029;&#23436;&#21892;&#20854;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;SelfLearner&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#26415;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;GSM8K&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;LLMs&#65292;&#22312;GSM8K&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#21487;&#35266;&#30340;54.8%-&gt;61.8%&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce DiversiGATE, a unified framework that consolidates diverse methodologies for LLM verification. The proposed framework comprises two main components: Diversification and Aggregation which provide a holistic perspective on existing verification approaches, such as Self-Consistency, Math Prompter and WebGPT. Furthermore, we propose a novel `SelfLearner' model that conforms to the DiversiGATE framework which can learn from its own outputs and refine its performance over time, leading to improved accuracy. To evaluate the effectiveness of SelfLearner, we conducted a rigorous series of experiments, including tests on synthetic data as well as on popular arithmetic reasoning benchmarks such as GSM8K. Our results demonstrate that our approach outperforms traditional LLMs, achieving a considerable 54.8% -&gt; 61.8% improvement on the GSM8K benchmark.
&lt;/p&gt;</description></item><item><title>&#36880;&#27493;&#32423;&#32852;&#27169;&#22411;FuXi&#22312;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#36798;&#21040;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.12873</link><description>&lt;p&gt;
FuXi: &#19968;&#20010;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#32423;&#32852;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FuXi: A cascade machine learning forecasting system for 15-day global weather forecast. (arXiv:2306.12873v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12873
&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#32423;&#32852;&#27169;&#22411;FuXi&#22312;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#36798;&#21040;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;0.25&#24230;&#31354;&#38388;&#20998;&#36776;&#29575;&#19979;&#30340;10&#22825;&#22825;&#27668;&#39044;&#25253;&#20013;&#24050;&#32463;&#34920;&#29616;&#20986;&#27604;&#27431;&#27954;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#20013;&#24515;(ECMWF)&#30340;&#39640;&#20998;&#36776;&#29575;&#39044;&#25253;(HRES)&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#22312;15&#22825;&#39044;&#25253;&#20013;&#34920;&#29616;&#19982;ECMWF&#38598;&#21512;&#24179;&#22343;(EM)&#30456;&#24403;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32531;&#35299;&#39044;&#25253;&#35823;&#24046;&#30340;&#31215;&#32047;&#23545;&#20110;&#26377;&#25928;&#30340;&#38271;&#26399;&#39044;&#25253;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#20943;&#23569;&#31215;&#32047;&#35823;&#24046;&#30340;&#21162;&#21147;&#65292;&#21253;&#25324;&#33258;&#22238;&#24402;&#22810;&#26102;&#38388;&#27493;&#38271;&#25439;&#22833;&#65292;&#20294;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#21457;&#29616;&#26080;&#27861;&#22312;&#30701;&#21644;&#38271;&#23548;&#20986;&#26102;&#38388;&#19978;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FuXi&#65292;&#36825;&#26159;&#19968;&#20010;&#32423;&#32852;&#26426;&#22120;&#23398;&#20064;&#22825;&#27668;&#39044;&#27979;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#20998;&#36776;&#29575;&#20026;0.25&#24230;&#12289;&#26102;&#38388;&#20998;&#36776;&#29575;&#20026;6&#23567;&#26102;&#30340;15&#22825;&#20840;&#29699;&#39044;&#27979;&#12290;FuXi&#22522;&#20110;&#32423;&#32852;&#38598;&#21512;&#27169;&#22411;&#24320;&#21457;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#24182;&#20943;&#23569;&#20102;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#12290;&#20351;&#29992;&#31354;&#27668;&#28201;&#24230;&#65292;&#27604;&#28287;&#24230;&#21644;&#20301;&#21183;&#39640;&#24230;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;(RMSE)&#21644;&#24322;&#24120;&#30456;&#20851;&#31995;&#25968;(ACC)&#35780;&#20272;&#20102;FuXi&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;ECMWF HRES&#30456;&#27604;&#65292;FuXi&#22312;15&#22825;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#31215;&#32047;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, due to the rapid development of machine learning (ML) models for weather forecasting, state-of-the-art ML models have shown superior performance compared to the European Centre for Medium-Range Weather Forecasts (ECMWF)'s high-resolution forecast (HRES) in 10-day forecasts at a spatial resolution of 0.25 degree. However, the challenge remains to perform comparably to the ECMWF ensemble mean (EM) in 15-day forecasts. Previous studies have demonstrated the importance of mitigating the accumulation of forecast errors for effective long-term forecasts. Despite numerous efforts to reduce accumulation errors, including autoregressive multi-time step loss, using a single model is found to be insufficient to achieve optimal performance in both short and long lead times. Therefore, we present FuXi, a cascaded ML weather forecasting system that provides 15-day global forecasts with a temporal resolution of 6 hours and a spatial resolution of 0.25 degree. FuXi is develope
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#33258;&#21160;&#35774;&#35745;&#20102;&#19968;&#20010;&#20013;&#22830;&#22788;&#29702;&#22120;(CPU)&#65292;&#36825;&#26159;&#20154;&#31867;&#35774;&#35745;&#36807;&#30340;&#26368;&#22797;&#26434;&#30340;&#35013;&#32622;&#20043;&#19968;&#65292;&#20174;&#32780;&#25506;&#32034;&#20102;&#26426;&#22120;&#35774;&#35745;&#30340;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.12456</link><description>&lt;p&gt;
&#26426;&#22120;&#35774;&#35745;&#30340;&#26497;&#38480;&#25361;&#25112;&#65306;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#33258;&#21160;CPU&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Pushing the Limits of Machine Design: Automated CPU Design with AI. (arXiv:2306.12456v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#33258;&#21160;&#35774;&#35745;&#20102;&#19968;&#20010;&#20013;&#22830;&#22788;&#29702;&#22120;(CPU)&#65292;&#36825;&#26159;&#20154;&#31867;&#35774;&#35745;&#36807;&#30340;&#26368;&#22797;&#26434;&#30340;&#35013;&#32622;&#20043;&#19968;&#65292;&#20174;&#32780;&#25506;&#32034;&#20102;&#26426;&#22120;&#35774;&#35745;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#27963;&#21160;&#8212;&#8212;&#26500;&#24314;&#19968;&#20010;&#28385;&#36275;&#32473;&#23450;&#30446;&#26631;&#21644;&#38480;&#21046;&#26465;&#20214;&#30340;&#24037;&#20214;&#25551;&#36848;&#8212;&#8212;&#21306;&#21035;&#20110;&#20154;&#31867;&#21644;&#20256;&#32479;&#26426;&#22120;&#65292;&#36171;&#20104;&#26426;&#22120;&#20154;&#31867;&#27700;&#24179;&#25110;&#26356;&#39640;&#30340;&#35774;&#35745;&#33021;&#21147;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#36861;&#27714;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#26426;&#22120;&#24050;&#32463;&#23637;&#31034;&#20102;&#36816;&#29992;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#35774;&#35745;&#26032;&#26448;&#26009;&#12289;&#34507;&#30333;&#36136;&#21644;&#35745;&#31639;&#26426;&#31243;&#24207;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#24037;&#20214;&#30340;&#35774;&#35745;&#31354;&#38388;&#30456;&#23545;&#36739;&#23567;&#65292;&#22240;&#27492;&#65292;&#8220;&#26426;&#22120;&#26159;&#21542;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#35774;&#35745;&#65311;&#8221;&#25104;&#20026;&#20102;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#25299;&#23637;&#26426;&#22120;&#35774;&#35745;&#30340;&#36793;&#30028;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#33258;&#21160;&#35774;&#35745;&#19968;&#20010;&#20013;&#22830;&#22788;&#29702;&#22120;(CPU)&#65292;&#21363;&#35745;&#31639;&#26426;&#30340;&#22823;&#33041;&#65292;&#36825;&#26159;&#20154;&#31867;&#35774;&#35745;&#36807;&#30340;&#26368;&#22797;&#26434;&#30340;&#35013;&#32622;&#20043;&#19968;&#12290;&#35813;&#26041;&#27861;&#20174;&#21482;&#26377;&#22806;&#37096;&#36755;&#20837;&#36755;&#20986;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;CPU&#35774;&#35745;&#30340;&#30005;&#36335;&#36923;&#36753;&#65292;&#35813;&#36923;&#36753;&#29992;&#20108;&#20803;&#25512;&#28436;&#22270;(BSD)&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Design activity -- constructing an artifact description satisfying given goals and constraints -- distinguishes humanity from other animals and traditional machines, and endowing machines with design abilities at the human level or beyond has been a long-term pursuit. Though machines have already demonstrated their abilities in designing new materials, proteins, and computer programs with advanced artificial intelligence (AI) techniques, the search space for designing such objects is relatively small, and thus, "Can machines design like humans?" remains an open question. To explore the boundary of machine design, here we present a new AI approach to automatically design a central processing unit (CPU), the brain of a computer, and one of the world's most intricate devices humanity have ever designed. This approach generates the circuit logic, which is represented by a graph structure called Binary Speculation Diagram (BSD), of the CPU design from only external input-output observations
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#22312;Poincar&#233;&#21452;&#26354;&#29699;&#27169;&#22411;&#20013;&#36816;&#29992;&#36229;bolic&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#21306;&#22495;&#20869;&#20687;&#32032;&#23884;&#20837;&#30340;&#21322;&#24452;&#21464;&#21270;&#20316;&#20026;&#26032;&#30340;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#65292;&#20197;&#25552;&#21319;&#22495;&#36716;&#31227;&#19979;&#35821;&#20041;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11180</link><description>&lt;p&gt;
&#36229;bolic&#27963;&#36291;&#23398;&#20064;&#22312;&#22495;&#36716;&#31227;&#19979;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Active Learning for Semantic Segmentation under Domain Shift. (arXiv:2306.11180v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11180
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#22312;Poincar&#233;&#21452;&#26354;&#29699;&#27169;&#22411;&#20013;&#36816;&#29992;&#36229;bolic&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#21306;&#22495;&#20869;&#20687;&#32032;&#23884;&#20837;&#30340;&#21322;&#24452;&#21464;&#21270;&#20316;&#20026;&#26032;&#30340;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#65292;&#20197;&#25552;&#21319;&#22495;&#36716;&#31227;&#19979;&#35821;&#20041;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22495;&#36716;&#31227;&#19979;&#30340;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#22522;&#20110;&#22270;&#20687;&#21306;&#22495;&#21644;&#20266;&#26631;&#31614;&#30340;&#20027;&#21160;&#23398;&#20064;&#33719;&#21462;&#31574;&#30053;&#26159;&#26368;&#20808;&#36827;&#30340;&#12290;&#22312;&#21306;&#22495;&#20869;&#23384;&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#21487;&#20197;&#35782;&#21035;&#20986;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#20687;&#32032;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#20027;&#21160;&#23398;&#20064;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#35745;&#38480;&#21046;&#65292;&#20266;&#26631;&#31614;&#30340;&#21464;&#21270;&#20165;&#38480;&#20110;&#36873;&#25321;&#31867;&#21035;&#30340;&#36718;&#24275;&#65292;&#38480;&#21046;&#20102;&#26368;&#32456;&#30340;&#20027;&#21160;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;Poincar&#233;&#21452;&#26354;&#29699;&#27169;&#22411;&#20013;&#20351;&#29992;&#36229;bolic&#26041;&#27861;&#26469;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#21306;&#22495;&#20869;&#20687;&#32032;&#23884;&#20837;&#30340;&#21322;&#24452;&#21464;&#21270;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#12290;&#36825;&#28304;&#20110;&#19968;&#31181;&#26080;&#23618;&#27425;&#32422;&#26463;&#35757;&#32451;&#30340;&#36229;bolic&#31354;&#38388;&#30340;&#26032;&#39062;&#20960;&#20309;&#29305;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#31867;&#21035;&#34987;&#26144;&#23556;&#21040;&#20855;&#26377;&#30456;&#24403;&#20869;&#31867;&#21322;&#24452;&#26041;&#24046;&#30340;&#32039;&#20945;&#36229;bolic&#21306;&#22495;&#65292;&#22240;&#20026;&#27169;&#22411;&#23558;&#38590;&#20197;&#35299;&#37322;&#30340;&#31867;&#21035;&#25918;&#32622;&#22312;&#26356;&#23494;&#38598;&#30340;&#36229;bolic&#21306;&#22495;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the task of semantic segmentation (SS) under domain shift, active learning (AL) acquisition strategies based on image regions and pseudo labels are state-of-the-art (SoA). The presence of diverse pseudo-labels within a region identifies pixels between different classes, which is a labeling efficient active learning data acquisition strategy. However, by design, pseudo-label variations are limited to only select the contours of classes, limiting the final AL performance. We approach AL for SS in the Poincar\'e hyperbolic ball model for the first time and leverage the variations of the radii of pixel embeddings within regions as a novel data acquisition strategy. This stems from a novel geometric property of a hyperbolic space trained without enforced hierarchies, which we experimentally prove. Namely, classes are mapped into compact hyperbolic areas with a comparable intra-class radii variance, as the model places classes of increasing explainable difficulty at denser hyperbolic are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10946</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network. (arXiv:2306.10946v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#33616;&#31639;&#27861;&#22312;&#30456;&#23545;&#25104;&#29087;&#38454;&#27573;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25512;&#33616;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#20363;&#22914;&#22312;&#26053;&#28216;&#39046;&#22495;&#65292;&#36873;&#25321;&#36866;&#21512;&#30340;&#26053;&#28216;&#26223;&#28857;&#23646;&#24615;&#27969;&#31243;&#20316;&#20026;&#25512;&#33616;&#22522;&#30784;&#36739;&#20026;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#25913;&#36827;&#30340;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;(Att-KGCN)&#65292;&#33258;&#21160;&#35821;&#20041;&#22320;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#23558;&#30456;&#23545;&#30456;&#20284;&#30340;&#20301;&#32622;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#36890;&#36807;&#25512;&#29702;&#26053;&#23458;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#12290;&#23454;&#39564;&#20013;&#65292;&#37319;&#29992;&#32034;&#31185;&#29305;&#25289;&#23707;-&#20063;&#38376;&#30340;&#26053;&#28216;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#26053;&#28216;&#39046;&#22495;&#30340;&#26223;&#28857;&#25512;&#33616;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation algorithm based on knowledge graphs is at a relatively mature stage. However, there are still some problems in the recommendation of specific areas. For example, in the tourism field, selecting suitable tourist attraction attributes process is complicated as the recommendation basis for tourist attractions. In this paper, we propose the improved Attention Knowledge Graph Convolution Network model, named (Att-KGCN), which automatically discovers the neighboring entities of the target scenic spot semantically. The attention layer aggregates relatively similar locations and represents them with an adjacent vector. Then, according to the tourist's preferred choices, the model predicts the probability of similar spots as a recommendation system. A knowledge graph dataset of tourist attractions used based on tourism data on Socotra Island-Yemen. Through experiments, it is verified that the Attention Knowledge Graph Convolution Network has a good effect on the recommendatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25506;&#27979;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#36328;&#24230;&#25552;&#21462;&#21644;&#26102;&#25935;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08952</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models. (arXiv:2306.08952v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25506;&#27979;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#36328;&#24230;&#25552;&#21462;&#21644;&#26102;&#25935;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#25512;&#29702;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#35768;&#22810;&#20107;&#23454;&#26159;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;&#12290;&#20363;&#22914;&#65292;&#36816;&#21160;&#21592;&#20250;&#19981;&#26102;&#22320;&#26356;&#25442;&#29699;&#38431;&#65292;&#19981;&#21516;&#30340;&#25919;&#24220;&#23448;&#21592;&#20250;&#23450;&#26399;&#36827;&#34892;&#36873;&#20030;&#12290;&#20808;&#21069;&#30340;&#26102;&#38388;&#30456;&#20851;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#24448;&#24448;&#22312;&#26102;&#38388;&#36328;&#24230;&#25110;&#38382;&#39064;&#31867;&#22411;&#30340;&#28085;&#30422;&#19978;&#23384;&#22312;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25506;&#27979;&#25968;&#25454;&#38598;\tempreason&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#19977;&#20010;&#26102;&#38388;&#25512;&#29702;&#32423;&#21035;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36328;&#24230;&#25552;&#21462;&#21644;&#26102;&#25935;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#23553;&#38381;&#20070;&#24335;QA&#12289;&#24320;&#25918;&#20070;&#24335;QA&#21644;&#25512;&#29702;QA&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#24050;&#22312;https://github.com/DAMO-NLP-SG/TempReason&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset \tempreason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach. Our code and data are released on https://github.com/DAMO-NLP-SG/TempReason.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08158</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#38750;&#39044;&#26399;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#35770;&#25991;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20559;&#35265;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#21361;&#23475;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#20511;&#37492;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20559;&#35265;&#31867;&#22411;&#12289;&#37327;&#21270;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#23545;&#20110;&#37327;&#21270;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#20559;&#35265;&#24230;&#37327;&#24182;&#19981;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20559;&#35265;&#65292;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#26159;&#34920;&#38754;&#30340;&#65292;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#20174;&#20004;&#20010;&#35282;&#24230;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#22914;&#20309;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35843;&#25972;LLM&#21644;&#35843;&#25972;LLM&#26102;&#22312;&#21738;&#37324;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.05817</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22914;&#20309;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21463;&#30410;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Can Recommender Systems Benefit from Large Language Models: A Survey. (arXiv:2306.05817v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#20174;&#20004;&#20010;&#35282;&#24230;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#22914;&#20309;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35843;&#25972;LLM&#21644;&#35843;&#25972;LLM&#26102;&#22312;&#21738;&#37324;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#21305;&#37197;&#20114;&#32852;&#32593;&#24212;&#29992;&#31243;&#24207;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#26032;&#20852;&#33021;&#21147;&#65288;&#20363;&#22914;&#25351;&#20196;&#36319;&#36394;&#12289;&#25512;&#29702;&#65289;&#65292;&#20174;&#32780;&#20026;&#23558;LLM&#35843;&#25972;&#21040;&#25512;&#33616;&#31995;&#32479;&#20013;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#30340;&#30740;&#31350;&#26041;&#21521;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#24212;&#29992;&#23548;&#21521;&#30340;&#35282;&#24230;&#23545;&#27492;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#20004;&#20010;&#27491;&#20132;&#30340;&#35282;&#24230;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#22914;&#20309;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35843;&#25972;LLM&#21644;&#35843;&#25972;LLM&#26102;&#22312;&#21738;&#37324;&#35843;&#25972;&#12290;&#23545;&#20110;&#8220;&#22312;&#21738;&#37324;&#8221;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;LLM&#22312;&#25512;&#33616;&#27969;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#20013;&#21487;&#33021;&#21457;&#25381;&#30340;&#20316;&#29992;&#65292;&#21363;&#29305;&#24449;&#24037;&#31243;&#12289;&#29305;&#24449;&#32534;&#30721;&#22120;&#12289;&#35780;&#20998;/&#25490;&#21517;&#20989;&#25968;&#21644;&#27969;&#31243;&#25511;&#21046;&#22120;&#12290;&#23545;&#20110;&#8220;&#22914;&#20309;&#8221;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#20174;&#32780;&#24471;&#20986;&#20004;&#20010;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#26631;&#20934;&#65292;&#21363;&#26159;&#21542;&#35843;&#25972;LLM&#21644;&#26159;&#21542;&#23558;LLM&#20316;&#20026;&#29420;&#31435;&#27169;&#22411;&#25110;&#28151;&#21512;&#27169;&#22411;&#32452;&#20214;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#23558;LLM&#35843;&#25972;&#21040;RS&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#28508;&#22312;&#26041;&#21521;&#65292;&#21253;&#25324;&#19982;&#29616;&#26377;&#31995;&#32479;&#30340;&#38598;&#25104;&#12289;&#29992;&#25143;&#21453;&#39304;&#12289;&#35780;&#20272;&#24230;&#37327;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems (RS) play important roles to match users' information needs for Internet applications. In natural language processing (NLP) domains, large language model (LLM) has shown astonishing emergent abilities (e.g., instruction following, reasoning), thus giving rise to the promising research direction of adapting LLM to RS for performance enhancements and user experience improvements. In this paper, we conduct a comprehensive survey on this research direction from an application-oriented view. We first summarize existing research works from two orthogonal perspectives: where and how to adapt LLM to RS. For the "WHERE" question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, and pipeline controller. For the "HOW" question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLMs or not, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#35282;&#23398;&#20064;&#21644;k-means&#32858;&#31867;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05437</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#65306;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
One-step Multi-view Clustering with Diverse Representation. (arXiv:2306.05437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#35282;&#23398;&#20064;&#21644;k-means&#32858;&#31867;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#22240;&#20854;&#33021;&#22815;&#21033;&#29992;&#19981;&#21516;&#35270;&#35282;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#25928;&#26524;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#35282;&#23398;&#20064;&#21644;k-means&#32858;&#31867;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#21516;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering has attracted broad attention due to its capacity to utilize consistent and complementary information among views. Although tremendous progress has been made recently, most existing methods undergo high complexity, preventing them from being applied to large-scale tasks. Multi-view clustering via matrix factorization is a representative to address this issue. However, most of them map the data matrices into a fixed dimension, which limits the expressiveness of the model. Moreover, a range of methods suffer from a two-step process, i.e., multimodal learning and the subsequent $k$-means, inevitably causing a sub-optimal clustering result. In light of this, we propose a one-step multi-view clustering with diverse representation method, which incorporates multi-view learning and $k$-means into a unified framework. Specifically, we first project original data matrices into various latent spaces to attain comprehensive information and auto-weight them in a self-supervis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#65292;&#23427;&#20351;&#29992;&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#23545;&#36739;&#22823;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.04265</link><description>&lt;p&gt;
&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#22312;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning. (arXiv:2306.04265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#65292;&#23427;&#20351;&#29992;&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#23545;&#36739;&#22823;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#22270;&#30340;&#26412;&#36136;&#19982;&#21516;&#36136;&#22270;&#26174;&#33879;&#19981;&#21516;&#65292;&#36825;&#34920;&#26126;1-hop&#20197;&#22806;&#30340;&#32858;&#21512;&#26041;&#24335;&#24182;&#24341;&#36215;&#26089;&#26399;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#23610;&#24230;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#32622;&#25442;&#31561;&#21464;&#24615;&#65292;&#39640;&#25928;&#24615;&#21644;&#31232;&#30095;&#24615;&#30340;Haar-type&#22270;&#26694;&#26550;&#65292;&#22312;&#22270;&#19978;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#26500;&#24314;&#30340;&#22270;&#26694;&#26550;&#35774;&#35745;&#20102;&#22270;&#26694;&#26550;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#12290;&#23454;&#39564;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;9&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#24322;&#36136;&#22270;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#30456;&#23545;&#36739;&#22823;&#21644;&#26356;&#23494;&#38598;&#30340;&#36830;&#25509;&#30340;&#22823;&#37096;&#20998;&#24322;&#36136;&#25968;&#25454;&#38598;&#65289;&#19978;&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#22312;&#20854;&#20313;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The nature of heterophilous graphs is significantly different with that of homophilous graphs, which suggests aggregations beyond 1-hop neighborhood and causes difficulties in early graph neural network models. In this paper, we develop a new way to implement multi-scale extraction via constructing Haar-type graph framelets with desired properties of permutation equivariance, efficiency, and sparsity, for deep learning tasks on graphs. We further deisgn a graph framelet neural network model PEGFAN using our constructed graph framelets. The experiments are conducted on a synthetic dataset and 9 benchmark datasets to compare performance with other state-of-the-art models. The result shows that our model can achieve best performance on certain datasets of heterophilous graphs (including the majority of heterophilous datasets with relatively larger sizes and denser connections) and competitive performance on the remaining.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#37327;&#21270;&#32034;&#24341;&#35843;&#21046;&#65288;QIM&#65289;&#30340;&#22522;&#20110;&#21487;&#36870;&#25968;&#25454;&#38544;&#34255;&#65288;RDH&#65289;&#30340;&#38745;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#21487;&#29992;&#24615;&#12289;&#23481;&#37327;&#21644;&#20445;&#30495;&#24230;&#26041;&#38754;&#30340;&#24369;&#28857;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#32467;&#26524;&#39564;&#35777;&#20102;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17879</link><description>&lt;p&gt;
&#21487;&#36870;&#37327;&#21270;&#32034;&#24341;&#35843;&#21046;&#29992;&#20110;&#38745;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Reversible Quantization Index Modulation for Static Deep Neural Network Watermarking. (arXiv:2305.17879v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#37327;&#21270;&#32034;&#24341;&#35843;&#21046;&#65288;QIM&#65289;&#30340;&#22522;&#20110;&#21487;&#36870;&#25968;&#25454;&#38544;&#34255;&#65288;RDH&#65289;&#30340;&#38745;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#21487;&#29992;&#24615;&#12289;&#23481;&#37327;&#21644;&#20445;&#30495;&#24230;&#26041;&#38754;&#30340;&#24369;&#28857;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#32467;&#26524;&#39564;&#35777;&#20102;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38745;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27700;&#21360;&#25216;&#26415;&#36890;&#24120;&#37319;&#29992;&#19981;&#21487;&#36870;&#30340;&#26041;&#27861;&#23558;&#27700;&#21360;&#23884;&#20837;DNN&#27169;&#22411;&#26435;&#37325;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23545;&#27700;&#21360;&#27169;&#22411;&#36896;&#25104;&#27704;&#20037;&#24615;&#25439;&#22351;&#65292;&#26080;&#27861;&#28385;&#36275;&#23436;&#25972;&#24615;&#39564;&#35777;&#30340;&#35201;&#27714;&#12290;&#21487;&#36870;&#25968;&#25454;&#38544;&#34255;&#65288;RDH&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#21487;&#29992;&#24615;&#12289;&#23481;&#37327;&#21644;&#20445;&#30495;&#24230;&#26041;&#38754;&#23384;&#22312;&#24369;&#28857;&#65292;&#38459;&#30861;&#20102;&#20854;&#23454;&#38469;&#37319;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RDH&#30340;&#38745;&#24577;DNN&#27700;&#21360;&#26041;&#26696;&#65292;&#37319;&#29992;&#37327;&#21270;&#32034;&#24341;&#35843;&#21046;&#65288;QIM&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#26696;&#37319;&#29992;&#20102;&#19968;&#32500;&#37327;&#21270;&#22120;&#30340;&#26032;&#26041;&#27861;&#36827;&#34892;&#27700;&#21360;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26041;&#26696;&#26469;&#35299;&#20915;DNN&#30340;&#23436;&#25972;&#24615;&#20445;&#25252;&#21644;&#21512;&#27861;&#35748;&#35777;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23545;&#35757;&#32451;&#25439;&#22833;&#21644;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#27169;&#25311;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#20248;&#36234;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Static deep neural network (DNN) watermarking techniques typically employ irreversible methods to embed watermarks into the DNN model weights. However, this approach causes permanent damage to the watermarked model and fails to meet the requirements of integrity authentication. Reversible data hiding (RDH) methods offer a potential solution, but existing approaches suffer from weaknesses in terms of usability, capacity, and fidelity, hindering their practical adoption. In this paper, we propose a novel RDH-based static DNN watermarking scheme using quantization index modulation (QIM). Our scheme incorporates a novel approach based on a one-dimensional quantizer for watermark embedding. Furthermore, we design two schemes to address the challenges of integrity protection and legitimate authentication for DNNs. Through simulation results on training loss and classification accuracy, we demonstrate the feasibility and effectiveness of our proposed schemes, highlighting their superior adapt
&lt;/p&gt;</description></item><item><title>LoReTTa&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20855;&#26377;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#20013;&#36716;&#25442;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14243</link><description>&lt;p&gt;
&#20869;&#23481;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#36716;&#25442;&#22120;&#35757;&#32451;&#19982; LoReTTa
&lt;/p&gt;
&lt;p&gt;
Training Transitive and Commutative Multimodal Transformers with LoReTTa. (arXiv:2305.14243v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14243
&lt;/p&gt;
&lt;p&gt;
LoReTTa&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20855;&#26377;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#20013;&#36716;&#25442;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#65292;&#25910;&#38598;&#20004;&#20010;&#21305;&#37197;&#30340;&#24418;&#24577;A&#21644;B&#25110;B&#21644;C&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#24456;&#22256;&#38590;&#65292;&#33719;&#24471;&#21253;&#21547;&#19977;&#20010;&#23545;&#40784;&#24418;&#24577;A&#12289;B&#21644;C&#30340;&#25968;&#25454;&#38598;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LoReTTa&#20197;&#24212;&#23545;&#36825;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;&#32467;&#21512;&#20102;&#22240;&#26524;&#25513;&#30721;&#24314;&#27169;&#21644;&#20132;&#25442;&#24459;&#21644;&#20256;&#36882;&#24615;&#30340;&#35268;&#21017;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#20013;&#36716;&#25442;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#21512;&#25104;&#26174;&#30528;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting a multimodal dataset with two paired modalities A and B or B and C is difficult in practice. Obtaining a dataset with three aligned modalities A, B, and C is even more challenging. For example, some public medical datasets have only genetic sequences and microscopic images for one patient, and only genetic sequences and radiological images for another - but no dataset includes both microscopic and radiological images for the same patient. This makes it difficult to integrate and combine all modalities into a large pre-trained neural network. We introduce LoReTTa (Linking mOdalities with a tRansitive and commutativE pre-Training sTrAtegy) to address this understudied problem. Our self-supervised framework combines causal masked modeling with the rules of commutativity and transitivity to transition within and between different modalities. Thus, it can model the relation A -&gt; C with A -&gt; B -&gt; C. Given a dataset containing only the disjoint combinations (A, B) and (B, C), we sh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HDR&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#21382;&#21490;&#20419;&#38144;&#25968;&#25454;&#65292;&#26469;&#25429;&#25417;&#20419;&#38144;&#36716;&#21270;&#27169;&#24335;&#65292;&#36798;&#21040;&#26356;&#22909;&#22320;&#36866;&#24212;&#20419;&#38144;&#27169;&#24335;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.12837</link><description>&lt;p&gt;
&#25429;&#25417;&#20419;&#38144;&#26399;&#38388;&#30340;&#36716;&#21270;&#29575;&#27874;&#21160;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#21382;&#21490;&#25968;&#25454;&#20877;&#21033;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Capturing Conversion Rate Fluctuation during Sales Promotions: A Novel Historical Data Reuse Approach. (arXiv:2305.12837v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HDR&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#21382;&#21490;&#20419;&#38144;&#25968;&#25454;&#65292;&#26469;&#25429;&#25417;&#20419;&#38144;&#36716;&#21270;&#27169;&#24335;&#65292;&#36798;&#21040;&#26356;&#22909;&#22320;&#36866;&#24212;&#20419;&#38144;&#27169;&#24335;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#21270;&#29575;&#65288;CVR&#65289;&#39044;&#27979;&#26159;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#30340;&#26680;&#24515;&#32452;&#20214;&#20043;&#19968;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;CVR&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#35757;&#32451;&#33391;&#22909;&#30340;CVR&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#20419;&#38144;&#26399;&#38388;&#20063;&#32463;&#24120;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#20854;&#20013;&#20256;&#32479;&#26041;&#27861;&#19981;&#20877;&#36215;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23547;&#27714;&#24320;&#21457;&#26367;&#20195;&#24314;&#27169;&#25216;&#26415;&#29992;&#20110;CVR&#39044;&#27979;&#12290;&#35266;&#23519;&#21040;&#19981;&#21516;&#20419;&#38144;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#30340;&#36141;&#20080;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#29992;&#21382;&#21490;&#20419;&#38144;&#25968;&#25454;&#20197;&#25429;&#25417;&#20419;&#38144;&#36716;&#21270;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21382;&#21490;&#25968;&#25454;&#20877;&#21033;&#29992;&#65288;HDR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#26816;&#32034;&#21382;&#21490;&#19978;&#30456;&#20284;&#30340;&#20419;&#38144;&#25968;&#25454;&#65292;&#28982;&#21518;&#20351;&#29992;&#33719;&#21462;&#30340;&#25968;&#25454;&#24494;&#35843;CVR&#39044;&#27979;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#20419;&#38144;&#27169;&#24335;&#12290;HDR&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#33258;&#21160;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Conversion rate (CVR) prediction is one of the core components in online recommender systems, and various approaches have been proposed to obtain accurate and well-calibrated CVR estimation. However, we observe that a well-trained CVR prediction model often performs sub-optimally during sales promotions. This can be largely ascribed to the problem of the data distribution shift, in which the conventional methods no longer work. To this end, we seek to develop alternative modeling techniques for CVR prediction. Observing similar purchase patterns across different promotions, we propose reusing the historical promotion data to capture the promotional conversion patterns. Herein, we propose a novel \textbf{H}istorical \textbf{D}ata \textbf{R}euse (\textbf{HDR}) approach that first retrieves historically similar promotion data and then fine-tunes the CVR prediction model with the acquired data for better adaptation to the promotion mode. HDR consists of three components: an automated data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#23398;&#20064;&#24230;&#37327;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;&#65292;&#36890;&#36807;&#35757;&#32451;&#25342;&#21462;&#25104;&#21151;&#39044;&#27979;&#22120;&#21644;&#23398;&#20064;&#25342;&#21462;&#36136;&#37327;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#33021;&#22815;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#24378;&#21147;&#25235;&#25569;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.10272</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#24230;&#37327;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Package Manipulation via Learned Metrics of Pick Success. (arXiv:2305.10272v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#23398;&#20064;&#24230;&#37327;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;&#65292;&#36890;&#36807;&#35757;&#32451;&#25342;&#21462;&#25104;&#21151;&#39044;&#27979;&#22120;&#21644;&#23398;&#20064;&#25342;&#21462;&#36136;&#37327;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#33021;&#22815;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#24378;&#21147;&#25235;&#25569;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#21487;&#20197;&#38477;&#20302;&#29289;&#27969;&#25104;&#26412;&#65292;&#26368;&#32456;&#38477;&#20302;&#28040;&#36153;&#21697;&#20215;&#26684;&#65292;&#25552;&#39640;&#20132;&#36135;&#36895;&#24230;&#65292;&#24182;&#22686;&#24378;&#23545;&#21171;&#21160;&#21147;&#27874;&#21160;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#33258;&#21160;&#21270;&#37325;&#22797;&#20219;&#21153;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;&#20294;&#22823;&#22810;&#25968;&#26159;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#12290;&#20174;&#26434;&#20081;&#30340;&#22534;&#22534;&#20013;&#25361;&#36873;&#29289;&#21697;&#31561;&#20219;&#21153;&#30452;&#21040;&#26368;&#36817;&#25165;&#21464;&#24471;&#36275;&#22815;&#24378;&#22823;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#20154;&#24037;&#24178;&#39044;&#19979;&#36827;&#34892;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#30340;Robot Induction&#65288;Robin&#65289;&#32676;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;&#65292;&#35813;&#32676;&#21033;&#29992;&#22312;&#23454;&#38469;&#29983;&#20135;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25342;&#21462;&#25104;&#21151;&#39044;&#27979;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31995;&#32479;&#22312;&#36229;&#36807;394K&#20010;&#25342;&#21462;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#23427;&#29992;&#20110;&#25226;&#27599;&#22825;&#39640;&#36798;5&#30334;&#19975;&#20010;&#21253;&#35065;&#36827;&#34892;&#20102;&#20998;&#31163;&#65292;&#26412;&#25991;&#30340;&#35780;&#20272;&#26399;&#38388;&#25805;&#20316;&#20102;&#36229;&#36807;2&#20159;&#20010;&#21253;&#35065;&#12290;&#24320;&#21457;&#30340;&#23398;&#20064;&#25342;&#21462;&#36136;&#37327;&#24230;&#37327;&#23454;&#26102;&#25490;&#21517;&#21508;&#31181;&#25342;&#21462;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#39640;&#25104;&#21151;&#29575;&#30340;&#24378;&#21147;&#25235;&#25569;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to workforce fluctuations. The past few years have seen increased interest in automating such repeated tasks but mostly in controlled settings. Tasks such as picking objects from unstructured, cluttered piles have only recently become robust enough for large-scale deployment with minimal human intervention.  This paper demonstrates a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which utilizes a pick success predictor trained on real production data. Specifically, the system was trained on over 394K picks. It is used for singulating up to 5~million packages per day and has manipulated over 200~million packages during this paper's evaluation period.  The developed learned pick quality measure ranks various pick alternatives in real-time and p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25581;&#31034;CNN&#23618;&#20869;&#35821;&#20041;&#20449;&#24687;&#30340;&#27969;&#21160;&#21644;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.07663</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23450;&#37327;&#35821;&#20041;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Quantified Semantic Comparison of Convolutional Neural Networks. (arXiv:2305.07663v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25581;&#31034;CNN&#23618;&#20869;&#35821;&#20041;&#20449;&#24687;&#30340;&#27969;&#21160;&#21644;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#24212;&#29992;&#22788;&#20110;&#39046;&#20808;&#22320;&#20301;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#24037;&#20316;&#21407;&#29702;&#21364;&#24456;&#38590;&#38416;&#26126;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36825;&#31867;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65292;&#27169;&#22411;&#36873;&#25321;&#36824;&#24212;&#32771;&#34385;&#20505;&#36873;&#27169;&#22411;&#22312;&#27169;&#22411;&#36879;&#26126;&#24615;&#26041;&#38754;&#22914;&#20309;&#34920;&#31034;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;CNN&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#25581;&#31034;CNN&#23618;&#20869;&#35821;&#20041;&#20449;&#24687;&#30340;&#27969;&#21160;&#21644;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#30340;&#33879;&#21517;&#25216;&#26415;&#20316;&#20026;&#22522;&#30784;&#65292;&#36825;&#20123;&#25216;&#26415;&#29992;&#20110;&#33719;&#24471;&#27599;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#27010;&#24565;&#30340;&#20840;&#23616;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#22312;&#27979;&#35797;&#36755;&#20837;&#19978;&#30340;&#28608;&#27963;&#36827;&#34892;&#27604;&#36739;&#12290;&#26412;&#24037;&#20316;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#21644;&#20004;&#20010;&#19981;&#21516;&#33539;&#22260;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state-of-the-art in convolutional neural networks (CNNs) for computer vision excels in performance, while remaining opaque. But due to safety regulations for safety-critical applications, like perception for automated driving, the choice of model should also take into account how candidate models represent semantic information for model transparency reasons. To tackle this yet unsolved problem, our work proposes two methods for quantifying the similarity between semantic information in CNN latent spaces. These allow insights into both the flow and similarity of semantic information within CNN layers, and into the degree of their similitude between different networks. As a basis, we use renown techniques from the field of explainable artificial intelligence (XAI), which are used to obtain global vector representations of semantic concepts in each latent space. These are compared with respect to their activation on test inputs. When applied to three diverse object detectors and two d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;&#30340;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#37319;&#29992;&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#12289;&#32858;&#28966;&#25439;&#22833;&#21644;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#31561;&#25216;&#26415;&#65292;&#22312;LVIS&#19978;&#21462;&#24471;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07011</link><description>&lt;p&gt;
&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#65306;&#35270;&#35273;&#21464;&#21387;&#22120;&#19979;&#30340;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2305.07011v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;&#30340;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#37319;&#29992;&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#12289;&#32858;&#28966;&#25439;&#22833;&#21644;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#31561;&#25216;&#26415;&#65292;&#22312;LVIS&#19978;&#21462;&#24471;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21306;&#22495;&#24863;&#30693;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;RO-ViT&#65289;&#65292;&#19968;&#31181;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#22635;&#34917;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24314;&#35758;&#38543;&#26426;&#35009;&#21098;&#24182;&#35843;&#25972;&#20301;&#32622;&#23884;&#20837;&#30340;&#21306;&#22495;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#25972;&#20010;&#22270;&#20687;&#20301;&#32622;&#23884;&#20837;&#12290;&#36825;&#26356;&#22909;&#22320;&#21305;&#37197;&#20102;&#26816;&#27979;&#24494;&#35843;&#38454;&#27573;&#20013;&#21306;&#22495;&#32423;&#21035;&#19978;&#20351;&#29992;&#20301;&#32622;&#23884;&#20837;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29992;&#32858;&#28966;&#25439;&#22833;&#26367;&#25442;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;softmax&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#37027;&#20123;&#26377;&#20449;&#24687;&#37327;&#20294;&#38590;&#20197;&#25429;&#25417;&#30340;&#20363;&#23376;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20197;&#25913;&#36827;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;LVIS&#21644;COCO&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#23436;&#25972;&#27169;&#22411;&#21644;&#38646;-shot&#36716;&#31227;&#24615;&#33021;&#12290;RO-ViT&#22312;LVIS&#19978;&#23454;&#29616;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#65292;&#36229;&#36807;&#29616;&#26377;&#26368;&#20339;&#26041;&#27861;5.8&#20010;&#30334;&#20998;&#28857;&#65292;&#21516;&#26102;&#36824;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#38646;-shot&#36716;&#31227;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a contrastive image-text pretraining recipe to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we propose to randomly crop and resize regions of positional embeddings instead of using the whole image positional embeddings. This better matches the use of positional embeddings at region-level in the detection finetuning phase. In addition, we replace the common softmax cross entropy loss in contrastive learning with focal loss to better learn the informative yet difficult examples. Finally, we leverage recent advances in novel object proposals to improve open-vocabulary detection finetuning. We evaluate our full model on the LVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer. RO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the best existing approach by +5.8 points in addition to competitive zero-shot transfer detec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#29992;&#20110;&#36817;&#22330;&#19981;&#35268;&#21017;SAR&#36229;&#20998;&#36776;&#29575;&#30340;&#31639;&#27861;&#65292;&#20197;&#24212;&#23545;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#20013;&#36935;&#21040;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20026;&#23454;&#29616;&#36793;&#32536;&#21644;&#29289;&#32852;&#32593;(IoT)&#25216;&#26415;&#22880;&#23450;&#25216;&#26415;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2305.02074</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36817;&#22330;&#19981;&#35268;&#21017;SAR&#36229;&#20998;&#36776;&#29575;&#30340;&#35270;&#35273;Transformer&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Vision Transformer Approach for Efficient Near-Field Irregular SAR Super-Resolution. (arXiv:2305.02074v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#29992;&#20110;&#36817;&#22330;&#19981;&#35268;&#21017;SAR&#36229;&#20998;&#36776;&#29575;&#30340;&#31639;&#27861;&#65292;&#20197;&#24212;&#23545;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#20013;&#36935;&#21040;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20026;&#23454;&#29616;&#36793;&#32536;&#21644;&#29289;&#32852;&#32593;(IoT)&#25216;&#26415;&#22880;&#23450;&#25216;&#26415;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#38024;&#23545;&#38750;&#35268;&#21017;&#25195;&#25551;&#20960;&#20309;&#30340;&#36817;&#22330;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;(SAR)&#36229;&#20998;&#36776;&#29575;&#31639;&#27861;&#12290;&#38543;&#30528;&#31532;&#20116;&#20195;(5G)&#27627;&#31859;&#27874;(mmWave)&#35774;&#22791;&#21464;&#24471;&#36234;&#26469;&#36234;&#23454;&#24800;&#21644;&#21487;&#29992;&#65292;&#39640;&#20998;&#36776;&#29575;SAR&#25104;&#20687;&#23545;&#29992;&#25143;&#24212;&#29992;&#21644;&#38750;&#23454;&#39564;&#23460;&#29615;&#22659;&#21464;&#24471;&#21487;&#34892;&#12290;&#26032;&#20852;&#24212;&#29992;&#22914;&#25163;&#25345;&#25104;&#20687;&#12289;&#26080;&#20154;&#26426;&#25104;&#20687;&#21644;&#27773;&#36710;SAR&#38754;&#20020;&#30528;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#30340;&#20960;&#20010;&#29420;&#29305;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#24674;&#22797;SAR&#22270;&#20687;&#38656;&#35201;&#22312;&#25972;&#20010;&#25195;&#25551;&#26399;&#38388;&#20102;&#35299;&#38453;&#21015;&#20301;&#32622;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#22522;&#20110;&#30456;&#26426;&#30340;&#23450;&#20301;&#31995;&#32479;&#65292;&#33021;&#22815;&#36275;&#22815;&#22320;&#20272;&#35745;&#20301;&#32622;&#65292;&#20294;&#23454;&#29616;&#39640;&#25928;&#30340;&#24674;&#22797;&#31639;&#27861;&#26159;&#23454;&#29616;&#36793;&#32536;&#21644;&#29289;&#32852;&#32593;(IoT)&#25216;&#26415;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#38750;&#21512;&#20316;&#36817;&#22330;SAR&#37319;&#26679;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20294;&#26159;&#36825;&#20123;&#31639;&#27861;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#35797;&#39564;&#24615;&#30340;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop a novel super-resolution algorithm for near-field synthetic-aperture radar (SAR) under irregular scanning geometries. As fifth-generation (5G) millimeter-wave (mmWave) devices are becoming increasingly affordable and available, high-resolution SAR imaging is feasible for end-user applications and non-laboratory environments. Emerging applications such freehand imaging, wherein a handheld radar is scanned throughout space by a user, unmanned aerial vehicle (UAV) imaging, and automotive SAR face several unique challenges for high-resolution imaging. First, recovering a SAR image requires knowledge of the array positions throughout the scan. While recent work has introduced camera-based positioning systems capable of adequately estimating the position, recovering the algorithm efficiently is a requirement to enable edge and Internet of Things (IoT) technologies. Efficient algorithms for non-cooperative near-field SAR sampling have been explored in recent work, bu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#20687;&#32032;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20174;&#29289;&#20307;&#20013;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#36716;&#21270;&#20026;&#8220;&#38382;&#39064;&#8221;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#38543;&#21518;&#23398;&#20064;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#26469;&#36827;&#34892;&#25511;&#21046;&#65292;&#22312;&#38745;&#24577;&#21644;&#38750;&#38745;&#24577;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#20165;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#32780;&#19988;&#22260;&#32469;&#30528;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#29289;&#20307;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.13892</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Discovering Object-Centric Generalized Value Functions From Pixels. (arXiv:2304.13892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#20687;&#32032;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20174;&#29289;&#20307;&#20013;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#36716;&#21270;&#20026;&#8220;&#38382;&#39064;&#8221;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#38543;&#21518;&#23398;&#20064;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#26469;&#36827;&#34892;&#25511;&#21046;&#65292;&#22312;&#38745;&#24577;&#21644;&#38750;&#38745;&#24577;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#20165;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#32780;&#19988;&#22260;&#32469;&#30528;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#29289;&#20307;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23637;&#29616;&#20102;&#20174;&#39640;&#32500;&#36755;&#20837;&#20013;&#25552;&#21462;&#26377;&#29992;&#34920;&#31034;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#26159;&#25163;&#24037;&#36741;&#21161;&#20219;&#21153;&#21644;&#20266;&#22870;&#21169;&#12290;&#33258;&#21160;&#21270;&#22320;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#23398;&#20064;&#27492;&#31867;&#34920;&#31034;&#65292;&#20197;&#26399;&#23454;&#29616;&#25511;&#21046;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35797;&#22270;&#20174;&#29289;&#20307;&#20013;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#23558;&#23427;&#20204;&#36716;&#21270;&#20026;&#26102;&#38388;&#19978;&#36830;&#36143;&#30340;&#8220;&#38382;&#39064;&#8221;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#38543;&#21518;&#23398;&#20064;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#26469;&#36827;&#34892;&#25511;&#21046;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38745;&#24577;&#21644;&#38750;&#38745;&#24577;&#35774;&#32622;&#19979;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#34987;&#21457;&#29616;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#20165;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#32780;&#19988;&#22260;&#32469;&#30528;&#29289;&#20307;&#65292;&#36825;&#20123;&#29289;&#20307;&#23545;&#20219;&#21153;&#30340;&#21464;&#21270;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning has shown significant progress in extracting useful representations from high-dimensional inputs albeit using hand-crafted auxiliary tasks and pseudo rewards. Automatically learning such representations in an object-centric manner geared towards control and fast adaptation remains an open research problem. In this paper, we introduce a method that tries to discover meaningful features from objects, translating them to temporally coherent "question" functions and leveraging the subsequent learned general value functions for control. We compare our approach with state-of-the-art techniques alongside other ablations and show competitive performance in both stationary and non-stationary settings. Finally, we also investigate the discovered general value functions and through qualitative analysis show that the learned representations are not only interpretable but also, centered around objects that are invariant to changes across tasks facilitating fast adaptatio
&lt;/p&gt;</description></item><item><title>EPVT&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#30142;&#30149;&#19981;&#30456;&#20851;&#22270;&#20687;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23884;&#20837;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#21644;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#36827;&#34892;&#39046;&#22495;&#19968;&#33324;&#21270;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2304.01508</link><description>&lt;p&gt;
EPVT: &#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#22312;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#39046;&#22495;&#19968;&#33324;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition. (arXiv:2304.01508v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01508
&lt;/p&gt;
&lt;p&gt;
EPVT&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#30142;&#30149;&#19981;&#30456;&#20851;&#22270;&#20687;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23884;&#20837;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#21644;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#36827;&#34892;&#39046;&#22495;&#19968;&#33324;&#21270;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#24050;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#65292;&#32780;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#37096;&#32626;&#36825;&#20123;&#31995;&#32479;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#20110;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#20110;&#19982;&#30142;&#30149;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65288;&#22914;&#26263;&#35282;&#12289;&#27987;&#23494;&#27611;&#21457;&#65289;&#65292;&#23548;&#33268;&#22312;&#30475;&#19981;&#35265;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#19968;&#33324;&#21270;&#26041;&#27861;&#8212;&#8212;EPVT&#65292;&#23427;&#23558;&#25552;&#31034;&#23884;&#20837;&#21040;Vision Transformer&#20013;&#65292;&#20197;&#21327;&#21516;&#23398;&#20064;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EPVT&#21033;&#29992;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#65292;&#27599;&#20010;&#39046;&#22495;&#25552;&#31034;&#37117;&#25198;&#28436;&#39046;&#22495;&#19987;&#23478;&#30340;&#35282;&#33394;&#65292;&#20197;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#65307;&#20197;&#21450;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#33719;&#24471;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#36890;&#29992;&#30693;&#35782;&#12290;&#20026;&#20102;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#21644;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#65292;&#23427;&#20351;&#24471;&#39046;&#22495;&#25552;&#31034;&#19982;&#20849;&#20139;&#25552;&#31034;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#20302;&#31209;&#20056;&#24615;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin lesion recognition using deep learning has made remarkable progress, and there is an increasing need for deploying these systems in real-world scenarios. However, recent research has revealed that deep neural networks for skin lesion recognition may overly depend on disease-irrelevant image artifacts (i.e. dark corners, dense hairs), leading to poor generalization in unseen environments. To address this issue, we propose a novel domain generalization method called EPVT, which involves embedding prompts into the vision transformer to collaboratively learn knowledge from diverse domains. Concretely, EPVT leverages a set of domain prompts, each of which plays as a domain expert, to capture domain-specific knowledge; and a shared prompt for general knowledge over the entire dataset. To facilitate knowledge sharing and the interaction of different prompts, we introduce a domain prompt generator that enables low-rank multiplicative updates between domain prompts and the shared prompt. A
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.09901</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;3&#19978;&#30340;mCPT&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26694;&#26550;&#26816;&#27979;&#30340;&#22810;&#35821;&#35328;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38646;&#26679;&#26412;&#30340;&#35199;&#29677;&#29273;&#35821;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#33719;&#32988;&#31995;&#32479;&#65292;&#24182;&#22312;&#21478;&#22806;&#20843;&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#25361;&#25112;&#22312;&#20110;&#22312;&#21482;&#26377;&#23569;&#37327;&#25110;&#38646;&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#19968;&#32452;14&#20010;&#26694;&#26550;&#65292;&#21363;&#22810;&#35821;&#35328;&#22810;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;&#38500;&#20102;&#25551;&#36848;&#31995;&#32479;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23884;&#20837;&#31354;&#38388;&#20998;&#26512;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#22914;&#20309;&#25903;&#25345;&#26694;&#26550;&#26816;&#27979;&#20197;&#25512;&#36827;&#35745;&#31639;&#26694;&#26550;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#27425;&#30340;&#26041;&#27861;&#26469;&#23457;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;&#27835;&#29702;&#23457;&#35745;&#12289;&#27169;&#22411;&#23457;&#35745;&#21644;&#24212;&#29992;&#23457;&#35745;&#65292;&#35299;&#20915;LLMs&#24102;&#26469;&#30340;&#20262;&#29702;&#21644;&#31038;&#20250;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.08500</link><description>&lt;p&gt;
&#23457;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#20010;&#19977;&#23618;&#27425;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Auditing large language models: a three-layered approach. (arXiv:2302.08500v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#27425;&#30340;&#26041;&#27861;&#26469;&#23457;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;&#27835;&#29702;&#23457;&#35745;&#12289;&#27169;&#22411;&#23457;&#35745;&#21644;&#24212;&#29992;&#23457;&#35745;&#65292;&#35299;&#20915;LLMs&#24102;&#26469;&#30340;&#20262;&#29702;&#21644;&#31038;&#20250;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#22823;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#24191;&#27867;&#20351;&#29992;&#20063;&#20276;&#38543;&#30528;&#37325;&#22823;&#30340;&#20262;&#29702;&#21644;&#31038;&#20250;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#23457;&#35745;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#27835;&#29702;&#26426;&#21046;&#65292;&#26377;&#21161;&#20110;&#30830;&#20445;AI&#31995;&#32479;&#35774;&#35745;&#21644;&#37096;&#32626;&#30340;&#36947;&#24503;&#12289;&#27861;&#24459;&#21644;&#25216;&#26415;&#30340;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23457;&#35745;&#31243;&#24207;&#26080;&#27861;&#35299;&#20915;LLMs&#24102;&#26469;&#30340;&#27835;&#29702;&#25361;&#25112;&#65292;&#22240;&#20026;LLMs&#26174;&#31034;&#20986;&#26032;&#20852;&#33021;&#21147;&#65292;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27010;&#36848;&#19968;&#31181;&#26032;&#39062;&#30340;&#23457;&#35745;LLMs&#30340;&#34013;&#22270;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#27425;&#30340;&#26041;&#27861;&#65292;&#21363;&#27835;&#29702;&#23457;&#35745;&#65288;&#38024;&#23545;&#35774;&#35745;&#21644;&#20256;&#25773;LLMs&#30340;&#25216;&#26415;&#25552;&#20379;&#21830;&#65289;&#12289;&#27169;&#22411;&#23457;&#35745;&#65288;&#38024;&#23545;LLMs&#36827;&#34892;&#39044;&#35757;&#32451;&#20294;&#23578;&#26410;&#21457;&#24067;&#30340;&#23457;&#35745;&#65289;&#21644;&#24212;&#29992;&#23457;&#35745;&#65288;&#22522;&#20110;LLMs&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#23457;&#35745;&#65289;&#65292;&#30456;&#20114;&#34917;&#20805;&#21644;&#30456;&#20114;&#36890;&#30693;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23457;&#35745;&#22312;LLMs&#19978;&#30340;&#23454;&#26045;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#20262;&#29702;&#21644;&#31038;&#20250;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) represent a major advance in artificial intelligence (AI) research. However, the widespread use of LLMs is also coupled with significant ethical and social challenges. Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks. In this article, we address that gap by outlining a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training but prior to their release), and application audits (of applications based on LLMs) complement and inform each other. We show how audits, when conducte
&lt;/p&gt;</description></item><item><title>GibbsDDRM&#26159;&#19968;&#31181;&#25193;&#23637;&#30340;&#23616;&#37096;&#25240;&#21472;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#36870;&#38382;&#39064;&#20013;&#32447;&#24615;&#31639;&#23376;&#26410;&#30693;&#30340;&#30450;&#22330;&#26223;&#12290;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#20102;&#25968;&#25454;&#12289;&#27979;&#37327;&#21644;&#32447;&#24615;&#31639;&#23376;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#30450;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;&#35821;&#38899;&#21435;&#28151;&#21709;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#39640;&#24615;&#33021;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#36870;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.12686</link><description>&lt;p&gt;
GibbsDDRM:&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#30450;&#36870;&#38382;&#39064;&#30340;&#23616;&#37096;&#25240;&#21472;Gibbs&#37319;&#26679;&#22120;&#65292;&#24102;&#26377;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#30340;&#20808;&#39564;&#12290;(arXiv:2301.12686v2 [cs.LG] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration. (arXiv:2301.12686v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12686
&lt;/p&gt;
&lt;p&gt;
GibbsDDRM&#26159;&#19968;&#31181;&#25193;&#23637;&#30340;&#23616;&#37096;&#25240;&#21472;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#36870;&#38382;&#39064;&#20013;&#32447;&#24615;&#31639;&#23376;&#26410;&#30693;&#30340;&#30450;&#22330;&#26223;&#12290;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#20102;&#25968;&#25454;&#12289;&#27979;&#37327;&#21644;&#32447;&#24615;&#31639;&#23376;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#30450;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;&#35821;&#38899;&#21435;&#28151;&#21709;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#39640;&#24615;&#33021;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#36870;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#32447;&#24615;&#36870;&#38382;&#39064;&#20013;&#65292;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#21151;&#29992;&#20316;&#20808;&#39564;&#65292;&#30446;&#26631;&#26159;&#20174;&#22122;&#22768;&#32447;&#24615;&#27979;&#37327;&#20013;&#37325;&#26500;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20102;&#35299;&#32447;&#24615;&#31639;&#23376;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GibbsDDRM&#65292;&#23427;&#26159;&#23558;Denoising Diffusion Restoration Models(DDRM)&#25193;&#23637;&#21040;&#32447;&#24615;&#27979;&#37327;&#31639;&#23376;&#26410;&#30693;&#30340;&#30450;&#22330;&#26223;&#12290;GibbsDDRM&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#20102;&#25968;&#25454;&#12289;&#27979;&#37327;&#21644;&#32447;&#24615;&#31639;&#23376;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#39640;&#25928;&#30340;Gibbs&#37319;&#26679;&#22120;&#30340;&#21464;&#20307;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#38382;&#39064;&#19981;&#21487;&#30693;&#30340;&#65292;&#24847;&#21619;&#30528;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#36870;&#38382;&#39064;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;&#23613;&#31649;&#20351;&#29992;&#20102;&#31616;&#21333;&#30340;&#36890;&#29992;&#20808;&#39564;&#26469;&#22788;&#29702;&#24213;&#23618;&#32447;&#24615;&#31639;&#23376;&#65292;&#35813;&#26041;&#27861;&#22312;&#30450;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;&#35821;&#38899;&#21435;&#28151;&#21709;&#20219;&#21153;&#19978;&#22343;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained diffusion models have been successfully used as priors in a variety of linear inverse problems, where the goal is to reconstruct a signal from noisy linear measurements. However, existing approaches require knowledge of the linear operator. In this paper, we propose GibbsDDRM, an extension of Denoising Diffusion Restoration Models (DDRM) to a blind setting in which the linear measurement operator is unknown. GibbsDDRM constructs a joint distribution of the data, measurements, and linear operator by using a pre-trained diffusion model for the data prior, and it solves the problem by posterior sampling with an efficient variant of a Gibbs sampler. The proposed method is problem-agnostic, meaning that a pre-trained diffusion model can be applied to various inverse problems without fine-tuning. In experiments, it achieved high performance on both blind image deblurring and vocal dereverberation tasks, despite the use of simple generic priors for the underlying linear operators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#21453;&#21521;&#25293;&#21334;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#20013;&#22312;&#25307;&#21215;&#22810;&#20010;&#26410;&#30693;&#21644;&#26377;&#31574;&#30053;&#30340;&#24037;&#20316;&#32773;&#26102;&#20986;&#29616;&#30340;&#25968;&#25454;&#21487;&#20449;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.08563</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#24863;&#30693;&#29575;&#23398;&#20064;&#30340;CMAB&#26041;&#26696;&#65292;&#36890;&#36807;&#21487;&#20449;&#25968;&#25454;&#25910;&#38598;&#22312;&#20154;&#32676;&#20013;&#25239;&#20987;COVID-19
&lt;/p&gt;
&lt;p&gt;
A Semi-supervised Sensing Rate Learning based CMAB Scheme to Combat COVID-19 by Trustful Data Collection in the Crowd. (arXiv:2301.08563v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#21453;&#21521;&#25293;&#21334;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#20013;&#22312;&#25307;&#21215;&#22810;&#20010;&#26410;&#30693;&#21644;&#26377;&#31574;&#30053;&#30340;&#24037;&#20316;&#32773;&#26102;&#20986;&#29616;&#30340;&#25968;&#25454;&#21487;&#20449;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#65292;&#25307;&#21215;&#21487;&#20449;&#21644;&#39640;&#36136;&#37327;&#30340;&#24037;&#20316;&#32773;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#20551;&#35774;&#24037;&#20316;&#32773;&#30340;&#33021;&#21147;&#26159;&#20107;&#20808;&#24050;&#30693;&#30340;&#65292;&#35201;&#20040;&#20551;&#35774;&#24179;&#21488;&#19968;&#26086;&#25509;&#25910;&#21040;&#20182;&#20204;&#25910;&#38598;&#30340;&#25968;&#25454;&#23601;&#30693;&#36947;&#20182;&#20204;&#30340;&#33021;&#21147;&#12290;&#23454;&#38469;&#19978;&#65292;&#20026;&#20102;&#38477;&#20302;&#25104;&#26412;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#25910;&#20837;&#65292;&#35768;&#22810;&#26377;&#31574;&#30053;&#30340;&#24037;&#20316;&#32773;&#19981;&#35802;&#23454;&#22320;&#25191;&#34892;&#20854;&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#21521;&#24179;&#21488;&#25253;&#21578;&#34394;&#20551;&#25968;&#25454;&#65292;&#36825;&#34987;&#31216;&#20026;&#34394;&#20551;&#25968;&#25454;&#25915;&#20987;&#12290;&#23545;&#20110;&#24179;&#21488;&#26469;&#35828;&#65292;&#35780;&#20272;&#25152;&#25910;&#21040;&#30340;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#21313;&#20998;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#21322;&#30417;&#30563;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#21453;&#21521;&#25293;&#21334;&#65288;SCMABA&#65289;&#30340;&#28608;&#21169;&#26426;&#21046;&#26469;&#35299;&#20915;MCS&#20013;&#22810;&#20010;&#26410;&#30693;&#21644;&#26377;&#31574;&#30053;&#30340;&#24037;&#20316;&#32773;&#30340;&#25307;&#32856;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24037;&#20316;&#32773;&#25307;&#21215;&#24314;&#27169;&#20026;&#22810;&#33218;&#36172;&#21338;&#21453;&#21521;&#25293;&#21334;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#26469;&#20998;&#31163;&#25506;&#32034;&#21644;&#24320;&#21457;&#65292;&#23558;&#24050;&#25307;&#21215;&#30340;&#24037;&#20316;&#32773;&#30340;&#24863;&#30693;&#29575;&#35270;&#20026;&#8220;&#33218;&#8220;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recruitment of trustworthy and high-quality workers is an important research issue for MCS. Previous studies either assume that the qualities of workers are known in advance, or assume that the platform knows the qualities of workers once it receives their collected data. In reality, to reduce costs and thus maximize revenue, many strategic workers do not perform their sensing tasks honestly and report fake data to the platform, which is called False data attacks. And it is very hard for the platform to evaluate the authenticity of the received data. In this paper, an incentive mechanism named Semi-supervision based Combinatorial Multi-Armed Bandit reverse Auction (SCMABA) is proposed to solve the recruitment problem of multiple unknown and strategic workers in MCS. First, we model the worker recruitment as a multi-armed bandit reverse auction problem and design an UCB-based algorithm to separate the exploration and exploitation, regarding the Sensing Rates (SRs) of recruited worke
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#24230;&#20013;&#26356;&#24555;&#30340;&#26368;&#22823;&#20869;&#31215;&#25628;&#32034;&#31639;&#27861; BanditMIPS&#65292;&#35813;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#19982;&#32500;&#24230;&#26080;&#20851;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2212.07551</link><description>&lt;p&gt;
&#22312;&#39640;&#32500;&#24230;&#20013;&#26356;&#24555;&#30340;&#26368;&#22823;&#20869;&#31215;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Faster Maximum Inner Product Search in High Dimensions. (arXiv:2212.07551v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#24230;&#20013;&#26356;&#24555;&#30340;&#26368;&#22823;&#20869;&#31215;&#25628;&#32034;&#31639;&#27861; BanditMIPS&#65292;&#35813;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#19982;&#32500;&#24230;&#26080;&#20851;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20869;&#31215;&#25628;&#32034;&#65288;MIPS&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20219;&#21153;&#65292;&#22914;&#25512;&#33616;&#31995;&#32479;&#12290;&#32473;&#23450;&#19968;&#20010;&#26597;&#35810;&#21521;&#37327;&#21644;d&#32500;&#31354;&#38388;&#20013;&#30340;n&#20010;&#21407;&#23376;&#21521;&#37327;&#65292;MIPS&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19982;&#26597;&#35810;&#21521;&#37327;&#20855;&#26377;&#26368;&#39640;&#20869;&#31215;&#30340;&#21407;&#23376;&#12290;&#29616;&#26377;&#30340;MIPS&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#33267;&#23569;&#20026;O(&#8730;d)&#65292;&#22312;&#39640;&#32500;&#35774;&#32622;&#19979;&#21464;&#24471;&#35745;&#31639;&#19978;&#31105;&#27490;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BanditMIPS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;MIPS&#31639;&#27861;&#65292;&#20854;&#22797;&#26434;&#24230;&#19982;d&#26080;&#20851;&#12290;BanditMIPS&#36890;&#36807;&#23376;&#37319;&#26679;&#22352;&#26631;&#26469;&#20272;&#35745;&#27599;&#20010;&#21407;&#23376;&#30340;&#20869;&#31215;&#65292;&#24182;&#36866;&#24212;&#24615;&#22320;&#35780;&#20272;&#26356;&#26377;&#21069;&#26223;&#30340;&#21407;&#23376;&#30340;&#22352;&#26631;&#12290;&#20855;&#20307;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#21463;&#21040;&#20102;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#65292;BanditMIPS&#20197;&#39640;&#27010;&#29575;&#36820;&#22238;&#27491;&#30830;&#30340;&#31572;&#26696;&#65292;&#21516;&#26102;&#23558;&#22797;&#26434;&#24230;&#20174;O(&#8730;d)&#25913;&#36827;&#21040;O(1)&#12290;&#25105;&#20204;&#36824;&#22312;&#22235;&#20010;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum Inner Product Search (MIPS) is a ubiquitous task in machine learning applications such as recommendation systems. Given a query vector and $n$ atom vectors in $d$-dimensional space, the goal of MIPS is to find the atom that has the highest inner product with the query vector. Existing MIPS algorithms scale at least as $O(\sqrt{d})$, which becomes computationally prohibitive in high-dimensional settings. In this work, we present BanditMIPS, a novel randomized MIPS algorithm whose complexity is independent of $d$. BanditMIPS estimates the inner product for each atom by subsampling coordinates and adaptively evaluates more coordinates for more promising atoms. The specific adaptive sampling strategy is motivated by multi-armed bandits. We provide theoretical guarantees that BanditMIPS returns the correct answer with high probability, while improving the complexity in $d$ from $O(\sqrt{d})$ to $O(1)$. We also perform experiments on four synthetic and real-world datasets and demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#21644;&#35780;&#20272;&#24191;&#20041;&#35268;&#21010;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23618;&#27425;&#21270;&#20998;&#35299;&#21644;&#20351;&#29992;&#22270;&#35770;&#25512;&#23548;&#20986;&#30340;&#32456;&#27490;&#35770;&#35777;&#65292;&#21487;&#20197;&#35299;&#20915;&#24191;&#20041;&#35268;&#21010;&#20013;&#30340;&#38590;&#39064;&#65292;&#24182;&#25351;&#23548;&#32508;&#21512;&#21644;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2212.02823</link><description>&lt;p&gt;
&#23618;&#27425;&#21270;&#20998;&#35299;&#21644;&#20998;&#26512;&#24191;&#20041;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Decomposition and Analysis for Generalized Planning. (arXiv:2212.02823v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#21644;&#35780;&#20272;&#24191;&#20041;&#35268;&#21010;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23618;&#27425;&#21270;&#20998;&#35299;&#21644;&#20351;&#29992;&#22270;&#35770;&#25512;&#23548;&#20986;&#30340;&#32456;&#27490;&#35770;&#35777;&#65292;&#21487;&#20197;&#35299;&#20915;&#24191;&#20041;&#35268;&#21010;&#20013;&#30340;&#38590;&#39064;&#65292;&#24182;&#25351;&#23548;&#32508;&#21512;&#21644;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#21644;&#35780;&#20272;&#24191;&#20041;&#35268;&#21010;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#24191;&#27867;&#30340;&#30456;&#20851;&#35268;&#21010;&#38382;&#39064;&#12290;&#34429;&#28982;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24191;&#20041;&#35268;&#21010;&#30340;&#32508;&#21512;&#21644;&#23398;&#20064;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30446;&#26631;&#65292;&#20294;&#30001;&#20110;&#23545;&#32473;&#23450;&#24191;&#20041;&#35268;&#21010;&#30340;&#33539;&#22260;&#21644;&#25928;&#29992;&#36827;&#34892;&#20998;&#26512;&#30340;&#26041;&#27861;&#23384;&#22312;&#26681;&#26412;&#24046;&#36317;&#65292;&#36825;&#20010;&#30446;&#26631;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#26694;&#26550;&#20197;&#21450;&#35777;&#26126;&#25216;&#26415;&#21644;&#31639;&#27861;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#65292;&#29992;&#20110;&#35780;&#20272;&#24191;&#20041;&#35268;&#21010;&#30340;&#32456;&#27490;&#21644;&#30446;&#26631;&#21487;&#36798;&#24615;&#30456;&#20851;&#23646;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#35770;&#30340;&#32463;&#20856;&#32467;&#26524;&#23558;&#24191;&#20041;&#35268;&#21010;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#32452;&#20214;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#32452;&#20214;&#25512;&#23548;&#20986;&#23618;&#27425;&#21270;&#30340;&#32456;&#27490;&#35770;&#35777;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#30830;&#23450;&#32473;&#23450;&#24191;&#20041;&#35268;&#21010;&#30340;&#25928;&#29992;&#65292;&#20197;&#21450;&#25351;&#23548;&#24191;&#20041;&#35268;&#21010;&#30340;&#32508;&#21512;&#21644;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#26469;&#35828;&#26126;&#36825;&#31181;&#26032;&#26041;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents new methods for analyzing and evaluating generalized plans that can solve broad classes of related planning problems. Although synthesis and learning of generalized plans has been a longstanding goal in AI, it remains challenging due to fundamental gaps in methods for analyzing the scope and utility of a given generalized plan. This paper addresses these gaps by developing a new conceptual framework along with proof techniques and algorithmic processes for assessing termination and goal-reachability related properties of generalized plans. We build upon classic results from graph theory to decompose generalized plans into smaller components that are then used to derive hierarchical termination arguments. These methods can be used to determine the utility of a given generalized plan, as well as to guide the synthesis and learning processes for generalized plans. We present theoretical as well as empirical results illustrating the scope of this new approach. Our analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#20219;&#21153;&#27867;&#21270;&#20013;&#36866;&#37197;&#22120;&#36335;&#30001;&#30340;&#20316;&#29992;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#27492;&#30340;&#26032;&#21464;&#20307;Multi-Head Routing (MHR)&#65292;&#36890;&#36807;&#31934;&#32454;&#30340;&#36335;&#30001;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.03831</link><description>&lt;p&gt;
&#36328;&#20219;&#21153;&#27867;&#21270;&#30340;&#22810;&#22836;&#36866;&#37197;&#22120;&#36335;&#30001;
&lt;/p&gt;
&lt;p&gt;
Multi-Head Adapter Routing for Cross-Task Generalization. (arXiv:2211.03831v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#20219;&#21153;&#27867;&#21270;&#20013;&#36866;&#37197;&#22120;&#36335;&#30001;&#30340;&#20316;&#29992;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#27492;&#30340;&#26032;&#21464;&#20307;Multi-Head Routing (MHR)&#65292;&#36890;&#36807;&#31934;&#32454;&#30340;&#36335;&#30001;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#20219;&#21153;&#27867;&#21270;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;(PEFT)&#22312;&#23569;&#26679;&#26412;&#20219;&#21153;&#36866;&#37197;&#20043;&#21069;&#36890;&#36807;&#22312;&#22810;&#20219;&#21153;&#35757;&#32451;&#38598;&#19978;&#39044;&#35757;&#32451;&#36866;&#37197;&#22120;&#26469;&#23454;&#29616;&#12290;Ponti&#31561;&#20154;&#30340;Polytropon [Ponti et al., 2023] ($\texttt{Poly}$)&#22312;&#39044;&#35757;&#32451;&#21644;&#23569;&#26679;&#26412;&#36866;&#37197;&#26399;&#38388;&#20849;&#21516;&#23398;&#20064;&#20102;&#19968;&#32452;&#36866;&#37197;&#22120;&#21644;&#36873;&#25321;&#27599;&#20010;&#20219;&#21153;&#30340;&#36866;&#37197;&#22120;&#23376;&#38598;&#30340;&#36335;&#30001;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36866;&#37197;&#22120;&#36335;&#30001;&#22312;&#20854;&#25104;&#21151;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#26681;&#25454;&#25105;&#20204;&#30340;&#21457;&#29616;&#35774;&#35745;&#20102;&#22522;&#20110;&#27492;&#30340;&#26032;&#21464;&#20307;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#26356;&#31934;&#32454;&#30340;&#36335;&#30001;&#33021;&#25552;&#20379;&#26356;&#22810;&#34920;&#36798;&#24615;&#30340;&#30452;&#35273;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multi-Head Routing (MHR)&#65292;&#23427;&#32467;&#21512;&#20102;&#36866;&#37197;&#22120;&#21442;&#25968;&#30340;&#23376;&#38598;&#65292;&#24182;&#22312;&#21487;&#27604;&#36739;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#34920;&#29616;&#26356;&#22909;&#65307;&#36890;&#36807;&#20165;&#24494;&#35843;&#36335;&#30001;&#20989;&#25968;&#32780;&#19981;&#26159;&#36866;&#37197;&#22120;(MHR-z)&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#26497;&#39640;&#21442;&#25968;&#25928;&#29575;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;Poly/MHR&#30340;&#20132;&#21449;&#27169;&#22411;&#36866;&#37197;&#22312;&#23569;&#26679;&#26412;&#20219;&#21153;&#19978;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) for cross-task generalization consists in pre-training adapters on a multi-task training set before few-shot adaptation to test tasks. Polytropon [Ponti et al., 2023] ($\texttt{Poly}$) jointly learns an inventory of adapters and a routing function that selects a (variable-size) subset of adapters for each task during both pre-training and few-shot adaptation. In this paper, we investigate the role that adapter routing plays in its success and design new variants based on our findings. First, we build on the intuition that finer-grained routing provides more expressivity. Hence, we propose $\texttt{MHR}$ (Multi-Head Routing), which combines $\textit{subsets}$ of adapter parameters and outperforms $\texttt{Poly}$ under a comparable parameter budget; by only fine-tuning the routing function and not the adapters ($\texttt{MHR}$-$z$), we achieve competitive performance with extreme parameter efficiency. Second, we find that $\texttt{Poly}$/$\texttt{MHR
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26367;&#20195;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#33258;&#22238;&#24402;&#20302;&#24310;&#36831;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#22330;&#26223;&#19979;&#22343;&#33021;&#24102;&#26469;&#31283;&#23450;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2211.01751</link><description>&lt;p&gt;
&#36845;&#20195;&#33258;&#22238;&#24402;&#65306;&#25552;&#39640;&#20302;&#24310;&#36831;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#30340;&#26032;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Iterative autoregression: a novel trick to improve your low-latency speech enhancement model. (arXiv:2211.01751v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26367;&#20195;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#33258;&#22238;&#24402;&#20302;&#24310;&#36831;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#22330;&#26223;&#19979;&#22343;&#33021;&#24102;&#26469;&#31283;&#23450;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24335;&#27169;&#22411;&#26159;&#23454;&#26102;&#35821;&#38899;&#22686;&#24378;&#24037;&#20855;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#27969;&#24335;&#27169;&#24335;&#38480;&#21046;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#20165;&#33021;&#20351;&#29992;&#26497;&#23569;&#37327;&#26410;&#26469;&#20449;&#24687;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;&#22240;&#27492;&#65292;&#20302;&#24310;&#36831;&#27969;&#24335;&#35774;&#32622;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#23545;&#27169;&#22411;&#30340;&#36136;&#37327;&#26377;&#30528;&#26174;&#33879;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#27969;&#24335;&#29983;&#25104;&#30340;&#39034;&#24207;&#24615;&#25552;&#20379;&#20102;&#33258;&#22238;&#24402;&#30340;&#33258;&#28982;&#21487;&#33021;&#24615;&#65292;&#21363;&#22312;&#36827;&#34892;&#24403;&#21069;&#39044;&#27979;&#26102;&#21033;&#29992;&#20197;&#21069;&#30340;&#39044;&#27979;&#12290;&#24120;&#35268;&#35757;&#32451;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#26159;&#25945;&#24072;&#24378;&#21046;&#65292;&#20294;&#20854;&#20027;&#35201;&#32570;&#28857;&#22312;&#20110;&#35757;&#32451;-&#25512;&#29702;&#19981;&#21305;&#37197;&#21487;&#33021;&#20250;&#23548;&#33268;&#22823;&#24133;&#24230;&#30340;&#36136;&#37327;&#38477;&#32423;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26367;&#20195;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#33258;&#22238;&#24402;&#20302;&#24310;&#36831;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#22330;&#26223;&#19979;&#37117;&#33021;&#24102;&#26469;&#31283;&#23450;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Streaming models are an essential component of real-time speech enhancement tools. The streaming regime constrains speech enhancement models to use only a tiny context of future information. As a result, the low-latency streaming setup is generally considered a challenging task and has a significant negative impact on the model's quality. However, the sequential nature of streaming generation offers a natural possibility for autoregression, that is, utilizing previous predictions while making current ones. The conventional method for training autoregressive models is teacher forcing, but its primary drawback lies in the training-inference mismatch that can lead to a substantial degradation in quality. In this study, we propose a straightforward yet effective alternative technique for training autoregressive low-latency speech enhancement models. We demonstrate that the proposed approach leads to stable improvement across diverse architectures and training scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#20027;&#23548;&#33322;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25552;&#20986;&#20102;&#22235;&#20010;&#26399;&#26395;&#65306;&#19981;&#30830;&#23450;&#24615;&#25512;&#29702;&#12289;&#23433;&#20840;&#24615;&#12289;&#26377;&#38480;&#35797;&#38169;&#25968;&#25454;&#23398;&#20064;&#21644;&#23545;&#22810;&#26679;&#29615;&#22659;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.04839</link><description>&lt;p&gt;
&#33258;&#20027;&#23548;&#33322;&#30340;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Reinforcement Learning Techniques for Autonomous Navigation. (arXiv:2210.04839v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#20027;&#23548;&#33322;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25552;&#20986;&#20102;&#22235;&#20010;&#26399;&#26395;&#65306;&#19981;&#30830;&#23450;&#24615;&#25512;&#29702;&#12289;&#23433;&#20840;&#24615;&#12289;&#26377;&#38480;&#35797;&#38169;&#25968;&#25454;&#23398;&#20064;&#21644;&#23545;&#22810;&#26679;&#29615;&#22659;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20026;&#33258;&#20027;&#26426;&#22120;&#20154;&#23548;&#33322;&#24102;&#26469;&#20102;&#35768;&#22810;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#30340;&#38480;&#21046;&#65292;&#38459;&#27490;&#20102;&#22522;&#20110;RL&#30340;&#23548;&#33322;&#31995;&#32479;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#20363;&#22914;&#65292;&#22823;&#22810;&#25968;&#23398;&#20064;&#26041;&#27861;&#32570;&#20047;&#23433;&#20840;&#20445;&#35777;&#65307;&#32780;&#23398;&#20064;&#21040;&#30340;&#23548;&#33322;&#31995;&#32479;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#26410;&#30693;&#29615;&#22659;&#12290;&#23613;&#31649;&#26368;&#36817;&#20986;&#29616;&#20102;&#21508;&#31181;&#23398;&#20064;&#25216;&#26415;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#32570;&#20047;&#38024;&#23545;&#33258;&#20027;&#23548;&#33322;&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#21644;&#21487;&#37325;&#22797;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#23398;&#23478;&#38590;&#20197;&#36873;&#25321;&#29992;&#20110;&#20854;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#20063;&#38590;&#20197;&#30830;&#23450;&#24403;&#21069;&#36890;&#29992;&#23398;&#20064;&#26041;&#27861;&#22312;&#33258;&#20027;&#23548;&#33322;&#26041;&#38754;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24212;&#29992;&#28145;&#24230;RL&#26041;&#27861;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#30340;&#22235;&#20010;&#20027;&#35201;&#26399;&#26395;&#65306;&#65288;D1&#65289;&#19981;&#30830;&#23450;&#24615;&#25512;&#29702;&#12289;&#65288;D2&#65289;&#23433;&#20840;&#24615;&#12289;&#65288;D3&#65289;&#20174;&#26377;&#38480;&#30340;&#35797;&#38169;&#25968;&#25454;&#20013;&#23398;&#20064;&#12289;&#65288;D4&#65289;&#23545;&#22810;&#26679;&#21644;&#26032;&#39062;&#29615;&#22659;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) has brought many successes for autonomous robot navigation. However, there still exists important limitations that prevent real-world use of RL-based navigation systems. For example, most learning approaches lack safety guarantees; and learned navigation systems may not generalize well to unseen environments. Despite a variety of recent learning techniques to tackle these challenges in general, a lack of an open-source benchmark and reproducible learning methods specifically for autonomous navigation makes it difficult for roboticists to choose what learning methods to use for their mobile robots and for learning researchers to identify current shortcomings of general learning methods for autonomous navigation. In this paper, we identify four major desiderata of applying deep RL approaches for autonomous navigation: (D1) reasoning under uncertainty, (D2) safety, (D3) learning from limited trial-and-error data, and (D4) generalization to diverse and nove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;ReLoD&#30340;&#23454;&#26102;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#26412;&#22320;&#21644;&#36828;&#31243;&#35745;&#31639;&#26426;&#26469;&#20998;&#37197;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35745;&#31639;&#65292;&#24182;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;SAC&#30340;&#24615;&#33021;&#19979;&#38477;&#20102;&#12290;</title><link>http://arxiv.org/abs/2210.02317</link><description>&lt;p&gt;
&#23454;&#26102;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#35270;&#35273;&#30340;&#26426;&#22120;&#20154;&#65292;&#21033;&#29992;&#26412;&#22320;&#21644;&#36828;&#31243;&#35745;&#31639;&#26426;
&lt;/p&gt;
&lt;p&gt;
Real-Time Reinforcement Learning for Vision-Based Robotics Utilizing Local and Remote Computers. (arXiv:2210.02317v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;ReLoD&#30340;&#23454;&#26102;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#26412;&#22320;&#21644;&#36828;&#31243;&#35745;&#31639;&#26426;&#26469;&#20998;&#37197;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35745;&#31639;&#65292;&#24182;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;SAC&#30340;&#24615;&#33021;&#19979;&#38477;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#23398;&#20064;&#23545;&#20110;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#38750;&#31283;&#23450;&#29615;&#22659;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#35774;&#32622;&#26159;&#21516;&#26102;&#26377;&#20004;&#21488;&#19981;&#21516;&#30340;&#35745;&#31639;&#26426;&#65306;&#19982;&#26426;&#22120;&#20154;&#30456;&#36830;&#30340;&#36164;&#28304;&#21463;&#38480;&#26412;&#22320;&#35745;&#31639;&#26426;&#21644;&#26080;&#32447;&#36830;&#25509;&#30340;&#24378;&#22823;&#36828;&#31243;&#35745;&#31639;&#26426;&#12290;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#19979;&#65292;&#23578;&#19981;&#28165;&#26970;&#23398;&#20064;&#31995;&#32479;&#30340;&#24615;&#33021;&#22312;&#36164;&#28304;&#38480;&#21046;&#26041;&#38754;&#33021;&#21463;&#21040;&#22810;&#22823;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22914;&#20309;&#39640;&#25928;&#21033;&#29992;&#26080;&#32447;&#36830;&#25509;&#30340;&#24378;&#22823;&#35745;&#31639;&#26426;&#26469;&#24357;&#34917;&#24615;&#33021;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#26102;&#23398;&#20064;&#31995;&#32479;&#65292;&#31216;&#20026;Remote-Local Distributed (ReLoD)&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#26412;&#22320;&#21644;&#36828;&#31243;&#35745;&#31639;&#26426;&#20043;&#38388;&#20998;&#37197;&#20004;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Soft Actor-Critic (SAC)&#21644;Proximal Policy Optimization (PPO)&#30340;&#35745;&#31639;&#12290;&#35813;&#31995;&#32479;&#30340;&#24615;&#33021;&#22312;&#20351;&#29992;&#26426;&#26800;&#33218;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#24320;&#21457;&#30340;&#20004;&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;SAC&#30340;&#24615;&#33021;&#19979;&#38477;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time learning is crucial for robotic agents adapting to ever-changing, non-stationary environments. A common setup for a robotic agent is to have two different computers simultaneously: a resource-limited local computer tethered to the robot and a powerful remote computer connected wirelessly. Given such a setup, it is unclear to what extent the performance of a learning system can be affected by resource limitations and how to efficiently use the wirelessly connected powerful computer to compensate for any performance loss. In this paper, we implement a real-time learning system called the Remote-Local Distributed (ReLoD) system to distribute computations of two deep reinforcement learning (RL) algorithms, Soft Actor-Critic (SAC) and Proximal Policy Optimization (PPO), between a local and a remote computer. The performance of the system is evaluated on two vision-based control tasks developed using a robotic arm and a mobile robot. Our results show that SAC's performance degrades
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20869;&#22312;&#22870;&#21169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#24403;&#21069;&#35266;&#23519;&#19982;&#21382;&#21490;&#30693;&#35782;&#30340;&#24046;&#24322;&#26469;&#35780;&#20272;&#22909;&#22855;&#24515;&#65292;&#24182;&#21033;&#29992;&#26102;&#38388;&#19981;&#19968;&#33268;&#24615;&#20316;&#20026;&#20869;&#22312;&#22870;&#21169;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#31232;&#30095;&#22806;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#22122;&#22768;&#23481;&#24525;&#24230;&#12290;</title><link>http://arxiv.org/abs/2208.11361</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#19981;&#19968;&#33268;&#30340;&#33258;&#30417;&#30563;&#25506;&#32034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Exploration via Temporal Inconsistency in Reinforcement Learning. (arXiv:2208.11361v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20869;&#22312;&#22870;&#21169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#24403;&#21069;&#35266;&#23519;&#19982;&#21382;&#21490;&#30693;&#35782;&#30340;&#24046;&#24322;&#26469;&#35780;&#20272;&#22909;&#22855;&#24515;&#65292;&#24182;&#21033;&#29992;&#26102;&#38388;&#19981;&#19968;&#33268;&#24615;&#20316;&#20026;&#20869;&#22312;&#22870;&#21169;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#31232;&#30095;&#22806;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#22122;&#22768;&#23481;&#24525;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31232;&#30095;&#22806;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23613;&#31649;&#23545;&#36825;&#20010;&#39046;&#22495;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#12290;&#20808;&#21069;&#30340;&#23581;&#35797;&#34920;&#26126;&#65292;&#20869;&#22312;&#22870;&#21169;&#21487;&#20197;&#32531;&#35299;&#31232;&#30095;&#24615;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20154;&#31867;&#23398;&#20064;&#21551;&#21457;&#30340;&#26032;&#22411;&#20869;&#22312;&#22870;&#21169;&#65292;&#20154;&#31867;&#36890;&#36807;&#23558;&#24403;&#21069;&#35266;&#23519;&#19982;&#21382;&#21490;&#30693;&#35782;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#22909;&#22855;&#24515;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35757;&#32451;&#19968;&#20010;&#33258;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#65292;&#20445;&#23384;&#27169;&#22411;&#21442;&#25968;&#30340;&#24555;&#29031;&#65292;&#24182;&#20351;&#29992;&#26680;&#33539;&#25968;&#26469;&#35780;&#20272;&#19981;&#21516;&#24555;&#29031;&#20043;&#38388;&#39044;&#27979;&#30340;&#26102;&#38388;&#19981;&#19968;&#33268;&#24615;&#20316;&#20026;&#20869;&#22312;&#22870;&#21169;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#21152;&#26435;&#26426;&#21046;&#65292;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#20026;&#19981;&#21516;&#30340;&#24555;&#29031;&#20998;&#37197;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#25104;&#26412;&#19988;&#20855;&#26377;&#26356;&#39640;&#22122;&#22768;&#23481;&#24525;&#24230;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#20869;&#22312;&#22870;&#21169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Under sparse extrinsic reward settings, reinforcement learning has remained challenging, despite surging interests in this field. Previous attempts suggest that intrinsic reward can alleviate the issue caused by sparsity. In this article, we present a novel intrinsic reward that is inspired by human learning, as humans evaluate curiosity by comparing current observations with historical knowledge. Our method involves training a self-supervised prediction model, saving snapshots of the model parameters, and using nuclear norm to evaluate the temporal inconsistency between the predictions of different snapshots as intrinsic rewards. We also propose a variational weighting mechanism to assign weight to different snapshots in an adaptive manner. Our experimental results on various benchmark environments demonstrate the efficacy of our method, which outperforms other intrinsic reward-based methods without additional training costs and with higher noise tolerance. This work has been submitte
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#24037;&#20316;&#27969;&#20013;&#24322;&#27493;&#20219;&#21153;&#25191;&#34892;&#30340;&#35201;&#27714;&#21644;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#30830;&#23450;&#24322;&#27493;&#25191;&#34892;&#25910;&#30410;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#36890;&#36807;&#22312;Summit&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#24322;&#27493;&#25191;&#34892;&#23545;&#24615;&#33021;&#30340;&#22686;&#24378;&#19982;&#27169;&#22411;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2208.11069</link><description>&lt;p&gt;
&#24322;&#26500;&#20219;&#21153;&#22312;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#24037;&#20316;&#27969;&#20013;&#30340;&#24322;&#27493;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Execution of Heterogeneous Tasks in ML-driven HPC Workflows. (arXiv:2208.11069v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11069
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#24037;&#20316;&#27969;&#20013;&#24322;&#27493;&#20219;&#21153;&#25191;&#34892;&#30340;&#35201;&#27714;&#21644;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#30830;&#23450;&#24322;&#27493;&#25191;&#34892;&#25910;&#30410;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#36890;&#36807;&#22312;Summit&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#24322;&#27493;&#25191;&#34892;&#23545;&#24615;&#33021;&#30340;&#22686;&#24378;&#19982;&#27169;&#22411;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#31185;&#23398;&#24037;&#20316;&#27969;&#30001;&#35768;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#32452;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#22312;&#24322;&#26500;&#36164;&#28304;&#19978;&#25191;&#34892;&#12290;&#24322;&#27493;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#23545;&#20110;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#12289;&#20219;&#21153;&#21534;&#21520;&#37327;&#21644;&#20943;&#23569;&#24037;&#20316;&#27969;&#30340;&#26102;&#38388;&#38750;&#24120;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#33021;&#22815;&#22312;&#24322;&#26500;&#36164;&#28304;&#19978;&#35843;&#24230;&#21644;&#25191;&#34892;&#19981;&#21516;&#31867;&#22411;&#20219;&#21153;&#30340;&#20013;&#38388;&#20214;&#24517;&#39035;&#25903;&#25345;&#24322;&#27493;&#25191;&#34892;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#24037;&#20316;&#27969;&#30340;&#24322;&#27493;&#20219;&#21153;&#25191;&#34892;&#30340;&#35201;&#27714;&#21644;&#29305;&#24615;&#12290;&#25105;&#20204;&#27169;&#22411;&#21270;&#20102;&#20219;&#24847;&#24037;&#20316;&#27969;&#30340;&#24322;&#27493;&#24615;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#29992;&#20110;&#30830;&#23450;&#24322;&#27493;&#25191;&#34892;&#24102;&#26469;&#30340;&#23450;&#24615;&#25910;&#30410;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20195;&#34920;&#20102;&#30456;&#20851;&#31185;&#23398;&#39537;&#21160;&#65292;&#25105;&#20204;&#22312;Summit&#19978;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#20110;&#24322;&#27493;&#25191;&#34892;&#25152;&#24102;&#26469;&#30340;&#24615;&#33021;&#22686;&#24378;&#19982;&#25105;&#20204;&#30340;&#27169;&#22411;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous scientific workflows consist of numerous types of tasks that require executing on heterogeneous resources. Asynchronous execution of those tasks is crucial to improve resource utilization, task throughput and reduce workflows' makespan. Therefore, middleware capable of scheduling and executing different task types across heterogeneous resources must enable asynchronous execution of tasks. In this paper, we investigate the requirements and properties of the asynchronous task execution of machine learning (ML)-driven high performance computing (HPC) workflows. We model the degree of asynchronicity permitted for arbitrary workflows and propose key metrics that can be used to determine qualitative benefits when employing asynchronous execution. Our experiments represent relevant scientific drivers, we perform them at scale on Summit, and we show that the performance enhancements due to asynchronous execution are consistent with our model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21387;&#32553;&#26041;&#27861;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#32676;&#20307;&#12289;&#24615;&#21035;&#21644;&#35821;&#20041;&#20559;&#24046;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2205.10828</link><description>&lt;p&gt;
&#21387;&#32553;&#22411;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20250;&#24573;&#30053;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Do Compressed Multilingual Machine Translation Models Forget?. (arXiv:2205.10828v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21387;&#32553;&#26041;&#27861;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#32676;&#20307;&#12289;&#24615;&#21035;&#21644;&#35821;&#20041;&#20559;&#24046;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38750;&#24120;&#24222;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#22823;&#23567;&#20351;&#24471;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#24212;&#29992;&#23427;&#20204;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#21387;&#32553;&#25216;&#26415;&#21487;&#20197;&#22823;&#24133;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#20174;&#32780;&#20943;&#23569;&#25512;&#29702;&#26102;&#38388;&#65292;&#24182;&#23545;&#39030;&#32423;&#25351;&#26631;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#23545;&#22810;&#20010;&#20219;&#21153;&#21644;/&#25110;&#35821;&#35328;&#36827;&#34892;&#24179;&#22343;&#30340;&#32508;&#21512;&#24615;&#33021;&#21487;&#33021;&#25513;&#30422;&#20102;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#21151;&#33021;&#19978;&#30340;&#20005;&#37325;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#25152;&#32534;&#30721;&#30340;&#20559;&#35265;&#30340;&#25918;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#26426;&#22120;&#32763;&#35793;&#22522;&#20934;&#65288;FLORES-101&#12289;MT-Gender&#21644;DiBiMT&#65289;&#19978;&#30340;&#21387;&#32553;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#21387;&#32553;&#26041;&#27861;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65288;MNMT&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#32676;&#20307;&#12289;&#24615;&#21035;&#21644;&#35821;&#20041;&#20559;&#24046;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#32780;&#24179;&#22343;BLEU&#24230;&#37327;&#20540;&#21017;&#27809;&#20160;&#20040;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, very large pre-trained models achieve state-of-the-art results in various natural language processing (NLP) tasks, but their size makes it more challenging to apply them in resource-constrained environments. Compression techniques allow to drastically reduce the size of the models and therefore their inference time with negligible impact on top-tier metrics. However, the general performance averaged across multiple tasks and/or languages may hide a drastic performance drop on under-represented features, which could result in the amplification of biases encoded by the models. In this work, we assess the impact of compression methods on Multilingual Neural Machine Translation models (MNMT) for various language groups, gender, and semantic biases by extensive analysis of compressed models on different machine translation benchmarks, i.e. FLORES-101, MT-Gender, and DiBiMT. We show that the performance of under-represented languages drops significantly, while the average BLEU metr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28857;&#31215;&#20302;&#31209;&#36817;&#20284;&#26469;&#34920;&#31034;Q&#20540;&#30340;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;&#21521;&#37327;&#22330;&#25429;&#25417;&#29366;&#24577;&#30340;&#23616;&#37096;&#21160;&#24577;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#25429;&#25417;&#24403;&#21069;&#29366;&#24577;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#20840;&#23616;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.13695</link><description>&lt;p&gt;
&#21452;&#32447;&#24615;&#20215;&#20540;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bilinear value networks. (arXiv:2204.13695v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13695
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28857;&#31215;&#20302;&#31209;&#36817;&#20284;&#26469;&#34920;&#31034;Q&#20540;&#30340;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;&#21521;&#37327;&#22330;&#25429;&#25417;&#29366;&#24577;&#30340;&#23616;&#37096;&#21160;&#24577;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#25429;&#25417;&#24403;&#21069;&#29366;&#24577;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#20840;&#23616;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#27969;&#26694;&#26550;&#28041;&#21450;&#20272;&#35745;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;Q&#20540;&#20989;&#25968;&#12290;&#24403;&#23398;&#20064;&#23454;&#29616;&#22810;&#20010;&#19981;&#21516;&#30446;&#26631;&#26102;&#65292;&#25968;&#25454;&#25928;&#29575;&#19982;Q&#20989;&#25968;&#23545;&#26032;&#30446;&#26631;&#30340;&#27867;&#21270;&#23494;&#20999;&#30456;&#20851;&#12290;&#30446;&#21069;&#30340;&#33539;&#24335;&#26159;&#20351;&#29992;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;Q(s, a, g)&#12290;&#20026;&#20102;&#25913;&#36827;Q&#20989;&#25968;&#30340;&#27867;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#21521;&#37327;&#22330;&#20043;&#38388;&#30340;&#28857;&#31215;&#30340;&#20302;&#31209;&#36817;&#20284;&#26469;&#34920;&#31034;Q&#20540;&#12290;&#31532;&#19968;&#20010;&#21521;&#37327;&#22330;f(s, a)&#25429;&#25417;&#29366;&#24577;s&#22788;&#30340;&#29615;&#22659;&#23616;&#37096;&#21160;&#24577;&#65307;&#32780;&#31532;&#20108;&#20010;&#37096;&#20998;{&#981;}(s, g)&#21017;&#25429;&#25417;&#24403;&#21069;&#29366;&#24577;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#20840;&#23616;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#26696;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#22788;&#20110;&#20998;&#24067;&#33539;&#22260;&#20043;&#22806;&#30340;&#30446;&#26631;&#20855;&#26377;&#26356;&#22909;&#30340;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#30340;Fetch&#26426;&#22120;&#20154;&#20219;&#21153;&#22871;&#20214;&#21644;DeepMind Control Suite&#19978;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant framework for off-policy multi-goal reinforcement learning involves estimating goal conditioned Q-value function. When learning to achieve multiple goals, data efficiency is intimately connected with the generalization of the Q-function to new goals. The de-facto paradigm is to approximate Q(s, a, g) using monolithic neural networks. To improve the generalization of the Q-function, we propose a bilinear decomposition that represents the Q-value via a low-rank approximation in the form of a dot product between two vector fields. The first vector field, f(s, a), captures the environment's local dynamics at the state s; whereas the second component, {\phi}(s, g), captures the global relationship between the current state and the goal. We show that our bilinear decomposition scheme substantially improves data efficiency, and has superior transfer to out-of-distribution goals compared to prior methods. Empirical evidence is provided on the simulated Fetch robot task-suite and d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#32463;&#39564;&#22238;&#25918;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22270;&#26469;&#26126;&#30830;&#29366;&#24577;&#30340; Q &#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#37319;&#26679;&#31574;&#30053;&#24573;&#35270;&#29366;&#24577;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#28145;&#24230; Q &#20989;&#25968;&#26102;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.15845</link><description>&lt;p&gt;
&#25299;&#25169;&#32463;&#39564;&#22238;&#25918;
&lt;/p&gt;
&lt;p&gt;
Topological Experience Replay. (arXiv:2203.15845v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#32463;&#39564;&#22238;&#25918;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22270;&#26469;&#26126;&#30830;&#29366;&#24577;&#30340; Q &#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#37319;&#26679;&#31574;&#30053;&#24573;&#35270;&#29366;&#24577;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#28145;&#24230; Q &#20989;&#25968;&#26102;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#28145;&#24230; Q &#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#20174;&#32463;&#39564;&#37325;&#25918;&#32531;&#20914;&#21306;&#20013;&#37319;&#26679;&#30340;&#29366;&#24577;&#36716;&#25442;&#20803;&#32452;&#26356;&#26032; Q &#20540;&#12290;&#36825;&#31181;&#31574;&#30053;&#36890;&#24120;&#22343;&#21248;&#21644;&#38543;&#26426;&#22320;&#37319;&#26679;&#65292;&#25110;&#22522;&#20110;&#35832;&#22914;&#26102;&#38388;&#24046;&#65288;TD&#65289;&#35823;&#24046;&#31561;&#24230;&#37327;&#20248;&#20808;&#12290;&#36825;&#26679;&#30340;&#37319;&#26679;&#31574;&#30053;&#22312;&#23398;&#20064; Q &#20989;&#25968;&#26102;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#19968;&#20010;&#29366;&#24577;&#30340; Q &#20540;&#21462;&#20915;&#20110;&#32487;&#25215;&#29366;&#24577;&#30340; Q &#20540;&#12290;&#22914;&#26524;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#24573;&#30053;&#20102;&#19979;&#19968;&#20010;&#29366;&#24577;&#30340; Q &#20540;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#23427;&#21487;&#33021;&#20250;&#23548;&#33268;&#26080;&#29992;&#21644;&#24120;&#24120;&#19981;&#27491;&#30830;&#30340; Q &#20540;&#26356;&#26032;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26234;&#33021;&#20307;&#30340;&#32463;&#39564;&#32452;&#32455;&#25104;&#19968;&#20010;&#22270;&#65292;&#26126;&#30830;&#36319;&#36394;&#29366;&#24577;&#30340; Q &#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22270;&#20013;&#30340;&#27599;&#26465;&#36793;&#20195;&#34920;&#36890;&#36807;&#25191;&#34892;&#21333;&#20010;&#25805;&#20316;&#22312;&#20004;&#20010;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#19968;&#32452;&#32456;&#31471;&#29366;&#24577;&#24320;&#22987;&#25193;&#23637;&#22270;&#20013;&#30340;&#39030;&#28857;&#65292;&#24182;&#36880;&#27493;&#21521;&#21518;&#31227;&#21160;&#30340;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26469;&#25191;&#34892;&#20540;&#22791;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. This strategy often uniformly and randomly samples or prioritizes data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies can be inefficient at learning Q-function because a state's Q-value depends on the Q-value of successor states. If the data sampling strategy ignores the precision of the Q-value estimate of the next state, it can lead to useless and often incorrect updates to the Q-values. To mitigate this issue, we organize the agent's experience into a graph that explicitly tracks the dependency between Q-values of states. Each edge in the graph represents a transition between two states by executing a single action. We perform value backups via a breadth-first search starting from that expands vertices in the graph starting from the set of terminal states and successively moving backward. We empirically sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#36741;&#21161;&#30340;&#20998;&#24067;&#24335;&#32676;&#20307;&#20248;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;&#22320;&#36136;&#31185;&#23398;&#27169;&#22411;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#22522;&#20934;&#20248;&#21270;&#38382;&#39064;&#21644;Badlands&#39118;&#35980;&#28436;&#21270;&#27169;&#22411;&#20013;&#37117;&#21462;&#24471;&#20102;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2201.06843</link><description>&lt;p&gt;
&#29992;&#20195;&#29702;&#27169;&#22411;&#36741;&#21161;&#30340;&#20998;&#24067;&#24335;&#32676;&#20307;&#20248;&#21270;&#26041;&#27861;&#26469;&#22788;&#29702;&#35745;&#31639;&#26114;&#36149;&#30340;&#22320;&#36136;&#31185;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Surrogate-assisted distributed swarm optimisation for computationally expensive geoscientific models. (arXiv:2201.06843v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#36741;&#21161;&#30340;&#20998;&#24067;&#24335;&#32676;&#20307;&#20248;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;&#22320;&#36136;&#31185;&#23398;&#27169;&#22411;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#22522;&#20934;&#20248;&#21270;&#38382;&#39064;&#21644;Badlands&#39118;&#35980;&#28436;&#21270;&#27169;&#22411;&#20013;&#37117;&#21462;&#24471;&#20102;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#25552;&#20379;&#26080;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#65292;&#23545;&#20110;&#38590;&#20197;&#33719;&#21462;&#26799;&#24230;&#30340;&#27169;&#22411;&#65288;&#22914;&#22320;&#36136;&#31185;&#23398;&#39118;&#35980;&#28436;&#21270;&#27169;&#22411;&#65289;&#26377;&#30410;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#21363;&#20351;&#26159;&#24182;&#34892;&#35745;&#31639;&#30340;&#20998;&#24067;&#24335;&#32676;&#20307;&#20248;&#21270;&#20063;&#24456;&#21507;&#21147;&#12290;&#25105;&#20204;&#21487;&#20197;&#37319;&#29992;&#20195;&#29702;&#27169;&#22411;&#36741;&#21161;&#20248;&#21270;&#31561;&#39640;&#25928;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#26159;&#23454;&#26045;&#20195;&#29702;&#27169;&#22411;&#30340;&#36827;&#31243;&#38388;&#36890;&#20449;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#32676;&#20307;&#20248;&#21270;&#20013;&#20195;&#29702;&#27169;&#22411;&#35780;&#20272;&#36866;&#24212;&#20540;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#24182;&#34892;&#35745;&#31639;&#26550;&#26500;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#19968;&#32452;&#22522;&#20934;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20855;&#26377;&#39118;&#35980;&#28436;&#21270;&#27169;&#22411;&#30340;&#22320;&#36136;&#31185;&#23398;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#22522;&#20934;&#20989;&#25968;&#21644;Badlands&#39118;&#35980;&#28436;&#21270;&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#35745;&#31639;&#26102;&#38388;&#30340;&#32553;&#30701;
&lt;/p&gt;
&lt;p&gt;
Evolutionary algorithms provide gradient-free optimisation which is beneficial for models that have difficulty in obtaining gradients; for instance, geoscientific landscape evolution models. However, such models are at times computationally expensive and even distributed swarm-based optimisation with parallel computing struggles. We can incorporate efficient strategies such as surrogate-assisted optimisation to address the challenges; however, implementing inter-process communication for surrogate-based model training is difficult. In this paper, we implement surrogate-based estimation of fitness evaluation in distributed swarm optimisation over a parallel computing architecture. We first test the framework on a set of benchmark optimisation problems and then apply it to a geoscientific model that features a landscape evolution model. Our results demonstrate very promising results for benchmark functions and the Badlands landscape evolution model. We obtain a reduction in computational
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#22797;&#21512;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35299;&#37322;&#24615;&#30340;&#20027;&#39064;&#34920;&#31034;&#21644;&#21477;&#23376;&#32423;&#30340;&#20027;&#39064;&#23545;&#35805;&#65292;&#23558;&#20027;&#39064;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;&#35813;&#27169;&#22411;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2006.10632</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#12289;&#23545;&#35805;&#20027;&#39064;&#24863;&#30693;&#30340;&#31070;&#32463;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Explainable and Discourse Topic-aware Neural Language Understanding. (arXiv:2006.10632v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.10632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#22797;&#21512;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35299;&#37322;&#24615;&#30340;&#20027;&#39064;&#34920;&#31034;&#21644;&#21477;&#23376;&#32423;&#30340;&#20027;&#39064;&#23545;&#35805;&#65292;&#23558;&#20027;&#39064;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;&#35813;&#27169;&#22411;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20027;&#39064;&#27169;&#22411;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20027;&#39064;&#23558;&#35821;&#35328;&#29702;&#35299;&#25193;&#23637;&#21040;&#21477;&#23376;&#20043;&#22806;&#30340;&#26356;&#24191;&#27867;&#30340;&#25991;&#26723;&#32423;&#19978;&#19979;&#25991;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#20027;&#39064;&#35821;&#20041;&#65292;&#20294;&#24573;&#30053;&#20102;&#25991;&#26723;&#20013;&#21477;&#23376;&#30340;&#20027;&#39064;&#23545;&#35805;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20026;&#27599;&#20010;&#28508;&#22312;&#20027;&#39064;&#30340;&#27604;&#20363;&#30456;&#23545;&#24212;&#22320;&#24341;&#20837;&#21487;&#35299;&#37322;&#24615;&#20027;&#39064;&#34920;&#31034;&#26469;&#25193;&#23637;&#30740;&#31350;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;&#25991;&#26723;&#20013;&#30340;&#27599;&#20010;&#21477;&#23376;&#24314;&#27169;&#20027;&#39064;&#23545;&#35805;&#65292;&#20445;&#30041;&#21477;&#23376;-&#20027;&#39064;&#20851;&#32852;&#20197;&#21450;&#25991;&#26723;-&#20027;&#39064;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#22797;&#21512;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20027;&#39064;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#20013;&#21516;&#26102;&#21033;&#29992;&#28508;&#22312;&#20027;&#39064;&#21644;&#21487;&#35299;&#37322;&#20027;&#39064;&#20197;&#21450;&#21477;&#23376;&#32423;&#30340;&#20027;&#39064;&#23545;&#35805;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#22914;&#35821;&#35328;&#24314;&#27169;&#12289;&#35789;&#20041;&#28040;&#27495;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Marrying topic models and language models exposes language understanding to a broader source of document-level context beyond sentences via topics. While introducing topical semantics in language models, existing approaches incorporate latent document topic proportions and ignore topical discourse in sentences of the document. This work extends the line of research by additionally introducing an explainable topic representation in language understanding, obtained from a set of key terms correspondingly for each latent topic of the proportion. Moreover, we retain sentence-topic associations along with document-topic association by modeling topical discourse for every sentence in the document. We present a novel neural composite language model that exploits both the latent and explainable topics along with topical discourse at sentence-level in a joint learning framework of topic and language models. Experiments over a range of tasks such as language modeling, word sense disambiguation, 
&lt;/p&gt;</description></item></channel></rss>