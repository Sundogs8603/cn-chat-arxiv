<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#31616;&#21333;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#33719;&#24471;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#20511;&#21161;&#31163;&#25955;&#23545;&#25239;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#21319;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#26694;&#26550;&#26469;&#25903;&#25345;&#22312;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#35774;&#32622;&#20013;&#20351;&#29992;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#23398;&#29983;&#27169;&#22411;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#36127;&#36733;&#26041;&#38754;&#30340;&#24320;&#38144;&#36739;&#23567;&#65292;&#24182;&#21487;&#20197;&#19982;&#20854;&#20182;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36731;&#26494;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2311.01441</link><description>&lt;p&gt;
&#20174;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models. (arXiv:2311.01441v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#31616;&#21333;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#33719;&#24471;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#20511;&#21161;&#31163;&#25955;&#23545;&#25239;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#21319;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#26694;&#26550;&#26469;&#25903;&#25345;&#22312;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#35774;&#32622;&#20013;&#20351;&#29992;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#23398;&#29983;&#27169;&#22411;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#36127;&#36733;&#26041;&#38754;&#30340;&#24320;&#38144;&#36739;&#23567;&#65292;&#24182;&#21487;&#20197;&#19982;&#20854;&#20182;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36731;&#26494;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#31616;&#21333;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#32467;&#21512;&#26469;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#36827;&#34892;&#33976;&#39311;&#65292;&#23637;&#31034;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#33719;&#24471;&#30340;&#40065;&#26834;&#24615;&#22686;&#30410;&#65292;&#20197;&#27492;&#21453;&#39539;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#20250;&#25104;&#20026;&#26356;&#22909;&#30340;&#25945;&#24072;&#30340;&#29468;&#24819;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#31163;&#25955;&#23545;&#25239;&#33976;&#39311;&#65288;Discrete Adversarial Distillation&#65292;DAD&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#40065;&#26834;&#30340;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;VQGAN&#23558;&#20854;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#21019;&#36896;&#20986;&#27604;&#26631;&#20934;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#40065;&#26834;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23398;&#29983;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#30340;&#40065;&#26834;&#24615;&#21644;&#24178;&#20928;&#20934;&#30830;&#24615;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19982;&#31867;&#20284;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#21152;&#20102;&#23569;&#37327;&#30340;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#19982;&#20854;&#20182;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a conceptually simple and lightweight framework for improving the robustness of vision models through the combination of knowledge distillation and data augmentation. We address the conjecture that larger models do not make for better teachers by showing strong gains in out-of-distribution robustness when distilling from pretrained foundation models. Following this finding, we propose Discrete Adversarial Distillation (DAD), which leverages a robust teacher to generate adversarial examples and a VQGAN to discretize them, creating more informative samples than standard data augmentation techniques. We provide a theoretical framework for the use of a robust teacher in the knowledge distillation with data augmentation setting and demonstrate strong gains in out-of-distribution robustness and clean accuracy across different student architectures. Notably, our method adds minor computational overhead compared to similar techniques and can be easily combined with other data augmen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#25197;&#26354;&#20989;&#25968;&#23545;Mixup&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23454;&#29616;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#30456;&#20284;&#25968;&#25454;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01434</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#25197;&#26354;&#20989;&#25968;&#23450;&#21046;Mixup&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Tailoring Mixup to Data using Kernel Warping functions. (arXiv:2311.01434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#25197;&#26354;&#20989;&#25968;&#23545;Mixup&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23454;&#29616;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#30456;&#20284;&#25968;&#25454;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#23398;&#20064;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#22522;&#30784;&#12290;&#22312;&#25152;&#26377;&#25552;&#20986;&#30340;&#22686;&#24378;&#25216;&#26415;&#20013;&#65292;&#32447;&#24615;&#25554;&#20540;&#35757;&#32451;&#25968;&#25454;&#28857;&#65288;&#20063;&#31216;&#20026;Mixup&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#28857;&#36827;&#34892;&#28151;&#21512;&#65292;&#25110;&#32773;&#24212;&#29992;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#25554;&#20540;&#65292;&#32780;&#25105;&#20204;&#21017;&#23545;&#26356;&#30456;&#20284;&#30340;&#28857;&#36827;&#34892;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#24863;&#20852;&#36259;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#25197;&#26354;&#20989;&#25968;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#21462;&#20915;&#20110;&#35201;&#32452;&#21512;&#30340;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#39640;&#25928;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20197;&#36991;&#20813;&#22810;&#26679;&#24615;&#30340;&#25439;&#22833;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26082;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21448;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/ENSTA-U2IS/torch-uncertainty&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is an essential building block for learning efficient deep learning models. Among all augmentation techniques proposed so far, linear interpolation of training data points, also called mixup, has found to be effective for a large panel of applications. While the majority of works have focused on selecting the right points to mix, or applying complex non-linear interpolation, we are interested in mixing similar points more frequently and strongly than less similar ones. To this end, we propose to dynamically change the underlying distribution of interpolation coefficients through warping functions, depending on the similarity between data points to combine. We define an efficient and flexible framework to do so without losing in diversity. We provide extensive experiments for classification and regression tasks, showing that our proposed method improves both performance and calibration of models. Code available in https://github.com/ENSTA-U2IS/torch-uncertainty
&lt;/p&gt;</description></item><item><title>&#8220;Castor&#8221;&#26159;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#32508;&#21512;&#23398;&#20064;&#21508;&#20010;&#21306;&#22495;&#30340;&#22240;&#26524;&#22270;&#12290;&#23427;&#36890;&#36807;&#26368;&#22823;&#21270;&#24471;&#20998;&#20989;&#25968;&#26469;&#25512;&#26029;&#21306;&#22495;&#30340;&#25968;&#37327;&#65292;&#24182;&#23398;&#20064;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2311.01412</link><description>&lt;p&gt;
Castor: &#22240;&#26524;&#26102;&#24207;&#21306;&#22495;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Castor: Causal Temporal Regime Structure Learning. (arXiv:2311.01412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01412
&lt;/p&gt;
&lt;p&gt;
&#8220;Castor&#8221;&#26159;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#32508;&#21512;&#23398;&#20064;&#21508;&#20010;&#21306;&#22495;&#30340;&#22240;&#26524;&#22270;&#12290;&#23427;&#36890;&#36807;&#26368;&#22823;&#21270;&#24471;&#20998;&#20989;&#25968;&#26469;&#25512;&#26029;&#21306;&#22495;&#30340;&#25968;&#37327;&#65292;&#24182;&#23398;&#20064;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#65292;&#28041;&#21450;&#21040;&#20174;&#27668;&#20505;&#31185;&#23398;&#21040;&#21307;&#30103;&#20445;&#20581;&#31561;&#21508;&#20010;&#23398;&#31185;&#30340;&#24191;&#27867;&#33539;&#22260;&#12290;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#24120;&#36981;&#24490;&#22810;&#20010;&#20808;&#39564;&#26410;&#30693;&#30340;&#21306;&#22495;&#12290;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#21487;&#20197;&#20174;&#20855;&#26377;&#24050;&#30693;&#21306;&#22495;&#30340;&#24322;&#26500;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#25688;&#35201;&#22240;&#26524;&#22270;&#65292;&#20294;&#22312;&#20840;&#38754;&#23398;&#20064;&#21306;&#22495;&#21644;&#30456;&#24212;&#30340;&#22240;&#26524;&#22270;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CASTOR&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#30001;&#19981;&#21516;&#22240;&#26524;&#22270;&#32479;&#27835;&#30340;&#21508;&#31181;&#24322;&#26500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;EM&#31639;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#19968;&#20010;&#24471;&#20998;&#20989;&#25968;&#65292;CASTOR&#25512;&#26029;&#20986;&#21306;&#22495;&#30340;&#25968;&#37327;&#24182;&#23398;&#20064;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CASTOR&#30340;&#31283;&#20581;&#25910;&#25947;&#24615;&#36136;&#65292;&#29305;&#21035;&#31361;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of uncovering causal relationships among multivariate time series data stands as an essential and challenging objective that cuts across a broad array of disciplines ranging from climate science to healthcare. Such data entails linear or non-linear relationships, and usually follow multiple a priori unknown regimes. Existing causal discovery methods can infer summary causal graphs from heterogeneous data with known regimes, but they fall short in comprehensively learning both regimes and the corresponding causal graph. In this paper, we introduce CASTOR, a novel framework designed to learn causal relationships in heterogeneous time series data composed of various regimes, each governed by a distinct causal graph. Through the maximization of a score function via the EM algorithm, CASTOR infers the number of regimes and learns linear or non-linear causal relationships in each regime. We demonstrate the robust convergence properties of CASTOR, specifically highlighting its profic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20998;&#26512;&#20197;&#22826;&#22346;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#27169;&#24335;&#65292;&#24182;&#20248;&#21270;&#32593;&#32476;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#26368;&#32456;&#30446;&#26631;&#26159;&#23398;&#20064;&#22312;&#19981;&#21516;&#32593;&#32476;&#29366;&#24577;&#19979;&#37319;&#21462;&#30340;&#26368;&#20339;&#34892;&#21160;&#65292;&#25552;&#39640;&#32593;&#32476;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01406</link><description>&lt;p&gt;
&#20351;&#29992;&#32508;&#21512;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#20998;&#26512;&#20197;&#22826;&#22346;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#65292;&#20197;&#20248;&#21270;&#32593;&#32476;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analysis of Information Propagation in Ethereum Network Using Combined Graph Attention Network and Reinforcement Learning to Optimize Network Efficiency and Scalability. (arXiv:2311.01406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20998;&#26512;&#20197;&#22826;&#22346;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#27169;&#24335;&#65292;&#24182;&#20248;&#21270;&#32593;&#32476;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#26368;&#32456;&#30446;&#26631;&#26159;&#23398;&#20064;&#22312;&#19981;&#21516;&#32593;&#32476;&#29366;&#24577;&#19979;&#37319;&#21462;&#30340;&#26368;&#20339;&#34892;&#21160;&#65292;&#25552;&#39640;&#32593;&#32476;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#25216;&#26415;&#22312;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#38761;&#26032;&#20102;&#20449;&#24687;&#20256;&#25773;&#30340;&#26041;&#24335;&#12290;&#20197;&#22826;&#22346;&#22312;&#25512;&#21160;&#26234;&#33021;&#21512;&#32422;&#21644;&#20998;&#24067;&#24335;&#24212;&#29992;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#20102;&#35299;&#20197;&#22826;&#22346;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#21160;&#24577;&#23545;&#20110;&#30830;&#20445;&#32593;&#32476;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#20998;&#26512;&#20197;&#22826;&#22346;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#31532;&#19968;&#38454;&#27573;&#28041;&#21450;&#20174;&#20197;&#22826;&#22346;&#21306;&#22359;&#38142;&#20013;&#25910;&#38598;&#25968;&#25454;&#65292;&#21253;&#25324;&#21306;&#22359;&#12289;&#20132;&#26131;&#21644;&#33410;&#28857;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#37051;&#25509;&#30697;&#38453;&#26500;&#24314;&#20102;&#19968;&#20010;&#20132;&#26131;&#22270;&#34920;&#31034;&#65292;&#20197;&#25429;&#25417;&#33410;&#28857;&#23884;&#20837;&#65307;&#32780;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#26469;&#20248;&#21270;&#32593;&#32476;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23427;&#23398;&#20064;&#22312;&#19981;&#21516;&#32593;&#32476;&#29366;&#24577;&#19979;&#37319;&#21462;&#30340;&#26368;&#20339;&#34892;&#21160;&#65292;&#26368;&#32456;&#23548;&#33268;&#32593;&#32476;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchain technology has revolutionized the way information is propagated in decentralized networks. Ethereum plays a pivotal role in facilitating smart contracts and decentralized applications. Understanding information propagation dynamics in Ethereum is crucial for ensuring network efficiency, security, and scalability. In this study, we propose an innovative approach that utilizes Graph Convolutional Networks (GCNs) to analyze the information propagation patterns in the Ethereum network. The first phase of our research involves data collection from the Ethereum blockchain, consisting of blocks, transactions, and node degrees. We construct a transaction graph representation using adjacency matrices to capture the node embeddings; while our major contribution is to develop a combined Graph Attention Network (GAT) and Reinforcement Learning (RL) model to optimize the network efficiency and scalability. It learns the best actions to take in various network states, ultimately leading t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#30340;&#26377;&#25928;&#27169;&#20223;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2311.01378</link><description>&lt;p&gt;
Vision-Language Foundation Models&#20316;&#20026;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#27169;&#20223;&#32773;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Foundation Models as Effective Robot Imitators. (arXiv:2311.01378v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01378
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#30340;&#26377;&#25928;&#27169;&#20223;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#23427;&#20204;&#29702;&#35299;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65288;&#21253;&#25324;&#26426;&#22120;&#20154;&#25805;&#20316;&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#24335;&#26469;&#21033;&#29992;&#29616;&#26377;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#26426;&#22120;&#20154;&#25968;&#25454;&#19978;&#36827;&#34892;&#31616;&#21333;&#24494;&#35843;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#35270;&#35273;&#35821;&#35328;&#25805;&#20316;&#26694;&#26550;&#65292;&#21517;&#20026;RoboFlamingo&#65292;&#23427;&#24314;&#31435;&#22312;&#24320;&#28304;&#30340;VLMs&#65292;OpenFlamingo&#20043;&#19978;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;RoboFlamingo&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;VLMs&#36827;&#34892;&#21333;&#27493;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#65292;&#20351;&#29992;&#26174;&#24335;&#31574;&#30053;&#22836;&#27169;&#25311;&#39034;&#24207;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#21482;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#31181;&#20998;&#35299;&#20026;RoboFlamingo&#25552;&#20379;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#36827;&#34892;&#24320;&#29615;&#25511;&#21046;&#21644;&#37096;&#32626;&#30340;&#28789;&#27963;&#24615;&#12290;&#36890;&#36807;&#22312;&#27979;&#35797;&#22522;&#20934;&#19978;&#22823;&#24133;&#36229;&#36807;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RoboFlamingo&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#27169;&#20223;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RegionSpot&#30340;&#26032;&#22411;&#12289;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#21306;&#22495;&#35782;&#21035;&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29702;&#35299;&#26080;&#32422;&#26463;&#22270;&#20687;&#20013;&#21306;&#22495;&#30340;&#35821;&#20041;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01373</link><description>&lt;p&gt;
&#35748;&#35777;&#20219;&#20309;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Recognize Any Regions. (arXiv:2311.01373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RegionSpot&#30340;&#26032;&#22411;&#12289;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#21306;&#22495;&#35782;&#21035;&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29702;&#35299;&#26080;&#32422;&#26463;&#22270;&#20687;&#20013;&#21306;&#22495;&#30340;&#35821;&#20041;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26080;&#32422;&#26463;&#22270;&#20687;&#20013;&#21508;&#20010;&#21306;&#22495;&#25110;&#22359;&#30340;&#35821;&#20041;&#65292;&#20363;&#22914;&#22312;&#24320;&#25918;&#19990;&#30028;&#29289;&#20307;&#26816;&#27979;&#20013;&#65292;&#20195;&#34920;&#20102;&#19968;&#39033;&#20851;&#38190;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#12290;&#22312;&#22522;&#20110;&#24378;&#22823;&#30340;&#22270;&#20687;&#32423;&#35270;&#35273;&#35821;&#35328;&#65288;ViL&#65289;&#22522;&#30784;&#27169;&#22411;&#22914;CLIP&#30340;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#26368;&#36817;&#30340;&#21162;&#21147;&#35201;&#20040;&#36890;&#36807;&#20351;&#29992;&#24191;&#27867;&#30340;&#21306;&#22495;-&#26631;&#31614;&#23545;&#38598;&#21512;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23545;&#27604;&#27169;&#22411;&#65292;&#35201;&#20040;&#23558;&#26816;&#27979;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#21306;&#22495;&#24314;&#35758;&#30340;&#22270;&#20687;&#32423;&#34920;&#31034;&#23545;&#40784;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#37117;&#21463;&#21040;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#35757;&#32451;&#38656;&#27714;&#12289;&#25968;&#25454;&#22122;&#22768;&#30340;&#24433;&#21709;&#20197;&#21450;&#29615;&#22659;&#20449;&#24687;&#30340;&#19981;&#36275;&#31561;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29616;&#25104;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#21327;&#21516;&#28508;&#21147;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#23450;&#20301;&#21644;&#35821;&#20041;&#26041;&#38754;&#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#39640;&#25928;&#30340;&#21306;&#22495;&#35782;&#21035;&#26550;&#26500;&#65292;&#31216;&#20026;RegionSpot&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the semantics of individual regions or patches within unconstrained images, such as in open-world object detection, represents a critical yet challenging task in computer vision. Building on the success of powerful image-level vision-language (ViL) foundation models like CLIP, recent efforts have sought to harness their capabilities by either training a contrastive model from scratch with an extensive collection of region-label pairs or aligning the outputs of a detection model with image-level representations of region proposals. Despite notable progress, these approaches are plagued by computationally intensive training requirements, susceptibility to data noise, and deficiency in contextual information. To address these limitations, we explore the synergistic potential of off-the-shelf foundation models, leveraging their respective strengths in localization and semantics. We introduce a novel, generic, and efficient region recognition architecture, named RegionSpot, de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25925;&#38556;&#20195;&#29702;&#30340;&#35748;&#30693;&#36923;&#36753;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#21333;&#32431;&#27169;&#22411;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20801;&#35768;&#19990;&#30028;&#20013;&#21442;&#19982;&#30340;&#20195;&#29702;&#25968;&#37327;&#21487;&#20197;&#21464;&#21270;&#12290;&#36825;&#23545;&#20110;&#23481;&#38169;&#20998;&#24067;&#24335;&#35745;&#31639;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.01351</link><description>&lt;p&gt;
&#25925;&#38556;&#20195;&#29702;&#30340;&#35748;&#30693;&#36923;&#36753;&#30340;&#21333;&#32431;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Simplicial Models for the Epistemic Logic of Faulty Agents. (arXiv:2311.01351v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25925;&#38556;&#20195;&#29702;&#30340;&#35748;&#30693;&#36923;&#36753;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#21333;&#32431;&#27169;&#22411;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20801;&#35768;&#19990;&#30028;&#20013;&#21442;&#19982;&#30340;&#20195;&#29702;&#25968;&#37327;&#21487;&#20197;&#21464;&#21270;&#12290;&#36825;&#23545;&#20110;&#23481;&#38169;&#20998;&#24067;&#24335;&#35745;&#31639;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#19968;&#20123;&#20316;&#32773;&#19968;&#30452;&#22312;&#30740;&#31350;&#21333;&#32431;&#27169;&#22411;&#65292;&#36825;&#26159;&#22522;&#20110;&#31216;&#20026;&#21333;&#32431;&#22797;&#21512;&#20307;&#30340;&#39640;&#32500;&#32467;&#26500;&#30340;&#35748;&#30693;&#36923;&#36753;&#27169;&#22411;&#12290;&#22312;&#26368;&#21021;&#30340;&#24418;&#24335;&#20013;&#65292;&#21333;&#32431;&#27169;&#22411;&#22987;&#32456;&#34987;&#20551;&#35774;&#20026;&#32431;&#31929;&#30340;&#65292;&#24847;&#21619;&#30528;&#25152;&#26377;&#19990;&#30028;&#20855;&#26377;&#30456;&#21516;&#30340;&#32500;&#24230;&#12290;&#36825;&#30456;&#24403;&#20110;&#22522;&#20110;&#20811;&#37324;&#26222;&#20811;&#27169;&#22411;&#30340;&#26631;&#20934;S5n&#35821;&#20041;&#30340;&#35748;&#30693;&#36923;&#36753;&#12290;&#36890;&#36807;&#31227;&#38500;&#27169;&#22411;&#24517;&#39035;&#26159;&#32431;&#31929;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#21487;&#20197;&#36229;&#36234;&#24120;&#35268;&#30340;&#20811;&#37324;&#26222;&#20811;&#35821;&#20041;&#65292;&#24182;&#30740;&#31350;&#21442;&#19982;&#19968;&#20010;&#19990;&#30028;&#30340;&#20195;&#29702;&#25968;&#37327;&#21487;&#20197;&#21464;&#21270;&#30340;&#35748;&#30693;&#36923;&#36753;&#12290;&#36825;&#31181;&#26041;&#27861;&#24050;&#22312;&#35768;&#22810;&#35770;&#25991;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#20854;&#20013;&#24212;&#29992;&#20110;&#23481;&#38169;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#20854;&#20013;&#22312;&#31995;&#32479;&#25191;&#34892;&#26399;&#38388;&#21487;&#33021;&#20250;&#21457;&#29983;&#36807;&#31243;&#23849;&#28291;&#12290;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#22312;&#23450;&#20041;&#19981;&#32431;&#30340;&#21333;&#32431;&#27169;&#22411;&#26102;&#65292;&#24494;&#22937;&#30340;&#35774;&#35745;&#36873;&#25321;&#21487;&#33021;&#20250;&#23548;&#33268;&#25152;&#24471;&#21040;&#30340;&#36923;&#36753;&#30340;&#19981;&#21516;&#20844;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23545;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20844;&#29702;&#21270;&#30456;&#24212;&#30340;&#35748;&#30693;&#36923;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, several authors have been investigating simplicial models, a model of epistemic logic based on higher-dimensional structures called simplicial complexes. In the original formulation, simplicial models were always assumed to be pure, meaning that all worlds have the same dimension. This is equivalent to the standard S5n semantics of epistemic logic, based on Kripke models. By removing the assumption that models must be pure, we can go beyond the usual Kripke semantics and study epistemic logics where the number of agents participating in a world can vary. This approach has been developed in a number of papers, with applications in fault-tolerant distributed computing where processes may crash during the execution of a system. A difficulty that arises is that subtle design choices in the definition of impure simplicial models can result in different axioms of the resulting logic. In this paper, we classify those design choices systematically, and axiomatize the correspon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#25552;&#21462;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26550;&#26500;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2311.01344</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#22312;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#38405;&#35835;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers. (arXiv:2311.01344v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#25552;&#21462;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26550;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#25552;&#21462;&#26159;AI&#31995;&#32479;&#23433;&#20840;&#30340;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#20851;&#27880;&#28857;&#12290;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#35828;&#65292;&#26550;&#26500;&#26159;&#23545;&#25163;&#35797;&#22270;&#24674;&#22797;&#30340;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#20316;&#20026;&#19968;&#31995;&#21015;&#37325;&#22797;&#35745;&#31639;&#22359;&#65292;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23558;&#20135;&#29983;&#29420;&#29305;&#30340;&#20391;&#20449;&#36947;&#27844;&#38706;&#12290;&#24403;&#30446;&#26631;&#24179;&#21488;&#22312;&#29289;&#29702;&#19978;&#21487;&#35775;&#38382;&#26102;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#20449;&#36947;&#27844;&#38706;&#26469;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;&#36890;&#36807;&#32467;&#21512;&#23545;&#28145;&#24230;&#23398;&#20064;&#23454;&#36341;&#30340;&#29702;&#35770;&#30693;&#35782;&#21644;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#23454;&#29616;&#24211;&#65288;ARM CMSIS-NN&#65289;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#22238;&#31572;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#36890;&#36807;&#31616;&#21333;&#22320;&#26816;&#26597;&#19968;&#20010;EM&#20391;&#20449;&#36947;&#36319;&#36394;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#21462;&#22810;&#36828;&#30340;&#26550;&#26500;&#20449;&#24687;&#65311;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20256;&#32479;MLP&#21644;CNN&#27169;&#22411;&#30340;&#25552;&#21462;&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#39640;&#31471;32&#20301;&#24494;&#25511;&#21046;&#22120;&#65288;Cortex-M7&#65289;&#19978;&#36816;&#34892;&#65292;&#20165;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#27169;&#24335;&#35782;&#21035;&#20998;&#26512;&#12290;&#23613;&#31649;&#23384;&#22312;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#20294;&#25105;&#20204;&#22768;&#31216;&#65292;&#19982;&#21442;&#25968;&#25552;&#21462;&#30456;&#21453;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#20174;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#20013;&#25552;&#21462;&#20986;&#26550;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model extraction is a growing concern for the security of AI systems. For deep neural network models, the architecture is the most important information an adversary aims to recover. Being a sequence of repeated computation blocks, neural network models deployed on edge-devices will generate distinctive side-channel leakages. The latter can be exploited to extract critical information when targeted platforms are physically accessible. By combining theoretical knowledge about deep learning practices and analysis of a widespread implementation library (ARM CMSIS-NN), our purpose is to answer this critical question: how far can we extract architecture information by simply examining an EM side-channel trace? For the first time, we propose an extraction methodology for traditional MLP and CNN models running on a high-end 32-bit microcontroller (Cortex-M7) that relies only on simple pattern recognition analysis. Despite few challenging cases, we claim that, contrary to parameters extraction
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;Wasserstein&#36317;&#31163;&#26469;&#21305;&#37197;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#20013;&#27169;&#20223;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01331</link><description>&lt;p&gt;
&#36890;&#36807;&#21407;&#22987;Wasserstein&#29366;&#24577;&#21344;&#29992;&#21305;&#37197;&#23454;&#29616;&#30340;&#31163;&#32447;&#35266;&#23519;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching. (arXiv:2311.01331v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;Wasserstein&#36317;&#31163;&#26469;&#21305;&#37197;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#20013;&#27169;&#20223;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#22659;&#20013;&#65292;&#19982;&#29615;&#22659;&#30340;&#20219;&#24847;&#20132;&#20114;&#24448;&#24448;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#19987;&#23478;&#31034;&#33539;&#30340;&#34892;&#20026;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20004;&#32773;&#30340;&#38656;&#27714;&#65292;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#65288;LfO&#65289;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#21482;&#26377;&#19987;&#23478;&#29366;&#24577;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#38750;&#19987;&#23478;&#29366;&#24577;-&#21160;&#20316;&#23545;&#26469;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#20998;&#24067;&#26657;&#27491;&#20272;&#35745;&#65288;DICE&#65289;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#23398;&#20064;&#32773;&#21644;&#19987;&#23478;&#31574;&#30053;&#20043;&#38388;&#30340;&#29366;&#24577;&#21344;&#29992;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#38480;&#20110;$f$-divergences&#65288;KL&#21644;$\chi^2$&#65289;&#25110;&#24102;&#26377;Rubinstein&#23545;&#20598;&#30340;Wasserstein&#36317;&#31163;&#65292;&#21518;&#32773;&#38480;&#21046;&#20102;&#23545;&#24615;&#33021;&#20851;&#38190;&#30340;&#22522;&#30784;&#36317;&#31163;&#24230;&#37327;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#22987;Wasserstein DICE&#65288;PW-DICE&#65289;&#65292;&#23427;&#36890;&#36807;&#24754;&#35266;&#27491;&#21017;&#21270;&#22120;&#26368;&#23567;&#21270;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#20043;&#38388;&#30340;&#21407;&#22987;Wasserstein&#36317;&#31163;&#65292;&#24182;&#21033;&#29992;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;dis
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios, arbitrary interactions with the environment can often be costly, and actions of expert demonstrations are not always available. To reduce the need for both, Offline Learning from Observations (LfO) is extensively studied, where the agent learns to solve a task with only expert states and \textit{task-agnostic} non-expert state-action pairs. The state-of-the-art DIstribution Correction Estimation (DICE) methods minimize the state occupancy divergence between the learner and expert policies. However, they are limited to either $f$-divergences (KL and $\chi^2$) or Wasserstein distance with Rubinstein duality, the latter of which constrains the underlying distance metric crucial to the performance of Wasserstein-based solutions. To address this problem, we propose Primal Wasserstein DICE (PW-DICE), which minimizes the primal Wasserstein distance between the expert and learner state occupancies with a pessimistic regularizer and leverages a contrastively learned dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#35266;&#27979;&#21644;&#31034;&#20363;&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#36712;&#36857;&#30340;&#21152;&#26435;&#34892;&#20026;&#20811;&#38534;&#21644;&#19987;&#23478;&#29366;&#24577;&#37492;&#21035;&#22120;&#26469;&#31283;&#23450;&#22320;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#27169;&#20223;&#20013;&#30001;&#20110;&#19981;&#23436;&#25972;&#36712;&#36857;&#32780;&#23548;&#33268;&#30340;&#19981;&#31283;&#23450;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01329</link><description>&lt;p&gt;
&#31163;&#32447;&#35266;&#27979;&#21644;&#31034;&#20363;&#20013;&#29992;&#20110;&#27169;&#20223;&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#33021;&#21253;&#21547;&#19981;&#23436;&#25972;&#30340;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories. (arXiv:2311.01329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#35266;&#27979;&#21644;&#31034;&#20363;&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#36712;&#36857;&#30340;&#21152;&#26435;&#34892;&#20026;&#20811;&#38534;&#21644;&#19987;&#23478;&#29366;&#24577;&#37492;&#21035;&#22120;&#26469;&#31283;&#23450;&#22320;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#27169;&#20223;&#20013;&#30001;&#20110;&#19981;&#23436;&#25972;&#36712;&#36857;&#32780;&#23548;&#33268;&#30340;&#19981;&#31283;&#23450;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#20013;&#36827;&#34892;&#31163;&#32447;&#27169;&#20223;&#26088;&#22312;&#35299;&#20915;&#20165;&#20855;&#26377;&#20219;&#21153;&#29305;&#23450;&#19987;&#23478;&#29366;&#24577;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#38750;&#19987;&#23478;&#29366;&#24577;-&#25805;&#20316;&#23545;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#31163;&#32447;&#27169;&#20223;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22330;&#26223;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#20854;&#20013;&#20219;&#24847;&#30340;&#20132;&#20114;&#37117;&#26159;&#26114;&#36149;&#30340;&#19988;&#19987;&#23478;&#30340;&#25805;&#20316;&#19981;&#21487;&#29992;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#8220;&#20998;&#24067;&#30699;&#27491;&#20272;&#35745;&#8221;&#65288;DICE&#65289;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#31574;&#30053;&#20043;&#38388;&#30340;&#29366;&#24577;&#21344;&#39046;&#24046;&#24322;&#65292;&#24182;&#20351;&#29992;&#21152;&#26435;&#34892;&#20026;&#20811;&#38534;&#26816;&#32034;&#21040;&#19968;&#20010;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#24403;&#20174;&#19981;&#23436;&#25972;&#36712;&#36857;&#23398;&#20064;&#26102;&#65292;&#30001;&#20110;&#21452;&#22495;&#30340;&#38750;&#40065;&#26834;&#20248;&#21270;&#65292;&#23427;&#20204;&#30340;&#32467;&#26524;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35266;&#27979;&#30340;&#36712;&#36857;&#24863;&#30693;&#27169;&#20223;&#23398;&#20064;&#65288;TAILO&#65289;&#12290;TAILO&#20351;&#29992;&#26410;&#26469;&#36712;&#36857;&#19978;&#30340;&#25240;&#29616;&#21644;&#20316;&#20026;&#21152;&#26435;&#34892;&#20026;&#20811;&#38534;&#30340;&#26435;&#37325;&#12290;&#36825;&#20123;&#26435;&#37325;&#26159;&#30001;&#19968;&#20010;&#26088;&#22312;&#35782;&#21035;&#19987;&#23478;&#29366;&#24577;&#30340;&#37492;&#21035;&#22120;&#30340;&#36755;&#20986;&#36827;&#34892;&#32553;&#25918;&#30340;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;TAILO&#22312;&#23384;&#22312;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline imitation from observations aims to solve MDPs where only task-specific expert states and task-agnostic non-expert state-action pairs are available. Offline imitation is useful in real-world scenarios where arbitrary interactions are costly and expert actions are unavailable. The state-of-the-art "DIstribution Correction Estimation" (DICE) methods minimize divergence of state occupancy between expert and learner policies and retrieve a policy with weighted behavior cloning; however, their results are unstable when learning from incomplete trajectories, due to a non-robust optimization in the dual domain. To address the issue, in this paper, we propose Trajectory-Aware Imitation Learning from Observations (TAILO). TAILO uses a discounted sum along the future trajectory as the weight for weighted behavior cloning. The terms for the sum are scaled by the output of a discriminator, which aims to identify expert states. Despite simplicity, TAILO works well if there exist trajectorie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#33410;&#28857;&#37051;&#23621;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#21152;&#20837;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25913;&#36827;&#30693;&#35782;&#22270;&#35889;&#23436;&#21892;&#26041;&#27861;&#12290;&#22312;&#24402;&#32435;&#21644;&#20256;&#36882;&#24335;Wikidata&#23376;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KGC&#26041;&#27861;&#12290;&#37051;&#23621;&#20449;&#24687;&#23545;&#27169;&#22411;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.01326</link><description>&lt;p&gt;
&#26356;&#22909;&#22320;&#22312;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#23436;&#21892;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#37051;&#23621;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information. (arXiv:2311.01326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#33410;&#28857;&#37051;&#23621;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#21152;&#20837;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25913;&#36827;&#30693;&#35782;&#22270;&#35889;&#23436;&#21892;&#26041;&#27861;&#12290;&#22312;&#24402;&#32435;&#21644;&#20256;&#36882;&#24335;Wikidata&#23376;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KGC&#26041;&#27861;&#12290;&#37051;&#23621;&#20449;&#24687;&#23545;&#27169;&#22411;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#32463;&#24120;&#23384;&#22312;&#19981;&#23436;&#25972;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#20854;&#28508;&#22312;&#24615;&#33021;&#12290;&#30693;&#35782;&#22270;&#35889;&#23436;&#21892;&#65288;KGC&#65289;&#25216;&#26415;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;KGC&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#19978;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#65292;&#19981;&#23454;&#38469;&#65292;&#38656;&#35201;&#23398;&#20064;&#23494;&#38598;&#30340;&#33410;&#28857;&#23884;&#20837;&#21644;&#35745;&#31639;&#25104;&#23545;&#36317;&#31163;&#12290;&#29983;&#25104;&#24335;&#36716;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;T5&#21644;&#26368;&#36817;&#30340;KGT5&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#30452;&#25509;&#39044;&#27979;&#23614;&#33410;&#28857;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#19978;&#21253;&#21547;&#33410;&#28857;&#37051;&#23621;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#26469;&#25913;&#36827;KGC&#26041;&#27861;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#36825;&#31181;&#34917;&#20840;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#24402;&#32435;&#21644;&#20256;&#36882;&#24335;Wikidata&#23376;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;KGT5&#21644;&#20256;&#32479;&#30340;KGC&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23545;&#37051;&#23621;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#36890;&#36807;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;KGC&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world Knowledge Graphs (KGs) often suffer from incompleteness, which limits their potential performance. Knowledge Graph Completion (KGC) techniques aim to address this issue. However, traditional KGC methods are computationally intensive and impractical for large-scale KGs, necessitating the learning of dense node embeddings and computing pairwise distances. Generative transformer-based language models (e.g., T5 and recent KGT5) offer a promising solution as they can predict the tail nodes directly. In this study, we propose to include node neighborhoods as additional information to improve KGC methods based on language models. We examine the effects of this imputation and show that, on both inductive and transductive Wikidata subsets, our method outperforms KGT5 and conventional KGC approaches. We also provide an extensive analysis of the impact of neighborhood on model prediction and show its importance. Furthermore, we point the way to significantly improve KGC through more ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65288;SVT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20809;&#35889;&#28151;&#21512;&#26469;&#35299;&#20915;&#35270;&#35273;&#21464;&#25442;&#20013;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24615;&#21644;&#20449;&#24687;&#25429;&#25417;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01310</link><description>&lt;p&gt;
&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65306;&#20809;&#35889;&#28151;&#21512;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scattering Vision Transformer: Spectral Mixing Matters. (arXiv:2311.01310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65288;SVT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20809;&#35889;&#28151;&#21512;&#26469;&#35299;&#20915;&#35270;&#35273;&#21464;&#25442;&#20013;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24615;&#21644;&#20449;&#24687;&#25429;&#25417;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#23454;&#20363;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#27880;&#24847;&#21147;&#22797;&#26434;&#24615;&#21644;&#26377;&#25928;&#25429;&#25417;&#22270;&#20687;&#20013;&#32454;&#31890;&#24230;&#20449;&#24687;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#37319;&#29992;&#38477;&#37319;&#26679;&#25805;&#20316;&#65288;&#22914;&#27744;&#21270;&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25805;&#20316;&#26159;&#19981;&#21487;&#36870;&#30340;&#65292;&#21487;&#33021;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65288;SVT&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;SVT&#32467;&#21512;&#20102;&#19968;&#20010;&#20809;&#35889;&#25955;&#23556;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#22270;&#20687;&#32454;&#33410;&#30340;&#25429;&#25417;&#12290;SVT&#36890;&#36807;&#20998;&#31163;&#20302;&#39057;&#21644;&#39640;&#39057;&#20998;&#37327;&#65292;&#20811;&#26381;&#20102;&#19982;&#38477;&#37319;&#26679;&#25805;&#20316;&#30456;&#20851;&#30340;&#19981;&#21487;&#36870;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;SVT&#24341;&#20837;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#20809;&#35889;&#38376;&#25511;&#32593;&#32476;&#65292;&#21033;&#29992;Einstein&#20056;&#27861;&#26469;&#22788;&#29702;&#20196;&#29260;&#21644;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have gained significant attention and achieved state-of-the-art performance in various computer vision tasks, including image classification, instance segmentation, and object detection. However, challenges remain in addressing attention complexity and effectively capturing fine-grained information within images. Existing solutions often resort to down-sampling operations, such as pooling, to reduce computational cost. Unfortunately, such operations are non-invertible and can result in information loss. In this paper, we present a novel approach called Scattering Vision Transformer (SVT) to tackle these challenges. SVT incorporates a spectrally scattering network that enables the capture of intricate image details. SVT overcomes the invertibility issue associated with down-sampling operations by separating low-frequency and high-frequency components. Furthermore, SVT introduces a unique spectral gating network utilizing Einstein multiplication for token and channel 
&lt;/p&gt;</description></item><item><title>AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01305</link><description>&lt;p&gt;
AWEQ&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01305
&lt;/p&gt;
&lt;p&gt;
AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#20063;&#30456;&#23545;&#36739;&#39640;&#12290;&#37327;&#21270;&#36825;&#20123;&#27169;&#22411;&#26159;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#30828;&#20214;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AWEQ&#65292;&#19968;&#31181;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;AWEQ&#22312;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;(W8A8)&#37327;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#35266;&#23519;&#21040;&#26435;&#37325;&#37327;&#21270;&#27604;&#28608;&#27963;&#37327;&#21270;&#26356;&#23481;&#26131;&#12290;AWEQ&#36890;&#36807;&#36890;&#36947;&#22343;&#34913;&#23558;&#28608;&#27963;&#37327;&#21270;&#30340;&#38590;&#24230;&#36716;&#31227;&#21040;&#26435;&#37325;&#19978;&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#37327;&#21270;&#22256;&#38590;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#22343;&#34913;&#26041;&#27861;&#65292;&#20943;&#23567;&#20102;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#30830;&#20445;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#20687;LLaMA&#36825;&#26679;&#30340;&#27969;&#34892;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA a
&lt;/p&gt;</description></item><item><title>TRIALSCOPE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23558;&#20020;&#24202;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#37319;&#29992;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#28151;&#26434;&#22240;&#32032;&#65292;&#20197;&#20174;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#35777;&#35777;&#25454;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2311.01301</link><description>&lt;p&gt;
TRIALSCOPE&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#22240;&#26524;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#23454;&#38469;&#19990;&#30028;&#35777;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models. (arXiv:2311.01301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01301
&lt;/p&gt;
&lt;p&gt;
TRIALSCOPE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23558;&#20020;&#24202;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#37319;&#29992;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#28151;&#26434;&#22240;&#32032;&#65292;&#20197;&#20174;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#35777;&#35777;&#25454;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#30340;&#24555;&#36895;&#25968;&#23383;&#21270;&#20026;&#20248;&#21270;&#21307;&#30103;&#26381;&#21153;&#21644;&#21152;&#36895;&#29983;&#29289;&#21307;&#23398;&#21457;&#29616;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#20197;&#38750;&#32467;&#26500;&#21270;&#24418;&#24335;&#23384;&#22312;&#65292;&#22914;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#20013;&#30340;&#20020;&#24202;&#31508;&#35760;&#65292;&#24182;&#19988;&#36890;&#24120;&#21463;&#21040;&#28151;&#26434;&#22240;&#32032;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TRIALSCOPE&#65292;&#19968;&#20010;&#29992;&#20110;&#20174;&#20154;&#32676;&#32423;&#35266;&#23519;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#38469;&#19990;&#30028;&#35777;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;TRIALSCOPE&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#26469;&#25193;&#23637;&#35268;&#27169;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#65292;&#37319;&#29992;&#20808;&#36827;&#30340;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#32467;&#21512;&#26368;&#20808;&#36827;&#30340;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#24120;&#35265;&#30340;&#28151;&#26434;&#22240;&#32032;&#12290;&#21033;&#29992;&#20020;&#24202;&#35797;&#39564;&#35268;&#33539;&#20316;&#20026;&#36890;&#29992;&#34920;&#31034;&#24418;&#24335;&#65292;TRIALSCOPE&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#38190;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20351;&#29992;&#35266;&#23519;&#25968;&#25454;&#29983;&#25104;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#30284;&#30151;&#24739;&#32773;&#30340;&#22823;&#35268;&#27169;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid digitization of real-world data offers an unprecedented opportunity for optimizing healthcare delivery and accelerating biomedical discovery. In practice, however, such data is most abundantly available in unstructured forms, such as clinical notes in electronic medical records (EMRs), and it is generally plagued by confounders. In this paper, we present TRIALSCOPE, a unifying framework for distilling real-world evidence from population-level observational data. TRIALSCOPE leverages biomedical language models to structure clinical text at scale, employs advanced probabilistic modeling for denoising and imputation, and incorporates state-of-the-art causal inference techniques to combat common confounders. Using clinical trial specification as generic representation, TRIALSCOPE provides a turn-key solution to generate and reason with clinical hypotheses using observational data. In extensive experiments and analyses on a large-scale real-world dataset with over one million canc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;UniFolding&#65292;&#19968;&#31181;&#39640;&#25928;&#21033;&#29992;&#26679;&#26412;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#29992;&#20110;&#23637;&#24320;&#21644;&#25240;&#21472;&#21508;&#31181;&#26381;&#35013;&#12290;&#36890;&#36807;&#25972;&#21512;&#23637;&#24320;&#21644;&#25240;&#21472;&#20915;&#31574;&#65292;&#20197;&#21450;&#21033;&#29992;&#37096;&#20998;&#28857;&#20113;&#25968;&#25454;&#65292;UniFolding&#20855;&#26377;&#36739;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#32441;&#29702;&#21644;&#24418;&#29366;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01267</link><description>&lt;p&gt;
UniFolding: &#23454;&#29616;&#39640;&#25928;&#26679;&#26412;&#21033;&#29992;&#29575;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#26426;&#22120;&#20154;&#26381;&#35013;&#25240;&#21472;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding. (arXiv:2311.01267v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;UniFolding&#65292;&#19968;&#31181;&#39640;&#25928;&#21033;&#29992;&#26679;&#26412;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#29992;&#20110;&#23637;&#24320;&#21644;&#25240;&#21472;&#21508;&#31181;&#26381;&#35013;&#12290;&#36890;&#36807;&#25972;&#21512;&#23637;&#24320;&#21644;&#25240;&#21472;&#20915;&#31574;&#65292;&#20197;&#21450;&#21033;&#29992;&#37096;&#20998;&#28857;&#20113;&#25968;&#25454;&#65292;UniFolding&#20855;&#26377;&#36739;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#32441;&#29702;&#21644;&#24418;&#29366;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;UniFolding&#30340;&#24320;&#21457;&#65292;&#23427;&#26159;&#19968;&#31181;&#39640;&#25928;&#21033;&#29992;&#26679;&#26412;&#12289;&#21487;&#25193;&#23637;&#21644;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#29992;&#20110;&#23637;&#24320;&#21644;&#25240;&#21472;&#21508;&#31181;&#26381;&#35013;&#12290;UniFolding&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;UFONet&#31070;&#32463;&#32593;&#32476;&#23558;&#23637;&#24320;&#21644;&#25240;&#21472;&#20915;&#31574;&#25972;&#21512;&#21040;&#19968;&#20010;&#21333;&#19968;&#31574;&#30053;&#27169;&#22411;&#20013;&#65292;&#35813;&#27169;&#22411;&#21487;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#21644;&#29366;&#24577;&#30340;&#26381;&#35013;&#12290;UniFolding&#30340;&#35774;&#35745;&#22522;&#20110;&#26381;&#35013;&#30340;&#37096;&#20998;&#28857;&#20113;&#65292;&#36825;&#26377;&#21161;&#20110;&#27867;&#21270;&#24182;&#38477;&#20302;&#23545;&#32441;&#29702;&#21644;&#24418;&#29366;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;&#35757;&#32451;&#27969;&#31243;&#20248;&#20808;&#32771;&#34385;&#20302;&#25104;&#26412;&#12289;&#39640;&#25928;&#26679;&#26412;&#25910;&#38598;&#12290;&#36890;&#36807;&#22522;&#20110;&#20154;&#30340;&#36807;&#31243;&#36827;&#34892;&#31163;&#32447;&#21644;&#22312;&#32447;&#38454;&#27573;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#31163;&#32447;&#38454;&#27573;&#36890;&#36807;&#34394;&#25311;&#29616;&#23454;&#36827;&#34892;&#20154;&#31867;&#30340;&#23637;&#24320;&#21644;&#25240;&#21472;&#25805;&#20316;&#65292;&#32780;&#22312;&#32447;&#38454;&#27573;&#21017;&#21033;&#29992;&#20154;&#26426;&#21327;&#21516;&#23398;&#20064;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#35813;&#31995;&#32479;&#22312;&#20004;&#31181;&#26381;&#35013;&#31867;&#22411;&#65306;&#38271;&#34966;&#21644;&#30701;&#34966;&#34924;&#34923;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#24615;&#33021;&#35780;&#20272;&#22522;&#20110;20&#20214;&#20855;&#26377;&#26174;&#33879;&#21464;&#21270;&#30340;&#34924;&#34923;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the development of UniFolding, a sample-efficient, scalable, and generalizable robotic system for unfolding and folding various garments. UniFolding employs the proposed UFONet neural network to integrate unfolding and folding decisions into a single policy model that is adaptable to different garment types and states. The design of UniFolding is based on a garment's partial point cloud, which aids in generalization and reduces sensitivity to variations in texture and shape. The training pipeline prioritizes low-cost, sample-efficient data collection. Training data is collected via a human-centric process with offline and online stages. The offline stage involves human unfolding and folding actions via Virtual Reality, while the online stage utilizes human-in-the-loop learning to fine-tune the model in a real-world setting. The system is tested on two garment types: long-sleeve and short-sleeve shirts. Performance is evaluated on 20 shirts with significant variation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23569;&#37327;&#20154;&#24037;&#26631;&#27880;&#30340;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#39537;&#21160;&#30340;&#34920;&#36798;&#24615;&#25991;&#26412;&#21040;&#35821;&#38899;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#34920;&#36798;&#24615;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#21464;&#20026;&#26679;&#24335;&#26816;&#32034;&#20219;&#21153;&#65292;&#20351;&#24471;&#21512;&#25104;&#28436;&#35762;&#20855;&#26377;&#39044;&#26399;&#26679;&#24335;&#12290;</title><link>http://arxiv.org/abs/2311.01260</link><description>&lt;p&gt;
&#20351;&#29992;&#23569;&#37327;&#20154;&#24037;&#26631;&#27880;&#30340;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#39537;&#21160;&#30340;&#34920;&#36798;&#24615;&#25991;&#26412;&#21040;&#35821;&#38899;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Expressive TTS Driven by Natural Language Prompts Using Few Human Annotations. (arXiv:2311.01260v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23569;&#37327;&#20154;&#24037;&#26631;&#27880;&#30340;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#39537;&#21160;&#30340;&#34920;&#36798;&#24615;&#25991;&#26412;&#21040;&#35821;&#38899;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#34920;&#36798;&#24615;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#21464;&#20026;&#26679;&#24335;&#26816;&#32034;&#20219;&#21153;&#65292;&#20351;&#24471;&#21512;&#25104;&#28436;&#35762;&#20855;&#26377;&#39044;&#26399;&#26679;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#36798;&#24615;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26088;&#22312;&#21512;&#25104;&#20855;&#26377;&#20154;&#31867;&#33324;&#30340;&#35821;&#35843;&#12289;&#24773;&#32490;&#29978;&#33267;&#33402;&#26415;&#23646;&#24615;&#30340;&#28436;&#35762;&#12290;&#26368;&#36817;&#22312;&#34920;&#36798;&#24615;TTS&#26041;&#38754;&#30340;&#36827;&#23637;&#20351;&#29992;&#25143;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30452;&#25509;&#25511;&#21046;&#21512;&#25104;&#39118;&#26684;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#24102;&#26377;&#26631;&#27880;&#26679;&#24335;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#21487;&#33021;&#38590;&#20197;&#33719;&#21462;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22266;&#23450;&#30340;&#26679;&#24335;&#26631;&#27880;&#65292;&#23427;&#20204;&#30340;&#36866;&#24212;&#33021;&#21147;&#21487;&#33021;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FreeStyleTTS&#65288;FS-TTS&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#26368;&#23567;&#20154;&#24037;&#26631;&#27880;&#30340;&#21487;&#25511;&#34920;&#36798;&#24615;TTS&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#34920;&#36798;&#24615;TTS&#36716;&#21464;&#20026;&#26679;&#24335;&#26816;&#32034;&#20219;&#21153;&#12290;LLM&#26681;&#25454;&#22806;&#37096;&#26679;&#24335;&#25552;&#31034;&#20174;&#24050;&#27880;&#37322;&#30340;&#35821;&#21477;&#20013;&#36873;&#25321;&#26368;&#20339;&#21305;&#37197;&#30340;&#26679;&#24335;&#21442;&#32771;&#65292;&#36825;&#20123;&#26679;&#24335;&#25552;&#31034;&#21487;&#20197;&#26159;&#21407;&#22987;&#36755;&#20837;&#25991;&#26412;&#25110;&#33258;&#28982;&#35821;&#35328;&#26679;&#24335;&#25551;&#36848;&#12290;&#25152;&#36873;&#30340;&#21442;&#32771;&#25351;&#23548;TTS&#31649;&#36947;&#20197;&#21512;&#25104;&#20855;&#26377;&#39044;&#26399;&#26679;&#24335;&#30340;&#28436;&#35762;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#24341;&#36827;&#20102;&#23569;&#37327;&#20154;&#24037;&#26631;&#27880;&#26469;&#39537;&#21160;&#34920;&#36798;&#24615;TTS&#12290;
&lt;/p&gt;
&lt;p&gt;
Expressive text-to-speech (TTS) aims to synthesize speeches with human-like tones, moods, or even artistic attributes. Recent advancements in expressive TTS empower users with the ability to directly control synthesis style through natural language prompts. However, these methods often require excessive training with a significant amount of style-annotated data, which can be challenging to acquire. Moreover, they may have limited adaptability due to fixed style annotations. In this work, we present FreeStyleTTS (FS-TTS), a controllable expressive TTS model with minimal human annotations. Our approach utilizes a large language model (LLM) to transform expressive TTS into a style retrieval task. The LLM selects the best-matching style references from annotated utterances based on external style prompts, which can be raw input text or natural language style descriptions. The selected reference guides the TTS pipeline to synthesize speeches with the intended style. This innovative approach
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#33258;&#20027;&#31995;&#32479;&#39046;&#22495;&#20013;&#24212;&#29992;&#24418;&#24335;&#21270;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#26500;&#36896;&#27491;&#30830;&#24615;&#21512;&#25104;&#12289;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#21644;&#31995;&#32479;&#30417;&#27979;&#31561;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2311.01258</link><description>&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Formal Methods for Autonomous Systems. (arXiv:2311.01258v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#33258;&#20027;&#31995;&#32479;&#39046;&#22495;&#20013;&#24212;&#29992;&#24418;&#24335;&#21270;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#26500;&#36896;&#27491;&#30830;&#24615;&#21512;&#25104;&#12289;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#21644;&#31995;&#32479;&#30417;&#27979;&#31561;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#24335;&#21270;&#26041;&#27861;&#25351;&#30340;&#26159;&#31995;&#32479;&#24320;&#21457;&#20013;&#20005;&#35880;&#30340;&#12289;&#25968;&#23398;&#26041;&#27861;&#65292;&#23545;&#20110;&#30830;&#31435;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#30340;&#27491;&#30830;&#24615;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#24418;&#24335;&#21270;&#26041;&#27861;&#30340;&#20027;&#35201;&#26500;&#24314;&#27169;&#22359;&#26159;&#27169;&#22411;&#21644;&#35268;&#33539;&#65292;&#31867;&#20284;&#20110;&#31995;&#32479;&#35774;&#35745;&#20013;&#30340;&#34892;&#20026;&#21644;&#38656;&#27714;&#65292;&#23427;&#20204;&#32473;&#25105;&#20204;&#25552;&#20379;&#20102;&#39564;&#35777;&#21644;&#21512;&#25104;&#31995;&#32479;&#34892;&#20026;&#30340;&#25163;&#27573;&#21644;&#24418;&#24335;&#21270;&#20445;&#35777;&#12290;&#26412;&#19987;&#33879;&#23545;&#33258;&#20027;&#31995;&#32479;&#39046;&#22495;&#20013;&#24418;&#24335;&#21270;&#26041;&#27861;&#24212;&#29992;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#19981;&#21516;&#30340;&#24418;&#24335;&#21270;&#26465;&#20214;&#19979;&#36827;&#34892;&#26500;&#36896;&#27491;&#30830;&#24615;&#21512;&#25104;&#65292;&#21253;&#25324;&#23553;&#38381;&#31995;&#32479;&#12289;&#21453;&#24212;&#24335;&#31995;&#32479;&#21644;&#27010;&#29575;&#35774;&#32622;&#31561;&#12290;&#38500;&#20102;&#22312;&#24050;&#30693;&#29615;&#22659;&#20013;&#21512;&#25104;&#31995;&#32479;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#24418;&#24335;&#21270;&#26041;&#27861;&#22788;&#29702;&#20102;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#23545;&#37319;&#29992;&#23398;&#20064;&#30340;&#31995;&#32479;&#34892;&#20026;&#36827;&#34892;&#20102;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19982;&#30417;&#27979;&#30456;&#20851;&#30340;&#31995;&#32479;&#21512;&#25104;&#65292;&#36825;&#26159;&#19968;&#31181;&#20445;&#35777;&#31995;&#32479;&#22312;&#20559;&#31163;&#39044;&#26399;&#34892;&#20026;&#21518;&#20173;&#33021;&#25214;&#21040;&#19968;&#31181;&#26041;&#24335;&#30340;&#32531;&#35299;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formal methods refer to rigorous, mathematical approaches to system development and have played a key role in establishing the correctness of safety-critical systems. The main building blocks of formal methods are models and specifications, which are analogous to behaviors and requirements in system design and give us the means to verify and synthesize system behaviors with formal guarantees.  This monograph provides a survey of the current state of the art on applications of formal methods in the autonomous systems domain. We consider correct-by-construction synthesis under various formulations, including closed systems, reactive, and probabilistic settings. Beyond synthesizing systems in known environments, we address the concept of uncertainty and bound the behavior of systems that employ learning using formal methods. Further, we examine the synthesis of systems with monitoring, a mitigation technique for ensuring that once a system deviates from expected behavior, it knows a way o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#32508;&#21512;&#32771;&#34385;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21155;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#24615;&#33021;&#30456;&#36817;&#30340;&#24773;&#20917;&#19979;&#24212;&#37325;&#35270;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#26041;&#38754;&#30340;&#32771;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01256</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#28304;&#30340;&#27861;&#24459;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#24120;&#35265;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An energy-based comparative analysis of common approaches to text classification in the Legal domain. (arXiv:2311.01256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#32508;&#21512;&#32771;&#34385;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21155;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#24615;&#33021;&#30456;&#36817;&#30340;&#24773;&#20917;&#19979;&#24212;&#37325;&#35270;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#26041;&#38754;&#30340;&#32771;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#35780;&#20272;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36861;&#27714;&#26368;&#20339;&#24615;&#33021;&#30340;&#31454;&#20105;&#20013;&#65292;&#32463;&#24120;&#24573;&#35270;&#35768;&#22810;&#37325;&#35201;&#22240;&#32032;&#65292;&#32780;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#22240;&#32032;&#24212;&#35813;&#34987;&#20180;&#32454;&#32771;&#34385;&#12290;&#23454;&#38469;&#19978;&#65292;&#26377;&#26102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#32780;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#22240;&#32032;&#24517;&#39035;&#32771;&#34385;&#22312;&#20869;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;NLP&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;LexGLUE&#22522;&#20934;&#19978;&#23545;LLM&#21644;&#20256;&#32479;&#26041;&#27861;&#65288;&#20363;&#22914;SVM&#65289;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#37327;&#27604;&#36739;&#65292;&#21516;&#26102;&#32771;&#34385;&#24615;&#33021;&#65288;&#26631;&#20934;&#25351;&#26631;&#65289;&#21644;&#20854;&#20182;&#25351;&#26631;&#65292;&#22914;&#26102;&#38388;&#12289;&#32791;&#33021;&#21644;&#25104;&#26412;&#65292;&#24635;&#20043;&#23601;&#26159;&#30899;&#36275;&#36857;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#20998;&#21035;&#32771;&#34385;&#20102;&#21407;&#22411;&#35774;&#35745;&#38454;&#27573;&#65288;&#36890;&#36807;&#35757;&#32451;-&#39564;&#35777;-&#27979;&#35797;&#36845;&#20195;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65289;&#21644;&#29983;&#20135;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most Machine Learning research evaluates the best solutions in terms of performance. However, in the race for the best performing model, many important aspects are often overlooked when, on the contrary, they should be carefully considered. In fact, sometimes the gaps in performance between different approaches are neglectable, whereas factors such as production costs, energy consumption, and carbon footprint must take into consideration. Large Language Models (LLMs) are extensively adopted to address NLP problems in academia and industry. In this work, we present a detailed quantitative comparison of LLM and traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes into account both performance (standard indices) and alternative metrics such as timing, power consumption and cost, in a word: the carbon-footprint. In our analysis, we considered the prototyping phase (model selection by training-validation-test iterations) and in-production phases separately, since they fol
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2311.01248</link><description>&lt;p&gt;
&#23558;&#20854;&#25512;&#21521;&#23637;&#31034;&#26497;&#38480;&#65306;&#22810;&#27169;&#24577;&#35270;&#35273;&#35302;&#35273;&#27169;&#20223;&#23398;&#20064;&#19982;&#21147;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching. (arXiv:2311.01248v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#36807;&#31243;&#20013;&#33719;&#21462;&#23494;&#38598;&#25509;&#35302;&#20449;&#24687;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#8220;&#36879;&#35270;&#20320;&#30340;&#30382;&#32932;&#8221;&#65288;STS&#65289;&#22411;&#20256;&#24863;&#22120;&#20855;&#26377;&#35270;&#35273;&#21644;&#35302;&#35273;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#21322;&#36879;&#26126;&#34920;&#38754;&#21644;&#21487;&#25511;&#29031;&#26126;&#23454;&#29616;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#19982;&#27169;&#20223;&#23398;&#20064;&#22312;&#23500;&#26377;&#25509;&#35302;&#30340;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#35302;&#35273;&#21147;&#27979;&#37327;&#21644;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#36816;&#21160;&#31034;&#33539;&#20013;&#20135;&#29983;&#26356;&#22909;&#21305;&#37197;&#20154;&#20307;&#31034;&#33539;&#32773;&#30340;&#21147;&#26354;&#32447;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#35270;&#35273;/&#35302;&#35273;STS&#27169;&#24335;&#20999;&#25442;&#20316;&#20026;&#25511;&#21046;&#31574;&#30053;&#36755;&#20986;&#65292;&#31616;&#21270;&#20256;&#24863;&#22120;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#35266;&#23519;&#37197;&#32622;&#65292;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#35270;&#35273;/&#35302;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#27169;&#24335;&#20999;&#25442;&#21644;&#19981;&#20999;&#25442;&#65289;&#19982;&#25163;&#33109;&#25346;&#36733;&#30340;&#30524;&#22312;&#25163;&#25668;&#20687;&#26426;&#30340;&#35270;&#35273;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24191;&#27867;&#30340;&#23454;&#39564;&#31995;&#21015;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical tactile sensors have emerged as an effective means to acquire dense contact information during robotic manipulation. A recently-introduced `see-through-your-skin' (STS) variant of this type of sensor has both visual and tactile modes, enabled by leveraging a semi-transparent surface and controllable lighting. In this work, we investigate the benefits of pairing visuotactile sensing with imitation learning for contact-rich manipulation tasks. First, we use tactile force measurements and a novel algorithm during kinesthetic teaching to yield a force profile that better matches that of the human demonstrator. Second, we add visual/tactile STS mode switching as a control policy output, simplifying the application of the sensor. Finally, we study multiple observation configurations to compare and contrast the value of visual/tactile data (both with and without mode switching) with visual data from a wrist-mounted eye-in-hand camera. We perform an extensive series of experiments on a
&lt;/p&gt;</description></item><item><title>FacadeNet&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#20102;&#20174;&#19981;&#21516;&#35270;&#35282;&#21512;&#25104;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36873;&#25321;&#24615;&#32534;&#36753;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35282;&#30456;&#20851;&#20803;&#32032;&#36827;&#34892;&#31934;&#30830;&#20462;&#25913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24314;&#31569;&#31435;&#38754;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01240</link><description>&lt;p&gt;
FacadeNet: &#36890;&#36807;&#36873;&#25321;&#24615;&#32534;&#36753;&#36827;&#34892;&#26465;&#20214;&#31435;&#38754;&#32508;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FacadeNet: Conditional Facade Synthesis via Selective Editing. (arXiv:2311.01240v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01240
&lt;/p&gt;
&lt;p&gt;
FacadeNet&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#20102;&#20174;&#19981;&#21516;&#35270;&#35282;&#21512;&#25104;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36873;&#25321;&#24615;&#32534;&#36753;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35282;&#30456;&#20851;&#20803;&#32032;&#36827;&#34892;&#31934;&#30830;&#20462;&#25913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24314;&#31569;&#31435;&#38754;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;FacadeNet&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#19981;&#21516;&#30340;&#35270;&#35282;&#21512;&#25104;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23558;&#19968;&#20010;&#31435;&#38754;&#30340;&#21333;&#19968;&#35270;&#22270;&#21644;&#25152;&#38656;&#30340;&#35270;&#35282;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#19968;&#20010;&#20174;&#19981;&#21516;&#35270;&#35282;&#30475;&#21040;&#30340;&#31435;&#38754;&#22270;&#20687;&#12290;&#20026;&#20102;&#31934;&#30830;&#20462;&#25913;&#19982;&#35270;&#35282;&#30456;&#20851;&#30340;&#20803;&#32032;&#65288;&#22914;&#31383;&#25143;&#21644;&#38376;&#65289;&#65292;&#21516;&#26102;&#20445;&#30041;&#19982;&#35270;&#35282;&#26080;&#20851;&#30340;&#32467;&#26500;&#65288;&#22914;&#22681;&#22721;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36873;&#25321;&#24615;&#32534;&#36753;&#27169;&#22359;&#12290;&#35813;&#27169;&#22359;&#21033;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#25552;&#21462;&#30340;&#22270;&#20687;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24314;&#31569;&#31435;&#38754;&#29983;&#25104;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce FacadeNet, a deep learning approach for synthesizing building facade images from diverse viewpoints. Our method employs a conditional GAN, taking a single view of a facade along with the desired viewpoint information and generates an image of the facade from the distinct viewpoint. To precisely modify view-dependent elements like windows and doors while preserving the structure of view-independent components such as walls, we introduce a selective editing module. This module leverages image embeddings extracted from a pre-trained vision transformer. Our experiments demonstrated state-of-the-art performance on building facade generation, surpassing alternative methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;AI&#21103;&#39550;&#39542;&#21592;&#26469;&#23548;&#33322;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;&#65292;&#24182;&#25506;&#35752;&#20102;&#29983;&#25104;AI&#21644;&#36741;&#21161;&#20195;&#29702;&#30340;&#20986;&#29616;&#23545;&#20110;&#25903;&#25345;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;&#30340;&#28508;&#21147;&#21644;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01235</link><description>&lt;p&gt;
&#20351;&#29992;AI&#21103;&#39550;&#39542;&#21592;&#23548;&#33322;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Navigating Complex Search Tasks with AI Copilots. (arXiv:2311.01235v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01235
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;AI&#21103;&#39550;&#39542;&#21592;&#26469;&#23548;&#33322;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;&#65292;&#24182;&#25506;&#35752;&#20102;&#29983;&#25104;AI&#21644;&#36741;&#21161;&#20195;&#29702;&#30340;&#20986;&#29616;&#23545;&#20110;&#25903;&#25345;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;&#30340;&#28508;&#21147;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#22914;&#20449;&#24687;&#26816;&#32034;(IR)&#30740;&#31350;&#30028;&#30340;&#35768;&#22810;&#20154;&#25152;&#30693;&#21644;&#27427;&#36175;&#30340;&#37027;&#26679;&#65292;&#25628;&#32034;&#36828;&#26410;&#35299;&#20915;&#12290;&#27599;&#22825;&#37117;&#26377;&#25968;&#30334;&#19975;&#20154;&#22312;&#25628;&#32034;&#24341;&#25806;&#19978;&#38754;&#23545;&#20219;&#21153;&#30340;&#22256;&#38590;&#12290;&#20182;&#20204;&#30340;&#22256;&#38590;&#36890;&#24120;&#19982;&#20219;&#21153;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#20197;&#21450;&#25628;&#32034;&#31995;&#32479;&#26080;&#27861;&#23436;&#20840;&#29702;&#35299;&#20219;&#21153;&#21644;&#25552;&#20379;&#30456;&#20851;&#32467;&#26524;&#26377;&#20851;&#12290;&#20219;&#21153;&#28608;&#21457;&#20102;&#25628;&#32034;&#65292;&#21019;&#24314;&#20102;&#25628;&#32034;&#32773;&#23581;&#35797;&#36830;&#25509;/&#35299;&#20915;&#30340;&#24046;&#36317;/&#38382;&#39064;&#24773;&#20917;&#65292;&#24182;&#22312;&#20182;&#20204;&#22788;&#29702;&#19981;&#21516;&#20219;&#21153;&#26041;&#38754;&#26102;&#39537;&#21160;&#25628;&#32034;&#34892;&#20026;&#12290;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;&#38656;&#35201;&#30340;&#19981;&#20165;&#26159;&#22522;&#26412;&#20107;&#23454;&#26597;&#25214;&#25110;&#25628;&#32034;&#30340;&#25903;&#25345;&#12290;&#25903;&#25345;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#30740;&#31350;&#21253;&#25324;&#29983;&#25104;&#26597;&#35810;&#21644;&#32593;&#31449;&#24314;&#35758;&#65292;&#20010;&#24615;&#21270;&#21644;&#19978;&#19979;&#25991;&#21270;&#25628;&#32034;&#65292;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#25628;&#32034;&#20307;&#39564;&#65292;&#21253;&#25324;&#36328;&#26102;&#38388;&#21644;&#31354;&#38388;&#12290;&#26368;&#36817;&#20852;&#36215;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;(AI)&#21644;&#22522;&#20110;&#35813;&#25216;&#26415;&#30340;&#36741;&#21161;&#20195;&#29702;&#65292;&#25110;&#32773;&#35828;&#21103;&#39550;&#39542;&#21592;&#65292;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
As many of us in the information retrieval (IR) research community know and appreciate, search is far from being a solved problem. Millions of people struggle with tasks on search engines every day. Often, their struggles relate to the intrinsic complexity of their task and the failure of search systems to fully understand the task and serve relevant results. The task motivates the search, creating the gap/problematic situation that searchers attempt to bridge/resolve and drives search behavior as they work through different task facets. Complex search tasks require more than support for rudimentary fact finding or re-finding. Research on methods to support complex tasks includes work on generating query and website suggestions, personalizing and contextualizing search, and developing new search experiences, including those that span time and space. The recent emergence of generative artificial intelligence (AI) and the arrival of assistive agents, or copilots, based on this technology
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22312;&#38271;&#31687;&#22810;&#27169;&#24577;&#21465;&#36848;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#20013;&#25193;&#23637;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#8220;&#38271;&#35805;&#30701;&#35828;&#8221;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#35270;&#39057;&#30340;&#21465;&#36848;&#24635;&#32467;&#25104;&#31616;&#30701;&#24773;&#33410;&#65292;&#24182;&#25628;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#39057;&#37096;&#20998;&#65292;&#26469;&#36827;&#34892;&#21465;&#36848;&#24615;&#35270;&#39057;&#38382;&#31572;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38271;&#35270;&#39057;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#38646;&#26679;&#26412;&#38382;&#31572;&#22312;&#38271;&#35270;&#39057;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01233</link><description>&lt;p&gt;
&#38271;&#35805;&#30701;&#35828;&#65306;&#29992;&#20110;&#38271;&#35270;&#39057;&#38382;&#31572;&#30340;&#24635;&#32467;&#21518;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Long Story Short: a Summarize-then-Search Method for Long Video Question Answering. (arXiv:2311.01233v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22312;&#38271;&#31687;&#22810;&#27169;&#24577;&#21465;&#36848;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#20013;&#25193;&#23637;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#8220;&#38271;&#35805;&#30701;&#35828;&#8221;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#35270;&#39057;&#30340;&#21465;&#36848;&#24635;&#32467;&#25104;&#31616;&#30701;&#24773;&#33410;&#65292;&#24182;&#25628;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#39057;&#37096;&#20998;&#65292;&#26469;&#36827;&#34892;&#21465;&#36848;&#24615;&#35270;&#39057;&#38382;&#31572;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38271;&#35270;&#39057;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#38646;&#26679;&#26412;&#38382;&#31572;&#22312;&#38271;&#35270;&#39057;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-3&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#21363;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#21465;&#36848;&#24335;&#38382;&#39064;&#22238;&#31572;&#31561;&#22330;&#26223;&#20013;&#29305;&#21035;&#26377;&#25928;&#65292;&#36825;&#20123;&#22330;&#26223;&#20013;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#24456;&#22823;&#65292;&#20294;&#21487;&#29992;&#30340;&#30417;&#30563;&#25968;&#25454;&#24456;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#31181;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#23558;&#20854;&#38646;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#25193;&#23637;&#21040;&#21253;&#21547;&#21095;&#24773;&#12289;&#30005;&#24433;&#21644;&#21160;&#30011;&#31561;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#38271;&#31687;&#22810;&#27169;&#24577;&#21465;&#36848;&#20013;&#65292;&#20854;&#20013;&#25925;&#20107;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#38271;&#35805;&#30701;&#35828;&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#21465;&#36848;&#24615;&#35270;&#39057;&#38382;&#31572;&#65292;&#35813;&#26694;&#26550;&#39318;&#20808;&#23558;&#35270;&#39057;&#30340;&#21465;&#36848;&#24635;&#32467;&#25104;&#19968;&#20010;&#31616;&#30701;&#30340;&#24773;&#33410;&#65292;&#28982;&#21518;&#25628;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#39057;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;CLIPCheck&#26469;&#22686;&#24378;&#35270;&#35273;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22823;&#24133;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#27169;&#22411;&#65292;&#31361;&#26174;&#20102;&#38646;&#26679;&#26412;&#38382;&#31572;&#22312;&#38271;&#35270;&#39057;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models such as GPT-3 have demonstrated an impressive capability to adapt to new tasks without requiring task-specific training data. This capability has been particularly effective in settings such as narrative question answering, where the diversity of tasks is immense, but the available supervision data is small. In this work, we investigate if such language models can extend their zero-shot reasoning abilities to long multimodal narratives in multimedia content such as drama, movies, and animation, where the story plays an essential role. We propose Long Story Short, a framework for narrative video QA that first summarizes the narrative of the video to a short plot and then searches parts of the video relevant to the question. We also propose to enhance visual matching with CLIPCheck. Our model outperforms state-of-the-art supervised models by a large margin, highlighting the potential of zero-shot QA for long videos.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#36827;&#34892;&#34920;&#36798;&#24335;&#25512;&#23548;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#23454;&#20363;&#21270;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#32534;&#30721;&#26426;&#21046;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#26041;&#31243;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01230</link><description>&lt;p&gt;
&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#25512;&#23548;
&lt;/p&gt;
&lt;p&gt;
Multi-Operational Mathematical Derivations in Latent Space. (arXiv:2311.01230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#36827;&#34892;&#34920;&#36798;&#24335;&#25512;&#23548;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#23454;&#20363;&#21270;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#32534;&#30721;&#26426;&#21046;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#26041;&#31243;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#36827;&#34892;&#34920;&#36798;&#24335;&#25512;&#23548;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#22810;&#25805;&#20316;&#34920;&#31034;&#33539;&#24335;&#65292;&#23558;&#25968;&#23398;&#36816;&#31639;&#24314;&#27169;&#20026;&#26174;&#24335;&#30340;&#20960;&#20309;&#21464;&#25442;&#12290;&#36890;&#36807;&#21033;&#29992;&#31526;&#21495;&#24341;&#25806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;61K&#20010;&#21069;&#25552;&#21644;6&#20010;&#36816;&#31639;&#31526;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20998;&#26512;&#20102;&#27599;&#20010;&#33539;&#24335;&#22312;&#19982;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#23454;&#20363;&#21270;&#26102;&#30340;&#24615;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#26426;&#21046;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22914;&#20309;&#36924;&#36817;&#26041;&#31243;&#25512;&#29702;&#65292;&#24182;&#25506;&#35752;&#20102;&#23398;&#20064;&#19981;&#21516;&#36816;&#31639;&#31526;&#21644;&#22312;&#21333;&#20010;&#36816;&#31639;&#20013;&#19987;&#38376;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#25903;&#25345;&#22810;&#27493;&#25512;&#23548;&#21644;&#36229;&#36234;&#20998;&#24067;&#24191;&#20041;&#21270;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#22810;&#25805;&#20316;&#33539;&#24335;&#23545;&#20110;&#35299;&#24320;&#19981;&#21516;&#36816;&#31639;&#31526;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21516;&#26102;&#21487;&#20197;&#21306;&#20998;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the possibility of approximating multiple mathematical operations in latent space for expression derivation. To this end, we introduce different multi-operational representation paradigms, modelling mathematical operations as explicit geometric transformations. By leveraging a symbolic engine, we construct a large-scale dataset comprising 1.7M derivation steps stemming from 61K premises and 6 operators, analysing the properties of each paradigm when instantiated with state-of-the-art neural encoders. Specifically, we investigate how different encoding mechanisms can approximate equational reasoning in latent space, exploring the trade-off between learning different operators and specialising within single operations, as well as the ability to support multi-step derivations and out-of-distribution generalisation. Our empirical analysis reveals that the multi-operational paradigm is crucial for disentangling different operators, while discriminating the conclusion
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.01223</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;: &#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01223
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#24050;&#32463;&#20986;&#29616;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#21253;&#25324;&#20316;&#20026;&#36712;&#36857;&#35268;&#21010;&#22120;&#12289;&#34920;&#36798;&#33021;&#21147;&#20016;&#23500;&#30340;&#31574;&#30053;&#31867;&#21035;&#12289;&#25968;&#25454;&#21512;&#25104;&#22120;&#31561;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#35813;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#21551;&#21457;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#24403;&#21069;RL&#31639;&#27861;&#36935;&#21040;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#25193;&#25955;&#27169;&#22411;&#22312;RL&#20013;&#25152;&#25198;&#28436;&#30340;&#35282;&#33394;&#65292;&#25552;&#20986;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#35299;&#20915;&#29616;&#26377;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27010;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#19982;RL&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#39033;&#32508;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#37325;&#28857;&#26159;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions, including as trajectory planners, expressive policy classes, data synthesizers, etc. This survey aims to provide an overview of the advancements in this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by current RL algorithms. Then, we present a taxonomy of existing methods based on the roles played by diffusion models in RL and explore how the existing challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks while discussing the limitations of current approaches. Finally, we conclude the survey and offer insights into future research directions, focusing on enhancing model performance and applying diffusion m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#20013;&#23398;&#20064;&#26679;&#26412;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#19981;&#21516;&#35270;&#35282;&#23545;&#26679;&#26412;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#25913;&#36827;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01212</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#20851;&#31995;&#23398;&#20064;&#29992;&#20110;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-view Relation Learning for Cross-domain Few-shot Hyperspectral Image Classification. (arXiv:2311.01212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#20013;&#23398;&#20064;&#26679;&#26412;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#19981;&#21516;&#35270;&#35282;&#23545;&#26679;&#26412;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#25913;&#36827;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#20391;&#37325;&#20110;&#20174;&#28304;&#39046;&#22495;&#20013;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#20013;&#23398;&#20064;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#20013;&#20165;&#21253;&#21547;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#30340;&#20219;&#21153;&#20013;&#12290;&#22312;&#24230;&#37327;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#35768;&#22810;&#24403;&#21069;&#30340;&#26041;&#27861;&#39318;&#20808;&#25552;&#21462;&#26597;&#35810;&#26679;&#26412;&#21644;&#25903;&#25345;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#26681;&#25454;&#26597;&#35810;&#26679;&#26412;&#21040;&#25903;&#25345;&#26679;&#26412;&#25110;&#21407;&#22411;&#30340;&#36317;&#31163;&#30452;&#25509;&#39044;&#27979;&#26597;&#35810;&#26679;&#26412;&#30340;&#31867;&#21035;&#12290;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#19982;&#24403;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#19981;&#21516;&#35270;&#35282;&#23398;&#20064;&#26679;&#26412;&#20851;&#31995;&#24182;&#23558;&#20854;&#32435;&#20837;&#27169;&#22411;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#20197;&#25552;&#39640;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#22312;&#24403;&#21069;DCFSL&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#23398;&#20064;&#31867;&#21035;&#32423;&#21035;&#30340;&#26679;&#26412;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain few-shot hyperspectral image classification focuses on learning prior knowledge from a large number of labeled samples from source domain and then transferring the knowledge to the tasks which contain only few labeled samples in target domains. Following the metric-based manner, many current methods first extract the features of the query and support samples, and then directly predict the classes of query samples according to their distance to the support samples or prototypes. The relations between samples have not been fully explored and utilized. Different from current works, this paper proposes to learn sample relations from different views and take them into the model learning process, to improve the cross-domain few-shot hyperspectral image classification. Building on current DCFSL method which adopts a domain discriminator to deal with domain-level distribution difference, the proposed method applys contrastive learning to learn the class-level sample relations to o
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#35774;&#35745;&#20102;Injectivity Bit Flip Attack&#26469;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#20854;&#23545;&#22270;&#32467;&#26500;&#30340;&#35782;&#21035;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20854;&#23545;&#20301;&#21453;&#36716;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01205</link><description>&lt;p&gt;
&#20351;&#29992;&#20301;&#21453;&#36716;&#25915;&#20987;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;Weisfeiler&#21644;Lehman&#21464;&#24471;&#20919;&#28448;&#20102;
&lt;/p&gt;
&lt;p&gt;
Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go Indifferent. (arXiv:2311.01205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01205
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;Injectivity Bit Flip Attack&#26469;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#20854;&#23545;&#22270;&#32467;&#26500;&#30340;&#35782;&#21035;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20854;&#23545;&#20301;&#21453;&#36716;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25915;&#20987;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#27602;&#21270;&#21644;&#35268;&#36991;&#19978;&#65292;&#24573;&#30053;&#20102;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#26435;&#37325;&#30340;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#65292;&#22914;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20301;&#21453;&#36716;&#25915;&#20987;&#65292;&#27809;&#26377;&#32771;&#34385;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29420;&#29305;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27880;&#20837;&#29575;&#20301;&#21453;&#36716;&#25915;&#20987;&#65288;Injectivity Bit Flip Attack&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#20301;&#21453;&#36716;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#38024;&#23545;&#37327;&#21270;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#23398;&#20064;&#37051;&#22495;&#32858;&#21512;&#20989;&#25968;&#65292;&#38477;&#20302;&#20102;&#20854;&#21306;&#20998;&#22270;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22833;&#21435;&#20102;Weisfeiler-Lehman&#27979;&#35797;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21033;&#29992;&#29305;&#23450;&#20110;&#26576;&#20123;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25968;&#23398;&#23646;&#24615;&#21487;&#33021;&#20250;&#26174;&#33879;&#22686;&#21152;&#20854;&#23545;&#20301;&#21453;&#36716;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#27880;&#20837;&#29575;&#20301;&#21453;&#36716;&#25915;&#20987;&#21487;&#20197;&#23558;&#21508;&#31181;&#22270;&#23646;&#24615;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#26368;&#22823;&#34920;&#36798;&#24615;&#21516;&#26500;&#32593;&#32476;&#38477;&#32423;&#20026;&#38543;&#26426;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior attacks on graph neural networks have mostly focused on graph poisoning and evasion, neglecting the network's weights and biases. Traditional weight-based fault injection attacks, such as bit flip attacks used for convolutional neural networks, do not consider the unique properties of graph neural networks. We propose the Injectivity Bit Flip Attack, the first bit flip attack designed specifically for graph neural networks. Our attack targets the learnable neighborhood aggregation functions in quantized message passing neural networks, degrading their ability to distinguish graph structures and losing the expressivity of the Weisfeiler-Lehman test. Our findings suggest that exploiting mathematical properties specific to certain graph neural network architectures can significantly increase their vulnerability to bit flip attacks. Injectivity Bit Flip Attacks can degrade the maximal expressive Graph Isomorphism Networks trained on various graph property prediction datasets to rando
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#24341;&#23548;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;2D&#22270;&#20687;&#20013;&#23398;&#21040;&#30340;&#35270;&#35273;&#20449;&#24687;&#26469;&#23454;&#29616;&#31934;&#30830;&#19988;&#40065;&#26834;&#30340;&#28857;&#20113;&#37197;&#20934;&#12290;</title><link>http://arxiv.org/abs/2311.01202</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#24341;&#23548;&#32593;&#32476;&#36827;&#34892;&#28857;&#20113;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Information-Guided Network using Contrastive Learning for Point Cloud Registration. (arXiv:2311.01202v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01202
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#24341;&#23548;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;2D&#22270;&#20687;&#20013;&#23398;&#21040;&#30340;&#35270;&#35273;&#20449;&#24687;&#26469;&#23454;&#29616;&#31934;&#30830;&#19988;&#40065;&#26834;&#30340;&#28857;&#20113;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#28857;&#20113;&#37197;&#20934;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#28857;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#38480;&#20110;&#20165;&#20174;&#21333;&#19968;&#27169;&#24577;&#30340;&#28857;&#33719;&#21462;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#23548;&#33268;&#20840;&#23616;&#29305;&#24449;&#24863;&#30693;&#19981;&#36275;&#21644;&#32570;&#20047;&#32441;&#29702;&#20449;&#24687;&#31561;&#38382;&#39064;&#12290;&#23454;&#38469;&#19978;&#65292;&#20154;&#31867;&#21487;&#20197;&#21033;&#29992;&#20174;2D&#22270;&#20687;&#20013;&#23398;&#21040;&#30340;&#35270;&#35273;&#20449;&#24687;&#26469;&#29702;&#35299;3D&#19990;&#30028;&#12290;&#22522;&#20110;&#36825;&#19968;&#20107;&#23454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#24341;&#23548;&#32593;&#32476;&#65288;CMIGNet&#65289;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#20449;&#24687;&#33719;&#24471;&#20840;&#23616;&#24418;&#29366;&#24863;&#30693;&#65292;&#20197;&#23454;&#29616;&#31934;&#30830;&#19988;&#40065;&#26834;&#30340;&#28857;&#20113;&#37197;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#28857;&#20113;&#30340;&#25237;&#24433;&#22270;&#20687;&#19982;&#27880;&#24847;&#26426;&#21046;&#34701;&#21512;&#65292;&#22312;&#20132;&#21449;&#27169;&#24577;&#29305;&#24449;&#19978;&#36827;&#34892;&#34701;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20004;&#31181;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#21363;&#37325;&#21472;&#23545;&#27604;&#23398;&#20064;&#21644;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#12290;&#21069;&#32773;&#20391;&#37325;&#20110;&#37325;&#21472;&#21306;&#22495;&#30340;&#29305;&#24449;&#65292;&#32780;&#21518;&#32773;&#24378;&#35843;&#36328;&#27169;&#24577;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The majority of point cloud registration methods currently rely on extracting features from points. However, these methods are limited by their dependence on information obtained from a single modality of points, which can result in deficiencies such as inadequate perception of global features and a lack of texture information. Actually, humans can employ visual information learned from 2D images to comprehend the 3D world. Based on this fact, we present a novel Cross-Modal Information-Guided Network (CMIGNet), which obtains global shape perception through cross-modal information to achieve precise and robust point cloud registration. Specifically, we first incorporate the projected images from the point clouds and fuse the cross-modal features using the attention mechanism. Furthermore, we employ two contrastive learning strategies, namely overlapping contrastive learning and cross-modal contrastive learning. The former focuses on features in overlapping regions, while the latter emph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#36793;&#32536;&#24863;&#30693;&#35774;&#22791;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#36793;&#32536;&#24863;&#30693;&#35774;&#22791;&#30340;&#24555;&#36895;&#22686;&#38271;&#20351;&#24471;&#30417;&#27979;&#29615;&#22659;&#29305;&#24449;&#24182;&#33719;&#21462;&#29615;&#22659;&#20449;&#24687;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#20113;&#31471;&#25110;&#26381;&#21153;&#22120;&#19978;&#30340;&#20998;&#26512;&#38754;&#20020;&#38544;&#31169;&#12289;&#30828;&#20214;&#21644;&#36830;&#25509;&#24615;&#31561;&#25361;&#25112;&#12290;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2311.01201</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#24863;&#30693;&#35774;&#22791;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Federated Learning on Edge Sensing Devices: A Review. (arXiv:2311.01201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#36793;&#32536;&#24863;&#30693;&#35774;&#22791;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#36793;&#32536;&#24863;&#30693;&#35774;&#22791;&#30340;&#24555;&#36895;&#22686;&#38271;&#20351;&#24471;&#30417;&#27979;&#29615;&#22659;&#29305;&#24449;&#24182;&#33719;&#21462;&#29615;&#22659;&#20449;&#24687;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#20113;&#31471;&#25110;&#26381;&#21153;&#22120;&#19978;&#30340;&#20998;&#26512;&#38754;&#20020;&#38544;&#31169;&#12289;&#30828;&#20214;&#21644;&#36830;&#25509;&#24615;&#31561;&#25361;&#25112;&#12290;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#22686;&#38271;&#30340;&#36793;&#32536;&#24863;&#30693;&#35774;&#22791;&#65288;&#22914;&#29289;&#32852;&#32593;&#12289;&#31227;&#21160;&#35774;&#22791;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#65289;&#21450;&#20854;&#38598;&#25104;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#33021;&#21147;&#20351;&#24471;&#30417;&#25511;&#29615;&#22659;&#29305;&#24449;&#12289;&#19982;&#20043;&#20132;&#20114;&#24182;&#33719;&#21462;&#29615;&#22659;&#20449;&#24687;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#36825;&#20123;&#35774;&#22791;&#20307;&#31215;&#23567;&#12289;&#25968;&#25454;&#23384;&#20648;&#21644;&#22788;&#29702;&#33021;&#21147;&#36739;&#20302;&#65292;&#20294;&#23427;&#20204;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#25910;&#38598;&#21644;&#22788;&#29702;&#24212;&#29992;&#39046;&#22495;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#12289;&#29615;&#22659;&#65288;&#21253;&#25324;&#31354;&#27668;&#36136;&#37327;&#21644;&#27745;&#26579;&#27700;&#24179;&#65289;&#12289;&#27773;&#36710;&#12289;&#24037;&#19994;&#12289;&#33322;&#31354;&#33322;&#22825;&#21644;&#20892;&#19994;&#31561;&#12290;&#20174;&#36793;&#32536;&#35774;&#22791;&#25910;&#38598;&#30340;&#36825;&#20123;&#28023;&#37327;&#24863;&#30693;&#25968;&#25454;&#20351;&#29992;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#22312;&#20113;&#31471;&#25110;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#20998;&#26512;&#20250;&#24102;&#26469;&#19982;&#38544;&#31169;&#12289;&#30828;&#20214;&#21644;&#36830;&#25509;&#24615;&#38480;&#21046;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#27491;&#36880;&#28176;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to monitor ambient characteristics, interact with them, and derive information about the surroundings has been made possible by the rapid proliferation of edge sensing devices like IoT, mobile, and wearable devices and their measuring capabilities with integrated sensors. Even though these devices are small and have less capacity for data storage and processing, they produce vast amounts of data. Some example application areas where sensor data is collected and processed include healthcare, environmental (including air quality and pollution levels), automotive, industrial, aerospace, and agricultural applications. These enormous volumes of sensing data collected from the edge devices are analyzed using a variety of Machine Learning (ML) and Deep Learning (DL) approaches. However, analyzing them on the cloud or a server presents challenges related to privacy, hardware, and connectivity limitations. Federated Learning (FL) is emerging as a solution to these problems while pre
&lt;/p&gt;</description></item><item><title>AiluRus&#26159;&#19968;&#31181;&#29992;&#20110;&#23494;&#38598;&#39044;&#27979;&#30340;&#21487;&#25193;&#23637;ViT&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#33258;&#36866;&#24212;&#20998;&#36776;&#29575;&#21644;&#23494;&#24230;&#32858;&#31867;&#31639;&#27861;&#65292;&#23558;&#35821;&#20041;&#30456;&#20284;&#30340;&#20196;&#29260;&#21512;&#24182;&#22312;&#19968;&#36215;&#65292;&#20197;&#22788;&#29702;&#38271;&#20196;&#29260;&#24207;&#21015;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01197</link><description>&lt;p&gt;
AiluRus: &#19968;&#31181;&#29992;&#20110;&#23494;&#38598;&#39044;&#27979;&#30340;&#21487;&#25193;&#23637;ViT&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AiluRus: A Scalable ViT Framework for Dense Prediction. (arXiv:2311.01197v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01197
&lt;/p&gt;
&lt;p&gt;
AiluRus&#26159;&#19968;&#31181;&#29992;&#20110;&#23494;&#38598;&#39044;&#27979;&#30340;&#21487;&#25193;&#23637;ViT&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#33258;&#36866;&#24212;&#20998;&#36776;&#29575;&#21644;&#23494;&#24230;&#32858;&#31867;&#31639;&#27861;&#65292;&#23558;&#35821;&#20041;&#30456;&#20284;&#30340;&#20196;&#29260;&#21512;&#24182;&#22312;&#19968;&#36215;&#65292;&#20197;&#22788;&#29702;&#38271;&#20196;&#29260;&#24207;&#21015;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision transformers (ViTs)&#22240;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#25104;&#20026;&#35270;&#35273;&#20219;&#21153;&#20013;&#24120;&#35265;&#30340;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#38271;&#20196;&#29260;&#24207;&#21015;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;ViTs&#30340;&#22797;&#26434;&#24615;&#26174;&#33879;&#22686;&#21152;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#65288;&#22914;&#35821;&#20041;&#20998;&#21106;&#25110;&#30446;&#26631;&#26816;&#27979;&#65289;&#26356;&#21152;&#24378;&#35843;&#23545;&#35937;&#30340;&#36718;&#24275;&#25110;&#24418;&#29366;&#65292;&#32780;&#23545;&#35937;&#20869;&#37096;&#30340;&#32441;&#29702;&#20449;&#24687;&#36739;&#23569;&#12290;&#21463;&#27492;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#26681;&#25454;&#22270;&#20687;&#20013;&#19981;&#21516;&#21306;&#22495;&#30340;&#37325;&#35201;&#24615;&#24212;&#29992;&#33258;&#36866;&#24212;&#20998;&#36776;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;ViT&#30340;&#20013;&#38388;&#23618;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#24863;&#30693;&#30340;&#23494;&#24230;&#32858;&#31867;&#31639;&#27861;&#20174;&#20196;&#29260;&#24207;&#21015;&#20013;&#36873;&#25321;&#20195;&#34920;&#24615;&#30340;&#20196;&#29260;&#12290;&#30830;&#23450;&#20102;&#20195;&#34920;&#24615;&#20196;&#29260;&#21518;&#65292;&#25105;&#20204;&#32487;&#32493;&#23558;&#20854;&#20182;&#20196;&#29260;&#21512;&#24182;&#21040;&#26368;&#36817;&#30340;&#20195;&#34920;&#24615;&#20196;&#29260;&#20013;&#12290;&#22240;&#27492;&#65292;&#35821;&#20041;&#30456;&#20284;&#30340;&#20196;&#29260;&#34987;&#21512;&#24182;&#22312;&#19968;&#36215;&#24418;&#25104;&#20302;&#20998;&#36776;&#29575;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers (ViTs) have emerged as a prevalent architecture for vision tasks owing to their impressive performance. However, when it comes to handling long token sequences, especially in dense prediction tasks that require high-resolution input, the complexity of ViTs increases significantly. Notably, dense prediction tasks, such as semantic segmentation or object detection, emphasize more on the contours or shapes of objects, while the texture inside objects is less informative. Motivated by this observation, we propose to apply adaptive resolution for different regions in the image according to their importance. Specifically, at the intermediate layer of the ViT, we utilize a spatial-aware density-based clustering algorithm to select representative tokens from the token sequence. Once the representative tokens are determined, we proceed to merge other tokens into their closest representative token. Consequently, semantic similar tokens are merged together to form low-resoluti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#21487;&#22797;&#21046;&#23454;&#39564;&#35774;&#35745;&#30340;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#22797;&#21046;&#27425;&#25968;&#26469;&#24179;&#34913;&#35780;&#20272;&#26356;&#22810;&#21807;&#19968;&#26465;&#20214;&#19982;&#22686;&#21152;&#27599;&#20010;&#26465;&#20214;&#30340;&#22797;&#21046;&#27425;&#25968;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#32771;&#34385;&#21040;&#20102;&#20174;&#19994;&#20154;&#21592;&#30340;&#39118;&#38505;&#35268;&#36991;&#20542;&#21521;&#12290;</title><link>http://arxiv.org/abs/2311.01195</link><description>&lt;p&gt;
Batch Bayesian Optimization for Replicable Experimental Design
&lt;/p&gt;
&lt;p&gt;
Batch Bayesian Optimization for Replicable Experimental Design. (arXiv:2311.01195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01195
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#21487;&#22797;&#21046;&#23454;&#39564;&#35774;&#35745;&#30340;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#22797;&#21046;&#27425;&#25968;&#26469;&#24179;&#34913;&#35780;&#20272;&#26356;&#22810;&#21807;&#19968;&#26465;&#20214;&#19982;&#22686;&#21152;&#27599;&#20010;&#26465;&#20214;&#30340;&#22797;&#21046;&#27425;&#25968;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#32771;&#34385;&#21040;&#20102;&#20174;&#19994;&#20154;&#21592;&#30340;&#39118;&#38505;&#35268;&#36991;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#22312;&#24182;&#34892;&#35780;&#20272;&#22810;&#20010;&#23454;&#39564;&#26465;&#20214;&#30340;&#21516;&#26102;&#65292;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#21644;&#24322;&#26041;&#24046;&#30340;&#35266;&#27979;&#22122;&#22768;&#65292;&#36824;&#20250;&#23545;&#27599;&#20010;&#26465;&#20214;&#36827;&#34892;&#22810;&#27425;&#22797;&#21046;&#12290;&#22312;&#32473;&#23450;&#22266;&#23450;&#24635;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#33258;&#28982;&#22320;&#24341;&#21457;&#20102;&#22312;&#35780;&#20272;&#26356;&#22810;&#21807;&#19968;&#26465;&#20214;&#30340;&#21516;&#26102;&#20943;&#23569;&#27599;&#20010;&#26465;&#20214;&#30340;&#22797;&#21046;&#27425;&#25968;&#19982;&#35780;&#20272;&#36739;&#23569;&#30340;&#21807;&#19968;&#26465;&#20214;&#24182;&#22686;&#21152;&#27599;&#20010;&#26465;&#20214;&#30340;&#22797;&#21046;&#27425;&#25968;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#20174;&#19994;&#20154;&#21592;&#21487;&#33021;&#20855;&#26377;&#39118;&#38505;&#35268;&#36991;&#20542;&#21521;&#65292;&#22240;&#27492;&#26356;&#21152;&#20559;&#22909;&#26082;&#20855;&#26377;&#33391;&#22909;&#24179;&#22343;&#24615;&#33021;&#21448;&#20855;&#26377;&#36739;&#23567;&#21464;&#24322;&#24615;&#30340;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#21487;&#22797;&#21046;&#23454;&#39564;&#35774;&#35745;&#30340;&#25209;&#37327; Thompson Sampling (BTS-RED) &#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#20010;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340; BTS-RED-Known &#21644; BTS-RED-Unknown &#31639;&#27861;&#20998;&#21035;&#29992;&#20110;&#24050;&#30693;&#21644;&#26410;&#30693;&#22122;&#22768;&#26041;&#24046;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#22797;&#21046;&#27425;&#25968;&#32780;&#19981;&#26159;&#30830;&#23450;&#24615;&#22320;&#20026;&#20855;&#26377;&#36739;&#22823;&#22122;&#22768;&#26041;&#24046;&#30340;&#36755;&#20837;&#36827;&#34892;&#26356;&#22810;&#27425;&#22797;&#21046;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#36755;&#20837;&#30340;&#22122;&#22768;&#26041;&#24046;&#36739;&#22823;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20173;&#33021;&#23454;&#29616;&#22797;&#21046;&#26356;&#22810;&#27425;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world experimental design problems (a) evaluate multiple experimental conditions in parallel and (b) replicate each condition multiple times due to large and heteroscedastic observation noise. Given a fixed total budget, this naturally induces a trade-off between evaluating more unique conditions while replicating each of them fewer times vs. evaluating fewer unique conditions and replicating each more times. Moreover, in these problems, practitioners may be risk-averse and hence prefer an input with both good average performance and small variability. To tackle both challenges, we propose the Batch Thompson Sampling for Replicable Experimental Design (BTS-RED) framework, which encompasses three algorithms. Our BTS-RED-Known and BTS-RED-Unknown algorithms, for, respectively, known and unknown noise variance, choose the number of replications adaptively rather than deterministically such that an input with a larger noise variance is replicated more times. As a result, despite 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#38754;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#23545;&#19978;&#19979;&#25991;&#32622;&#20449;&#24230;&#30340;&#25361;&#25112;&#26102;&#65292;&#37319;&#21462;&#30340;&#20004;&#31867;&#31574;&#30053;&#65306;&#36943;&#21046;&#31574;&#30053;&#21644;&#21160;&#21592;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2311.01193</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#32622;&#20449;&#24230;&#21644;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Contextual Confidence and Generative AI. (arXiv:2311.01193v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#38754;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#23545;&#19978;&#19979;&#25991;&#32622;&#20449;&#24230;&#30340;&#25361;&#25112;&#26102;&#65292;&#37319;&#21462;&#30340;&#20004;&#31867;&#31574;&#30053;&#65306;&#36943;&#21046;&#31574;&#30053;&#21644;&#21160;&#21592;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25200;&#20081;&#20102;&#26377;&#25928;&#20154;&#38469;&#27807;&#36890;&#30340;&#22522;&#30784;&#65292;&#32473;&#19978;&#19979;&#25991;&#32622;&#20449;&#24230;&#24102;&#26469;&#26032;&#30340;&#25361;&#25112;&#65292;&#20351;&#21442;&#19982;&#32773;&#38590;&#20197;&#30830;&#23450;&#27807;&#36890;&#30340;&#30495;&#23454;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#20445;&#25252;&#27807;&#36890;&#19981;&#34987;&#22312;&#24847;&#22270;&#20043;&#22806;&#30340;&#29615;&#22659;&#20013;&#37325;&#22797;&#20351;&#29992;&#21644;&#37325;&#32452;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#26088;&#22312;&#22312;&#38754;&#23545;&#36825;&#20123;&#25361;&#25112;&#26102;&#31283;&#23450;&#27807;&#36890;&#30340;&#31574;&#30053;-&#24037;&#20855;&#12289;&#25216;&#26415;&#21644;&#25919;&#31574;&#12290;&#25105;&#20204;&#35752;&#35770;&#30340;&#31574;&#30053;&#21487;&#20998;&#20026;&#20004;&#20010;&#22823;&#31867;&#12290;&#36943;&#21046;&#31574;&#30053;&#26088;&#22312;&#22312;&#24403;&#21069;&#34987;&#23041;&#32961;&#19978;&#19979;&#25991;&#33258;&#30001;&#30340;&#26399;&#26395;&#21644;&#35268;&#33539;&#30340;&#29615;&#22659;&#20013;&#37325;&#26032;&#30830;&#23450;&#19978;&#19979;&#25991;-&#36825;&#26159;&#23545;&#20114;&#32852;&#32593;&#25152;&#24314;&#31435;&#30340;&#26080;&#19978;&#19979;&#25991;&#26399;&#26395;&#21644;&#35268;&#33539;&#30340;&#19968;&#31181;&#21453;&#24212;&#12290;&#30456;&#21453;&#65292;&#21160;&#21592;&#31574;&#30053;&#23558;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#23835;&#36215;&#35270;&#20026;&#22312;&#23186;&#20307;&#27807;&#36890;&#20013;&#20027;&#21160;&#24314;&#31435;&#38544;&#31169;&#21644;&#30495;&#23454;&#24615;&#26032;&#26399;&#26395;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models perturb the foundations of effective human communication. They present new challenges to contextual confidence, disrupting participants' ability to identify the authentic context of communication and their ability to protect communication from reuse and recombination outside its intended context. In this paper, we describe strategies--tools, technologies and policies--that aim to stabilize communication in the face of these challenges. The strategies we discuss fall into two broad categories. Containment strategies aim to reassert context in environments where it is currently threatened--a reaction to the context-free expectations and norms established by the internet. Mobilization strategies, by contrast, view the rise of generative AI as an opportunity to proactively set new and higher expectations around privacy and authenticity in mediated communication.
&lt;/p&gt;</description></item><item><title>VIGraph&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#26469;&#35299;&#20915;&#22270;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23402;&#29983;&#23545;&#27604;&#31574;&#30053;&#25552;&#39640;&#29983;&#25104;&#33410;&#28857;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01191</link><description>&lt;p&gt;
VIGraph&#65306;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification. (arXiv:2311.01191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01191
&lt;/p&gt;
&lt;p&gt;
VIGraph&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#26469;&#35299;&#20915;&#22270;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23402;&#29983;&#23545;&#27604;&#31574;&#30053;&#25552;&#39640;&#29983;&#25104;&#33410;&#28857;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#20026;&#33410;&#28857;&#20998;&#31867;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;SMOTE&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#24179;&#34913;&#22330;&#26223;&#26500;&#24314;&#36807;&#31243;&#20013;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#21512;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#20854;&#28508;&#21147;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;SMOTE&#30340;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;VIGraph&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;VGAE&#65289;&#30340;&#26032;&#22411;SSL&#27169;&#22411;&#65292;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;VIGraph&#22312;&#26500;&#24314;&#19981;&#24179;&#34913;&#22270;&#26102;&#20005;&#26684;&#36981;&#24490;&#19981;&#24179;&#34913;&#30340;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#22411;VGAE&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#12290;&#27492;&#22806;&#65292;VIGraph&#22312;&#35299;&#30721;&#38454;&#27573;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23402;&#29983;&#23545;&#27604;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#33410;&#28857;&#30340;&#25972;&#20307;&#36136;&#37327;&#12290;VIGraph&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33410;&#28857;&#65292;&#26080;&#38656;&#37325;&#26032;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance in graph data poses significant challenges for node classification. Existing methods, represented by SMOTE-based approaches, partially alleviate this issue but still exhibit limitations during imbalanced scenario construction. Self-supervised learning (SSL) offers a promising solution by synthesizing minority nodes from the data itself, yet its potential remains unexplored. In this paper, we analyze the limitations of SMOTE-based approaches and introduce VIGraph, a novel SSL model based on the self-supervised Variational Graph Auto-Encoder (VGAE) that leverages Variational Inference (VI) to generate minority nodes. Specifically, VIGraph strictly adheres to the concept of imbalance when constructing imbalanced graphs and utilizes the generative VGAE to generate minority nodes. Moreover, VIGraph introduces a novel Siamese contrastive strategy at the decoding phase to improve the overall quality of generated nodes. VIGraph can generate high-quality nodes without reintegrat
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38654;&#35745;&#31639;&#21644;&#25913;&#36827;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21307;&#30103;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#30103;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01185</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#30123;&#24773;&#30340;&#38654;&#20113;&#35745;&#31639;&#26550;&#26500;&#20013;&#38761;&#26032;&#21307;&#30103;&#24433;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Healthcare Image Analysis in Pandemic-Based Fog-Cloud Computing Architectures. (arXiv:2311.01185v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01185
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38654;&#35745;&#31639;&#21644;&#25913;&#36827;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21307;&#30103;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#30103;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31361;&#21457;&#30123;&#24773;&#30340;&#20986;&#29616;&#26497;&#22823;&#22320;&#24378;&#35843;&#20102;&#22312;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#20013;&#23547;&#27714;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#29305;&#21035;&#30340;&#25361;&#25112;&#26159;&#23545;&#21307;&#23398;&#24433;&#20687;&#22914;X&#20809;&#21644;CT&#25195;&#25551;&#36827;&#34892;&#25163;&#21160;&#26816;&#26597;&#12290;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#19988;&#28041;&#21450;&#23558;&#36825;&#20123;&#24433;&#20687;&#20256;&#36755;&#21040;&#38598;&#20013;&#30340;&#20113;&#35745;&#31639;&#26381;&#21153;&#22120;&#30340;&#29289;&#27969;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#22270;&#20687;&#20998;&#26512;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#23545;&#20110;&#39640;&#25928;&#30340;&#21307;&#30103;&#24433;&#20687;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#21307;&#30103;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#20998;&#26512;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#22320;&#65292;&#25552;&#20986;&#30340;&#26550;&#26500;&#21033;&#29992;&#20102;&#38654;&#35745;&#31639;&#65292;&#24182;&#21576;&#29616;&#20102;&#19968;&#20010;&#19987;&#20026;&#22270;&#20687;&#20998;&#26512;&#35774;&#35745;&#30340;&#25913;&#36827;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#19981;&#21516;&#30340;CNN&#23618;&#26550;&#26500;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#25506;&#32034;&#21644;&#35780;&#20272;&#65292;&#20197;&#20248;&#21270;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of pandemics has significantly emphasized the need for effective solutions in healthcare data analysis. One particular challenge in this domain is the manual examination of medical images, such as X-rays and CT scans. This process is time-consuming and involves the logistical complexities of transferring these images to centralized cloud computing servers. Additionally, the speed and accuracy of image analysis are vital for efficient healthcare image management. This research paper introduces an innovative healthcare architecture that tackles the challenges of analysis efficiency and accuracy by harnessing the capabilities of Artificial Intelligence (AI). Specifically, the proposed architecture utilizes fog computing and presents a modified Convolutional Neural Network (CNN) designed specifically for image analysis. Different architectures of CNN layers are thoroughly explored and evaluated to optimize overall performance. To demonstrate the effectiveness of the proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#36755;&#20837;&#33539;&#24335;GeneInput&#65292;&#32467;&#21512;&#25552;&#31034;&#21644;&#29992;&#25143;&#21453;&#39304;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#36755;&#20837;&#22788;&#29702;&#65292;&#22312;&#20013;&#25991;&#36755;&#20837;&#27861;&#24341;&#25806;&#30340;&#26500;&#24314;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01166</link><description>&lt;p&gt;
&#29983;&#25104;&#36755;&#20837;&#65306;&#36808;&#21521;&#19979;&#19968;&#20195;&#36755;&#20837;&#26041;&#27861;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Generative Input: Towards Next-Generation Input Methods Paradigm. (arXiv:2311.01166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#36755;&#20837;&#33539;&#24335;GeneInput&#65292;&#32467;&#21512;&#25552;&#31034;&#21644;&#29992;&#25143;&#21453;&#39304;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#36755;&#20837;&#22788;&#29702;&#65292;&#22312;&#20013;&#25991;&#36755;&#20837;&#27861;&#24341;&#25806;&#30340;&#26500;&#24314;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;ChatGPT&#21457;&#24067;&#20197;&#26469;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24182;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#36755;&#20837;&#27861;&#39046;&#22495;&#20013;&#65292;&#20854;&#24212;&#29992;&#20173;&#28982;&#19981;&#22815;&#28145;&#20837;&#12290;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#24050;&#34987;&#24212;&#29992;&#20110;&#20013;&#25991;&#36755;&#20837;&#27861;&#24341;&#25806;&#30340;&#26500;&#24314;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24120;&#24120;&#20551;&#35774;&#36755;&#20837;&#30340;&#25340;&#38899;&#26159;&#27491;&#30830;&#30340;&#65292;&#24182;&#20851;&#27880;&#25340;&#38899;&#21040;&#23383;&#31526;&#30340;&#36716;&#25442;&#20219;&#21153;&#65292;&#36825;&#22312;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#26041;&#38754;&#26174;&#28982;&#19981;&#36275;&#22815;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#26080;&#27861;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#26469;&#20248;&#21270;&#27169;&#22411;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeneInput&#30340;&#20840;&#26032;&#29983;&#25104;&#36755;&#20837;&#33539;&#24335;&#12290;&#23427;&#20351;&#29992;&#25552;&#31034;&#22788;&#29702;&#25152;&#26377;&#36755;&#20837;&#22330;&#26223;&#21644;&#20854;&#20182;&#26234;&#33021;&#36741;&#21161;&#36755;&#20837;&#21151;&#33021;&#65292;&#36890;&#36807;&#29992;&#25143;&#21453;&#39304;&#20248;&#21270;&#27169;&#22411;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#20840;&#27169;&#24335;&#25353;&#38190;&#24207;&#21015;&#21040;&#23383;&#31526;&#30340;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the release of ChatGPT, generative models have achieved tremendous success and become the de facto approach for various NLP tasks. However, its application in the field of input methods remains under-explored. Many neural network approaches have been applied to the construction of Chinese input method engines(IMEs).Previous research often assumed that the input pinyin was correct and focused on Pinyin-to-character(P2C) task, which significantly falls short of meeting users' demands. Moreover, previous research could not leverage user feedback to optimize the model and provide personalized results. In this study, we propose a novel Generative Input paradigm named GeneInput. It uses prompts to handle all input scenarios and other intelligent auxiliary input functions, optimizing the model with user feedback to deliver personalized results. The results demonstrate that we have achieved state-of-the-art performance for the first time in the Full-mode Key-sequence to Characters(FK2C) 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31243;&#24207;&#25191;&#34892;&#32467;&#26524;&#30340;&#39046;&#22495;&#26080;&#20851;&#31579;&#36873;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;&#35821;&#20041;&#35299;&#26512;&#20013;&#30340;&#34394;&#20551;&#31243;&#24207;&#38382;&#39064;</title><link>http://arxiv.org/abs/2311.01161</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#19979;&#22522;&#20110;&#25191;&#34892;&#30340;&#34394;&#20551;&#31243;&#24207;&#36807;&#28388;&#30340;&#35821;&#20041;&#35299;&#26512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Semantic Parsing with Execution-based Spurious Program Filtering. (arXiv:2311.01161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31243;&#24207;&#25191;&#34892;&#32467;&#26524;&#30340;&#39046;&#22495;&#26080;&#20851;&#31579;&#36873;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;&#35821;&#20041;&#35299;&#26512;&#20013;&#30340;&#34394;&#20551;&#31243;&#24207;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24369;&#30417;&#30563;&#19979;&#35757;&#32451;&#35821;&#20041;&#35299;&#26512;&#22120;&#26102;&#65292;&#34394;&#20551;&#31243;&#24207;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#28040;&#38500;&#20855;&#26377;&#38169;&#35823;&#35821;&#20041;&#20294;&#27491;&#30830;&#25351;&#31034;&#30340;&#31243;&#24207;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#30528;&#37325;&#20110;&#21033;&#29992;&#22522;&#20110;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20363;&#23376;&#30456;&#20284;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31243;&#24207;&#25191;&#34892;&#32467;&#26524;&#30340;&#39046;&#22495;&#26080;&#20851;&#31579;&#36873;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#36890;&#36807;&#25628;&#32034;&#36807;&#31243;&#33719;&#24471;&#30340;&#27599;&#20010;&#31243;&#24207;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#34920;&#31034;&#65292;&#23427;&#20197;&#21508;&#31181;&#36755;&#20837;&#19979;&#30340;&#25191;&#34892;&#32467;&#26524;&#25429;&#25417;&#31243;&#24207;&#30340;&#35821;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#34920;&#31034;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#65292;&#20197;&#35782;&#21035;&#24182;&#36807;&#28388;&#25481;&#19982;&#20854;&#20182;&#31243;&#24207;&#20855;&#26377;&#26126;&#26174;&#19981;&#21516;&#35821;&#20041;&#30340;&#31243;&#24207;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#31243;&#24207;&#25628;&#32034;&#36807;&#31243;&#27491;&#20132;&#65292;&#22240;&#27492;&#21487;&#20197;&#36731;&#26494;&#22686;&#24378;&#20219;&#20309;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#35299;&#26512;&#26694;&#26550;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#21644;WikiTableQuestions&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
The problem of spurious programs is a longstanding challenge when training a semantic parser from weak supervision. To eliminate such programs that have wrong semantics but correct denotation, existing methods focus on exploiting similarities between examples based on domain-specific knowledge. In this paper, we propose a domain-agnostic filtering mechanism based on program execution results. Specifically, for each program obtained through the search process, we first construct a representation that captures the program's semantics as execution results under various inputs. Then, we run a majority vote on these representations to identify and filter out programs with significantly different semantics from the other programs. In particular, our method is orthogonal to the program search process so that it can easily augment any of the existing weakly supervised semantic parsing frameworks. Empirical evaluations on the Natural Language Visual Reasoning and WikiTableQuestions demonstrate 
&lt;/p&gt;</description></item><item><title>&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#19982;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#20043;&#38388;&#30340;&#24378;&#20114;&#21160;&#21487;&#20197;&#25913;&#21892;&#25968;&#23383;&#24179;&#21488;&#30340;&#32593;&#32476;&#23433;&#20840;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#20449;&#24687;&#21644;&#23433;&#20840;&#26631;&#20934;&#65292;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#33021;&#22815;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#23545;&#25972;&#20010;&#38598;&#21512;&#36896;&#25104;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2311.01154</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#19982;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Digital Twins and their Application in Cybersecurity based on Artificial Intelligence. (arXiv:2311.01154v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01154
&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#19982;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#20043;&#38388;&#30340;&#24378;&#20114;&#21160;&#21487;&#20197;&#25913;&#21892;&#25968;&#23383;&#24179;&#21488;&#30340;&#32593;&#32476;&#23433;&#20840;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#20449;&#24687;&#21644;&#23433;&#20840;&#26631;&#20934;&#65292;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#33021;&#22815;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#23545;&#25972;&#20010;&#38598;&#21512;&#36896;&#25104;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#30340;&#28508;&#21147;&#23578;&#26410;&#23436;&#20840;&#21457;&#25381;&#65292;&#30001;&#20110;&#20854;&#22810;&#26679;&#24615;&#21644;&#26410;&#24320;&#21457;&#30340;&#28508;&#21147;&#12290;&#25968;&#23383;&#23402;&#29983;&#20351;&#24471;&#31995;&#32479;&#30340;&#20998;&#26512;&#12289;&#35774;&#35745;&#12289;&#20248;&#21270;&#21644;&#28436;&#21270;&#33021;&#22815;&#36890;&#36807;&#25968;&#23383;&#26041;&#24335;&#25110;&#19982;&#21327;&#21516;&#30340;&#32593;&#32476;-&#29289;&#29702;&#26041;&#27861;&#30456;&#32467;&#21512;&#36827;&#34892;&#65292;&#20197;&#25552;&#39640;&#20256;&#32479;&#24037;&#31243;&#26041;&#27861;&#30340;&#36895;&#24230;&#12289;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#24037;&#19994;4.0&#12289;&#26410;&#26469;&#24037;&#21378;&#21644;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#32487;&#32493;&#20174;&#35813;&#25216;&#26415;&#20013;&#21463;&#30410;&#65292;&#24182;&#22312;&#29616;&#26377;&#31995;&#32479;&#20013;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#25928;&#29575;&#12290;&#30001;&#20110;&#32570;&#20047;&#19982;&#32593;&#32476;&#25968;&#23383;&#21270;&#36807;&#28193;&#30456;&#20851;&#30340;&#20449;&#24687;&#21644;&#23433;&#20840;&#26631;&#20934;&#65292;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#33021;&#22815;&#21033;&#29992;&#36825;&#31181;&#24773;&#20917;&#12290;&#35775;&#38382;&#20135;&#21697;&#25110;&#26381;&#21153;&#30340;&#25968;&#23383;&#23402;&#29983;&#31561;&#21516;&#20110;&#23041;&#32961;&#25972;&#20010;&#38598;&#21512;&#12290;&#25968;&#23383;&#23402;&#29983;&#19982;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#20043;&#38388;&#23384;&#22312;&#30528;&#24378;&#26377;&#21147;&#30340;&#20114;&#21160;&#65292;&#36825;&#23548;&#33268;&#36825;&#20123;&#25216;&#26415;&#20043;&#38388;&#23384;&#22312;&#30528;&#32039;&#23494;&#30340;&#20114;&#21160;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#26469;&#25913;&#21892;&#36825;&#20123;&#25968;&#23383;&#24179;&#21488;&#30340;&#32593;&#32476;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of digital twin technology is yet to be fully realized due to its diversity and untapped potential. Digital twins enable systems' analysis, design, optimization, and evolution to be performed digitally or in conjunction with a cyber-physical approach to improve speed, accuracy, and efficiency over traditional engineering methods. Industry 4.0, factories of the future, and digital twins continue to benefit from the technology and provide enhanced efficiency within existing systems. Due to the lack of information and security standards associated with the transition to cyber digitization, cybercriminals have been able to take advantage of the situation. Access to a digital twin of a product or service is equivalent to threatening the entire collection. There is a robust interaction between digital twins and artificial intelligence tools, which leads to strong interaction between these technologies, so it can be used to improve the cybersecurity of these digital platforms ba
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#65292;&#21457;&#29616;&#23558;&#26410;&#23545;&#40784;&#30340;&#38543;&#26426;&#30693;&#35782;&#27880;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20197;&#21462;&#24471;&#19982;&#23545;&#40784;&#30693;&#35782;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#27491;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01150</link><description>&lt;p&gt;
&#37325;&#35775;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Knowledge Injection Frameworks. (arXiv:2311.01150v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01150
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#65292;&#21457;&#29616;&#23558;&#26410;&#23545;&#40784;&#30340;&#38543;&#26426;&#30693;&#35782;&#27880;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20197;&#21462;&#24471;&#19982;&#23545;&#40784;&#30693;&#35782;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#27491;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT&#65292;&#24050;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#20135;&#29983;&#20102;&#24040;&#22823;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#20351;&#36825;&#20123;LLMs&#26356;&#36866;&#24212;&#22402;&#30452;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#38382;&#39064;&#23578;&#26410;&#23436;&#20840;&#35299;&#20915;&#12290;&#23454;&#38469;&#19978;&#65292;&#22312;&#36825;&#26041;&#38754;&#24050;&#32463;&#20986;&#29616;&#20102;&#19968;&#20123;&#24037;&#20316;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#26500;&#24314;&#23545;&#40784;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#36890;&#36807;&#23558;&#30456;&#24212;&#30340;&#30693;&#35782;&#20803;&#32452;&#27880;&#20837;&#21040;&#30456;&#20851;&#30340;&#25991;&#26412;&#26679;&#26412;&#20013;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#24076;&#26395;&#65292;&#20294;&#25105;&#20204;&#26222;&#36941;&#21457;&#29616;&#36825;&#39033;&#24037;&#20316;&#20013;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#21457;&#29616;&#23558;&#26410;&#23545;&#40784;&#65288;&#21363;&#38543;&#26426;&#65289;&#30340;&#30693;&#35782;&#20803;&#32452;&#27880;&#20837;&#21040;LLMs&#20013;&#65292;&#21487;&#20197;&#21462;&#24471;&#19982;&#27880;&#20837;&#23545;&#40784;&#30693;&#35782;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#36825;&#19968;&#20196;&#20154;&#27822;&#20007;&#30340;&#21457;&#29616;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#35843;&#26597;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#21487;&#33021;&#30340;&#35299;&#37322;&#12290;&#22522;&#20110;&#36825;&#19968;&#20999;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#27491;&#25216;&#26415;&#12290;&#31616;&#35201;&#22320;&#35828;&#65292;&#36825;&#31181;&#25216;&#26415;&#30340;&#26680;&#24515;&#26681;&#26893;&#20110;...
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs), such as GPTs, have attained great impact worldwide. However, how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved. Indeed, there have emerged a few works on this line where most of them rely on an alignment heuristic that is built to inject the corresponding knowledge tuple into the associated text sample.  However, despite the promise, we identify a pivotal problem in this work ubiquitously. Simply put, we find that injecting unaligned (i.e., random) knowledge tuple into the LLMs achieves comparable (and sometimes better) results than the aligned knowledge being injected. We therefore take a thorough investigation of this frustrating finding on a variety of related prior work and further provide a chain of potential interpretations for the phenomenon. Based on all that, we offer a simple remediated technique. Briefly, the core of this technique is rooted in
&lt;/p&gt;</description></item><item><title>GREEMA&#26159;&#19968;&#31181;&#36890;&#36807;&#21507;&#29615;&#22659;&#26448;&#26009;&#26469;&#22766;&#22823;&#30340;&#21019;&#26032;&#26426;&#22120;&#20154;&#65292;&#22312;&#28369;&#22369;&#31561;&#26080;&#27861;&#36827;&#20837;&#30340;&#22320;&#21306;&#33021;&#22815;&#21462;&#20195;&#20154;&#21147;&#24037;&#20316;&#65292;&#20943;&#23569;&#36816;&#36755;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2311.01107</link><description>&lt;p&gt;
GREEMA:&#36890;&#36807;&#21507;&#29615;&#22659;&#26448;&#26009;&#26469;&#22766;&#22823;&#30340;&#26426;&#22120;&#20154;&#30340;&#25552;&#26696;&#21644;&#23454;&#39564;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
GREEMA: Proposal and Experimental Verification of Growing Robot by Eating Environmental MAterial for Landslide Disaster. (arXiv:2311.01107v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01107
&lt;/p&gt;
&lt;p&gt;
GREEMA&#26159;&#19968;&#31181;&#36890;&#36807;&#21507;&#29615;&#22659;&#26448;&#26009;&#26469;&#22766;&#22823;&#30340;&#21019;&#26032;&#26426;&#22120;&#20154;&#65292;&#22312;&#28369;&#22369;&#31561;&#26080;&#27861;&#36827;&#20837;&#30340;&#22320;&#21306;&#33021;&#22815;&#21462;&#20195;&#20154;&#21147;&#24037;&#20316;&#65292;&#20943;&#23569;&#36816;&#36755;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#20154;&#31867;&#19981;&#21487;&#36827;&#20837;&#30340;&#22320;&#21306;&#65292;&#22914;&#26376;&#29699;&#34920;&#38754;&#21644;&#28369;&#22369;&#29616;&#22330;&#65292;&#38656;&#35201;&#22810;&#20010;&#21487;&#33258;&#20027;&#31227;&#21160;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#26469;&#21462;&#20195;&#20154;&#21147;&#24037;&#20316;&#12290;&#29305;&#21035;&#26159;&#22312;&#27827;&#36947;&#22581;&#22622;&#31561;&#28369;&#22369;&#29616;&#22330;&#65292;&#38656;&#35201;&#26426;&#22120;&#20154;&#23613;&#24555;&#28165;&#38500;&#27700;&#21644;&#27785;&#31215;&#29289;&#12290;&#20256;&#32479;&#19978;&#65292;&#20960;&#21488;&#24314;&#31569;&#26426;&#26800;&#34987;&#37096;&#32626;&#21040;&#29616;&#22330;&#36827;&#34892;&#22303;&#26408;&#24037;&#31243;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20256;&#32479;&#24314;&#31569;&#35774;&#22791;&#30340;&#20307;&#31215;&#21644;&#37325;&#37327;&#36739;&#22823;&#65292;&#24456;&#38590;&#23558;&#22810;&#21488;&#24314;&#31569;&#35774;&#22791;&#31227;&#21160;&#21040;&#29616;&#22330;&#65292;&#23548;&#33268;&#36739;&#22823;&#30340;&#36816;&#36755;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GREEMA&#30340;&#36890;&#36807;&#21507;&#29615;&#22659;&#26448;&#26009;&#26469;&#22766;&#22823;&#30340;&#21019;&#26032;&#26426;&#22120;&#20154;&#65292;&#20854;&#22312;&#36816;&#36755;&#36807;&#31243;&#20013;&#37325;&#37327;&#36731;&#24039;&#32039;&#20945;&#65292;&#19968;&#26086;&#21040;&#36798;&#29616;&#22330;&#23601;&#33021;&#36890;&#36807;&#21507;&#29615;&#22659;&#26448;&#26009;&#36827;&#34892;&#21151;&#33021;&#36816;&#20316;&#12290;GREEMA&#20027;&#21160;&#21560;&#25910;&#27700;&#21644;&#27785;&#31215;&#29289;&#31561;&#29615;&#22659;&#26448;&#26009;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;
&lt;/p&gt;
&lt;p&gt;
In areas that are inaccessible to humans, such as the lunar surface and landslide sites, there is a need for multiple autonomous mobile robot systems that can replace human workers. In particular, at landslide sites such as river channel blockages, robots are required to remove water and sediment from the site as soon as possible. Conventionally, several construction machines have been deployed to the site for civil engineering work. However, because of the large size and weight of conventional construction equipment, it is difficult to move multiple units of construction equipment to the site, resulting in significant transportation costs and time. To solve such problems, this study proposes a novel growing robot by eating environmental material called GREEMA, which is lightweight and compact during transportation, but can function by eating on environmental materials once it arrives at the site. GREEMA actively takes in environmental materials such as water and sediment, uses them as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26234;&#33021;&#30524;&#38236;&#19978;&#23454;&#29616;&#36229;&#39640;&#25928;&#35774;&#22791;&#20869;&#30446;&#26631;&#26816;&#27979;&#30340;&#35774;&#35745;&#21644;&#23454;&#26045;&#65292;&#21033;&#29992;&#26032;&#22411;&#20302;&#21151;&#32791;&#22788;&#29702;&#22120;&#23454;&#29616;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#20415;&#22312;&#20855;&#26377;&#23567;&#23610;&#23544;&#21644;&#26377;&#38480;&#30005;&#27744;&#23481;&#37327;&#30340;&#26234;&#33021;&#30524;&#38236;&#19978;&#23454;&#29616;&#38271;&#26102;&#38388;&#36830;&#32493;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2311.01057</link><description>&lt;p&gt;
&#24102;&#26377;TinyissimoYOLO&#30340;AI&#38598;&#25104;&#26234;&#33021;&#30524;&#38236;&#19978;&#30340;&#36229;&#39640;&#25928;&#35774;&#22791;&#20869;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ultra-Efficient On-Device Object Detection on AI-Integrated Smart Glasses with TinyissimoYOLO. (arXiv:2311.01057v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26234;&#33021;&#30524;&#38236;&#19978;&#23454;&#29616;&#36229;&#39640;&#25928;&#35774;&#22791;&#20869;&#30446;&#26631;&#26816;&#27979;&#30340;&#35774;&#35745;&#21644;&#23454;&#26045;&#65292;&#21033;&#29992;&#26032;&#22411;&#20302;&#21151;&#32791;&#22788;&#29702;&#22120;&#23454;&#29616;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#20415;&#22312;&#20855;&#26377;&#23567;&#23610;&#23544;&#21644;&#26377;&#38480;&#30005;&#27744;&#23481;&#37327;&#30340;&#26234;&#33021;&#30524;&#38236;&#19978;&#23454;&#29616;&#38271;&#26102;&#38388;&#36830;&#32493;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30524;&#38236;&#20511;&#21161;&#23574;&#31471;&#35745;&#31639;&#25216;&#26415;&#12289;&#21152;&#36895;&#30828;&#20214;&#26550;&#26500;&#21644;&#23567;&#22411;AI&#31639;&#27861;&#65292;&#27491;&#36805;&#36895;&#33719;&#24471;&#20808;&#36827;&#21151;&#33021;&#12290;&#22312;&#38754;&#21521;&#20840;&#22825;&#20351;&#29992;&#20197;&#23454;&#29616;&#28385;&#24847;&#29992;&#25143;&#20307;&#39564;&#26102;&#65292;&#23558;AI&#38598;&#25104;&#21040;&#20855;&#26377;&#23567;&#23610;&#23544;&#21644;&#26377;&#38480;&#30005;&#27744;&#23481;&#37327;&#30340;&#26234;&#33021;&#30524;&#38236;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#21033;&#29992;&#26032;&#22411;&#20302;&#21151;&#32791;&#22788;&#29702;&#22120;&#23454;&#29616;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#20197;&#22312;&#26234;&#33021;&#30524;&#38236;&#20013;&#23454;&#29616;&#38271;&#26102;&#38388;&#36830;&#32493;&#36816;&#34892;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#26234;&#33021;&#30524;&#38236;&#22312;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#24773;&#20917;&#19979;&#30340;&#33021;&#37327;&#21644;&#26102;&#24310;&#25928;&#29575;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26234;&#33021;&#30524;&#38236;&#21407;&#22411;&#20316;&#20026;&#30740;&#31350;&#24179;&#21488;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#24494;&#25511;&#21046;&#22120;&#65292;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;&#35270;&#35273;AI&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#26032;&#22411;&#27627;&#29926;&#32423;&#21151;&#29575;RISC-V&#24182;&#34892;&#22788;&#29702;&#22120;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#36890;&#20449;&#30340;&#20302;&#21151;&#32791;&#34013;&#29273;&#27169;&#22359;&#12290;&#26234;&#33021;&#30524;&#38236;&#38598;&#25104;&#20102;&#22270;&#20687;&#21644;&#38899;&#39057;&#24863;&#24212;&#25509;&#21475;&#31561;&#30005;&#28304;&#24490;&#29615;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart glasses are rapidly gaining advanced functionality thanks to cutting-edge computing technologies, accelerated hardware architectures, and tiny AI algorithms. Integrating AI into smart glasses featuring a small form factor and limited battery capacity is still challenging when targeting full-day usage for a satisfactory user experience. This paper illustrates the design and implementation of tiny machine-learning algorithms exploiting novel low-power processors to enable prolonged continuous operation in smart glasses. We explore the energy- and latency-efficient of smart glasses in the case of real-time object detection. To this goal, we designed a smart glasses prototype as a research platform featuring two microcontrollers, including a novel milliwatt-power RISC-V parallel processor with a hardware accelerator for visual AI, and a Bluetooth low-power module for communication. The smart glasses integrate power cycling mechanisms, including image and audio sensing interfaces. Fur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#32500;&#25968;&#25454;&#31934;&#21270;&#31574;&#30053;&#65292;&#21253;&#25324;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#29983;&#25104;&#22411;AI&#24037;&#20855;&#24320;&#21457;&#25968;&#25454;&#29228;&#21462;&#33050;&#26412;&#65292;&#29992;&#20110;&#35843;&#20248;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35813;&#31574;&#30053;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#20174;&#25552;&#31034;&#29983;&#25104;&#36234;&#21335;&#26032;&#38395;&#25991;&#31456;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01049</link><description>&lt;p&gt;
&#29992;&#20110;&#26377;&#25928;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#32500;&#25968;&#25454;&#31934;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Multi-dimensional data refining strategy for effective fine-tuning LLMs. (arXiv:2311.01049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#32500;&#25968;&#25454;&#31934;&#21270;&#31574;&#30053;&#65292;&#21253;&#25324;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#29983;&#25104;&#22411;AI&#24037;&#20855;&#24320;&#21457;&#25968;&#25454;&#29228;&#21462;&#33050;&#26412;&#65292;&#29992;&#20110;&#35843;&#20248;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35813;&#31574;&#30053;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#20174;&#25552;&#31034;&#29983;&#25104;&#36234;&#21335;&#26032;&#38395;&#25991;&#31456;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#35843;&#20248;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#65292;&#20294;&#33719;&#21462;&#21512;&#36866;&#30340;&#25968;&#25454;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#25968;&#25454;&#31232;&#32570;&#12289;&#35821;&#35328;&#22810;&#26679;&#24615;&#21644;&#29305;&#23450;&#39046;&#22495;&#20869;&#23481;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#29228;&#21462;&#21644;&#31934;&#21270;&#38024;&#23545;&#35843;&#20248;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#26102;&#25152;&#23398;&#21040;&#30340;&#32463;&#39564;&#12290;&#21046;&#20316;&#36825;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#38656;&#35201;&#32454;&#33268;&#30340;&#35745;&#21010;&#65292;&#32771;&#34385;&#21040;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#22312;&#21253;&#23481;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#31574;&#30053;&#65292;&#21253;&#25324;&#21033;&#29992;&#33521;&#35821;&#20013;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#20511;&#21161;&#29983;&#25104;&#22411;AI&#24037;&#20855;&#24320;&#21457;&#23450;&#21046;&#30340;&#25968;&#25454;&#29228;&#21462;&#33050;&#26412;&#12290;&#20351;&#29992;&#30001;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#20248;&#27169;&#22411;&#65292;&#22312;&#20174;&#25552;&#31034;&#29983;&#25104;&#36234;&#21335;&#26032;&#38395;&#25991;&#31456;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#20026;&#23558;&#26469;&#35843;&#20248;&#36234;&#21335;&#35821;&#31561;&#35821;&#35328;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data is a cornerstone for fine-tuning large language models, yet acquiring suitable data remains challenging. Challenges encompassed data scarcity, linguistic diversity, and domain-specific content. This paper presents lessons learned while crawling and refining data tailored for fine-tuning Vietnamese language models. Crafting such a dataset, while accounting for linguistic intricacies and striking a balance between inclusivity and accuracy, demands meticulous planning. Our paper presents a multidimensional strategy including leveraging existing datasets in the English language and developing customized data-crawling scripts with the assistance of generative AI tools. A fine-tuned LLM model for the Vietnamese language, which was produced using resultant datasets, demonstrated good performance while generating Vietnamese news articles from prompts. The study offers practical solutions and guidance for future fine-tuning models in languages like Vietnamese.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#20316;&#20026;&#19968;&#31181;AI&#25945;&#23398;&#24037;&#20855;&#22312;&#39640;&#31561;&#25945;&#32946;&#30005;&#23376;&#24037;&#31243;&#35838;&#31243;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#25552;&#20379;&#35265;&#35299;&#12289;&#20010;&#24615;&#21270;&#25903;&#25345;&#21644;&#20114;&#21160;&#23398;&#20064;&#20307;&#39564;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;ChatGPT&#20316;&#20026;AI&#24037;&#20855;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#25216;&#26415;&#23398;&#31185;&#20013;&#21019;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#23545;&#25945;&#32946;&#39046;&#22495;&#30340;&#25968;&#23383;&#21270;&#36716;&#22411;&#26377;&#25152;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2311.01048</link><description>&lt;p&gt;
&#39640;&#31561;&#25945;&#32946;&#20013;AI&#36741;&#21161;&#23398;&#20064;&#30005;&#23376;&#24037;&#31243;&#35838;&#31243;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AI-assisted Learning for Electronic Engineering Courses in High Education. (arXiv:2311.01048v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#20316;&#20026;&#19968;&#31181;AI&#25945;&#23398;&#24037;&#20855;&#22312;&#39640;&#31561;&#25945;&#32946;&#30005;&#23376;&#24037;&#31243;&#35838;&#31243;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#25552;&#20379;&#35265;&#35299;&#12289;&#20010;&#24615;&#21270;&#25903;&#25345;&#21644;&#20114;&#21160;&#23398;&#20064;&#20307;&#39564;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;ChatGPT&#20316;&#20026;AI&#24037;&#20855;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#25216;&#26415;&#23398;&#31185;&#20013;&#21019;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#23545;&#25945;&#32946;&#39046;&#22495;&#30340;&#25968;&#23383;&#21270;&#36716;&#22411;&#26377;&#25152;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20122;&#27954;&#26576;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#30340;&#38598;&#25104;&#30005;&#36335;&#31995;&#32479;&#35838;&#31243;&#20013;&#20351;&#29992;ChatGPT&#20316;&#20026;AI&#25945;&#23398;&#21644;&#23398;&#20064;&#25903;&#25345;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;&#23436;&#25104;&#20102;&#21508;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#23545;ChatGPT&#30340;&#22238;&#31572;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#33719;&#24471;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#24037;&#31243;&#25945;&#32946;&#20013;&#25552;&#20379;&#35265;&#35299;&#12289;&#20010;&#24615;&#21270;&#25903;&#25345;&#21644;&#20114;&#21160;&#23398;&#20064;&#20307;&#39564;&#30340;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;&#23545;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#26041;&#65288;&#23398;&#29983;&#12289;&#35762;&#24072;&#21644;&#24037;&#31243;&#24072;&#65289;&#30340;&#35780;&#20272;&#21644;&#21453;&#24605;&#12290;&#26412;&#30740;&#31350;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;ChatGPT&#20316;&#20026;AI&#24037;&#20855;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#20026;&#25216;&#26415;&#23398;&#31185;&#20013;&#21019;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#26377;&#21161;&#20110;&#25105;&#20204;&#23545;&#25968;&#23383;&#21270;&#36716;&#22411;&#22312;&#25945;&#32946;&#39046;&#22495;&#22914;&#20309;&#23637;&#24320;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study evaluates the efficacy of ChatGPT as an AI teaching and learning support tool in an integrated circuit systems course at a higher education institution in an Asian country. Various question types were completed, and ChatGPT responses were assessed to gain valuable insights for further investigation. The objective is to assess ChatGPT's ability to provide insights, personalized support, and interactive learning experiences in engineering education. The study includes the evaluation and reflection of different stakeholders: students, lecturers, and engineers. The findings of this study shed light on the benefits and limitations of ChatGPT as an AI tool, paving the way for innovative learning approaches in technical disciplines. Furthermore, the study contributes to our understanding of how digital transformation is likely to unfold in the education sector.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#21457;&#23637;&#36235;&#21183;&#65292;&#20174;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#36807;&#28193;&#21040;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#24182;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#27169;&#22411;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#33021;&#21147;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2311.01043</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models for Autonomous Driving. (arXiv:2311.01043v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01043
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#21457;&#23637;&#36235;&#21183;&#65292;&#20174;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#36807;&#28193;&#21040;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#24182;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#27169;&#22411;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#33021;&#21147;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#20316;&#20026;&#25913;&#21464;&#20132;&#36890;&#21644;&#22478;&#24066;&#27969;&#21160;&#24615;&#30340;&#20652;&#21270;&#21058;&#65292;&#27491;&#36235;&#21521;&#20110;&#20174;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#36716;&#21521;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#12290;&#20256;&#32479;&#30340;&#27169;&#22359;&#21270;&#31995;&#32479;&#21463;&#21040;&#32423;&#32852;&#27169;&#22359;&#20013;&#30340;&#32047;&#31215;&#35823;&#24046;&#21644;&#19981;&#28789;&#27963;&#30340;&#39044;&#35774;&#35268;&#21017;&#30340;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#36890;&#36807;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#26377;&#28508;&#21147;&#36991;&#20813;&#38169;&#35823;&#32047;&#31215;&#65292;&#23613;&#31649;&#30001;&#20110;&#20854;&#40657;&#30418;&#24615;&#36136;&#65292;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#20351;&#24471;&#20915;&#31574;&#30340;&#39564;&#35777;&#21644;&#21487;&#36861;&#28335;&#24615;&#21464;&#24471;&#22797;&#26434;&#12290;&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#29702;&#35299;&#32972;&#26223;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#31561;&#33021;&#21147;&#12290;&#33258;&#28982;&#32780;&#28982;&#30340;&#24819;&#27861;&#26159;&#21033;&#29992;&#36825;&#20123;&#33021;&#21147;&#36171;&#20104;&#33258;&#21160;&#39550;&#39542;&#20197;&#26356;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;LLM&#19982;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#32467;&#21512;&#65292;&#21487;&#33021;&#25171;&#24320;&#23545;&#24320;&#25918;&#19990;&#30028;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#22823;&#38376;&#65292;&#36825;&#26159;&#24403;&#21069;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#25152;&#32570;&#20047;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving technology, a catalyst for revolutionizing transportation and urban mobility, has the tend to transition from rule-based systems to data-driven strategies. Traditional module-based systems are constrained by cumulative errors among cascaded modules and inflexible pre-set rules. In contrast, end-to-end autonomous driving systems have the potential to avoid error accumulation due to their fully data-driven training process, although they often lack transparency due to their ``black box" nature, complicating the validation and traceability of decisions. Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers. A natural thought is to utilize these abilities to empower autonomous driving. By combining LLM with foundation vision models, it could open the door to open-world understanding, reasoning, and few-shot learning, which current autonomous driving systems are lacking. In this paper,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20250;&#25298;&#32477;&#65288;L2R&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#25298;&#32477;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#35782;&#21035;&#21644;&#25298;&#32477;&#38590;&#20197;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01041</link><description>&lt;p&gt;
&#23398;&#20250;&#25298;&#32477;&#65306;&#36890;&#36807;&#30693;&#35782;&#33539;&#22260;&#38480;&#21046;&#21644;&#25298;&#32477;&#26426;&#21046;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#21487;&#25511;&#21644;&#21487;&#38752;
&lt;/p&gt;
&lt;p&gt;
Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism. (arXiv:2311.01041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20250;&#25298;&#32477;&#65288;L2R&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#25298;&#32477;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#35782;&#21035;&#21644;&#25298;&#32477;&#38590;&#20197;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22238;&#31572;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24182;&#19981;&#23436;&#32654;&#65292;&#32463;&#24120;&#20135;&#29983;&#21547;&#26377;&#38169;&#35823;&#25110;&#38169;&#35823;&#20449;&#24687;&#30340;&#22238;&#31572;&#12290;&#36825;&#20123;&#19981;&#20934;&#30830;&#24615;&#65292;&#36890;&#24120;&#31216;&#20026;&#24187;&#35273;&#65292;&#20351;&#24471;LLMs&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#19981;&#21487;&#38752;&#29978;&#33267;&#19981;&#21487;&#29992;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#22312;LLMs&#20013;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38382;&#31572;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#25298;&#32477;&#26426;&#21046;&#65292;&#25351;&#23548;LLMs&#25298;&#32477;&#22238;&#31572;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20197;&#36991;&#20813;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;Learn to Refuse (L2R)&#65292;&#23427;&#23558;&#25298;&#32477;&#26426;&#21046;&#32435;&#20837;&#21040;LLMs&#20013;&#65292;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#21644;&#25298;&#32477;&#37027;&#20123;&#23427;&#20204;&#38590;&#20197;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#24211;&#26469;&#34920;&#31034;&#25152;&#26377;LLMs&#25152;&#38656;&#35201;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, enabling them to answer a wide range of questions across various domains. However, these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios. In this paper, our focus is on mitigating the issue of hallucination in LLMs, particularly in the context of question-answering. Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors. We then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM
&lt;/p&gt;</description></item><item><title>ATHENA&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24605;&#32500;&#25193;&#23637;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#30340;&#24605;&#32500;&#25193;&#23637;&#26426;&#21046;&#26469;&#35299;&#20915;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#25361;&#25112;&#65292;&#23427;&#33021;&#22815;&#20135;&#29983;&#21512;&#29702;&#30340;&#24605;&#32771;&#36335;&#24452;&#20197;&#35299;&#20915;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01036</link><description>&lt;p&gt;
ATHENA: &#25968;&#23398;&#25512;&#29702;&#19982;&#24605;&#32500;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
ATHENA: Mathematical Reasoning with Thought Expansion. (arXiv:2311.01036v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01036
&lt;/p&gt;
&lt;p&gt;
ATHENA&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24605;&#32500;&#25193;&#23637;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#30340;&#24605;&#32500;&#25193;&#23637;&#26426;&#21046;&#26469;&#35299;&#20915;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#25361;&#25112;&#65292;&#23427;&#33021;&#22815;&#20135;&#29983;&#21512;&#29702;&#30340;&#24605;&#32771;&#36335;&#24452;&#20197;&#35299;&#20915;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#21462;&#20915;&#20110;&#22914;&#20309;&#34920;&#36798;&#38382;&#39064;&#65292;&#20197;&#21450;&#27169;&#22411;&#22914;&#20309;&#30475;&#24453;&#20154;&#31867;&#35821;&#35328;&#34920;&#36798;&#30340;&#35282;&#24230;&#12290;&#23454;&#38469;&#19990;&#30028;&#30340;&#24773;&#22659;&#26356;&#20381;&#36182;&#36825;&#31181;&#26041;&#27861;&#65292;&#22240;&#20026;&#21516;&#26679;&#30340;&#25968;&#23398;&#36816;&#31639;&#26377;&#22810;&#31181;&#23454;&#36341;&#26041;&#24335;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#36807;&#38480;&#21046;&#39044;&#27979;&#31574;&#30053;&#26469;&#38480;&#21046;&#21487;&#29992;&#30340;&#24605;&#32500;&#36807;&#31243;&#65292;&#32780;&#24573;&#30053;&#20102;&#36825;&#20123;&#31574;&#30053;&#22312;&#33719;&#21462;&#25968;&#23398;&#30693;&#35782;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24605;&#32500;&#25193;&#23637;&#32593;&#32476;&#26550;&#26500; (ATHENA) &#26469;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25361;&#25112;&#65292;&#27169;&#20223;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#30340;&#24418;&#24335;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24605;&#32500;&#25193;&#23637;&#26426;&#21046;&#12290;&#24605;&#32500;&#25193;&#23637;&#36890;&#36807;&#36882;&#24402;&#29983;&#25104;&#20505;&#36873;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#20174;&#19978;&#19968;&#27493;&#39537;&#21160;&#24182;&#36873;&#25321;&#36890;&#21521;&#30446;&#26631;&#30340;&#26377;&#25928;&#36335;&#24452;&#65292;&#20135;&#29983;&#21512;&#29702;&#30340;&#24605;&#32500;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ATHENA &#22312;&#21508;&#31181;&#38382;&#39064;&#20013;&#37117;&#21462;&#24471;&#20102;&#20840;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#21363;&#20351;&#22312;&#20449;&#24687;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#32473;&#20986;&#20196;&#20154;&#20449;&#26381;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving math word problems depends on how to articulate the problems, the lens through which models view human linguistic expressions. Real-world settings count on such a method even more due to the diverse practices of the same mathematical operations. Earlier works constrain available thinking processes by limited prediction strategies without considering their significance in acquiring mathematical knowledge. We introduce Attention-based THought Expansion Network Architecture (ATHENA) to tackle the challenges of real-world practices by mimicking human thought expansion mechanisms in the form of neural network propagation. A thought expansion recurrently generates the candidates carrying the thoughts of possible math expressions driven from the previous step and yields reasonable thoughts by selecting the valid pathways to the goal. Our experiments show that ATHENA achieves a new state-of-the-art stage toward the ideal model that is compelling in variant questions even when the infor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#38750;&#33258;&#22238;&#24402;&#36830;&#32493;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#26399;&#20107;&#20214;&#39044;&#27979;&#12290;&#36890;&#36807;&#25972;&#20307;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24207;&#21015;&#12289;&#24320;&#21457;&#21452;&#21521;&#26144;&#23556;&#21644;&#35774;&#35745;&#38477;&#22122;&#32593;&#32476;&#31561;&#25163;&#27573;&#65292;&#24471;&#21040;&#20102;&#26356;&#20248;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01033</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#22522;&#20110;&#25193;&#25955;&#30340;&#36830;&#32493;&#26102;&#38388;&#38271;&#26399;&#20107;&#20214;&#39044;&#27979;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Non-Autoregressive Diffusion-based Temporal Point Processes for Continuous-Time Long-Term Event Prediction. (arXiv:2311.01033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#38750;&#33258;&#22238;&#24402;&#36830;&#32493;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#26399;&#20107;&#20214;&#39044;&#27979;&#12290;&#36890;&#36807;&#25972;&#20307;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24207;&#21015;&#12289;&#24320;&#21457;&#21452;&#21521;&#26144;&#23556;&#21644;&#35774;&#35745;&#38477;&#22122;&#32593;&#32476;&#31561;&#25163;&#27573;&#65292;&#24471;&#21040;&#20102;&#26356;&#20248;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#38271;&#26399;&#20107;&#20214;&#39044;&#27979;&#22312;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#26694;&#26550;&#26469;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#32047;&#31215;&#38169;&#35823;&#65292;&#20174;&#32780;&#24433;&#21709;&#39044;&#27979;&#30340;&#36136;&#37327;&#12290;&#21463;&#21040;&#38477;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#38750;&#33258;&#22238;&#24402;&#36830;&#32493;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#26399;&#20107;&#20214;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25972;&#20307;&#39044;&#27979;&#26410;&#26469;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#36880;&#20010;&#29983;&#25104;&#20107;&#20214;&#12290;&#20026;&#20102;&#22312;&#20107;&#20214;&#24207;&#21015;&#19978;&#36827;&#34892;&#25193;&#25955;&#22788;&#29702;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#30446;&#26631;&#20107;&#20214;&#24207;&#21015;&#21644;&#27431;&#20960;&#37324;&#24503;&#21521;&#37327;&#31354;&#38388;&#20043;&#38388;&#30340;&#21452;&#21521;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38477;&#22122;&#32593;&#32476;&#65292;&#20197;&#25429;&#25417;&#39034;&#24207;&#21644;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38271;&#26399;&#20107;&#20214;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous-time long-term event prediction plays an important role in many application scenarios. Most existing works rely on autoregressive frameworks to predict event sequences, which suffer from error accumulation, thus compromising prediction quality. Inspired by the success of denoising diffusion probabilistic models, we propose a diffusion-based non-autoregressive temporal point process model for long-term event prediction in continuous time. Instead of generating events one at a time in an autoregressive way, our model predicts the future event sequence entirely as a whole. In order to perform diffusion processes on event sequences, we develop a bidirectional map between target event sequences and the Euclidean vector space. Furthermore, we design a novel denoising network to capture both sequential and contextual features for better sample quality. Extensive experiments are conducted to prove the superiority of our proposed model over state-of-the-art methods on long-term event
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21253;&#21547;&#39640;&#26031;&#25513;&#30721;&#23618;&#21644;&#21327;&#26041;&#24046;&#33258;&#27880;&#24847;&#23618;&#30340;&#23616;&#37096;&#32534;&#30721;&#22120;&#65292;&#22312;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#23616;&#37096;&#19978;&#19979;&#25991;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01030</link><description>&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Joint Learning of Local and Global Features for Aspect-based Sentiment Classification. (arXiv:2311.01030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01030
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21253;&#21547;&#39640;&#26031;&#25513;&#30721;&#23618;&#21644;&#21327;&#26041;&#24046;&#33258;&#27880;&#24847;&#23618;&#30340;&#23616;&#37096;&#32534;&#30721;&#22120;&#65292;&#22312;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#23616;&#37096;&#19978;&#19979;&#25991;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#26088;&#22312;&#21028;&#26029;&#21477;&#23376;&#20013;&#32473;&#23450;&#26041;&#38754;&#26415;&#35821;&#25152;&#20256;&#36798;&#30340;&#24773;&#24863;&#26497;&#24615;&#12290;&#24773;&#24863;&#26497;&#24615;&#19981;&#20165;&#30001;&#23616;&#37096;&#19978;&#19979;&#25991;&#20915;&#23450;&#65292;&#36824;&#19982;&#36828;&#31163;&#32473;&#23450;&#26041;&#38754;&#26415;&#35821;&#30340;&#35789;&#27719;&#30456;&#20851;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26080;&#27861;&#36275;&#22815;&#22320;&#21306;&#20998;&#24212;&#35813;&#26356;&#20851;&#27880;&#21738;&#20123;&#35789;&#35821;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#27491;&#22312;&#36827;&#20837;&#22522;&#20110;&#26041;&#21521;&#30340;&#24773;&#24863;&#20998;&#31867;&#20197;&#32534;&#30721;&#21477;&#27861;&#20381;&#36182;&#26641;&#20449;&#24687;&#12290;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21477;&#27861;&#20381;&#36182;&#26641;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#35270;&#20102;&#23558;&#20381;&#36182;&#20851;&#31995;&#26631;&#31614;&#20449;&#24687;&#26377;&#25928;&#22320;&#25972;&#21512;&#21040;&#34920;&#31034;&#23398;&#20064;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#25928;&#22320;&#24314;&#27169;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21253;&#21547;&#39640;&#26031;&#25513;&#30721;&#23618;&#21644;&#21327;&#26041;&#24046;&#33258;&#27880;&#24847;&#23618;&#30340;&#23616;&#37096;&#32534;&#30721;&#22120;&#12290;&#39640;&#26031;&#25513;&#30721;&#23618;&#20542;&#21521;&#20110;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#21608;&#22260;&#26041;&#38754;&#26415;&#35821;&#30340;&#24863;&#21463;&#37326;&#65292;&#20197;&#20351;&#20854;&#19981;&#37325;&#35201;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment classification (ASC) aims to judge the sentiment polarity conveyed by the given aspect term in a sentence. The sentiment polarity is not only determined by the local context but also related to the words far away from the given aspect term. Most recent efforts related to the attention-based models can not sufficiently distinguish which words they should pay more attention to in some cases. Meanwhile, graph-based models are coming into ASC to encode syntactic dependency tree information. But these models do not fully leverage syntactic dependency trees as they neglect to incorporate dependency relation tag information into representation learning effectively. In this paper, we address these problems by effectively modeling the local and global features. Firstly, we design a local encoder containing: a Gaussian mask layer and a covariance self-attention layer. The Gaussian mask layer tends to adjust the receptive field around aspect terms adaptively to deemphasize 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#20256;&#25773;&#31574;&#30053;TAGNet&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#20013;&#30340;&#39640;&#25928;&#25512;&#29702;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;TAGNet&#33021;&#22815;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#20256;&#25773;&#28040;&#24687;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#22797;&#26434;&#24230;&#19982;&#23618;&#25968;&#26080;&#20851;&#12290;</title><link>http://arxiv.org/abs/2311.01024</link><description>&lt;p&gt;
&#22522;&#20110;&#36317;&#31163;&#30340;&#20256;&#25773;&#31574;&#30053;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#25928;&#29575;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Distance-Based Propagation for Efficient Knowledge Graph Reasoning. (arXiv:2311.01024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01024
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#20256;&#25773;&#31574;&#30053;TAGNet&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#20013;&#30340;&#39640;&#25928;&#25512;&#29702;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;TAGNet&#33021;&#22815;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#20256;&#25773;&#28040;&#24687;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#22797;&#26434;&#24230;&#19982;&#23618;&#25968;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#20013;&#26410;&#35265;&#30340;&#36793;&#65292;&#20174;&#32780;&#21457;&#29616;&#26032;&#30340;&#20107;&#23454;&#12290;&#19968;&#31867;&#26032;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#36890;&#36807;&#32858;&#21512;&#36335;&#24452;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#39281;&#21463;&#25928;&#29575;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#19968;&#20123;&#23581;&#35797;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#36335;&#24452;&#20462;&#21098;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#22312;&#24615;&#33021;&#19978;&#20570;&#20986;&#20102;&#29306;&#29298;&#20197;&#25442;&#21462;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#20004;&#20010;&#22266;&#26377;&#38480;&#21046;&#65292;&#36825;&#20123;&#38480;&#21046;&#24433;&#21709;&#20102;&#25928;&#29575;&#21644;&#34920;&#31034;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;TAGNet&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#20256;&#25773;&#20449;&#24687;&#12290;&#36825;&#26159;&#36890;&#36807;&#20165;&#22312;&#27599;&#23545;&#28304;-&#30446;&#26631;&#23545;&#20013;&#32858;&#21512;&#22266;&#23450;&#31383;&#21475;&#20013;&#30340;&#36335;&#24452;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;TAGNet&#30340;&#22797;&#26434;&#24230;&#19982;&#23618;&#25968;&#26080;&#20851;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;TAGNet&#21487;&#20197;&#20943;&#23569;&#20256;&#25773;&#28040;&#24687;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC) aims to predict unseen edges in knowledge graphs (KGs), resulting in the discovery of new facts. A new class of methods have been proposed to tackle this problem by aggregating path information. These methods have shown tremendous ability in the task of KGC. However they are plagued by efficiency issues. Though there are a few recent attempts to address this through learnable path pruning, they often sacrifice the performance to gain efficiency. In this work, we identify two intrinsic limitations of these methods that affect the efficiency and representation quality. To address the limitations, we introduce a new method, TAGNet, which is able to efficiently propagate information. This is achieved by only aggregating paths in a fixed window for each source-target pair. We demonstrate that the complexity of TAGNet is independent of the number of layers. Extensive experiments demonstrate that TAGNet can cut down on the number of propagated messages by as m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#39537;&#21160;&#30340;&#23545;&#27604;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#22686;&#24378;&#35270;&#22270;&#36827;&#34892;&#23545;&#27604;&#24615;&#23398;&#20064;&#19981;&#21464;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#20811;&#26381;&#25968;&#25454;&#30701;&#32570;&#21644;&#35745;&#31639;&#36164;&#28304;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01023</link><description>&lt;p&gt;
&#22686;&#24378;&#26159;AUtO-Net&#65306;&#22522;&#20110;&#22686;&#24378;&#39537;&#21160;&#30340;&#23545;&#27604;&#22810;&#35270;&#22270;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Augmentation is AUtO-Net: Augmentation-Driven Contrastive Multiview Learning for Medical Image Segmentation. (arXiv:2311.01023v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#39537;&#21160;&#30340;&#23545;&#27604;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#22686;&#24378;&#35270;&#22270;&#36827;&#34892;&#23545;&#27604;&#24615;&#23398;&#20064;&#19981;&#21464;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#20811;&#26381;&#25968;&#25454;&#30701;&#32570;&#21644;&#35745;&#31639;&#36164;&#28304;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#31639;&#27861;&#23398;&#20064;&#22797;&#26434;&#22120;&#23448;&#21644;&#32452;&#32455;&#27169;&#24335;&#65292;&#24182;&#20174;&#22122;&#22768;&#32972;&#26223;&#20013;&#25552;&#21462;&#24863;&#20852;&#36259;&#30340;&#37325;&#35201;&#21306;&#22495;&#65292;&#20197;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#30340;&#35270;&#35273;&#33021;&#21147;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#35745;&#31639;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#26412;&#35770;&#25991;&#20851;&#27880;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#30340;&#24191;&#27867;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#27604;&#36739;&#20102;&#21508;&#31181;&#26041;&#27861;&#21644;&#23454;&#35777;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36824;&#36890;&#36807;&#25351;&#20986;&#20004;&#20010;&#37325;&#35201;&#29616;&#26377;&#38480;&#21046;&#65288;&#25968;&#25454;&#22823;&#23567;&#32422;&#26463;&#21644;&#23545;&#39640;&#35745;&#31639;&#36164;&#28304;&#30340;&#20381;&#36182;&#24615;&#65289;&#26469;&#26816;&#26597;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#31616;&#21333;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#22686;&#24378;&#35270;&#22270;&#36827;&#34892;&#23545;&#27604;&#24615;&#23398;&#20064;&#19981;&#21464;&#30340;&#34880;&#31649;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#20811;&#26381;&#25968;&#25454;&#30701;&#32570;&#21644;&#35745;&#31639;&#36164;&#28304;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilisation of deep learning segmentation algorithms that learn complex organs and tissue patterns and extract essential regions of interest from the noisy background to improve the visual ability for medical image diagnosis has achieved impressive results in Medical Image Computing (MIC). This thesis focuses on retinal blood vessel segmentation tasks, providing an extensive literature review of deep learning-based medical image segmentation approaches while comparing the methodologies and empirical performances. The work also examines the limitations of current state-of-the-art methods by pointing out the two significant existing limitations: data size constraints and the dependency on high computational resources. To address such problems, this work proposes a novel efficient, simple multiview learning framework that contrastively learns invariant vessel feature representation by comparing with multiple augmented views by various transformations to overcome data shortage and impr
&lt;/p&gt;</description></item><item><title>NeuroWrite&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#39044;&#27979;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35782;&#21035;&#21644;&#20998;&#31867;&#25163;&#20889;&#25968;&#23383;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#21487;&#20197;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#31867;&#21644;&#31283;&#20581;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#22312;&#25968;&#23383;&#21270;&#25991;&#26723;&#20013;&#36827;&#34892;&#25968;&#23383;&#35782;&#21035;&#12289;&#31614;&#21517;&#39564;&#35777;&#21644;&#33258;&#21160;&#37038;&#25919;&#32534;&#30721;&#35782;&#21035;&#31561;&#23454;&#38469;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01022</link><description>&lt;p&gt;
NeuroWrite: &#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
NeuroWrite: Predictive Handwritten Digit Classification using Deep Neural Networks. (arXiv:2311.01022v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01022
&lt;/p&gt;
&lt;p&gt;
NeuroWrite&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#39044;&#27979;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35782;&#21035;&#21644;&#20998;&#31867;&#25163;&#20889;&#25968;&#23383;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#21487;&#20197;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#31867;&#21644;&#31283;&#20581;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#22312;&#25968;&#23383;&#21270;&#25991;&#26723;&#20013;&#36827;&#34892;&#25968;&#23383;&#35782;&#21035;&#12289;&#31614;&#21517;&#39564;&#35777;&#21644;&#33258;&#21160;&#37038;&#25919;&#32534;&#30721;&#35782;&#21035;&#31561;&#23454;&#38469;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#20351;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;NeuroWrite&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#25163;&#20889;&#25968;&#23383;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#30340;&#20248;&#21183;&#65292;&#22312;&#35782;&#21035;&#21644;&#20998;&#31867;&#25163;&#20889;&#25968;&#23383;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#30740;&#31350;&#20102;NeuroWrite&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#20934;&#22791;&#26041;&#27861;&#12289;&#32593;&#32476;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#26045;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NeuroWrite&#22312;&#25163;&#20889;&#25968;&#23383;&#25968;&#25454;&#38598;&#65288;&#22914;MNIST&#65289;&#19978;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#20998;&#31867;&#21644;&#31283;&#20581;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35813;&#27169;&#22411;&#22312;&#25968;&#23383;&#21270;&#25991;&#26723;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#23383;&#35782;&#21035;&#12289;&#31614;&#21517;&#39564;&#35777;&#21644;&#33258;&#21160;&#37038;&#25919;&#32534;&#30721;&#35782;&#21035;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid evolution of deep neural networks has revolutionized the field of machine learning, enabling remarkable advancements in various domains. In this article, we introduce NeuroWrite, a unique method for predicting the categorization of handwritten digits using deep neural networks. Our model exhibits outstanding accuracy in identifying and categorising handwritten digits by utilising the strength of convolutional neural networks (CNNs) and recurrent neural networks (RNNs).In this article, we give a thorough examination of the data preparation methods, network design, and training methods used in NeuroWrite. By implementing state-of-the-art techniques, we showcase how NeuroWrite can achieve high classification accuracy and robust generalization on handwritten digit datasets, such as MNIST. Furthermore, we explore the model's potential for real-world applications, including digit recognition in digitized documents, signature verification, and automated postal code recognition. Neur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2311.01017</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#25945;&#20250;&#26234;&#33021;&#20307;&#19990;&#30028;&#30340;&#36816;&#20316;&#26041;&#24335;&#12290;&#23613;&#31649;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20294;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#19982;&#20351;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#25193;&#23637;&#19990;&#30028;&#27169;&#22411;&#30340;&#36827;&#23637;&#30456;&#23545;&#36739;&#24930;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#29942;&#39048;&#65306;&#22788;&#29702;&#22797;&#26434;&#21644;&#26080;&#32467;&#26500;&#30340;&#35266;&#23519;&#31354;&#38388;&#20197;&#21450;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;&#39318;&#20808;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#28982;&#21518;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#24182;&#34892;&#35299;&#30721;&#21644;&#21435;&#22122;&#26631;&#35760;&#65292;&#25105;&#20204;&#23558;&#36974;&#34109;&#29983;&#25104;&#22270;&#20687;&#36716;&#25442;&#22120;&#36716;&#25442;&#20026;&#31163;&#25955;&#25193;&#25955;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20123;&#31616;&#21333;&#30340;&#25913;&#36827;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;&#24403;&#24212;&#29992;&#20110;&#28857;&#20113;&#35266;&#23519;&#30340;&#19990;&#30028;&#27169;&#22411;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, our model reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOT&#27169;&#22411;&#30340;&#20840;&#33021;-&#23618;&#27425;-&#20998;&#24067;&#20043;&#22806;-&#20020;&#24202;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#23618;&#27425;&#39044;&#27979;&#12289;&#23545;&#20998;&#24067;&#20043;&#22806;&#22270;&#20687;&#30340;&#35686;&#31034;&#20197;&#21450;&#23545;&#20020;&#24202;&#22270;&#20687;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#24314;&#35758;&#36827;&#34892;&#30382;&#32932;&#38236;&#26816;&#26597;&#65292;&#20197;&#25552;&#39640;&#30382;&#32932;&#30149;&#21464;&#35786;&#26029;&#30340;&#25928;&#26524;&#21644;&#21327;&#21516;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2311.01009</link><description>&lt;p&gt;
&#37325;&#22609;&#30382;&#32932;&#30149;&#23398;&#20013;&#30340;AI&#27169;&#22411;&#65306;&#20811;&#26381;&#20851;&#38190;&#25361;&#25112;&#20197;&#25552;&#39640;&#30382;&#32932;&#30149;&#21464;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Revamping AI Models in Dermatology: Overcoming Critical Challenges for Enhanced Skin Lesion Diagnosis. (arXiv:2311.01009v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01009
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOT&#27169;&#22411;&#30340;&#20840;&#33021;-&#23618;&#27425;-&#20998;&#24067;&#20043;&#22806;-&#20020;&#24202;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#23618;&#27425;&#39044;&#27979;&#12289;&#23545;&#20998;&#24067;&#20043;&#22806;&#22270;&#20687;&#30340;&#35686;&#31034;&#20197;&#21450;&#23545;&#20020;&#24202;&#22270;&#20687;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#24314;&#35758;&#36827;&#34892;&#30382;&#32932;&#38236;&#26816;&#26597;&#65292;&#20197;&#25552;&#39640;&#30382;&#32932;&#30149;&#21464;&#35786;&#26029;&#30340;&#25928;&#26524;&#21644;&#21327;&#21516;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#22270;&#20687;&#20998;&#26512;&#24320;&#21457;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35786;&#26029;&#30382;&#32932;&#30149;&#21464;&#30340;&#25968;&#37327;&#28608;&#22686;&#65292;&#28982;&#32780;&#23427;&#20204;&#38754;&#20020;&#20020;&#24202;&#25361;&#25112;&#12290;&#24403;&#21069;&#30382;&#32932;&#30149;&#23398;AI&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#26377;&#38480;&#30340;&#35786;&#26029;&#32467;&#26524;&#25968;&#37327;&#65292;&#23545;&#19981;&#24120;&#35265;&#30149;&#21464;&#30340;&#29616;&#23454;&#19990;&#30028;&#27979;&#35797;&#30340;&#32570;&#20047;&#65292;&#26080;&#27861;&#26816;&#27979;&#21040;&#20998;&#24067;&#20043;&#22806;&#30340;&#22270;&#20687;&#65292;&#24182;&#36807;&#24230;&#20381;&#36182;&#20110;&#30382;&#32932;&#38236;&#22270;&#20687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20840;&#33021;-&#23618;&#27425;-&#20998;&#24067;&#20043;&#22806;-&#20020;&#24202;&#20998;&#31867;&#65288;HOT&#65289;&#8221;&#27169;&#22411;&#12290;&#23545;&#20110;&#20020;&#24202;&#22270;&#20687;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#19977;&#20010;&#36755;&#20986;&#65306;&#23618;&#27425;&#39044;&#27979;&#12289;&#23545;&#20998;&#24067;&#20043;&#22806;&#22270;&#20687;&#30340;&#35686;&#31034;&#20197;&#21450;&#22914;&#26524;&#20165;&#20973;&#20020;&#24202;&#22270;&#20687;&#26080;&#27861;&#35786;&#26029;&#21017;&#24314;&#35758;&#36827;&#34892;&#30382;&#32932;&#38236;&#26816;&#26597;&#12290;&#24403;&#37319;&#32435;&#36825;&#20010;&#24314;&#35758;&#26102;&#65292;&#23427;&#23558;&#25972;&#21512;&#20020;&#24202;&#21644;&#30382;&#32932;&#38236;&#22270;&#20687;&#20197;&#24471;&#20986;&#26368;&#32456;&#35786;&#26029;&#12290;&#23545;&#20195;&#34920;&#24615;&#30340;&#30382;&#32932;&#30149;&#21464;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26694;&#26550;&#20013;&#27599;&#20010;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#21644;&#21327;&#21516;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
The surge in developing deep learning models for diagnosing skin lesions through image analysis is notable, yet their clinical black faces challenges. Current dermatology AI models have limitations: limited number of possible diagnostic outputs, lack of real-world testing on uncommon skin lesions, inability to detect out-of-distribution images, and over-reliance on dermoscopic images. To address these, we present an All-In-One \textbf{H}ierarchical-\textbf{O}ut of Distribution-\textbf{C}linical Triage (HOT) model. For a clinical image, our model generates three outputs: a hierarchical prediction, an alert for out-of-distribution images, and a recommendation for dermoscopy if clinical image alone is insufficient for diagnosis. When the recommendation is pursued, it integrates both clinical and dermoscopic images to deliver final diagnosis. Extensive experiments on a representative cutaneous lesion dataset demonstrate the effectiveness and synergy of each component within our framework. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#25214;&#21040;&#25968;&#25454;&#30340;&#23616;&#37096;&#21306;&#22495;&#21644;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25551;&#36848;&#65292;&#25105;&#20204;&#25945;&#23548;&#20154;&#31867;&#22914;&#20309;&#19982;AI&#21512;&#20316;&#12290;&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#21644;&#38382;&#31572;&#20219;&#21153;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#26356;&#21152;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2311.01007</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#24341;&#23548;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Effective Human-AI Teams via Learned Natural Language Rules and Onboarding. (arXiv:2311.01007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#25214;&#21040;&#25968;&#25454;&#30340;&#23616;&#37096;&#21306;&#22495;&#21644;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25551;&#36848;&#65292;&#25105;&#20204;&#25945;&#23548;&#20154;&#31867;&#22914;&#20309;&#19982;AI&#21512;&#20316;&#12290;&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#21644;&#38382;&#31572;&#20219;&#21153;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#26356;&#21152;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;AI&#20195;&#29702;&#26469;&#24110;&#21161;&#20182;&#20204;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;&#20154;&#31867;&#24517;&#39035;&#30693;&#36947;&#20309;&#26102;&#20381;&#36182;&#20110;&#20195;&#29702;&#65292;&#19982;&#20195;&#29702;&#21512;&#20316;&#25110;&#24573;&#30053;&#20854;&#24314;&#35758;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#21306;&#22495;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#23398;&#20064;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20197;&#35828;&#26126;&#20154;&#31867;&#24212;&#35813;&#22914;&#20309;&#19982;AI&#21512;&#20316;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#21306;&#22495;&#21457;&#29616;&#31639;&#27861;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25214;&#21040;&#25968;&#25454;&#30340;&#23616;&#37096;&#21306;&#22495;&#20316;&#20026;&#37051;&#22495;&#65292;&#32416;&#27491;&#20102;&#20154;&#31867;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#21306;&#22495;&#37117;&#36890;&#36807;&#36845;&#20195;&#21644;&#23545;&#27604;&#36807;&#31243;&#36827;&#34892;&#25551;&#36848;&#65292;&#20854;&#20013;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25551;&#36848;&#35813;&#21306;&#22495;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#24341;&#23548;&#38454;&#27573;&#23558;&#36825;&#20123;&#35268;&#21017;&#25945;&#32473;&#20154;&#31867;&#12290;&#36890;&#36807;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#26356;&#21152;&#20934;&#30830;&#12290;&#25105;&#20204;&#36824;&#20998;&#21035;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#21306;&#22495;&#21457;&#29616;&#21644;&#25551;&#36848;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
People are relying on AI agents to assist them with various tasks. The human must know when to rely on the agent, collaborate with the agent, or ignore its suggestions. In this work, we propose to learn rules grounded in data regions and described in natural language that illustrate how the human should collaborate with the AI. Our novel region discovery algorithm finds local regions in the data as neighborhoods in an embedding space that corrects the human prior. Each region is then described using an iterative and contrastive procedure where a large language model describes the region. We then teach these rules to the human via an onboarding stage. Through user studies on object detection and question-answering tasks, we show that our method can lead to more accurate human-AI teams. We also evaluate our region discovery and description algorithms separately.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#35821;&#20041;&#23398;&#20064;&#30340;Sam&#24341;&#23548;&#22686;&#24378;&#32454;&#31890;&#24230;&#32534;&#30721;&#30340;&#21307;&#23398;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#35814;&#32454;&#29305;&#24449;&#65292;&#24182;&#22312;&#35780;&#20272;&#25351;&#26631;&#19978;&#20248;&#20110;&#39044;&#35757;&#32451;&#30340;BLIP2&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2311.01004</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#35821;&#20041;&#23398;&#20064;&#30340;Sam&#24341;&#23548;&#22686;&#24378;&#32454;&#31890;&#24230;&#32534;&#30721;&#30340;&#21307;&#23398;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning. (arXiv:2311.01004v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#35821;&#20041;&#23398;&#20064;&#30340;Sam&#24341;&#23548;&#22686;&#24378;&#32454;&#31890;&#24230;&#32534;&#30721;&#30340;&#21307;&#23398;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#35814;&#32454;&#29305;&#24449;&#65292;&#24182;&#22312;&#35780;&#20272;&#25351;&#26631;&#19978;&#20248;&#20110;&#39044;&#35757;&#32451;&#30340;BLIP2&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#25216;&#26415;&#20855;&#26377;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35786;&#26029;&#24314;&#35758;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36890;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#25551;&#36848;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#22797;&#26434;&#32454;&#33410;&#26102;&#26080;&#27861;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#24341;&#23548;&#30340;&#26032;&#22411;&#21307;&#23398;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#19968;&#33324;&#21644;&#35814;&#32454;&#29305;&#24449;&#30340;&#22686;&#24378;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#29420;&#29305;&#30340;&#28151;&#21512;&#35821;&#20041;&#23398;&#20064;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21516;&#26102;&#25429;&#25417;&#21307;&#23398;&#22270;&#20687;&#30340;&#25972;&#20307;&#20449;&#24687;&#21644;&#26356;&#32454;&#33410;&#30340;&#32454;&#33410;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#22312;&#29983;&#25104;&#21307;&#23398;&#22270;&#20687;&#25551;&#36848;&#30340;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#20248;&#20110;&#39044;&#35757;&#32451;&#30340;BLIP2&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of multimodality and large language models, the deep learning-based technique for medical image captioning holds the potential to offer valuable diagnostic recommendations. However, current generic text and image pre-trained models do not yield satisfactory results when it comes to describing intricate details within medical images. In this paper, we present a novel medical image captioning method guided by the segment anything model (SAM) to enable enhanced encoding with both general and detailed feature extraction. In addition, our approach employs a distinctive pre-training strategy with mixed semantic learning to simultaneously capture both the overall information and finer details within medical images. We demonstrate the effectiveness of this approach, as it outperforms the pre-trained BLIP2 model on various evaluation metrics for generating descriptions of medical images.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#37325;&#26032;&#26631;&#35760;&#20934;&#30830;&#24615;&#26469;&#36827;&#34892;&#40065;&#26834;&#25968;&#25454;&#20462;&#21098;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#23376;&#38598;&#65292;&#20351;&#24471;&#25152;&#26377;&#35757;&#32451;&#31034;&#20363;&#30340;&#37051;&#22495;&#32622;&#20449;&#24230;&#20043;&#21644;&#26368;&#22823;&#21270;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2311.01002</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#21270;&#37325;&#26032;&#26631;&#35760;&#20934;&#30830;&#24615;&#26469;&#36827;&#34892;&#40065;&#26834;&#25968;&#25454;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Robust Data Pruning under Label Noise via Maximizing Re-labeling Accuracy. (arXiv:2311.01002v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01002
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#37325;&#26032;&#26631;&#35760;&#20934;&#30830;&#24615;&#26469;&#36827;&#34892;&#40065;&#26834;&#25968;&#25454;&#20462;&#21098;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#23376;&#38598;&#65292;&#20351;&#24471;&#25152;&#26377;&#35757;&#32451;&#31034;&#20363;&#30340;&#37051;&#22495;&#32622;&#20449;&#24230;&#20043;&#21644;&#26368;&#22823;&#21270;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20462;&#21098;&#26088;&#22312;&#23558;&#22823;&#22411;&#35757;&#32451;&#38598;&#32553;&#20943;&#20026;&#19968;&#20010;&#23567;&#32780;&#20449;&#24687;&#20016;&#23500;&#30340;&#23376;&#38598;&#65292;&#23545;&#20110;&#20943;&#23569;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#21547;&#26377;&#27880;&#37322;&#22122;&#22768;&#65292;&#24182;&#19988;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#40065;&#26834;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22122;&#22768;&#40065;&#26834;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#25968;&#25454;&#20462;&#21098;&#20960;&#20046;&#26410;&#21463;&#21040;&#20851;&#27880;&#12290;&#36890;&#36807;&#33258;&#26657;&#27491;&#38169;&#35823;&#26631;&#31614;&#30340;&#26368;&#26032;&#37325;&#26032;&#26631;&#35760;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#24456;&#38590;&#30830;&#23450;&#21738;&#20010;&#23376;&#38598;&#33021;&#22815;&#22312;&#25972;&#20010;&#35757;&#32451;&#38598;&#20013;&#24341;&#21457;&#26368;&#20934;&#30830;&#30340;&#37325;&#26032;&#26631;&#35760;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#37325;&#26032;&#26631;&#35760;&#30340;&#25968;&#25454;&#20462;&#21098;&#38382;&#39064;&#12290;&#39318;&#20808;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#35757;&#32451;&#31034;&#20363;&#34987;&#27491;&#30830;&#37325;&#26032;&#26631;&#35760;&#30340;&#21487;&#33021;&#24615;&#19982;&#20854;&#37051;&#22495;&#20013;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#25104;&#27604;&#20363;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#65292;&#21517;&#20026;Prune4Rel&#65292;&#23427;&#33021;&#25214;&#21040;&#19968;&#20010;&#23376;&#38598;&#65292;&#20351;&#24471;&#25152;&#26377;&#35757;&#32451;&#31034;&#20363;&#30340;&#37051;&#22495;&#32622;&#20449;&#24230;&#20043;&#21644;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data pruning, which aims to downsize a large training set into a small informative subset, is crucial for reducing the enormous computational costs of modern deep learning. Though large-scale data collections invariably contain annotation noise and numerous robust learning methods have been developed, data pruning for the noise-robust learning scenario has received little attention. With state-of-the-art Re-labeling methods that self-correct erroneous labels while training, it is challenging to identify which subset induces the most accurate re-labeling of erroneous labels in the entire training set. In this paper, we formalize the problem of data pruning with re-labeling. We first show that the likelihood of a training example being correctly re-labeled is proportional to the prediction confidence of its neighborhood in the subset. Therefore, we propose a novel data pruning algorithm, Prune4Rel, that finds a subset maximizing the total neighborhood confidence of all training examples,
&lt;/p&gt;</description></item><item><title>&#22312;&#32771;&#34385;&#31227;&#21160;&#22270;&#20687;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26497;&#20302;&#20301;&#36731;&#37327;&#32423;&#30340;&#20154;&#33080;&#26816;&#27979;&#22120;&#65292;&#26088;&#22312;&#24357;&#21512;&#22987;&#32456;&#24320;&#21551;&#22330;&#26223;&#19979;&#30340;&#20154;&#33080;&#26816;&#27979;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2311.01001</link><description>&lt;p&gt;
&#22312;&#32771;&#34385;&#31227;&#21160;&#22270;&#20687;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#23436;&#20840;&#37327;&#21270;&#30340;&#22987;&#32456;&#24320;&#21551;&#30340;&#20154;&#33080;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fully Quantized Always-on Face Detector Considering Mobile Image Sensors. (arXiv:2311.01001v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01001
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#31227;&#21160;&#22270;&#20687;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26497;&#20302;&#20301;&#36731;&#37327;&#32423;&#30340;&#20154;&#33080;&#26816;&#27979;&#22120;&#65292;&#26088;&#22312;&#24357;&#21512;&#22987;&#32456;&#24320;&#21551;&#22330;&#26223;&#19979;&#30340;&#20154;&#33080;&#26816;&#27979;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#20110;&#36793;&#32536;&#35774;&#22791;&#35774;&#35745;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#24403;&#21069;&#30340;&#20154;&#33080;&#26816;&#27979;&#22120;&#24182;&#19981;&#33021;&#23436;&#20840;&#28385;&#36275;&#38598;&#25104;&#20102;&#23884;&#20837;&#24335;DNN&#30340;"&#26234;&#33021;"CMOS&#22270;&#20687;&#20256;&#24863;&#22120;&#65288;iCIS&#65289;&#30340;&#35201;&#27714;&#12290;&#36825;&#20123;&#20256;&#24863;&#22120;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#33410;&#33021;&#25163;&#26426;&#21644;&#24102;&#22987;&#32456;&#24320;&#21551;&#33021;&#21147;&#30340;&#30417;&#25511;&#31995;&#32479;&#12290;&#20854;&#20013;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#23616;&#38480;&#24615;&#26159;&#22987;&#32456;&#24320;&#21551;&#22330;&#26223;&#19979;&#30340;&#21512;&#36866;&#20154;&#33080;&#26816;&#27979;&#22120;&#30340;&#32570;&#22833;&#65292;&#36825;&#26159;&#22270;&#20687;&#20256;&#24863;&#22120;&#32423;&#21035;&#24212;&#29992;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#36825;&#20123;&#26816;&#27979;&#22120;&#24517;&#39035;&#22312;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#22120;&#65288;ISP&#65289;&#25509;&#31649;&#20043;&#21069;&#30452;&#25509;&#22788;&#29702;&#20256;&#24863;&#22120;&#21407;&#22987;&#25968;&#25454;&#12290;&#36825;&#31181;&#24046;&#36317;&#22312;&#23454;&#29616;&#36825;&#31181;&#22330;&#26223;&#19979;&#30340;&#26368;&#20339;&#24615;&#33021;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#20805;&#20998;&#21033;&#29992;iCIS&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#25506;&#32034;&#26497;&#20302;&#20301;&#36731;&#37327;&#32423;&#20154;&#33080;&#26816;&#27979;&#22120;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#37325;&#28857;&#26159;&#22987;&#32456;&#24320;&#21551;&#30340;&#20154;&#33080;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant research on lightweight deep neural networks (DNNs) designed for edge devices, the current face detectors do not fully meet the requirements for "intelligent" CMOS image sensors (iCISs) integrated with embedded DNNs. These sensors are essential in various practical applications, such as energy-efficient mobile phones and surveillance systems with always-on capabilities. One noteworthy limitation is the absence of suitable face detectors for the always-on scenario, a crucial aspect of image sensor-level applications. These detectors must operate directly with sensor RAW data before the image signal processor (ISP) takes over. This gap poses a significant challenge in achieving optimal performance in such scenarios. Further research and development are necessary to bridge this gap and fully leverage the potential of iCIS applications. In this study, we aim to bridge the gap by exploring extremely low-bit lightweight face detectors, focusing on the always-on face detec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#30340;&#20302;&#36164;&#28304;&#26412;&#22320;&#35821;&#35328;&#19978;&#35757;&#32451;NMT&#31995;&#32479;&#30340;&#32508;&#21512;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290; &#23613;&#31649;&#35745;&#31639;&#36164;&#28304;&#21644;&#25991;&#26412;&#25968;&#25454;&#26377;&#38480;&#65292;&#20294;&#25105;&#20204;&#30340;&#20960;&#20010;NMT&#31995;&#32479;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#24615;&#33021;&#65292;&#19982;&#38646;-shot gpt-3.5-turbo&#30340;&#32763;&#35793;&#36136;&#37327;&#30456;&#23218;&#32654;&#12290; &#36825;&#20123;&#21457;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;NMT&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2311.00998</link><description>&lt;p&gt;
&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#30340;&#20302;&#36164;&#28304;&#26412;&#22320;&#35821;&#35328;&#19978;&#21487;&#22797;&#21046;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Replicable Benchmarking of Neural Machine Translation (NMT) on Low-Resource Local Languages in Indonesia. (arXiv:2311.00998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#30340;&#20302;&#36164;&#28304;&#26412;&#22320;&#35821;&#35328;&#19978;&#35757;&#32451;NMT&#31995;&#32479;&#30340;&#32508;&#21512;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290; &#23613;&#31649;&#35745;&#31639;&#36164;&#28304;&#21644;&#25991;&#26412;&#25968;&#25454;&#26377;&#38480;&#65292;&#20294;&#25105;&#20204;&#30340;&#20960;&#20010;NMT&#31995;&#32479;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#24615;&#33021;&#65292;&#19982;&#38646;-shot gpt-3.5-turbo&#30340;&#32763;&#35793;&#36136;&#37327;&#30456;&#23218;&#32654;&#12290; &#36825;&#20123;&#21457;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;NMT&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#20302;&#36164;&#28304;&#26412;&#22320;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#38656;&#35201;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#21644;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#20026;&#21360;&#24230;&#23612;&#35199;&#20122;&#30340;&#22235;&#31181;&#20302;&#36164;&#28304;&#26412;&#22320;&#35821;&#35328;&#65288;&#29226;&#21703;&#35821;&#12289;&#33487;&#20025;&#23612;&#26031;&#35821;&#12289;&#31859;&#21335;&#21345;&#21338;&#21644;&#24052;&#21400;&#35821;&#65289;&#35757;&#32451;NMT&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#21508;&#31181;&#35757;&#32451;&#26041;&#27861;&#12289;&#33539;&#20363;&#12289;&#25968;&#25454;&#35268;&#27169;&#20197;&#21450;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#29983;&#25104;&#30340;&#21021;&#27493;&#30740;&#31350;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#32763;&#35793;&#23454;&#29992;&#31574;&#30053;&#30340;&#20855;&#20307;&#36235;&#21183;&#21644;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#35745;&#31639;&#36164;&#28304;&#21644;&#25991;&#26412;&#25968;&#25454;&#26377;&#38480;&#65292;&#25105;&#20204;&#30340;&#20960;&#20010;NMT&#31995;&#32479;&#22312;&#31454;&#20105;&#24615;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#19982;&#38646;-shot gpt-3.5-turbo&#30340;&#32763;&#35793;&#36136;&#37327;&#30456;&#23218;&#32654;&#12290;&#36825;&#20123;&#21457;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;NMT&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation (NMT) for low-resource local languages in Indonesia faces significant challenges, including the need for a representative benchmark and limited data availability. This work addresses these challenges by comprehensively analyzing training NMT systems for four low-resource local languages in Indonesia: Javanese, Sundanese, Minangkabau, and Balinese. Our study encompasses various training approaches, paradigms, data sizes, and a preliminary study into using large language models for synthetic low-resource languages parallel data generation. We reveal specific trends and insights into practical strategies for low-resource language translation. Our research demonstrates that despite limited computational resources and textual data, several of our NMT systems achieve competitive performances, rivaling the translation quality of zero-shot gpt-3.5-turbo. These findings significantly advance NMT for low-resource languages, offering valuable guidance for researchers in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24211;&#23384;&#37197;&#36865;&#38382;&#39064;&#65292;&#36890;&#36807;&#30452;&#25509;&#38598;&#25104;&#24211;&#23384;&#39044;&#27979;&#21644;&#36335;&#24452;&#20248;&#21270;&#65292;&#21487;&#33021;&#30830;&#20445;&#19968;&#20010;&#24378;&#22823;&#30340;&#20379;&#24212;&#38142;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2311.00983</link><description>&lt;p&gt;
&#20248;&#21270;&#24211;&#23384;&#37197;&#36865;&#65306;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Inventory Routing: A Decision-Focused Learning Approach using Neural Networks. (arXiv:2311.00983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24211;&#23384;&#37197;&#36865;&#38382;&#39064;&#65292;&#36890;&#36807;&#30452;&#25509;&#38598;&#25104;&#24211;&#23384;&#39044;&#27979;&#21644;&#36335;&#24452;&#20248;&#21270;&#65292;&#21487;&#33021;&#30830;&#20445;&#19968;&#20010;&#24378;&#22823;&#30340;&#20379;&#24212;&#38142;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24211;&#23384;&#37197;&#36865;&#38382;&#39064;&#65288;IRP&#65289;&#26159;&#20379;&#24212;&#38142;&#31649;&#29702;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#22312;&#32771;&#34385;&#24211;&#23384;&#38656;&#27714;&#35268;&#21010;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#26377;&#25928;&#30340;&#36335;&#24452;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;IRP&#38382;&#39064;&#65292;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#38656;&#27714;&#65292;&#28982;&#21518;&#20351;&#29992;&#20248;&#21270;&#31639;&#27861;&#26469;&#26368;&#23567;&#21270;&#37197;&#36865;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#29616;&#23436;&#32654;&#20934;&#30830;&#24230;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;&#24211;&#23384;&#27700;&#24179;&#21463;&#21160;&#24577;&#19994;&#21153;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#36827;&#32780;&#24433;&#21709;&#21040;&#19979;&#19968;&#38454;&#27573;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#23548;&#33268;&#27425;&#20248;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;IRP&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#20013;&#30452;&#25509;&#38598;&#25104;&#20102;&#24211;&#23384;&#39044;&#27979;&#21644;&#36335;&#24452;&#20248;&#21270;&#65292;&#21487;&#33021;&#30830;&#20445;&#19968;&#20010;&#24378;&#22823;&#30340;&#20379;&#24212;&#38142;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inventory Routing Problem (IRP) is a crucial challenge in supply chain management as it involves optimizing efficient route selection while considering the uncertainty of inventory demand planning. To solve IRPs, usually a two-stage approach is employed, where demand is predicted using machine learning techniques first, and then an optimization algorithm is used to minimize routing costs. Our experiment shows machine learning models fall short of achieving perfect accuracy because inventory levels are influenced by the dynamic business environment, which, in turn, affects the optimization problem in the next stage, resulting in sub-optimal decisions. In this paper, we formulate and propose a decision-focused learning-based approach to solving real-world IRPs. This approach directly integrates inventory prediction and routing optimization within an end-to-end system potentially ensuring a robust supply chain strategy.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#24212;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21015;&#36710;&#36816;&#34892;&#22270;&#38382;&#39064;(TTP)&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;TTP&#30340;&#35299;&#20915;&#25928;&#29575;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2311.00971</link><description>&lt;p&gt;
&#19968;&#20010;&#38598;&#25104;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#21015;&#36710;&#36816;&#34892;&#22270;&#38382;&#39064;&#32508;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Integrated Framework Integrating Monte Carlo Tree Search and Supervised Learning for Train Timetabling Problem. (arXiv:2311.00971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00971
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#24212;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21015;&#36710;&#36816;&#34892;&#22270;&#38382;&#39064;(TTP)&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;TTP&#30340;&#35299;&#20915;&#25928;&#29575;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#32447;&#38081;&#36335;&#21015;&#36710;&#36816;&#34892;&#22270;&#38382;&#39064;(TTP)&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#24212;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#12289;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;(MCTS)&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;TTP&#12290;&#26412;&#25991;&#39318;&#20808;&#25551;&#36848;&#20102;TTP&#30340;&#25968;&#23398;&#27169;&#22411;&#21644;&#20223;&#30495;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#24182;&#20174;MCTS&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#29305;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#20197;&#25913;&#36827;MCTS&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#26041;&#27861;&#35270;&#20026;&#25152;&#25552;&#26694;&#26550;&#20013;&#30340;&#35268;&#21010;&#22120;&#12290;&#20854;&#27425;&#65292;&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#33410;&#28857;&#30340;&#20540;&#65292;&#24182;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;MCTS&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#31216;&#20026;&#23398;&#20064;&#22120;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#21551;&#21457;&#24335;MCTS&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;TTP&#65307;&#23558;&#35268;&#21010;&#22120;&#21644;&#23398;&#20064;&#22120;&#38598;&#25104;&#21040;&#31639;&#27861;&#26694;&#26550;&#20013;&#21487;&#20197;&#25552;&#39640;&#35299;&#20915;TTP&#30340;&#25968;&#25454;&#25928;&#29575;&#65307;&#35813;&#31639;&#27861;&#26694;&#26550;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#31867;&#20284;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The single-track railway train timetabling problem (TTP) is an important and complex problem. This article proposes an integrated Monte Carlo Tree Search (MCTS) computing framework that combines heuristic methods, unsupervised learning methods, and supervised learning methods for solving TTP in discrete action spaces. This article first describes the mathematical model and simulation system dynamics of TTP, analyzes the characteristics of the solution from the perspective of MCTS, and proposes some heuristic methods to improve MCTS. This article considers these methods as planners in the proposed framework. Secondly, this article utilizes deep convolutional neural networks to approximate the value of nodes and further applies them to the MCTS search process, referred to as learners. The experiment shows that the proposed heuristic MCTS method is beneficial for solving TTP; The algorithm framework that integrates planners and learners can improve the data efficiency of solving TTP; The 
&lt;/p&gt;</description></item><item><title>Video2Music&#26159;&#19968;&#20010;&#29983;&#25104;&#38899;&#20048;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#35270;&#39057;&#29983;&#25104;&#30456;&#21305;&#37197;&#30340;&#38899;&#20048;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20998;&#26512;&#35270;&#39057;&#30340;&#35821;&#20041;&#12289;&#22330;&#26223;&#20559;&#31227;&#12289;&#21160;&#20316;&#21644;&#24773;&#24863;&#29305;&#24449;&#65292;&#37319;&#29992;Affective Multimodal Transformer (AMT)&#27169;&#22411;&#29983;&#25104;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2311.00968</link><description>&lt;p&gt;
Video2Music&#65306;&#20351;&#29992;&#24773;&#24863;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#20174;&#35270;&#39057;&#20013;&#29983;&#25104;&#21512;&#36866;&#30340;&#38899;&#20048;
&lt;/p&gt;
&lt;p&gt;
Video2Music: Suitable Music Generation from Videos using an Affective Multimodal Transformer model. (arXiv:2311.00968v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00968
&lt;/p&gt;
&lt;p&gt;
Video2Music&#26159;&#19968;&#20010;&#29983;&#25104;&#38899;&#20048;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#35270;&#39057;&#29983;&#25104;&#30456;&#21305;&#37197;&#30340;&#38899;&#20048;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20998;&#26512;&#35270;&#39057;&#30340;&#35821;&#20041;&#12289;&#22330;&#26223;&#20559;&#31227;&#12289;&#21160;&#20316;&#21644;&#24773;&#24863;&#29305;&#24449;&#65292;&#37319;&#29992;Affective Multimodal Transformer (AMT)&#27169;&#22411;&#29983;&#25104;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38899;&#20048;&#29983;&#25104;&#39046;&#22495;&#65292;&#35768;&#22810;&#30740;&#31350;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#26681;&#25454;&#35270;&#39057;&#29983;&#25104;&#30456;&#21305;&#37197;&#30340;&#38899;&#20048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#25104;&#38899;&#20048;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;Video2Music&#65292;&#23427;&#21487;&#20197;&#21305;&#37197;&#25552;&#20379;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#39318;&#20808;&#31934;&#24515;&#31574;&#21010;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#38899;&#20048;&#35270;&#39057;&#38598;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#38899;&#20048;&#35270;&#39057;&#20197;&#33719;&#24471;&#35821;&#20041;&#12289;&#22330;&#26223;&#20559;&#31227;&#12289;&#21160;&#20316;&#21644;&#24773;&#24863;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#19981;&#21516;&#30340;&#29305;&#24449;&#34987;&#29992;&#20316;&#25105;&#20204;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#30340;&#24341;&#23548;&#36755;&#20837;&#12290;&#25105;&#20204;&#23558;&#38899;&#39057;&#25991;&#20214;&#36716;&#24405;&#20026;MIDI&#21644;&#21644;&#24358;&#65292;&#24182;&#25552;&#21462;&#38899;&#31526;&#23494;&#24230;&#21644;&#38899;&#37327;&#31561;&#29305;&#24449;&#12290;&#36825;&#20135;&#29983;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;MuVi-Sync&#65292;&#25105;&#20204;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24773;&#24863;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#65288;AMT&#65289;&#26469;&#26681;&#25454;&#35270;&#39057;&#29983;&#25104;&#38899;&#20048;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#21046;&#26469;&#24378;&#21046;&#35270;&#39057;&#21644;&#38899;&#20048;&#20043;&#38388;&#30340;&#24773;&#24863;&#30456;&#20284;&#24615;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#19968;&#20010;&#22522;&#20110;&#21452;&#21521;GRU&#30340;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#20272;&#35745;&#38899;&#31526;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous studies in the field of music generation have demonstrated impressive performance, yet virtually no models are able to directly generate music to match accompanying videos. In this work, we develop a generative music AI framework, Video2Music, that can match a provided video. We first curated a unique collection of music videos. Then, we analysed the music videos to obtain semantic, scene offset, motion, and emotion features. These distinct features are then employed as guiding input to our music generation model. We transcribe the audio files into MIDI and chords, and extract features such as note density and loudness. This results in a rich multimodal dataset, called MuVi-Sync, on which we train a novel Affective Multimodal Transformer (AMT) model to generate music given a video. This model includes a novel mechanism to enforce affective similarity between video and music. Finally, post-processing is performed based on a biGRU-based regression model to estimate note density 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vision-Language Interpreter&#65288;ViLaIn&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26426;&#22120;&#20154;&#20219;&#21153;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#31526;&#21495;&#35268;&#21010;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ViLaIn&#21644;&#31526;&#21495;&#35268;&#21010;&#22120;&#33021;&#22815;&#20934;&#30830;&#29983;&#25104;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2311.00967</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#35270;&#35273;&#35821;&#35328;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Interpreter for Robot Task Planning. (arXiv:2311.00967v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vision-Language Interpreter&#65288;ViLaIn&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26426;&#22120;&#20154;&#20219;&#21153;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#31526;&#21495;&#35268;&#21010;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ViLaIn&#21644;&#31526;&#21495;&#35268;&#21010;&#22120;&#33021;&#22815;&#20934;&#30830;&#29983;&#25104;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#21152;&#36895;&#35821;&#35328;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#35268;&#21010;&#22120;&#30340;&#21457;&#23637;&#12290;&#21516;&#26102;&#65292;&#31526;&#21495;&#35268;&#21010;&#22120;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#23558;&#36825;&#20004;&#31181;&#36235;&#21183;&#30456;&#32467;&#21512;&#65292;&#21363;&#22810;&#27169;&#24577;&#35268;&#21010;&#38382;&#39064;&#35268;&#33539;&#12290;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#38382;&#39064;&#25551;&#36848;&#65288;PD&#65289;&#65292;&#36825;&#26159;&#35268;&#21010;&#22120;&#29992;&#26469;&#26597;&#25214;&#35745;&#21010;&#30340;&#26426;&#22120;&#21487;&#35835;&#25991;&#20214;&#12290;&#36890;&#36807;&#20174;&#35821;&#35328;&#25351;&#20196;&#21644;&#22330;&#26223;&#35266;&#27979;&#20013;&#29983;&#25104;PD&#65292;&#25105;&#20204;&#21487;&#20197;&#39537;&#21160;&#31526;&#21495;&#35268;&#21010;&#22120;&#22312;&#35821;&#35328;&#24341;&#23548;&#26694;&#26550;&#19979;&#24037;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vision-Language Interpreter&#65288;ViLaIn&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20808;&#36827;&#30340;LLM&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;PD&#12290;ViLaIn&#21487;&#20197;&#36890;&#36807;&#31526;&#21495;&#35268;&#21010;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#21453;&#39304;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;PD&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65306;ViLaIn&#21644;&#31526;&#21495;&#35268;&#21010;&#22120;&#33021;&#22815;&#20934;&#30830;&#22320;&#29983;&#25104;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#35745;&#21010;&#21527;&#65311;&#20026;&#20102;&#35780;&#20272;ViLaIn&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#38382;&#39064;&#25551;&#36848;&#29983;&#25104;&#65288;ProDG&#65289;&#25968;&#25454;&#38598;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#23558;&#22312;&#35780;&#20272;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are accelerating the development of language-guided robot planners. Meanwhile, symbolic planners offer the advantage of interpretability. This paper proposes a new task that bridges these two trends, namely, multimodal planning problem specification. The aim is to generate a problem description (PD), a machine-readable file used by the planners to find a plan. By generating PDs from language instruction and scene observation, we can drive symbolic planners in a language-guided framework. We propose a Vision-Language Interpreter (ViLaIn), a new framework that generates PDs using state-of-the-art LLM and vision-language models. ViLaIn can refine generated PDs via error message feedback from the symbolic planner. Our aim is to answer the question: How accurately can ViLaIn and the symbolic planner generate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset called the problem description generation (ProDG) dataset. The framework is evaluated wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;IndoToD&#65292;&#19968;&#20010;&#29992;&#20110;&#21360;&#23612;&#35821;&#30340;&#31471;&#21040;&#31471;&#22810;&#39046;&#22495;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22522;&#20934;&#12290;&#36890;&#36807;&#23558;&#33521;&#35821;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#21360;&#23612;&#35821;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#36825;&#20010;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#38599;&#20323;&#27597;&#35821;&#20026;&#21360;&#23612;&#35821;&#30340;&#20154;&#21592;&#36827;&#34892;&#32763;&#35793;&#21644;&#25968;&#25454;&#25910;&#38598;&#65292;&#36825;&#20026;&#35780;&#20272;&#21360;&#23612;&#35821;&#21644;&#33521;&#35821;&#23545;&#35805;&#31995;&#32479;&#20197;&#21450;&#36328;&#35821;&#35328;&#21644;&#21452;&#35821;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2311.00958</link><description>&lt;p&gt;
IndoToD: &#19968;&#20010;&#29992;&#20110;&#22810;&#39046;&#22495;&#21360;&#23612;&#35821;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
IndoToD: A Multi-Domain Indonesian Benchmark For End-to-End Task-Oriented Dialogue Systems. (arXiv:2311.00958v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;IndoToD&#65292;&#19968;&#20010;&#29992;&#20110;&#21360;&#23612;&#35821;&#30340;&#31471;&#21040;&#31471;&#22810;&#39046;&#22495;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22522;&#20934;&#12290;&#36890;&#36807;&#23558;&#33521;&#35821;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#21360;&#23612;&#35821;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#36825;&#20010;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#38599;&#20323;&#27597;&#35821;&#20026;&#21360;&#23612;&#35821;&#30340;&#20154;&#21592;&#36827;&#34892;&#32763;&#35793;&#21644;&#25968;&#25454;&#25910;&#38598;&#65292;&#36825;&#20026;&#35780;&#20272;&#21360;&#23612;&#35821;&#21644;&#33521;&#35821;&#23545;&#35805;&#31995;&#32479;&#20197;&#21450;&#36328;&#35821;&#35328;&#21644;&#21452;&#35821;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#21482;&#26159;&#20026;&#39640;&#36164;&#28304;&#35821;&#35328;&#22914;&#33521;&#35821;&#21644;&#27721;&#35821;&#21019;&#24314;&#30340;&#65292;&#28982;&#32780;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#20854;&#20182;&#21306;&#22495;&#25110;&#26412;&#22320;&#35821;&#35328;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#20197;&#25193;&#23637;&#23427;&#20204;&#29702;&#35299;&#19981;&#21516;&#35821;&#35328;&#23545;&#35805;&#32972;&#26223;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IndoToD&#65292;&#19968;&#20010;&#21360;&#23612;&#35821;&#31471;&#21040;&#31471;&#22810;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35789;&#27861;&#20998;&#26512;&#23558;&#20004;&#20010;&#33521;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;&#25193;&#23637;&#21040;&#21360;&#23612;&#35821;&#65292;&#21253;&#25324;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#65292;&#20197;&#39640;&#25928;&#22320;&#20943;&#23569;&#27880;&#37322;&#30340;&#22823;&#23567;&#12290;&#20026;&#20102;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#25105;&#20204;&#38599;&#29992;&#27597;&#35821;&#20026;&#21360;&#23612;&#35821;&#30340;&#20154;&#25163;&#21160;&#32763;&#35793;&#23545;&#35805;&#12290;&#38500;&#20102;&#21407;&#22987;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#22806;&#65292;&#36825;&#20123;&#26032;&#30340;&#21360;&#23612;&#35821;&#25968;&#25454;&#38598;&#21487;&#20197;&#20316;&#20026;&#35780;&#20272;&#21360;&#23612;&#35821;&#21644;&#33521;&#35821;&#23545;&#35805;&#31995;&#32479;&#20197;&#21450;&#25506;&#32034;&#36328;&#35821;&#35328;&#21644;&#21452;&#35821;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#28508;&#22312;&#30410;&#22788;&#30340;&#26377;&#25928;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue (ToD) systems have been mostly created for high-resource languages, such as English and Chinese. However, there is a need to develop ToD systems for other regional or local languages to broaden their ability to comprehend the dialogue contexts in various languages. This paper introduces IndoToD, an end-to-end multi domain ToD benchmark in Indonesian. We extend two English ToD datasets to Indonesian, comprising four different domains by delexicalization to efficiently reduce the size of annotations. To ensure a high-quality data collection, we hire native speakers to manually translate the dialogues. Along with the original English datasets, these new Indonesian datasets serve as an effective benchmark for evaluating Indonesian and English ToD systems as well as exploring the potential benefits of cross-lingual and bilingual transfer learning approaches.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#28151;&#21512;&#35299;&#31639;&#22120;(GMS)&#30340;&#26032;&#22411;SDE-based&#35299;&#31639;&#22120;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20272;&#35745;&#21069;&#19977;&#38454;&#30697;&#24182;&#20248;&#21270;&#39640;&#26031;&#28151;&#21512;&#21442;&#25968;&#26469;&#35299;&#20915;&#29616;&#26377;SDE-based&#35299;&#31639;&#22120;&#30340;&#25928;&#29575;-&#26377;&#25928;&#24615;&#22256;&#22659;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00941</link><description>&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#35299;&#31639;&#22120;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaussian Mixture Solvers for Diffusion Models. (arXiv:2311.00941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00941
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#28151;&#21512;&#35299;&#31639;&#22120;(GMS)&#30340;&#26032;&#22411;SDE-based&#35299;&#31639;&#22120;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20272;&#35745;&#21069;&#19977;&#38454;&#30697;&#24182;&#20248;&#21270;&#39640;&#26031;&#28151;&#21512;&#21442;&#25968;&#26469;&#35299;&#20915;&#29616;&#26377;SDE-based&#35299;&#31639;&#22120;&#30340;&#25928;&#29575;-&#26377;&#25928;&#24615;&#22256;&#22659;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#31561;&#20215;&#20110;&#35299;&#20915;&#21453;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#25110;&#30456;&#24212;&#30340;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#12290;&#19982;&#20043;&#30456;&#27604;&#65292;&#22522;&#20110;SDE&#30340;&#35299;&#31639;&#22120;&#21487;&#20197;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#36866;&#29992;&#20110;&#22522;&#20110;&#31508;&#21010;&#21512;&#25104;&#30340;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;SDE&#30340;&#35299;&#31639;&#22120;&#21463;&#21040;&#25928;&#29575;-&#26377;&#25928;&#24615;&#22256;&#22659;&#30340;&#20005;&#37325;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#34920;&#26126;&#65292;&#36825;&#26159;&#22240;&#20026;&#21453;&#21521;&#36716;&#25442;&#26680;&#20013;&#30340;&#39640;&#26031;&#20551;&#35774;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#31163;&#25955;&#21270;&#27493;&#39588;&#20013;&#32463;&#24120;&#34987;&#36829;&#21453;&#65288;&#21363;&#20351;&#22312;&#31616;&#21333;&#28151;&#21512;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SDE&#30340;&#35299;&#31639;&#22120;&#31867;&#65292;&#31216;&#20026;&#39640;&#26031;&#28151;&#21512;&#35299;&#31639;&#22120;&#65288;GMS&#65289;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35299;&#31639;&#22120;&#20272;&#35745;&#20102;&#21069;&#19977;&#38454;&#30697;&#65292;&#24182;&#20248;&#21270;&#20102;&#39640;&#26031;&#28151;&#21512;&#30340;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Recently, diffusion models have achieved great success in generative tasks. Sampling from diffusion models is equivalent to solving the reverse diffusion stochastic differential equations (SDEs) or the corresponding probability flow ordinary differential equations (ODEs). In comparison, SDE-based solvers can generate samples of higher quality and are suited for image translation tasks like stroke-based synthesis. During inference, however, existing SDE-based solvers are severely constrained by the efficiency-effectiveness dilemma. Our investigation suggests that this is because the Gaussian assumption in the reverse transition kernel is frequently violated (even in the case of simple mixture data) given a limited number of discretization steps. To overcome this limitation, we introduce a novel class of SDE-based solvers called \emph{Gaussian Mixture Solvers (GMS)} for diffusion models. Our solver estimates the first three-order moments and optimizes the parameters of a Gaussian mixture
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#25152;&#26399;&#26395;&#30340;&#26465;&#20214;&#37319;&#26679;&#34892;&#20026;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#37319;&#26679;&#26102;&#38388;&#27493;&#38271;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#23545;&#20110;&#24341;&#23548;&#35268;&#27169;&#30340;&#36873;&#25321;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00938</link><description>&lt;p&gt;
&#32553;&#23567;&#24046;&#36317;&#65306;&#35299;&#20915;&#20998;&#31867;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24046;&#24322;&#38382;&#39064;&#20197;&#23454;&#29616;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap: Addressing Discrepancies in Diffusion Model Training for Classifier-Free Guidance. (arXiv:2311.00938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#25152;&#26399;&#26395;&#30340;&#26465;&#20214;&#37319;&#26679;&#34892;&#20026;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#37319;&#26679;&#26102;&#38388;&#27493;&#38271;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#23545;&#20110;&#24341;&#23548;&#35268;&#27169;&#30340;&#36873;&#25321;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#23637;&#65292;&#20026;&#29983;&#25104;&#30340;&#23454;&#20363;&#36136;&#37327;&#35774;&#23450;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;&#26412;&#25991;&#26088;&#22312;&#24378;&#35843;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#19982;&#36825;&#20123;&#27169;&#22411;&#25152;&#26399;&#26395;&#30340;&#26465;&#20214;&#37319;&#26679;&#34892;&#20026;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#24322;&#12290;&#34429;&#28982;&#27969;&#34892;&#30340;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#25216;&#26415;&#25928;&#26524;&#19981;&#38169;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#32570;&#38519;&#12290;&#22312;&#24341;&#23548;&#35268;&#27169;&#21442;&#25968;$w$&#21462;&#36739;&#39640;&#20540;&#26102;&#65292;&#25105;&#20204;&#32463;&#24120;&#24471;&#21040;&#20998;&#24067;&#20043;&#22806;&#30340;&#26679;&#26412;&#21644;&#27169;&#24335;&#23849;&#28291;&#65292;&#32780;&#22312;$w$&#21462;&#36739;&#20302;&#20540;&#26102;&#65292;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#25152;&#26399;&#26395;&#30340;&#29305;&#24322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26356;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26356;&#22909;&#22320;&#23558;&#35757;&#32451;&#30446;&#26631;&#19982;&#37319;&#26679;&#34892;&#20026;&#23545;&#40784;&#12290;&#22312;CIFAR-10&#19978;&#30340;FID&#20998;&#25968;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20197;&#36739;&#23569;&#30340;&#37319;&#26679;&#26102;&#38388;&#27493;&#38271;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#23545;&#20110;&#24341;&#23548;&#35268;&#27169;$w$&#30340;&#36873;&#25321;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#22312;&#25552;&#20986;&#30340;&#25439;&#22833;&#19978;&#23545;&#31283;&#23450;&#24615;&#25193;&#25955;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#20379;e&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a pivotal advancement in generative models, setting new standards to the quality of the generated instances. In the current paper we aim to underscore a discrepancy between conventional training methods and the desired conditional sampling behavior of these models. While the prevalent classifier-free guidance technique works well, it's not without flaws. At higher values for the guidance scale parameter $w$, we often get out of distribution samples and mode collapse, whereas at lower values for $w$ we may not get the desired specificity. To address these challenges, we introduce an updated loss function that better aligns training objectives with sampling behaviors. Experimental validation with FID scores on CIFAR-10 elucidates our method's ability to produce higher quality samples with fewer sampling timesteps, and be more robust to the choice of guidance scale $w$. We also experiment with fine-tuning Stable Diffusion on the proposed loss, to provide e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#20013;&#20272;&#35745;&#22810;&#20010;&#24863;&#20852;&#36259;&#37327;&#30340;&#21453;&#20107;&#23454;&#32852;&#21512;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#21407;&#22987;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#19968;&#32500;&#28508;&#22312;&#23376;&#31354;&#38388;&#21644;&#21333;&#19968;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#22810;&#21464;&#37327;&#32467;&#26524;&#30340;&#30456;&#20851;&#32467;&#26500;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#21453;&#20107;&#23454;&#20998;&#24067;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2311.00927</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#21453;&#20107;&#23454;&#20998;&#24067;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Scalable Counterfactual Distribution Estimation in Multivariate Causal Models. (arXiv:2311.00927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00927
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#20013;&#20272;&#35745;&#22810;&#20010;&#24863;&#20852;&#36259;&#37327;&#30340;&#21453;&#20107;&#23454;&#32852;&#21512;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#21407;&#22987;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#19968;&#32500;&#28508;&#22312;&#23376;&#31354;&#38388;&#21644;&#21333;&#19968;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#22810;&#21464;&#37327;&#32467;&#26524;&#30340;&#30456;&#20851;&#32467;&#26500;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#21453;&#20107;&#23454;&#20998;&#24067;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#32463;&#20856;&#30340;&#24046;&#24322;&#24046;&#24322;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#25193;&#23637;&#30340;&#22810;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#20013;&#20272;&#35745;&#22810;&#20010;&#24863;&#20852;&#36259;&#37327;&#65288;&#20363;&#22914;&#32467;&#26524;&#65289;&#30340;&#21453;&#20107;&#23454;&#32852;&#21512;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#24573;&#30053;&#22810;&#21464;&#37327;&#32467;&#26524;&#21508;&#32500;&#24230;&#38388;&#30340;&#30456;&#20851;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#32500;&#24230;&#19978;&#32771;&#34385;&#21333;&#19968;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#32780;&#20135;&#29983;&#38169;&#35823;&#30340;&#21453;&#20107;&#23454;&#20998;&#24067;&#65307;&#35201;&#20040;&#22312;&#30452;&#25509;&#22788;&#29702;&#36825;&#31181;&#22810;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#26102;&#65292;&#22312;&#20013;&#31561;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#20943;&#36731;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26041;&#27861;&#26159;&#21033;&#29992;&#21407;&#22987;&#39640;&#32500;&#31354;&#38388;&#20013;&#40065;&#26834;&#30340;&#19968;&#32500;&#28508;&#22312;&#23376;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#21333;&#19968;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#22312;&#35813;&#31354;&#38388;&#19978;&#30340;&#39640;&#25928;&#20272;&#35745;&#12290;&#30001;&#20110;&#19968;&#32500;&#23376;&#31354;&#38388;&#30340;&#26500;&#24314;&#20351;&#29992;&#20102;&#26469;&#33258;&#25152;&#26377;&#32500;&#24230;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#30456;&#20851;&#32467;&#26500;&#24182;&#20135;&#29983;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#33391;&#22909;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating the counterfactual joint distribution of multiple quantities of interests (e.g., outcomes) in a multivariate causal model extended from the classical difference-in-difference design. Existing methods for this task either ignore the correlation structures among dimensions of the multivariate outcome by considering univariate causal models on each dimension separately and hence produce incorrect counterfactual distributions, or poorly scale even for moderate-size datasets when directly dealing with such multivariate causal model. We propose a method that alleviates both issues simultaneously by leveraging a robust latent one-dimensional subspace of the original high-dimension space and exploiting the efficient estimation from the univariate causal model on such space. Since the construction of the one-dimensional subspace uses information from all the dimensions, our method can capture the correlation structures and produce good estimates of the coun
&lt;/p&gt;</description></item><item><title>M2T2&#26159;&#19968;&#20010;&#29992;&#20110;&#29289;&#20307;&#25805;&#20316;&#30340;&#22810;&#20219;&#21153;&#36974;&#34109;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#25512;&#29702;&#22330;&#26223;&#30340;&#21407;&#22987;&#28857;&#20113;&#65292;&#21487;&#20197;&#31283;&#23450;&#22320;&#25552;&#20379;&#19981;&#21516;&#31867;&#22411;&#30340;&#20302;&#32423;&#25805;&#20316;&#65292;&#24182;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#20102;&#38646;-shot&#20223;&#30495;&#21040;&#23454;&#38469;&#30340;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2311.00926</link><description>&lt;p&gt;
M2T2:&#22810;&#20219;&#21153;&#36974;&#34109;&#21464;&#25442;&#22120;&#29992;&#20110;&#29289;&#20307;&#20013;&#24515;&#30340;&#25342;&#21462;&#21644;&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place. (arXiv:2311.00926v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00926
&lt;/p&gt;
&lt;p&gt;
M2T2&#26159;&#19968;&#20010;&#29992;&#20110;&#29289;&#20307;&#25805;&#20316;&#30340;&#22810;&#20219;&#21153;&#36974;&#34109;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#25512;&#29702;&#22330;&#26223;&#30340;&#21407;&#22987;&#28857;&#20113;&#65292;&#21487;&#20197;&#31283;&#23450;&#22320;&#25552;&#20379;&#19981;&#21516;&#31867;&#22411;&#30340;&#20302;&#32423;&#25805;&#20316;&#65292;&#24182;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#20102;&#38646;-shot&#20223;&#30495;&#21040;&#23454;&#38469;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#30340;&#20986;&#29616;&#65292;&#39640;&#32423;&#29289;&#20307;&#25805;&#20316;&#30340;&#39640;&#32423;&#20915;&#31574;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#36890;&#29992;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#35821;&#35328;&#25351;&#20196;&#35299;&#37322;&#22797;&#26434;&#20219;&#21153;&#65292;&#20294;&#30001;&#20110;&#20302;&#32423;&#21160;&#20316;&#21407;&#35821;&#30340;&#33021;&#21147;&#19981;&#36275;&#65292;&#23427;&#20204;&#36890;&#24120;&#38590;&#20197;&#25512;&#24191;&#21040;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23545;&#35937;&#12290;&#30456;&#21453;&#65292;&#29616;&#26377;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#22312;&#26410;&#30693;&#23545;&#35937;&#30340;&#20302;&#32423;&#25805;&#20316;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20165;&#36866;&#29992;&#20110;&#21333;&#19968;&#31867;&#22411;&#30340;&#21160;&#20316;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M2T2&#65292;&#23427;&#26159;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26434;&#20081;&#22330;&#26223;&#20013;&#31283;&#23450;&#22320;&#25552;&#20379;&#19981;&#21516;&#31867;&#22411;&#30340;&#20302;&#32423;&#25805;&#20316;&#12290;M2T2&#26159;&#19968;&#20010;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#22330;&#26223;&#30340;&#21407;&#22987;&#28857;&#20113;&#36827;&#34892;&#25512;&#29702;&#65292;&#25512;&#26029;&#25509;&#35302;&#28857;&#65292;&#24182;&#39044;&#27979;&#19981;&#21516;&#21160;&#20316;&#27169;&#24335;&#30340;&#26377;&#25928;&#22841;&#25345;&#22120;&#23039;&#21183;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;128K&#20010;&#22330;&#26223;&#30340;&#22823;&#35268;&#27169;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;M2T2&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#20102;&#38646;-shot&#20223;&#30495;&#21040;&#23454;&#38469;&#30340;&#36716;&#31227;&#65292;&#24182;&#36229;&#36807;&#20102;&#22522;&#20934;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of large language models and large-scale robotic datasets, there has been tremendous progress in high-level decision-making for object manipulation. These generic models are able to interpret complex tasks using language commands, but they often have difficulties generalizing to out-of-distribution objects due to the inability of low-level action primitives. In contrast, existing task-specific models excel in low-level manipulation of unknown objects, but only work for a single type of action. To bridge this gap, we present M2T2, a single model that supplies different types of low-level actions that work robustly on arbitrary objects in cluttered scenes. M2T2 is a transformer model which reasons about contact points and predicts valid gripper poses for different action modes given a raw point cloud of the scene. Trained on a large-scale synthetic dataset with 128K scenes, M2T2 achieves zero-shot sim2real transfer on the real robot, outperforming the baseline system with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#23631;&#34109;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34701;&#21512;&#35270;&#35273;&#21644;&#35302;&#35273;&#20449;&#24687;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#21333;&#19968;&#24863;&#23448;&#30340;&#36890;&#29992;&#21270;&#25805;&#25511;&#33021;&#21147;&#65292;&#24182;&#19988;&#36825;&#31181;&#22810;&#27169;&#24577;&#23398;&#20064;&#23545;&#20110;&#20165;&#35270;&#35273;&#31574;&#30053;&#20063;&#20855;&#26377;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2311.00924</link><description>&lt;p&gt;
&#35270;&#35273;&#21644;&#35302;&#35273;&#30340;&#34701;&#21512;&#23398;&#20064;&#65306;&#22522;&#20110;&#23631;&#34109;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#36890;&#29992;&#21270;&#25805;&#25511;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Power of the Senses: Generalizable Manipulation from Vision and Touch through Masked Multimodal Learning. (arXiv:2311.00924v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#23631;&#34109;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34701;&#21512;&#35270;&#35273;&#21644;&#35302;&#35273;&#20449;&#24687;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#21333;&#19968;&#24863;&#23448;&#30340;&#36890;&#29992;&#21270;&#25805;&#25511;&#33021;&#21147;&#65292;&#24182;&#19988;&#36825;&#31181;&#22810;&#27169;&#24577;&#23398;&#20064;&#23545;&#20110;&#20165;&#35270;&#35273;&#31574;&#30053;&#20063;&#20855;&#26377;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#22823;&#37096;&#20998;&#22522;&#26412;&#20219;&#21153;&#20013;&#20381;&#38752;&#24863;&#23448;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#23545;&#20110;&#38656;&#35201;&#29289;&#20307;&#25805;&#32437;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#26080;&#32541;&#19988;&#26377;&#25928;&#22320;&#21033;&#29992;&#35270;&#35273;&#21644;&#35302;&#35273;&#30340;&#20114;&#34917;&#24615;&#12290;&#26412;&#25991;&#21463;&#21040;&#36825;&#31181;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#26088;&#22312;&#25214;&#21040;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#22312;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#34701;&#21512;&#35270;&#35273;&#21644;&#35302;&#35273;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23631;&#34109;&#22810;&#27169;&#24577;&#23398;&#20064;&#65288;M3L&#65289;&#65292;&#36890;&#36807;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#65292;&#21516;&#26102;&#23398;&#20064;&#31574;&#30053;&#21644;&#35270;&#35273;-&#35302;&#35273;&#34920;&#36848;&#12290;&#20174;&#35270;&#35273;&#21644;&#35302;&#35273;&#20013;&#20849;&#21516;&#23398;&#20064;&#30340;&#34920;&#36848;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19988;&#23637;&#29616;&#20102;&#36229;&#36234;&#21333;&#19968;&#24863;&#23448;&#21333;&#29420;&#23454;&#29616;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#34920;&#36848;&#20063;&#23545;&#27979;&#35797;&#26102;&#30340;&#20165;&#35270;&#35273;&#31574;&#30053;&#26377;&#30410;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#27169;&#25311;&#29615;&#22659;&#65288;&#26426;&#22120;&#20154;&#25554;&#20837;&#12289;&#24320;&#38376;&#21644;&#28789;&#24039;&#30340;&#25163;&#19978;&#25805;&#20316;&#65289;&#20013;&#35780;&#20272;&#20102;M3L&#65292;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans rely on the synergy of their senses for most essential tasks. For tasks requiring object manipulation, we seamlessly and effectively exploit the complementarity of our senses of vision and touch. This paper draws inspiration from such capabilities and aims to find a systematic approach to fuse visual and tactile information in a reinforcement learning setting. We propose Masked Multimodal Learning (M3L), which jointly learns a policy and visual-tactile representations based on masked autoencoding. The representations jointly learned from vision and touch improve sample efficiency, and unlock generalization capabilities beyond those achievable through each of the senses separately. Remarkably, representations learned in a multimodal setting also benefit vision-only policies at test time. We evaluate M3L on three simulated environments with both visual and tactile observations: robotic insertion, door opening, and dexterous in-hand manipulation, demonstrating the benefits of learn
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#33719;&#21462;&#20813;&#36153;&#24037;&#20855;&#12289;&#35838;&#31243;&#22810;&#26679;&#24615;&#12289;&#20262;&#29702;&#21407;&#21017;&#30340;&#26126;&#30830;&#38416;&#36848;&#20197;&#21450;&#35299;&#20915;&#8220;&#40657;&#21283;&#23376;&#8221;&#24605;&#32500;&#31561;&#38382;&#39064;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#20005;&#26684;&#30340;&#25216;&#26415;&#22521;&#35757;&#12289;&#28165;&#26224;&#30340;&#25991;&#26723;&#21644;&#20262;&#29702;&#30417;&#25511;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00903</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#25945;&#32946;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;&#65306;&#19968;&#20010;&#28966;&#28857;&#23567;&#32452;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence Ethics Education in Cybersecurity: Challenges and Opportunities: a focus group report. (arXiv:2311.00903v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00903
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#33719;&#21462;&#20813;&#36153;&#24037;&#20855;&#12289;&#35838;&#31243;&#22810;&#26679;&#24615;&#12289;&#20262;&#29702;&#21407;&#21017;&#30340;&#26126;&#30830;&#38416;&#36848;&#20197;&#21450;&#35299;&#20915;&#8220;&#40657;&#21283;&#23376;&#8221;&#24605;&#32500;&#31561;&#38382;&#39064;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#20005;&#26684;&#30340;&#25216;&#26415;&#22521;&#35757;&#12289;&#28165;&#26224;&#30340;&#25991;&#26723;&#21644;&#20262;&#29702;&#30417;&#25511;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#20986;&#29616;&#24102;&#26469;&#20102;&#35768;&#22810;&#26426;&#36935;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#32593;&#32476;&#23433;&#20840;&#39640;&#32423;&#30740;&#31350;&#29983;&#30340;&#28966;&#28857;&#23567;&#32452;&#35752;&#35770;&#25581;&#31034;&#20102;&#25361;&#25112;&#21644;&#26426;&#36935;&#30340;&#28508;&#22312;&#28145;&#24230;&#21644;&#24191;&#24230;&#12290;&#37325;&#35201;&#38382;&#39064;&#21253;&#25324;&#33719;&#21462;&#24320;&#28304;&#25110;&#20813;&#36153;&#24037;&#20855;&#12289;&#25991;&#26723;&#12289;&#35838;&#31243;&#22810;&#26679;&#24615;&#20197;&#21450;&#23545;&#20154;&#24037;&#26234;&#33021;&#32593;&#32476;&#23433;&#20840;&#25945;&#32946;&#20262;&#29702;&#21407;&#21017;&#30340;&#26126;&#30830;&#38416;&#36848;&#12290;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#32593;&#32476;&#23433;&#20840;&#24037;&#20316;&#20013;&#30340;&#8220;&#40657;&#21283;&#23376;&#8221;&#24605;&#32500;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#21152;&#24378;&#23545;&#22522;&#30784;&#20154;&#24037;&#26234;&#33021;&#24037;&#20316;&#30340;&#28145;&#20837;&#21644;&#20808;&#34892;&#25945;&#32946;&#12290;&#31995;&#32479;&#24605;&#32771;&#21644;&#26377;&#25928;&#27807;&#36890;&#34987;&#35748;&#20026;&#26159;&#25945;&#32946;&#25913;&#36827;&#30340;&#30456;&#20851;&#39046;&#22495;&#12290;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#32773;&#21644;&#20174;&#19994;&#32773;&#38656;&#35201;&#36890;&#36807;&#23454;&#26045;&#20005;&#26684;&#30340;&#25216;&#26415;&#22521;&#35757;&#35838;&#31243;&#12289;&#28165;&#26224;&#30340;&#25991;&#26723;&#21644;&#20262;&#29702;&#30417;&#25511;&#20154;&#24037;&#26234;&#33021;&#30340;&#26694;&#26550;&#65292;&#24182;&#32467;&#21512;&#25209;&#21028;&#24615;&#21644;&#31995;&#32479;&#24605;&#32771;&#20197;&#21450;&#27807;&#36890;&#25216;&#33021;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of AI tools in cybersecurity creates many opportunities and uncertainties. A focus group with advanced graduate students in cybersecurity revealed the potential depth and breadth of the challenges and opportunities. The salient issues are access to open source or free tools, documentation, curricular diversity, and clear articulation of ethical principles for AI cybersecurity education. Confronting the "black box" mentality in AI cybersecurity work is also of the greatest importance, doubled by deeper and prior education in foundational AI work. Systems thinking and effective communication were considered relevant areas of educational improvement. Future AI educators and practitioners need to address these issues by implementing rigorous technical training curricula, clear documentation, and frameworks for ethically monitoring AI combined with critical and system's thinking and communication skills.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;SALLMS&#35780;&#20272;LLM&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#65292;&#25351;&#20986;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#30495;&#23454;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2311.00889</link><description>&lt;p&gt;
&#29983;&#25104;&#21644;&#39564;&#35777;&#65306;&#20351;&#29992;SALLMS&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code. (arXiv:2311.00889v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;SALLMS&#35780;&#20272;LLM&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#65292;&#25351;&#20986;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#30495;&#23454;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GitHub Copilot&#65292;ChatGPT&#31561;&#65289;&#22312;&#36719;&#20214;&#24037;&#31243;&#24072;&#30340;&#26085;&#24120;&#23454;&#36341;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#30830;&#20445;&#36825;&#20123;&#24037;&#20855;&#29983;&#25104;&#30340;&#20195;&#30721;&#19981;&#20165;&#21151;&#33021;&#27491;&#30830;&#65292;&#32780;&#19988;&#27809;&#26377;&#28431;&#27934;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;LLM&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#25552;&#39640;&#29983;&#20135;&#21147;&#65292;&#20294;&#20043;&#21069;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;LLM&#21487;&#33021;&#20250;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#12290;&#23384;&#22312;&#20004;&#20010;&#23548;&#33268;&#19981;&#23433;&#20840;&#20195;&#30721;&#29983;&#25104;&#30340;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#27809;&#26377;&#20805;&#20998;&#22320;&#20195;&#34920;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#30495;&#23454;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#36890;&#24120;&#22522;&#20110;&#31454;&#25216;&#32534;&#31243;&#25361;&#25112;&#25110;&#20197;&#35838;&#22530;&#24418;&#24335;&#20026;&#22522;&#30784;&#30340;&#32534;&#30721;&#20219;&#21153;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#23558;&#34987;&#38598;&#25104;&#21040;&#26356;&#22823;&#30340;&#20195;&#30721;&#24211;&#20013;&#65292;&#24341;&#20837;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30446;&#21069;&#32570;&#20047;&#19987;&#27880;&#20110;&#35780;&#20272;&#29983;&#25104;&#20195;&#30721;&#23433;&#20840;&#24615;&#30340;&#22522;&#20934;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#20027;&#35201;&#20391;&#37325;&#20110;&#21151;&#33021;&#24615;&#32780;&#24573;&#35270;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing popularity of Large Language Models (e.g. GitHub Copilot, ChatGPT, etc.) in software engineers' daily practices, it is important to ensure that the code generated by these tools is not only functionally correct but also free of vulnerabilities. Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code. There are two contributing factors to the insecure code generation. First, existing datasets used to evaluate Large Language Models (LLMs) do not adequately represent genuine software engineering tasks sensitive to security. Instead, they are often based on competitive programming challenges or classroom-type coding tasks. In real-world applications, the code produced is integrated into larger codebases, introducing potential security risks. There's a clear absence of benchmarks that focus on evaluating the security of the generated code. Second, existing evaluation metrics primarily focus on the func
&lt;/p&gt;</description></item><item><title>SCPO&#26159;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#25209;&#21028;&#22120;&#26469;&#30830;&#20445;&#36981;&#23432;&#23433;&#20840;&#32422;&#26463;&#24182;&#24179;&#34913;&#22238;&#25253;&#30340;&#26368;&#22823;&#21270;&#12290;</title><link>http://arxiv.org/abs/2311.00880</link><description>&lt;p&gt;
SCPO: &#23433;&#20840;&#25209;&#21028;&#31574;&#30053;&#20248;&#21270;&#19979;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SCPO: Safe Reinforcement Learning with Safety Critic Policy Optimization. (arXiv:2311.00880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00880
&lt;/p&gt;
&lt;p&gt;
SCPO&#26159;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#25209;&#21028;&#22120;&#26469;&#30830;&#20445;&#36981;&#23432;&#23433;&#20840;&#32422;&#26463;&#24182;&#24179;&#34913;&#22238;&#25253;&#30340;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23433;&#20840;&#24615;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#65292;&#24341;&#20837;&#20102;&#34920;&#31034;&#23433;&#20840;&#36829;&#35268;&#30340;&#29420;&#31435;&#25104;&#26412;&#20989;&#25968;&#12290;&#22312;CMDPs&#30340;&#35774;&#32622;&#20013;&#65292;&#20808;&#21069;&#30340;&#31639;&#27861;&#37319;&#29992;&#20102;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#25216;&#26415;&#23558;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#26080;&#32422;&#26463;&#23545;&#20598;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#33021;&#20250;&#19981;&#20934;&#30830;&#22320;&#39044;&#27979;&#19981;&#23433;&#20840;&#34892;&#20026;&#65292;&#23548;&#33268;&#22312;&#23398;&#20064;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#26102;&#20135;&#29983;&#19981;&#31283;&#23450;&#24615;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#23433;&#20840;&#25209;&#21028;&#31574;&#30053;&#20248;&#21270;&#65288;SCPO&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#23433;&#20840;&#25209;&#21028;&#22120;&#65292;&#19968;&#31181;&#36890;&#36807;&#36829;&#21453;&#23433;&#20840;&#32422;&#26463;&#32780;&#33719;&#24471;&#30340;&#22870;&#21169;&#34987;&#25269;&#28040;&#30340;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#24179;&#34913;&#22312;&#36981;&#23432;&#23433;&#20840;&#32422;&#26463;&#21644;&#26368;&#22823;&#21270;&#22238;&#25253;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating safety is an essential prerequisite for broadening the practical applications of reinforcement learning in real-world scenarios. To tackle this challenge, Constrained Markov Decision Processes (CMDPs) are leveraged, which introduce a distinct cost function representing safety violations. In CMDPs' settings, Lagrangian relaxation technique has been employed in previous algorithms to convert constrained optimization problems into unconstrained dual problems. However, these algorithms may inaccurately predict unsafe behavior, resulting in instability while learning the Lagrange multiplier. This study introduces a novel safe reinforcement learning algorithm, Safety Critic Policy Optimization (SCPO). In this study, we define the safety critic, a mechanism that nullifies rewards obtained through violating safety constraints. Furthermore, our theoretical analysis indicates that the proposed algorithm can automatically balance the trade-off between adhering to safety constraints 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#20248;&#20808;&#20307;&#39564;&#20013;&#32487;&#65292;&#20195;&#29702;&#20043;&#38388;&#20849;&#20139;&#26377;&#38480;&#25968;&#37327;&#30340;&#35757;&#32451;&#32463;&#39564;&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#22522;&#20934;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00865</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#20998;&#20139;&#20307;&#39564;&#25552;&#21319;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning. (arXiv:2311.00865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#20248;&#20808;&#20307;&#39564;&#20013;&#32487;&#65292;&#20195;&#29702;&#20043;&#38388;&#20849;&#20139;&#26377;&#38480;&#25968;&#37327;&#30340;&#35757;&#32451;&#32463;&#39564;&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#22522;&#20934;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#20248;&#20808;&#20307;&#39564;&#20013;&#32487;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#20998;&#20139;&#35757;&#32451;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#26377;&#38480;&#30340;&#36716;&#25442;&#19982;&#20854;&#20182;&#20195;&#29702;&#20849;&#20139;&#12290;&#20854;&#32972;&#21518;&#30340;&#30452;&#35273;&#26159;&#65292;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#30340;&#23569;&#37327;&#30456;&#20851;&#32463;&#39564;&#21487;&#20197;&#24110;&#21161;&#27599;&#20010;&#20195;&#29702;&#23398;&#20064;&#12290;&#19982;&#35768;&#22810;&#20854;&#20182;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#22522;&#26412;&#21435;&#20013;&#24515;&#21270;&#30340;&#35757;&#32451;&#65292;&#21482;&#38656;&#35201;&#20195;&#29702;&#20043;&#38388;&#30340;&#26377;&#38480;&#36890;&#20449;&#28192;&#36947;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#26080;&#20849;&#20139;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#21644;&#26368;&#20808;&#36827;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#20165;&#20998;&#20139;&#23569;&#37327;&#39640;&#24230;&#30456;&#20851;&#30340;&#32463;&#39564;&#20248;&#20110;&#20195;&#29702;&#20043;&#38388;&#30340;&#25152;&#26377;&#32463;&#39564;&#20849;&#20139;&#65292;&#32780;&#19988;&#36873;&#25321;&#24615;&#20307;&#39564;&#20849;&#20139;&#30340;&#24615;&#33021;&#25552;&#21319;&#22312;&#21508;&#31181;&#36229;&#21442;&#25968;&#21644;DQN&#21464;&#20307;&#20013;&#22343;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#21487;&#22312;https://github.com/mgerstgrasser/super&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel multi-agent RL approach, Selective Multi-Agent Prioritized Experience Relay, in which agents share with other agents a limited number of transitions they observe during training. The intuition behind this is that even a small number of relevant experiences from other agents could help each agent learn. Unlike many other multi-agent RL algorithms, this approach allows for largely decentralized training, requiring only a limited communication channel between agents. We show that our approach outperforms baseline no-sharing decentralized training and state-of-the art multi-agent RL algorithms. Further, sharing only a small number of highly relevant experiences outperforms sharing all experiences between agents, and the performance uplift from selective experience sharing is robust across a range of hyperparameters and DQN variants. A reference implementation of our algorithm is available at https://github.com/mgerstgrasser/super.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#19978;&#19979;&#25991;N-Gram&#30340;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#19978;&#19979;&#25991;&#31070;&#32463;&#20803;&#23384;&#22312;&#20110;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;N-Gram&#30005;&#36335;&#20013;&#65292;&#36825;&#34987;&#31216;&#20026;&#20108;&#38454;&#30005;&#36335;&#12290;&#22312;&#35757;&#32451;&#26089;&#26399;&#65292;&#36825;&#20004;&#20010;&#30005;&#36335;&#20855;&#26377;&#30456;&#20114;&#29420;&#31435;&#30340;&#21151;&#33021;&#65292;&#21482;&#26377;&#22312;&#23427;&#20204;&#37117;&#24418;&#25104;&#20043;&#21518;&#25165;&#33021;&#32452;&#21512;&#25104;&#19968;&#20010;&#20108;&#38454;&#30005;&#36335;&#12290;</title><link>http://arxiv.org/abs/2311.00863</link><description>&lt;p&gt;
&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;N-Gram&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Training Dynamics of Contextual N-Grams in Language Models. (arXiv:2311.00863v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00863
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#19978;&#19979;&#25991;N-Gram&#30340;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#19978;&#19979;&#25991;&#31070;&#32463;&#20803;&#23384;&#22312;&#20110;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;N-Gram&#30005;&#36335;&#20013;&#65292;&#36825;&#34987;&#31216;&#20026;&#20108;&#38454;&#30005;&#36335;&#12290;&#22312;&#35757;&#32451;&#26089;&#26399;&#65292;&#36825;&#20004;&#20010;&#30005;&#36335;&#20855;&#26377;&#30456;&#20114;&#29420;&#31435;&#30340;&#21151;&#33021;&#65292;&#21482;&#26377;&#22312;&#23427;&#20204;&#37117;&#24418;&#25104;&#20043;&#21518;&#25165;&#33021;&#32452;&#21512;&#25104;&#19968;&#20010;&#20108;&#38454;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19978;&#19979;&#25991;&#31070;&#32463;&#20803;&#65292;&#21253;&#25324;&#19968;&#20010;&#22312;&#24503;&#35821;&#25991;&#26412;&#19978;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31070;&#32463;&#20803;&#23384;&#22312;&#20110;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;N-Gram&#30005;&#36335;&#20013;&#65306;&#25105;&#20204;&#21457;&#29616;&#26202;&#26399;&#30340;&#31070;&#32463;&#20803;&#33021;&#22815;&#35782;&#21035;&#21644;&#32487;&#32493;&#24503;&#35821;&#25991;&#26412;&#20013;&#24120;&#35265;&#30340;N-Gram&#65292;&#20294;&#21482;&#26377;&#22312;&#24503;&#35821;&#31070;&#32463;&#20803;&#28608;&#27963;&#26102;&#25165;&#20250;&#34987;&#28608;&#27963;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#36825;&#20010;&#30005;&#36335;&#30340;&#24418;&#25104;&#65292;&#24182;&#21457;&#29616;&#36825;&#26159;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026;&#20108;&#38454;&#30005;&#36335;&#30340;&#31034;&#20363;&#12290;&#29305;&#21035;&#22320;&#65292;&#26089;&#26399;&#30340;&#35757;&#32451;&#20013;&#65292;&#32452;&#25104;N-Gram&#30005;&#36335;&#21644;&#26368;&#32456;&#24418;&#25104;&#24503;&#35821;&#31070;&#32463;&#20803;&#30340;&#24503;&#35821;&#26816;&#27979;&#30005;&#36335;&#20855;&#26377;&#29420;&#31435;&#30340;&#21151;&#33021;-&#24503;&#35821;&#26816;&#27979;&#30005;&#36335;&#37096;&#20998;&#36890;&#36807;&#24314;&#27169;&#24503;&#35821;&#21333;&#23383;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#24418;&#25104;&#65292;&#32780;N-Gram&#21017;&#36890;&#36807;&#25552;&#21319;&#36866;&#24403;&#30340;&#23436;&#25104;&#26469;&#24418;&#25104;&#12290;&#21482;&#26377;&#22312;&#36825;&#20004;&#20010;&#30005;&#36335;&#24050;&#32463;&#24418;&#25104;&#20043;&#21518;&#65292;&#23427;&#20204;&#25165;&#33021;&#32452;&#21512;&#25104;&#20026;&#19968;&#20010;&#20108;&#38454;&#30005;&#36335;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#20551;&#35774;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#19978;&#19979;&#25991;N-Gram&#30005;&#36335;&#36880;&#28176;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has shown the existence of contextual neurons in language models, including a neuron that activates on German text. We show that this neuron exists within a broader contextual n-gram circuit: we find late layer neurons which recognize and continue n-grams common in German text, but which only activate if the German neuron is active. We investigate the formation of this circuit throughout training and find that it is an example of what we call a second-order circuit. In particular, both the constituent n-gram circuits and the German detection circuit which culminates in the German neuron form with independent functions early in training - the German detection circuit partially through modeling German unigram statistics, and the n-grams by boosting appropriate completions. Only after both circuits have already formed do they fit together into a second-order circuit. Contrary to the hypotheses presented in prior work, we find that the contextual n-gram circuit forms gradually r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#26032;&#22411;&#33258;&#21160;&#24494;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#65292;&#23558;&#25152;&#38656;&#23548;&#25968;&#30340;&#22797;&#26434;&#24230;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00860</link><description>&lt;p&gt;
&#38646;&#22352;&#26631;&#31227;&#21160;&#65306;&#38024;&#23545;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#20248;&#21270;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero Coordinate Shift: Whetted Automatic Differentiation for Physics-informed Operator Learning. (arXiv:2311.00860v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#26032;&#22411;&#33258;&#21160;&#24494;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#65292;&#23558;&#25152;&#38656;&#23548;&#25968;&#30340;&#22797;&#26434;&#24230;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#65288;AD&#65289;&#26159;&#29289;&#29702;&#32422;&#26463;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#29992;&#20110;&#35745;&#31639;&#32593;&#32476;&#36755;&#20986;&#30456;&#23545;&#20110;&#22352;&#26631;&#30340;&#39640;&#38454;&#23548;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36731;&#37327;&#32423;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#38024;&#23545;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#33258;&#21160;&#24494;&#20998;&#65292;&#31216;&#20026;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#12290;ZCS&#24341;&#20837;&#20102;&#19968;&#20010;&#26631;&#37327;&#20540;&#30340;&#21494;&#21464;&#37327;&#65292;&#29992;&#20110;&#27599;&#20010;&#31354;&#38388;&#25110;&#26102;&#38388;&#32500;&#24230;&#65292;&#36890;&#36807;&#23558;&#25152;&#38656;&#23548;&#25968;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#24040;&#22823;&#25552;&#21319;&#12290;ZCS&#24456;&#23481;&#26131;&#22312;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#23454;&#29616;&#65307;&#25105;&#20204;&#20351;&#29992;DeepXDE&#36719;&#20214;&#21253;&#36827;&#34892;&#20102;&#33258;&#24049;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#20998;&#26512;&#21644;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#35757;&#32451;&#29289;&#29702;&#32422;&#26463;&#30340;DeepONets&#26469;&#35299;&#20915;&#26080;&#25968;&#25454;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ZCS&#19968;&#30452;&#36890;&#36807;&#38477;&#20302;GPU&#20869;&#23384;&#28040;&#32791;&#25552;&#20379;&#20102;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic differentiation (AD) is a critical step in physics-informed machine learning, required for computing the high-order derivatives of network output w.r.t. coordinates. In this paper, we present a novel and lightweight algorithm to conduct such AD for physics-informed operator learning, as we call the trick of Zero Coordinate Shift (ZCS). Instead of making all sampled coordinates leaf variables, ZCS introduces only one scalar-valued leaf variable for each spatial or temporal dimension, leading to a game-changing performance leap by simplifying the wanted derivatives from "many-roots-many-leaves" to "one-root-many-leaves". ZCS is easy to implement with current deep learning libraries; our own implementation is by extending the DeepXDE package. We carry out a comprehensive benchmark analysis and several case studies, training physics-informed DeepONets to solve partial differential equations (PDEs) without data. The results show that ZCS has persistently brought down GPU memory co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#25104;&#26412;&#32422;&#26463;&#30340;&#20998;&#24067;&#24335;&#25915;&#20987;&#20195;&#29702;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#26174;&#33879;&#38477;&#20302;&#21463;&#25915;&#20987;&#20195;&#29702;&#33719;&#24471;&#30340;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2311.00859</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#22522;&#20110;&#26368;&#20248;&#25104;&#26412;&#32422;&#26463;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Optimal Cost Constrained Adversarial Attacks For Multiple Agent Systems. (arXiv:2311.00859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#25104;&#26412;&#32422;&#26463;&#30340;&#20998;&#24067;&#24335;&#25915;&#20987;&#20195;&#29702;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#26174;&#33879;&#38477;&#20302;&#21463;&#25915;&#20987;&#20195;&#29702;&#33719;&#24471;&#30340;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#26368;&#20248;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;&#26159;&#24378;&#21270;&#23398;&#20064;&#21644;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#19968;&#20010;&#20840;&#30693;&#30340;&#21327;&#35843;&#32773;&#65288;&#25915;&#20987;&#32773;&#65289;&#65292;&#25915;&#20987;&#19981;&#21516;&#30340;&#25509;&#25910;&#32773;&#65288;&#21463;&#23475;&#32773;&#65289;&#20195;&#29702;&#20250;&#20135;&#29983;&#32479;&#19968;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#25915;&#20987;&#36890;&#24120;&#38656;&#35201;&#30001;&#20998;&#24067;&#24335;&#25915;&#20987;&#20195;&#29702;&#25191;&#34892;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#19968;&#20010;&#26080;&#38480;&#21046;&#30340;&#20013;&#24515;&#25915;&#20987;&#32773;&#12290;&#25105;&#20204;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#20351;&#29992;&#20998;&#24067;&#24335;&#25915;&#20987;&#20195;&#29702;&#36827;&#34892;&#26368;&#20248;&#30340;&#23545;&#25239;&#20195;&#29702;&#38388;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#27599;&#20010;&#19981;&#21516;&#30340;&#25915;&#20987;&#32773;-&#21463;&#23475;&#32773;&#23545;&#37117;&#26045;&#21152;&#19981;&#21516;&#30340;&#25104;&#26412;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27493;&#39588;&#20869;&#36827;&#34892;&#38745;&#24577;&#32422;&#26463;&#25915;&#20987;&#36164;&#28304;&#20998;&#37197;&#20248;&#21270;&#65292;&#24182;&#22312;&#27493;&#39588;&#38388;&#36827;&#34892;&#21160;&#24577;&#35268;&#21010;&#65292;&#20197;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#21463;&#25915;&#20987;&#20195;&#29702;&#25152;&#33719;&#24471;&#30340;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding optimal adversarial attack strategies is an important topic in reinforcement learning and the Markov decision process. Previous studies usually assume one all-knowing coordinator (attacker) for whom attacking different recipient (victim) agents incurs uniform costs. However, in reality, instead of using one limitless central attacker, the attacks often need to be performed by distributed attack agents. We formulate the problem of performing optimal adversarial agent-to-agent attacks using distributed attack agents, in which we impose distinct cost constraints on each different attacker-victim pair. We propose an optimal method integrating within-step static constrained attack-resource allocation optimization and between-step dynamic programming to achieve the optimal adversarial attack in a multi-agent system. Our numerical results show that the proposed attacks can significantly reduce the rewards received by the attacked agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#32654;&#22269;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#36827;&#34892;&#29305;&#23450;&#22320;&#21306;&#30340;&#20915;&#31574;&#20998;&#26512;&#65292;&#24182;&#32771;&#34385;&#21040;&#22320;&#21306;&#20043;&#38388;&#30340;&#27969;&#34892;&#30149;&#23398;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.00855</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#32654;&#22269;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Multi-Agent Reinforcement Learning Framework for Evaluating the U.S. Ending the HIV Epidemic Plan. (arXiv:2311.00855v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#32654;&#22269;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#36827;&#34892;&#29305;&#23450;&#22320;&#21306;&#30340;&#20915;&#31574;&#20998;&#26512;&#65292;&#24182;&#32771;&#34385;&#21040;&#22320;&#21306;&#20043;&#38388;&#30340;&#27969;&#34892;&#30149;&#23398;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20813;&#30123;&#32570;&#38519;&#30149;&#27602;&#65288;HIV&#65289;&#26159;&#32654;&#22269;&#30340;&#20027;&#35201;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#65292;&#27599;&#24180;&#26377;&#32422;1.2&#19975;&#20154;&#24863;&#26579;HIV&#65292;&#20854;&#20013;&#26377;3.5&#19975;&#20154;&#26159;&#26032;&#24863;&#26579;&#32773;&#12290;&#32654;&#22269;&#30340;HIV&#36127;&#25285;&#21644;&#25252;&#29702;&#25509;&#35302;&#23384;&#22312;&#30528;&#22320;&#29702;&#24046;&#24322;&#12290;2019&#24180;&#30340;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#26088;&#22312;&#21040;2030&#24180;&#23558;&#26032;&#24863;&#26579;&#20154;&#25968;&#20943;&#23569;90%&#65292;&#36890;&#36807;&#25552;&#39640;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#39044;&#38450;&#24178;&#39044;&#25514;&#26045;&#30340;&#35206;&#30422;&#29575;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;HIV&#39640;&#27969;&#34892;&#22320;&#21306;&#12290;&#30830;&#23450;&#26368;&#20339;&#24178;&#39044;&#25514;&#26045;&#30340;&#35268;&#27169;&#25193;&#22823;&#23558;&#26377;&#21161;&#20110;&#36164;&#28304;&#20998;&#37197;&#30340;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;HIV&#20915;&#31574;&#27169;&#22411;&#35201;&#20040;&#21482;&#35780;&#20272;&#29305;&#23450;&#22478;&#24066;&#65292;&#35201;&#20040;&#35780;&#20272;&#25972;&#20010;&#22269;&#23478;&#20154;&#21475;&#65292;&#24573;&#35270;&#22320;&#26041;&#30340;&#30456;&#20114;&#20316;&#29992;&#25110;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#36827;&#34892;&#29305;&#23450;&#22320;&#21306;&#30340;&#20915;&#31574;&#20998;&#26512;&#65292;&#21516;&#26102;&#32771;&#34385;&#36328;&#22320;&#21306;&#30340;&#27969;&#34892;&#30149;&#20114;&#21160;&#12290;&#22312;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Human immunodeficiency virus (HIV) is a major public health concern in the United States, with about 1.2 million people living with HIV and 35,000 newly infected each year. There are considerable geographical disparities in HIV burden and care access across the U.S. The 2019 Ending the HIV Epidemic (EHE) initiative aims to reduce new infections by 90% by 2030, by improving coverage of diagnoses, treatment, and prevention interventions and prioritizing jurisdictions with high HIV prevalence. Identifying optimal scale-up of intervention combinations will help inform resource allocation. Existing HIV decision analytic models either evaluate specific cities or the overall national population, thus overlooking jurisdictional interactions or differences. In this paper, we propose a multi-agent reinforcement learning (MARL) model, that enables jurisdiction-specific decision analyses but in an environment with cross-jurisdictional epidemiological interactions. In experimental analyses, conduct
&lt;/p&gt;</description></item><item><title>healthAIChain&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#25913;&#36827;AI&#21307;&#30103;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#35299;&#20915;&#20102;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#20013;&#19982;&#23433;&#20840;&#24615;&#65292;&#24615;&#33021;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00842</link><description>&lt;p&gt;
healthAIChain: &#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#25913;&#36827;AI&#21307;&#30103;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
healthAIChain: Improving security and safety using Blockchain Technology applications in AI-based healthcare systems. (arXiv:2311.00842v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00842
&lt;/p&gt;
&lt;p&gt;
healthAIChain&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#25913;&#36827;AI&#21307;&#30103;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#35299;&#20915;&#20102;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#20013;&#19982;&#23433;&#20840;&#24615;&#65292;&#24615;&#33021;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#20316;&#20026;&#19968;&#31181;&#25968;&#23383;&#20998;&#31867;&#24080;&#65292;&#29992;&#20110;&#35760;&#24405;&#25968;&#23383;&#20132;&#26131;&#21644;&#20854;&#20182;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#23433;&#20840;&#19988;&#21435;&#20013;&#24515;&#21270;&#30340;&#25216;&#26415;&#12290;&#27599;&#22825;&#20840;&#29699;&#25968;&#23383;&#20154;&#21475;&#30340;&#22686;&#38271;&#23545;&#22312;&#32447;&#25968;&#25454;&#65292;&#21253;&#25324;&#21307;&#30103;&#21644;&#24739;&#32773;&#25968;&#25454;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#21306;&#22359;&#38142;&#25216;&#26415;&#22312;&#21307;&#30103;&#34892;&#19994;&#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#26368;&#39640;&#23433;&#20840;&#26631;&#20934;&#30340;&#21516;&#26102;&#20419;&#36827;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#24320;&#25918;&#24615;&#65292;&#29992;&#20110;&#21307;&#30103;&#24739;&#32773;&#30340;&#20851;&#38190;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#35760;&#24405;&#20445;&#23384;&#65292;&#20351;&#25968;&#23383;&#36164;&#20135;&#19981;&#21487;&#25913;&#21464;&#19988;&#36879;&#26126;&#65292;&#36890;&#36807;&#21152;&#23494;&#21704;&#24076;&#21644;&#21435;&#20013;&#24515;&#21270;&#32593;&#32476;&#12290;&#35813;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;AI&#30340;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#20013;&#23454;&#26045;&#21306;&#22359;&#38142;&#25152;&#24102;&#26469;&#30340;&#23433;&#20840;&#24615;&#21644;&#23433;&#20840;&#24615;&#25913;&#36827;&#12290;&#21306;&#22359;&#38142;&#21551;&#29992;&#30340;AI&#35299;&#20915;&#20102;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#20013;&#19982;&#23433;&#20840;&#24615;&#65292;&#24615;&#33021;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#30456;&#20851;&#30340;&#29616;&#26377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchain as a digital ledger for keeping records of digital transactions and other information, it is secure and decentralized technology. The globally growing number of digital population every day possesses a significant threat to online data including the medical and patients data. After bitcoin, blockchain technology has emerged into a general-purpose technology with applications in medical industries and healthcare. Blockchain can promote highly configurable openness while retaining the highest security standards for critical data of medical patients. Referred to as distributed record keeping for healthcare systems which makes digital assets unalterable and transparent via a cryptographic hash and decentralized network. The study delves into the security and safety improvement associated with implementing blockchain in AI-based healthcare systems. Blockchain-enabled AI tackles the existing issues related to security, performance efficiencies, and safety in healthcare systems. We
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#24847;&#26102;&#38388;&#25552;&#28860;&#26041;&#27861;&#65292;&#32467;&#21512;&#24658;&#23450;&#26102;&#38388;&#21160;&#20316;&#35268;&#21010;&#22120;&#65288;CTMP&#65289;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#25913;&#21892;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#20154;&#31995;&#32479;&#25317;&#26377;&#30340;&#22810;&#20313;&#35745;&#21010;&#26102;&#38388;&#26469;&#23436;&#21892;&#36816;&#21160;&#35268;&#21010;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00837</link><description>&lt;p&gt;
&#20219;&#24847;&#26102;&#38388;&#25552;&#28860;&#30340;&#24658;&#23450;&#26102;&#38388;&#36816;&#21160;&#35268;&#21010;&#21644;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Constant-time Motion Planning with Anytime Refinement for Manipulation. (arXiv:2311.00837v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00837
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#24847;&#26102;&#38388;&#25552;&#28860;&#26041;&#27861;&#65292;&#32467;&#21512;&#24658;&#23450;&#26102;&#38388;&#21160;&#20316;&#35268;&#21010;&#22120;&#65288;CTMP&#65289;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#25913;&#21892;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#20154;&#31995;&#32479;&#25317;&#26377;&#30340;&#22810;&#20313;&#35745;&#21010;&#26102;&#38388;&#26469;&#23436;&#21892;&#36816;&#21160;&#35268;&#21010;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#23545;&#20110;&#26410;&#26469;&#30340;&#33258;&#20027;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#20854;&#33258;&#20027;&#24615;&#30340;&#20449;&#20219;&#26377;&#38480;&#65292;&#23558;&#20854;&#38480;&#21046;&#20026;&#21018;&#24615;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#31995;&#32479;&#12290;&#25805;&#20316;&#22120;&#30340;&#22797;&#26434;&#37197;&#32622;&#31354;&#38388;&#65292;&#20197;&#21450;&#36991;&#38556;&#21644;&#32422;&#26463;&#28385;&#36275;&#30340;&#25361;&#25112;&#32463;&#24120;&#20351;&#24471;&#36816;&#21160;&#35268;&#21010;&#25104;&#20026;&#23454;&#29616;&#21487;&#38752;&#21644;&#36866;&#24212;&#24615;&#33258;&#20027;&#24615;&#30340;&#29942;&#39048;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#19968;&#31867;&#24658;&#23450;&#26102;&#38388;&#36816;&#21160;&#35268;&#21010;&#22120;&#65288;CTMP&#65289;&#12290;&#36825;&#20123;&#35268;&#21010;&#22120;&#21033;&#29992;&#39044;&#22788;&#29702;&#38454;&#27573;&#35745;&#31639;&#25968;&#25454;&#32467;&#26500;&#65292;&#33021;&#22815;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#26102;&#38388;&#38480;&#21046;&#20869;&#29983;&#25104;&#36816;&#21160;&#35268;&#21010;&#65292;&#34429;&#28982;&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20248;&#35299;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#35768;&#22810;&#26102;&#38388;&#20851;&#38190;&#30340;&#20219;&#21153;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#20154;&#31995;&#32479;&#36890;&#24120;&#26377;&#27604;CTMP&#25152;&#38656;&#30340;&#22312;&#32447;&#37096;&#20998;&#26356;&#22810;&#30340;&#35745;&#21010;&#26102;&#38388;&#65292;&#36825;&#20123;&#26102;&#38388;&#21487;&#20197;&#29992;&#26469;&#25913;&#21892;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;CTMP&#20013;&#19982;&#20219;&#24847;&#26102;&#38388;&#25552;&#28860;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic manipulators are essential for future autonomous systems, yet limited trust in their autonomy has confined them to rigid, task-specific systems. The intricate configuration space of manipulators, coupled with the challenges of obstacle avoidance and constraint satisfaction, often makes motion planning the bottleneck for achieving reliable and adaptable autonomy. Recently, a class of constant-time motion planners (CTMP) was introduced. These planners employ a preprocessing phase to compute data structures that enable online planning provably guarantee the ability to generate motion plans, potentially sub-optimal, within a user defined time bound. This framework has been demonstrated to be effective in a number of time-critical tasks. However, robotic systems often have more time allotted for planning than the online portion of CTMP requires, time that can be used to improve the solution. To this end, we propose an anytime refinement approach that works in combination with CTMP a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#21253;&#25324;&#26102;&#38388;&#29305;&#24449;&#30340;&#22810;&#27969;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#35270;&#39057;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#21253;&#25324;&#35270;&#39057;&#21644;&#26102;&#38388;&#27969;&#65292;&#20943;&#23569;&#20102;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.00800</link><description>&lt;p&gt;
&#36229;&#36234;&#38745;&#27490;&#22270;&#20687;&#65306;&#24378;&#22823;&#30340;&#22810;&#27969;&#26102;&#31354;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Beyond Still Images: Robust Multi-Stream Spatiotemporal Networks. (arXiv:2311.00800v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#21253;&#25324;&#26102;&#38388;&#29305;&#24449;&#30340;&#22810;&#27969;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#35270;&#39057;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#21253;&#25324;&#35270;&#39057;&#21644;&#26102;&#38388;&#27969;&#65292;&#20943;&#23569;&#20102;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35270;&#35273;&#30340;&#19968;&#20010;&#23450;&#20041;&#29305;&#24449;&#26159;&#20854;&#33021;&#22815;&#25215;&#21463;&#21508;&#31181;&#36755;&#20837;&#21464;&#21270;&#65292;&#20174;&#32780;&#21019;&#24314;&#21608;&#22260;&#29615;&#22659;&#30340;&#19981;&#21464;&#34920;&#31034;&#12290;&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#26576;&#20123;&#24418;&#24335;&#30340;&#31354;&#38388;&#36755;&#20837;&#21464;&#21270;&#20855;&#26377;&#38887;&#24615;&#65292;&#20294;&#31354;&#38388;&#21644;&#26102;&#38388;&#26041;&#38754;&#30340;&#20462;&#25913;&#21487;&#20197;&#26174;&#30528;&#24433;&#21709;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#35270;&#39057;&#20869;&#23481;&#30340;&#34920;&#31034;&#12290;&#21463;&#33258;&#28982;&#35270;&#35273;&#23545;&#36755;&#20837;&#21464;&#21270;&#30340;&#38887;&#24615;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27969;&#27169;&#22411;&#26469;&#25506;&#32034;&#20854;&#36890;&#36807;&#21253;&#21547;&#26102;&#38388;&#29305;&#24449;&#26469;&#22788;&#29702;&#26102;&#31354;&#21464;&#21270;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24341;&#20837;&#19968;&#20010;&#32463;&#36807;&#35270;&#39057;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#22810;&#26679;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#26102;&#38388;&#29305;&#24449;&#22312;&#19981;&#21464;&#35782;&#21035;&#20013;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35757;&#32451;&#26102;&#21253;&#25324;&#35270;&#39057;&#21644;&#26102;&#38388;&#27969;&#21487;&#20197;&#23558;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;mAP&#19979;&#38477;&#29575;&#20943;&#23569;1.36%&#21644;3.14%&#12290;
&lt;/p&gt;
&lt;p&gt;
A defining characteristic of natural vision is its ability to withstand a variety of input alterations, resulting in the creation of an invariant representation of the surroundings. While convolutional neural networks exhibit resilience to certain forms of spatial input variation, modifications in the spatial and temporal aspects can significantly affect the representations of video content in deep neural networks. Inspired by the resilience of natural vision to input variations, we employ a simple multi-stream model to explore its potential to address spatiotemporal changes by including temporal features. Our primary goal is to introduce a video-trained model and evaluate its robustness to diverse image and video inputs, with a particular focus on exploring the role of temporal features in invariant recognition. Results show that including videos and the temporal stream during training mitigates the decline in accuracy and mAP in image and video understanding tasks by 1.36% and 3.14%,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#26131;&#24863;-&#24863;&#26579;-&#26131;&#24863;&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#38598;&#20307;&#21160;&#21147;&#23398;&#12290;&#20182;&#20204;&#35782;&#21035;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#25551;&#36848;&#32593;&#32476;&#30340;&#28436;&#21270;&#34892;&#20026;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#32597;&#35265;&#30340;&#22823;&#24133;&#24230;&#38598;&#20307;&#25391;&#33633;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2311.00797</link><description>&lt;p&gt;
&#28436;&#21270;&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#65306;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#25968;&#25454;&#39537;&#21160;&#26377;&#25928;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling. (arXiv:2311.00797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#26131;&#24863;-&#24863;&#26579;-&#26131;&#24863;&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#38598;&#20307;&#21160;&#21147;&#23398;&#12290;&#20182;&#20204;&#35782;&#21035;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#25551;&#36848;&#32593;&#32476;&#30340;&#28436;&#21270;&#34892;&#20026;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#32597;&#35265;&#30340;&#22823;&#24133;&#24230;&#38598;&#20307;&#25391;&#33633;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#26041;&#24335;&#30740;&#31350;&#33258;&#36866;&#24212;&#26131;&#24863;-&#24863;&#26579;-&#26131;&#24863;(SIS)&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#38598;&#20307;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#36890;&#36807;&#21463;&#25968;&#20540;&#38543;&#26426;&#31215;&#20998;&#22120;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;ResNet&#26550;&#26500;&#65292;&#35782;&#21035;&#20986;&#19968;&#20010;&#21442;&#25968;&#30456;&#20851;&#30340;&#22522;&#20110;&#29289;&#29702;&#24847;&#20041;&#30340;&#31895;&#31890;&#24230;&#22343;&#22330;&#21464;&#37327;&#30340;&#26377;&#25928;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(eSDE)&#12290;&#25105;&#20204;&#22522;&#20110;eSDE&#30340;&#30830;&#23450;&#20559;&#31227;&#39033;&#26500;&#24314;&#20102;&#19968;&#20010;&#36817;&#20284;&#30340;&#26377;&#25928;&#20998;&#23700;&#22270;&#65292;&#24182;&#23558;&#20854;&#19982;&#22343;&#22330;SIS&#27169;&#22411;&#30340;&#20998;&#23700;&#22270;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#28436;&#21270;&#32593;&#32476;&#30340;&#26377;&#25928;SIS&#21160;&#21147;&#23398;&#20013;&#30340;&#27425;&#20020;&#30028;Hopf&#20998;&#23700;&#65292;&#23427;&#24341;&#36215;&#20102;&#20020;&#30028;&#36716;&#25240;&#34892;&#20026;&#65307;&#36825;&#34920;&#29616;&#20026;&#22823;&#24133;&#24230;&#30340;&#38598;&#20307;&#25391;&#33633;&#65292;&#23427;&#20204;&#20174;(&#22122;&#22768;&#30340;)&#22266;&#23450;&#29366;&#24577;&#30340;&#37051;&#22495;&#20013;&#33258;&#21457;&#22320;&#12289;&#32597;&#35265;&#22320;&#20986;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#22797;&#30340;&#26292;&#21147;&#27169;&#25311;&#21644;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;&#25968;&#23398;&#24037;&#20855;&#30740;&#31350;&#20102;&#36825;&#20123;&#32597;&#35265;&#20107;&#20214;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the tipping point collective dynamics of an adaptive susceptible-infected-susceptible (SIS) epidemiological network in a data-driven, machine learning-assisted manner. We identify a parameter-dependent effective stochastic differential equation (eSDE) in terms of physically meaningful coarse mean-field variables through a deep-learning ResNet architecture inspired by numerical stochastic integrators. We construct an approximate effective bifurcation diagram based on the identified drift term of the eSDE and contrast it with the mean-field SIS model bifurcation diagram. We observe a subcritical Hopf bifurcation in the evolving network's effective SIS dynamics, that causes the tipping point behavior; this takes the form of large amplitude collective oscillations that spontaneously -- yet rarely -arise from the neighborhood of a (noisy) stationary state. We study the statistics of these rare events both through repeated brute force simulations and by using established mathemati
&lt;/p&gt;</description></item><item><title>SAGE&#26694;&#26550;&#36890;&#36807;&#26367;&#25442;&#25163;&#21160;&#23450;&#20041;&#30340;&#25512;&#29702;&#36923;&#36753;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#23454;&#38469;&#25191;&#34892;&#30340;&#26234;&#33021;&#23478;&#23621;&#21161;&#25163;&#65292;&#25552;&#39640;&#20102;&#28789;&#27963;&#24615;&#12290;&#23427;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#19982;&#35774;&#22791;&#20132;&#20114;&#65292;&#30417;&#35270;&#35774;&#22791;&#65292;&#24182;&#29702;&#35299;&#33258;&#28982;&#30340;&#35774;&#22791;&#24341;&#29992;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;SAGE&#22312;43&#20010;&#26234;&#33021;&#23478;&#23621;&#20219;&#21153;&#20013;&#25104;&#21151;&#23436;&#25104;&#20102;23&#20010;&#20219;&#21153;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;LLM&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2311.00772</link><description>&lt;p&gt;
SAGE: &#20855;&#26377;&#22522;&#20110;&#23454;&#38469;&#25191;&#34892;&#30340;&#26234;&#33021;&#23478;&#23621;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
SAGE: Smart home Agent with Grounded Execution. (arXiv:2311.00772v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00772
&lt;/p&gt;
&lt;p&gt;
SAGE&#26694;&#26550;&#36890;&#36807;&#26367;&#25442;&#25163;&#21160;&#23450;&#20041;&#30340;&#25512;&#29702;&#36923;&#36753;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#23454;&#38469;&#25191;&#34892;&#30340;&#26234;&#33021;&#23478;&#23621;&#21161;&#25163;&#65292;&#25552;&#39640;&#20102;&#28789;&#27963;&#24615;&#12290;&#23427;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#19982;&#35774;&#22791;&#20132;&#20114;&#65292;&#30417;&#35270;&#35774;&#22791;&#65292;&#24182;&#29702;&#35299;&#33258;&#28982;&#30340;&#35774;&#22791;&#24341;&#29992;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;SAGE&#22312;43&#20010;&#26234;&#33021;&#23478;&#23621;&#20219;&#21153;&#20013;&#25104;&#21151;&#23436;&#25104;&#20102;23&#20010;&#20219;&#21153;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;LLM&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SAGE&#65288;&#20855;&#26377;&#22522;&#20110;&#23454;&#38469;&#25191;&#34892;&#30340;&#26234;&#33021;&#23478;&#23621;&#21161;&#25163;&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#25163;&#21160;&#23450;&#20041;&#30340;&#25512;&#29702;&#36923;&#36753;&#26367;&#25442;&#20026;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#20195;&#29702;&#31995;&#32479;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#26234;&#33021;&#23478;&#23621;&#21161;&#25163;&#30340;&#28789;&#27963;&#24615;&#12290;SAGE&#36890;&#36807;&#21327;&#35843;&#19968;&#31995;&#21015;&#24037;&#20855;&#25972;&#21512;&#29992;&#25143;&#20559;&#22909;&#12289;&#35774;&#22791;&#29366;&#24577;&#21644;&#22806;&#37096;&#22240;&#32032;&#65288;&#22914;&#22825;&#27668;&#21644;&#30005;&#35270;&#33410;&#30446;&#34920;&#65289;&#12290;SAGE&#30340;&#21151;&#33021;&#21253;&#25324;&#20174;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#20013;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#36890;&#36807;&#38405;&#35835;&#35774;&#22791;&#30340;API&#25991;&#26723;&#19982;&#35774;&#22791;&#20132;&#20114;&#65292;&#32534;&#20889;&#20195;&#30721;&#20197;&#25345;&#32493;&#30417;&#35270;&#35774;&#22791;&#65292;&#24182;&#29702;&#35299;&#33258;&#28982;&#30340;&#35774;&#22791;&#24341;&#29992;&#12290;&#20026;&#20102;&#35780;&#20272;SAGE&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;43&#20010;&#39640;&#24230;&#25361;&#25112;&#30340;&#26234;&#33021;&#23478;&#23621;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;SAGE&#25104;&#21151;&#23436;&#25104;&#20102;23&#20010;&#20219;&#21153;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;LLM&#22522;&#20934;&#65288;5/43&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces SAGE (Smart home Agent with Grounded Execution), a framework designed to maximize the flexibility of smart home assistants by replacing manually-defined inference logic with an LLM-powered autonomous agent system. SAGE integrates information about user preferences, device states, and external factors (such as weather and TV schedules) through the orchestration of a collection of tools. SAGE's capabilities include learning user preferences from natural-language utterances, interacting with devices by reading their API documentation, writing code to continuously monitor devices, and understanding natural device references. To evaluate SAGE, we develop a benchmark of 43 highly challenging smart home tasks, where SAGE successfully achieves 23 tasks, significantly outperforming existing LLM-enabled baselines (5/43).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Praxis&#25968;&#25454;&#38598;&#19978;&#30340;&#25163;&#21183;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20307;&#20851;&#33410;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#25163;&#21183;&#20998;&#31867;&#22120;&#27169;&#22411;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;&#36890;&#36807;&#20351;&#29992;&#31383;&#21475;&#25216;&#26415;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#32467;&#26500;&#65292;&#20165;&#20351;&#29992;&#36523;&#20307;&#20851;&#33410;&#25968;&#25454;&#23601;&#36798;&#21040;&#20102;70.8%&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#19968;&#31181;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#26041;&#27861;&#26469;&#25552;&#21462;&#21644;&#20998;&#26512;&#20851;&#33410;&#36816;&#21160;&#29305;&#24449;&#20197;&#35782;&#21035;&#25163;&#21183;&#12290;</title><link>http://arxiv.org/abs/2311.00767</link><description>&lt;p&gt;
&#22312;Praxis&#25968;&#25454;&#38598;&#19978;&#30340;&#25163;&#21183;&#20998;&#31867;&#65306;&#20197;&#31934;&#30830;&#24615;&#20026;&#20195;&#20215;
&lt;/p&gt;
&lt;p&gt;
Hand Gesture Classification on Praxis Dataset: Trading Accuracy for Expense. (arXiv:2311.00767v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Praxis&#25968;&#25454;&#38598;&#19978;&#30340;&#25163;&#21183;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20307;&#20851;&#33410;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#25163;&#21183;&#20998;&#31867;&#22120;&#27169;&#22411;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;&#36890;&#36807;&#20351;&#29992;&#31383;&#21475;&#25216;&#26415;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#32467;&#26500;&#65292;&#20165;&#20351;&#29992;&#36523;&#20307;&#20851;&#33410;&#25968;&#25454;&#23601;&#36798;&#21040;&#20102;70.8%&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#19968;&#31181;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#26041;&#27861;&#26469;&#25552;&#21462;&#21644;&#20998;&#26512;&#20851;&#33410;&#36816;&#21160;&#29305;&#24449;&#20197;&#35782;&#21035;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20381;&#38752;&#36890;&#36807;RGB-Depth&#20256;&#24863;&#22120;&#35760;&#24405;&#30340;&#25277;&#35937;&#21270;&#30340;&#8220;&#39592;&#39612;&#8221;&#25968;&#25454;&#30340;&#25163;&#21183;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#20851;&#27880;Praxis&#25968;&#25454;&#38598;&#20013;&#30001;&#36523;&#20307;&#20851;&#33410;&#22352;&#26631;&#34920;&#31034;&#30340;&#8220;&#39592;&#39612;&#8221;&#25968;&#25454;&#12290;Praxis&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#24739;&#26377;&#22823;&#33041;&#30382;&#23618;&#30149;&#29702;&#65288;&#22914;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65289;&#30340;&#24739;&#32773;&#22312;&#20020;&#24202;&#21307;&#24072;&#25351;&#23548;&#19979;&#36827;&#34892;Praxis&#27979;&#35797;&#30340;&#35760;&#24405;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#25163;&#21183;&#20998;&#31867;&#22120;&#22312;Praxis&#25968;&#25454;&#38598;&#19978;&#27604;&#20043;&#21069;&#25552;&#20986;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;&#36523;&#20307;&#20851;&#33410;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#20197;&#19987;&#38376;&#29992;&#20110;&#25163;&#21183;&#35782;&#21035;&#30340;&#21387;&#32553;&#25968;&#25454;&#24418;&#24335;&#12290;&#36890;&#36807;&#20351;&#29992;&#31383;&#21475;&#25216;&#26415;&#19982;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#31561;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#32467;&#21512;&#65292;&#25105;&#20204;&#20165;&#20351;&#29992;&#36523;&#20307;&#20851;&#33410;&#25968;&#25454;&#23601;&#36798;&#21040;&#20102;70.8%&#30340;&#25972;&#20307;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#31181;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#26041;&#27861;&#65292;&#20197;&#25552;&#21462;&#21644;&#20998;&#26512;&#20851;&#33410;&#30340;&#36816;&#21160;&#29305;&#24449;&#20197;&#35782;&#21035;&#27491;&#22312;&#25191;&#34892;&#30340;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate hand gesture classifiers that rely upon the abstracted 'skeletal' data recorded using the RGB-Depth sensor. We focus on 'skeletal' data represented by the body joint coordinates, from the Praxis dataset. The PRAXIS dataset contains recordings of patients with cortical pathologies such as Alzheimer's disease, performing a Praxis test under the direction of a clinician. In this paper, we propose hand gesture classifiers that are more effective with the PRAXIS dataset than previously proposed models. Body joint data offers a compressed form of data that can be analyzed specifically for hand gesture recognition. Using a combination of windowing techniques with deep learning architecture such as a Recurrent Neural Network (RNN), we achieved an overall accuracy of 70.8% using only body joint data. In addition, we investigated a long-short-term-memory (LSTM) to extract and analyze the movement of the joints through time to recognize the hand gestures being perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#19968;&#31181;&#35774;&#35745;&#31574;&#30053;&#26469;&#21046;&#20316;&#36866;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#30340;&#19987;&#29992;&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#36825;&#20123;&#24037;&#20855;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#32437;&#12290;&#36825;&#21487;&#20197;&#35299;&#38145;&#26426;&#22120;&#20154;&#30340;&#39069;&#22806;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00754</link><description>&lt;p&gt;
&#23398;&#20064;&#35774;&#35745;&#21644;&#20351;&#29992;&#26426;&#22120;&#20154;&#25805;&#32437;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Learning to Design and Use Tools for Robotic Manipulation. (arXiv:2311.00754v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#19968;&#31181;&#35774;&#35745;&#31574;&#30053;&#26469;&#21046;&#20316;&#36866;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#30340;&#19987;&#29992;&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#36825;&#20123;&#24037;&#20855;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#32437;&#12290;&#36825;&#21487;&#20197;&#35299;&#38145;&#26426;&#22120;&#20154;&#30340;&#39069;&#22806;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38754;&#20020;&#33258;&#36523;&#24418;&#24577;&#38480;&#21046;&#26102;&#65292;&#20154;&#31867;&#21644;&#26576;&#20123;&#21160;&#29289;&#31181;&#31867;&#20855;&#26377;&#20351;&#29992;&#29615;&#22659;&#20013;&#30340;&#29289;&#20307;&#26469;&#23436;&#25104;&#21407;&#26412;&#19981;&#21487;&#33021;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#26426;&#22120;&#20154;&#21487;&#33021;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#35299;&#38145;&#19968;&#31995;&#21015;&#39069;&#22806;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26469;&#32852;&#21512;&#20248;&#21270;&#24418;&#24577;&#21644;&#25511;&#21046;&#30340;&#25216;&#26415;&#22312;&#35774;&#35745;&#31227;&#21160;&#26426;&#22120;&#20154;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;&#36755;&#20986;&#19968;&#20010;&#21333;&#19968;&#30340;&#24418;&#24577;&#23545;&#20110;&#31227;&#21160;&#26469;&#35828;&#26159;&#26377;&#24847;&#20041;&#30340;&#65292;&#20294;&#26159;&#25805;&#32437;&#28041;&#21450;&#21040;&#26681;&#25454;&#25163;&#22836;&#30340;&#20219;&#21153;&#30446;&#26631;&#37319;&#29992;&#21508;&#31181;&#31574;&#30053;&#12290;&#19968;&#20010;&#25805;&#32437;&#26426;&#22120;&#20154;&#24517;&#39035;&#33021;&#22815;&#24555;&#36895;&#21046;&#20316;&#20986;&#36866;&#29992;&#20110;&#19981;&#21516;&#30446;&#26631;&#30340;&#19987;&#29992;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23398;&#20064;&#19968;&#20010;&#35774;&#35745;&#31574;&#30053;&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;&#35774;&#35745;&#12290;&#35774;&#35745;&#31574;&#30053;&#20197;&#20219;&#21153;&#20449;&#24687;&#20026;&#26465;&#20214;&#65292;&#24182;&#36755;&#20986;&#19968;&#20010;&#26377;&#21161;&#20110;&#35299;&#20915;&#20219;&#21153;&#30340;&#24037;&#20855;&#35774;&#35745;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#20197;&#35774;&#35745;&#26465;&#20214;&#20026;&#22522;&#30784;&#30340;&#25511;&#21046;&#31574;&#30053;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#36827;&#34892;&#25805;&#32437;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26397;&#30528;&#36825;&#20010;&#30446;&#26631;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When limited by their own morphologies, humans and some species of animals have the remarkable ability to use objects from the environment toward accomplishing otherwise impossible tasks. Robots might similarly unlock a range of additional capabilities through tool use. Recent techniques for jointly optimizing morphology and control via deep learning are effective at designing locomotion agents. But while outputting a single morphology makes sense for locomotion, manipulation involves a variety of strategies depending on the task goals at hand. A manipulation agent must be capable of rapidly prototyping specialized tools for different goals. Therefore, we propose learning a designer policy, rather than a single design. A designer policy is conditioned on task information and outputs a tool design that helps solve the task. A design-conditioned controller policy can then perform manipulation using these tools. In this work, we take a step towards this goal by introducing a reinforcement
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#20307;&#20869;&#22312;&#29305;&#24615;&#30340;&#22270;&#20687;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36890;&#29992;&#23545;&#35937;&#31867;&#21035;&#36827;&#34892;&#25193;&#23637;&#65292;&#24182;&#25910;&#38598;&#20102;&#22823;&#35268;&#27169;&#30340;CUTE&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#35813;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.00750</link><description>&lt;p&gt;
&#36825;&#20123;&#26159;&#21516;&#19968;&#20010;&#33529;&#26524;&#21527;&#65311;&#22522;&#20110;&#29289;&#20307;&#20869;&#22312;&#29305;&#24615;&#27604;&#36739;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Are These the Same Apple? Comparing Images Based on Object Intrinsics. (arXiv:2311.00750v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#20307;&#20869;&#22312;&#29305;&#24615;&#30340;&#22270;&#20687;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36890;&#29992;&#23545;&#35937;&#31867;&#21035;&#36827;&#34892;&#25193;&#23637;&#65292;&#24182;&#25910;&#38598;&#20102;&#22823;&#35268;&#27169;&#30340;CUTE&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;&#22312;&#19981;&#21516;&#30340;&#22806;&#22312;&#22240;&#32032;&#19979;&#65288;&#22914;&#20809;&#29031;&#12289;&#29289;&#20307;&#23039;&#21183;&#21644;&#32972;&#26223;&#65289;&#30340;&#23545;&#35937;&#65292;&#28982;&#32780;&#24403;&#21069;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#22312;&#36825;&#20123;&#21464;&#24322;&#26041;&#38754;&#32463;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#29702;&#35299;&#21644;&#25913;&#36827;&#20154;&#24037;&#35270;&#35273;&#31995;&#32479;&#30340;&#37325;&#35201;&#19968;&#27493;&#26159;&#32431;&#22522;&#20110;&#23450;&#20041;&#23545;&#35937;&#36523;&#20221;&#30340;&#20869;&#22312;&#29289;&#20307;&#29305;&#24615;&#26469;&#27979;&#37327;&#22270;&#20687;&#30456;&#20284;&#24615;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#25991;&#29486;&#20013;&#24050;&#34987;&#30740;&#31350;&#20026;&#37325;&#26032;&#35782;&#21035;&#65292;&#23613;&#31649;&#20027;&#35201;&#23616;&#38480;&#20110;&#29305;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#22914;&#20154;&#21644;&#27773;&#36710;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#20854;&#25193;&#23637;&#21040;&#36890;&#29992;&#23545;&#35937;&#31867;&#21035;&#65292;&#25506;&#32034;&#22522;&#20110;&#29289;&#20307;&#20869;&#22312;&#29305;&#24615;&#30340;&#22270;&#20687;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#27979;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;Common paired objects Under differenT Extrinsics (CUTE)&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;180&#20010;&#23545;&#35937;&#30340;18000&#24352;&#22270;&#20687;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#22806;&#22312;&#22240;&#32032;&#65292;&#22914;&#20809;&#29031;&#12289;&#23039;&#21183;&#21644;&#25104;&#20687;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22914;LPIPS&#21644;CLIP&#20998;&#25968;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#27979;&#37327;&#29289;&#20307;&#20869;&#22312;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human visual system can effortlessly recognize an object under different extrinsic factors such as lighting, object poses, and background, yet current computer vision systems often struggle with these variations. An important step to understanding and improving artificial vision systems is to measure image similarity purely based on intrinsic object properties that define object identity. This problem has been studied in the computer vision literature as re-identification, though mostly restricted to specific object categories such as people and cars. We propose to extend it to general object categories, exploring an image similarity metric based on object intrinsics. To benchmark such measurements, we collect the Common paired objects Under differenT Extrinsics (CUTE) dataset of $18,000$ images of $180$ objects under different extrinsic factors such as lighting, poses, and imaging conditions. While existing methods such as LPIPS and CLIP scores do not measure object intrinsics wel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35266;&#23519;&#12289;&#23545;&#35805;&#21644;&#24341;&#23548;&#65288;WTaG&#65289;&#65292;&#20197;&#21450;&#20004;&#20010;&#20219;&#21153;&#65306;&#29992;&#25143;&#21644;&#29615;&#22659;&#29702;&#35299;&#20197;&#21450;&#25351;&#23548;&#32773;&#20915;&#31574;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#30784;&#27169;&#22411;&#22312;&#24863;&#30693;&#21270;&#20219;&#21153;&#24341;&#23548;&#26041;&#38754;&#26377;&#19968;&#23450;&#30340;&#24615;&#33021;&#65292;&#20294;&#24555;&#36895;&#32780;&#21487;&#38752;&#30340;&#36866;&#24212;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.00738</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#35266;&#23519;&#12289;&#23545;&#35805;&#21644;&#24341;&#23548;&#65306;&#21046;&#20316;&#34507;&#31957;
&lt;/p&gt;
&lt;p&gt;
Can Foundation Models Watch, Talk and Guide You Step by Step to Make a Cake?. (arXiv:2311.00738v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35266;&#23519;&#12289;&#23545;&#35805;&#21644;&#24341;&#23548;&#65288;WTaG&#65289;&#65292;&#20197;&#21450;&#20004;&#20010;&#20219;&#21153;&#65306;&#29992;&#25143;&#21644;&#29615;&#22659;&#29702;&#35299;&#20197;&#21450;&#25351;&#23548;&#32773;&#20915;&#31574;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#30784;&#27169;&#22411;&#22312;&#24863;&#30693;&#21270;&#20219;&#21153;&#24341;&#23548;&#26041;&#38754;&#26377;&#19968;&#23450;&#30340;&#24615;&#33021;&#65292;&#20294;&#24555;&#36895;&#32780;&#21487;&#38752;&#30340;&#36866;&#24212;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#27493;&#65292;&#20294;&#24320;&#21457;&#33021;&#22815;&#25552;&#20379;&#24773;&#22659;&#21270;&#12289;&#20010;&#24615;&#21270;&#25351;&#23548;&#24182;&#21327;&#21161;&#20154;&#31867;&#36827;&#34892;&#21508;&#31181;&#20219;&#21153;&#30340;&#20132;&#20114;&#24335;&#20219;&#21153;&#24341;&#23548;&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#31995;&#32479;&#38656;&#35201;&#23545;&#29992;&#25143;&#21644;&#29615;&#22659;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#24182;&#21450;&#26102;&#20934;&#30830;&#22320;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#35201;&#35828;&#20160;&#20040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;Watch, Talk and Guide&#65288;WTaG&#65289;&#65292;&#22522;&#20110;&#20154;&#31867;&#29992;&#25143;&#21644;&#20154;&#31867;&#25351;&#23548;&#32773;&#20043;&#38388;&#30340;&#33258;&#28982;&#20132;&#20114;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#20010;&#20219;&#21153;&#65306;&#29992;&#25143;&#21644;&#29615;&#22659;&#29702;&#35299;&#20197;&#21450;&#25351;&#23548;&#32773;&#20915;&#31574;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#20960;&#20010;&#22522;&#30784;&#27169;&#22411;&#26469;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#22312;&#24863;&#30693;&#21270;&#20219;&#21153;&#24341;&#23548;&#26041;&#38754;&#30340;&#24555;&#36895;&#36866;&#24212;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#12289;&#23450;&#24615;&#21644;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#23637;&#31034;&#20986;&#20844;&#24179;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#27809;&#26377;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#65292;&#20294;&#24555;&#36895;&#32780;&#21487;&#38752;&#30340;&#36866;&#24212;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite tremendous advances in AI, it remains a significant challenge to develop interactive task guidance systems that can offer situated, personalized guidance and assist humans in various tasks. These systems need to have a sophisticated understanding of the user as well as the environment, and make timely accurate decisions on when and what to say. To address this issue, we created a new multimodal benchmark dataset, Watch, Talk and Guide (WTaG) based on natural interaction between a human user and a human instructor. We further proposed two tasks: User and Environment Understanding, and Instructor Decision Making. We leveraged several foundation models to study to what extent these models can be quickly adapted to perceptually enabled task guidance. Our quantitative, qualitative, and human evaluation results show that these models can demonstrate fair performances in some cases with no task-specific training, but a fast and reliable adaptation remains a significant challenge. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26816;&#27979;&#33258;&#25105;&#25253;&#21578;COVID-19&#35786;&#26029;&#25512;&#25991;&#30340;&#19981;&#21516;&#25991;&#26412;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#33719;&#24471;&#20102;&#27604;&#24179;&#22343;&#20540;&#39640;&#20986;4.1%&#30340;84.5%&#30340;F1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2311.00732</link><description>&lt;p&gt;
tmn&#22312;#SMM4H 2023&#19978;&#30340;&#35770;&#25991;:&#27604;&#36739;&#29992;&#20110;&#26816;&#27979;&#33258;&#25105;&#25253;&#21578;COVID-19&#35786;&#26029;&#25512;&#25991;&#30340;&#25991;&#26412;&#39044;&#22788;&#29702;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
tmn at #SMM4H 2023: Comparing Text Preprocessing Techniques for Detecting Tweets Self-reporting a COVID-19 Diagnosis. (arXiv:2311.00732v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26816;&#27979;&#33258;&#25105;&#25253;&#21578;COVID-19&#35786;&#26029;&#25512;&#25991;&#30340;&#19981;&#21516;&#25991;&#26412;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#33719;&#24471;&#20102;&#27604;&#24179;&#22343;&#20540;&#39640;&#20986;4.1%&#30340;84.5%&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#29992;&#20110;SMM4H 2023&#20219;&#21153;1&#30340;&#31995;&#32479;&#12290;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#33258;&#21160;&#21306;&#20998;&#37027;&#20123;&#33258;&#25105;&#25253;&#21578;COVID-19&#35786;&#26029;&#30340;&#25512;&#25991;&#65288;&#20363;&#22914;&#65292;&#38451;&#24615;&#26816;&#27979;&#12289;&#20020;&#24202;&#35786;&#26029;&#25110;&#20303;&#38498;&#65289;&#21644;&#37027;&#20123;&#27809;&#26377;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#20351;&#29992;&#22235;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25512;&#25991;&#39044;&#22788;&#29702;&#25216;&#26415;&#12290;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#33719;&#24471;&#20102;84.5%&#30340;F1&#24471;&#20998;&#65292;&#27604;&#24179;&#22343;&#20540;&#39640;&#20986;4.1%&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper describes a system developed for Task 1 at SMM4H 2023. The goal of the task is to automatically distinguish tweets that self-report a COVID-19 diagnosis (for example, a positive test, clinical diagnosis, or hospitalization) from those that do not. We investigate the use of different techniques for preprocessing tweets using four transformer-based models. The ensemble of fine-tuned language models obtained an F1-score of 84.5%, which is 4.1% higher than the average value.
&lt;/p&gt;</description></item><item><title>ZEETAD&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#31471;&#21040;&#31471;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#21452;&#23450;&#20301;&#21644;&#38646;&#26679;&#26412;&#25552;&#35758;&#20998;&#31867;&#20004;&#20010;&#27169;&#22359;&#12290;&#21069;&#32773;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#26816;&#27979;&#21160;&#20316;&#20107;&#20214;&#24182;&#25910;&#38598;&#20851;&#38190;&#30340;&#35821;&#20041;&#23884;&#20837;&#65292;&#21518;&#32773;&#26159;&#22522;&#20110;CLIP&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#21644;&#24103;&#36755;&#20837;&#30340;&#35821;&#20041;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2311.00729</link><description>&lt;p&gt;
ZEETAD: &#20026;&#38646;&#26679;&#26412;&#31471;&#21040;&#31471;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#25913;&#36827;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection. (arXiv:2311.00729v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00729
&lt;/p&gt;
&lt;p&gt;
ZEETAD&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#31471;&#21040;&#31471;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#21452;&#23450;&#20301;&#21644;&#38646;&#26679;&#26412;&#25552;&#35758;&#20998;&#31867;&#20004;&#20010;&#27169;&#22359;&#12290;&#21069;&#32773;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#26816;&#27979;&#21160;&#20316;&#20107;&#20214;&#24182;&#25910;&#38598;&#20851;&#38190;&#30340;&#35821;&#20041;&#23884;&#20837;&#65292;&#21518;&#32773;&#26159;&#22522;&#20110;CLIP&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#21644;&#24103;&#36755;&#20837;&#30340;&#35821;&#20041;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#65288;TAD&#65289;&#28041;&#21450;&#22312;&#38750;&#21098;&#36753;&#35270;&#39057;&#20013;&#23450;&#20301;&#21644;&#20998;&#31867;&#21160;&#20316;&#23454;&#20363;&#12290;&#23613;&#31649;&#20256;&#32479;TAD&#22312;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#19978;&#37319;&#29992;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#21644;&#23553;&#38381;&#38598;&#35774;&#32622;&#65292;&#20294;&#26368;&#36817;&#30340;&#38646;&#26679;&#26412;TAD&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#23545;&#27604;&#30340;&#35270;&#35273;-&#35821;&#35328;&#65288;ViL&#65289;&#39044;&#35757;&#32451;&#27169;&#22411;&#23637;&#31034;&#20102;&#24320;&#25918;&#38598;&#35774;&#32622;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;TAD&#26041;&#27861;&#22312;&#22914;&#20309;&#36866;&#24403;&#26500;&#24314;&#23450;&#20301;&#21644;&#20998;&#31867;&#36825;&#20004;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#20043;&#38388;&#30340;&#24378;&#20851;&#31995;&#20197;&#21450;&#23558;ViL&#27169;&#22411;&#36866;&#24212;&#20110;&#35270;&#39057;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ZEETAD&#65292;&#21253;&#21547;&#20004;&#20010;&#27169;&#22359;&#65306;&#21452;&#23450;&#20301;&#21644;&#38646;&#26679;&#26412;&#25552;&#35758;&#20998;&#31867;&#12290;&#21069;&#32773;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#26816;&#27979;&#21160;&#20316;&#20107;&#20214;&#65292;&#24182;&#36873;&#25321;&#24615;&#22320;&#25910;&#38598;&#20851;&#38190;&#30340;&#35821;&#20041;&#23884;&#20837;&#20197;&#20379;&#21518;&#32493;&#35782;&#21035;&#12290;&#21518;&#32773;&#26159;&#22522;&#20110;CLIP&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#20026;&#27599;&#20010;&#26102;&#38388;&#21333;&#20301;&#20174;&#25991;&#26412;&#21644;&#24103;&#36755;&#20837;&#29983;&#25104;&#35821;&#20041;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal action detection (TAD) involves the localization and classification of action instances within untrimmed videos. While standard TAD follows fully supervised learning with closed-set setting on large training data, recent zero-shot TAD methods showcase the promising of open-set setting by leveraging large-scale contrastive visual-language (ViL) pretrained models. However, existing zero-shot TAD methods have limitations on how to properly construct the strong relationships between two interdependent tasks of localization and classification and adapt ViL model to video understanding. In this work, we present ZEETAD, featuring two modules: dual-localization and zero-shot proposal classification. The former is a Transformer-based module that detects action events while selectively collecting crucial semantic embeddings for later recognition. The latter one, CLIP-based module, generates semantic embeddings from text and frame inputs for each temporal unit. Additionally, we enhance d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#23398;&#20064;&#38382;&#39064;&#30340;&#20004;&#31181;&#26041;&#27861;&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#22330;&#26223;&#20013;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#26041;&#27861;&#30340;&#31283;&#20581;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2311.00727</link><description>&lt;p&gt;
&#30740;&#31350;&#36801;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#30456;&#23545;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Investigating Relative Performance of Transfer and Meta Learning. (arXiv:2311.00727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#23398;&#20064;&#38382;&#39064;&#30340;&#20004;&#31181;&#26041;&#27861;&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#22330;&#26223;&#20013;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#26041;&#27861;&#30340;&#31283;&#20581;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#23613;&#31649;&#22270;&#20687;&#35782;&#21035;&#31995;&#32479;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#23601;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#22312;&#20998;&#24067;&#22806;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#35201;&#27714;&#24403;&#31070;&#32463;&#32593;&#32476;&#36935;&#21040;&#19982;&#20854;&#35757;&#32451;&#25968;&#25454;&#19981;&#19968;&#33268;&#30340;&#26465;&#20214;&#26102;&#37325;&#26032;&#35757;&#32451;&#12290;&#36825;&#20010;&#38480;&#21046;&#26174;&#33879;&#24433;&#21709;&#20102;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#32039;&#36843;&#38382;&#39064;&#24341;&#21457;&#20102;&#23545;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26088;&#22312;&#27604;&#36739;&#20004;&#31181;&#19981;&#21516;&#26041;&#27861;&#8212;&#8212;&#36801;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#22312;&#26041;&#26696;&#12290;&#24635;&#20307;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#22330;&#26223;&#20013;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#26041;&#27861;&#30340;&#31283;&#20581;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, the field of machine learning has experienced remarkable advancements. While image recognition systems have achieved impressive levels of accuracy, they continue to rely on extensive training datasets. Additionally, a significant challenge has emerged in the form of poor out-of-distribution performance, which necessitates retraining neural networks when they encounter conditions that deviate from their training data. This limitation has notably contributed to the slow progress in self-driving car technology. These pressing issues have sparked considerable interest in methods that enable neural networks to learn effectively from limited data. This paper presents the outcomes of an extensive investigation designed to compare two distinct approaches, transfer learning and meta learning, as potential solutions to this problem. The overarching objective was to establish a robust criterion for selecting the most suitable method in diverse machine learning scenarios. Bui
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#19994;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#21644;&#22823;&#25968;&#25454;&#25216;&#26415;&#26469;&#20934;&#30830;&#12289;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#22320;&#26816;&#27979;&#30005;&#20449;&#34892;&#19994;&#30340;&#27450;&#35784;&#12290;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#26816;&#27979;&#22269;&#38469;&#25910;&#20837;&#20998;&#25104;&#27450;&#35784;&#65292;&#24182;&#21457;&#29616;&#20102;&#26032;&#30340;&#27450;&#35784;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2311.00724</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#36827;&#34892;&#30005;&#20449;&#34892;&#19994;&#30340;&#27450;&#35784;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Fraud Analytics Using Machine-learning &amp; Engineering on Big Data (FAME) for Telecom. (arXiv:2311.00724v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#19994;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#21644;&#22823;&#25968;&#25454;&#25216;&#26415;&#26469;&#20934;&#30830;&#12289;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#22320;&#26816;&#27979;&#30005;&#20449;&#34892;&#19994;&#30340;&#27450;&#35784;&#12290;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#26816;&#27979;&#22269;&#38469;&#25910;&#20837;&#20998;&#25104;&#27450;&#35784;&#65292;&#24182;&#21457;&#29616;&#20102;&#26032;&#30340;&#27450;&#35784;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#20449;&#34892;&#19994;&#30001;&#20110;&#27450;&#35784;&#34892;&#20026;&#27599;&#24180;&#20840;&#29699;&#25439;&#22833;463&#20159;&#32654;&#20803;&#12290;&#36807;&#21435;&#20351;&#29992;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;&#38500;&#20102;&#35268;&#21017;&#23548;&#21521;&#26041;&#27861;&#65289;&#26469;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#65292;&#20294;&#25928;&#29575;&#24456;&#20302;&#65292;&#22240;&#20026;&#27450;&#35784;&#27169;&#24335;&#21464;&#21270;&#38750;&#24120;&#24555;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#19994;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#21644;&#22823;&#25968;&#25454;&#25216;&#26415;&#26469;&#20934;&#30830;&#12289;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#22320;&#26816;&#27979;&#27450;&#35784;&#24182;&#21457;&#29616;&#26032;&#30340;&#27450;&#35784;&#27169;&#24335;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&lt;5%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#22269;&#38469;&#25910;&#20837;&#20998;&#25104;&#27450;&#35784;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#30693;&#21517;&#25209;&#21457;&#36816;&#33829;&#21830;&#21644;&#28023;&#22806;&#30005;&#20449;&#20013;&#36716;&#36816;&#33829;&#21830;&#30340;&#36229;&#36807;1TB&#30340;&#36890;&#35805;&#35814;&#21333;&#35760;&#24405;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Telecom industries lose globally 46.3 Billion USD due to fraud. Data mining and machine learning techniques (apart from rules oriented approach) have been used in past, but efficiency has been low as fraud pattern changes very rapidly. This paper presents an industrialized solution approach with self adaptive data mining technique and application of big data technologies to detect fraud and discover novel fraud patterns in accurate, efficient and cost effective manner. Solution has been successfully demonstrated to detect International Revenue Share Fraud with &lt;5% false positive. More than 1 Terra Bytes of Call Detail Record from a reputed wholesale carrier and overseas telecom transit carrier has been used to conduct this study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;AI&#22312;&#30028;&#38754;&#35774;&#35745;&#21644;&#35780;&#20272;&#20013;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35268;&#33539;&#23545;&#40784;&#12289;&#36807;&#31243;&#23545;&#40784;&#21644;&#35780;&#20272;&#25903;&#25345;&#31561;&#19977;&#20010;&#23545;&#40784;&#30446;&#26631;&#65292;&#24182;&#20171;&#32461;&#20102;&#20195;&#29702;&#36807;&#31243;&#21644;&#36807;&#31243;&#28023;&#28286;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2311.00710</link><description>&lt;p&gt;
AI&#20114;&#21160;&#20013;&#30340;AI&#23545;&#40784;&#65306;&#35268;&#33539;&#23545;&#40784;&#65292;&#36807;&#31243;&#23545;&#40784;&#21644;&#35780;&#20272;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
AI Alignment in the Design of Interactive AI: Specification Alignment, Process Alignment, and Evaluation Support. (arXiv:2311.00710v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;AI&#22312;&#30028;&#38754;&#35774;&#35745;&#21644;&#35780;&#20272;&#20013;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35268;&#33539;&#23545;&#40784;&#12289;&#36807;&#31243;&#23545;&#40784;&#21644;&#35780;&#20272;&#25903;&#25345;&#31561;&#19977;&#20010;&#23545;&#40784;&#30446;&#26631;&#65292;&#24182;&#20171;&#32461;&#20102;&#20195;&#29702;&#36807;&#31243;&#21644;&#36807;&#31243;&#28023;&#28286;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#23545;&#40784;&#26159;&#30830;&#20445;AI&#20135;&#29983;&#26399;&#26395;&#32467;&#26524;&#32780;&#36991;&#20813;&#19981;&#33391;&#21103;&#20316;&#29992;&#30340;&#25972;&#20307;&#38382;&#39064;&#12290;&#34429;&#28982;&#36890;&#24120;&#20174;&#23433;&#20840;&#21644;&#20154;&#31867;&#20215;&#20540;&#30340;&#35282;&#24230;&#32771;&#34385;AI&#23545;&#40784;&#65292;&#20294;&#20063;&#21487;&#20197;&#22312;&#35774;&#35745;&#21644;&#35780;&#20272;&#20132;&#20114;&#24335;AI&#31995;&#32479;&#30340;&#30028;&#38754;&#30340;&#32972;&#26223;&#19979;&#32771;&#34385;AI&#23545;&#40784;&#12290;&#26412;&#25991;&#23558;AI&#23545;&#40784;&#30340;&#27010;&#24565;&#26144;&#23556;&#21040;&#22522;&#26412;&#30340;&#19977;&#27493;&#20132;&#20114;&#24490;&#29615;&#20013;&#65292;&#24471;&#20986;&#30456;&#24212;&#30340;&#23545;&#40784;&#30446;&#26631;&#65306;1&#65289;&#35268;&#33539;&#23545;&#40784;&#65306;&#30830;&#20445;&#29992;&#25143;&#33021;&#22815;&#39640;&#25928;&#21487;&#38752;&#22320;&#23558;&#30446;&#26631;&#20256;&#36798;&#32473;AI&#65307;2&#65289;&#36807;&#31243;&#23545;&#40784;&#65306;&#25552;&#20379;&#39564;&#35777;&#21644;&#21487;&#36873;&#25321;&#25511;&#21046;AI&#25191;&#34892;&#36807;&#31243;&#30340;&#33021;&#21147;&#65307;3&#65289;&#35780;&#20272;&#25903;&#25345;&#65306;&#30830;&#20445;&#29992;&#25143;&#33021;&#22815;&#39564;&#35777;&#21644;&#29702;&#35299;AI&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20195;&#29702;&#36807;&#31243;&#30340;&#27010;&#24565;&#65292;&#23427;&#34987;&#23450;&#20041;&#20026;AI&#23454;&#38469;&#36807;&#31243;&#30340;&#31616;&#21270;&#12289;&#20998;&#31163;&#27966;&#29983;&#20294;&#21487;&#25511;&#21046;&#30340;&#34920;&#31034;&#65307;&#20197;&#21450;&#36807;&#31243;&#28023;&#28286;&#30340;&#27010;&#24565;&#65292;&#23427;&#31361;&#26174;&#20154;&#31867;&#21644;AI&#36807;&#31243;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI alignment considers the overall problem of ensuring an AI produces desired outcomes, without undesirable side effects. While often considered from the perspectives of safety and human values, AI alignment can also be considered in the context of designing and evaluating interfaces for interactive AI systems. This paper maps concepts from AI alignment onto a basic, three step interaction cycle, yielding a corresponding set of alignment objectives: 1) specification alignment: ensuring the user can efficiently and reliably communicate objectives to the AI, 2) process alignment: providing the ability to verify and optionally control the AI's execution process, and 3) evaluation support: ensuring the user can verify and understand the AI's output. We also introduce the concepts of a surrogate process, defined as a simplified, separately derived, but controllable representation of the AI's actual process; and the notion of a Process Gulf, which highlights how differences between human and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35797;&#22270;&#35843;&#26597;&#26426;&#22120;&#25512;&#33616;&#26159;&#21542;&#21487;&#20197;&#32531;&#35299;&#20154;&#31867;&#24863;&#30693;&#20559;&#24046;&#12290;&#36890;&#36807;&#19968;&#39033;&#23454;&#39564;&#65292;&#21457;&#29616;AI&#21161;&#25163;&#22312;&#38598;&#21512;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21487;&#20197;&#25552;&#39640;&#21442;&#19982;&#32773;&#30340;&#24037;&#20316;&#25928;&#29575;&#65292;&#20294;&#24182;&#27809;&#26377;&#36890;&#36807;&#32479;&#35745;&#23398;&#20998;&#26512;&#35777;&#26126;AI&#21161;&#25163;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#8220;&#19979;&#25289;&#8221;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#24310;&#36831;AI&#22238;&#24212;&#20063;&#27809;&#26377;&#23545;&#20154;&#31867;&#20915;&#31574;&#20934;&#30830;&#24615;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.00706</link><description>&lt;p&gt;
AI&#26159;&#21542;&#21487;&#20197;&#32531;&#35299;&#20154;&#31867;&#24863;&#30693;&#20559;&#24046;&#65311;&#19968;&#39033;&#23454;&#39564;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can AI Mitigate Human Perceptual Biases? A Pilot Study. (arXiv:2311.00706v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35797;&#22270;&#35843;&#26597;&#26426;&#22120;&#25512;&#33616;&#26159;&#21542;&#21487;&#20197;&#32531;&#35299;&#20154;&#31867;&#24863;&#30693;&#20559;&#24046;&#12290;&#36890;&#36807;&#19968;&#39033;&#23454;&#39564;&#65292;&#21457;&#29616;AI&#21161;&#25163;&#22312;&#38598;&#21512;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21487;&#20197;&#25552;&#39640;&#21442;&#19982;&#32773;&#30340;&#24037;&#20316;&#25928;&#29575;&#65292;&#20294;&#24182;&#27809;&#26377;&#36890;&#36807;&#32479;&#35745;&#23398;&#20998;&#26512;&#35777;&#26126;AI&#21161;&#25163;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#8220;&#19979;&#25289;&#8221;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#24310;&#36831;AI&#22238;&#24212;&#20063;&#27809;&#26377;&#23545;&#20154;&#31867;&#20915;&#31574;&#20934;&#30830;&#24615;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#26088;&#22312;&#34913;&#37327;&#26426;&#22120;&#25512;&#33616;&#26159;&#21542;&#33021;&#22815;&#32416;&#27491;&#20154;&#31867;&#24863;&#30693;&#20559;&#24046;&#30340;&#35797;&#28857;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#8220;&#19979;&#25289;&#8221;&#25928;&#24212;&#65292;&#21363;&#20154;&#20204;&#20302;&#20272;&#20102;&#32447;&#30340;&#24179;&#22343;&#20301;&#32622;&#65292;&#22312;&#32447;&#22270;&#20013;&#20272;&#35745;&#25968;&#25454;&#28857;&#30340;&#38598;&#21512;&#24179;&#22343;&#20540;&#30340;&#20219;&#21153;&#20013;&#12290;&#36825;&#20123;&#32447;&#22270;&#21487;&#20197;&#26174;&#31034;&#20363;&#22914;12&#20010;&#26376;&#20869;&#30340;&#28201;&#24230;&#25110;&#38477;&#27700;&#37327;&#12290;&#20845;&#21517;&#21442;&#19982;&#32773;&#20351;&#29992;&#25110;&#19981;&#20351;&#29992;AI&#21161;&#25163;&#20272;&#35745;&#38598;&#21512;&#24179;&#22343;&#20540;&#12290;&#24403;&#26377;AI&#21161;&#25163;&#26102;&#65292;&#21161;&#25163;&#20197;&#19981;&#21516;&#36895;&#24230;&#22238;&#24212;&#65292;&#20197;&#27169;&#25311;&#21487;&#33021;&#24310;&#36831;&#22238;&#24212;&#30340;&#20154;&#31867;&#21327;&#20316;&#32773;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#35797;&#28857;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38598;&#21512;&#20219;&#21153;&#20013;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;AI&#21161;&#25163;&#27604;&#27809;&#26377;&#20351;&#29992;AI&#21161;&#25163;&#30340;&#22522;&#20934;&#32452;&#26356;&#24555;&#12290;&#23613;&#31649;&#8220;&#19979;&#25289;&#8221;&#20559;&#24046;&#24471;&#21040;&#20102;&#20943;&#23569;&#65292;&#20294;AI&#21161;&#25163;&#30340;&#24433;&#21709;&#24182;&#19981;&#20855;&#26377;&#32479;&#35745;&#23398;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#24310;&#36831;AI&#22238;&#24212;&#23545;&#20154;&#31867;&#20915;&#31574;&#20934;&#30830;&#24615;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present results from a pilot experiment to measure if machine recommendations can debias human perceptual biases in visualization tasks. We specifically studied the ``pull-down'' effect, i.e., people underestimate the average position of lines, for the task of estimating the ensemble average of data points in line charts. These line charts can show for example temperature or precipitation in 12 months. Six participants estimated ensemble averages with or without an AI assistant. The assistant, when available, responded at three different speeds to assemble the conditions of a human collaborator who may delay his or her responses. Our pilot study showed that participants were faster with AI assistance in ensemble tasks, compared to the baseline without AI assistance. Although ``pull-down'' biases were reduced, the effect of AI assistance was not statistically significant. Also, delaying AI responses had no significant impact on human decision accuracy. We discuss the implications of 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#24067;&#24335;&#20803;&#24378;&#21270;&#23398;&#20064;&#22312;&#24320;&#25918;&#24335;&#20219;&#21153;&#20998;&#24067;&#19978;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#38598;&#20307;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#22797;&#26434;&#30340;&#21512;&#20316;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2311.00651</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38598;&#20307;&#33258;&#21457;&#24320;&#25918;&#24335;&#25506;&#32034;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Collective Open-Ended Exploration from Decentralized Meta-Reinforcement Learning. (arXiv:2311.00651v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00651
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#24067;&#24335;&#20803;&#24378;&#21270;&#23398;&#20064;&#22312;&#24320;&#25918;&#24335;&#20219;&#21153;&#20998;&#24067;&#19978;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#38598;&#20307;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#22797;&#26434;&#30340;&#21512;&#20316;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#33258;&#25105;&#23545;&#25112;&#30340;&#24320;&#25918;&#24335;&#20219;&#21153;&#20998;&#24067;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#20803;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#20135;&#29983;&#22797;&#26434;&#30340;&#21512;&#20316;&#34892;&#20026;&#12290;&#34429;&#28982;&#32467;&#26524;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#65292;&#33258;&#25105;&#23545;&#25112;&#21644;&#20854;&#20182;&#38598;&#20013;&#21270;&#35757;&#32451;&#25216;&#26415;&#24182;&#19981;&#33021;&#20934;&#30830;&#22320;&#21453;&#26144;&#33258;&#28982;&#30028;&#20013;&#26222;&#36941;&#30340;&#38598;&#20307;&#25506;&#32034;&#31574;&#30053;&#26159;&#22914;&#20309;&#20986;&#29616;&#30340;&#65306;&#36890;&#36807;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#23545;&#20219;&#21153;&#30340;&#26080;&#38480;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38598;&#20307;&#25506;&#32034;&#31574;&#30053;&#30340;&#20986;&#29616;&#65292;&#20854;&#20013;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#19968;&#20010;&#26080;&#38480;&#30340;&#20219;&#21153;&#20998;&#24067;&#20013;&#29420;&#31435;&#22320;&#20803;&#23398;&#20064;&#24490;&#29615;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#29615;&#22659;&#65292;&#23427;&#20855;&#26377;&#19968;&#20010;&#26080;&#38480;&#30340;&#36807;&#31243;&#29983;&#25104;&#30340;&#20219;&#21153;&#31354;&#38388;&#65292;&#21160;&#24577;&#32452;&#21512;&#20102;&#20174;&#20116;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#20013;&#25277;&#26679;&#30340;&#22810;&#20010;&#23376;&#20219;&#21153;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#24222;&#22823;&#30340;&#20219;&#21153;&#26641;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#20998;&#25955;&#26234;&#33021;&#20307;&#22312;&#38754;&#23545;&#26032;&#30340;&#30446;&#26631;&#26102;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have proven that intricate cooperative behaviors can emerge in agents trained using meta reinforcement learning on open ended task distributions using self-play. While the results are impressive, we argue that self-play and other centralized training techniques do not accurately reflect how general collective exploration strategies emerge in the natural world: through decentralized training and over an open-ended distribution of tasks. In this work we therefore investigate the emergence of collective exploration strategies, where several agents meta-learn independent recurrent policies on an open ended distribution of tasks. To this end we introduce a novel environment with an open ended procedurally generated task space which dynamically combines multiple subtasks sampled from five diverse task types to form a vast distribution of task trees. We show that decentralized agents trained in our environment exhibit strong generalization abilities when confronted with novel obj
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#30446;&#26631;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;Nash&#22343;&#34913;&#24182;&#20855;&#26377;&#29305;&#23450;&#20215;&#20540;&#33539;&#22260;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2311.00582</link><description>&lt;p&gt;
&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20197;&#23454;&#29616;&#20219;&#24847;Nash&#22343;&#34913;&#21644;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value. (arXiv:2311.00582v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#30446;&#26631;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;Nash&#22343;&#34913;&#24182;&#20855;&#26377;&#29305;&#23450;&#20215;&#20540;&#33539;&#22260;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20301;&#21892;&#24847;&#30340;&#28216;&#25103;&#35774;&#35745;&#32773;&#25110;&#24694;&#24847;&#30340;&#23545;&#25163;&#20462;&#25913;&#20102;&#19968;&#20010;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#20415;&#19968;&#20010;&#30446;&#26631;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#30340;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;&#39532;&#23572;&#21487;&#22827;&#23436;&#32654;Nash&#22343;&#34913;&#65292;&#24182;&#19988;&#22312;&#30446;&#26631;&#33539;&#22260;&#20869;&#20855;&#26377;&#20215;&#20540;&#65292;&#20197;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#33021;&#22815;&#23433;&#35013;&#20026;&#26576;&#20010;&#28216;&#25103;&#30340;&#21807;&#19968;&#22343;&#34913;&#30340;&#31574;&#30053;&#37197;&#32622;&#30340;&#38598;&#21512;&#65292;&#24182;&#24314;&#31435;&#20102;&#25104;&#21151;&#23433;&#35013;&#30340;&#20805;&#20998;&#21644;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#19968;&#20010;&#24102;&#26377;&#32447;&#24615;&#32422;&#26463;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#28982;&#21518;&#36827;&#34892;&#38543;&#26426;&#25200;&#21160;&#65292;&#26469;&#33719;&#24471;&#19968;&#20010;&#25104;&#26412;&#36817;&#20046;&#26368;&#20248;&#30340;&#20462;&#25913;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the game modification problem, where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost. We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation. We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#32454;&#32990;&#26426;&#22120;&#20154;&#31895;&#32454;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;&#26354;&#23884;&#20837;&#26694;&#26550;&#22312;&#20849;&#20139;&#30340;&#21452;&#26354;&#31354;&#38388;&#20869;&#32479;&#19968;&#20102;&#21508;&#31181;&#31890;&#24230;&#30340;&#26426;&#22120;&#20154;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#33258;&#20027;&#22320;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#30830;&#23450;&#25506;&#32034;&#30340;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2311.00462</link><description>&lt;p&gt;
&#21033;&#29992;&#21452;&#26354;&#23884;&#20837;&#36827;&#34892;&#31895;&#32454;&#35774;&#35745;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Leveraging Hyperbolic Embeddings for Coarse-to-Fine Robot Design. (arXiv:2311.00462v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#32454;&#32990;&#26426;&#22120;&#20154;&#31895;&#32454;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;&#26354;&#23884;&#20837;&#26694;&#26550;&#22312;&#20849;&#20139;&#30340;&#21452;&#26354;&#31354;&#38388;&#20869;&#32479;&#19968;&#20102;&#21508;&#31181;&#31890;&#24230;&#30340;&#26426;&#22120;&#20154;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#33258;&#20027;&#22320;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#30830;&#23450;&#25506;&#32034;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32454;&#32990;&#26426;&#22120;&#20154;&#35774;&#35745;&#26088;&#22312;&#21019;&#24314;&#30001;&#35768;&#22810;&#32454;&#32990;&#32452;&#25104;&#30340;&#26426;&#22120;&#20154;&#65292;&#20197;&#20415;&#33021;&#22815;&#39640;&#25928;&#22320;&#25511;&#21046;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#29983;&#25104;&#21508;&#31181;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#30452;&#25509;&#22312;&#24222;&#22823;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#20248;&#21270;&#65292;&#23548;&#33268;&#20102;&#38590;&#20197;&#25511;&#21046;&#30340;&#22797;&#26434;&#24418;&#24577;&#30340;&#26426;&#22120;&#20154;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#32454;&#32990;&#26426;&#22120;&#20154;&#31895;&#32454;&#35774;&#35745;&#26041;&#27861;&#12290;&#35813;&#31574;&#30053;&#39318;&#20808;&#23547;&#27714;&#26368;&#20339;&#30340;&#31895;&#31890;&#24230;&#26426;&#22120;&#20154;&#65292;&#28982;&#21518;&#36880;&#27493;&#23545;&#20854;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#22312;&#31895;&#32454;&#36716;&#25442;&#36807;&#31243;&#20013;&#30830;&#23450;&#31934;&#32454;&#35843;&#25972;&#20851;&#33410;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#26426;&#22120;&#20154;&#35774;&#35745;&#30340;&#21452;&#26354;&#23884;&#20837; (HERD) &#26694;&#26550;&#12290;HERD &#22312;&#19968;&#20010;&#20849;&#20139;&#30340;&#21452;&#26354;&#31354;&#38388;&#20869;&#32479;&#19968;&#20102;&#21508;&#31181;&#31890;&#24230;&#30340;&#26426;&#22120;&#20154;&#65292;&#24182;&#21033;&#29992;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#20010;&#26694;&#26550;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#33258;&#20027;&#22320;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#30830;&#23450;&#25506;&#32034;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-cellular robot design aims to create robots comprised of numerous cells that can be efficiently controlled to perform diverse tasks. Previous research has demonstrated the ability to generate robots for various tasks, but these approaches often optimize robots directly in the vast design space, resulting in robots with complicated morphologies that are hard to control. In response, this paper presents a novel coarse-to-fine method for designing multi-cellular robots. Initially, this strategy seeks optimal coarse-grained robots and progressively refines them. To mitigate the challenge of determining the precise refinement juncture during the coarse-to-fine transition, we introduce the Hyperbolic Embeddings for Robot Design (HERD) framework. HERD unifies robots of various granularity within a shared hyperbolic space and leverages a refined Cross-Entropy Method for optimization. This framework enables our method to autonomously identify areas of exploration in hyperbolic space and c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#23450;&#20041;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#21363;&#26080;&#38480;&#26102;&#38388;&#20869;&#19981;&#26029;&#20135;&#29983;&#26032;&#20803;&#32032;&#12290;&#22312;&#36825;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#30340;&#27010;&#24565;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#24320;&#25918;&#24335;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2311.00344</link><description>&lt;p&gt;
&#20026;&#30446;&#26631;&#26465;&#20214;&#26234;&#33021;&#20307;&#23450;&#20041;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Definition of Open-Ended Learning Problems for Goal-Conditioned Agents. (arXiv:2311.00344v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#23450;&#20041;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#21363;&#26080;&#38480;&#26102;&#38388;&#20869;&#19981;&#26029;&#20135;&#29983;&#26032;&#20803;&#32032;&#12290;&#22312;&#36825;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#30340;&#27010;&#24565;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#24320;&#25918;&#24335;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#35770;&#25991;&#20013;&#37117;&#25552;&#21040;&#20102;&#8220;&#24320;&#25918;&#24335;&#23398;&#20064;&#8221;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#23581;&#35797;&#23450;&#20041;&#36825;&#20010;&#26415;&#35821;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#24403;&#20180;&#32454;&#30740;&#31350;&#26102;&#65292;&#20284;&#20046;&#23545;&#20110;&#24320;&#25918;&#24335;&#23398;&#20064;&#19982;&#36830;&#32493;&#23398;&#20064;&#12289;&#32456;&#36523;&#23398;&#20064;&#25110;&#33258;&#20026;&#30446;&#30340;&#23398;&#20064;&#31561;&#30456;&#20851;&#27010;&#24565;&#30340;&#21306;&#21035;&#27809;&#26377;&#20849;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#12290;&#36890;&#36807;&#38416;&#36848;&#36825;&#20010;&#27010;&#24565;&#30340;&#36215;&#28304;&#21644;&#26368;&#36817;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#23646;&#24615;&#30340;&#22797;&#21512;&#27010;&#24565;&#12290;&#19982;&#36825;&#20123;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#24320;&#25918;&#24335;&#36807;&#31243;&#30340;&#19968;&#20010;&#20851;&#38190;&#22522;&#26412;&#23646;&#24615;&#19982;&#26102;&#38388;&#26080;&#38480;&#21046;&#22320;&#20135;&#29983;&#26032;&#20803;&#32032;&#30456;&#20998;&#31163;&#30340;&#24819;&#27861;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#30340;&#27010;&#24565;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#20102;&#24320;&#25918;&#24335;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
A lot of recent machine learning research papers have "Open-ended learning" in their title. But very few of them attempt to define what they mean when using the term. Even worse, when looking more closely there seems to be no consensus on what distinguishes open-ended learning from related concepts such as continual learning, lifelong learning or autotelic learning. In this paper, we contribute to fixing this situation. After illustrating the genealogy of the concept and more recent perspectives about what it truly means, we outline that open-ended learning is generally conceived as a composite notion encompassing a set of diverse properties. In contrast with these previous approaches, we propose to isolate a key elementary property of open-ended processes, which is to always produce novel elements from time to time over an infinite horizon. From there, we build the notion of open-ended learning problems and focus in particular on the subset of open-ended goal-conditioned reinforcement
&lt;/p&gt;</description></item><item><title>JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00286</link><description>&lt;p&gt;
JADE&#65306;&#22522;&#20110;&#35821;&#35328;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00286
&lt;/p&gt;
&lt;p&gt;
JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JADE&#65292;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#20998;&#26512;&#30340;&#27169;&#31946;&#27979;&#35797;&#24179;&#21488;&#65292;&#36890;&#36807;&#22686;&#24378;&#31181;&#23376;&#38382;&#39064;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#24182;&#22987;&#32456;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#19977;&#31867;LLM&#65306;&#20843;&#20010;&#24320;&#28304;&#20013;&#25991;LLM&#65292;&#20845;&#20010;&#21830;&#19994;&#20013;&#25991;LLM&#21644;&#22235;&#20010;&#21830;&#19994;&#33521;&#25991;LLM&#12290;JADE&#20026;&#36825;&#19977;&#31867;LLM&#29983;&#25104;&#20102;&#19977;&#20010;&#23433;&#20840;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#65306;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#21516;&#26102;&#35302;&#21457;&#22810;&#20010;LLM&#30340;&#26377;&#23475;&#29983;&#25104;&#65292;&#24179;&#22343;&#19981;&#23433;&#20840;&#29983;&#25104;&#27604;&#20363;&#20026;70%&#65288;&#35831;&#21442;&#35265;&#19979;&#34920;&#65289;&#65292;&#21516;&#26102;&#36825;&#20123;&#38382;&#39064;&#20173;&#28982;&#26159;&#33258;&#28982;&#12289;&#27969;&#30021;&#19988;&#20445;&#30041;&#20102;&#26680;&#24515;&#30340;&#19981;&#23433;&#20840;&#35821;&#20041;&#12290;&#25105;&#20204;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#21457;&#24067;&#20102;&#23545;&#21830;&#19994;&#33521;&#25991;LLM&#21644;&#24320;&#28304;&#33521;&#25991;LLM&#29983;&#25104;&#30340;&#22522;&#20934;&#28436;&#31034;&#65306;https://github.com/whitzard-ai/jade-db&#12290;&#23545;&#20110;&#23545;JADE&#29983;&#25104;&#30340;&#26356;&#22810;&#38382;&#39064;&#24863;&#20852;&#36259;&#30340;&#35835;&#32773;&#65292;&#35831;&#19982;&#25105;&#20204;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present \textit{JADE}, a targeted linguistic fuzzing platform which strengthens the linguistic complexity of seed questions to simultaneously and consistently break a wide range of widely-used LLMs categorized in three groups: eight open-sourced Chinese, six commercial Chinese and four commercial English LLMs. JADE generates three safety benchmarks for the three groups of LLMs, which contain unsafe questions that are highly threatening: the questions simultaneously trigger harmful generation of multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$} (please see the table below), while are still natural questions, fluent and preserving the core unsafe semantics. We release the benchmark demos generated for commercial English LLMs and open-sourced English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For readers who are interested in evaluating on more questions generated by JADE, please contact us.  \textit{JADE} is based on Noam
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;AutoMixer&#65292;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#36890;&#36947;&#21387;&#32553;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#24037;&#20316;&#27969;&#25216;&#26415;&#65292;&#26377;&#25928;&#35299;&#32806;&#20102;BizITOps&#25968;&#25454;&#20013;&#26377;&#29992;&#21644;&#22024;&#26434;&#30340;&#36328;&#36890;&#36947;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20280</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#21160;&#28151;&#21512;&#27169;&#22411;&#22312;BizITOps&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data. (arXiv:2310.20280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20280
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;AutoMixer&#65292;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#36890;&#36947;&#21387;&#32553;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#24037;&#20316;&#27969;&#25216;&#26415;&#65292;&#26377;&#25928;&#35299;&#32806;&#20102;BizITOps&#25968;&#25454;&#20013;&#26377;&#29992;&#21644;&#22024;&#26434;&#30340;&#36328;&#36890;&#36947;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19994;&#21153;&#36807;&#31243;&#30340;&#25928;&#29575;&#20381;&#36182;&#20110;&#19994;&#21153;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65288;Biz-KPIs&#65289;&#65292;&#32780;IT&#25925;&#38556;&#21487;&#33021;&#23545;&#20854;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;BizITOps&#25968;&#25454;&#23558;Biz-KPIs&#21644;IT&#20107;&#20214;&#36890;&#36947;&#34701;&#21512;&#25104;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25552;&#21069;&#39044;&#27979;Biz-KPIs&#21487;&#20197;&#36890;&#36807;&#20027;&#21160;&#30340;&#32416;&#27491;&#25514;&#26045;&#25552;&#39640;&#25928;&#29575;&#21644;&#25910;&#30410;&#12290;&#28982;&#32780;&#65292;BizITOps&#25968;&#25454;&#36890;&#24120;&#23637;&#31034;&#20986;Biz-KPIs&#21644;IT&#20107;&#20214;&#20043;&#38388;&#26377;&#29992;&#21644;&#22024;&#26434;&#30340;&#36328;&#36890;&#36947;&#20132;&#20114;&#65292;&#38656;&#35201;&#26377;&#25928;&#35299;&#32806;&#12290;&#24403;&#20351;&#29992;&#29616;&#26377;&#30340;&#22810;&#21464;&#37327;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#36825;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;AutoMixer&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#26032;&#39062;&#30340;&#36890;&#36947;&#21387;&#32553;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#24037;&#20316;&#27969;&#25216;&#26415;&#12290;AutoMixer&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#36890;&#36947;&#21387;&#32553;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#36827;&#30340;TSMixer&#27169;&#22411;&#38598;&#25104;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36825;&#31181;&#34701;&#21512;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;TSM
&lt;/p&gt;
&lt;p&gt;
The efficiency of business processes relies on business key performance indicators (Biz-KPIs), that can be negatively impacted by IT failures. BizITOps data fuses both Biz-KPIs and IT event channels together as multivariate time series data. Forecasting Biz-KPIs in advance can enhance efficiency and revenue through proactive corrective measures. However, BizITOps data generally exhibit both useful and noisy inter-channel interactions between Biz-KPIs and IT events that need to be effectively decoupled. This leads to suboptimal forecasting performance when existing multivariate forecasting models are employed. To address this, we introduce AutoMixer, a time-series Foundation Model (FM) approach, grounded on the novel technique of channel-compressed pretrain and finetune workflows. AutoMixer leverages an AutoEncoder for channel-compressed pretraining and integrates it with the advanced TSMixer model for multivariate time series forecasting. This fusion greatly enhances the potency of TSM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#38750;&#23545;&#31216;&#20851;&#31995;&#65292;&#19988;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18348</link><description>&lt;p&gt;
&#24847;&#20041;&#34920;&#24449;&#26469;&#33258;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Meaning Representations from Trajectories in Autoregressive Models. (arXiv:2310.18348v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#38750;&#23545;&#31216;&#20851;&#31995;&#65292;&#19988;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#32771;&#34385;&#25193;&#23637;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#26469;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#12290;&#36825;&#31181;&#31574;&#30053;&#26159;&#26080;&#25552;&#31034;&#30340;&#65292;&#19981;&#38656;&#35201;&#24494;&#35843;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;&#21521;&#37327;&#30340;&#34920;&#24449;&#19981;&#21516;&#65292;&#22522;&#20110;&#20998;&#24067;&#30340;&#34920;&#24449;&#36824;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20284;&#28982;&#20989;&#25968;&#20043;&#38388;&#30340;&#20195;&#25968;&#36816;&#31639;&#26469;&#24314;&#27169;&#38750;&#23545;&#31216;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#36923;&#36753;&#34164;&#28085;&#30340;&#26041;&#21521;&#65292;&#19978;&#20301;&#35789;/&#19979;&#20301;&#35789;&#20851;&#31995;&#65289;&#12290;&#36825;&#20123;&#24819;&#27861;&#22522;&#20110;&#35821;&#20041;&#30340;&#20998;&#24067;&#35270;&#35282;&#65292;&#24182;&#19982;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#26631;&#20934;&#26500;&#36896;&#30456;&#36830;&#25509;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#20204;&#23578;&#26410;&#24212;&#29992;&#20110;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20174;&#22823;&#22411;&#27169;&#22411;&#33719;&#24471;&#30340;&#34920;&#24449;&#19982;&#20154;&#31867;&#27880;&#37322;&#24456;&#22909;&#22320;&#19968;&#33268;&#65292;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#38646;&#26679;&#26412;&#21644;&#26080;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#34164;&#28085;&#21644;&#21253;&#21547;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to extract meaning representations from autoregressive language models by considering the distribution of all possible trajectories extending an input text. This strategy is prompt-free, does not require fine-tuning, and is applicable to any pre-trained autoregressive model. Moreover, unlike vector-based representations, distribution-based representations can also model asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym relations) by using algebraic operations between likelihood functions. These ideas are grounded in distributional perspectives on semantics and are connected to standard constructions in automata theory, but to our knowledge they have not been applied to modern language models. We empirically show that the representations obtained from large models align well with human annotations, outperform other zero-shot and prompt-free methods on semantic similarity tasks, and can be used to solve more complex entailment and containment tasks 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#32852;&#21512;&#35268;&#21010;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#19982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#22312;&#39640;&#24230;&#20132;&#20114;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#20026;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#35268;&#21010;&#23433;&#20840;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.18301</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#20132;&#20114;&#24335;&#36816;&#21160;&#35268;&#21010;&#19982;&#32852;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Interactive Motion Planning for Autonomous Vehicles with Joint Optimization. (arXiv:2310.18301v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#32852;&#21512;&#35268;&#21010;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#19982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#22312;&#39640;&#24230;&#20132;&#20114;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#20026;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#35268;&#21010;&#23433;&#20840;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24230;&#20132;&#20114;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#65292;&#19968;&#20010;&#36710;&#36742;&#30340;&#34892;&#21160;&#20250;&#26497;&#22823;&#22320;&#24433;&#21709;&#21040;&#20854;&#21608;&#22260;&#36710;&#36742;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#26679;&#30340;&#20132;&#20114;&#29615;&#22659;&#20013;&#20026;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#35268;&#21010;&#23433;&#20840;&#30340;&#36816;&#21160;&#36335;&#24452;&#38656;&#35201;&#32771;&#34385;&#33258;&#36523;&#24847;&#22270;&#34892;&#21160;&#23545;&#21608;&#22260;&#36710;&#36742;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#22312;&#30456;&#20851;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#35768;&#22810;&#27169;&#22411;&#37117;&#25903;&#25345;&#20197;&#33258;&#36523;&#26465;&#20214;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#21033;&#29992;&#33258;&#36523;&#26465;&#20214;&#30340;&#39044;&#27979;&#22312;&#19979;&#28216;&#35268;&#21010;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38480;&#21046;&#20102;&#35268;&#21010;&#22120;&#30340;&#32467;&#26500;&#65292;&#20363;&#22914;&#37319;&#26679;&#22411;&#35268;&#21010;&#22120;&#12290;&#23613;&#31649;&#37319;&#26679;&#22411;&#35268;&#21010;&#22120;&#33021;&#22815;&#29983;&#25104;&#31934;&#32454;&#30340;&#39640;&#36136;&#37327;&#36816;&#21160;&#36335;&#24452;&#65292;&#20294;&#22522;&#20110;&#26799;&#24230;&#30340;&#35268;&#21010;&#31639;&#27861;&#65292;&#22914;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#65292;&#30001;&#20110;&#20854;&#36845;&#20195;&#24615;&#36136;&#21644;&#23545;&#26799;&#24230;&#30340;&#38656;&#27714;&#65292;&#24456;&#38590;&#21033;&#29992;&#33258;&#36523;&#26465;&#20214;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#32852;&#21512;&#35268;&#21010;&#65288;IJP&#65289;&#65292;&#23558;MPC&#19982;
&lt;/p&gt;
&lt;p&gt;
In highly interactive driving scenarios, the actions of one agent greatly influences those of its neighbors. Planning safe motions for autonomous vehicles in such interactive environments, therefore, requires reasoning about the impact of the ego's intended motion plan on nearby agents' behavior. Deep-learning-based models have recently achieved great success in trajectory prediction and many models in the literature allow for ego-conditioned prediction. However, leveraging ego-conditioned prediction remains challenging in downstream planning due to the complex nature of neural networks, limiting the planner structure to simple ones, e.g., sampling-based planner. Despite their ability to generate fine-grained high-quality motion plans, it is difficult for gradient-based planning algorithms, such as model predictive control (MPC), to leverage ego-conditioned prediction due to their iterative nature and need for gradient. We present Interactive Joint Planning (IJP) that bridges MPC with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#30340;&#24178;&#25200;&#26469;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;LLMs&#21463;&#21040;&#24178;&#25200;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#25913;&#21892;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17490</link><description>&lt;p&gt;
&#25552;&#39640;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#23545;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24178;&#25200;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#30340;&#24178;&#25200;&#26469;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;LLMs&#21463;&#21040;&#24178;&#25200;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#25913;&#21892;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20351;&#24471;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;(ODQA)&#20013;&#23454;&#29616;&#38646;&#26679;&#26412;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#26159;&#30001;&#20110;&#38405;&#35835;&#22120;&#30456;&#23545;&#20110;&#26816;&#32034;&#22120;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#19968;&#31181;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#25104;&#26412;&#21644;&#26631;&#27880;&#25968;&#25454;&#38656;&#27714;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#30001;&#20110;&#26816;&#32034;&#21040;&#30340;&#26080;&#20851;&#25991;&#26723;&#20197;&#21450;&#20316;&#20026;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#26102;&#29983;&#25104;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#32780;&#21463;&#21040;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21542;&#23450;&#30340;&#25351;&#20196;&#21644;&#20998;&#25968;&#35843;&#25972;&#30340;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#25991;&#26723;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22788;&#29702;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;&#38754;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#32780;&#22256;&#38590;&#37325;&#37325;&#30340;&#30417;&#30563;&#24335;&#38405;&#35835;&#22120;&#19981;&#21516;&#65292;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) enable zero-shot approaches in open-domain question answering (ODQA), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cost and the need for labeled data. We find that LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers. To tackle these problems, we mitigate the impact of such documents via Distraction-aware Answer Selection (DAS) with a negation-based instruction and score adjustment for proper answer selection. Experimental results show that our approach successfully handles distraction across diverse scenarios, enhancing the performance of zero-shot readers. Furthermore, unlike supervised readers struggling with unseen data, zero-shot readers demonstrate outstanding transferability without any training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22240;&#26524;&#22270;&#25277;&#35937;&#20174;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#24178;&#39044;&#24635;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#24635;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#21487;&#35782;&#21035;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#22270;&#24418;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#35843;&#25972;&#38598;&#21512;&#20197;&#20272;&#35745;&#24635;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.14691</link><description>&lt;p&gt;
&#26102;&#24207;&#22240;&#26524;&#22270;&#30340;&#25277;&#35937;&#20013;&#24635;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Identifiability of total effects from abstractions of time series causal graphs. (arXiv:2310.14691v2 [math.ST] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22240;&#26524;&#22270;&#25277;&#35937;&#20174;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#24178;&#39044;&#24635;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#24635;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#21487;&#35782;&#21035;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#22270;&#24418;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#35843;&#25972;&#38598;&#21512;&#20197;&#20272;&#35745;&#24635;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20165;&#22522;&#20110;&#31995;&#32479;&#30340;&#22240;&#26524;&#22270;&#25277;&#35937;&#20174;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#24178;&#39044;&#24635;&#25928;&#24212;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25277;&#35937;&#65306;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#23558;&#25152;&#26377;&#28382;&#21518;&#22240;&#26524;&#20851;&#31995;&#28151;&#28102;&#22312;&#19968;&#36215;&#65292;&#20294;&#21306;&#20998;&#28382;&#21518;&#21644;&#30636;&#26102;&#20851;&#31995;&#65307;&#32780;&#25688;&#35201;&#22240;&#26524;&#22270;&#21017;&#19981;&#25552;&#20379;&#20219;&#20309;&#20851;&#20110;&#22240;&#26524;&#20851;&#31995;&#28382;&#21518;&#30340;&#25351;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#24635;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#25152;&#24517;&#38656;&#21644;&#20805;&#20998;&#30340;&#22270;&#24418;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#22312;&#24635;&#25928;&#24212;&#21487;&#35782;&#21035;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#20272;&#35745;&#24635;&#25928;&#24212;&#30340;&#35843;&#25972;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of identifiability of the total effect of an intervention from observational time series only given an abstraction of the causal graph of the system. Specifically, we consider two types of abstractions: the extended summary causal graph which conflates all lagged causal relations but distinguishes between lagged and instantaneous relations; and the summary causal graph which does not give any indication about the lag between causal relations. We show that the total effect is always identifiable in extended summary causal graphs and we provide necessary and sufficient graphical conditions for identifiability in summary causal graphs. Furthermore, we provide adjustment sets allowing to estimate the total effect whenever it is identifiable.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#34920;&#31034;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#34920;&#31034;&#26159;&#21542;&#19968;&#33268;&#20197;&#21450;&#22914;&#20309;&#35843;&#25972;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20854;&#20182;&#31995;&#32479;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20316;&#20026;&#20849;&#21516;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2310.13018</link><description>&lt;p&gt;
&#23545;&#34920;&#31034;&#19968;&#33268;&#24615;&#36798;&#25104;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
Getting aligned on representational alignment. (arXiv:2310.13018v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#34920;&#31034;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#34920;&#31034;&#26159;&#21542;&#19968;&#33268;&#20197;&#21450;&#22914;&#20309;&#35843;&#25972;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20854;&#20182;&#31995;&#32479;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20316;&#20026;&#20849;&#21516;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#26500;&#24314;&#21487;&#20197;&#29992;&#26469;&#36827;&#34892;&#20998;&#31867;&#12289;&#25512;&#29702;&#12289;&#35268;&#21010;&#12289;&#23548;&#33322;&#21644;&#20915;&#31574;&#30340;&#19990;&#30028;&#34920;&#31034;&#12290;&#36825;&#20123;&#22810;&#26679;&#21270;&#31995;&#32479;&#25152;&#26500;&#24314;&#30340;&#34920;&#31034;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#19968;&#33268;&#30340;&#65311;&#21363;&#20351;&#34920;&#31034;&#19981;&#21516;&#65292;&#26159;&#21542;&#20173;&#28982;&#33021;&#22815;&#23548;&#33268;&#30456;&#21516;&#30340;&#34892;&#20026;&#65311;&#31995;&#32479;&#22914;&#20309;&#20462;&#25913;&#23427;&#20204;&#30340;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#21478;&#19968;&#20010;&#31995;&#32479;&#30340;&#34920;&#31034;&#65311;&#36825;&#20123;&#20851;&#20110;&#34920;&#31034;&#19968;&#33268;&#24615;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#24403;&#20195;&#35748;&#30693;&#31185;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20123;&#26368;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#26680;&#24515;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#20110;&#23545;&#34920;&#31034;&#19968;&#33268;&#24615;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#31038;&#21306;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#26377;&#38480;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#22312;&#19968;&#20010;&#39046;&#22495;&#30340;&#36827;&#23637;&#26368;&#32456;&#20250;&#22312;&#21478;&#19968;&#20010;&#39046;&#22495;&#29420;&#31435;&#22320;&#37325;&#26032;&#21457;&#29616;&#65292;&#32780;&#26356;&#24191;&#27867;&#30340;&#39046;&#22495;&#38388;&#20132;&#27969;&#23558;&#26159;&#26377;&#21033;&#30340;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#20849;&#21516;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. To what extent do the representations formed by these diverse systems agree? Can diverging representations still lead to the same behaviors? And how can systems modify their representations to better match those of another system? These questions pertaining to the study of \textbf{\emph{representational alignment}} are at the heart of some of the most active research areas in contemporary cognitive science, neuroscience, and machine learning. Unfortunately, there is limited knowledge-transfer between research communities interested in representational alignment, and much of the progress in one field ends up being rediscovered independently in another, when greater cross-field communication would be advantageous. To improve communication between fields, we propose a unifying framework that can serve as a common language b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#20998;&#31163;&#34920;&#31034;&#26469;&#22788;&#29702;&#19981;&#23436;&#20840;&#35268;&#23450;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#25463;&#24452;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#24573;&#30053;&#25463;&#24452;&#32447;&#32034;&#24182;&#36798;&#21040;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02230</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#20998;&#31163;&#34920;&#31034;&#26469;&#32531;&#35299;&#19981;&#23436;&#20840;&#35268;&#23450;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25463;&#24452;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Diffusion Disentangled Representations to Mitigate Shortcuts in Underspecified Visual Tasks. (arXiv:2310.02230v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#20998;&#31163;&#34920;&#31034;&#26469;&#22788;&#29702;&#19981;&#23436;&#20840;&#35268;&#23450;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#25463;&#24452;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#24573;&#30053;&#25463;&#24452;&#32447;&#32034;&#24182;&#36798;&#21040;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#20266;&#30456;&#20851;&#24615;&#65292;&#20854;&#20013;&#22810;&#20010;&#32447;&#32034;&#39044;&#27979;&#30446;&#26631;&#26631;&#31614;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#25463;&#24452;&#23398;&#20064;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#21487;&#33021;&#20381;&#36182;&#20110;&#38169;&#35823;&#30340;&#12289;&#26131;&#20110;&#23398;&#20064;&#30340;&#32447;&#32034;&#32780;&#24573;&#30053;&#21487;&#38752;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#30340;&#38598;&#25104;&#22810;&#26679;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#20013;&#36825;&#20123;&#32447;&#32034;&#39640;&#24230;&#30456;&#20851;&#65292;DPMs&#20855;&#26377;&#29420;&#31435;&#34920;&#31034;&#22810;&#20010;&#35270;&#35273;&#32447;&#32034;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#29305;&#24615;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#20960;&#20010;&#22810;&#26679;&#21270;&#30446;&#26631;&#19978;&#23454;&#35777;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25193;&#25955;&#24341;&#23548;&#30340;&#22810;&#26679;&#21270;&#21487;&#20197;&#20351;&#27169;&#22411;&#36991;&#24320;&#25463;&#24452;&#32447;&#32034;&#30340;&#27880;&#24847;&#65292;&#23454;&#29616;&#20102;&#19982;&#38656;&#35201;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#30340;&#20808;&#21069;&#26041;&#27861;&#21487;&#27604;&#36739;&#30340;&#38598;&#25104;&#22810;&#26679;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to shortcut learning phenomena, where a model may rely on erroneous, easy-to-learn, cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting the generation of synthetic counterfactuals using Diffusion Probabilistic Models (DPMs). We discover that DPMs have the inherent capability to represent multiple visual cues independently, even when they are largely correlated in the training data. We leverage this characteristic to encourage model diversity and empirically show the efficacy of the approach with respect to several diversification objectives. We show that diffusion-guided diversification can lead models to avert attention from shortcut cues, achieving ensemble diversity performance comparable to previous methods requiring additional data collection.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#35013;&#37197;&#36136;&#37327;&#24182;&#20248;&#21270;&#20154;&#20307;&#24037;&#31243;&#23398;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.13745</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Computer Vision Technology for Robotized Wire Harness Assembly. (arXiv:2309.13745v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#35013;&#37197;&#36136;&#37327;&#24182;&#20248;&#21270;&#20154;&#20307;&#24037;&#31243;&#23398;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#26463;&#22312;&#29616;&#20195;&#27773;&#36710;&#20013;&#26159;&#30005;&#23376;&#31995;&#32479;&#30340;&#37325;&#35201;&#30828;&#20214;&#12290;&#38543;&#30528;&#27773;&#36710;&#34892;&#19994;&#21521;&#30005;&#21160;&#21270;&#21644;&#33258;&#21160;&#39550;&#39542;&#30340;&#36716;&#21464;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#27773;&#36710;&#30005;&#23376;&#20135;&#21697;&#36127;&#36131;&#33021;&#37327;&#20256;&#36755;&#21644;&#23433;&#20840;&#20851;&#38190;&#21151;&#33021;&#65292;&#22914;&#25805;&#32437;&#12289;&#39550;&#39542;&#36741;&#21161;&#21644;&#23433;&#20840;&#31995;&#32479;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#20174;&#23433;&#20840;&#35282;&#24230;&#23545;&#27773;&#36710;&#32447;&#26463;&#25552;&#20986;&#26356;&#39640;&#30340;&#35201;&#27714;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#36710;&#36742;&#20013;&#30340;&#39640;&#36136;&#37327;&#32447;&#26463;&#35013;&#37197;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#37096;&#20998;&#32447;&#26463;&#35013;&#37197;&#25805;&#20316;&#20173;&#30001;&#29087;&#32451;&#24037;&#20154;&#25163;&#21160;&#23436;&#25104;&#65292;&#20854;&#20013;&#19968;&#20123;&#25163;&#21160;&#24037;&#24207;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#23384;&#22312;&#38382;&#39064;&#65292;&#22914;&#36136;&#37327;&#25511;&#21046;&#21644;&#20154;&#20307;&#24037;&#31243;&#23398;&#12290;&#34892;&#19994;&#20013;&#20063;&#25345;&#32493;&#38656;&#27714;&#25552;&#39640;&#31454;&#20105;&#21147;&#24182;&#33719;&#24471;&#24066;&#22330;&#20221;&#39069;&#12290;&#22240;&#27492;&#65292;&#24076;&#26395;&#33021;&#22312;&#25552;&#39640;&#20154;&#20307;&#24037;&#31243;&#23398;&#21644;&#20248;&#21270;&#21171;&#21160;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#35777;&#35013;&#37197;&#36136;&#37327;&#12290;&#26426;&#22120;&#20154;&#21270;&#35013;&#37197;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wire harnesses are essential hardware for electronic systems in modern automotive vehicles. With a shift in the automotive industry towards electrification and autonomous driving, more and more automotive electronics are responsible for energy transmission and safety-critical functions such as maneuvering, driver assistance, and safety system. This paradigm shift places more demand on automotive wiring harnesses from the safety perspective and stresses the greater importance of high-quality wire harness assembly in vehicles. However, most of the current operations of wire harness assembly are still performed manually by skilled workers, and some of the manual processes are problematic from different perspectives, such as quality control and ergonomics. There is also a persistent demand in the industry to increase competitiveness and gain market share. Hence, assuring assembly quality while improving ergonomics and optimizing labor costs is desired. Robotized assembly, accomplished by r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24635;&#32467;&#20986;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2309.13744</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#20013;&#30340;&#24212;&#29992;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Literature Review of Computer Vision Applications in Robotized Wire Harness Assembly. (arXiv:2309.13744v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24635;&#32467;&#20986;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#20013;&#30340;&#24212;&#29992;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20174;&#29616;&#26377;&#30740;&#31350;&#20013;&#25552;&#21462;&#20102;&#25361;&#25112;&#65292;&#24182;&#25214;&#21040;&#20102;&#26410;&#26469;&#30740;&#31350;&#20419;&#36827;&#26356;&#23454;&#38469;&#30340;&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a systematic literature review on computer vision applications that have been proposed for robotized wire harness assembly, derives challenges from existing studies, and identifies opportunities for future research to promote a more practical robotized assembly of wire harnesses.
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#29702;&#35299;&#65292;&#36827;&#19968;&#27493;&#20419;&#36827;&#20102;&#30446;&#26631;&#35782;&#21035;&#12289;&#21270;&#21512;&#29289;&#35774;&#35745;&#21644;&#27602;&#24615;&#39044;&#27979;&#31561;&#26041;&#38754;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.12177</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#24320;&#21457;&#20013;&#30340;&#24212;&#29992; - &#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence for Drug Discovery and Development -- A Comprehensive Survey. (arXiv:2309.12177v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12177
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#29702;&#35299;&#65292;&#36827;&#19968;&#27493;&#20419;&#36827;&#20102;&#30446;&#26631;&#35782;&#21035;&#12289;&#21270;&#21512;&#29289;&#35774;&#35745;&#21644;&#27602;&#24615;&#39044;&#27979;&#31561;&#26041;&#38754;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;AI&#21644;ML&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23545;&#20110;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#21152;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#39044;&#27979;&#30340;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#29702;&#35299;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#23558;XAI&#25216;&#26415;&#24212;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#20840;&#38754;&#27010;&#36848;&#20102;XAI&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#21508;&#31181;XAI&#26041;&#27861;&#12289;&#23427;&#20204;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;XAI&#25216;&#26415;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#35813;&#25991;&#31456;&#36824;&#28085;&#30422;&#20102;XAI&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#30446;&#26631;&#35782;&#21035;&#12289;&#21270;&#21512;&#29289;&#35774;&#35745;&#21644;&#27602;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of drug discovery has experienced a remarkable transformation with the advent of artificial intelligence (AI) and machine learning (ML) technologies. However, as these AI and ML models are becoming more complex, there is a growing need for transparency and interpretability of the models. Explainable Artificial Intelligence (XAI) is a novel approach that addresses this issue and provides a more interpretable understanding of the predictions made by machine learning models. In recent years, there has been an increasing interest in the application of XAI techniques to drug discovery. This review article provides a comprehensive overview of the current state-of-the-art in XAI for drug discovery, including various XAI methods, their application in drug discovery, and the challenges and limitations of XAI techniques in drug discovery. The article also covers the application of XAI in drug discovery, including target identification, compound design, and toxicity prediction. Furtherm
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#38454;&#27573;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#20351;&#29992;&#23376;&#27169;&#35757;&#32451;&#20989;&#25968;&#26469;&#20943;&#23569;&#24213;&#23618;&#38598;&#21512;&#65292;&#24182;&#24341;&#20837;&#20102;&#38750;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#30340;&#31532;&#19968;&#20010;&#24658;&#23450;&#22240;&#23376;&#36924;&#36817;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.05183</link><description>&lt;p&gt;
&#36229;&#36234;&#21333;&#35843;&#24615;&#30340;&#25968;&#25454;&#27719;&#24635;&#65306;&#38750;&#21333;&#35843;&#30340;&#20004;&#38454;&#27573;&#23376;&#27169;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Data Summarization beyond Monotonicity: Non-monotone Two-Stage Submodular Maximization. (arXiv:2309.05183v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05183
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#38454;&#27573;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#20351;&#29992;&#23376;&#27169;&#35757;&#32451;&#20989;&#25968;&#26469;&#20943;&#23569;&#24213;&#23618;&#38598;&#21512;&#65292;&#24182;&#24341;&#20837;&#20102;&#38750;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#30340;&#31532;&#19968;&#20010;&#24658;&#23450;&#22240;&#23376;&#36924;&#36817;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#38454;&#27573;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#20351;&#29992;&#25552;&#20379;&#30340;&#23376;&#27169;&#35757;&#32451;&#20989;&#25968;&#26469;&#20943;&#23569;&#24213;&#23618;&#38598;&#21512;&#65292;&#20197;&#30830;&#20445;&#22312;&#20943;&#23567;&#21518;&#30340;&#24213;&#23618;&#38598;&#21512;&#19978;&#20248;&#21270;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#32467;&#26524;&#19982;&#22312;&#21407;&#22987;&#24213;&#23618;&#38598;&#21512;&#19978;&#33719;&#24471;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#25968;&#25454;&#27719;&#24635;&#31561;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#65292;&#32780;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36825;&#20010;&#30740;&#31350;&#25193;&#23637;&#21040;&#20102;&#38750;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36825;&#31181;&#26356;&#19968;&#33324;&#24773;&#20917;&#30340;&#31532;&#19968;&#20010;&#24658;&#23450;&#22240;&#23376;&#36924;&#36817;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of a two-stage submodular maximization problem is to reduce the ground set using provided training functions that are submodular, with the aim of ensuring that optimizing new objective functions over the reduced ground set yields results comparable to those obtained over the original ground set. This problem has applications in various domains including data summarization. Existing studies often assume the monotonicity of the objective function, whereas our work pioneers the extension of this research to accommodate non-monotone submodular functions. We have introduced the first constant-factor approximation algorithms for this more general case.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#28982;&#20114;&#21160;&#20013;&#23454;&#29616;&#22797;&#26434;&#34892;&#20026;&#22686;&#37327;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#24182;&#28436;&#31034;&#20102;&#22312;&#19968;&#20010;&#20154;&#24418;&#26426;&#22120;&#20154;&#19978;&#30340;&#24212;&#29992;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#36827;&#34892;&#39640;&#32423;&#21327;&#35843;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25511;&#21046;&#21488;&#29983;&#25104;Python&#35821;&#21477;&#26469;&#35843;&#29992;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#21644;&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#23558;&#20154;&#31867;&#25351;&#20196;&#12289;&#29615;&#22659;&#35266;&#27979;&#21644;&#25191;&#34892;&#32467;&#26524;&#21453;&#39304;&#32473;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#24490;&#29615;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2309.04316</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#20114;&#21160;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#37327;&#23398;&#20064;&#20154;&#24418;&#26426;&#22120;&#20154;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models. (arXiv:2309.04316v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#28982;&#20114;&#21160;&#20013;&#23454;&#29616;&#22797;&#26434;&#34892;&#20026;&#22686;&#37327;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#24182;&#28436;&#31034;&#20102;&#22312;&#19968;&#20010;&#20154;&#24418;&#26426;&#22120;&#20154;&#19978;&#30340;&#24212;&#29992;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#36827;&#34892;&#39640;&#32423;&#21327;&#35843;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25511;&#21046;&#21488;&#29983;&#25104;Python&#35821;&#21477;&#26469;&#35843;&#29992;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#21644;&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#23558;&#20154;&#31867;&#25351;&#20196;&#12289;&#29615;&#22659;&#35266;&#27979;&#21644;&#25191;&#34892;&#32467;&#26524;&#21453;&#39304;&#32473;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#24490;&#29615;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#23545;&#20110;&#30452;&#35266;&#30340;&#20154;&#26426;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#19981;&#20165;&#21487;&#20197;&#29992;&#26469;&#34920;&#36798;&#20154;&#31867;&#30340;&#24847;&#22270;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#29992;&#26469;&#20256;&#36798;&#25351;&#20196;&#20197;&#25913;&#36827;&#26426;&#22120;&#20154;&#23545;&#21629;&#20196;&#30340;&#29702;&#35299;&#12290;&#38750;&#24120;&#37325;&#35201;&#30340;&#26159;&#35201;&#36171;&#20104;&#26426;&#22120;&#20154;&#20174;&#36825;&#31181;&#20132;&#20114;&#32463;&#39564;&#20013;&#22686;&#37327;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20197;&#20351;&#23427;&#20204;&#33021;&#22815;&#25913;&#36827;&#33258;&#24049;&#30340;&#34892;&#20026;&#25110;&#36991;&#20813;&#26410;&#26469;&#30340;&#38169;&#35823;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#28982;&#20114;&#21160;&#20013;&#23454;&#29616;&#22797;&#26434;&#34892;&#20026;&#22686;&#37327;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#24182;&#22312;&#19968;&#20010;&#20154;&#24418;&#26426;&#22120;&#20154;&#19978;&#28436;&#31034;&#20854;&#23454;&#29616;&#12290;&#22522;&#20110;&#26368;&#26032;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#36827;&#34892;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#39640;&#23618;&#21327;&#35843;&#30340;&#31995;&#32479;&#65292;&#36825;&#20010;&#31995;&#32479;&#30340;&#24605;&#24819;&#26159;&#35753;LLM&#22312;&#20132;&#20114;&#24335;&#25511;&#21046;&#21488;&#20013;&#29983;&#25104;Python&#35821;&#21477;&#26469;&#35843;&#29992;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#21644;&#21160;&#20316;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#25351;&#20196;&#12289;&#29615;&#22659;&#35266;&#27979;&#21644;&#25191;&#34892;&#32467;&#26524;&#21453;&#39304;&#32473;LLM&#26469;&#20851;&#38381;&#20132;&#20114;&#24490;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural-language dialog is key for intuitive human-robot interaction. It can be used not only to express humans' intents, but also to communicate instructions for improvement if a robot does not understand a command correctly. Of great importance is to endow robots with the ability to learn from such interaction experience in an incremental way to allow them to improve their behaviors or avoid mistakes in the future. In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot. Building on recent advances, we present a system that deploys Large Language Models (LLMs) for high-level orchestration of the robot's behavior, based on the idea of enabling the LLM to generate Python statements in an interactive console to invoke both robot perception and action. The interaction loop is closed by feeding back human instructions, environment observations, and execution results to the LLM, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#29615;&#23433;&#20840;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#21512;&#25104;&#26426;&#22120;&#20154;&#30340;&#23433;&#20840;&#25511;&#21046;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#26426;&#22120;&#20154;&#23398;&#20064;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#33021;&#22815;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#22330;&#26223;&#65292;&#20174;&#32780;&#22312;&#30830;&#20445;&#23433;&#20840;&#30340;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.01267</link><description>&lt;p&gt;
&#20266;&#35013;&#28216;&#25103;&#65306;&#22312;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#33258;&#20027;&#24615;&#20013;&#38381;&#29615;&#23433;&#20840;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deception Game: Closing the Safety-Learning Loop in Interactive Robot Autonomy. (arXiv:2309.01267v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#29615;&#23433;&#20840;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#21512;&#25104;&#26426;&#22120;&#20154;&#30340;&#23433;&#20840;&#25511;&#21046;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#26426;&#22120;&#20154;&#23398;&#20064;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#33021;&#22815;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#22330;&#26223;&#65292;&#20174;&#32780;&#22312;&#30830;&#20445;&#23433;&#20840;&#30340;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#31561;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#24191;&#27867;&#37096;&#32626;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#30830;&#20445;&#19982;&#20154;&#31867;&#23433;&#20840;&#20114;&#21160;&#32780;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#23433;&#20840;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#26426;&#22120;&#20154;&#22312;&#36816;&#34892;&#26102;&#23398;&#20064;&#21644;&#36866;&#24212;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#36807;&#24230;&#20445;&#23432;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38381;&#29615;&#33539;&#24335;&#65292;&#29992;&#20110;&#21512;&#25104;&#23433;&#20840;&#25511;&#21046;&#31574;&#30053;&#65292;&#26126;&#30830;&#32771;&#34385;&#26426;&#22120;&#20154;&#19981;&#26029;&#21464;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#20854;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#22330;&#26223;&#30340;&#33021;&#21147;&#65292;&#24182;&#32852;&#21512;&#32771;&#34385;&#29289;&#29702;&#21160;&#21147;&#23398;&#21644;&#26426;&#22120;&#20154;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#25239;&#24615;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#21487;&#34892;&#30340;&#26041;&#24335;&#36827;&#34892;&#39640;&#32500;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#23433;&#20840;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#20449;&#24565;&#20256;&#25773;&#21644;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#36712;&#36857;&#39044;&#27979;&#22120;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
An outstanding challenge for the widespread deployment of robotic systems like autonomous vehicles is ensuring safe interaction with humans without sacrificing performance. Existing safety methods often neglect the robot's ability to learn and adapt at runtime, leading to overly conservative behavior. This paper proposes a new closed-loop paradigm for synthesizing safe control policies that explicitly account for the robot's evolving uncertainty and its ability to quickly respond to future scenarios as they arise, by jointly considering the physical dynamics and the robot's learning algorithm. We leverage adversarial reinforcement learning for tractable safety analysis under high-dimensional learning dynamics and demonstrate our framework's ability to work with both Bayesian belief propagation and implicit learning through large pre-trained neural trajectory predictors.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Socratis&#65292;&#19968;&#20010;&#26032;&#30340;&#31038;&#20250;&#21453;&#24212;&#22522;&#20934;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#27169;&#24577;&#20869;&#23481;&#30340;&#22810;&#26679;&#21270;&#24773;&#32490;&#21453;&#24212;&#12290;&#26681;&#25454;&#20154;&#31867;&#30740;&#31350;&#32467;&#26524;&#65292;&#20154;&#20204;&#26356;&#21916;&#27426;&#20154;&#24037;&#25776;&#20889;&#30340;&#24773;&#24863;&#21407;&#22240;&#65292;&#27604;&#26426;&#22120;&#29983;&#25104;&#30340;&#35201;&#22810;2&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2308.16741</link><description>&lt;p&gt;
Socratis&#65306;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24773;&#32490;&#24847;&#35782;&#65311;
&lt;/p&gt;
&lt;p&gt;
Socratis: Are large multimodal models emotionally aware?. (arXiv:2308.16741v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16741
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Socratis&#65292;&#19968;&#20010;&#26032;&#30340;&#31038;&#20250;&#21453;&#24212;&#22522;&#20934;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#27169;&#24577;&#20869;&#23481;&#30340;&#22810;&#26679;&#21270;&#24773;&#32490;&#21453;&#24212;&#12290;&#26681;&#25454;&#20154;&#31867;&#30740;&#31350;&#32467;&#26524;&#65292;&#20154;&#20204;&#26356;&#21916;&#27426;&#20154;&#24037;&#25776;&#20889;&#30340;&#24773;&#24863;&#21407;&#22240;&#65292;&#27604;&#26426;&#22120;&#29983;&#25104;&#30340;&#35201;&#22810;2&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24773;&#32490;&#39044;&#27979;&#22522;&#20934;&#21253;&#21547;&#31895;&#31961;&#30340;&#24773;&#32490;&#26631;&#31614;&#65292;&#19981;&#32771;&#34385;&#22270;&#20687;&#21644;&#25991;&#26412;&#22312;&#20154;&#31867;&#20013;&#24341;&#21457;&#22810;&#26679;&#21270;&#24773;&#32490;&#30340;&#21508;&#31181;&#21407;&#22240;&#12290;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#23545;&#20110;&#22810;&#27169;&#24577;&#20869;&#23481;&#30340;&#21453;&#24212;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#26234;&#33021;&#26426;&#22120;&#22312;&#29983;&#25104;&#21644;&#20256;&#36882;&#20869;&#23481;&#32473;&#31038;&#20250;&#20013;&#36215;&#21040;&#26680;&#24515;&#20316;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Socratis&#65292;&#19968;&#20010;&#31038;&#20250;&#21453;&#24212;&#22522;&#20934;&#65292;&#22312;&#20854;&#20013;&#27599;&#20010;&#22270;&#20687;-&#26631;&#39064;&#65288;IC&#65289;&#23545;&#37117;&#38468;&#24102;&#26377;&#22810;&#31181;&#24773;&#32490;&#21644;&#24863;&#21463;&#23427;&#20204;&#30340;&#21407;&#22240;&#30340;&#27880;&#37322;&#12290;Socratis&#21253;&#21547;&#20102;&#26469;&#33258;5&#20010;&#24191;&#27867;&#38405;&#35835;&#30340;&#26032;&#38395;&#21644;&#22270;&#20687;&#26631;&#39064;&#65288;IC&#65289;&#25968;&#25454;&#38598;&#30340;2075&#20010;&#22270;&#20687;-&#26631;&#39064;&#23545;&#30340;980&#20010;&#24773;&#32490;&#30340;18K&#20010;&#33258;&#30001;&#24418;&#24335;&#21453;&#24212;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#23450;IC&#23545;&#30340;&#24773;&#24863;&#21407;&#22240;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26681;&#25454;&#19968;&#20010;&#21021;&#27493;&#30340;&#20154;&#31867;&#30740;&#31350;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20154;&#20204;&#26356;&#21916;&#27426;&#20154;&#24037;&#25776;&#20889;&#30340;&#21407;&#22240;&#65292;&#27604;&#26426;&#22120;&#29983;&#25104;&#30340;&#35201;&#22810;2&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a \underline{soc}ietal \underline{r}e\underline{a}c\underline{ti}on\underline{s} benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-genera
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#24490;&#29615;&#31070;&#32463;&#30005;&#36335;&#22914;&#20309;&#20174;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#65292;&#24182;&#35777;&#26126;&#20102;&#24102;&#26377;&#21333;&#29420;&#36755;&#20986;&#21333;&#20803;&#30340;&#31070;&#32463;&#30005;&#36335;&#30340;&#21457;&#25918;&#29575;&#21160;&#21147;&#23398;&#21487;&#20197;&#20174;&#20219;&#24847;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#12290;</title><link>http://arxiv.org/abs/2308.11809</link><description>&lt;p&gt;
&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#36798;&#24615;&#27010;&#29575;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Expressive probabilistic sampling in recurrent neural networks. (arXiv:2308.11809v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11809
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#24490;&#29615;&#31070;&#32463;&#30005;&#36335;&#22914;&#20309;&#20174;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#65292;&#24182;&#35777;&#26126;&#20102;&#24102;&#26377;&#21333;&#29420;&#36755;&#20986;&#21333;&#20803;&#30340;&#31070;&#32463;&#30005;&#36335;&#30340;&#21457;&#25918;&#29575;&#21160;&#21147;&#23398;&#21487;&#20197;&#20174;&#20219;&#24847;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#37319;&#26679;&#30340;&#22823;&#33041;&#21151;&#33021;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#65292;&#20551;&#35774;&#31070;&#32463;&#27963;&#21160;&#26159;&#26469;&#33258;&#22823;&#33041;&#29992;&#20110;&#27010;&#29575;&#35745;&#31639;&#30340;&#27010;&#29575;&#20998;&#24067;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#31070;&#32463;&#21160;&#21147;&#23398;&#26426;&#21046;&#27169;&#22411;&#22914;&#20309;&#20174;&#20219;&#24847;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#20989;&#25968;&#20998;&#26512;&#21644;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24037;&#20855;&#26469;&#25506;&#32034;$\textit{&#24490;&#29615;}$&#31070;&#32463;&#30005;&#36335;&#20174;&#22797;&#26434;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#30340;&#26368;&#23567;&#26550;&#26500;&#35201;&#27714;&#12290;&#39318;&#20808;&#25105;&#20204;&#32771;&#34385;&#20256;&#32479;&#30340;&#37319;&#26679;&#27169;&#22411;&#65292;&#23427;&#30001;&#19968;&#20010;&#31070;&#32463;&#20803;&#32593;&#32476;&#32452;&#25104;&#65292;&#20854;&#36755;&#20986;&#30452;&#25509;&#34920;&#31034;&#26679;&#26412;&#65288;&#20165;&#37319;&#26679;&#22120;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#31361;&#35302;&#30005;&#27969;&#21644;&#21457;&#25918;&#29575;&#21160;&#21147;&#23398;&#33021;&#22815;&#20174;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#24102;&#26377;&#21333;&#29420;&#30340;&#36755;&#20986;&#21333;&#20803;&#38598;&#30340;&#24490;&#29615;&#31070;&#32463;&#30005;&#36335;&#30340;&#21457;&#25918;&#29575;&#21160;&#21147;&#23398;&#21487;&#20197;&#20174;&#20219;&#24847;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
In sampling-based Bayesian models of brain function, neural activities are assumed to be samples from probability distributions that the brain uses for probabilistic computation. However, a comprehensive understanding of how mechanistic models of neural dynamics can sample from arbitrary distributions is still lacking. We use tools from functional analysis and stochastic differential equations to explore the minimum architectural requirements for $\textit{recurrent}$ neural circuits to sample from complex distributions. We first consider the traditional sampling model consisting of a network of neurons whose outputs directly represent the samples (sampler-only network). We argue that synaptic current and firing-rate dynamics in the traditional model have limited capacity to sample from a complex probability distribution. We show that the firing rate dynamics of a recurrent neural circuit with a separate set of output units can sample from an arbitrary probability distribution. We call 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06534</link><description>&lt;p&gt;
&#35299;&#20915;&#21307;&#23398;&#24433;&#20687;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23567;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#38382;&#39064;&#65306;&#23545;&#27604;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#26377;&#28508;&#21147;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#30340;&#39118;&#38505;&#12289;&#20943;&#36731;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#36127;&#25285;&#24182;&#21152;&#36895;&#30830;&#35786;&#12290;&#35757;&#32451;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#22411;&#19988;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25552;&#20379;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#65292;&#30001;&#20110;&#27880;&#37322;&#30340;&#39640;&#22797;&#26434;&#24615;&#12289;&#21463;&#38480;&#30340;&#33719;&#21462;&#26041;&#24335;&#25110;&#30142;&#30149;&#30340;&#32597;&#35265;&#24615;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#23567;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#23567;&#22411;&#30340;&#24050;&#27880;&#37322;&#25968;&#25454;&#38598;&#23601;&#36275;&#20197;&#23545;&#27169;&#22411;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#19979;&#28216;&#20219;&#21153;&#8221;&#12290;&#21307;&#23398;&#24433;&#20687;&#20013;&#26368;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#22522;&#20110;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#33258;&#28982;&#22270;&#20687;&#22788;&#29702;&#30740;&#31350;&#34920;&#26126;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20108;&#32773;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task, the so-called ``downstream task". The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares sta
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;AI&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#20154;&#31867;&#22312;&#20915;&#31574;&#20013;&#24179;&#34913;&#33258;&#36523;&#21033;&#30410;&#19982;&#20182;&#20154;&#21033;&#30410;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#20294;&#23384;&#22312;&#39640;&#20272;&#20182;&#20154;&#20851;&#27880;&#34892;&#20026;&#30340;&#20542;&#21521;&#65292;&#36825;&#23545;AI&#30340;&#24320;&#21457;&#32773;&#21644;&#29992;&#25143;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.12776</link><description>&lt;p&gt;
&#39044;&#27979;&#20154;&#31867;&#22914;&#20309;&#22312;&#33258;&#36523;&#21033;&#30410;&#19982;&#20182;&#20154;&#21033;&#30410;&#20043;&#38388;&#24179;&#34913;&#30340;&#21487;&#39044;&#27979;&#24615;
&lt;/p&gt;
&lt;p&gt;
Predict-AI-bility of how humans balance self-interest with the interest of others. (arXiv:2307.12776v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12776
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#20154;&#31867;&#22312;&#20915;&#31574;&#20013;&#24179;&#34913;&#33258;&#36523;&#21033;&#30410;&#19982;&#20182;&#20154;&#21033;&#30410;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#20294;&#23384;&#22312;&#39640;&#20272;&#20182;&#20154;&#20851;&#27880;&#34892;&#20026;&#30340;&#20542;&#21521;&#65292;&#36825;&#23545;AI&#30340;&#24320;&#21457;&#32773;&#21644;&#29992;&#25143;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20855;&#26377;&#38761;&#21629;&#24615;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#25913;&#21464;&#20174;&#26085;&#24120;&#29983;&#27963;&#21040;&#39640;&#39118;&#38505;&#22330;&#26223;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35768;&#22810;&#20915;&#31574;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#65292;&#20026;&#20102;&#20351;AI&#33021;&#22815;&#25104;&#20026;&#21487;&#38752;&#30340;&#20915;&#31574;&#21161;&#25163;&#65292;&#23427;&#24517;&#39035;&#33021;&#22815;&#25429;&#25417;&#33258;&#36523;&#21033;&#30410;&#19982;&#20182;&#20154;&#21033;&#30410;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#23545;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#26469;&#33258;12&#20010;&#22269;&#23478;&#30340;78&#20010;&#23454;&#39564;&#30340;&#29420;&#35009;&#32773;&#28216;&#25103;&#20915;&#31574;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#26377;GPT-4&#65288;&#32780;&#19981;&#26159;Bard&#25110;Bing&#65289;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#34892;&#20026;&#27169;&#24335;&#30340;&#23450;&#24615;&#29305;&#24449;&#65292;&#35782;&#21035;&#20986;&#19977;&#31181;&#20027;&#35201;&#30340;&#34892;&#20026;&#31867;&#21035;&#65306;&#33258;&#31169;&#30340;&#12289;&#19981;&#20844;&#24179;&#21388;&#24694;&#30340;&#21644;&#23436;&#20840;&#26080;&#31169;&#30340;&#12290;&#28982;&#32780;&#65292;GPT-4&#19968;&#30452;&#39640;&#20272;&#20102;&#20182;&#20154;&#20851;&#27880;&#34892;&#20026;&#65292;&#22840;&#22823;&#20102;&#19981;&#20844;&#24179;&#21388;&#24694;&#21644;&#23436;&#20840;&#26080;&#31169;&#21442;&#19982;&#32773;&#30340;&#27604;&#20363;&#12290;&#36825;&#31181;&#20559;&#35265;&#23545;&#20110;AI&#24320;&#21457;&#20154;&#21592;&#21644;&#29992;&#25143;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative artificial intelligence holds enormous potential to revolutionize decision-making processes, from everyday to high-stake scenarios. However, as many decisions carry social implications, for AI to be a reliable assistant for decision-making it is crucial that it is able to capture the balance between self-interest and the interest of others. We investigate the ability of three of the most advanced chatbots to predict dictator game decisions across 78 experiments with human participants from 12 countries. We find that only GPT-4 (not Bard nor Bing) correctly captures qualitative behavioral patterns, identifying three major classes of behavior: self-interested, inequity-averse, and fully altruistic. Nonetheless, GPT-4 consistently overestimates other-regarding behavior, inflating the proportion of inequity-averse and fully altruistic participants. This bias has significant implications for AI developers and users.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#38170;&#28857;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#26032;&#30340;&#31867;&#21035;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24494;&#35843;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20195;&#29702;&#38170;&#28857;&#65292;&#23558;&#26679;&#26412;&#20998;&#20026;&#26087;&#30340;&#21644;&#26032;&#30340;&#31867;&#21035;&#65292;&#24182;&#29983;&#25104;&#20195;&#34920;&#24615;&#30340;&#31867;&#21035;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.10943</link><description>&lt;p&gt;
&#22522;&#20110;&#20195;&#29702;&#38170;&#28857;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#36830;&#32493;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Proxy Anchor-based Unsupervised Learning for Continuous Generalized Category Discovery. (arXiv:2307.10943v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#38170;&#28857;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#26032;&#30340;&#31867;&#21035;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24494;&#35843;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20195;&#29702;&#38170;&#28857;&#65292;&#23558;&#26679;&#26412;&#20998;&#20026;&#26087;&#30340;&#21644;&#26032;&#30340;&#31867;&#21035;&#65292;&#24182;&#29983;&#25104;&#20195;&#34920;&#24615;&#30340;&#31867;&#21035;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#22686;&#37327;&#23398;&#20064;&#22330;&#26223;&#20013;&#21457;&#29616;&#26032;&#30340;&#31867;&#21035;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#32570;&#20047;&#20851;&#20110;&#26032;&#31867;&#21035;&#25968;&#37327;&#21644;&#24615;&#36136;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;&#26032;&#31867;&#21035;&#21457;&#29616;&#26041;&#27861;&#21463;&#38480;&#20110;&#23545;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#25209;&#27425;&#20013;&#26032;&#26679;&#26412;&#25968;&#37327;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#24182;&#26356;&#20934;&#30830;&#22320;&#21453;&#26144;&#23454;&#38469;&#22330;&#26223;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#26631;&#31614;&#38598;&#19978;&#21457;&#29616;&#26032;&#30340;&#31867;&#21035;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#26631;&#35760;&#38598;&#19978;&#24494;&#35843;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20195;&#29702;&#38170;&#28857;&#65292;&#28982;&#21518;&#23558;&#26679;&#26412;&#20998;&#20026;&#26087;&#30340;&#21644;&#26032;&#30340;&#31867;&#21035;&#65292;&#24182;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20195;&#29702;&#38170;&#28857;&#30340;&#26679;&#26412;&#29983;&#25104;&#20195;&#34920;&#24615;&#31867;&#21035;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning have significantly improved the performance of various computer vision applications. However, discovering novel categories in an incremental learning scenario remains a challenging problem due to the lack of prior knowledge about the number and nature of new categories. Existing methods for novel category discovery are limited by their reliance on labeled datasets and prior knowledge about the number of novel categories and the proportion of novel samples in the batch. To address the limitations and more accurately reflect real-world scenarios, in this paper, we propose a novel unsupervised class incremental learning approach for discovering novel categories on unlabeled sets without prior knowledge. The proposed method fine-tunes the feature extractor and proxy anchors on labeled sets, then splits samples into old and novel categories and clusters on the unlabeled dataset. Furthermore, the proxy anchors-based exemplar generates representative category 
&lt;/p&gt;</description></item><item><title>VoxPoser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#22312;&#22810;&#31181;&#25805;&#20316;&#20219;&#21153;&#19979;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#21644;&#23545;&#35937;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05973</link><description>&lt;p&gt;
VoxPoser: &#29992;&#20110;&#24102;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#21487;&#32452;&#21512;&#30340;3D&#20215;&#20540;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05973
&lt;/p&gt;
&lt;p&gt;
VoxPoser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#22312;&#22810;&#31181;&#25805;&#20316;&#20219;&#21153;&#19979;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#21644;&#23545;&#35937;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20016;&#23500;&#30340;&#21487;&#34892;&#21160;&#30693;&#35782;&#65292;&#21487;&#20197;&#20197;&#25512;&#29702;&#21644;&#35268;&#21010;&#30340;&#24418;&#24335;&#25552;&#21462;&#20986;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#20173;&#28982;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#36816;&#21160;&#21407;&#35821;&#26469;&#25191;&#34892;&#19982;&#29615;&#22659;&#30340;&#29289;&#29702;&#20132;&#20114;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#24320;&#38598;&#25351;&#20196;&#21644;&#24320;&#38598;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#65292;&#21363;&#19968;&#31995;&#21015;&#23494;&#38598;&#30340;6-DoF&#26411;&#31471;&#25191;&#34892;&#22120;&#36335;&#24452;&#28857;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;LLMs&#22312;&#32473;&#23450;&#33258;&#30001;&#24418;&#24335;&#30340;&#35821;&#35328;&#25351;&#20196;&#26102;&#25797;&#38271;&#25512;&#26029;&#21487;&#34892;&#24615;&#21644;&#32422;&#26463;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#30340;&#20195;&#30721;&#32534;&#20889;&#33021;&#21147;&#65292;&#23427;&#20204;&#21487;&#20197;&#19982;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20132;&#20114;&#65292;&#20197;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#23558;&#30693;&#35782;&#25509;&#22320;&#21040;Agent&#30340;&#35266;&#27979;&#31354;&#38388;&#20013;&#12290;&#28982;&#21518;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#26694;&#26550;&#20013;&#20351;&#29992;&#32452;&#21512;&#30340;&#20215;&#20540;&#26144;&#23556;&#26469;&#38646;&#35797;&#21512;&#25104;&#38381;&#29615;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a visual-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop ro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#23601;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24378;&#20195;&#29702;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#39537;&#21160;&#21644;&#23618;&#27425;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03486</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21457;&#29616;&#23618;&#27425;&#21270;&#25104;&#23601;
&lt;/p&gt;
&lt;p&gt;
Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning. (arXiv:2307.03486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03486
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#23601;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24378;&#20195;&#29702;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#39537;&#21160;&#21644;&#23618;&#27425;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#29615;&#22659;&#20013;&#21457;&#29616;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#25104;&#23601;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#38656;&#35201;&#26234;&#33021;&#20307;&#20855;&#22791;&#24191;&#27867;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#27867;&#21270;&#21644;&#38271;&#26399;&#25512;&#29702;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#25110;&#23618;&#27425;&#21270;&#26041;&#27861;&#65292;&#35748;&#20026;&#26174;&#24335;&#30340;&#38271;&#26399;&#35268;&#21010;&#27169;&#22359;&#23545;&#20110;&#23398;&#20064;&#23618;&#27425;&#21270;&#25104;&#23601;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#29615;&#22659;&#20132;&#20114;&#25110;&#22823;&#22411;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36817;&#26399;&#23454;&#26045;&#23454;&#36341;&#20013;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#31639;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;PPO&#26234;&#33021;&#20307;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#39044;&#27979;&#19979;&#19968;&#20010;&#35201;&#35299;&#38145;&#30340;&#25104;&#23601;&#65292;&#23613;&#31649;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#36739;&#20302;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#25104;&#23601;&#33976;&#39311;&#65292;&#21487;&#20197;&#21152;&#24378;PPO&#26234;&#33021;&#20307;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering achievements with a hierarchical structure on procedurally generated environments poses a significant challenge. This requires agents to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods are built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be beneficial for learning hierarchical achievements. However, these methods require an excessive amount of environment interactions or large model sizes, limiting their practicality. In this work, we identify that proximal policy optimization (PPO), a simple and versatile model-free algorithm, outperforms the prior methods with recent implementation practices. Moreover, we find that the PPO agent can predict the next achievement to be unlocked to some extent, though with low confidence. Based on this observation, we propose a novel contrastive learning method, called achievement distillation, that strengthens the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;EHRSHOT&#65292;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;&#12290;&#35813;&#35770;&#25991;&#21033;&#29992;EHRSHOT&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;CLMBR-T-base&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.02028</link><description>&lt;p&gt;
EHRSHOT:&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models. (arXiv:2307.02028v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;EHRSHOT&#65292;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;&#12290;&#35813;&#35770;&#25991;&#21033;&#29992;EHRSHOT&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;CLMBR-T-base&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#19968;&#33324;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#31038;&#21306;&#24050;&#32463;&#21463;&#30410;&#20110;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#20294;&#26159;ML&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#20849;&#20139;&#36164;&#20135;&#30340;&#32570;&#20047;&#30340;&#38459;&#30861;&#12290;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35775;&#38382;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#39564;&#35777;&#24615;&#33021;&#20248;&#21183;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#36129;&#29486;&#26469;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;EHRSHOT&#65292;&#20854;&#20013;&#21253;&#21547;6,739&#21517;&#26469;&#33258;&#26031;&#22374;&#31119;&#21307;&#23398;&#30340;&#24739;&#32773;&#30340;&#21435;&#35782;&#21035;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#25968;&#25454;&#12290;&#19982;MIMIC-III/IV&#21644;&#20854;&#20182;&#27969;&#34892;&#30340;EHR&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;EHRSHOT&#26159;&#32437;&#21521;&#30340;&#65292;&#19981;&#20165;&#23616;&#38480;&#20110;ICU/ED&#24739;&#32773;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;CLMBR-T-base&#30340;&#26435;&#37325;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#30340;141M&#21442;&#25968;&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#25968;&#25454;&#21253;&#25324;2.57M&#21517;&#24739;&#32773;&#12290;&#25105;&#20204;&#26159;&#26368;&#26089;&#23436;&#20840;&#21457;&#24067;&#36825;&#26679;&#19968;&#20010;&#29992;&#20110;&#32534;&#30721;EHR&#25968;&#25454;&#30340;&#27169;&#22411;&#20043;&#19968;&#65307;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#21457;&#24067;&#30340;&#20020;&#24202;&#25968;&#25454;&#27169;&#22411;&#65288;&#22914;GatorTron&#12289;ClinicalBER&#65289;&#24182;&#27809;&#26377;&#23436;&#20840;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains deidentified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBER
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;Physion++&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#38656;&#35201;&#20934;&#30830;&#20272;&#35745;&#22330;&#26223;&#20013;&#29289;&#20307;&#28508;&#22312;&#29289;&#29702;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#30340;&#35270;&#35273;&#29289;&#29702;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.15668</link><description>&lt;p&gt;
Physion++&#65306;&#23545;&#38656;&#35201;&#22312;&#32447;&#25512;&#29702;&#19981;&#21516;&#29289;&#29702;&#23646;&#24615;&#30340;&#29289;&#29702;&#22330;&#26223;&#29702;&#35299;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Physion++: Evaluating Physical Scene Understanding that Requires Online Inference of Different Physical Properties. (arXiv:2306.15668v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15668
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;Physion++&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#38656;&#35201;&#20934;&#30830;&#20272;&#35745;&#22330;&#26223;&#20013;&#29289;&#20307;&#28508;&#22312;&#29289;&#29702;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#30340;&#35270;&#35273;&#29289;&#29702;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#30340;&#29289;&#29702;&#22330;&#26223;&#29702;&#35299;&#38656;&#35201;&#30340;&#19981;&#20165;&#20165;&#26159;&#23450;&#20301;&#21644;&#35782;&#21035;&#29289;&#20307;&#65292;&#36824;&#38656;&#35201;&#20102;&#35299;&#29289;&#20307;&#21487;&#20197;&#20855;&#26377;&#19981;&#21516;&#30340;&#28508;&#22312;&#23646;&#24615;&#65288;&#20363;&#22914;&#36136;&#37327;&#25110;&#24377;&#24615;&#65289;&#65292;&#24182;&#19988;&#36825;&#20123;&#23646;&#24615;&#20250;&#24433;&#21709;&#29289;&#29702;&#20107;&#20214;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#29289;&#29702;&#21644;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#35780;&#20272;&#23427;&#20204;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#36890;&#24120;&#19981;&#35201;&#27714;&#29702;&#35299;&#29289;&#20307;&#20855;&#26377;&#20010;&#20307;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;&#25110;&#32773;&#26368;&#22810;&#21482;&#27979;&#35797;&#21487;&#20197;&#30452;&#25509;&#35266;&#23519;&#21040;&#30340;&#23646;&#24615;&#65288;&#20363;&#22914;&#22823;&#23567;&#25110;&#39068;&#33394;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;Physion++&#65292;&#22312;&#36825;&#20010;&#20154;&#24037;&#31995;&#32479;&#19979;&#20005;&#26684;&#35780;&#20272;&#20102;&#35270;&#35273;&#29289;&#29702;&#39044;&#27979;&#65292;&#20854;&#20013;&#39044;&#27979;&#20381;&#36182;&#20110;&#22330;&#26223;&#20013;&#29289;&#20307;&#28508;&#22312;&#29289;&#29702;&#23646;&#24615;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#20934;&#30830;&#39044;&#27979;&#20381;&#36182;&#20110;&#36136;&#37327;&#12289;&#25705;&#25830;&#21147;&#12289;&#24377;&#24615;&#21644;&#21487;&#21464;&#24615;&#31561;&#23646;&#24615;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
General physical scene understanding requires more than simply localizing and recognizing objects -- it requires knowledge that objects can have different latent properties (e.g., mass or elasticity), and that those properties affect the outcome of physical events. While there has been great progress in physical and video prediction models in recent years, benchmarks to test their performance typically do not require an understanding that objects have individual physical properties, or at best test only those properties that are directly observable (e.g., size or color). This work proposes a novel dataset and benchmark, termed Physion++, that rigorously evaluates visual physical prediction in artificial systems under circumstances where those predictions rely on accurate estimates of the latent physical properties of objects in the scene. Specifically, we test scenarios where accurate prediction relies on estimates of properties such as mass, friction, elasticity, and deformability, an
&lt;/p&gt;</description></item><item><title>Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.09750</link><description>&lt;p&gt;
Fedstellar&#65306;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Fedstellar: A Platform for Decentralized Federated Learning. (arXiv:2306.09750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09750
&lt;/p&gt;
&lt;p&gt;
Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2016&#24180;&#65292;&#35895;&#27468;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#36328;&#32852;&#30431;&#21442;&#19982;&#32773;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#34429;&#28982;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;CFL&#65289;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#23384;&#22312;&#36890;&#20449;&#29942;&#39048;&#12289;&#21333;&#28857;&#25925;&#38556;&#21644;&#23545;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#20381;&#36182;&#31561;&#23616;&#38480;&#12290;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#36890;&#36807;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#27169;&#22411;&#32858;&#21512;&#21644;&#26368;&#23567;&#21270;&#23545;&#20013;&#22830;&#23454;&#20307;&#30340;&#20381;&#36182;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35757;&#32451;DFL&#27169;&#22411;&#30340;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Fedstellar&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#24179;&#21488;&#65292;&#26088;&#22312;&#22312;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#19981;&#21516;&#32852;&#30431;&#20013;&#20197;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;FL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train Machine Learning (ML) models across the participants of a federation while preserving data privacy. Since its birth, Centralized FL (CFL) has been the most used approach, where a central entity aggregates participants' models to create a global one. However, CFL presents limitations such as communication bottlenecks, single point of failure, and reliance on a central server. Decentralized Federated Learning (DFL) addresses these issues by enabling decentralized model aggregation and minimizing dependency on a central entity. Despite these advances, current platforms training DFL models struggle with key issues such as managing heterogeneous federation network topologies. To overcome these challenges, this paper presents Fedstellar, a novel platform designed to train FL models in a decentralized, semi-decentralized, and centralized fashion across diverse federations of physical or virtualized devices. The Feds
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#25152;&#38656;&#30340;&#22806;&#37096;&#30693;&#35782;&#33719;&#21462;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.08129</link><description>&lt;p&gt;
AVIS:&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#35270;&#35273;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
AVIS: Autonomous Visual Information Seeking with Large Language Models. (arXiv:2306.08129v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#25152;&#38656;&#30340;&#22806;&#37096;&#30693;&#35782;&#33719;&#21462;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23454;&#29616;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#30340;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLM&#21160;&#24577;&#22320;&#21046;&#23450;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31574;&#30053;&#65292;&#24182;&#35843;&#26597;&#23427;&#20204;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#33719;&#21462;&#25552;&#20379;&#25152;&#25552;&#20986;&#38382;&#39064;&#25152;&#38656;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#30693;&#35782;&#12290;&#22238;&#31572;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#39064;&#65292;&#22914;&#8220;&#36825;&#24133;&#22270;&#20687;&#25152;&#25551;&#32472;&#30340;&#24314;&#31569;&#29289;&#26159;&#20026;&#20102;&#32426;&#24565;&#21738;&#20010;&#20107;&#20214;&#65311;&#8221;&#65292;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#21576;&#29616;&#20986;&#19968;&#20010;&#32452;&#21512;&#25628;&#32034;&#31354;&#38388;&#65292;&#38656;&#35201;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#21253;&#25324;&#35843;&#29992;API&#12289;&#20998;&#26512;&#23427;&#20204;&#30340;&#21709;&#24212;&#24182;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#25910;&#38598;&#20102;&#20154;&#31867;&#38754;&#23545;&#36825;&#20010;&#20219;&#21153;&#26102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20915;&#31574;&#23454;&#20363;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#30340;&#31995;&#32479;&#65306;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#30830;&#23450;&#19979;&#19968;&#20010;&#35201;&#20351;&#29992;&#30340;&#24037;&#20855;&#65307;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#25512;&#29702;&#22120;&#65292;&#20998;&#26512;&#24182;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as "What event is commemorated by the building depicted in this image?", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;nuPlan&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#26696;&#65292;&#38024;&#23545;&#31934;&#30830;&#30340;&#30701;&#26399;&#35268;&#21010;&#21644;&#38271;&#26399;&#30446;&#26631;&#39044;&#27979;&#12290;&#35777;&#23454;&#20102;&#29616;&#26377;&#31995;&#32479;&#38590;&#20197;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#35201;&#27714;&#12290;&#26368;&#32456;&#25552;&#20986;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#39640;&#25928;&#30340;&#35268;&#21010;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.07962</link><description>&lt;p&gt;
&#19982;&#23398;&#20064;&#23548;&#21521;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#30340;&#35823;&#35299;&#21578;&#21035;
&lt;/p&gt;
&lt;p&gt;
Parting with Misconceptions about Learning-based Vehicle Motion Planning. (arXiv:2306.07962v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07962
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;nuPlan&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#26696;&#65292;&#38024;&#23545;&#31934;&#30830;&#30340;&#30701;&#26399;&#35268;&#21010;&#21644;&#38271;&#26399;&#30446;&#26631;&#39044;&#27979;&#12290;&#35777;&#23454;&#20102;&#29616;&#26377;&#31995;&#32479;&#38590;&#20197;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#35201;&#27714;&#12290;&#26368;&#32456;&#25552;&#20986;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#39640;&#25928;&#30340;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
nuPlan&#30340;&#21457;&#24067;&#26631;&#24535;&#30528;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#26102;&#20195;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#38656;&#35201;&#31934;&#30830;&#30340;&#30701;&#26399;&#35268;&#21010;&#21644;&#38271;&#26399;&#30446;&#26631;&#39044;&#27979;&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#26696;&#12290;&#29616;&#26377;&#31995;&#32479;&#38590;&#20197;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#35201;&#27714;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#20219;&#21153;&#23384;&#22312;&#26681;&#26412;&#19978;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24212;&#35813;&#20998;&#21035;&#36827;&#34892;&#35299;&#20915;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#39046;&#22495;&#20869;&#38381;&#29615;&#35268;&#21010;&#30340;&#29616;&#29366;&#65292;&#25581;&#31034;&#20102;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#22312;&#22797;&#26434;&#30340;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#36873;&#25321;&#36890;&#36807;&#36710;&#36947;&#22270;&#25628;&#32034;&#31639;&#27861;&#30340;&#31616;&#21333;&#22522;&#20110;&#35268;&#21017;&#30340;&#20808;&#39564;&#39033;&#65288;&#20363;&#22914;&#20013;&#24515;&#32447;&#36873;&#25321;&#65289;&#30340;&#20215;&#20540;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#24320;&#29615;&#23376;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#20165;&#20351;&#29992;&#36825;&#20010;&#20013;&#24515;&#32447;&#20316;&#20026;&#22330;&#26223;&#19978;&#19979;&#25991;&#26102;&#65288;&#21363;&#24573;&#30053;&#25152;&#26377;&#26377;&#20851;&#22320;&#22270;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20449;&#24687;&#65289;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#32467;&#21512;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#39640;&#25928;&#30340;&#35268;&#21010;&#22120;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#22823;&#37327;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of nuPlan marks a new era in vehicle motion planning research, offering the first large-scale real-world dataset and evaluation schemes requiring both precise short-term planning and long-horizon ego-forecasting. Existing systems struggle to simultaneously meet both requirements. Indeed, we find that these tasks are fundamentally misaligned and should be addressed independently. We further assess the current state of closed-loop planning in the field, revealing the limitations of learning-based methods in complex real-world scenarios and the value of simple rule-based priors such as centerline selection through lane graph search algorithms. More surprisingly, for the open-loop sub-task, we observe that the best results are achieved when using only this centerline as scene context (\ie, ignoring all information regarding the map and other agents). Combining these insights, we propose an extremely simple and efficient planner which outperforms an extensive set of competitors,
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#29289;&#21697;&#27969;&#34892;&#24230;&#12289;&#36136;&#37327;&#21644;&#20301;&#32622;&#20559;&#24046;&#23545;&#29992;&#25143;&#31119;&#21033;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#25506;&#32034;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#36127;&#38754;&#24433;&#21709;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18333</link><description>&lt;p&gt;
&#20855;&#26377;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#25490;&#21517;&#65306;&#33258;&#22686;&#24378;&#21160;&#24577;&#19979;&#30340;&#29992;&#25143;&#31119;&#21033;
&lt;/p&gt;
&lt;p&gt;
Ranking with Popularity Bias: User Welfare under Self-Amplification Dynamics. (arXiv:2305.18333v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18333
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#29289;&#21697;&#27969;&#34892;&#24230;&#12289;&#36136;&#37327;&#21644;&#20301;&#32622;&#20559;&#24046;&#23545;&#29992;&#25143;&#31119;&#21033;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#25506;&#32034;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#36127;&#38754;&#24433;&#21709;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24050;&#32463;&#30830;&#35748;&#27969;&#34892;&#24230;&#20559;&#35265;&#22312;&#25512;&#33616;&#65288;&#21644;&#20854;&#20182;&#22522;&#20110;&#25490;&#21517;&#30340;&#65289;&#31995;&#32479;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#20854;&#23545;&#29992;&#25143;&#31119;&#21033;&#30340;&#24433;&#21709;&#30340;&#35814;&#32454;&#20998;&#26512;&#20173;&#28982;&#32570;&#20047;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26426;&#21046;&#65292;&#36890;&#36807;&#23427;&#65292;&#29289;&#21697;&#30340;&#27969;&#34892;&#24230;&#12289;&#36136;&#37327;&#21644;&#20301;&#32622;&#20559;&#24046;&#21487;&#20197;&#24433;&#21709;&#29992;&#25143;&#36873;&#25321;&#65292;&#24182;&#19988;&#21487;&#20197;&#36127;&#38754;&#24433;&#21709;&#21508;&#31181;&#25512;&#33616;&#31574;&#30053;&#30340;&#38598;&#20307;&#29992;&#25143;&#25928;&#29992;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#38750;&#24179;&#31283;&#19978;&#19979;&#25991;&#33073;&#38774;&#26426;&#65292;&#24378;&#35843;&#19981;&#26159;&#20026;&#20102;&#28040;&#38500;&#27969;&#34892;&#24230;&#20559;&#35265;&#32780;&#26159;&#20026;&#20102;&#20943;&#36731;&#20854;&#36127;&#38754;&#24433;&#21709;&#32780;&#36827;&#34892;&#25506;&#32034;&#30340;&#37325;&#35201;&#24615;&#12290;&#39318;&#20808;&#65292;&#26222;&#36890;&#30340;&#26377;&#27969;&#34892;&#24230;&#20559;&#24046;&#30340;&#25512;&#33616;&#31995;&#32479;&#20250;&#36890;&#36807;&#28151;&#28102;&#29289;&#21697;&#36136;&#37327;&#21644;&#27969;&#34892;&#24230;&#32780;&#24341;&#21457;&#32447;&#24615;&#36951;&#25022;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#32447;&#24615;&#35774;&#32622;&#19979;&#65292;&#30001;&#20110;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#29289;&#21697;&#36136;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#20063;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#36275;&#22815;&#21464;&#24322;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31867;UCB&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#26377;&#25928;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#23454;&#20102;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
While popularity bias is recognized to play a role in recommmender (and other ranking-based) systems, detailed analyses of its impact on user welfare have largely been lacking. We propose a general mechanism by which item popularity, item quality, and position bias can impact user choice, and how it can negatively impact the collective user utility of various recommender policies. Formulating the problem as a non-stationary contextual bandit, we highlight the importance of exploration, not to eliminate popularity bias, but to mitigate its negative effects. First, naive popularity-biased recommenders are shown to induce linear regret by conflating item quality and popularity. More generally, we show that, even in linear settings, identifiability of item quality may not be possible due to the confounding effects of popularity bias. However, under sufficient variability assumptions, we develop an efficient UCB-style algorithm and prove efficient regret guarantees. We complement our analys
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#26174;&#24335;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35268;&#21010;&#12290;&#36890;&#36807;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;PDDL&#21644;&#32416;&#27491;&#21453;&#39304;&#28304;&#20043;&#38388;&#30340;&#25509;&#21475;&#65292;&#22312;&#29992;&#25143;&#19981;&#25026;PDDL&#30340;&#24773;&#20917;&#19979;&#23558;PDDL&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#24182;&#26377;&#25928;&#22320;&#32534;&#30721;&#32416;&#27491;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2305.14909</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#21644;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#20219;&#21153;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning. (arXiv:2305.14909v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#26174;&#24335;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35268;&#21010;&#12290;&#36890;&#36807;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;PDDL&#21644;&#32416;&#27491;&#21453;&#39304;&#28304;&#20043;&#38388;&#30340;&#25509;&#21475;&#65292;&#22312;&#29992;&#25143;&#19981;&#25026;PDDL&#30340;&#24773;&#20917;&#19979;&#23558;PDDL&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#24182;&#26377;&#25928;&#22320;&#32534;&#30721;&#32416;&#27491;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#35268;&#21010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#35268;&#21010;&#22120;&#30340;&#26041;&#27861;&#30446;&#21069;&#22312;&#23454;&#38469;&#25805;&#20316;&#20013;&#24182;&#19981;&#21487;&#34892;&#65292;&#21407;&#22240;&#21253;&#25324;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#26377;&#38480;&#65292;&#19982;&#27169;&#25311;&#22120;&#25110;&#23454;&#38469;&#29615;&#22659;&#30340;&#20132;&#20114;&#21453;&#39304;&#20381;&#36182;&#36739;&#24378;&#65292;&#20197;&#21450;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26367;&#20195;&#33539;&#24335;&#65292;&#36890;&#36807;&#35268;&#21010;&#39046;&#22495;&#23450;&#20041;&#35821;&#35328;&#65288;PDDL&#65289;&#26500;&#24314;&#19968;&#20010;&#26174;&#24335;&#30340;&#19990;&#30028;&#65288;&#39046;&#22495;&#65289;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#26469;&#36827;&#34892;&#20855;&#26377;&#33391;&#22909;&#39046;&#22495;&#29420;&#31435;&#24615;&#30340;&#35268;&#21010;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#21487;&#33021;&#26080;&#27861;&#26368;&#21021;&#29983;&#25104;&#23436;&#20840;&#21151;&#33021;&#30340;PDDL&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;LLMs&#20316;&#20026;PDDL&#21644;&#32416;&#27491;&#21453;&#39304;&#28304;&#65288;&#22914;PDDL&#39564;&#35777;&#22120;&#21644;&#20154;&#31867;&#65289;&#20043;&#38388;&#30340;&#25509;&#21475;&#12290;&#23545;&#20110;&#27809;&#26377;PDDL&#32972;&#26223;&#30340;&#29992;&#25143;&#65292;&#25105;&#20204;&#35777;&#26126;LLMs&#21487;&#20197;&#23558;PDDL&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#24182;&#23558;&#32416;&#27491;&#21453;&#39304;&#26377;&#25928;&#22320;&#32534;&#30721;&#22238;&#24213;&#23618;&#39046;&#22495;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in applying pre-trained large language models (LLMs) to planning problems. However, methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback. In this work, we introduce a novel alternative paradigm that constructs an explicit world (domain) model in planning domain definition language (PDDL) and then uses it to plan with sound domain-independent planners. To address the fact that LLMs may not generate a fully functional PDDL model initially, we employ LLMs as an interface between PDDL and sources of corrective feedback, such as PDDL validators and humans. For users who lack a background in PDDL, we show that LLMs can translate PDDL into natural language and effectively encode corrective feedback back to the underlying domain model. Our framewo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UniControl&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#25972;&#21512;&#24191;&#27867;&#30340;&#21487;&#25511;&#26465;&#20214;&#19982;&#22270;&#20687;&#20219;&#21153;&#65292;&#24182;&#20173;&#28982;&#20801;&#35768;&#20219;&#24847;&#35821;&#35328;&#25552;&#31034;&#65292;UniControl&#33021;&#22815;&#36827;&#34892;&#20687;&#32032;&#32423;&#31934;&#30830;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#20854;&#20013;&#35270;&#35273;&#26465;&#20214;&#20027;&#35201;&#24433;&#21709;&#29983;&#25104;&#30340;&#32467;&#26500;&#65292;&#35821;&#35328;&#25552;&#31034;&#21017;&#24341;&#23548;&#26679;&#24335;&#21644;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#23558;&#25193;&#25955;&#36807;&#31243;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniControl&#22312;&#21487;&#25511;&#24615;&#21644;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11147</link><description>&lt;p&gt;
UniControl&#65306;&#32479;&#19968;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21487;&#25511;&#30340;&#29983;&#21160;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild. (arXiv:2305.11147v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UniControl&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#25972;&#21512;&#24191;&#27867;&#30340;&#21487;&#25511;&#26465;&#20214;&#19982;&#22270;&#20687;&#20219;&#21153;&#65292;&#24182;&#20173;&#28982;&#20801;&#35768;&#20219;&#24847;&#35821;&#35328;&#25552;&#31034;&#65292;UniControl&#33021;&#22815;&#36827;&#34892;&#20687;&#32032;&#32423;&#31934;&#30830;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#20854;&#20013;&#35270;&#35273;&#26465;&#20214;&#20027;&#35201;&#24433;&#21709;&#29983;&#25104;&#30340;&#32467;&#26500;&#65292;&#35821;&#35328;&#25552;&#31034;&#21017;&#24341;&#23548;&#26679;&#24335;&#21644;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#23558;&#25193;&#25955;&#36807;&#31243;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniControl&#22312;&#21487;&#25511;&#24615;&#21644;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#26426;&#22120;&#33258;&#27835;&#21644;&#20154;&#31867;&#25511;&#21046;&#24448;&#24448;&#20195;&#34920;&#30528;&#30456;&#20114;&#30683;&#30462;&#30340;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;&#22312;&#20132;&#20114;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35774;&#35745;&#20013;&#12290;&#29616;&#23454;&#20013;&#30340;&#21487;&#25511;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#31283;&#23450;&#25193;&#25955;&#65289;&#22312;&#20351;&#29992;&#20219;&#24847;&#35821;&#35328;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20855;&#26377;&#31354;&#38388;&#12289;&#32467;&#26500;&#25110;&#20960;&#20309;&#25511;&#21046;&#30340;&#22270;&#20687;&#26041;&#38754;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;UniControl&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#23558;&#24191;&#27867;&#30340;&#21487;&#25511;&#26465;&#20214;&#19982;&#22270;&#20687;&#65288;C2I&#65289;&#20219;&#21153;&#25972;&#21512;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#21516;&#26102;&#20173;&#20801;&#35768;&#20219;&#24847;&#35821;&#35328;&#25552;&#31034;&#12290;UniControl&#33021;&#22815;&#36827;&#34892;&#20687;&#32032;&#32423;&#31934;&#30830;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#20854;&#20013;&#35270;&#35273;&#26465;&#20214;&#20027;&#35201;&#24433;&#21709;&#29983;&#25104;&#30340;&#32467;&#26500;&#65292;&#35821;&#35328;&#25552;&#31034;&#21017;&#24341;&#23548;&#26679;&#24335;&#21644;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#20351;UniControl&#20855;&#22791;&#22788;&#29702;&#21508;&#31181;&#35270;&#35273;&#26465;&#20214;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#23558;&#25193;&#25955;&#36807;&#31243;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;UniControl&#22312;&#21487;&#25511;&#24615;&#21644;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving machine autonomy and human control often represent divergent objectives in the design of interactive AI systems. Visual generative foundation models such as Stable Diffusion show promise in navigating these goals, especially when prompted with arbitrary languages. However, they often fall short in generating images with spatial, structural, or geometric controls. The integration of such controls, which can accommodate various visual conditions in a single unified model, remains an unaddressed challenge. In response, we introduce UniControl, a new generative foundation model that consolidates a wide array of controllable condition-to-image (C2I) tasks within a singular framework, while still allowing for arbitrary language prompts. UniControl enables pixel-level-precise image generation, where visual conditions primarily influence the generated structures and language prompts guide the style and context. To equip UniControl with the capacity to handle diverse visual conditions
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;Calibrated Explanations (CE)&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#24555;&#36895;&#12289;&#21487;&#38752;&#19988;&#24378;&#20581;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02305</link><description>&lt;p&gt;
&#26657;&#20934;&#21270;&#35299;&#37322;&#65306;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#21644;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Calibrated Explanations: with Uncertainty Information and Counterfactuals. (arXiv:2305.02305v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;Calibrated Explanations (CE)&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#24555;&#36895;&#12289;&#21487;&#38752;&#19988;&#24378;&#20581;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#39046;&#22495;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#31995;&#32479;&#20013;&#39044;&#27979;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#21487;&#33021;&#23548;&#33268;&#28389;&#29992;&#25110;&#19981;&#20351;&#29992;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26088;&#22312;&#21019;&#24314;&#21487;&#20197;&#21521;&#20154;&#31867;&#29992;&#25143;&#35299;&#37322;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23616;&#37096;&#35299;&#37322;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#20010;&#21035;&#39044;&#27979;&#21407;&#22240;&#30340;&#20449;&#24687;&#65292;&#20294;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#31561;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;&#26657;&#20934;&#21270;&#35299;&#37322;(Calibrated Explanations&#65292;CE)&#65292;&#23427;&#22522;&#20110; Venn-Abers&#65292;&#21516;&#26102;&#22312;&#29983;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#30340;&#21516;&#26102;&#26657;&#20934;&#24213;&#23618;&#27169;&#22411;&#12290;CE&#19981;&#20165;&#25552;&#20379;&#24555;&#36895;&#12289;&#21487;&#38752;&#12289;&#31283;&#23450;&#21644;&#24378;&#20581;&#30340;&#35299;&#37322;&#65292;&#36824;&#25552;&#20379;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#20855;&#26377;&#26131;&#20110;&#29702;&#35299;&#30340;&#26465;&#20214;&#35268;&#21017;&#65292;&#20063;&#21487;&#20197;&#29983;&#25104;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has become an integral part of decision support systems (DSSs) in various domains, but the lack of transparency in the predictive models used in AI-based DSSs can lead to misuse or disuse. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance, but they suffer from drawbacks such as instability. To address these issues, we propose a new feature importance explanation method, Calibrated Explanations (CE), which is based on Venn-Abers and calibrates the underlying model while generating feature importance explanations. CE provides fast, reliable, stable, and robust explanations, along with uncertainty quantification of the probability estimates and feature importance weights. Furthermore, the method is model agnostic with easily understood conditional rules and can also genera
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00586</link><description>&lt;p&gt;
GPT-2&#26159;&#22914;&#20309;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#30340;&#65311;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#21151;&#33021;&#21364;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#30340;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20197;GPT-2 Small&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#33021;&#21542;&#36890;&#36807;&#36755;&#20837;"&#25112;&#20105;&#25345;&#32493;&#26102;&#38388;&#26159;&#20174;1732&#24180;&#21040;17&#24180;"&#65292;&#39044;&#27979;&#20986;&#26377;&#25928;&#30340;&#20004;&#20301;&#25968;&#23383;&#30340;&#25130;&#27490;&#24180;&#20221; (&#22823;&#20110;32&#24180;)&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#30005;&#36335;&#65292;&#21363;GPT-2 Small&#35745;&#31639;&#22270;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#29992;&#20110;&#35745;&#31639;&#36825;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#25105;&#20204;&#35299;&#37322;&#20102;&#27599;&#20010;&#30005;&#36335;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;GPT-2 Small&#30340;&#26368;&#32456;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#22312;&#20854;&#20182;&#22823;&#20110;&#22330;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years &gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17760</link><description>&lt;p&gt;
CAMEL: &#29992;&#20110;&#8220;&#24515;&#26234;&#8221;&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#31038;&#32676;&#30340;&#20132;&#20114;&#24335;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society. (arXiv:2303.17760v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#24050;&#21462;&#24471;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20154;&#31867;&#30340;&#25351;&#23548;&#65292;&#20197;&#24341;&#23548;&#23545;&#35805;&#65292;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26500;&#24314;&#21487;&#25193;&#23637;&#25216;&#26415;&#20197;&#20419;&#36827;&#20132;&#20114;&#24335;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#24182;&#28145;&#20837;&#20102;&#35299;&#23427;&#20204;&#30340;&#8220;&#35748;&#30693;&#8221;&#36807;&#31243;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#23454;&#29616;&#33258;&#20027;&#21512;&#20316;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#21551;&#21160;&#25552;&#31034;&#26469;&#24341;&#23548;&#32842;&#22825;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20154;&#31867;&#24847;&#22270;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35282;&#33394;&#25198;&#28436;&#26469;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#32842;&#22825;&#20195;&#29702;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of conversational and chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and provide insight into their "cognitive" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of chat agents, providing a valuable resource for investigating conversational language models. Our contributions include introducing a novel communicative agent framewor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HopCPT &#30340;&#26032;&#19968;&#33268;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#26102;&#38388;&#32467;&#26500;&#65292;&#32780;&#19988;&#33021;&#22815;&#21033;&#29992;&#20854;&#20248;&#21183;&#65292;&#24050;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12783</link><description>&lt;p&gt;
&#22522;&#20110;&#29616;&#20195; Hopfield &#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction for Time Series with Modern Hopfield Networks. (arXiv:2303.12783v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HopCPT &#30340;&#26032;&#19968;&#33268;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#26102;&#38388;&#32467;&#26500;&#65292;&#32780;&#19988;&#33021;&#22815;&#21033;&#29992;&#20854;&#20248;&#21183;&#65292;&#24050;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38590;&#20197;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#22240;&#20026;&#26102;&#38388;&#24207;&#21015;&#30340;&#33258;&#30456;&#20851;&#32467;&#26500;&#36829;&#21453;&#20102;&#19968;&#33268;&#24615;&#39044;&#27979;&#25152;&#38656;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; HopCPT&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110; Hopfield &#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#22815;&#24212;&#23545;&#26102;&#38388;&#32467;&#26500;&#65292;&#32780;&#19988;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23384;&#22312;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#22312;&#29702;&#35770;&#19978;&#26159;&#26377;&#24456;&#22909;&#30340;&#29702;&#35770;&#22522;&#30784;&#30340;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22312;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To quantify uncertainty, conformal prediction methods are gaining continuously more interest and have already been successfully applied to various domains. However, they are difficult to apply to time series as the autocorrelative structure of time series violates basic assumptions required by conformal prediction. We propose HopCPT, a novel conformal prediction approach for time series that not only copes with temporal structures but leverages them. We show that our approach is theoretically well justified for time series where temporal dependencies are present. In experiments, we demonstrate that our new approach outperforms state-of-the-art conformal prediction methods on multiple real-world time series datasets from four different domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#23454;&#29616;&#20840;&#40635;&#33647;&#29289;&#21058;&#37327;&#25511;&#21046;&#65292;&#28155;&#21152;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#21644;&#31574;&#30053;&#32422;&#26463;&#39033;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.10180</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#23433;&#20840;&#30340;&#19993;&#27850;&#37210;&#20840;&#40635;&#21058;&#37327;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning. (arXiv:2303.10180v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#23454;&#29616;&#20840;&#40635;&#33647;&#29289;&#21058;&#37327;&#25511;&#21046;&#65292;&#28155;&#21152;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#21644;&#31574;&#30053;&#32422;&#26463;&#39033;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#40635;&#37257;&#26377;&#26395;&#23454;&#29616;&#26356;&#31934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#40635;&#37257;&#31649;&#29702;&#65292;&#20351;&#40635;&#37257;&#24072;&#20813;&#20110;&#37325;&#22797;&#24615;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#24739;&#32773;&#25163;&#26415;&#25252;&#29702;&#30340;&#26368;&#20851;&#38190;&#26041;&#38754;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#20110;&#21019;&#24314;&#27169;&#25311;&#29615;&#22659;&#65292;&#20197;&#20415;&#26234;&#33021;&#20307;&#36827;&#34892;&#23398;&#20064;&#12290;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20986;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20294;&#31163;&#20020;&#24202;&#24212;&#29992;&#36824;&#26377;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#35299;&#20915;&#23398;&#20064;&#40635;&#37257;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#24341;&#20837;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#20197;&#32531;&#35299;&#33073;&#32447;&#24773;&#20917;&#19979;Q&#20989;&#25968;&#36807;&#24230;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#22312;&#26234;&#33021;&#20307;&#30340;&#22521;&#35757;&#20013;&#28155;&#21152;&#19968;&#39033;&#31574;&#30053;&#32422;&#26463;&#39033;&#65292;&#20197;&#20445;&#25345;&#26234;&#33021;&#20307;&#21644;&#40635;&#37257;&#24072;&#30340;&#31574;&#30053;&#20998;&#24067;&#19968;&#33268;&#65292;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#22312;&#20840;&#40635;&#24773;&#26223;&#19979;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;PCQL&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated anesthesia promises to enable more precise and personalized anesthetic administration and free anesthesiologists from repetitive tasks, allowing them to focus on the most critical aspects of a patient's surgical care. Current research has typically focused on creating simulated environments from which agents can learn. These approaches have demonstrated good experimental results, but are still far from clinical application. In this paper, Policy Constraint Q-Learning (PCQL), a data-driven reinforcement learning algorithm for solving the problem of learning anesthesia strategies on real clinical datasets, is proposed. Conservative Q-Learning was first introduced to alleviate the problem of Q function overestimation in an offline context. A policy constraint term is added to agent training to keep the policy distribution of the agent and the anesthesiologist consistent to ensure safer decisions made by the agent in anesthesia scenarios. The effectiveness of PCQL was validated b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#39044;&#27979;&#32974;&#20799;&#33041;&#37096;&#36229;&#22768;&#24179;&#38754;&#30340;&#23039;&#21183;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#36229;&#22768;&#24179;&#38754;&#23450;&#20301;&#31995;&#32479;&#65292;&#24182;&#23545;&#20854;&#20934;&#30830;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#37327;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#27880;&#20876;&#36136;&#37327;&#30340;&#25913;&#36827;&#21644;&#25968;&#25454;&#25193;&#20805;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.08317</link><description>&lt;p&gt;
&#36229;&#22768;&#24179;&#38754;&#23039;&#21183;&#22238;&#24402;&#65306;&#35780;&#20272;&#32974;&#20799;&#33041;&#37096;&#30340;&#24191;&#20041;&#23039;&#21183;&#22352;&#26631;
&lt;/p&gt;
&lt;p&gt;
Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain. (arXiv:2301.08317v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#39044;&#27979;&#32974;&#20799;&#33041;&#37096;&#36229;&#22768;&#24179;&#38754;&#30340;&#23039;&#21183;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#36229;&#22768;&#24179;&#38754;&#23450;&#20301;&#31995;&#32479;&#65292;&#24182;&#23545;&#20854;&#20934;&#30830;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#37327;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#27880;&#20876;&#36136;&#37327;&#30340;&#25913;&#36827;&#21644;&#25968;&#25454;&#25193;&#20805;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20135;&#31185;&#36229;&#22768;&#25195;&#25551;&#20013;&#65292;&#20174;&#20108;&#32500;&#36229;&#22768;&#22270;&#20687;&#20013;&#24515;&#29702;&#26500;&#24314;&#32974;&#20799;&#30340;&#19977;&#32500;&#22320;&#22270;&#26159;&#25216;&#33021;&#20064;&#24471;&#20013;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#36229;&#22768;&#24179;&#38754;&#23450;&#20301;&#31995;&#32479;&#65292;&#29992;&#20110;&#19977;&#32500;&#21487;&#35270;&#21270;&#12289;&#35757;&#32451;&#21644;&#24341;&#23548;&#65292;&#32780;&#26080;&#38656;&#38598;&#25104;&#39069;&#22806;&#30340;&#20256;&#24863;&#22120;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#25105;&#20204;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22238;&#24402;&#32593;&#32476;&#39044;&#27979;&#30456;&#23545;&#20110;&#26631;&#20934;&#21442;&#32771;&#24103;&#30340;&#20219;&#24847;&#23450;&#21521;&#36229;&#22768;&#24179;&#38754;&#20999;&#21106;&#32974;&#20799;&#33041;&#37096;&#30340;&#20845;&#32500;&#23039;&#21183;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#26631;&#20934;&#32974;&#20799;&#33041;&#21442;&#32771;&#24103;&#30340;&#20551;&#35774;&#65292;&#24182;&#37327;&#21270;&#20102;&#20854;&#30456;&#23545;&#20110;&#32974;&#20799;&#29983;&#29289;&#27979;&#37327;&#20013;&#32463;&#33041;&#23460;&#26631;&#20934;&#24179;&#38754;&#30340;&#33719;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#27880;&#20876;&#36136;&#37327;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#24433;&#21709;&#20197;&#21450;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#21518;&#32493;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25968;&#25454;&#25193;&#20805;&#21644;&#26356;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In obstetric ultrasound (US) scanning, the learner's ability to mentally build a three-dimensional (3D) map of the fetus from a two-dimensional (2D) US image represents a significant challenge in skill acquisition. We aim to build a US plane localization system for 3D visualization, training, and guidance without integrating additional sensors. This work builds on top of our previous work, which predicts the six-dimensional (6D) pose of arbitrarily oriented US planes slicing the fetal brain with respect to a normalized reference frame using a convolutional neural network (CNN) regression network. Here, we analyze in detail the assumptions of the normalized fetal brain reference frame and quantify its accuracy with respect to the acquisition of transventricular (TV) standard plane (SP) for fetal biometry. We investigate the impact of registration quality in the training and testing data and its subsequent effect on trained models. Finally, we introduce data augmentations and larger trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35782;&#21035;&#32593;&#32476;&#22914;&#20309;&#27169;&#25311;&#30495;&#23454;&#21518;&#39564;&#20998;&#24067;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#36890;&#36807;&#23548;&#20986;&#20840;&#23616;&#26465;&#20214;&#21644;&#23616;&#37096;&#26465;&#20214;&#65292;&#21457;&#29616;&#23436;&#32654;&#24615;&#20026;&#20854;&#20855;&#22791;&#26399;&#26395;&#24615;&#36136;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.10649</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#36870;&#25512;
&lt;/p&gt;
&lt;p&gt;
Inversion of Bayesian Networks. (arXiv:2212.10649v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35782;&#21035;&#32593;&#32476;&#22914;&#20309;&#27169;&#25311;&#30495;&#23454;&#21518;&#39564;&#20998;&#24067;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#36890;&#36807;&#23548;&#20986;&#20840;&#23616;&#26465;&#20214;&#21644;&#23616;&#37096;&#26465;&#20214;&#65292;&#21457;&#29616;&#23436;&#32654;&#24615;&#20026;&#20854;&#20855;&#22791;&#26399;&#26395;&#24615;&#36136;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;Helmholtz&#26426;&#20351;&#29992;&#19968;&#20010;&#35782;&#21035;&#32593;&#32476;&#65288;&#32534;&#30721;&#22120;&#65289;&#26469;&#36817;&#20284;&#29983;&#25104;&#27169;&#22411;&#65288;&#35299;&#30721;&#22120;&#65289;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35782;&#21035;&#32593;&#32476;&#20855;&#22791;&#27169;&#25311;&#30495;&#23454;&#21518;&#39564;&#20998;&#24067;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#36825;&#20123;&#32467;&#26524;&#22522;&#20110;&#27010;&#29575;&#22270;&#27169;&#22411;&#65295;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#19968;&#33324;&#32972;&#26223;&#65292;&#20854;&#20013;&#32593;&#32476;&#20195;&#34920;&#20102;&#19968;&#32452;&#26465;&#20214;&#29420;&#31435;&#24615;&#35821;&#21477;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#20840;&#23616;&#26465;&#20214;&#65288;&#36890;&#36807;d-&#20998;&#31163;&#65289;&#21644;&#23616;&#37096;&#26465;&#20214;&#65292;&#20351;&#24471;&#35782;&#21035;&#32593;&#32476;&#20855;&#22791;&#26399;&#26395;&#30340;&#24615;&#36136;&#12290;&#23616;&#37096;&#26465;&#20214;&#20013;&#65292;&#23436;&#32654;&#24615;&#65288;&#27599;&#20010;&#33410;&#28857;&#21482;&#19982;&#20854;&#29238;&#33410;&#28857;&#30456;&#36830;&#65289;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders and Helmholtz machines use a recognition network (encoder) to approximate the posterior distribution of a generative model (decoder). In this paper we study the necessary and sufficient properties of a recognition network so that it can model the true posterior distribution exactly. These results are derived in the general context of probabilistic graphical modelling / Bayesian networks, for which the network represents a set of conditional independence statements. We derive both global conditions, in terms of d-separation, and local conditions for the recognition network to have the desired qualities. It turns out that for the local conditions the property perfectness (for every node, all parents are joined) plays an important role.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#20195;&#29702;&#24314;&#27169;&#30340;&#25216;&#26415;H-Pro&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#27979;&#35797;&#20195;&#29702;&#39537;&#21160;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#27979;&#35797;&#39564;&#35777;&#26399;&#38388;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#25216;&#26415;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15092</link><description>&lt;p&gt;
&#25552;&#21319;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#20998;&#23618;&#20195;&#29702;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Proxy Modeling for Improved HPO in Time Series Forecasting. (arXiv:2211.15092v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15092
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#20195;&#29702;&#24314;&#27169;&#30340;&#25216;&#26415;H-Pro&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#27979;&#35797;&#20195;&#29702;&#39537;&#21160;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#27979;&#35797;&#39564;&#35777;&#26399;&#38388;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#25216;&#26415;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#36873;&#25321;&#27491;&#30830;&#30340;&#36229;&#21442;&#25968;&#38598;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20256;&#32479;&#30340;&#26102;&#38388;&#20132;&#21449;&#39564;&#35777;&#26694;&#26550;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#32463;&#24120;&#23548;&#33268;&#27979;&#35797;&#24615;&#33021;&#24046;&#65292;&#22240;&#20026;&#39564;&#35777;&#21644;&#27979;&#35797;&#26399;&#38388;&#21487;&#33021;&#23384;&#22312;&#19981;&#21305;&#37197;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#27979;&#35797;-&#39564;&#35777;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21363;H-Pro&#65292;&#36890;&#36807;&#21033;&#29992;&#19982;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#32463;&#24120;&#30456;&#20851;&#30340;&#25968;&#25454;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#27979;&#35797;&#20195;&#29702;&#26469;&#39537;&#21160;HPO&#12290;&#30001;&#20110;&#39640;&#23618;&#27425;&#30340;&#32858;&#21512;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#26174;&#31034;&#20986;&#36739;&#23569;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;&#26356;&#22909;&#30340;&#21487;&#39044;&#27979;&#24615;&#65292;&#30456;&#27604;&#20043;&#19979;&#26368;&#20302;&#23618;&#27425;&#30340;&#26102;&#38388;&#24207;&#21015;&#21487;&#33021;&#26159;&#31232;&#30095;&#21644;&#38388;&#27463;&#24615;&#30340;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#39640;&#23618;&#27425;&#39044;&#27979;&#22120;&#29983;&#25104;&#30340;&#27979;&#35797;&#26399;&#38388;&#30340;&#20195;&#29702;&#39044;&#27979;&#32467;&#26524;&#26469;&#20248;&#21270;&#26368;&#20302;&#23618;&#27425;&#30340;&#22522;&#26412;&#39044;&#27979;&#22120;&#30340;&#36229;&#21442;&#25968;&#12290;H-Pro&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;HPO&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20116;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting the right set of hyperparameters is crucial in time series forecasting. The classical temporal cross-validation framework for hyperparameter optimization (HPO) often leads to poor test performance because of a possible mismatch between validation and test periods. To address this test-validation mismatch, we propose a novel technique, H-Pro to drive HPO via test proxies by exploiting data hierarchies often associated with time series datasets. Since higher-level aggregated time series often show less irregularity and better predictability as compared to the lowest-level time series which can be sparse and intermittent, we optimize the hyperparameters of the lowest-level base-forecaster by leveraging the proxy forecasts for the test period generated from the forecasters at higher levels. H-Pro can be applied on any off-the-shelf machine learning model to perform HPO. We validate the efficacy of our technique with extensive empirical evaluation on five publicly available hierar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#33310;&#36424;&#21160;&#20316;&#21644;&#29983;&#25104;&#26032;&#21160;&#20316;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#20102;&#21069;&#20154;&#30340;&#21162;&#21147;&#26469;&#24320;&#21457;&#20986;&#19968;&#22871;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2210.04366</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#20307;&#21160;&#20316;&#21512;&#25104;&#36827;&#34892;&#35745;&#31639;&#32534;&#33310;
&lt;/p&gt;
&lt;p&gt;
Computational Choreography using Human Motion Synthesis. (arXiv:2210.04366v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#33310;&#36424;&#21160;&#20316;&#21644;&#29983;&#25104;&#26032;&#21160;&#20316;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#20102;&#21069;&#20154;&#30340;&#21162;&#21147;&#26469;&#24320;&#21457;&#20986;&#19968;&#22871;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#24212;&#35813;&#34987;&#35757;&#32451;&#26469;&#20998;&#26512;&#20154;&#20307;&#34920;&#28436;&#33402;&#26415;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21512;&#25104;&#33402;&#26415;&#20154;&#20307;&#21160;&#20316;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#20154;&#20307;&#36816;&#21160;&#21512;&#25104;&#20013;&#30340;&#38382;&#39064;&#20219;&#21153;&#21253;&#25324;&#39044;&#27979;&#37326;&#22806;&#29615;&#22659;&#20013;&#20154;&#20307;&#36816;&#21160;&#65292;&#20197;&#21450;&#29983;&#25104;&#22522;&#20110;&#36825;&#20123;&#39044;&#27979;&#30340;&#26032;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#23558;&#35752;&#35770;&#19968;&#20010;&#38750;&#20256;&#32479;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#21363;&#23558;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#39044;&#27979;&#33310;&#36424;&#21160;&#20316;&#12290;&#26368;&#36817;&#26377;&#19968;&#20123;&#26174;&#33879;&#30340;&#21162;&#21147;&#65292;&#20197;&#35745;&#31639;&#30340;&#26041;&#24335;&#20998;&#26512;&#33310;&#36424;&#21160;&#20316;&#65292;&#20363;&#22914;Everybody Dance Now&#65288;EDN&#65289;&#23398;&#20064;&#27169;&#22411;&#21644;Cal Poly&#30805;&#22763;&#35770;&#25991;Take The Lead&#65288;TTL&#65289;&#12290;&#25105;&#20204;&#26377;&#25928;&#22320;&#23558;&#36825;&#20004;&#20010;&#20316;&#21697;&#19982;&#25105;&#20204;&#33258;&#24049;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#65292;&#29983;&#25104;&#20102;&#19968;&#31181;&#26032;&#30340;&#33310;&#36424;&#21160;&#20316;&#39044;&#27979;&#31995;&#32479;&#12289;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#21644;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Should deep learning models be trained to analyze human performance art? To help answer this question, we explore an application of deep neural networks to synthesize artistic human motion. Problem tasks in human motion synthesis can include predicting the motions of humans in-the-wild, as well as generating new sequences of motions based on said predictions. We will discuss the potential of a less traditional application, where learning models are applied to predicting dance movements. There have been notable, recent efforts to analyze dance movements in a computational light, such as the Everybody Dance Now (EDN) learning model and a Cal Poly master's thesis, Take The Lead (TTL). We have effectively combined these two works along with our own deep neural network to produce a new system for dance motion prediction, image-to-image translation, and video generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Failed goal Aware HER (FAHER)&#30340;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#29616;&#30446;&#26631;&#19982;&#26410;&#23454;&#29616;&#30446;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#27169;&#22411;&#23545;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#21644;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2208.14741</link><description>&lt;p&gt;
&#22833;&#36133;&#30446;&#26631;&#24863;&#30693;&#30340;&#20107;&#21518;&#32463;&#39564;&#37325;&#25773;
&lt;/p&gt;
&lt;p&gt;
Failed Goal Aware Hindsight Experience Replay. (arXiv:2208.14741v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Failed goal Aware HER (FAHER)&#30340;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#29616;&#30446;&#26631;&#19982;&#26410;&#23454;&#29616;&#30446;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#27169;&#22411;&#23545;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#21644;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#33719;&#24471;&#30340;&#32463;&#39564;&#26469;&#23398;&#20064;&#23454;&#29616;&#22810;&#20010;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#20351;&#29992;&#31232;&#30095;&#20108;&#20803;&#22870;&#21169;&#35757;&#32451;&#20195;&#29702;&#65292;&#30001;&#20110;&#32570;&#20047;&#25104;&#21151;&#30340;&#32463;&#39564;&#65292;&#36825;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#20107;&#21518;&#32463;&#39564;&#37325;&#25773;&#65288;HER&#65289;&#20174;&#19981;&#25104;&#21151;&#30340;&#32463;&#39564;&#20013;&#29983;&#25104;&#25104;&#21151;&#30340;&#32463;&#39564;&#12290;&#28982;&#32780;&#65292;&#20174;&#22343;&#21248;&#25277;&#26679;&#30340;&#32463;&#39564;&#20013;&#29983;&#25104;&#25104;&#21151;&#30340;&#32463;&#39564;&#30340;&#36807;&#31243;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Failed goal Aware HER (FAHER)&#30340;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#37319;&#26679;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#23454;&#29616;&#30446;&#26631;&#19982;&#26410;&#23454;&#29616;&#30446;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#27169;&#22411;&#23545;&#20855;&#26377;&#19981;&#21516;&#23454;&#29616;&#30446;&#26631;&#30340;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#22312;HER&#30340;&#26041;&#24335;&#19979;&#23545;&#32463;&#39564;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-goal reinforcement learning for a given environment, agents learn policies to achieve multiple goals by using experiences gained from interactions with the environment. One of the key challenges in this setting is training agents using sparse binary rewards, which can be difficult due to a lack of successful experiences. To address this challenge, hindsight experience replay (HER) generates successful experiences from unsuccessful experiences. However, the process of generating successful experiences from uniformly sampled ones can be inefficient. In this paper, a novel approach called Failed goal Aware HER (FAHER) is proposed to enhance the sampling efficiency. The approach exploits the property of achieved goals in relation to failed goals that are defined as the original goals not achieved. The proposed method involves clustering episodes with different achieved goals using a cluster model and subsequently sampling experiences in the manner of HER. The cluster model is gene
&lt;/p&gt;</description></item><item><title>"SensorSCAN"&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#21644;&#28145;&#24230;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21270;&#24037;&#36807;&#31243;&#20013;&#36827;&#34892;&#25925;&#38556;&#35786;&#26029;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#19987;&#23478;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26377;&#25928;&#26816;&#27979;&#22823;&#22810;&#25968;&#36807;&#31243;&#25925;&#38556;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19978;&#24494;&#35843;&#65292;&#20960;&#20046;&#36798;&#21040;&#20102;&#26368;&#20248;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2208.08879</link><description>&lt;p&gt;
SensorSCAN: &#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#21644;&#28145;&#24230;&#32858;&#31867;&#22312;&#21270;&#24037;&#36807;&#31243;&#20013;&#30340;&#25925;&#38556;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SensorSCAN: Self-Supervised Learning and Deep Clustering for Fault Diagnosis in Chemical Processes. (arXiv:2208.08879v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08879
&lt;/p&gt;
&lt;p&gt;
"SensorSCAN"&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#21644;&#28145;&#24230;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21270;&#24037;&#36807;&#31243;&#20013;&#36827;&#34892;&#25925;&#38556;&#35786;&#26029;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#19987;&#23478;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26377;&#25928;&#26816;&#27979;&#22823;&#22810;&#25968;&#36807;&#31243;&#25925;&#38556;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19978;&#24494;&#35843;&#65292;&#20960;&#20046;&#36798;&#21040;&#20102;&#26368;&#20248;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24037;&#19994;&#35774;&#26045;&#22312;&#29983;&#20135;&#36807;&#31243;&#20013;&#20135;&#29983;&#22823;&#37327;&#30340;&#21407;&#22987;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#29992;&#20110;&#30417;&#25511;&#21644;&#25511;&#21046;&#36807;&#31243;&#65292;&#24182;&#21487;&#20998;&#26512;&#20197;&#26816;&#27979;&#21644;&#39044;&#27979;&#36807;&#31243;&#24322;&#24120;&#12290;&#36890;&#24120;&#65292;&#25968;&#25454;&#24517;&#39035;&#30001;&#19987;&#23478;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#20415;&#22312;&#39044;&#27979;&#24314;&#27169;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#25163;&#21160;&#27880;&#37322;&#22823;&#37327;&#25968;&#25454;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SensorSCAN&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#19987;&#20026;&#24037;&#19994;&#21270;&#23398;&#36807;&#31243;&#30417;&#27979;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;Tennessee Eastman&#24037;&#33402;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#21508;&#31181;&#25925;&#38556;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27809;&#26377;&#19987;&#23478;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65288;&#22312;&#22266;&#23450;FPR&#19979;&#65292;&#22686;&#21152;&#20102;0.2-0.3&#30340;TPR&#65289;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#20986;&#22823;&#22810;&#25968;&#36807;&#31243;&#25925;&#38556;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#20960;&#20046;&#36798;&#21040;&#20102;SOT&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern industrial facilities generate large volumes of raw sensor data during the production process. This data is used to monitor and control the processes and can be analyzed to detect and predict process abnormalities. Typically, the data has to be annotated by experts in order to be used in predictive modeling. However, manual annotation of large amounts of data can be difficult in industrial settings.  In this paper, we propose SensorSCAN, a novel method for unsupervised fault detection and diagnosis, designed for industrial chemical process monitoring. We demonstrate our model's performance on two publicly available datasets of the Tennessee Eastman Process with various faults. The results show that our method significantly outperforms existing approaches (+0.2-0.3 TPR for a fixed FPR) and effectively detects most of the process faults without expert annotation. Moreover, we show that the model fine-tuned on a small fraction of labeled data nearly reaches the performance of a SOT
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#32463;&#20856;&#31639;&#27861;Bellman-Ford&#20855;&#26377;&#30456;&#21516;&#30340;&#35299;&#65292;&#24182;&#19988;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.11104</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Combining optimal path search with task-dependent learning in a neural network. (arXiv:2201.11104v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11104
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#32463;&#20856;&#31639;&#27861;Bellman-Ford&#20855;&#26377;&#30456;&#21516;&#30340;&#35299;&#65292;&#24182;&#19988;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#25509;&#22270;&#20013;&#25214;&#21040;&#26368;&#20248;&#36335;&#24452;&#38656;&#35201;&#30830;&#23450;&#27839;&#30528;&#22270;&#30340;&#36793;&#32536;&#34892;&#36827;&#30340;&#26368;&#23567;&#24635;&#25104;&#26412;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#20960;&#31181;&#32463;&#20856;&#31639;&#27861;&#26469;&#35299;&#20915;&#65292;&#36890;&#24120;&#25152;&#26377;&#36793;&#32536;&#30340;&#25104;&#26412;&#37117;&#26159;&#39044;&#20808;&#23450;&#20041;&#22909;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#24819;&#35201;&#26681;&#25454;&#26576;&#20010;&#20219;&#21153;&#30340;&#35201;&#27714;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#25913;&#21464;&#25104;&#26412;&#26102;&#65292;&#36890;&#24120;&#26080;&#27861;&#20351;&#29992;&#20256;&#32479;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31361;&#35302;&#26435;&#37325;&#26469;&#23450;&#20041;&#36335;&#24452;&#25628;&#32034;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#65292;&#36825;&#20801;&#35768;&#20351;&#29992;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#36827;&#34892;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#24403;&#20174;&#19968;&#20010;&#21021;&#22987;&#27963;&#36291;&#24230;&#20540;&#20026;1&#24320;&#22987;&#26102;&#65292;&#22312;&#36825;&#20010;&#32593;&#32476;&#20013;&#30340;&#27963;&#21160;&#20256;&#25773;&#23558;&#23548;&#33268;&#19982;Bellman-Ford&#31639;&#27861;&#25214;&#21040;&#30340;&#35299;&#30456;&#21516;&#30340;&#35299;&#12290;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#19982;Bellman-Ford&#30456;&#21516;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#65288;&#22914;&#36203;&#24067;&#23398;&#20064;&#65289;&#21487;&#20197;&#35843;&#25972;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#26469;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding optimal paths in connected graphs requires determining the smallest total cost for traveling along the graph's edges. This problem can be solved by several classical algorithms where, usually, costs are predefined for all edges. Conventional planning methods can, thus, normally not be used when wanting to change costs in an adaptive way following the requirements of some task. Here we show that one can define a neural network representation of path finding problems by transforming cost values into synaptic weights, which allows for online weight adaptation using network learning mechanisms. When starting with an initial activity value of one, activity propagation in this network will lead to solutions, which are identical to those found by the Bellman-Ford algorithm. The neural network has the same algorithmic complexity as Bellman-Ford and, in addition, we can show that network learning mechanisms (such as Hebbian learning) can adapt the weights in the network augmenting the r
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#25688;&#35201;&#22240;&#26524;&#22270;&#65292;&#24182;&#36890;&#36807;PC-like&#21644;FCI-like&#31639;&#27861;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2105.10381</link><description>&lt;p&gt;
&#22522;&#20110;&#29109;&#30340;&#26102;&#38388;&#24207;&#21015;&#25688;&#35201;&#22240;&#26524;&#22270;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Entropy-based Discovery of Summary Causal Graphs in Time Series. (arXiv:2105.10381v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.10381
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#25688;&#35201;&#22240;&#26524;&#22270;&#65292;&#24182;&#36890;&#36807;PC-like&#21644;FCI-like&#31639;&#27861;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#37319;&#26679;&#39057;&#29575;&#30340;&#26102;&#38388;&#24207;&#21015;&#19978;&#23398;&#20064;&#25688;&#35201;&#22240;&#26524;&#22270;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#22240;&#26524;&#26102;&#24577;&#20114;&#20449;&#24687;&#24230;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#24230;&#37327;&#19982;&#29109;&#20943;&#21407;&#29702;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#27010;&#29575;&#25552;&#21319;&#21407;&#29702;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20004;&#20010;&#35201;&#32032;&#32467;&#21512;&#22312;&#31867;&#20284;&#20110;PC&#21644;FCI&#30340;&#31639;&#27861;&#20013;&#65292;&#26500;&#24314;&#20102;&#25688;&#35201;&#22240;&#26524;&#22270;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study addresses the problem of learning a summary causal graph on time series with potentially different sampling rates. To do so, we first propose a new causal temporal mutual information measure for time series. We then show how this measure relates to an entropy reduction principle that can be seen as a special case of the probability raising principle. We finally combine these two ingredients in PC-like and FCI-like algorithms to construct the summary causal graph. There algorithm are evaluated on several datasets, which shows both their efficacy and efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#23398;&#20064;&#27169;&#22411;&#21363;&#21487;&#38598;&#25104;&#22870;&#36175;&#26799;&#24230;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2103.05147</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;&#30340;&#22522;&#20110;&#22870;&#36175;&#26799;&#24230;&#30340;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-free Policy Learning with Reward Gradients. (arXiv:2103.05147v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.05147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#23398;&#20064;&#27169;&#22411;&#21363;&#21487;&#38598;&#25104;&#22870;&#36175;&#26799;&#24230;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#22312;&#26679;&#26412;&#31232;&#32570;&#30340;&#24212;&#29992;&#20013;&#65292;&#22914;&#26426;&#22120;&#20154;&#23398;&#65292;&#23427;&#20204;&#20173;&#26410;&#34987;&#24191;&#27867;&#21033;&#29992;&#12290;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#21487;&#29992;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#20316;&#20026;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22870;&#36175;&#20989;&#25968;&#36890;&#24120;&#34987;&#31934;&#24515;&#35774;&#35745;&#20197;&#24341;&#23548;&#26234;&#33021;&#20307;&#12290;&#22240;&#27492;&#65292;&#22870;&#36175;&#20989;&#25968;&#36890;&#24120;&#26159;&#24050;&#30693;&#30340;&#65292;&#21487;&#20197;&#35775;&#38382;&#26631;&#37327;&#22870;&#36175;&#20449;&#21495;&#21644;&#22870;&#36175;&#26799;&#24230;&#12290;&#20026;&#20102;&#20174;&#22870;&#36175;&#26799;&#24230;&#20013;&#33719;&#30410;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#38656;&#35201;&#20102;&#35299;&#29615;&#22659;&#21160;&#24577;&#65292;&#36825;&#26159;&#24456;&#38590;&#33719;&#24471;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#22870;&#36175;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#23427;&#21487;&#20197;&#38598;&#25104;&#22870;&#36175;&#26799;&#24230;&#32780;&#26080;&#38656;&#23398;&#20064;&#27169;&#22411;&#12290;&#32469;&#36807;&#27169;&#22411;&#21160;&#24577;&#20351;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#33021;&#22815;&#22312;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#36825;&#23548;&#33268;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22914;&#32463;&#39564;&#20998;&#26512;&#25152;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#25552;&#21319;&#20102;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the increasing popularity of policy gradient methods, they are yet to be widely utilized in sample-scarce applications, such as robotics. The sample efficiency could be improved by making best usage of available information. As a key component in reinforcement learning, the reward function is usually devised carefully to guide the agent. Hence, the reward function is usually known, allowing access to not only scalar reward signals but also reward gradients. To benefit from reward gradients, previous works require the knowledge of environment dynamics, which are hard to obtain. In this work, we develop the \textit{Reward Policy Gradient} estimator, a novel approach that integrates reward gradients without learning a model. Bypassing the model dynamics allows our estimator to achieve a better bias-variance trade-off, which results in a higher sample efficiency, as shown in the empirical analysis. Our method also boosts the performance of Proximal Policy Optimization on different 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20301;&#32622;&#28216;&#25103;&#32534;&#30721;&#20026;QBF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#29305;&#24615;&#21644;&#23545;&#38750;&#27861;&#31227;&#21160;&#30340;&#22788;&#29702;&#65292;&#29983;&#25104;&#26356;&#32039;&#20945;&#30340;&#23454;&#20363;&#65292;&#25552;&#39640;&#20102;&#27714;&#35299;&#36895;&#24230;&#12290;&#36825;&#20123;&#32534;&#30721;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#20301;&#32622;&#28216;&#25103;&#65292;&#24182;&#21487;&#29992;&#20110;&#32763;&#35793;&#23454;&#38469;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2005.05098</link><description>&lt;p&gt;
&#20301;&#32622;&#28216;&#25103;&#19982;QBF&#65306;&#19968;&#20010;&#23436;&#21892;&#30340;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Positional Games and QBF: A Polished Encoding. (arXiv:2005.05098v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.05098
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20301;&#32622;&#28216;&#25103;&#32534;&#30721;&#20026;QBF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#29305;&#24615;&#21644;&#23545;&#38750;&#27861;&#31227;&#21160;&#30340;&#22788;&#29702;&#65292;&#29983;&#25104;&#26356;&#32039;&#20945;&#30340;&#23454;&#20363;&#65292;&#25552;&#39640;&#20102;&#27714;&#35299;&#36895;&#24230;&#12290;&#36825;&#20123;&#32534;&#30721;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#20301;&#32622;&#28216;&#25103;&#65292;&#24182;&#21487;&#29992;&#20110;&#32763;&#35793;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20301;&#32622;&#28216;&#25103;&#26159;&#19968;&#31867;&#25968;&#23398;&#19978;&#30340;&#21452;&#20154;&#28216;&#25103;&#65292;&#21253;&#25324;&#20117;&#23383;&#26827;&#21450;&#20854;&#25512;&#24191;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23558;&#36825;&#20123;&#28216;&#25103;&#32534;&#30721;&#25104;&#37327;&#21270;&#24067;&#23572;&#20844;&#24335;&#65288;QBF&#65289;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#28216;&#25103;&#23454;&#20363;&#23384;&#22312;&#31532;&#19968;&#29609;&#23478;&#30340;&#32988;&#21033;&#31574;&#30053;&#24403;&#19988;&#20165;&#24403;&#30456;&#24212;&#30340;&#20844;&#24335;&#20026;&#30495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#26041;&#38754;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;QBF&#28216;&#25103;&#32534;&#30721;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#19988;&#20801;&#35768;&#25105;&#20204;&#32534;&#30721;&#20854;&#20182;&#20301;&#32622;&#28216;&#25103;&#65292;&#22914;&#20845;&#35282;&#26827;&#12290;&#20854;&#27425;&#65292;&#20301;&#32622;&#28216;&#25103;&#30340;&#32467;&#26500;&#29305;&#24615;&#20197;&#21450;&#23545;&#38750;&#27861;&#31227;&#21160;&#30340;&#20180;&#32454;&#22788;&#29702;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#26356;&#32039;&#20945;&#30340;&#23454;&#20363;&#65292;&#36825;&#20123;&#23454;&#20363;&#21487;&#20197;&#34987;&#26368;&#20808;&#36827;&#30340;QBF&#27714;&#35299;&#22120;&#26356;&#24555;&#22320;&#27714;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30830;&#35777;&#20102;&#21518;&#19968;&#20107;&#23454;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#26032;&#32534;&#30721;&#30340;&#32039;&#20945;&#24615;&#20351;&#24471;&#23558;&#29616;&#23454;&#20013;&#30340;&#28216;&#25103;&#38382;&#39064;&#36716;&#21270;&#25104;QBF&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#20855;&#26377;&#21382;&#21490;&#24847;&#20041;&#30340;&#23454;&#38469;&#38382;&#39064;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;&#38590;&#24230;&#36882;&#22686;&#30340;&#37324;&#31243;&#30865;&#21521;QBF&#31038;&#21306;&#25552;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positional games are a mathematical class of two-player games comprising Tic-tac-toe and its generalizations. We propose a novel encoding of these games into Quantified Boolean Formulas (QBFs) such that a game instance admits a winning strategy for the first player if and only if the corresponding formula is true. Our approach improves over previous QBF encodings of games in multiple ways. First, it is generic and lets us encode other positional games, such as Hex. Second, the structural properties of positional games, together with careful treatment of illegal moves, let us generate more compact instances that can be solved faster by state-of-the-art QBF solvers. We establish the latter fact through extensive experiments. Finally, the compactness of our new encoding makes it feasible to translate realistic game problems. We identify a few such problems of historical significance and put them forward to the QBF community as milestones of increasing difficulty.
&lt;/p&gt;</description></item></channel></rss>