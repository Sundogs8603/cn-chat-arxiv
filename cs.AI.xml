<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65288;SVT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20809;&#35889;&#28151;&#21512;&#26469;&#35299;&#20915;&#35270;&#35273;&#21464;&#25442;&#20013;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24615;&#21644;&#20449;&#24687;&#25429;&#25417;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01310</link><description>&lt;p&gt;
&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65306;&#20809;&#35889;&#28151;&#21512;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scattering Vision Transformer: Spectral Mixing Matters. (arXiv:2311.01310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65288;SVT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20809;&#35889;&#28151;&#21512;&#26469;&#35299;&#20915;&#35270;&#35273;&#21464;&#25442;&#20013;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24615;&#21644;&#20449;&#24687;&#25429;&#25417;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#23454;&#20363;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#27880;&#24847;&#21147;&#22797;&#26434;&#24615;&#21644;&#26377;&#25928;&#25429;&#25417;&#22270;&#20687;&#20013;&#32454;&#31890;&#24230;&#20449;&#24687;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#37319;&#29992;&#38477;&#37319;&#26679;&#25805;&#20316;&#65288;&#22914;&#27744;&#21270;&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25805;&#20316;&#26159;&#19981;&#21487;&#36870;&#30340;&#65292;&#21487;&#33021;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65288;SVT&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;SVT&#32467;&#21512;&#20102;&#19968;&#20010;&#20809;&#35889;&#25955;&#23556;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#22270;&#20687;&#32454;&#33410;&#30340;&#25429;&#25417;&#12290;SVT&#36890;&#36807;&#20998;&#31163;&#20302;&#39057;&#21644;&#39640;&#39057;&#20998;&#37327;&#65292;&#20811;&#26381;&#20102;&#19982;&#38477;&#37319;&#26679;&#25805;&#20316;&#30456;&#20851;&#30340;&#19981;&#21487;&#36870;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;SVT&#24341;&#20837;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#20809;&#35889;&#38376;&#25511;&#32593;&#32476;&#65292;&#21033;&#29992;Einstein&#20056;&#27861;&#26469;&#22788;&#29702;&#20196;&#29260;&#21644;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have gained significant attention and achieved state-of-the-art performance in various computer vision tasks, including image classification, instance segmentation, and object detection. However, challenges remain in addressing attention complexity and effectively capturing fine-grained information within images. Existing solutions often resort to down-sampling operations, such as pooling, to reduce computational cost. Unfortunately, such operations are non-invertible and can result in information loss. In this paper, we present a novel approach called Scattering Vision Transformer (SVT) to tackle these challenges. SVT incorporates a spectrally scattering network that enables the capture of intricate image details. SVT overcomes the invertibility issue associated with down-sampling operations by separating low-frequency and high-frequency components. Furthermore, SVT introduces a unique spectral gating network utilizing Einstein multiplication for token and channel 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#22312;&#23548;&#33322;&#39046;&#22495;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2311.00530</link><description>&lt;p&gt;
LLMs&#23545;&#20855;&#36523;&#23548;&#33322;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
The Development of LLMs for Embodied Navigation. (arXiv:2311.00530v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#22312;&#23548;&#33322;&#39046;&#22495;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35832;&#22914;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#20043;&#31867;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#30340;&#24212;&#29992;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#20247;&#22810;&#24212;&#29992;&#20013;&#65292;&#23548;&#33322;&#20219;&#21153;&#23588;&#20026;&#24341;&#20154;&#27880;&#30446;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;&#23545;&#29615;&#22659;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#21644;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;LLMs&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20854;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#22270;&#20687;&#22788;&#29702;&#33021;&#21147;&#65292;&#22686;&#24378;&#20855;&#36523;&#26234;&#33021;&#31995;&#32479;&#22312;&#29615;&#22659;&#24863;&#30693;&#21644;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#20102;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#20043;&#38388;&#22312;&#23548;&#33322;&#26041;&#38754;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#23457;&#35270;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12289;&#30740;&#31350;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#20855;&#36523;&#23548;&#33322;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#38416;&#26126;&#20102;LLMs&#22312;&#20855;&#36523;&#23548;&#33322;&#39046;&#22495;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications. The application of LLMs with Embodied Intelligence has emerged as a significant area of focus. Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making. LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities. This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation. It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets. Finally, the article elucidat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#32858;&#31867;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;EBTTA&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20110;EBTTA&#26041;&#27861;&#26469;&#35828;&#65292;&#29109;&#25439;&#22833;&#20250;&#36827;&#19968;&#27493;&#22686;&#21152;&#26368;&#22823;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#20026;&#20854;&#22312;&#32858;&#31867;&#20219;&#21153;&#19978;&#30340;&#36739;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.20327</link><description>&lt;p&gt;
&#20174;&#32858;&#31867;&#35270;&#35282;&#25913;&#36827;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Improving Entropy-Based Test-Time Adaptation from a Clustering View. (arXiv:2310.20327v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32858;&#31867;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;EBTTA&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20110;EBTTA&#26041;&#27861;&#26469;&#35828;&#65292;&#29109;&#25439;&#22833;&#20250;&#36827;&#19968;&#27493;&#22686;&#21152;&#26368;&#22823;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#20026;&#20854;&#22312;&#32858;&#31867;&#20219;&#21153;&#19978;&#30340;&#36739;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#39046;&#22495;&#20559;&#31227;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#36981;&#24490;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23436;&#20840;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#21033;&#29992;&#27979;&#35797;&#26102;&#38388;&#36935;&#21040;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#36866;&#24212;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;EBTTA&#65289;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#26679;&#26412;&#19978;&#26368;&#23567;&#21270;&#39044;&#27979;&#30340;&#29109;&#65292;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#32858;&#31867;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;EBTTA&#30340;&#26032;&#35270;&#35282;&#21644;&#35299;&#37322;&#12290;&#36825;&#26159;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#65306;1&#65289;&#22312;&#20998;&#37197;&#27493;&#39588;&#20013;&#65292;EBTTA&#27169;&#22411;&#30340;&#21069;&#21521;&#36807;&#31243;&#26159;&#20026;&#36825;&#20123;&#27979;&#35797;&#26679;&#26412;&#20998;&#37197;&#26631;&#31614;&#65307;2&#65289;&#22312;&#26356;&#26032;&#27493;&#39588;&#20013;&#65292;&#21453;&#21521;&#36807;&#31243;&#26159;&#36890;&#36807;&#24050;&#20998;&#37197;&#30340;&#26679;&#26412;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#26681;&#25454;&#36825;&#31181;&#35299;&#37322;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;EBTTA&#65292;&#20854;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#29109;&#25439;&#22833;&#20250;&#36827;&#19968;&#27493;&#22686;&#21152;&#26368;&#22823;&#30340;&#27010;&#29575;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#35299;&#37322;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;EBTTA&#26041;&#27861;&#22312;&#32858;&#31867;&#20219;&#21153;&#19978;&#27604;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain shift is a common problem in the realistic world, where training data and test data follow different data distributions. To deal with this problem, fully test-time adaptation (TTA) leverages the unlabeled data encountered during test time to adapt the model. In particular, Entropy-Based TTA (EBTTA) methods, which minimize the prediction's entropy on test samples, have shown great success. In this paper, we introduce a new perspective on the EBTTA, which interprets these methods from a view of clustering. It is an iterative algorithm: 1) in the assignment step, the forward process of the EBTTA models is the assignment of labels for these test samples, and 2) in the updating step, the backward process is the update of the model via the assigned samples. Based on the interpretation, we can gain a deeper understanding of EBTTA, where we show that the entropy loss would further increase the largest probability. Accordingly, we offer an alternative explanation that why existing EBTTA 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SURF&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;SURF&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20855;&#20307;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20049</link><description>&lt;p&gt;
SURF: GNN&#39044;&#27979;&#27969;&#20307;&#21160;&#21147;&#23398;&#30340;&#27867;&#21270;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics. (arXiv:2310.20049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20049
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SURF&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;SURF&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20855;&#20307;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#27969;&#20307;&#21160;&#21147;&#23398;&#23545;&#20110;&#35774;&#35745;&#21644;&#24320;&#21457;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#28085;&#30422;&#20102;&#20174;&#31616;&#21333;&#38400;&#38376;&#21040;&#22797;&#26434;&#28065;&#36718;&#26426;&#26800;&#30340;&#33539;&#22260;&#12290;&#20934;&#30830;&#27714;&#35299;&#28508;&#22312;&#30340;&#29289;&#29702;&#26041;&#31243;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#29305;&#28857;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#27714;&#35299;&#22120;&#22312;&#32593;&#26684;&#19978;&#24314;&#27169;&#30456;&#20114;&#20316;&#29992;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#21152;&#36895;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#30495;&#27491;&#29702;&#35299;&#28508;&#22312;&#30340;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#27867;&#21270;&#32780;&#38750;&#25554;&#20540;&#12290;&#27867;&#21270;&#26159;&#36890;&#29992;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#23427;&#24212;&#35813;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#25299;&#25169;&#32467;&#26500;&#12289;&#20998;&#36776;&#29575;&#25110;&#28909;&#21147;&#23398;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SURF&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#27979;&#35797;&#23398;&#20064;&#30340;&#22522;&#20110;&#22270;&#30340;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;SURF&#21253;&#25324;&#21508;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#20855;&#20307;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#23454;&#35777;&#22320;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating fluid dynamics is crucial for the design and development process, ranging from simple valves to complex turbomachinery. Accurately solving the underlying physical equations is computationally expensive. Therefore, learning-based solvers that model interactions on meshes have gained interest due to their promising speed-ups. However, it is unknown to what extent these models truly understand the underlying physical principles and can generalize rather than interpolate. Generalization is a key requirement for a general-purpose fluid simulator, which should adapt to different topologies, resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to test the \textit{generalization} of learned graph-based fluid simulators. SURF comprises individual datasets and provides specific performance and generalization metrics for evaluating and comparing different models. We empirically demonstrate the applicability of SURF by thoroughly investigating the two state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#38477;&#20302;&#32500;&#24230;&#21644;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20960;&#20309;&#24179;&#22343;&#21644;&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.19845</link><description>&lt;p&gt;
&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#65306;&#20197;XGBoost&#22312;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#20013;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction. (arXiv:2310.19845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#38477;&#20302;&#32500;&#24230;&#21644;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20960;&#20309;&#24179;&#22343;&#21644;&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#22403;&#22334;&#37038;&#20214;&#38382;&#39064;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#21644;&#21830;&#19994;&#30028;&#30340;&#20851;&#27880;&#12290;Twitter&#24050;&#25104;&#20026;&#20256;&#25773;&#22403;&#22334;&#37038;&#20214;&#20869;&#23481;&#30340;&#39318;&#36873;&#23186;&#20171;&#12290;&#35768;&#22810;&#30740;&#31350;&#21162;&#21147;&#35797;&#22270;&#24212;&#23545;&#31038;&#20132;&#32593;&#32476;&#22403;&#22334;&#37038;&#20214;&#12290;Twitter&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#29305;&#24449;&#31354;&#38388;&#30340;&#22823;&#23567;&#21644;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#36890;&#24120;&#65292;&#30456;&#20851;&#30740;&#31350;&#24037;&#20316;&#20851;&#27880;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#20027;&#35201;&#25361;&#25112;&#65292;&#25110;&#32773;&#20135;&#29983;&#40657;&#30418;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#38477;&#20302;&#32500;&#24230;&#21644;&#20248;&#21270;&#36229;&#21442;&#25968;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#12290;&#35813;&#31639;&#27861;&#21021;&#22987;&#21270;&#20102;&#19968;&#20010;eXtreme Gradient Boosting&#20998;&#31867;&#22120;&#65292;&#24182;&#20943;&#23569;&#20102;&#25512;&#25991;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#29983;&#25104;&#19968;&#20010;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;50&#27425;&#37325;&#22797;&#30340;10&#20493;&#20998;&#23618;&#20132;&#21449;&#39564;&#35777;&#36827;&#34892;&#39564;&#35777;&#65292;&#24182;&#20351;&#29992;&#38750;&#21442;&#25968;&#32479;&#35745;&#26816;&#39564;&#36827;&#34892;&#20998;&#26512;&#12290;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#22312;&#20960;&#20309;&#24179;&#22343;&#21644;&#20934;&#30830;&#29575;&#19978;&#24179;&#22343;&#36798;&#21040;82.32&#65285;&#21644;92.67&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, spam on online social networks has attracted attention in the research and business world. Twitter has become the preferred medium to spread spam content. Many research efforts attempted to encounter social networks spam. Twitter brought extra challenges represented by the feature space size, and imbalanced data distributions. Usually, the related research works focus on part of these main challenges or produce black-box models. In this paper, we propose a modified genetic algorithm for simultaneous dimensionality reduction and hyper parameter optimization over imbalanced datasets. The algorithm initialized an eXtreme Gradient Boosting classifier and reduced the features space of tweets dataset; to generate a spam prediction model. The model is validated using a 50 times repeated 10-fold stratified cross-validation, and analyzed using nonparametric statistical tests. The resulted prediction model attains on average 82.32\% and 92.67\% in terms of geometric mean and accuracy r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CP-MAE&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38754;&#37096;&#36974;&#32617;&#26469;&#23454;&#29616;MRI&#25195;&#25551;&#20013;&#30340;&#20154;&#33080;&#21435;&#35782;&#21035;&#65292;&#25552;&#39640;&#20102;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.15778</link><description>&lt;p&gt;
&#22686;&#24378;MRI&#25195;&#25551;&#38544;&#31169;&#30340;3D&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
3D Masked Autoencoders for Enhanced Privacy in MRI Scans. (arXiv:2310.15778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CP-MAE&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38754;&#37096;&#36974;&#32617;&#26469;&#23454;&#29616;MRI&#25195;&#25551;&#20013;&#30340;&#20154;&#33080;&#21435;&#35782;&#21035;&#65292;&#25552;&#39640;&#20102;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MRI&#25195;&#25551;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#21307;&#23398;&#20449;&#24687;&#65292;&#20294;&#20063;&#21253;&#21547;&#25935;&#24863;&#21644;&#21487;&#35782;&#21035;&#20010;&#20154;&#20449;&#24687;&#65288;PII&#65289;&#65292;&#38656;&#35201;&#20445;&#25252;&#12290;&#20256;&#32479;&#30340;MRI&#25968;&#25454;&#21435;&#35782;&#21035;&#26041;&#27861;&#36890;&#36807;&#21024;&#38500;&#38544;&#31169;&#25935;&#24863;&#37096;&#20301;&#65288;&#22914;&#30524;&#30555;&#12289;&#40763;&#23376;&#31561;&#65289;&#26469;&#23454;&#29616;&#65292;&#20294;&#20250;&#24341;&#20837;&#39046;&#22495;&#36716;&#25442;&#65292;&#24433;&#21709;&#19979;&#28216;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CP-MAE&#27169;&#22411;&#65292;&#36890;&#36807;&#38754;&#37096;&#36974;&#32617;&#26469;&#23454;&#29616;&#20154;&#33080;&#21435;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
MRI scans provide valuable medical information, however they also contain sensitive and personally identifiable information (PII) that needs to be protected. Whereas MRI metadata is easily sanitized, MRI image data is a privacy risk because it contains information to render highly-realistic 3D visualizations of a patient's head, enabling malicious actors to possibly identify the subject by cross-referencing a database. Data anonymization and de-identification is concerned with ensuring the privacy and confidentiality of individuals' personal information. Traditional MRI de-identification methods remove privacy-sensitive parts (e.g. eyes, nose etc.) from a given scan. This comes at the expense of introducing a domain shift that can throw off downstream analyses. Recently, a GAN-based approach was proposed to de-identify a patient's scan by remodeling it (e.g. changing the face) rather than by removing parts. In this work, we propose CP-MAE, a model that de-identifies the face using mask
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#20915;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#24110;&#21161;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#23545;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#20005;&#37325;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2310.09886</link><description>&lt;p&gt;
&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation. (arXiv:2310.09886v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09886
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#20915;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#24110;&#21161;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#23545;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#20005;&#37325;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#65288;LSG&#65289;&#26159;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#35753;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#25345;&#32493;&#35757;&#32451;&#65292;&#20197;&#19981;&#26029;&#23398;&#20064;&#26032;&#30340;&#29983;&#25104;&#27169;&#24335;&#24182;&#36991;&#20813;&#36951;&#24536;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;LSG&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#32500;&#25345;&#26087;&#30693;&#35782;&#65292;&#32780;&#23545;&#36328;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#20851;&#27880;&#36739;&#23569;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#33719;&#21462;&#30340;&#31867;&#20284;&#20219;&#21153;&#30340;&#30693;&#35782;&#26356;&#22909;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#21463;&#20154;&#31867;&#23398;&#20064;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#65288;DMEA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#30830;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#26368;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#20419;&#36827;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23398;&#20064;&#36807;&#31243;&#24456;&#23481;&#26131;&#20559;&#21521;&#20110;&#24403;&#21069;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong sequence generation (LSG), a problem in continual learning, aims to continually train a model on a sequence of generation tasks to learn constantly emerging new generation patterns while avoiding the forgetting of previous knowledge. Existing LSG methods mainly focus on maintaining old knowledge while paying little attention to knowledge transfer across tasks. In contrast, humans can better learn new tasks by leveraging previously acquired knowledge from similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic Module Expansion and Adaptation (DMEA), which enables the model to dynamically determine the architecture for acquiring new knowledge based on task correlation and select the most similar previous tasks to facilitate adaptation to new tasks. In addition, as the learning process can easily be biased towards the current task which might cause more severe forgetting of previously learned knowledge, we propose dynamic gradient scaling to balance the lea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#20462;&#21098;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#36825;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.08782</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#39537;&#21160;&#29983;&#20135;&#21147;&#65306;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning. (arXiv:2310.08782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#20462;&#21098;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#36825;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20294;&#21516;&#26102;&#20063;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#22522;&#30784;&#35774;&#26045;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#20462;&#21098;&#65288;DP&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#20986;&#29616;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#21024;&#38500;&#20887;&#20313;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;DP&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#19979;&#28216;&#30446;&#26631;&#20219;&#21153;&#20013;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#21644;&#23436;&#25972;&#24494;&#35843;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20462;&#21098;&#28304;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36801;&#31227;&#23398;&#20064;&#30340;DP&#38382;&#39064;&#20173;&#28982;&#26410;&#35299;&#20915;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;DP&#21644;&#36801;&#31227;&#23398;&#20064;&#35270;&#20026;&#29420;&#31435;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#23558;DP&#19982;&#36801;&#31227;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;DP&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;DP&#26041;&#27861;&#65292;&#21363;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#65292;&#29992;&#20110;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive data is often considered essential for deep learning applications, but it also incurs significant computational and infrastructural costs. Therefore, dataset pruning (DP) has emerged as an effective way to improve data efficiency by identifying and removing redundant training samples without sacrificing performance. In this work, we aim to address the problem of DP for transfer learning, i.e., how to prune a source dataset for improved pretraining efficiency and lossless finetuning accuracy on downstream target tasks. To our best knowledge, the problem of DP for transfer learning remains open, as previous studies have primarily addressed DP and transfer learning as separate problems. By contrast, we establish a unified viewpoint to integrate DP with transfer learning and find that existing DP methods are not suitable for the transfer learning paradigm. We then propose two new DP methods, label mapping and feature mapping, for supervised and self-supervised pretraining settings 
&lt;/p&gt;</description></item><item><title>Lag-Llama&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35757;&#32451;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#27169;&#22411;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#25193;&#23637;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.08278</link><description>&lt;p&gt;
Lag-Llama: &#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lag-Llama: Towards Foundation Models for Time Series Forecasting. (arXiv:2310.08278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08278
&lt;/p&gt;
&lt;p&gt;
Lag-Llama&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35757;&#32451;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#27169;&#22411;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#25193;&#23637;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#24182;&#30740;&#31350;&#20854;&#25193;&#23637;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#20013;&#30340; Lag-Llama &#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#8220;&#20998;&#24067;&#22806;&#8221;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#20248;&#20110;&#26377;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#25193;&#23637;&#34892;&#20026;&#12290;&#24320;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/kashif/pytorch-transformer-ts &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming to build foundation models for time-series forecasting and study their scaling behavior, we present here our work-in-progress on Lag-Llama, a general-purpose univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The model shows good zero-shot prediction capabilities on unseen "out-of-distribution" time-series datasets, outperforming supervised baselines. We use smoothly broken power-laws to fit and predict model scaling behavior. The open source code is made available at https://github.com/kashif/pytorch-transformer-ts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#65292;&#23398;&#20064;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#24449;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03358</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#36827;&#34892;&#40065;&#26834;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Representation Learning via Asymmetric Negative Contrast and Reverse Attention. (arXiv:2310.03358v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#65292;&#23398;&#20064;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#24449;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#22122;&#22768;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#34987;&#35777;&#26126;&#26159;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#20813;&#21463;&#27450;&#39575;&#30340;&#26368;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;AT&#24573;&#35270;&#20102;&#23398;&#20064;&#40065;&#26834;&#29305;&#24449;&#65292;&#23548;&#33268;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#40065;&#26834;&#34920;&#24449;&#30340;&#20004;&#20010;&#29305;&#24449;&#65306;&#65288;1&#65289;&#25490;&#20182;&#24615;&#65306;&#33258;&#28982;&#26679;&#26412;&#30340;&#29305;&#24449;&#36828;&#31163;&#20854;&#20182;&#31867;&#21035;&#30340;&#29305;&#24449;&#65307;&#65288;2&#65289;&#23545;&#40784;&#24615;&#65306;&#33258;&#28982;&#26679;&#26412;&#21644;&#30456;&#24212;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#29305;&#24449;&#24444;&#27492;&#25509;&#36817;&#12290;&#36825;&#20123;&#29305;&#28857;&#28608;&#21457;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;AT&#26694;&#26550;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#26469;&#33719;&#24471;&#40065;&#26834;&#30340;&#34920;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#39044;&#27979;&#27010;&#29575;&#30340;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#65292;&#23558;&#29305;&#24449;&#31354;&#38388;&#20013;&#19981;&#21516;&#31867;&#21035;&#30340;&#26679;&#26412;&#25512;&#24320;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#21442;&#25968;&#23545;&#29305;&#24449;&#36827;&#34892;&#21152;&#26435;&#65292;&#20316;&#20026;&#21453;&#21521;&#27880;&#24847;&#21147;&#65292;&#20197;&#33719;&#24471;&#40065;&#26834;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are vulnerable to adversarial noise. Adversarial training (AT) has been demonstrated to be the most effective defense strategy to protect neural networks from being fooled. However, we find AT omits to learning robust features, resulting in poor performance of adversarial robustness. To address this issue, we highlight two characteristics of robust representation: (1) $\bf{exclusion}$: the feature of natural examples keeps away from that of other classes; (2) $\bf{alignment}$: the feature of natural and corresponding adversarial examples is close to each other. These motivate us to propose a generic framework of AT to gain robust representation, by the asymmetric negative contrast and reverse attention. Specifically, we design an asymmetric negative contrast based on predicted probabilities, to push away examples of different classes in the feature space. Moreover, we propose to weight feature by parameters of the linear classifier as the reverse attention, to obta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#20998;&#31163;&#34920;&#31034;&#26469;&#22788;&#29702;&#19981;&#23436;&#20840;&#35268;&#23450;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#25463;&#24452;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#24573;&#30053;&#25463;&#24452;&#32447;&#32034;&#24182;&#36798;&#21040;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02230</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#20998;&#31163;&#34920;&#31034;&#26469;&#32531;&#35299;&#19981;&#23436;&#20840;&#35268;&#23450;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25463;&#24452;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Diffusion Disentangled Representations to Mitigate Shortcuts in Underspecified Visual Tasks. (arXiv:2310.02230v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#20998;&#31163;&#34920;&#31034;&#26469;&#22788;&#29702;&#19981;&#23436;&#20840;&#35268;&#23450;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#25463;&#24452;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#24573;&#30053;&#25463;&#24452;&#32447;&#32034;&#24182;&#36798;&#21040;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#20266;&#30456;&#20851;&#24615;&#65292;&#20854;&#20013;&#22810;&#20010;&#32447;&#32034;&#39044;&#27979;&#30446;&#26631;&#26631;&#31614;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#25463;&#24452;&#23398;&#20064;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#21487;&#33021;&#20381;&#36182;&#20110;&#38169;&#35823;&#30340;&#12289;&#26131;&#20110;&#23398;&#20064;&#30340;&#32447;&#32034;&#32780;&#24573;&#30053;&#21487;&#38752;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#30340;&#38598;&#25104;&#22810;&#26679;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#20013;&#36825;&#20123;&#32447;&#32034;&#39640;&#24230;&#30456;&#20851;&#65292;DPMs&#20855;&#26377;&#29420;&#31435;&#34920;&#31034;&#22810;&#20010;&#35270;&#35273;&#32447;&#32034;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#29305;&#24615;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#20960;&#20010;&#22810;&#26679;&#21270;&#30446;&#26631;&#19978;&#23454;&#35777;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25193;&#25955;&#24341;&#23548;&#30340;&#22810;&#26679;&#21270;&#21487;&#20197;&#20351;&#27169;&#22411;&#36991;&#24320;&#25463;&#24452;&#32447;&#32034;&#30340;&#27880;&#24847;&#65292;&#23454;&#29616;&#20102;&#19982;&#38656;&#35201;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#30340;&#20808;&#21069;&#26041;&#27861;&#21487;&#27604;&#36739;&#30340;&#38598;&#25104;&#22810;&#26679;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to shortcut learning phenomena, where a model may rely on erroneous, easy-to-learn, cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting the generation of synthetic counterfactuals using Diffusion Probabilistic Models (DPMs). We discover that DPMs have the inherent capability to represent multiple visual cues independently, even when they are largely correlated in the training data. We leverage this characteristic to encourage model diversity and empirically show the efficacy of the approach with respect to several diversification objectives. We show that diffusion-guided diversification can lead models to avert attention from shortcut cues, achieving ensemble diversity performance comparable to previous methods requiring additional data collection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20043;&#21069;&#23398;&#20064;&#30340;&#21407;&#22987;&#34892;&#20026;&#26469;&#24341;&#23548;&#20195;&#29702;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22238;&#39038;&#24335;&#32463;&#39564;&#37325;&#29616;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.01827</link><description>&lt;p&gt;
&#23398;&#20064;&#21644;&#37325;&#22797;&#20351;&#29992;&#21407;&#22987;&#34892;&#20026;&#20197;&#25552;&#39640;&#22238;&#39038;&#24335;&#32463;&#39564;&#37325;&#29616;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Learning and reusing primitive behaviours to improve Hindsight Experience Replay sample efficiency. (arXiv:2310.01827v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20043;&#21069;&#23398;&#20064;&#30340;&#21407;&#22987;&#34892;&#20026;&#26469;&#24341;&#23548;&#20195;&#29702;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22238;&#39038;&#24335;&#32463;&#39564;&#37325;&#29616;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#39038;&#24335;&#32463;&#39564;&#37325;&#29616;&#65288;HER&#65289;&#26159;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#24050;&#34987;&#35777;&#26126;&#23545;&#35757;&#32451;&#22522;&#20110;&#31163;&#32447;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20197;&#35299;&#20915;&#22522;&#20110;&#30446;&#26631;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20855;&#26377;&#38750;&#24120;&#39640;&#25928;&#30340;&#25928;&#26524;&#65292;&#20294;&#23613;&#31649;HER&#36890;&#36807;&#23398;&#20064;&#20197;&#24448;&#32463;&#39564;&#20013;&#30340;&#38169;&#35823;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#23427;&#22312;&#25506;&#32034;&#29615;&#22659;&#26102;&#24182;&#19981;&#25552;&#20379;&#20219;&#20309;&#25351;&#23548;&#65292;&#36825;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#38750;&#24120;&#38271;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20043;&#21069;&#23398;&#20064;&#30340;&#35299;&#20915;&#31616;&#21333;&#20219;&#21153;&#30340;&#21407;&#22987;&#34892;&#20026;&#65292;&#26469;&#24341;&#23548;&#20195;&#29702;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#26397;&#30528;&#26356;&#26377;&#22238;&#25253;&#30340;&#21160;&#20316;&#26041;&#21521;&#23398;&#20064;&#20854;&#20182;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#36825;&#31181;&#24341;&#23548;&#19981;&#26159;&#36890;&#36807;&#25163;&#21160;&#35774;&#35745;&#30340;&#35838;&#31243;&#26469;&#25191;&#34892;&#65292;&#32780;&#26159;&#20351;&#29992;&#35780;&#35770;&#23478;&#32593;&#32476;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#20915;&#23450;&#26159;&#21542;&#20351;&#29992;&#20197;&#21069;&#23398;&#20064;&#30340;&#21407;&#22987;&#31574;&#30053;&#25552;&#20379;&#30340;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hindsight Experience Replay (HER) is a technique used in reinforcement learning (RL) that has proven to be very efficient for training off-policy RL-based agents to solve goal-based robotic manipulation tasks using sparse rewards. Even though HER improves the sample efficiency of RL-based agents by learning from mistakes made in past experiences, it does not provide any guidance while exploring the environment. This leads to very large training times due to the volume of experience required to train an agent using this replay strategy. In this paper, we propose a method that uses primitive behaviours that have been previously learned to solve simple tasks in order to guide the agent toward more rewarding actions during exploration while learning other more complex tasks. This guidance, however, is not executed by a manually designed curriculum, but rather using a critic network to decide at each timestep whether or not to use the actions proposed by the previously-learned primitive pol
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#20803;&#36335;&#24452;&#21644;&#20803;&#36335;&#24452;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20851;&#38190;&#26159;&#20351;&#29992;&#35780;&#20998;&#20989;&#25968;&#26469;&#34913;&#37327;&#20851;&#31995;&#30340;&#28508;&#22312;&#20449;&#24687;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;GNNs&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17113</link><description>&lt;p&gt;
&#22810;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#36335;&#24452;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta-Path Learning for Multi-relational Graph Neural Networks. (arXiv:2309.17113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17113
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#20803;&#36335;&#24452;&#21644;&#20803;&#36335;&#24452;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20851;&#38190;&#26159;&#20351;&#29992;&#35780;&#20998;&#20989;&#25968;&#26469;&#34913;&#37327;&#20851;&#31995;&#30340;&#28508;&#22312;&#20449;&#24687;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;GNNs&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#20004;&#31181;&#31574;&#30053;&#26469;&#30830;&#23450;&#20449;&#24687;&#30456;&#20851;&#30340;&#20851;&#31995;&#65306;&#35201;&#20040;&#23558;&#36825;&#20010;&#38382;&#39064;&#31616;&#21270;&#20026;&#20302;&#32423;&#26435;&#37325;&#23398;&#20064;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#20851;&#31995;&#20381;&#36182;&#38142;&#65292;&#31216;&#20026;&#20803;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#21069;&#19968;&#31181;&#26041;&#27861;&#22312;&#23384;&#22312;&#22823;&#37327;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#30693;&#35782;&#22270;&#35889;&#65289;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#21518;&#19968;&#31181;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#30830;&#23450;&#30456;&#20851;&#30340;&#20803;&#36335;&#24452;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20803;&#36335;&#24452;&#21644;&#20803;&#36335;&#24452;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#32593;&#32476;&#22522;&#20110;&#23569;&#37327;&#26377;&#20449;&#24687;&#37327;&#30340;&#20803;&#36335;&#24452;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#19968;&#20010;&#35780;&#20998;&#20989;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#20803;&#36335;&#24452;&#30340;&#22686;&#37327;&#26500;&#24314;&#20013;&#20851;&#31995;&#30340;&#28508;&#22312;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#21363;&#20351;&#26377;&#22823;&#37327;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20173;&#33021;&#27491;&#30830;&#35782;&#21035;&#30456;&#20851;&#30340;&#20803;&#36335;&#24452;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing multi-relational graph neural networks use one of two strategies for identifying informative relations: either they reduce this problem to low-level weight learning, or they rely on handcrafted chains of relational dependencies, called meta-paths. However, the former approach faces challenges in the presence of many relations (e.g., knowledge graphs), while the latter requires substantial domain expertise to identify relevant meta-paths. In this work we propose a novel approach to learn meta-paths and meta-path GNNs that are highly accurate based on a small number of informative meta-paths. Key element of our approach is a scoring function for measuring the potential informativeness of a relation in the incremental construction of the meta-path. Our experimental evaluation shows that the approach manages to correctly identify relevant meta-paths even with a large number of relations, and substantially outperforms existing multi-relational GNNs on synthetic and real-world exper
&lt;/p&gt;</description></item><item><title>SA2-Net&#26159;&#19968;&#31181;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#26174;&#24494;&#22270;&#20687;&#20013;&#30340;&#22810;&#26679;&#32467;&#26500;&#12290;&#23427;&#32467;&#21512;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#23398;&#20064;&#21644;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2309.16661</link><description>&lt;p&gt;
SA2-Net: &#29992;&#20110;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#30340;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SA2-Net: Scale-aware Attention Network for Microscopic Image Segmentation. (arXiv:2309.16661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16661
&lt;/p&gt;
&lt;p&gt;
SA2-Net&#26159;&#19968;&#31181;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#26174;&#24494;&#22270;&#20687;&#20013;&#30340;&#22810;&#26679;&#32467;&#26500;&#12290;&#23427;&#32467;&#21512;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#23398;&#20064;&#21644;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#20026;&#32473;&#23450;&#30340;&#26174;&#24494;&#22270;&#20687;&#20013;&#30340;&#27599;&#20010;&#20687;&#32032;&#20998;&#37197;&#35821;&#20041;&#26631;&#31614;&#12290;&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#26159;&#35768;&#22810;&#29616;&#26377;&#26694;&#26550;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38590;&#20197;&#26126;&#30830;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#21464;&#21387;&#22120;&#26368;&#21021;&#26159;&#20026;&#20102;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#26174;&#24494;&#22270;&#20687;&#20013;&#65292;&#21253;&#25324;&#24418;&#29366;&#12289;&#22823;&#23567;&#12289;&#22806;&#35266;&#21644;&#30446;&#26631;&#21306;&#22495;&#23494;&#24230;&#30340;&#21508;&#31181;&#25361;&#25112;&#20013;&#65292;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SA2-Net&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#23398;&#20064;&#26469;&#26377;&#25928;&#22788;&#29702;&#26174;&#24494;&#22270;&#20687;&#20013;&#22810;&#26679;&#32467;&#26500;&#30340;&#27880;&#24847;&#24341;&#23548;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#65288;SA2&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#25429;&#25417;&#26174;&#24494;&#21306;&#22495;&#65288;&#22914;&#32454;&#32990;&#65289;&#23610;&#24230;&#21644;&#24418;&#29366;&#30340;&#22266;&#26377;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;&#36825;&#20010;&#27169;&#22359;&#32467;&#21512;&#20102;&#23616;&#37096;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Microscopic image segmentation is a challenging task, wherein the objective is to assign semantic labels to each pixel in a given microscopic image. While convolutional neural networks (CNNs) form the foundation of many existing frameworks, they often struggle to explicitly capture long-range dependencies. Although transformers were initially devised to address this issue using self-attention, it has been proven that both local and global features are crucial for addressing diverse challenges in microscopic images, including variations in shape, size, appearance, and target region density. In this paper, we introduce SA2-Net, an attention-guided method that leverages multi-scale feature learning to effectively handle diverse structures within microscopic images. Specifically, we propose scale-aware attention (SA2) module designed to capture inherent variations in scales and shapes of microscopic regions, such as cells, for accurate segmentation. This module incorporates local attention
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#39044;&#27979;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;FD&#24212;&#29992;&#20110;&#37329;&#34701;&#25968;&#25454;&#24182;&#32467;&#21512;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;FD&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#25972;&#25968;&#24046;&#20998;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.13409</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#25968;&#25454;&#37322;&#25918;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Time-Series Forecasting: Unleashing Long-Term Dependencies with Fractionally Differenced Data. (arXiv:2309.13409v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#39044;&#27979;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;FD&#24212;&#29992;&#20110;&#37329;&#34701;&#25968;&#25454;&#24182;&#32467;&#21512;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;FD&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#25972;&#25968;&#24046;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#27979;&#31574;&#30053;&#65292;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#65288;FD&#65289;&#30340;&#33021;&#21147;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#19982;&#20256;&#32479;&#30340;&#25972;&#25968;&#24046;&#20998;&#26041;&#27861;&#19981;&#21516;&#65292;FD&#22312;&#20445;&#25345;&#31995;&#21015;&#35760;&#24518;&#30340;&#21516;&#26102;&#31283;&#23450;&#20102;&#23427;&#20197;&#20379;&#24314;&#27169;&#30446;&#30340;&#12290;&#36890;&#36807;&#23558;FD&#24212;&#29992;&#20110;&#26469;&#33258;SPY&#25351;&#25968;&#30340;&#37329;&#34701;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#26032;&#38395;&#25253;&#36947;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36825;&#20010;&#23454;&#35777;&#20998;&#26512;&#25506;&#35752;&#20102;FD&#19982;&#30446;&#26631;&#21464;&#37327;&#30340;&#20108;&#20803;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;&#37319;&#29992;&#20102;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#26469;&#39564;&#35777;FD&#31995;&#21015;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;FD&#30456;&#27604;&#25972;&#25968;&#24046;&#20998;&#20855;&#26377;&#20248;&#36234;&#24615;&#65292;&#36825;&#19968;&#28857;&#36890;&#36807;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24449;/&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;ROCAUC&#65289;&#21644;&#39532;&#20462;&#26031;&#30456;&#20851;&#31995;&#25968;&#65288;MCC&#65289;&#30340;&#35780;&#20272;&#24471;&#21040;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces a novel forecasting strategy that leverages the power of fractional differencing (FD) to capture both short- and long-term dependencies in time series data. Unlike traditional integer differencing methods, FD preserves memory in series while stabilizing it for modeling purposes. By applying FD to financial data from the SPY index and incorporating sentiment analysis from news reports, this empirical analysis explores the effectiveness of FD in conjunction with binary classification of target variables. Supervised classification algorithms were employed to validate the performance of FD series. The results demonstrate the superiority of FD over integer differencing, as confirmed by Receiver Operating Characteristic/Area Under the Curve (ROCAUC) and Mathews Correlation Coefficient (MCC) evaluations.
&lt;/p&gt;</description></item><item><title>SALSA-CLRS&#26159;&#19968;&#31181;&#31232;&#30095;&#19988;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#65292;&#20248;&#20808;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#34920;&#31034;&#30340;&#21033;&#29992;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#36866;&#24212;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;CLRS&#22522;&#20934;&#20013;&#20869;&#23384;&#38656;&#27714;&#39640;&#21644;&#36816;&#34892;&#26102;&#38388;&#38590;&#20197;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12253</link><description>&lt;p&gt;
SALSA-CLRS:&#19968;&#31181;&#31232;&#30095;&#19988;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning. (arXiv:2309.12253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12253
&lt;/p&gt;
&lt;p&gt;
SALSA-CLRS&#26159;&#19968;&#31181;&#31232;&#30095;&#19988;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#65292;&#20248;&#20808;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#34920;&#31034;&#30340;&#21033;&#29992;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#36866;&#24212;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;CLRS&#22522;&#20934;&#20013;&#20869;&#23384;&#38656;&#27714;&#39640;&#21644;&#36816;&#34892;&#26102;&#38388;&#38590;&#20197;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CLRS&#31639;&#27861;&#23398;&#20064;&#22522;&#20934;&#30340;&#25193;&#23637;&#65292;&#20248;&#20808;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#34920;&#31034;&#30340;&#21033;&#29992;&#12290;CLRS&#20013;&#30340;&#35768;&#22810;&#31639;&#27861;&#38656;&#35201;&#20840;&#23616;&#23384;&#20648;&#22120;&#25110;&#20449;&#24687;&#20132;&#25442;&#65292;&#22312;&#20854;&#25191;&#34892;&#27169;&#22411;&#20013;&#38236;&#20687;&#34920;&#36798;&#20026;&#22522;&#20110;&#24213;&#23618;&#38382;&#39064;&#26500;&#24314;&#23436;&#20840;&#36830;&#25509;&#65288;&#32780;&#38750;&#31232;&#30095;&#65289;&#22270;&#30340;&#25805;&#20316;&#12290;&#23613;&#31649;CLRS&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#23398;&#20064;&#31639;&#27861;&#22312;&#26356;&#22823;&#23454;&#20363;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#29616;&#26377;&#30340;&#25191;&#34892;&#27169;&#22411;&#30001;&#20110;&#20854;&#35201;&#27714;&#39640;&#30340;&#20869;&#23384;&#38656;&#27714;&#21644;&#36816;&#34892;&#26102;&#38388;&#32780;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65288;&#38590;&#20197;&#25193;&#23637;&#65289;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#37325;&#35201;&#30340;&#31639;&#27861;&#24182;&#19981;&#38656;&#35201;&#23436;&#20840;&#36830;&#25509;&#30340;&#22270;&#65307;&#36825;&#20123;&#20027;&#35201;&#20998;&#24067;&#24335;&#31639;&#27861;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#37319;&#29992;&#30340;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SALSA-CLRS&#65292;&#19968;&#20010;&#19987;&#38376;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#24615;&#30340;CLRS&#22522;&#20934;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20174;&#21407;&#22987;CLRS&#22522;&#20934;&#20013;&#25913;&#32534;&#30340;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an extension to the CLRS algorithmic learning benchmark, prioritizing scalability and the utilization of sparse representations. Many algorithms in CLRS require global memory or information exchange, mirrored in its execution model, which constructs fully connected (not sparse) graphs based on the underlying problem. Despite CLRS's aim of assessing how effectively learned algorithms can generalize to larger instances, the existing execution model becomes a significant constraint due to its demanding memory requirements and runtime (hard to scale). However, many important algorithms do not demand a fully connected graph; these algorithms, primarily distributed in nature, align closely with the message-passing paradigm employed by Graph Neural Networks. Hence, we propose SALSA-CLRS, an extension of the current CLRS benchmark specifically with scalability and sparseness in mind. Our approach includes adapted algorithms from the original CLRS benchmark and introduces new probl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29289;&#20307;&#32423;&#30340;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#65292;&#20197;&#35299;&#20915;&#22810;&#20010;&#36974;&#25377;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#30340;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07510</link><description>&lt;p&gt;
&#23398;&#20064;&#29615;&#22659;&#24863;&#30693;&#30340;&#36974;&#25377;&#19979;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#30340;&#21487;&#20379;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions. (arXiv:2309.07510v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29289;&#20307;&#32423;&#30340;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#65292;&#20197;&#35299;&#20915;&#22810;&#20010;&#36974;&#25377;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#30340;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26679;&#30340;&#29615;&#22659;&#20013;&#24863;&#30693;&#21644;&#25805;&#20316;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#23545;&#20110;&#23478;&#24237;&#21161;&#29702;&#26426;&#22120;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28857;&#32423;&#21487;&#20379;&#24615;&#20026;&#19979;&#28216;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#20808;&#39564;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#29289;&#20307;&#22330;&#26223;&#20013;&#30340;&#22343;&#36136;&#20195;&#29702;&#65292;&#24573;&#35270;&#20102;&#29615;&#22659;&#21644;&#20195;&#29702;&#24418;&#24577;&#25152;&#26045;&#21152;&#30340;&#29616;&#23454;&#32422;&#26463;&#65292;&#22914;&#36974;&#25377;&#21644;&#29289;&#29702;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#29289;&#20307;&#32423;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#12290;&#19982;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#21487;&#20379;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#23398;&#20064;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#38754;&#20020;&#30528;&#30001;&#21508;&#31181;&#36974;&#25377;&#30340;&#22797;&#26434;&#24615;&#24341;&#36215;&#30340;&#32452;&#21512;&#29190;&#28856;&#25361;&#25112;&#65292;&#36825;&#20123;&#36974;&#25377;&#20197;&#20854;&#25968;&#37327;&#12289;&#20960;&#20309;&#24418;&#29366;&#12289;&#20301;&#32622;&#21644;&#23039;&#21183;&#26469;&#21051;&#30011;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#24335;&#21487;&#20379;&#24615;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21547;&#26377;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceiving and manipulating 3D articulated objects in diverse environments is essential for home-assistant robots. Recent studies have shown that point-level affordance provides actionable priors for downstream manipulation tasks. However, existing works primarily focus on single-object scenarios with homogeneous agents, overlooking the realistic constraints imposed by the environment and the agent's morphology, e.g., occlusions and physical limitations. In this paper, we propose an environment-aware affordance framework that incorporates both object-level actionable priors and environment constraints. Unlike object-centric affordance approaches, learning environment-aware affordance faces the challenge of combinatorial explosion due to the complexity of various occlusions, characterized by their quantities, geometries, positions and poses. To address this and enhance data efficiency, we introduce a novel contrastive affordance learning framework capable of training on scenes containin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28548;&#28165;&#20102;&#22312;&#38543;&#26426;&#32534;&#31243;&#20013;&#20351;&#29992;&#30340;&#23398;&#20064;&#27169;&#24335;Pearl&#21644;Jeffrey&#30340;&#26356;&#26032;&#26426;&#21046;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;Jeffrey&#30340;&#26356;&#26032;&#35268;&#21017;&#26159;&#36890;&#36807;&#21464;&#20998;&#25512;&#29702;&#24471;&#21040;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.07053</link><description>&lt;p&gt;
&#38543;&#26426;&#32534;&#31243;&#20013;&#30340;&#23398;&#20064;&#27169;&#24335;&#65306;Pearl&#21644;Jeffrey&#30340;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Pearl's and Jeffrey's Update as Modes of Learning in Probabilistic Programming. (arXiv:2309.07053v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28548;&#28165;&#20102;&#22312;&#38543;&#26426;&#32534;&#31243;&#20013;&#20351;&#29992;&#30340;&#23398;&#20064;&#27169;&#24335;Pearl&#21644;Jeffrey&#30340;&#26356;&#26032;&#26426;&#21046;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;Jeffrey&#30340;&#26356;&#26032;&#35268;&#21017;&#26159;&#36890;&#36807;&#21464;&#20998;&#25512;&#29702;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#26032;&#35777;&#25454;&#26356;&#26032;&#27010;&#29575;&#20998;&#24067;&#30340;&#27010;&#24565;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26680;&#24515;&#12290;Pearl&#21644;Jeffrey&#30340;&#35268;&#21017;&#26159;&#20004;&#31181;&#33258;&#28982;&#30340;&#26356;&#26032;&#26426;&#21046;&#65292;&#23427;&#20204;&#23548;&#33268;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#20294;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#20173;&#28982;&#31070;&#31192;&#12290;&#26412;&#25991;&#36890;&#36807;&#27010;&#29575;&#31243;&#24207;&#21644;&#37319;&#26679;&#35821;&#20041;&#30340;&#20998;&#21035;&#25551;&#36848;&#65292;&#20197;&#21450;&#20851;&#20110;Pearl&#21644;Jeffrey&#30340;&#19981;&#21516;&#20284;&#28982;&#24230;&#30340;&#27010;&#24565;&#65292;&#38416;&#26126;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;Jeffrey&#30340;&#26356;&#26032;&#35268;&#21017;&#26159;&#36890;&#36807;&#21464;&#20998;&#25512;&#29702;&#24471;&#21040;&#30340;&#12290;&#20174;&#20998;&#31867;&#27010;&#29575;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#30456;&#24403;&#20110;&#23545;&#22810;&#37325;&#38598;&#21512;&#20989;&#23376;&#22312;&#20998;&#24067;&#21333;&#23376;&#33539;&#30068;&#20013;&#30340;&#34892;&#20026;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of updating a probability distribution in the light of new evidence lies at the heart of statistics and machine learning. Pearl's and Jeffrey's rule are two natural update mechanisms which lead to different outcomes, yet the similarities and differences remain mysterious. This paper clarifies their relationship in several ways: via separate descriptions of the two update mechanisms in terms of probabilistic programs and sampling semantics, and via different notions of likelihood (for Pearl and for Jeffrey). Moreover, it is shown that Jeffrey's update rule arises via variational inference. In terms of categorical probability theory, this amounts to an analysis of the situation in terms of the behaviour of the multiset functor, extended to the Kleisli category of the distribution monad.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21306;&#22495;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#34701;&#21512;&#21306;&#22495;&#34917;&#19969;&#20449;&#24687;&#20197;&#24471;&#20986;&#28369;&#29255;&#32423;&#21035;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#21306;&#22495;&#32858;&#21512;&#26469;&#20998;&#23618;&#22788;&#29702;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32858;&#28966;&#20110;&#39640;&#20851;&#27880;&#21306;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.12634</link><description>&lt;p&gt;
&#38754;&#21521;&#20998;&#23618;&#21306;&#22495;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Hierarchical Regional Transformer-based Multiple Instance Learning. (arXiv:2308.12634v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21306;&#22495;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#34701;&#21512;&#21306;&#22495;&#34917;&#19969;&#20449;&#24687;&#20197;&#24471;&#20986;&#28369;&#29255;&#32423;&#21035;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#21306;&#22495;&#32858;&#21512;&#26469;&#20998;&#23618;&#22788;&#29702;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32858;&#28966;&#20110;&#39640;&#20851;&#27880;&#21306;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#21644;&#31934;&#30830;&#21307;&#23398;&#20013;&#65292;&#20351;&#29992;&#28145;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#23545;&#24040;&#20687;&#32032;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#24050;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29992;&#21306;&#22495;&#24615;&#30340;&#12289;&#21463;&#21040;&#35270;&#35273;&#21464;&#21387;&#22120;&#21551;&#21457;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26367;&#20195;&#20102;&#20256;&#32479;&#30340;&#23398;&#20064;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#21306;&#22495;&#34917;&#19969;&#20449;&#24687;&#20197;&#24471;&#20986;&#28369;&#29255;&#32423;&#21035;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#22534;&#21472;&#36825;&#31181;&#21306;&#22495;&#32858;&#21512;&#20197;&#20998;&#23618;&#22320;&#22788;&#29702;&#19981;&#21516;&#36317;&#31163;&#27700;&#24179;&#19978;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#23567;&#30340;&#23616;&#37096;&#24418;&#24577;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26399;&#38388;&#23558;&#22270;&#20687;&#22788;&#29702;&#38598;&#20013;&#22312;&#39640;&#20851;&#27880;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#20004;&#20010;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24182;&#25351;&#21521;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classification of gigapixel histopathology images with deep multiple instance learning models has become a critical task in digital pathology and precision medicine. In this work, we propose a Transformer-based multiple instance learning approach that replaces the traditional learned attention mechanism with a regional, Vision Transformer inspired self-attention mechanism. We present a method that fuses regional patch information to derive slide-level predictions and show how this regional aggregation can be stacked to hierarchically process features on different distance levels. To increase predictive accuracy, especially for datasets with small, local morphological features, we introduce a method to focus the image processing on high attention regions during inference. Our approach is able to significantly improve performance over the baseline on two histopathology datasets and points towards promising directions for further research.
&lt;/p&gt;</description></item><item><title>"&#24341;&#29992;GPT&#30340;&#8220;&#35930;&#40736;&#35797;&#39564;&#8221;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#20225;&#19994;&#36827;&#34892;&#31454;&#20105;&#21644;&#21246;&#32467;&#30740;&#31350;&#12290;&#23427;&#27604;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#23454;&#39564;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#36229;&#36234;&#20256;&#32479;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;"</title><link>http://arxiv.org/abs/2308.10974</link><description>&lt;p&gt;
"&#24341;&#29992;GPT&#30340;&#8220;&#35930;&#40736;&#35797;&#39564;&#8221;&#65306;&#19968;&#31181;&#30740;&#31350;&#20225;&#19994;&#31454;&#20105;&#21644;&#21246;&#32467;&#30340;&#21019;&#26032;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;"
&lt;/p&gt;
&lt;p&gt;
"Guinea Pig Trials" Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion. (arXiv:2308.10974v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10974
&lt;/p&gt;
&lt;p&gt;
"&#24341;&#29992;GPT&#30340;&#8220;&#35930;&#40736;&#35797;&#39564;&#8221;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#20225;&#19994;&#36827;&#34892;&#31454;&#20105;&#21644;&#21246;&#32467;&#30740;&#31350;&#12290;&#23427;&#27604;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#23454;&#39564;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#36229;&#36234;&#20256;&#32479;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#31454;&#20105;&#21644;&#21246;&#32467;&#28041;&#21450;&#22797;&#26434;&#30340;&#21160;&#24577;&#65292;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#20225;&#19994;&#20043;&#38388;&#30340;&#27807;&#36890;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#22797;&#26434;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#20256;&#32479;&#19978;&#36890;&#36807;&#28041;&#21450;&#20154;&#31867;&#20027;&#20307;&#25110;&#22522;&#20110;&#20195;&#29702;&#30340;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#25506;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#65288;SABM&#65289;&#65292;&#20854;&#20013;&#30001;GPT-4&#25216;&#26415;&#25903;&#25345;&#30340;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#20225;&#19994;&#24182;&#30456;&#20114;&#20132;&#20114;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#25511;&#21046;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#26465;&#20214;&#19979;&#20225;&#19994;&#20215;&#26684;&#31454;&#20105;&#21644;&#21246;&#32467;&#34892;&#20026;&#12290;&#19982;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#23454;&#39564;&#30456;&#27604;&#65292;SABM&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#28789;&#27963;&#24615;&#12290;&#26234;&#33021;&#20195;&#29702;&#25317;&#26377;&#20915;&#31574;&#30340;&#24191;&#27867;&#30693;&#35782;&#24211;&#65292;&#23637;&#29616;&#20986;&#31867;&#20284;&#20154;&#31867;&#30340;&#25112;&#30053;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#24314;&#27169;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#24182;&#20010;&#24615;&#21270;&#65292;&#20351;&#20854;&#25104;&#20026;&#30740;&#31350;&#28041;&#21450;&#27807;&#36890;&#30340;&#22797;&#26434;&#24773;&#20917;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Firm competition and collusion involve complex dynamics, particularly when considering communication among firms. Such issues can be modeled as problems of complex systems, traditionally approached through experiments involving human subjects or agent-based modeling methods. We propose an innovative framework called Smart Agent-Based Modeling (SABM), wherein smart agents, supported by GPT-4 technologies, represent firms, and interact with one another. We conducted a controlled experiment to study firm price competition and collusion behaviors under various conditions. SABM is more cost-effective and flexible compared to conducting experiments with human subjects. Smart agents possess an extensive knowledge base for decision-making and exhibit human-like strategic abilities, surpassing traditional ABM agents. Furthermore, smart agents can simulate human conversation and be personalized, making them ideal for studying complex situations involving communication. Our results demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65288;KC&#65289;&#65292;&#36890;&#36807;&#22312;&#20005;&#26684;&#30340;&#20302;&#24310;&#36831;&#32422;&#26463;&#19979;&#25552;&#21319;&#22312;&#32447;FastText&#27169;&#22411;&#30340;&#26597;&#35810;&#20998;&#31867;&#24615;&#33021;&#65292;&#22312;&#20140;&#19996;&#24191;&#21578;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.01098</link><description>&lt;p&gt;
&#22312;&#20140;&#19996;&#24191;&#21578;&#25628;&#32034;&#20013;&#21033;&#29992;&#22810;&#19987;&#23478;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#26356;&#22909;&#30340;&#26597;&#35810;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search. (arXiv:2308.01098v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65288;KC&#65289;&#65292;&#36890;&#36807;&#22312;&#20005;&#26684;&#30340;&#20302;&#24310;&#36831;&#32422;&#26463;&#19979;&#25552;&#21319;&#22312;&#32447;FastText&#27169;&#22411;&#30340;&#26597;&#35810;&#20998;&#31867;&#24615;&#33021;&#65292;&#22312;&#20140;&#19996;&#24191;&#21578;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#20998;&#31867;&#20316;&#20026;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#30830;&#20445;&#26356;&#20302;&#30340;&#24310;&#36831;&#65292;&#24120;&#20351;&#29992;&#27973;&#23618;&#27169;&#22411;&#65288;&#22914;FastText&#65289;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;FastText&#27169;&#22411;&#30340;&#34920;&#24449;&#33021;&#21147;&#19981;&#36275;&#65292;&#23548;&#33268;&#20998;&#31867;&#24615;&#33021;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#19968;&#20123;&#20302;&#39057;&#26597;&#35810;&#21644;&#23614;&#37096;&#31867;&#21035;&#19978;&#12290;&#20351;&#29992;&#26356;&#28145;&#20837;&#19988;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#23558;&#23548;&#33268;&#26356;&#39640;&#30340;&#22312;&#32447;&#25512;&#26029;&#24310;&#36831;&#21644;&#26356;&#26114;&#36149;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#22312;&#25512;&#26029;&#25928;&#29575;&#21644;&#20998;&#31867;&#24615;&#33021;&#20043;&#38388;&#25240;&#34935;&#26174;&#28982;&#20855;&#26377;&#37325;&#22823;&#23454;&#38469;&#24847;&#20041;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30693;&#35782;&#33976;&#39311;&#65288;KC&#65289;&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20197;&#22312;&#20005;&#26684;&#30340;&#20302;&#24310;&#36831;&#32422;&#26463;&#19979;&#25552;&#21319;&#22312;&#32447;FastText&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#19968;&#20010;&#31163;&#32447;&#27169;&#22411;&#65292;&#36890;&#36807;&#33976;&#39311;&#30693;&#35782;&#26469;&#25913;&#21892;&#22312;&#32447;&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Search query classification, as an effective way to understand user intents, is of great importance in real-world online ads systems. To ensure a lower latency, a shallow model (e.g. FastText) is widely used for efficient online inference. However, the representation ability of the FastText model is insufficient, resulting in poor classification performance, especially on some low-frequency queries and tailed categories. Using a deeper and more complex model (e.g. BERT) is an effective solution, but it will cause a higher online inference latency and more expensive computing costs. Thus, how to juggle both inference efficiency and classification performance is obviously of great practical importance. To overcome this challenge, in this paper, we propose knowledge condensation (KC), a simple yet effective knowledge distillation framework to boost the classification performance of the online FastText model under strict low latency constraints. Specifically, we propose to train an offline
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16387</link><description>&lt;p&gt;
Relation-Oriented: &#36808;&#21521;&#19982;&#30693;&#35782;&#23545;&#20934;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Relation-Oriented: Toward Knowledge-Aligned Causal AI. (arXiv:2307.16387v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24212;&#29992;&#19968;&#20010;&#35266;&#23519;&#23548;&#21521;&#30340;&#21407;&#21017;&#65292;&#20854;&#20013;&#35266;&#23519;&#21464;&#37327;&#20808;&#23384;&#22312;&#24182;&#20026;&#26500;&#24314;&#20851;&#31995;&#22880;&#23450;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#26469;&#35828;&#36275;&#22815;&#20102;&#65292;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#25968;&#25454;&#30340;&#25972;&#21512;&#26292;&#38706;&#20102;&#35266;&#23519;&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#23454;&#38469;&#29702;&#35299;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22609;&#36896;&#20102;&#30001;&#20851;&#31995;&#23450;&#20041;&#30340;&#35748;&#30693;&#23454;&#20307;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36328;&#36234;&#26102;&#38388;&#21644;&#36229;&#32500;&#24230;&#31354;&#38388;&#21046;&#23450;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#34987;&#38480;&#21046;&#22312;&#35266;&#23519;&#26500;&#24314;&#20013;&#12290;&#20174;&#19968;&#31181;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20581;&#24247;&#20449;&#24687;&#23398;&#30340;&#30452;&#35266;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22312;&#25105;&#20204;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#36825;&#31181;&#19981;&#23545;&#40784;&#30340;&#26681;&#28304;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#19968;&#31181;&#23454;&#38469;&#23454;&#26045;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, we naturally apply an Observation-Oriented principle, in which observational variables preexist and set the stage for constructing relationships. While sufficient for traditional models, the integration of AI with big data exposes the misalignment between the observational models and our actual comprehension. Contrarily, humans shape cognitive entities defined by relationships, enabling us to formulate knowledge across temporal and hyper-dimensional spaces, rather than being confined to observational constructs. From an innovative Relation-Oriented perspective, this study examines the roots of this misalignment within our current modeling paradigm, illuminated by intuitive examples from computer vision and health informatics. We also introduce the relation-defined representation learning methodology as a practical implementation of Relation-Oriented modeling, supported by extensive experimental validation.
&lt;/p&gt;</description></item><item><title>BOURNE&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#21161;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32479;&#19968;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#20811;&#26381;&#33410;&#28857;&#21644;&#36793;&#24322;&#24120;&#26816;&#27979;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#23616;&#38480;&#20197;&#21450;&#36127;&#23545;&#37319;&#26679;&#24102;&#26469;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15244</link><description>&lt;p&gt;
BOURNE: &#22522;&#20110;&#33258;&#21161;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32479;&#19968;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BOURNE: Bootstrapped Self-supervised Learning Framework for Unified Graph Anomaly Detection. (arXiv:2307.15244v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15244
&lt;/p&gt;
&lt;p&gt;
BOURNE&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#21161;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32479;&#19968;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#20811;&#26381;&#33410;&#28857;&#21644;&#36793;&#24322;&#24120;&#26816;&#27979;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#23616;&#38480;&#20197;&#21450;&#36127;&#23545;&#37319;&#26679;&#24102;&#26469;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#22312;&#31038;&#20132;&#32593;&#32476;&#12289;&#37329;&#34701;&#39118;&#38505;&#31649;&#29702;&#21644;&#27969;&#37327;&#20998;&#26512;&#31561;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#22270;&#24322;&#24120;&#26816;&#27979; (GAD) &#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;GAD&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#34987;&#26816;&#27979;&#30340;&#22270;&#23545;&#35937;&#30340;&#31867;&#22411;&#23558;&#20854;&#20998;&#31867;&#20026;&#33410;&#28857;&#21644;&#36793;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#33410;&#28857;&#21644;&#36793;&#24322;&#24120;&#35270;&#20026;&#29420;&#31435;&#30340;&#20219;&#21153;&#65292;&#24573;&#35270;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#20851;&#32852;&#21644;&#39057;&#32321;&#20849;&#29616;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26080;&#27861;&#21033;&#29992;&#33410;&#28857;&#21644;&#36793;&#24322;&#24120;&#25552;&#20379;&#30340;&#20114;&#30456;&#26816;&#27979;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#26368;&#20808;&#36827;&#30340;GAD&#26041;&#27861;&#65292;&#22914;CoLA&#21644;SL-GAD&#65292;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#20005;&#37325;&#20381;&#36182;&#36127;&#23545;&#37319;&#26679;&#65292;&#36825;&#23548;&#33268;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21161;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#32479;&#19968;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;BOURNE&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection (GAD) has gained increasing attention in recent years due to its critical application in a wide range of domains, such as social networks, financial risk management, and traffic analysis. Existing GAD methods can be categorized into node and edge anomaly detection models based on the type of graph objects being detected. However, these methods typically treat node and edge anomalies as separate tasks, overlooking their associations and frequent co-occurrences in real-world graphs. As a result, they fail to leverage the complementary information provided by node and edge anomalies for mutual detection. Additionally, state-of-the-art GAD methods, such as CoLA and SL-GAD, heavily rely on negative pair sampling in contrastive learning, which incurs high computational costs, hindering their scalability to large graphs. To address these limitations, we propose a novel unified graph anomaly detection framework based on bootstrapped self-supervised learning (named BOURN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26377;&#24207;&#29305;&#24449;&#36923;&#36753;&#25512;&#24191;&#21040;&#27169;&#31946;&#35774;&#32622;&#20013;&#65292;&#24182;&#32473;&#20986;&#20102;&#27169;&#31946;&#30340;OSF&#36923;&#36753;&#35821;&#20041;&#12290;&#36890;&#36807;&#25193;&#23637;&#21253;&#21547;&#20851;&#31995;&#21040;OSF&#26415;&#35821;&#65292;&#25105;&#20204;&#26500;&#25104;&#20102;&#19968;&#20010;&#27169;&#31946;&#20559;&#24207;&#12290;</title><link>http://arxiv.org/abs/2307.14669</link><description>&lt;p&gt;
&#27169;&#31946;&#26377;&#24207;&#29305;&#24449;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Fuzzy order-sorted feature logic. (arXiv:2307.14669v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26377;&#24207;&#29305;&#24449;&#36923;&#36753;&#25512;&#24191;&#21040;&#27169;&#31946;&#35774;&#32622;&#20013;&#65292;&#24182;&#32473;&#20986;&#20102;&#27169;&#31946;&#30340;OSF&#36923;&#36753;&#35821;&#20041;&#12290;&#36890;&#36807;&#25193;&#23637;&#21253;&#21547;&#20851;&#31995;&#21040;OSF&#26415;&#35821;&#65292;&#25105;&#20204;&#26500;&#25104;&#20102;&#19968;&#20010;&#27169;&#31946;&#20559;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#24207;&#29305;&#24449;&#65288;OSF&#65289;&#36923;&#36753;&#26159;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#34920;&#31034;&#29305;&#24449;&#31526;&#21495;&#21644;&#38598;&#21512;&#34920;&#31034;&#25490;&#24207;&#31526;&#21495;&#30340;&#30693;&#35782;&#34920;&#31034;&#21644;&#25512;&#29702;&#35821;&#35328;&#65292;&#36825;&#20123;&#31526;&#21495;&#22312;&#19968;&#20010;&#21253;&#21547;&#20851;&#31995;&#26684;&#20013;&#25490;&#21015;&#12290; OSF&#36923;&#36753;&#20801;&#35768;&#26500;&#24314;&#31867;&#20284;&#35760;&#24405;&#30340;&#26415;&#35821;&#65292;&#34920;&#31034;&#23454;&#20307;&#31867;&#21035;&#65292;&#24182;&#19988;&#36825;&#20123;&#26415;&#35821;&#26412;&#36523;&#20063;&#25353;&#29031;&#21253;&#21547;&#20851;&#31995;&#25490;&#24207;&#12290;&#36825;&#31181;&#32467;&#26500;&#30340;&#19968;&#33268;&#24615;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31867;&#22411;&#21253;&#21547;&#28436;&#31639;&#65292;&#24050;&#32463;&#24212;&#29992;&#20110;&#35745;&#31639;&#35821;&#35328;&#23398;&#65292;&#24182;&#22312;&#32422;&#26463;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#65288;&#22914;LOGIN&#21644;LIFE&#65289;&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#65288;&#22914;CEDAR&#65289;&#20013;&#23454;&#29616;&#12290;&#26412;&#25991;&#23558;OSF&#36923;&#36753;&#25512;&#24191;&#21040;&#27169;&#31946;&#35774;&#32622;&#20013;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#27169;&#31946;&#21253;&#21547;&#20851;&#31995;&#30340;&#23450;&#20041;&#65292;&#23427;&#25512;&#24191;&#20102;Zadeh&#30340;&#27169;&#31946;&#38598;&#21253;&#21547;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20010;&#23450;&#20041;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#27169;&#31946;&#30340;OSF&#36923;&#36753;&#35821;&#20041;&#65292;&#20854;&#20013;&#25490;&#24207;&#31526;&#21495;&#21644;OSF&#26415;&#35821;&#34920;&#31034;&#27169;&#31946;&#38598;&#21512;&#12290;&#25105;&#20204;&#23558;&#21253;&#21547;&#20851;&#31995;&#25193;&#23637;&#21040;OSF&#26415;&#35821;&#65292;&#24182;&#35777;&#26126;&#23427;&#26500;&#25104;&#20102;&#19968;&#20010;&#27169;&#31946;&#20559;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Order-Sorted Feature (OSF) logic is a knowledge representation and reasoning language based on function-denoting feature symbols and set-denoting sort symbols ordered in a subsumption lattice. OSF logic allows the construction of record-like terms that represent classes of entities and that are themselves ordered in a subsumption relation. The unification algorithm for such structures provides an efficient calculus of type subsumption, which has been applied in computational linguistics and implemented in constraint logic programming languages such as LOGIN and LIFE and automated reasoners such as CEDAR. This work generalizes OSF logic to a fuzzy setting. We give a flexible definition of a fuzzy subsumption relation which generalizes Zadeh's inclusion between fuzzy sets. Based on this definition we define a fuzzy semantics of OSF logic where sort symbols and OSF terms denote fuzzy sets. We extend the subsumption relation to OSF terms and prove that it constitutes a fuzzy partial order 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22806;&#35266;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#22270;&#34955;&#65288;Bag-of-Views&#65289;&#27169;&#22411;&#26469;&#23545;&#25429;&#33719;&#30340;&#35270;&#22270;&#36827;&#34892;&#31163;&#32447;&#25968;&#25454;&#38598;&#32454;&#21270;&#21644;&#22312;&#32447;&#19979;&#19968;&#20010;&#26368;&#20339;&#35270;&#22270;&#65288;NBV&#65289;&#35268;&#21010;&#24212;&#29992;&#20998;&#37197;&#25928;&#29992;&#65292;&#20197;&#23454;&#29616;3D&#37325;&#24314;&#30340;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#35270;&#22270;&#35268;&#21010;&#24037;&#20855;&#31665;&#65288;VPT&#65289;&#12290;</title><link>http://arxiv.org/abs/2307.05832</link><description>&lt;p&gt;
&#35270;&#22270;&#34955;&#65306;&#19968;&#31181;&#22522;&#20110;&#22806;&#35266;&#30340;&#29992;&#20110;3D&#37325;&#24314;&#19979;&#19968;&#20010;&#26368;&#20339;&#35270;&#22270;&#35268;&#21010;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bag of Views: An Appearance-based Approach to Next-Best-View Planning for 3D Reconstruction. (arXiv:2307.05832v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05832
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22806;&#35266;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#22270;&#34955;&#65288;Bag-of-Views&#65289;&#27169;&#22411;&#26469;&#23545;&#25429;&#33719;&#30340;&#35270;&#22270;&#36827;&#34892;&#31163;&#32447;&#25968;&#25454;&#38598;&#32454;&#21270;&#21644;&#22312;&#32447;&#19979;&#19968;&#20010;&#26368;&#20339;&#35270;&#22270;&#65288;NBV&#65289;&#35268;&#21010;&#24212;&#29992;&#20998;&#37197;&#25928;&#29992;&#65292;&#20197;&#23454;&#29616;3D&#37325;&#24314;&#30340;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#35270;&#22270;&#35268;&#21010;&#24037;&#20855;&#31665;&#65288;VPT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26080;&#20154;&#26426;&#30340;&#26234;&#33021;&#25968;&#25454;&#37319;&#38598;&#29992;&#20110;3D&#37325;&#24314;&#21644;&#22522;&#30784;&#35774;&#26045;&#30417;&#27979;&#65292;&#30001;&#20110;&#22270;&#20687;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#27491;&#32463;&#21382;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#35270;&#22270;&#35268;&#21010;&#26159;&#36825;&#20010;&#20219;&#21153;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#23427;&#20915;&#23450;&#20102;&#20449;&#24687;&#25429;&#33719;&#31574;&#30053;&#65292;&#24182;&#19988;&#20005;&#37325;&#24433;&#21709;&#20174;&#25429;&#33719;&#30340;&#25968;&#25454;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#25110;&#30446;&#26631;&#30340;&#37096;&#20998;&#37325;&#24314;&#26469;&#23454;&#29616;&#20027;&#21160;&#37325;&#24314;&#30340;&#35270;&#22270;&#35268;&#21010;&#65307;&#21069;&#19968;&#31181;&#26041;&#27861;&#23545;&#20110;&#22797;&#26434;&#25110;&#26032;&#35782;&#21035;&#30340;&#30446;&#26631;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#32780;&#21518;&#32773;&#35745;&#31639;&#24320;&#38144;&#24456;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35270;&#22270;&#34955;&#65288;BoV&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#22806;&#35266;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20026;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#32454;&#21270;&#21644;&#22312;&#32447;&#19979;&#19968;&#20010;&#26368;&#20339;&#35270;&#22270;&#65288;NBV&#65289;&#35268;&#21010;&#24212;&#29992;&#20998;&#37197;&#25928;&#29992;&#65292;&#20197;&#23454;&#29616;3D&#37325;&#24314;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#36825;&#20010;&#24037;&#20316;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#35270;&#22270;&#35268;&#21010;&#24037;&#20855;&#31665;&#65288;VPT&#65289;&#65292;
&lt;/p&gt;
&lt;p&gt;
UAV-based intelligent data acquisition for 3D reconstruction and monitoring of infrastructure has been experiencing an increasing surge of interest due to the recent advancements in image processing and deep learning-based techniques. View planning is an essential part of this task that dictates the information capture strategy and heavily impacts the quality of the 3D model generated from the captured data. Recent methods have used prior knowledge or partial reconstruction of the target to accomplish view planning for active reconstruction; the former approach poses a challenge for complex or newly identified targets while the latter is computationally expensive. In this work, we present Bag-of-Views (BoV), a fully appearance-based model used to assign utility to the captured views for both offline dataset refinement and online next-best-view (NBV) planning applications targeting the task of 3D reconstruction. With this contribution, we also developed the View Planning Toolbox (VPT), 
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.04962</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#32593;&#32476;&#29702;&#35770;&#36827;&#34892;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04962
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#22312;&#39537;&#21160;&#30340;&#25506;&#32034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#29992;&#36884;&#65292;&#21363;&#20351;&#27809;&#26377;&#39069;&#22806;&#30340;&#22806;&#22312;&#22870;&#21169;&#12290;&#24403;&#29615;&#22659;&#33258;&#28982;&#34920;&#31034;&#20026;&#22270;&#26102;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#24341;&#23548;&#25506;&#32034;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65306;&#20449;&#24687;&#24046;&#29702;&#35770;&#21644;&#21387;&#32553;&#36827;&#23637;&#29702;&#35770;&#65292;&#26469;&#28608;&#21169;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#25506;&#32034;&#12290;&#36825;&#20123;&#29702;&#35770;&#23558;&#22909;&#22855;&#24515;&#35270;&#20026;&#23545;&#29615;&#22659;&#20013;&#35775;&#38382;&#33410;&#28857;&#25152;&#24341;&#21457;&#30340;&#23376;&#22270;&#30340;&#25299;&#25169;&#29305;&#24449;&#36827;&#34892;&#20248;&#21270;&#30340;&#20869;&#22312;&#21160;&#26426;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25552;&#20986;&#30340;&#29305;&#24449;&#20316;&#20026;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#22870;&#21169;&#12290;&#22312;&#22810;&#20010;&#31867;&#21035;&#30340;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#20195;&#29702;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#21644;&#27604;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#38271;&#30340;&#25506;&#32034;&#24615;&#27493;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#20110;&#30456;&#20851;&#25299;&#25169;&#23646;&#24615;&#30340;&#36138;&#23146;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#20869;&#22312;&#21160;&#26426;&#20135;&#29983;&#30340;&#22870;&#21169;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#29983;&#25104;&#19978;&#25512;&#24191;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#33021;&#22815;&#22312;&#26356;&#22823;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated exploration has proven useful for reinforcement learning, even without additional extrinsic rewards. When the environment is naturally represented as a graph, how to guide exploration best remains an open question. In this work, we propose a novel approach for exploring graph-structured data motivated by two theories of human curiosity: the information gap theory and the compression progress theory. The theories view curiosity as an intrinsic motivation to optimize for topological features of subgraphs induced by the visited nodes in the environment. We use these proposed features as rewards for graph neural-network-based reinforcement learning. On multiple classes of synthetically generated graphs, we find that trained agents generalize to larger environments and to longer exploratory walks than are seen during training. Our method computes more efficiently than the greedy evaluation of the relevant topological properties. The proposed intrinsic motivations bea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleTTS 2&#30340;TTS&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#19982;&#20197;&#24448;&#27169;&#22411;&#19981;&#21516;&#65292;StyleTTS 2&#23558;&#26679;&#24335;&#35270;&#20026;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#22823;&#22411;SLMs&#21644;&#26032;&#30340;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2306.07691</link><description>&lt;p&gt;
StyleTTS 2&#65306;&#36890;&#36807;&#39118;&#26684;&#25193;&#25955;&#21644;&#19982;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models. (arXiv:2306.07691v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleTTS 2&#30340;TTS&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#19982;&#20197;&#24448;&#27169;&#22411;&#19981;&#21516;&#65292;StyleTTS 2&#23558;&#26679;&#24335;&#35270;&#20026;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#22823;&#22411;SLMs&#21644;&#26032;&#30340;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;StyleTTS2&#65292;&#19968;&#31181;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#39118;&#26684;&#25193;&#25955;&#21644;&#19982;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;TTS&#21512;&#25104;&#12290; StyleTTS 2&#36890;&#36807;&#23558;&#26679;&#24335;&#24314;&#27169;&#20026;&#28508;&#22312;&#30340;&#38543;&#26426;&#21464;&#37327;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#26080;&#38656;&#21442;&#32771;&#35821;&#38899;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#28508;&#22312;&#25193;&#25955;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#30340;&#22810;&#26679;&#21270;&#35821;&#38899;&#21512;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;SLMs&#65288;&#20363;&#22914;WavLM&#65289;&#20316;&#20026;&#37492;&#21035;&#22120;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#22411;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#38899;&#30340;&#33258;&#28982;&#24230;&#12290; StyleTTS 2&#22312;&#21333;&#25196;&#22768;&#22120;LJSpeech&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#20154;&#31867;&#24405;&#38899;&#65292;&#24182;&#22312;&#22810;&#25196;&#22768;&#22120;VCTK&#25968;&#25454;&#38598;&#19978;&#19982;&#20043;&#21305;&#37197;&#65292;&#32463;&#36807;&#27597;&#35821;&#20026;&#33521;&#35821;&#30340;&#20154;&#21592;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#24403;&#22312;LibriTTS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32988;&#36807;&#20102;&#20197;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#38646;&#26679;&#26412;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22521;&#35757;&#26041;&#27861;&#24433;&#21709;&#28145;&#23618;&#32593;&#32476;&#20998;&#31867;&#22120;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#25968;&#25454;&#38544;&#34255;&#34920;&#31034;&#20013;&#36798;&#21040;&#26356;&#39640;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06146</link><description>&lt;p&gt;
&#38544;&#34255;&#20998;&#31867;&#23618;&#65306;&#20851;&#20110;&#25968;&#25454;&#38544;&#34255;&#34920;&#31034;&#20013;&#26356;&#39640;&#32447;&#24615;&#21487;&#20998;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hidden Classification Layers: a study on Data Hidden Representations with a Higher Degree of Linear Separability between the Classes. (arXiv:2306.06146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22521;&#35757;&#26041;&#27861;&#24433;&#21709;&#28145;&#23618;&#32593;&#32476;&#20998;&#31867;&#22120;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#25968;&#25454;&#38544;&#34255;&#34920;&#31034;&#20013;&#36798;&#21040;&#26356;&#39640;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#20195;&#34920;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#37117;&#22522;&#20110;&#26631;&#20934;&#30340;&#22810;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#31181;&#12290;&#36825;&#20123;&#20063;&#34987;&#31216;&#20026;&#28145;&#24230;&#32593;&#32476;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#27599;&#20010;&#38544;&#34255;&#31070;&#32463;&#23618;&#23436;&#25104;&#19968;&#31181;&#25968;&#25454;&#36716;&#25442;&#65292;&#39044;&#26399;&#20351;&#25968;&#25454;&#34920;&#31034;&#8220;&#27604;&#20043;&#21069;&#26356;&#32447;&#24615;&#21487;&#20998;&#8221;&#65292;&#20197;&#33719;&#24471;&#23613;&#21487;&#33021;&#32447;&#24615;&#21487;&#20998;&#30340;&#26368;&#32456;&#25968;&#25454;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#21487;&#20197;&#25191;&#34892;&#36825;&#20123;&#36716;&#25442;&#30340;&#36866;&#24403;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22521;&#35757;&#26041;&#27861;&#23545;&#28145;&#23618;&#32593;&#32476;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#20542;&#21521;&#20110;&#20351;&#29992;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#38544;&#34255;&#23618;&#30340;&#25968;&#25454;&#34920;&#31034;&#20855;&#26377;&#26356;&#39640;&#30340;&#31867;&#20043;&#38388;&#32447;&#24615;&#21487;&#20998;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#24341;&#20837;&#20102;&#19968;&#20010;&#28041;&#21450;&#35823;&#24046;&#20989;&#25968;&#30340;&#26032;&#39062;&#22521;&#35757;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of classification problems, Deep Learning (DL) approaches represent state of art. Many DL approaches are based on variations of standard multi-layer feed-forward neural networks. These are also referred to as deep networks. The basic idea is that each hidden neural layer accomplishes a data transformation which is expected to make the data representation "somewhat more linearly separable" than the previous one to obtain a final data representation which is as linearly separable as possible. However, determining the appropriate neural network parameters that can perform these transformations is a critical problem. In this paper, we investigate the impact on deep network classifier performances of a training approach favouring solutions where data representations at the hidden layers have a higher degree of linear separability between the classes with respect to standard methods. To this aim, we propose a neural network architecture which induces an error function involvin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; Gram &#30697;&#38453;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#21644;&#23618;&#35268;&#33539;&#21270;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2305.18399</link><description>&lt;p&gt;
&#20851;&#20110;&#28608;&#27963;&#20989;&#25968;&#21644;&#35268;&#33539;&#21270;&#23545;&#21021;&#22987;&#21270;&#31561;&#36317;&#23884;&#20837;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the impact of activation and normalization in obtaining isometric embeddings at initialization. (arXiv:2305.18399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; Gram &#30697;&#38453;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#21644;&#23618;&#35268;&#33539;&#21270;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20498;&#25968;&#31532;&#20108;&#20010; Gram &#30697;&#38453;&#30340;&#32467;&#26500;&#65292;&#35813;&#30697;&#38453;&#21253;&#21547;&#19982;&#19968;&#25209;&#36755;&#20837;&#23545;&#24212;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#25104;&#23545;&#20869;&#31215;&#12290;&#22312;&#20960;&#31181;&#26550;&#26500;&#20013;&#65292;&#35266;&#23519;&#21040;&#22312;&#21021;&#22987;&#21270;&#26102;&#35813; Gram &#30697;&#38453;&#20250;&#38543;&#30528;&#28145;&#24230;&#21464;&#24471;&#36864;&#21270;&#65292;&#20174;&#32780;&#20005;&#37325;&#20943;&#32531;&#35757;&#32451;&#36895;&#24230;&#12290;&#35268;&#33539;&#21270;&#23618;&#22914;&#25209;&#22788;&#29702;&#35268;&#33539;&#21270;&#25110;&#23618;&#35268;&#33539;&#21270;&#65292;&#22312;&#38450;&#27490;&#31209;&#23849;&#28291;&#38382;&#39064;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#29616;&#26377;&#30340;&#29702;&#35770;&#32467;&#26524;&#26080;&#27861;&#20840;&#38754;&#35206;&#30422;&#24191;&#27867;&#29992;&#20110; transformer &#20013;&#30340;&#23618;&#35268;&#33539;&#21270;&#21644;&#26377;&#38480;&#28145;&#24230;&#19979;&#35268;&#33539;&#21270;&#30340;&#37327;&#21270;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#32467;&#21512;&#28608;&#27963;&#20989;&#25968;&#23618;&#20351;&#29992;&#30340;&#23618;&#35268;&#33539;&#21270;&#21487;&#20197;&#20351;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340; Gram &#30697;&#38453;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#24182;&#20351;&#29992;&#28608;&#27963;&#20989;&#25968;&#30340; Hermite &#23637;&#24320;&#26469;&#37327;&#21270;&#36825;&#20010;&#36895;&#24230;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the structure of the penultimate Gram matrix in deep neural networks, which contains the pairwise inner products of outputs corresponding to a batch of inputs. In several architectures it has been observed that this Gram matrix becomes degenerate with depth at initialization, which dramatically slows training. Normalization layers, such as batch or layer normalization, play a pivotal role in preventing the rank collapse issue. Despite promising advances, the existing theoretical results (i) do not extend to layer normalization, which is widely used in transformers, (ii) can not characterize the bias of normalization quantitatively at finite depth.  To bridge this gap, we provide a proof that layer normalization, in conjunction with activation layers, biases the Gram matrix of a multilayer perceptron towards isometry at an exponential rate with depth at initialization. We quantify this rate using the Hermite expansion of the activation function, highlighting th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GFlowNets&#30340;&#26426;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.17010</link><description>&lt;p&gt;
&#21033;&#29992;GFlowNets&#35299;&#20915;&#22270;&#24418;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Let the Flows Tell: Solving Graph Combinatorial Optimization Problems with GFlowNets. (arXiv:2305.17010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GFlowNets&#30340;&#26426;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#26159;NP&#38590;&#39064;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#31934;&#30830;&#31639;&#27861;&#65292;&#36825;&#20351;&#23427;&#20204;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#29702;&#24819;&#39046;&#22495;&#12290;&#36825;&#20123;&#38382;&#39064;&#20013;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#38480;&#21046;&#21487;&#33021;&#20250;&#30452;&#25509;&#38459;&#30861;&#20248;&#21270;&#25110;&#37319;&#26679;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;GFlowNets&#26368;&#36817;&#34987;&#21457;&#29616;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26426;&#22120;&#65292;&#21487;&#20197;&#39034;&#24207;&#22320;&#20174;&#22797;&#21512;&#38750;&#35268;&#33539;&#21270;&#23494;&#24230;&#20013;&#26377;&#25928;&#22320;&#37319;&#26679;&#65292;&#24182;&#20855;&#26377;&#22312;CO&#20013;&#20998;&#25674;&#27492;&#31867;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#36807;&#31243;&#20197;&#21450;&#29983;&#25104;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#20505;&#36873;&#39033;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#32452;&#21512;&#38382;&#39064;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#25552;&#20986;&#35757;&#32451;&#26377;&#26465;&#20214;&#30340;GFlowNets&#20174;&#35299;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#31574;&#30053;&#12290;&#36824;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#25216;&#26415;&#26469;&#21463;&#30410;&#20110;&#36828;&#31243;&#20449;&#29992;&#20998;&#37197;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#20351;&#29992;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#19981;&#21516;CO&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GFlowNet&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combinatorial optimization (CO) problems are often NP-hard and thus out of reach for exact algorithms, making them a tempting domain to apply machine learning methods. The highly structured constraints in these problems can hinder either optimization or sampling directly in the solution space. On the other hand, GFlowNets have recently emerged as a powerful machinery to efficiently sample from composite unnormalized densities sequentially and have the potential to amortize such solution-searching processes in CO, as well as generate diverse solution candidates. In this paper, we design Markov decision processes (MDPs) for different combinatorial problems and propose to train conditional GFlowNets to sample from the solution space. Efficient training techniques are also developed to benefit long-range credit assignment. Through extensive experiments on a variety of different CO tasks with synthetic and realistic data, we demonstrate that GFlowNet policies can efficiently find high-quali
&lt;/p&gt;</description></item><item><title>PDP&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.11203</link><description>&lt;p&gt;
PDP&#65306;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#21363;&#21487;&#25630;&#23450;
&lt;/p&gt;
&lt;p&gt;
PDP: Parameter-free Differentiable Pruning is All You Need. (arXiv:2305.11203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11203
&lt;/p&gt;
&lt;p&gt;
PDP&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#21098;&#26525;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#25552;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#24182;&#26368;&#23567;&#21270;DNN&#21152;&#36895;&#22120;&#19978;&#30340;&#21151;&#32791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#12289;&#26114;&#36149;&#25110;&#26080;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;/&#35821;&#35328;&#20219;&#21153;&#12289;DNN&#20307;&#31995;&#32467;&#26500;&#24182;&#36981;&#23432;&#32467;&#26500;&#21270;&#21098;&#26525;&#32422;&#26463;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;&#26102;&#38388;&#21098;&#26525;&#26041;&#26696;&#8212;&#8212;PDP&#65288;&#21442;&#25968;&#33258;&#30001;&#21487;&#24494;&#21098;&#26525;&#65289;&#65292;&#23427;&#22312;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;PDP&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26435;&#37325;&#30340;&#21160;&#24577;&#20989;&#25968;&#65292;&#20197;&#21442;&#25968;&#26080;&#20851;&#30340;&#26041;&#24335;&#20026;&#32473;&#23450;&#30340;&#21098;&#26525;&#30446;&#26631;&#29983;&#25104;&#36719;&#21098;&#26525;&#25513;&#30721;&#12290;&#34429;&#28982;&#26159;&#21487;&#24494;&#30340;&#65292;&#20294;&#26159;PDP&#30340;&#31616;&#21333;&#21644;&#39640;&#25928;&#20351;&#20854;&#36275;&#22815;&#26222;&#36941;&#65292;&#20197;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#38543;&#26426;/&#32467;&#26500;&#21270;/&#36890;&#36947;&#21098;&#26525;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;MobileNet-v1&#65292;PDP&#21487;&#20197;&#22312;86.6%&#30340;&#31232;&#30095;&#24230;&#19979;&#36798;&#21040;68.2%&#30340;ImageNet1k top-1&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNN pruning is a popular way to reduce the size of a model, improve the inference latency, and minimize the power consumption on DNN accelerators. However, existing approaches might be too complex, expensive or ineffective to apply to a variety of vision/language tasks, DNN architectures and to honor structured pruning constraints. In this paper, we propose an efficient yet effective train-time pruning scheme, Parameter-free Differentiable Pruning (PDP), which offers state-of-the-art qualities in model size, accuracy, and training cost. PDP uses a dynamic function of weights during training to generate soft pruning masks for the weights in a parameter-free manner for a given pruning target. While differentiable, the simplicity and efficiency of PDP make it universal enough to deliver state-of-the-art random/structured/channel pruning results on various vision and natural language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1 ImageNet1k accuracy at 86.6% sparsity, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARQ-Learning&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#38469;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#21516;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#32780;&#19988;&#65292;&#26412;&#25991;&#36824;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;PR-DQN&#21644;PR-DDPG&#12290;</title><link>http://arxiv.org/abs/2305.06657</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65306;&#30456;&#37051;&#19981;&#30830;&#23450;&#24615;&#38598;&#21644;&#21452;&#20195;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm. (arXiv:2305.06657v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARQ-Learning&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#38469;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#21516;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#32780;&#19988;&#65292;&#26412;&#25991;&#36824;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;PR-DQN&#21644;PR-DDPG&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#38598;&#19978;&#20248;&#21270;&#26368;&#24046;&#24615;&#33021;&#12290;&#32473;&#23450;&#19968;&#20010;&#20135;&#29983;&#35757;&#32451;&#26679;&#26412;&#30340;&#26631;&#20934;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;N-MDP&#65289;&#65292;&#35813;&#38598;&#21512;&#21253;&#21547;&#36890;&#36807;&#23545;N-MDP&#36827;&#34892;&#26576;&#20123;&#25200;&#21160;&#32780;&#33719;&#24471;&#30340;MDP&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#27604;&#29616;&#26377;&#38598;&#21512;&#26356;&#23454;&#38469;&#30340;MDP&#12290;&#20351;&#29992;&#36825;&#20010;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;RL&#31639;&#27861;&#65292;&#21517;&#20026;ARQ-Learning&#65292;&#29992;&#20110;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#35823;&#24046;&#30028;&#24182;&#35777;&#26126;&#23427;&#19982;Q-Learning&#21644;&#40065;&#26834;Q-Learning&#65288;&#21363;&#29616;&#26377;&#30340;&#40065;&#26834;RL&#26041;&#27861;&#65289;&#19968;&#26679;&#24555;&#22320;&#25910;&#25947;&#65292;&#21516;&#26102;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#23558;ARQ-Learning&#25193;&#23637;&#21040;&#22823;&#22411;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;&#20851;&#38190;&#29942;&#39048;&#12290;&#21033;&#29992;&#36825;&#19968;&#25216;&#26415;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;PRQ-Learning&#12290;&#25509;&#30528;&#65292;&#23558;&#20854;&#19982;DQN&#21644;DDPG&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#20998;&#21035;&#24320;&#21457;&#20102;PR-DQN&#21644;PR-DDPG&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#39046;&#22495;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust reinforcement learning (RL) aims at learning a policy that optimizes the worst-case performance over an uncertainty set. Given nominal Markov decision process (N-MDP) that generates samples for training, the set contains MDPs obtained by some perturbations from N-MDP. In this paper, we introduce a new uncertainty set containing more realistic MDPs in practice than the existing sets. Using this uncertainty set, we present a robust RL, named ARQ-Learning, for tabular cases. Also, we characterize the finite-time error bounds and prove that it converges as fast as Q-Learning and robust Q-Learning (i.e., the state-of-the-art robust RL method) while providing better robustness for real applications. We propose {\em pessimistic agent} that efficiently tackles the key bottleneck for the extension of ARQ-Learning into large or continuous state spaces. Using this technique, we first propose PRQ-Learning. To the next, combining this with DQN and DDPG, we develop PR-DQN and PR-DDPG, respect
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;&#65288;DBC&#65289;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#20998;&#24067;&#30340;&#26465;&#20214;&#21644;&#32852;&#21512;&#27010;&#29575;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#24314;&#27169;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13335</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model-Augmented Behavioral Cloning. (arXiv:2302.13335v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;&#65288;DBC&#65289;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#20998;&#24067;&#30340;&#26465;&#20214;&#21644;&#32852;&#21512;&#27010;&#29575;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#24314;&#27169;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#35299;&#20915;&#20102;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#28436;&#31034;&#32780;&#27809;&#26377;&#35775;&#38382;&#29615;&#22659;&#22870;&#21169;&#20449;&#21495;&#30340;&#23398;&#20064;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#19981;&#38656;&#35201;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#35201;&#20040;&#23558;&#19987;&#23478;&#20998;&#24067;&#24314;&#27169;&#20026;&#26465;&#20214;&#27010;&#29575;p(a|s)&#65288;&#20363;&#22914;&#65292;&#34892;&#20026;&#20811;&#38534;&#65292;BC&#65289;&#65292;&#35201;&#20040;&#23558;&#32852;&#21512;&#27010;&#29575;p(s,a)&#24314;&#27169;&#65288;&#20363;&#22914;&#65292;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#65289;&#12290;&#23613;&#31649;&#34892;&#20026;&#20811;&#38534;&#23545;&#20110;&#24314;&#27169;&#26465;&#20214;&#27010;&#29575;&#30340;&#31616;&#21333;&#24615;&#65292;&#20294;&#36890;&#24120;&#38590;&#20197;&#27867;&#21270;&#12290;&#34429;&#28982;&#23545;&#32852;&#21512;&#27010;&#29575;&#36827;&#34892;&#24314;&#27169;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#25512;&#29702;&#36807;&#31243;&#21487;&#33021;&#32791;&#26102;&#65292;&#24182;&#19988;&#24448;&#24448;&#20250;&#36973;&#21463;&#27969;&#24418;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20174;&#24314;&#27169;&#19987;&#23478;&#20998;&#24067;&#30340;&#26465;&#20214;&#21644;&#32852;&#21512;&#27010;&#29575;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;&#65288;DBC&#65289;&#37319;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#19987;&#23478;&#34892;&#20026;&#65292;&#24182;&#23398;&#20064;&#19968;&#31181;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#26681;&#25454;&#28151;&#21512;&#27010;&#29575;&#20998;&#24067;&#37319;&#26679;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning addresses the challenge of learning by observing an expert's demonstrations without access to reward signals from environments. Most existing imitation learning methods that do not require interacting with environments either model the expert distribution as the conditional probability p(a|s) (e.g., behavioral cloning, BC) or the joint probability p(s, a) (e.g., implicit behavioral cloning). Despite its simplicity, modeling the conditional probability with BC usually struggles with generalization. While modeling the joint probability can lead to improved generalization performance, the inference procedure can be time-consuming and it often suffers from manifold overfitting. This work proposes an imitation learning framework that benefits from modeling both the conditional and joint probability of the expert distribution. Our proposed diffusion model-augmented behavioral cloning (DBC) employs a diffusion model trained to model expert behaviors and learns a policy to o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.11552</link><description>&lt;p&gt;
&#20943;&#23569;&#12289;&#37325;&#22797;&#21033;&#29992;&#12289;&#22238;&#25910;&#65306;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC. (arXiv:2302.11552v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11552
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#25193;&#25955;&#27169;&#22411;&#38382;&#19990;&#20197;&#26469;&#65292;&#23427;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#32463;&#36805;&#36895;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#23427;&#20204;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#23398;&#20064;&#19968;&#31995;&#21015;&#26102;&#21464;&#30340;&#23545;&#25968;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#36825;&#31181;&#35299;&#37322;&#24050;&#32463;&#28608;&#21457;&#20102;&#22522;&#20110;&#20998;&#31867;&#22120;&#21644;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#24605;&#24819;&#25104;&#20026;&#21518;&#32493;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#36825;&#20123;&#24819;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#25968;-based&#35299;&#37322;&#65292;&#25506;&#32034;&#20102;&#29992;&#20110;&#28041;&#21450;&#32452;&#21512;&#29983;&#25104;&#21644;&#25351;&#23548;&#30340;&#26465;&#20214;&#12289;&#20462;&#25913;&#21644;&#37325;&#22797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20026;&#20160;&#20040;&#26576;&#20123;&#31867;&#22411;&#30340;&#32452;&#21512;&#20351;&#29992;&#24403;&#21069;&#25216;&#26415;&#22833;&#36133;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#37319;&#26679;&#32773;(&#32780;&#19981;&#26159;&#27169;&#22411;)&#23545;&#27492;&#22833;&#36133;&#36127;&#26377;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#37319;&#26679;&#22120;&#65292;&#21463;MCMC&#30340;&#21551;&#21457;&#65292;&#20351;&#32452;&#21512;&#29983;&#25104;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#26356;&#21152;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;Prompt Tuning&#65288;MPT&#65289;&#22914;&#20309;&#24110;&#21161;&#25913;&#21892;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#20351;&#29992;&#20803;&#23398;&#20064;&#21487;&#20197;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#20013;&#23398;&#20064;&#21021;&#22987;&#21270;Prompt&#23884;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#39564;&#20102;&#20195;&#34920;&#24615;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#22823;&#37327;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;MPT&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2302.08143</link><description>&lt;p&gt;
&#23398;&#20064;&#21021;&#22987;&#21270;&#65306;&#20803;&#23398;&#20064;&#33021;&#21542;&#25552;&#39640;Prompt Tuning&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?. (arXiv:2302.08143v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;Prompt Tuning&#65288;MPT&#65289;&#22914;&#20309;&#24110;&#21161;&#25913;&#21892;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#20351;&#29992;&#20803;&#23398;&#20064;&#21487;&#20197;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#20013;&#23398;&#20064;&#21021;&#22987;&#21270;Prompt&#23884;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#39564;&#20102;&#20195;&#34920;&#24615;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#22823;&#37327;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;MPT&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Tuning (PT)&#26159;&#19968;&#31181;&#21482;&#35843;&#25972;&#27599;&#20010;&#20219;&#21153;&#30340;&#19968;&#20010;&#39069;&#22806;&#26631;&#35760;&#24207;&#21015;&#30340;&#23884;&#20837;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#23436;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#19981;&#21464;&#65292;&#24050;&#32463;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;PT&#24050;&#32463;&#34987;&#35777;&#26126;&#26497;&#22823;&#22320;&#20381;&#36182;&#20110;&#24456;&#22909;&#30340;Prompt&#23884;&#20837;&#30340;&#21021;&#22987;&#21270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;Prompt Tuning (MPT) &#26469;&#31995;&#32479;&#22320;&#25506;&#32034;&#20803;&#23398;&#20064;&#22914;&#20309;&#24110;&#21161;&#36890;&#36807;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#23398;&#20064;&#21021;&#22987;&#21270;Prompt&#23884;&#20837;&#26469;&#25913;&#21892;&#65288;&#22914;&#26524;&#21487;&#20197;&#65289;PT&#20013;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#19978;&#20351;&#29992;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#26469;&#32463;&#39564;&#20998;&#26512;&#20102;&#19968;&#31995;&#21015;&#20195;&#34920;&#24615;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#20998;&#26512;&#19981;&#21516;&#28304;/&#30446;&#26631;&#20219;&#21153;&#37197;&#32622;&#19979;&#30340;&#21508;&#31181;&#35843;&#25972;&#35774;&#32622;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MPT&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#23427;&#29305;&#21035;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#25552;&#21319;&#26159;&#26174;&#33879;&#30340;&#12290;&#23545;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#38382;&#39064;&#22238;&#31572;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#34429;&#28982;MPT&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;PT&#65292;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT) which only tunes the embeddings of an additional sequence of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown remarkable performance in few-shot learning. Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings. In this work, we study meta prompt tuning (MPT) to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks. We empirically analyze a representative set of meta learning algorithms in a wide range of adaptation settings with different source/target task configurations on a large set of few-shot tasks. With extensive experiments and analysis, we demonstrate the effectiveness of MPT. We find the improvement to be significant particularly on classification tasks. For other kinds of tasks such as question answering, we observe that while MPT can outperform PT in most case
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22270;&#23398;&#20064;&#30340;&#21457;&#23637;&#21382;&#31243;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#34920;&#31034;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#31561;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23545;&#20197;&#21069;&#26377;&#20215;&#20540;&#30340;&#24037;&#20316;&#36827;&#34892;&#35843;&#26597;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2212.08966</link><description>&lt;p&gt;
&#22270;&#23398;&#20064;&#21450;&#20854;&#24212;&#29992;&#65306;&#19968;&#31687;&#20840;&#38754;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Learning and Its Applications: A Holistic Survey. (arXiv:2212.08966v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22270;&#23398;&#20064;&#30340;&#21457;&#23637;&#21382;&#31243;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#34920;&#31034;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#31561;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23545;&#20197;&#21069;&#26377;&#20215;&#20540;&#30340;&#24037;&#20316;&#36827;&#34892;&#35843;&#26597;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive survey of the development and application scenarios of graph learning, with a focus on the remarkable performance of representation learning in various fields such as text, image, chemistry, and biology. It also points out the need to investigate previous valuable works.
&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#23398;&#20064;&#33410;&#28857;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#36825;&#20123;&#20851;&#31995;&#20351;&#24471;&#22270;&#19982;&#20256;&#32479;&#30340;&#34920;&#26684;&#25968;&#25454;&#30456;&#27604;&#20855;&#26377;&#29420;&#29305;&#24615;&#65292;&#22240;&#20026;&#33410;&#28857;&#20381;&#36182;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65292;&#24182;&#21253;&#21547;&#20016;&#23500;&#30340;&#20449;&#24687;&#21487;&#20379;&#21033;&#29992;&#12290;&#38543;&#30528;&#34920;&#31034;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#22270;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#31561;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#22270;&#23398;&#20064;&#21560;&#24341;&#20102;&#23398;&#26415;&#30028;&#30340;&#22823;&#37327;&#20851;&#27880;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#24037;&#20316;&#25552;&#20986;&#20102;&#35299;&#20915;&#22270;&#23398;&#20064;&#20013;&#19981;&#21516;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20294;&#38656;&#35201;&#23545;&#20197;&#21069;&#26377;&#20215;&#20540;&#30340;&#24037;&#20316;&#36827;&#34892;&#35843;&#26597;&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24847;&#35782;&#21040;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#24182;&#22312;&#22270;&#23398;&#20064;&#26041;&#38754;&#23436;&#25104;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35843;&#26597;&#65292;&#20294;&#20182;&#20204;&#26410;&#33021;&#20197;&#26356;&#36830;&#36143;&#30340;&#26041;&#24335;&#36830;&#25509;&#30456;&#20851;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph learning is a prevalent domain that endeavors to learn the intricate relationships among nodes and the topological structure of graphs. These relationships endow graphs with uniqueness compared to conventional tabular data, as nodes rely on non-Euclidean space and encompass rich information to exploit. Over the years, graph learning has transcended from graph theory to graph data mining. With the advent of representation learning, it has attained remarkable performance in diverse scenarios, including text, image, chemistry, and biology. Owing to its extensive application prospects, graph learning attracts copious attention from the academic community. Despite numerous works proposed to tackle different problems in graph learning, there is a demand to survey previous valuable works. While some researchers have perceived this phenomenon and accomplished impressive surveys on graph learning, they failed to connect related objectives, methods, and applications in a more coherent way.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;&#20102;&#25903;&#25345;&#19981;&#26029;&#21457;&#23637;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25512;&#29702;&#20195;&#29702;&#20154;&#36827;&#34892;&#30340;&#20449;&#24687;&#20256;&#36755;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#29366;&#24577;&#21644;&#20854;&#21160;&#24577;&#30340;&#20195;&#25968;&#25551;&#36848;&#65292;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#21644;&#32452;&#21512;&#30693;&#35782;&#29366;&#24577;&#65292;&#20197;&#34920;&#31034;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2110.11482</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#20030;&#25514;&#20013;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#34920;&#36798;&#21450;&#20854;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Representations of epistemic uncertainty and its perception in data-driven initiatives. (arXiv:2110.11482v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#20102;&#25903;&#25345;&#19981;&#26029;&#21457;&#23637;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25512;&#29702;&#20195;&#29702;&#20154;&#36827;&#34892;&#30340;&#20449;&#24687;&#20256;&#36755;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#29366;&#24577;&#21644;&#20854;&#21160;&#24577;&#30340;&#20195;&#25968;&#25551;&#36848;&#65292;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#21644;&#32452;&#21512;&#30693;&#35782;&#29366;&#24577;&#65292;&#20197;&#34920;&#31034;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#25512;&#21160;&#30340;&#26032;&#20852;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#27491;&#22312;&#37325;&#22609;&#20915;&#31574;&#36807;&#31243;&#65292;&#36828;&#31163;&#23545;&#30452;&#25509;&#25968;&#25454;&#20132;&#20114;&#30340;&#20256;&#32479;&#20381;&#36182;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#24341;&#20837;&#20102;&#35780;&#20272;&#25968;&#25454;&#39537;&#21160;&#20030;&#25514;&#24433;&#21709;&#30340;&#26032;&#25361;&#25112;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20123;&#19981;&#26029;&#21457;&#23637;&#30340;&#26041;&#27861;&#35770;&#65292;&#36843;&#20999;&#38656;&#35201;&#26032;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25551;&#36848;&#28304;&#20110;&#26377;&#38480;&#25968;&#25454;&#21487;&#35266;&#27979;&#24615;&#20197;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#20915;&#31574;&#20013;&#30340;&#27495;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#27169;&#22411;&#65292;&#26088;&#22312;&#22788;&#29702;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25512;&#29702;&#20195;&#29702;&#20154;&#36827;&#34892;&#20449;&#24687;&#20256;&#36755;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20511;&#37492;&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#39537;&#21160;&#20030;&#25514;&#20135;&#29983;&#30340;&#20215;&#20540;&#30340;&#22810;&#32500;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#30693;&#35782;&#29366;&#24577;&#21450;&#20854;&#21160;&#24577;&#30340;&#20195;&#25968;&#25551;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36171;&#20104;&#25105;&#20204;&#30340;&#27169;&#22411;&#19968;&#31181;&#24418;&#24335;&#21270;&#32467;&#26500;&#65292;&#29992;&#20110;&#27604;&#36739;&#21644;&#32452;&#21512;&#30693;&#35782;&#29366;&#24577;&#65307;&#36890;&#36807;&#36825;&#20123;&#32452;&#21512;&#26469;&#34920;&#31034;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging data-driven strategies, powered by the advent of AI, are reshaping decision-making processes, moving away from traditional reliance on direct data interaction. This paradigm shift introduces new challenges in assessing the impact of data-driven initiatives. To support these evolving methodologies, there is a crucial need for new models capable of describing the uncertainties stemming from limited data observability and the resulting ambiguities in decision-making. This contribution presents a novel conceptual model designed to deal with uncertainty in knowledge representations and reasoning about information transfer mediated by agents. Drawing from the multidimensional frameworks currently adopted to assess the value generated in data-driven initiatives, we provide an algebraic description of knowledge states and their dynamics. Specifically, we endow our model with a formal structure to compare and combine knowledge states; an update is represented through these combinations
&lt;/p&gt;</description></item></channel></rss>