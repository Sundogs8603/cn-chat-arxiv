<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;E(2)-&#23545;&#31216;&#22270;&#35268;&#21010;&#29992;&#20110;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#21644;&#24320;&#21457;&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#31283;&#23450;&#21644;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.13043</link><description>&lt;p&gt;
E(2)-&#23545;&#31216;&#22270;&#35268;&#21010;&#29992;&#20110;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
E(2)-Equivariant Graph Planning for Navigation. (arXiv:2309.13043v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;E(2)-&#23545;&#31216;&#22270;&#35268;&#21010;&#29992;&#20110;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#21644;&#24320;&#21457;&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#31283;&#23450;&#21644;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#23398;&#20064;&#26159;&#19968;&#39033;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#21644;&#26114;&#36149;&#24615;&#38656;&#35201;&#39640;&#25928;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20108;&#32500;&#23548;&#33322;&#35268;&#21010;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#65292;&#35813;&#23545;&#31216;&#24615;&#28304;&#20110;&#21442;&#32771;&#26694;&#26550;&#20043;&#38388;&#30340;&#27431;&#20960;&#37324;&#24471;&#21464;&#25442;&#65292;&#24182;&#23454;&#29616;&#21442;&#25968;&#20849;&#20139;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#23548;&#33322;&#38382;&#39064;&#35268;&#21010;&#20026;&#22312;&#20960;&#20309;&#22270;&#19978;&#30340;&#35268;&#21010;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#26469;&#25191;&#34892;&#20215;&#20540;&#36845;&#20195;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22788;&#29702;&#22810;&#25668;&#20687;&#22836;&#36755;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#31561;&#21464;&#23618;&#23558;&#29305;&#24449;&#25552;&#21319;&#21040;&#25152;&#38656;&#31354;&#38388;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#12289;&#24050;&#30693;&#21644;&#26410;&#30693;&#22320;&#22270;&#20197;&#21450;&#32473;&#23450;&#28857;&#30446;&#26631;&#25110;&#35821;&#20041;&#30446;&#26631;&#30340;&#20116;&#20010;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35757;&#32451;&#25928;&#29575;&#12289;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning for robot navigation presents a critical and challenging task. The scarcity and costliness of real-world datasets necessitate efficient learning approaches. In this letter, we exploit Euclidean symmetry in planning for 2D navigation, which originates from Euclidean transformations between reference frames and enables parameter sharing. To address the challenges of unstructured environments, we formulate the navigation problem as planning on a geometric graph and develop an equivariant message passing network to perform value iteration. Furthermore, to handle multi-camera input, we propose a learnable equivariant layer to lift features to a desired space. We conduct comprehensive evaluations across five diverse tasks encompassing structured and unstructured environments, along with maps of known and unknown, given point goals or semantic goals. Our experiments confirm the substantial benefits on training efficiency, stability, and generalization.
&lt;/p&gt;</description></item><item><title>MosaicFusion&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.13042</link><description>&lt;p&gt;
MosaicFusion: &#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#22120;
&lt;/p&gt;
&lt;p&gt;
MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation. (arXiv:2309.13042v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13042
&lt;/p&gt;
&lt;p&gt;
MosaicFusion&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MosaicFusion&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26631;&#31614;&#30417;&#30563;&#12290;&#20004;&#20010;&#20851;&#38190;&#35774;&#35745;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#26377;&#29992;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#23545;&#35937;&#23454;&#20363;&#21644;&#33945;&#29256;&#27880;&#37322;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#30011;&#24067;&#20998;&#20026;&#20960;&#20010;&#21306;&#22495;&#65292;&#24182;&#25191;&#34892;&#19968;&#36718;&#25193;&#25955;&#36807;&#31243;&#65292;&#21516;&#26102;&#22522;&#20110;&#19981;&#21516;&#30340;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22810;&#20010;&#23454;&#20363;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#21512;&#19982;&#23545;&#35937;&#25552;&#31034;&#30456;&#20851;&#32852;&#30340;&#36328;&#27880;&#24847;&#21147;&#22270;&#22312;&#23618;&#21644;&#25193;&#25955;&#26102;&#38388;&#27493;&#19978;&#65292;&#28982;&#21518;&#36827;&#34892;&#31616;&#21333;&#30340;&#38408;&#20540;&#22788;&#29702;&#21644;&#36793;&#32536;&#24863;&#30693;&#30340;&#32454;&#21270;&#22788;&#29702;&#65292;&#24471;&#21040;&#30456;&#24212;&#30340;&#23454;&#20363;&#33945;&#29256;&#12290;&#25105;&#20204;&#30340;MosaicFusion&#21487;&#20197;&#20026;&#31232;&#32570;&#21644;&#26032;&#39062;&#31867;&#21035;&#20135;&#29983;&#22823;&#37327;&#30340;&#21512;&#25104;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#22788;&#29702;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;LVIS&#38271;&#23614;&#21644;&#24320;&#25918;&#35789;&#27719;&#22522;&#20934;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MosaicFusion, a simple yet effective diffusion-based data augmentation approach for large vocabulary instance segmentation. Our method is training-free and does not rely on any label supervision. Two key designs enable us to employ an off-the-shelf text-to-image diffusion model as a useful dataset generator for object instances and mask annotations. First, we divide an image canvas into several regions and perform a single round of diffusion process to generate multiple instances simultaneously, conditioning on different text prompts. Second, we obtain corresponding instance masks by aggregating cross-attention maps associated with object prompts across layers and diffusion time steps, followed by simple thresholding and edge-aware refinement processing. Without bells and whistles, our MosaicFusion can produce a significant amount of synthetic labeled data for both rare and novel categories. Experimental results on the challenging LVIS long-tailed and open-vocabulary benchma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35760;&#24518;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#28155;&#21152;&#21040;Conformer&#27169;&#22411;&#20013;&#20197;&#35299;&#20915;&#38271;&#35805;&#35821;&#24773;&#20917;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#22312;&#38271;&#35805;&#35821;&#19978;&#20248;&#20110;&#22522;&#20934;Conformer&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.13029</link><description>&lt;p&gt;
&#22686;&#24378;&#35760;&#24518;&#30340;Conformer&#29992;&#20110;&#25913;&#36827;&#31471;&#21040;&#31471;&#38271;&#31687;ASR
&lt;/p&gt;
&lt;p&gt;
Memory-augmented conformer for improved end-to-end long-form ASR. (arXiv:2309.13029v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13029
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35760;&#24518;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#28155;&#21152;&#21040;Conformer&#27169;&#22411;&#20013;&#20197;&#35299;&#20915;&#38271;&#35805;&#35821;&#24773;&#20917;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#22312;&#38271;&#35805;&#35821;&#19978;&#20248;&#20110;&#22522;&#20934;Conformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Conformer&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#20248;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#12290;&#28982;&#32780;&#65292;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#31471;&#21040;&#31471;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22411;&#65289;&#22312;&#38271;&#35805;&#35821;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;Conformer&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#28155;&#21152;&#19968;&#20010;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;&#35760;&#24518;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20010;&#22806;&#37096;&#35760;&#24518;&#21487;&#20197;&#22686;&#24378;&#23545;&#38271;&#35805;&#35821;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#31995;&#32479;&#24490;&#29615;&#22320;&#23384;&#20648;&#21644;&#26816;&#32034;&#26356;&#22810;&#30340;&#20449;&#24687;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31070;&#32463;&#22270;&#28789;&#26426;&#65288;NTM&#65289;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;Conformer-NTM&#27169;&#22411;&#26550;&#26500;&#29992;&#20110;ASR&#12290;&#20351;&#29992;Librispeech&#30340;train-clean-100&#21644;train-960&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#38271;&#35805;&#35821;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#20248;&#20110;&#27809;&#26377;&#35760;&#24518;&#30340;&#22522;&#20934;Conformer&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformers have recently been proposed as a promising modelling approach for automatic speech recognition (ASR), outperforming recurrent neural network-based approaches and transformers. Nevertheless, in general, the performance of these end-to-end models, especially attention-based models, is particularly degraded in the case of long utterances. To address this limitation, we propose adding a fully-differentiable memory-augmented neural network between the encoder and decoder of a conformer. This external memory can enrich the generalization for longer utterances since it allows the system to store and retrieve more information recurrently. Notably, we explore the neural Turing machine (NTM) that results in our proposed Conformer-NTM model architecture for ASR. Experimental results using Librispeech train-clean-100 and train-960 sets show that the proposed system outperforms the baseline conformer without memory for long utterances.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#22522;&#20110;&#29615;&#22659;&#30340;&#22522;&#22240;&#22411;&#36873;&#25321;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#20316;&#29289;&#21697;&#31181;&#30340;&#22825;&#27668;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21516;&#27668;&#20505;&#26465;&#20214;&#19979;&#65292;&#20934;&#30830;&#22320;&#39044;&#27979;&#20316;&#29289;&#20135;&#37327;&#23545;&#20110;&#29702;&#35299;&#20854;&#36866;&#24212;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#35770;&#25991;&#20013;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#24191;&#20041;&#38598;&#25104;&#26041;&#27861;&#30830;&#23450;&#20102;&#26368;&#20248;&#30340;&#22522;&#22240;&#22411;&#19982;&#29615;&#22659;&#36873;&#25321;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.13021</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#29992;&#20110;&#20248;&#21270;&#22522;&#20110;&#29615;&#22659;&#30340;&#22522;&#22240;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Deep Learning-based Approach for Optimal Genotype by Environment Selection. (arXiv:2309.13021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13021
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#22522;&#20110;&#29615;&#22659;&#30340;&#22522;&#22240;&#22411;&#36873;&#25321;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#20316;&#29289;&#21697;&#31181;&#30340;&#22825;&#27668;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21516;&#27668;&#20505;&#26465;&#20214;&#19979;&#65292;&#20934;&#30830;&#22320;&#39044;&#27979;&#20316;&#29289;&#20135;&#37327;&#23545;&#20110;&#29702;&#35299;&#20854;&#36866;&#24212;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#35770;&#25991;&#20013;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#24191;&#20041;&#38598;&#25104;&#26041;&#27861;&#30830;&#23450;&#20102;&#26368;&#20248;&#30340;&#22522;&#22240;&#22411;&#19982;&#29615;&#22659;&#36873;&#25321;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#23545;&#20110;&#25913;&#21892;&#20892;&#19994;&#23454;&#36341;&#21644;&#30830;&#20445;&#20316;&#29289;&#22312;&#19981;&#21516;&#27668;&#20505;&#26465;&#20214;&#19979;&#30340;&#38887;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;MLCAS2021&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#25361;&#25112;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21253;&#21547;93,028&#20010;&#35757;&#32451;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#39044;&#27979;&#20102;10,337&#20010;&#27979;&#35797;&#35760;&#24405;&#30340;&#20135;&#37327;&#65292;&#22312;13&#24180;&#65288;2003-2015&#24180;&#65289;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#35206;&#30422;&#20102;&#32654;&#22269;28&#20010;&#24030;&#21644;&#21152;&#25343;&#22823;&#30465;&#20221;&#30340;159&#20010;&#22320;&#28857;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;5,838&#20010;&#19981;&#21516;&#22522;&#22240;&#22411;&#30340;&#35814;&#32454;&#20449;&#24687;&#21644;&#20026;&#26399;214&#22825;&#30340;&#29983;&#38271;&#23395;&#33410;&#30340;&#27599;&#26085;&#22825;&#27668;&#25968;&#25454;&#65292;&#20351;&#24471;&#32508;&#21512;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;&#20316;&#20026;&#33719;&#32988;&#22242;&#38431;&#20043;&#19968;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#65306;CNN-DNN&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;CNN&#21644;&#20840;&#36830;&#25509;&#32593;&#32476;&#65307;&#20197;&#21450;CNN-LSTM-DNN&#27169;&#22411;&#65292;&#21152;&#20837;&#20102;LSTM&#23618;&#29992;&#20110;&#22825;&#27668;&#21464;&#37327;&#12290;&#21033;&#29992;&#24191;&#20041;&#38598;&#25104;&#26041;&#27861;&#65288;GEM&#65289;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26368;&#20248;&#30340;&#22522;&#22240;&#22411;&#19982;&#29615;&#22659;&#36873;&#25321;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise crop yield prediction is essential for improving agricultural practices and ensuring crop resilience in varying climates. Integrating weather data across the growing season, especially for different crop varieties, is crucial for understanding their adaptability in the face of climate change. In the MLCAS2021 Crop Yield Prediction Challenge, we utilized a dataset comprising 93,028 training records to forecast yields for 10,337 test records, covering 159 locations across 28 U.S. states and Canadian provinces over 13 years (2003-2015). This dataset included details on 5,838 distinct genotypes and daily weather data for a 214-day growing season, enabling comprehensive analysis. As one of the winning teams, we developed two novel convolutional neural network (CNN) architectures: the CNN-DNN model, combining CNN and fully-connected networks, and the CNN-LSTM-DNN model, with an added LSTM layer for weather variables. Leveraging the Generalized Ensemble Method (GEM), we determined opt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31639;&#27861;&#65292;&#26550;&#26500;&#21644;&#25968;&#25454;&#27969;&#21327;&#21516;&#35774;&#35745;&#30340;&#39640;&#25928;N:M&#31232;&#30095;DNN&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21033;&#29992;&#21452;&#21521;&#26435;&#37325;&#20462;&#21098;&#26041;&#27861;&#20248;&#21270;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#21152;&#36895;&#22120;&#30828;&#20214;&#25903;&#25345;&#23454;&#29616;&#39640;&#31232;&#30095;&#27604;&#30340;DNN&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2309.13015</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;N:M&#31232;&#30095;DNN&#35757;&#32451;&#20351;&#29992;&#31639;&#27861;&#65292;&#26550;&#26500;&#21644;&#25968;&#25454;&#27969;&#21327;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient N:M Sparse DNN Training Using Algorithm, Architecture, and Dataflow Co-Design. (arXiv:2309.13015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31639;&#27861;&#65292;&#26550;&#26500;&#21644;&#25968;&#25454;&#27969;&#21327;&#21516;&#35774;&#35745;&#30340;&#39640;&#25928;N:M&#31232;&#30095;DNN&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21033;&#29992;&#21452;&#21521;&#26435;&#37325;&#20462;&#21098;&#26041;&#27861;&#20248;&#21270;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#21152;&#36895;&#22120;&#30828;&#20214;&#25903;&#25345;&#23454;&#29616;&#39640;&#31232;&#30095;&#27604;&#30340;DNN&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#35757;&#32451;&#26159;&#20943;&#23569;DNN&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;&#29305;&#21035;&#26159;N:M&#32454;&#31890;&#24230;&#32467;&#26500;&#31232;&#30095;&#65292;&#20854;&#20013;&#21482;&#26377;&#36830;&#32493;M&#20010;&#20803;&#32032;&#20013;&#30340;N&#20010;&#21487;&#20197;&#26159;&#38750;&#38646;&#20540;&#65292;&#22240;&#20854;&#23545;&#30828;&#20214;&#21451;&#22909;&#30340;&#27169;&#24335;&#21644;&#36798;&#21040;&#39640;&#31232;&#30095;&#27604;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21152;&#36895;N:M&#31232;&#30095;DNN&#35757;&#32451;&#30340;&#28508;&#21147;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#65292;&#24182;&#19988;&#32570;&#20047;&#25903;&#25345;N:M&#31232;&#30095;&#35757;&#32451;&#30340;&#39640;&#25928;&#30828;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31639;&#27861;&#65292;&#26550;&#26500;&#21644;&#25968;&#25454;&#27969;&#21327;&#21516;&#35774;&#35745;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;N:M&#31232;&#30095;DNN&#35757;&#32451;&#26041;&#26696;&#12290;&#22312;&#31639;&#27861;&#23618;&#38754;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#26435;&#37325;&#20462;&#21098;&#26041;&#27861;&#65288;BDWP&#65289;&#65292;&#21033;&#29992;DNN&#35757;&#32451;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#26435;&#37325;&#30340;N:M&#31232;&#30095;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#22312;&#26550;&#26500;&#23618;&#38754;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;DNN&#31232;&#30095;&#21152;&#36895;&#30340;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22522;&#20110;&#25968;&#25454;&#27969;&#21327;&#35758;&#65292;&#21033;&#29992;&#31232;&#30095;&#26435;&#37325;&#30340;&#32467;&#26500;&#27169;&#24335;&#20248;&#21270;&#35745;&#31639;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse training is one of the promising techniques to reduce the computational cost of DNNs while retaining high accuracy. In particular, N:M fine-grained structured sparsity, where only N out of consecutive M elements can be nonzero, has attracted attention due to its hardware-friendly pattern and capability of achieving a high sparse ratio. However, the potential to accelerate N:M sparse DNN training has not been fully exploited, and there is a lack of efficient hardware supporting N:M sparse training. To tackle these challenges, this paper presents a computation-efficient training scheme for N:M sparse DNNs using algorithm, architecture, and dataflow co-design. At the algorithm level, a bidirectional weight pruning method, dubbed BDWP, is proposed to leverage the N:M sparsity of weights during both forward and backward passes of DNN training, which can significantly reduce the computational cost while maintaining model accuracy. At the architecture level, a sparse accelerator for DN
&lt;/p&gt;</description></item><item><title>ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.13007</link><description>&lt;p&gt;
ReConcile&#65306;&#22278;&#26700;&#20250;&#35758;&#36890;&#36807;&#22810;&#20803;LLM&#30340;&#20849;&#35782;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs. (arXiv:2309.13007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13007
&lt;/p&gt;
&lt;p&gt;
ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#21463;&#21040;&#24515;&#26234;&#31038;&#20250;&#29702;&#35770;&#65288;Minsky, 1988&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReConcile&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22810;&#26679;&#30340;LLM&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#22278;&#26700;&#20250;&#35758;&#26469;&#20419;&#36827;&#22810;&#26679;&#30340;&#24605;&#24819;&#21644;&#35752;&#35770;&#65292;&#20174;&#32780;&#25913;&#36827;&#19968;&#33268;&#24615;&#12290;ReConcile&#36890;&#36807;&#36827;&#34892;&#22810;&#36718;&#35752;&#35770;&#12289;&#23398;&#20064;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#25913;&#36827;&#31572;&#26696;&#20197;&#21450;&#37319;&#29992;&#32622;&#20449;&#24230;&#21152;&#26435;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;ReConcile&#36890;&#36807;&#8220;&#35752;&#35770;&#25552;&#31034;&#8221;&#26469;&#21551;&#21160;&#20195;&#29702;&#20154;&#38388;&#30340;&#35752;&#35770;&#65292;&#20854;&#20013;&#21253;&#25324;&#19978;&#19968;&#36718;&#27599;&#20010;&#20195;&#29702;&#20154;&#29983;&#25104;&#30340;&#31572;&#26696;&#21644;&#35299;&#37322;&#30340;&#20998;&#32452;&#12289;&#23427;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#29992;&#20110;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#31572;&#26696;&#20462;&#27491;&#20154;&#31867;&#35299;&#37322;&#30340;&#28436;&#31034;&#12290;&#36825;&#20010;&#35752;&#35770;&#25552;&#31034;&#20351;&#27599;&#20010;&#20195;&#29702;&#20154;&#33021;&#22815;&#26681;&#25454;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#35265;&#35299;&#20462;&#35746;&#33258;&#24049;&#30340;&#22238;&#31572;&#12290;&#19968;&#26086;&#36798;&#25104;&#19968;&#33268;&#24182;&#32467;&#26463;&#35752;&#35770;&#65292;ReConcile&#25191;&#34892;&#19968;&#27425;&#20840;&#20307;&#25237;&#31080;&#20197;&#30830;&#23450;&#26368;&#32456;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) still struggle with complex reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents to foster diverse thoughts and discussion for improved consensus. ReConcile enhances the reasoning capabilities of LLMs by holding multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their uncertainties, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. This discussion prompt enables each agent to revise their responses in light of insights from other agents. Once a consensus is reached and the discussion ends, ReConcil
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39034;&#24207;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#36861;&#27714;&#30340;&#21019;&#26032;&#26694;&#26550;&#23558;&#29615;&#22659;&#20449;&#24687;&#21644;&#25935;&#24863;&#23646;&#24615;&#19982;&#20998;&#31867;&#29305;&#24449;&#30340;&#23884;&#20837;&#34920;&#31034;&#20998;&#24320;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#19981;&#21516;&#21644;&#38476;&#29983;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13005</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#22495;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#36861;&#27714;
&lt;/p&gt;
&lt;p&gt;
Pursuing Counterfactual Fairness via Sequential Autoencoder Across Domains. (arXiv:2309.13005v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13005
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39034;&#24207;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#36861;&#27714;&#30340;&#21019;&#26032;&#26694;&#26550;&#23558;&#29615;&#22659;&#20449;&#24687;&#21644;&#25935;&#24863;&#23646;&#24615;&#19982;&#20998;&#31867;&#29305;&#24449;&#30340;&#23884;&#20837;&#34920;&#31034;&#20998;&#24320;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#19981;&#21516;&#21644;&#38476;&#29983;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#39046;&#22495;&#36716;&#31227;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#21508;&#31181;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#22788;&#29702;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#26102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#22312;&#36830;&#32493;&#30340;&#24207;&#21015;&#39046;&#22495;&#20013;&#36880;&#28176;&#21464;&#21270;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#22312;&#36825;&#20123;&#26032;&#39046;&#22495;&#20869;&#30340;&#27169;&#22411;&#25928;&#26524;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#20855;&#26377;&#39034;&#24207;&#33258;&#32534;&#30721;&#22120;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#65288;CDSAE&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#29615;&#22659;&#20449;&#24687;&#21644;&#25935;&#24863;&#23646;&#24615;&#19982;&#20998;&#31867;&#29305;&#24449;&#30340;&#23884;&#20837;&#34920;&#31034;&#20998;&#24320;&#12290;&#36825;&#31181;&#24182;&#34892;&#20998;&#31163;&#19981;&#20165;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#21644;&#38476;&#29983;&#30340;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36824;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing the prevalence of domain shift as a common challenge in machine learning, various domain generalization (DG) techniques have been developed to enhance the performance of machine learning systems when dealing with out-of-distribution (OOD) data. Furthermore, in real-world scenarios, data distributions can gradually change across a sequence of sequential domains. While current methodologies primarily focus on improving model effectiveness within these new domains, they often overlook fairness issues throughout the learning process. In response, we introduce an innovative framework called Counterfactual Fairness-Aware Domain Generalization with Sequential Autoencoder (CDSAE). This approach effectively separates environmental information and sensitive attributes from the embedded representation of classification features. This concurrent separation not only greatly improves model generalization across diverse and unfamiliar domains but also effectively addresses challenges rela
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#38024;&#23545;&#29305;&#23450;&#35266;&#20247;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#31034;&#20363;&#35299;&#37322;&#26469;&#35299;&#20915;&#32763;&#35793;&#20013;&#30340;&#25991;&#21270;&#24046;&#24322;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#21477;&#23376;&#20013;&#21253;&#21547;&#35299;&#37322;&#30340;&#27604;&#20363;&#12290;</title><link>http://arxiv.org/abs/2309.12998</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#30340;&#38024;&#23545;&#29305;&#23450;&#35266;&#20247;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Audience-specific Explanations for Machine Translation. (arXiv:2309.12998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12998
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#38024;&#23545;&#29305;&#23450;&#35266;&#20247;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#31034;&#20363;&#35299;&#37322;&#26469;&#35299;&#20915;&#32763;&#35793;&#20013;&#30340;&#25991;&#21270;&#24046;&#24322;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#21477;&#23376;&#20013;&#21253;&#21547;&#35299;&#37322;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#26576;&#20123;&#35789;&#30340;&#32763;&#35793;&#21363;&#20351;&#32763;&#35793;&#20102;&#20063;&#20250;&#22240;&#20026;&#19981;&#21516;&#30340;&#25991;&#21270;&#32972;&#26223;&#23548;&#33268;&#30446;&#26631;&#35821;&#35328;&#35266;&#20247;&#26080;&#27861;&#29702;&#35299;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#20026;&#36825;&#20123;&#35789;&#28155;&#21152;&#35299;&#37322;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#35782;&#21035;&#36825;&#20123;&#35789;&#25110;&#30701;&#35821;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20174;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#31034;&#20363;&#35299;&#37322;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#21253;&#21547;&#38656;&#35201;&#35299;&#37322;&#30340;&#35789;&#30340;&#21477;&#23376;&#30340;&#31232;&#32570;&#24615;&#20351;&#24471;&#26500;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#26497;&#20854;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#25216;&#26415;&#65292;&#21487;&#20197;&#20174;&#22823;&#22411;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#36825;&#20123;&#35299;&#37322;&#12290;&#22312;&#33521;&#35821;-&gt;&#24503;&#35821;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#21462;&#20986;&#36229;&#36807;10%&#30340;&#21477;&#23376;&#21253;&#21547;&#35299;&#37322;&#65292;&#32780;&#21407;&#22987;&#21477;&#23376;&#20013;&#21482;&#26377;1.9%&#21253;&#21547;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#22312;&#33521;&#35821;-&gt;&#27861;&#35821;&#21644;&#33521;&#35821;-&gt;&#20013;&#25991;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#20063;&#26174;&#31034;&#20986;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine translation, a common problem is that the translation of certain words even if translated can cause incomprehension of the target language audience due to different cultural backgrounds. A solution to solve this problem is to add explanations for these words. In a first step, we therefore need to identify these words or phrases. In this work we explore techniques to extract example explanations from a parallel corpus. However, the sparsity of sentences containing words that need to be explained makes building the training dataset extremely difficult. In this work, we propose a semi-automatic technique to extract these explanations from a large parallel corpus. Experiments on English-&gt;German language pair show that our method is able to extract sentence so that more than 10% of the sentences contain explanation, while only 1.9% of the original sentences contain explanations. In addition, experiments on English-&gt;French and English-&gt;Chinese language pairs also show similar conc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33457;&#29923;&#25289;&#26222;&#25289;&#26031;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#31616;&#21333;&#22797;&#21512;&#20307;&#26469;&#24314;&#27169;&#39640;&#38454;&#20132;&#20114;&#65292;&#22312;&#19981;&#21516;&#25299;&#25169;&#23610;&#24230;&#19978;&#35782;&#21035;&#20869;&#22312;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22270;&#28388;&#27874;&#22120;&#26469;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#24378;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.12971</link><description>&lt;p&gt;
&#22522;&#20110;&#33457;&#29923;&#25289;&#26222;&#25289;&#26031;&#22312;&#31616;&#21333;&#22797;&#21512;&#20307;&#19978;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes. (arXiv:2309.12971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33457;&#29923;&#25289;&#26222;&#25289;&#26031;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#31616;&#21333;&#22797;&#21512;&#20307;&#26469;&#24314;&#27169;&#39640;&#38454;&#20132;&#20114;&#65292;&#22312;&#19981;&#21516;&#25299;&#25169;&#23610;&#24230;&#19978;&#35782;&#21035;&#20869;&#22312;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22270;&#28388;&#27874;&#22120;&#26469;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26222;&#36890;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#22522;&#20110;&#37197;&#23545;&#20132;&#20114;&#32593;&#32476;&#30340;&#22522;&#30784;&#26412;&#36136;&#19978;&#38480;&#21046;&#20102;&#20854;&#35782;&#21035;&#22797;&#26434;&#31995;&#32479;&#20013;&#28508;&#22312;&#39640;&#38454;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#31181;&#33021;&#21147;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22797;&#26434;&#31995;&#32479;&#30340;&#39640;&#38454;&#20132;&#20114;&#24314;&#27169;&#30340;&#20016;&#23500;&#25968;&#23398;&#29702;&#35770;&#65292;&#21363;&#31616;&#21333;&#22797;&#21512;&#20307;&#65288;SCs&#65289;-&#19968;&#31181;&#23545;&#24314;&#27169;&#39640;&#38454;&#20132;&#20114;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#12290;&#30446;&#21069;&#22522;&#20110;SC&#30340;GNNs&#23384;&#22312;&#22797;&#26434;&#24230;&#39640;&#21644;&#21051;&#26495;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#24378;&#24230;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21019;&#26032;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#38454;&#33457;&#29923;&#65288;FP&#65289;&#27169;&#22411;&#65292;&#23558;FP&#25289;&#26222;&#25289;&#26031;&#24341;&#20837;&#21040;SC&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20197;FP&#25289;&#26222;&#25289;&#26031;&#20026;&#22522;&#30784;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;HiGCN&#65289;&#65292;&#33021;&#22815;&#35782;&#21035;&#19981;&#21516;&#25299;&#25169;&#23610;&#24230;&#19978;&#30340;&#20869;&#22312;&#29305;&#24449;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22270;&#28388;&#27874;&#22120;&#65292;FP&#25289;&#26222;&#25289;&#26031;&#22495;&#20869;&#30340;&#21442;&#25968;&#32452;&#65292;&#25105;&#20204;&#21487;&#20197;&#35782;&#21035;&#20986;&#20855;&#26377;&#19981;&#21516;&#27169;&#24335;&#30340;&#22270;&#26696;&#65292;&#20854;&#20013;&#28388;&#27874;&#22120;&#30340;&#26435;&#37325;&#29992;&#20316;&#25968;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent successes of vanilla Graph Neural Networks (GNNs) on many tasks, their foundation on pairwise interaction networks inherently limits their capacity to discern latent higher-order interactions in complex systems. To bridge this capability gap, we propose a novel approach exploiting the rich mathematical theory of simplicial complexes (SCs) - a robust tool for modeling higher-order interactions. Current SC-based GNNs are burdened by high complexity and rigidity, and quantifying higher-order interaction strengths remains challenging. Innovatively, we present a higher-order Flower-Petals (FP) model, incorporating FP Laplacians into SCs. Further, we introduce a Higher-order Graph Convolutional Network (HiGCN) grounded in FP Laplacians, capable of discerning intrinsic features across varying topological scales. By employing learnable graph filters, a parameter group within each FP Laplacian domain, we can identify diverse patterns where the filters' weights serve as a quan
&lt;/p&gt;</description></item><item><title>Trusta&#26159;&#19968;&#27454;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20445;&#35777;&#35770;&#35777;&#30340;&#26700;&#38754;&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#39564;&#35777;&#21487;&#20449;&#24230;&#25512;&#23548;&#26641;(TDT)&#65292;&#21516;&#26102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#35770;&#35777;&#30340;&#21019;&#24314;&#21644;&#35780;&#20272;&#26356;&#21152;&#20415;&#25463;&#12290;</title><link>http://arxiv.org/abs/2309.12941</link><description>&lt;p&gt;
Trusta: &#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20445;&#35777;&#35770;&#35777;
&lt;/p&gt;
&lt;p&gt;
Trusta: Reasoning about Assurance Cases with Formal Methods and Large Language Models. (arXiv:2309.12941v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12941
&lt;/p&gt;
&lt;p&gt;
Trusta&#26159;&#19968;&#27454;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20445;&#35777;&#35770;&#35777;&#30340;&#26700;&#38754;&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#39564;&#35777;&#21487;&#20449;&#24230;&#25512;&#23548;&#26641;(TDT)&#65292;&#21516;&#26102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#35770;&#35777;&#30340;&#21019;&#24314;&#21644;&#35780;&#20272;&#26356;&#21152;&#20415;&#25463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#35777;&#35770;&#35777;&#21487;&#20197;&#29992;&#20110;&#22312;&#23433;&#20840;&#24037;&#31243;&#20013;&#20026;&#20135;&#21697;&#30340;&#23433;&#20840;&#24615;&#25552;&#20379;&#35770;&#25454;&#12290;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#26500;&#24314;&#20445;&#35777;&#35770;&#35777;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#21487;&#20449;&#24230;&#25512;&#23548;&#26641;&#65288;TDT&#65289;&#36890;&#36807;&#32467;&#21512;&#24418;&#24335;&#26041;&#27861;&#21152;&#24378;&#20102;&#20445;&#35777;&#35770;&#35777;&#65292;&#20351;&#24471;&#33258;&#21160;&#25512;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Trustworthiness Derivation Tree Analyzer&#65288;Trusta&#65289;&#65292;&#19968;&#27454;&#26088;&#22312;&#33258;&#21160;&#26500;&#24314;&#21644;&#39564;&#35777;TDT&#30340;&#26700;&#38754;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#24037;&#20855;&#22312;&#20854;&#21518;&#31471;&#20855;&#26377;&#20869;&#32622;&#30340;Prolog&#35299;&#37322;&#22120;&#65292;&#24182;&#21463;&#21040;&#32422;&#26463;&#27714;&#35299;&#22120;Z3&#21644;MONA&#30340;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#28041;&#21450;&#31639;&#26415;&#12289;&#38598;&#21512;&#12289;Horn&#23376;&#21477;&#31561;&#36923;&#36753;&#20844;&#24335;&#30340;&#32422;&#26463;&#38382;&#39064;&#12290;Trusta&#36824;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20445;&#35777;&#35770;&#35777;&#30340;&#21019;&#24314;&#21644;&#35780;&#20272;&#26356;&#21152;&#20415;&#25463;&#12290;&#23427;&#20801;&#35768;&#36827;&#34892;&#20132;&#20114;&#24335;&#30340;&#20154;&#24037;&#23457;&#26597;&#21644;&#20462;&#25913;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT-3.5&#12289;ChatGPT-4&#21644;PaLM 2&#31561;&#39030;&#32423;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#20445;&#35777;&#35770;&#35777;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#26174;&#31034;&#65292;&#20854;&#29983;&#25104;&#30340;&#20445;&#35777;&#35770;&#35777;&#30340;&#20934;&#30830;&#29575;&#20026;50%-80%&#12290;
&lt;/p&gt;
&lt;p&gt;
Assurance cases can be used to argue for the safety of products in safety engineering. In safety-critical areas, the construction of assurance cases is indispensable. Trustworthiness Derivation Trees (TDTs) enhance assurance cases by incorporating formal methods, rendering it possible for automatic reasoning about assurance cases. We present Trustworthiness Derivation Tree Analyzer (Trusta), a desktop application designed to automatically construct and verify TDTs. The tool has a built-in Prolog interpreter in its backend, and is supported by the constraint solvers Z3 and MONA. Therefore, it can solve constraints about logical formulas involving arithmetic, sets, Horn clauses etc. Trusta also utilizes large language models to make the creation and evaluation of assurance cases more convenient. It allows for interactive human examination and modification. We evaluated top language models like ChatGPT-3.5, ChatGPT-4, and PaLM 2 for generating assurance cases. Our tests showed a 50%-80% s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35299;&#37322;&#25552;&#31034;&#31574;&#30053;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#23454;&#20854;&#22312;&#22797;&#26434;&#23545;&#35805;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12940</link><description>&lt;p&gt;
&#33258;&#35299;&#37322;&#25552;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#39640;&#23545;&#35805;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models. (arXiv:2309.12940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35299;&#37322;&#25552;&#31034;&#31574;&#30053;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#23454;&#20854;&#22312;&#22797;&#26434;&#23545;&#35805;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#36890;&#36807;&#22810;&#36718;&#23545;&#35805;&#24110;&#21161;&#29992;&#25143;&#25191;&#34892;&#21508;&#31181;&#27963;&#21160;&#65292;&#20294;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24448;&#24448;&#38590;&#20197;&#29702;&#35299;&#36825;&#20123;&#22797;&#26434;&#30340;&#35821;&#22659;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#33258;&#35299;&#37322;&#8221;&#25552;&#31034;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;LLMs&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#36825;&#31181;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#27861;&#35201;&#27714;&#27169;&#22411;&#22312;&#25191;&#34892;&#20219;&#21153;&#20043;&#21069;&#20998;&#26512;&#27599;&#20010;&#23545;&#35805;&#35805;&#35821;&#65292;&#20174;&#32780;&#25913;&#21892;&#21508;&#31181;&#23545;&#35805;&#20013;&#24515;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26469;&#33258;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#25454;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#38646;&#26679;&#26412;&#25552;&#31034;&#65292;&#24182;&#19988;&#19982;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#30456;&#24403;&#25110;&#36229;&#36807;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#25552;&#39640;LLMs&#22312;&#22797;&#26434;&#23545;&#35805;&#20219;&#21153;&#20013;&#30340;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue (TOD) systems facilitate users in executing various activities via multi-turn dialogues, but Large Language Models (LLMs) often struggle to comprehend these intricate contexts. In this study, we propose a novel "Self-Explanation" prompting strategy to enhance the comprehension abilities of LLMs in multi-turn dialogues. This task-agnostic approach requires the model to analyze each dialogue utterance before task execution, thereby improving performance across various dialogue-centric tasks. Experimental results from six benchmark datasets confirm that our method consistently outperforms other zero-shot prompts and matches or exceeds the efficacy of few-shot prompts, demonstrating its potential as a powerful tool in enhancing LLMs' comprehension in complex dialogue tasks.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21327;&#21161;&#24320;&#21457;&#32773;&#20462;&#27491;&#20195;&#30721;&#20174;&#32780;&#35299;&#20915;&#20195;&#30721;&#36136;&#37327;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CORE&#30340;&#24037;&#20855;&#65292;&#20351;&#29992;&#19968;&#23545;&#32452;&#32455;&#20026;&#25552;&#20379;&#32773;&#21644;&#35780;&#20272;&#32773;&#30340;LLMs&#12290;&#35813;&#24037;&#20855;&#36890;&#36807;&#25552;&#20379;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#30340;&#25512;&#33616;&#21644;&#29983;&#25104;&#20505;&#36873;&#30340;&#20195;&#30721;&#20462;&#35746;&#26469;&#25913;&#21892;&#20195;&#30721;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.12938</link><description>&lt;p&gt;
&#21463;&#20195;&#30721;&#36136;&#37327;&#38382;&#39064;&#22256;&#25200;&#65311;LLM&#21487;&#20197;&#24110;&#21161;&#65281;
&lt;/p&gt;
&lt;p&gt;
Frustrated with Code Quality Issues? LLMs can Help!. (arXiv:2309.12938v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12938
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21327;&#21161;&#24320;&#21457;&#32773;&#20462;&#27491;&#20195;&#30721;&#20174;&#32780;&#35299;&#20915;&#20195;&#30721;&#36136;&#37327;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CORE&#30340;&#24037;&#20855;&#65292;&#20351;&#29992;&#19968;&#23545;&#32452;&#32455;&#20026;&#25552;&#20379;&#32773;&#21644;&#35780;&#20272;&#32773;&#30340;LLMs&#12290;&#35813;&#24037;&#20855;&#36890;&#36807;&#25552;&#20379;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#30340;&#25512;&#33616;&#21644;&#29983;&#25104;&#20505;&#36873;&#30340;&#20195;&#30721;&#20462;&#35746;&#26469;&#25913;&#21892;&#20195;&#30721;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36719;&#20214;&#39033;&#30446;&#30340;&#36827;&#34892;&#65292;&#20195;&#30721;&#36136;&#37327;&#23545;&#36719;&#20214;&#30340;&#21487;&#38752;&#24615;&#12289;&#21487;&#32500;&#25252;&#24615;&#21644;&#23433;&#20840;&#24615;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22312;&#24320;&#21457;&#32773;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#20351;&#29992;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#26469;&#26631;&#35760;&#20195;&#30721;&#36136;&#37327;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#32773;&#38656;&#35201;&#39069;&#22806;&#21162;&#21147;&#26469;&#20462;&#25913;&#20182;&#20204;&#30340;&#20195;&#30721;&#20197;&#25913;&#21892;&#20195;&#30721;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#65288;&#25351;&#20196;&#36319;&#38543;&#65289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24320;&#21457;&#32773;&#20462;&#27491;&#20195;&#30721;&#20197;&#35299;&#20915;&#20195;&#30721;&#36136;&#37327;&#38382;&#39064;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#21517;&#20026;CORE&#65288;COde REvisions&#65289;&#65292;&#35813;&#24037;&#20855;&#20351;&#29992;&#19968;&#23545;&#32452;&#32455;&#20026;&#25552;&#20379;&#32773;&#21644;&#35780;&#20272;&#32773;&#30340;LLMs&#12290;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#30340;&#25552;&#20379;&#32773;&#25512;&#33616;&#35299;&#20915;&#24037;&#20855;&#35686;&#21578;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#32773;&#36981;&#24490;&#36825;&#20123;&#26041;&#27861;&#26469;&#20462;&#25913;&#20182;&#20204;&#30340;&#20195;&#30721;&#12290;CORE&#30340;\emph{&#25552;&#20379;&#32773;LLM}&#25509;&#21463;&#30456;&#21516;&#30340;&#25512;&#33616;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#29983;&#25104;&#20505;&#36873;&#30340;&#20195;&#30721;&#20462;&#35746;&#12290;&#36890;&#36807;&#38745;&#24577;&#36136;&#37327;&#26816;&#26597;&#30340;&#20505;&#36873;&#20195;&#30721;&#34987;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;
As software projects progress, quality of code assumes paramount importance as it affects reliability, maintainability and security of software. For this reason, static analysis tools are used in developer workflows to flag code quality issues. However, developers need to spend extra efforts to revise their code to improve code quality based on the tool findings. In this work, we investigate the use of (instruction-following) large language models (LLMs) to assist developers in revising code to resolve code quality issues. We present a tool, CORE (short for COde REvisions), architected using a pair of LLMs organized as a duo comprised of a proposer and a ranker. Providers of static analysis tools recommend ways to mitigate the tool warnings and developers follow them to revise their code. The \emph{proposer LLM} of CORE takes the same set of recommendations and applies them to generate candidate code revisions. The candidates which pass the static quality checks are retained. However, 
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#65292;&#36890;&#36807;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12931</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#30340;&#20998;&#21035;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Separate Normalization in Self-supervised Transformers. (arXiv:2309.12931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12931
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#65292;&#36890;&#36807;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#30340;&#35757;&#32451;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#20197;&#24448;&#30340;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65288;&#22914;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65289;&#36890;&#24120;&#20250;&#20026;[CLS]&#31526;&#21495;&#21644;&#26631;&#35760;&#20351;&#29992;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#22686;&#24378;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#32531;&#35299;&#23558;&#30456;&#21516;&#30340;&#24402;&#19968;&#21270;&#32479;&#35745;&#25968;&#25454;&#24212;&#29992;&#20110;&#20004;&#31181;&#26631;&#35760;&#31867;&#22411;&#21487;&#33021;&#24102;&#26469;&#30340;&#36127;&#38754;&#25928;&#26524;&#65292;&#36825;&#20123;&#32479;&#35745;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#19982;&#23427;&#20204;&#21508;&#33258;&#30340;&#35282;&#33394;&#26368;&#20339;&#21305;&#37197;&#12290;&#36890;&#36807;&#20351;&#29992;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;[CLS]&#23884;&#20837;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#22659;&#20449;&#24687;&#65292;&#24182;&#22312;&#20854;&#38750;&#21508;&#21521;&#21516;&#24615;&#31354;&#38388;&#20013;&#20998;&#24067;&#26356;&#22343;&#21248;&#12290;&#24403;&#29992;&#36825;&#20004;&#20010;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#26367;&#25442;&#24120;&#35268;&#30340;&#24402;&#19968;&#21270;&#23618;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24179;&#22343;&#24615;&#33021;&#25552;&#21319;&#20102;2.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised training methods for transformers have demonstrated remarkable performance across various domains. Previous transformer-based models, such as masked autoencoders (MAE), typically utilize a single normalization layer for both the [CLS] symbol and the tokens. We propose in this paper a simple modification that employs separate normalization layers for the tokens and the [CLS] symbol to better capture their distinct characteristics and enhance downstream task performance. Our method aims to alleviate the potential negative effects of using the same normalization statistics for both token types, which may not be optimally aligned with their individual roles. We empirically show that by utilizing a separate normalization layer, the [CLS] embeddings can better encode the global contextual information and are distributed more uniformly in its anisotropic space. When replacing the conventional normalization layer with the two separate layers, we observe an average 2.7% performa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26174;&#33879;&#22270;&#20013;&#32771;&#34385;&#26799;&#24230;&#31526;&#21495;&#21644;&#24433;&#21709;&#30340;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#26356;&#22909;&#22320;&#35782;&#21035;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20851;&#27880;&#30340;&#22270;&#20687;&#20687;&#32032;&#30340;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#36974;&#25377;&#25110;&#25913;&#21464;&#36825;&#20123;&#20687;&#32032;&#20250;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.12913</link><description>&lt;p&gt;
&#24577;&#24230;&#38382;&#39064;&#65306;&#20851;&#27880;&#31215;&#26497;&#21644;&#20027;&#21160;&#26799;&#24230;&#26469;&#25552;&#21319;&#26174;&#33879;&#22270;
&lt;/p&gt;
&lt;p&gt;
A matter of attitude: Focusing on positive and active gradients to boost saliency maps. (arXiv:2309.12913v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26174;&#33879;&#22270;&#20013;&#32771;&#34385;&#26799;&#24230;&#31526;&#21495;&#21644;&#24433;&#21709;&#30340;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#26356;&#22909;&#22320;&#35782;&#21035;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20851;&#27880;&#30340;&#22270;&#20687;&#20687;&#32032;&#30340;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#36974;&#25377;&#25110;&#25913;&#21464;&#36825;&#20123;&#20687;&#32032;&#20250;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#25552;&#20379;&#30340;&#35265;&#35299;&#36136;&#37327;&#65292;&#26174;&#33879;&#22270;&#24050;&#25104;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#35265;&#35299;&#26159;&#21542;&#21487;&#20449;&#20173;&#23384;&#22312;&#19968;&#20123;&#30097;&#38382;&#65292;&#26159;&#21542;&#30495;&#27491;&#20195;&#34920;CNN&#29992;&#20110;&#39044;&#27979;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#20174;&#26174;&#33879;&#22270;&#20013;&#25405;&#25937;&#26799;&#24230;&#30340;&#31526;&#21495;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#22810;&#31867;&#21035;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#21644;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;CNN&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#32771;&#34385;&#27491;&#30830;&#31867;&#21035;&#30340;&#31526;&#21495;&#21644;&#24433;&#21709;&#65292;&#20197;&#21450;&#20854;&#20182;&#31867;&#21035;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#32593;&#32476;&#30495;&#27491;&#20851;&#27880;&#30340;&#22270;&#20687;&#20687;&#32032;&#12290;&#27492;&#22806;&#65292;&#36974;&#25377;&#25110;&#25913;&#21464;&#36825;&#20123;&#20687;&#32032;&#39044;&#35745;&#20250;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#20063;&#21464;&#24471;&#26356;&#21152;&#28165;&#26224;&#12290;
&lt;/p&gt;
&lt;p&gt;
Saliency maps have become one of the most widely used interpretability techniques for convolutional neural networks (CNN) due to their simplicity and the quality of the insights they provide. However, there are still some doubts about whether these insights are a trustworthy representation of what CNNs use to come up with their predictions. This paper explores how rescuing the sign of the gradients from the saliency map can lead to a deeper understanding of multi-class classification problems. Using both pretrained and trained from scratch CNNs we unveil that considering the sign and the effect not only of the correct class, but also the influence of the other classes, allows to better identify the pixels of the image that the network is really focusing on. Furthermore, how occluding or altering those pixels is expected to affect the outcome also becomes clearer.
&lt;/p&gt;</description></item><item><title>KG-MDL&#26159;&#19968;&#31181;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#25366;&#25496;&#22270;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#22522;&#20110;MDL&#21407;&#21017;&#65292;&#35299;&#20915;&#20102;&#27169;&#24335;&#29190;&#28856;&#38382;&#39064;&#65292;&#36866;&#24212;&#20102;KGs&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.12908</link><description>&lt;p&gt;
KG-MDL&#65306;&#20351;&#29992;MDL&#21407;&#21017;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#25366;&#25496;&#22270;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
KG-MDL: Mining Graph Patterns in Knowledge Graphs with the MDL Principle. (arXiv:2309.12908v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12908
&lt;/p&gt;
&lt;p&gt;
KG-MDL&#26159;&#19968;&#31181;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#25366;&#25496;&#22270;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#22522;&#20110;MDL&#21407;&#21017;&#65292;&#35299;&#20915;&#20102;&#27169;&#24335;&#29190;&#28856;&#38382;&#39064;&#65292;&#36866;&#24212;&#20102;KGs&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#20197;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30340;&#24418;&#24335;&#21487;&#29992;&#12290;&#34429;&#28982;&#36825;&#31181;&#25968;&#25454;&#27169;&#22411;&#25903;&#25345;&#39640;&#32423;&#25512;&#29702;&#21644;&#26597;&#35810;&#65292;&#20294;&#30001;&#20110;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#20173;&#28982;&#24456;&#38590;&#36827;&#34892;&#25366;&#25496;&#12290;&#22270;&#25366;&#25496;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#20174;KGs&#20013;&#25552;&#21462;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22270;&#25366;&#25496;&#26041;&#27861;&#24448;&#24448;&#20250;&#25552;&#21462;&#20986;&#36807;&#22810;&#30340;&#27169;&#24335;&#65292;&#20197;&#33267;&#20110;&#20154;&#31867;&#20998;&#26512;&#24072;&#38590;&#20197;&#35299;&#37322;&#65288;&#27169;&#24335;&#29190;&#28856;&#65289;&#12290;&#20854;&#27425;&#65292;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;KGs&#24448;&#24448;&#19982;&#24120;&#35268;&#22270;&#25366;&#25496;&#22788;&#29702;&#30340;&#22270;&#19981;&#21516;&#65306;&#23427;&#20204;&#26159;&#22810;&#22270;&#65292;&#20854;&#39030;&#28857;&#24230;&#25968;&#24448;&#24448;&#36981;&#24490;&#24130;&#24459;&#65292;&#20197;&#21450;&#23427;&#20204;&#24314;&#27169;&#30693;&#35782;&#30340;&#26041;&#24335;&#21487;&#33021;&#20250;&#20135;&#29983;&#34394;&#20551;&#27169;&#24335;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21517;&#20026;GraphMDL+&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#27169;&#24335;&#29190;&#28856;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#65288;MDL&#65289;&#21407;&#21017;&#12290;&#28982;&#32780;&#65292;GraphMDL+&#21644;&#20854;&#20182;&#22270;&#25366;&#25496;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#26410;&#32463;&#20462;&#27491;&#30340;KGs&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MDL&#21407;&#21017;&#30340;KG-MDL&#22270;&#27169;&#24335;&#25366;&#25496;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, increasingly more data are available as knowledge graphs (KGs). While this data model supports advanced reasoning and querying, they remain difficult to mine due to their size and complexity. Graph mining approaches can be used to extract patterns from KGs. However this presents two main issues. First, graph mining approaches tend to extract too many patterns for a human analyst to interpret (pattern explosion). Second, real-life KGs tend to differ from the graphs usually treated in graph mining: they are multigraphs, their vertex degrees tend to follow a power-law, and the way in which they model knowledge can produce spurious patterns. Recently, a graph mining approach named GraphMDL+ has been proposed to tackle the problem of pattern explosion, using the Minimum Description Length (MDL) principle. However, GraphMDL+, like other graph mining approaches, is not suited for KGs without adaptations. In this paper we propose KG-MDL, a graph pattern mining approach based on the M
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#21152;&#24378;&#21305;&#37197;&#26694;&#26550;&#65288;ProtoEM&#65289;&#29992;&#20110;&#32852;&#21512;&#25277;&#21462;&#22810;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33719;&#21462;&#27599;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#20851;&#31995;&#30340;&#21407;&#22411;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21305;&#37197;&#36825;&#20123;&#20851;&#31995;&#65292;&#20174;&#32780;&#20840;&#38754;&#29702;&#35299;&#23427;&#20204;&#30340;&#20869;&#22312;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.12892</link><description>&lt;p&gt;
ProtoEM&#65306;&#19968;&#31181;&#29992;&#20110;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#30340;&#21407;&#22411;&#21152;&#24378;&#21305;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ProtoEM: A Prototype-Enhanced Matching Framework for Event Relation Extraction. (arXiv:2309.12892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#21152;&#24378;&#21305;&#37197;&#26694;&#26550;&#65288;ProtoEM&#65289;&#29992;&#20110;&#32852;&#21512;&#25277;&#21462;&#22810;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33719;&#21462;&#27599;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#20851;&#31995;&#30340;&#21407;&#22411;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21305;&#37197;&#36825;&#20123;&#20851;&#31995;&#65292;&#20174;&#32780;&#20840;&#38754;&#29702;&#35299;&#23427;&#20204;&#30340;&#20869;&#22312;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#65288;ERE&#65289;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#22810;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#21333;&#29420;&#23558;&#20107;&#20214;&#20851;&#31995;&#20998;&#31867;&#20026;&#19981;&#21516;&#31867;&#21035;&#65292;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#36825;&#20123;&#20851;&#31995;&#30340;&#20869;&#22312;&#35821;&#20041;&#12290;&#20026;&#20102;&#20840;&#38754;&#29702;&#35299;&#23427;&#20204;&#30340;&#20869;&#22312;&#35821;&#20041;&#65292;&#26412;&#25991;&#38024;&#23545;&#27599;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#20851;&#31995;&#33719;&#21462;&#21407;&#22411;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#21152;&#24378;&#21305;&#37197;&#65288;ProtoEM&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#25277;&#21462;&#22810;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ProtoEM&#20197;&#20004;&#27493;&#26041;&#24335;&#25552;&#21462;&#20107;&#20214;&#20851;&#31995;&#65292;&#21363;&#21407;&#22411;&#34920;&#31034;&#21644;&#21407;&#22411;&#21305;&#37197;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#20026;&#20102;&#25429;&#25417;&#19981;&#21516;&#20107;&#20214;&#20851;&#31995;&#30340;&#20869;&#28085;&#65292;ProtoEM&#21033;&#29992;&#31034;&#20363;&#26469;&#34920;&#31034;&#19982;&#36825;&#20123;&#20851;&#31995;&#30456;&#23545;&#24212;&#30340;&#21407;&#22411;&#12290;&#38543;&#21518;&#65292;&#20026;&#20102;&#25429;&#25417;&#20107;&#20214;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#65292;&#23427;&#20026;&#19982;&#36825;&#20123;&#20851;&#31995;&#30456;&#23545;&#24212;&#30340;&#21407;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#20381;&#36182;&#22270;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event Relation Extraction (ERE) aims to extract multiple kinds of relations among events in texts. However, existing methods singly categorize event relations as different classes, which are inadequately capturing the intrinsic semantics of these relations. To comprehensively understand their intrinsic semantics, in this paper, we obtain prototype representations for each type of event relation and propose a Prototype-Enhanced Matching (ProtoEM) framework for the joint extraction of multiple kinds of event relations. Specifically, ProtoEM extracts event relations in a two-step manner, i.e., prototype representing and prototype matching. In the first step, to capture the connotations of different event relations, ProtoEM utilizes examples to represent the prototypes corresponding to these relations. Subsequently, to capture the interdependence among event relations, it constructs a dependency graph for the prototypes corresponding to these relations and utilized a Graph Neural Network (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#21147;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#31227;&#21160;&#30340;&#37325;&#21147;&#28857;&#38170;&#28857;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#23567;&#30149;&#28790;&#26816;&#27979;&#65292;&#20851;&#20110;&#35813;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#33391;&#22909;&#30340;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.12876</link><description>&lt;p&gt;
&#37325;&#21147;&#32593;&#32476;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#23567;&#30149;&#28790;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Gravity Network for end-to-end small lesion detection. (arXiv:2309.12876v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#21147;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#31227;&#21160;&#30340;&#37325;&#21147;&#28857;&#38170;&#28857;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#23567;&#30149;&#28790;&#26816;&#27979;&#65292;&#20851;&#20110;&#35813;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#33391;&#22909;&#30340;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#31471;&#21040;&#31471;&#26816;&#27979;&#22120;&#65292;&#19987;&#38376;&#29992;&#20110;&#26816;&#27979;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#23567;&#30149;&#28790;&#12290;&#30001;&#20110;&#23567;&#30149;&#28790;&#30340;&#22806;&#35266;&#21644;&#22810;&#26679;&#24615;&#30340;&#32972;&#26223;&#29615;&#22659;&#65292;&#31934;&#30830;&#23450;&#20301;&#23567;&#30149;&#28790;&#23384;&#22312;&#19968;&#23450;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#20687;&#32032;&#30340;&#38170;&#28857;&#65292;&#23427;&#20250;&#21160;&#24577;&#22320;&#38752;&#36817;&#30446;&#26631;&#30149;&#28790;&#36827;&#34892;&#26816;&#27979;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26032;&#30340;&#26550;&#26500;&#31216;&#20026;GravityNet&#65292;&#23558;&#36825;&#20123;&#26032;&#22411;&#38170;&#28857;&#31216;&#20026;&#37325;&#21147;&#28857;&#65292;&#22240;&#20026;&#23427;&#20204;&#20284;&#20046;&#34987;&#30149;&#28790;&#25152;&#8220;&#21560;&#24341;&#8221;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24050;&#32463;&#25104;&#29087;&#30340;&#21307;&#23398;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#36825;&#20004;&#20010;&#38382;&#39064;&#20998;&#21035;&#28041;&#21450;&#25968;&#23383;&#20083;&#25151;X&#32447;&#29031;&#29255;&#20013;&#30340;&#24494;&#38041;&#21270;&#26816;&#27979;&#21644;&#25968;&#23383;&#30524;&#24213;&#22270;&#20687;&#20013;&#24494;&#21160;&#33033;&#30244;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20123;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#26816;&#27979;&#23567;&#30149;&#28790;&#65292;&#23637;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel one-stage end-to-end detector specifically designed to detect small lesions in medical images. Precise localization of small lesions presents challenges due to their appearance and the diverse contextual backgrounds in which they are found. To address this, our approach introduces a new type of pixel-based anchor that dynamically moves towards the targeted lesion for detection. We refer to this new architecture as GravityNet, and the novel anchors as gravity points since they appear to be "attracted" by the lesions. We conducted experiments on two well-established medical problems involving small lesions to evaluate the performance of the proposed approach: microcalcifications detection in digital mammograms and microaneurysms detection in digital fundus images. Our method demonstrates promising results in effectively detecting small lesions in these medical imaging tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#24555;&#36895;&#30340;&#21387;&#32553;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#21387;&#32553;&#22495;&#36827;&#34892;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#24103;&#37319;&#26679;&#21644;&#20887;&#20313;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.12867</link><description>&lt;p&gt;
&#20934;&#30830;&#24555;&#36895;&#30340;&#21387;&#32553;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Accurate and Fast Compressed Video Captioning. (arXiv:2309.12867v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12867
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#24555;&#36895;&#30340;&#21387;&#32553;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#21387;&#32553;&#22495;&#36827;&#34892;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#24103;&#37319;&#26679;&#21644;&#20887;&#20313;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20174;&#35299;&#30721;&#35270;&#39057;&#20013;&#39318;&#20808;&#37319;&#26679;&#35270;&#39057;&#24103;&#65292;&#28982;&#21518;&#36827;&#34892;&#21518;&#32493;&#22788;&#29702;&#65288;&#20363;&#22914;&#65292;&#29305;&#24449;&#25552;&#21462;&#21644;&#23383;&#24149;&#27169;&#22411;&#23398;&#20064;&#65289;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25163;&#21160;&#24103;&#37319;&#26679;&#21487;&#33021;&#20250;&#24573;&#30053;&#35270;&#39057;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20174;&#32780;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#37319;&#26679;&#24103;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#21487;&#33021;&#23548;&#33268;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#25512;&#29702;&#30340;&#25928;&#29575;&#20302;&#19979;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#21387;&#32553;&#22495;&#30340;&#19981;&#21516;&#35282;&#24230;&#30740;&#31350;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#65292;&#36825;&#31181;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#30340;&#27969;&#27700;&#32447;&#20855;&#26377;&#22810;&#37325;&#20248;&#21183;&#65306;1&#65289;&#19982;&#35299;&#30721;&#35270;&#39057;&#30340;&#21407;&#22987;&#22270;&#20687;&#30456;&#27604;&#65292;&#30001;I-&#24103;&#12289;&#36816;&#21160;&#30690;&#37327;&#21644;&#27531;&#24046;&#26500;&#25104;&#30340;&#21387;&#32553;&#35270;&#39057;&#26356;&#20855;&#21487;&#35782;&#21035;&#24615;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#19987;&#29992;&#27169;&#22411;&#35774;&#35745;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20351;&#29992;&#25972;&#20010;&#35270;&#39057;&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#37319;&#26679;&#65307;2&#65289;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#29702;&#25928;&#29575;&#26356;&#39640;&#65292;&#22240;&#20026;&#22788;&#29702;&#30340;&#20449;&#24687;&#26356;&#23569;&#19988;&#26356;&#23569;&#20887;&#20313;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing video captioning approaches typically require to first sample video frames from a decoded video and then conduct a subsequent process (e.g., feature extraction and/or captioning model learning). In this pipeline, manual frame sampling may ignore key information in videos and thus degrade performance. Additionally, redundant information in the sampled frames may result in low efficiency in the inference of video captioning. Addressing this, we study video captioning from a different perspective in compressed domain, which brings multi-fold advantages over the existing pipeline: 1) Compared to raw images from the decoded video, the compressed video, consisting of I-frames, motion vectors and residuals, is highly distinguishable, which allows us to leverage the entire video for learning without manual sampling through a specialized model design; 2) The captioning model is more efficient in inference as smaller and less redundant information is processed. We propose a simple yet e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#39046;&#22495;&#36866;&#24212;&#23545;&#38463;&#25289;&#20271;&#26426;&#22120;&#32763;&#35793;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#24179;&#34892;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#35797;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#23545;&#20110;&#25552;&#39640;&#38463;&#25289;&#20271;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#20855;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.12863</link><description>&lt;p&gt;
&#38024;&#23545;&#38463;&#25289;&#20271;&#37329;&#34701;&#25991;&#26412;&#30340;&#39046;&#22495;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation for Arabic Machine Translation: The Case of Financial Texts. (arXiv:2309.12863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12863
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#39046;&#22495;&#36866;&#24212;&#23545;&#38463;&#25289;&#20271;&#26426;&#22120;&#32763;&#35793;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#24179;&#34892;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#35797;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#23545;&#20110;&#25552;&#39640;&#38463;&#25289;&#20271;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#20855;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#26102;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#30340;NMT&#31995;&#32479;&#22312;&#39046;&#22495;&#22806;&#30340;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#24120;&#27604;&#36890;&#29992;&#30340;NMT&#31995;&#32479;&#26377;&#26356;&#22909;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#23613;&#31649;&#22312;NMT&#30340;&#33521;&#35821;&#21644;&#20854;&#20182;&#27431;&#27954;&#35821;&#35328;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#25345;&#32493;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#30340;&#39046;&#22495;&#36866;&#24212;&#22312;&#25991;&#29486;&#20013;&#21463;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#38463;&#25289;&#20271;&#26426;&#22120;&#32763;&#35793;&#65288;AMT&#65289;&#39046;&#22495;&#29305;&#23450;&#36866;&#24212;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23578;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#20013;&#65292;&#22914;&#37329;&#34701;&#26032;&#38395;&#25991;&#31456;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#31934;&#24515;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;-&#33521;&#35821;&#65288;AR-EN&#65289;&#37329;&#34701;&#39046;&#22495;&#32763;&#35793;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#20197;&#35780;&#20272;&#19981;&#21516;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#20960;&#20010;&#39044;&#35757;&#32451;&#30340;NMT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#21253;&#25324;ChatGPT-3.5 Turbo&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation (NMT) has shown impressive performance when trained on large-scale corpora. However, generic NMT systems have demonstrated poor performance on out-of-domain translation. To mitigate this issue, several domain adaptation methods have recently been proposed which often lead to better translation quality than genetic NMT systems. While there has been some continuous progress in NMT for English and other European languages, domain adaption in Arabic has received little attention in the literature. The current study, therefore, aims to explore the effectiveness of domain-specific adaptation for Arabic MT (AMT), in yet unexplored domain, financial news articles. To this end, we developed carefully a parallel corpus for Arabic-English (AR- EN) translation in the financial domain for benchmarking different domain adaptation methods. We then fine-tuned several pre-trained NMT and Large Language models including ChatGPT-3.5 Turbo on our dataset. The results showed that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#30340;&#25193;&#25955;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22686;&#24378;&#25968;&#25454;&#38598;&#65292;&#30452;&#25509;&#29992;&#20110;&#35757;&#32451;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#38271;&#23614;&#29992;&#25143;&#21644;&#25968;&#25454;&#31232;&#30095;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12858</link><description>&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#30340;&#25193;&#25955;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Diffusion Augmentation for Sequential Recommendation. (arXiv:2309.12858v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#30340;&#25193;&#25955;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22686;&#24378;&#25968;&#25454;&#38598;&#65292;&#30452;&#25509;&#29992;&#20110;&#35757;&#32451;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#38271;&#23614;&#29992;&#25143;&#21644;&#25968;&#25454;&#31232;&#30095;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#39034;&#24207;&#25512;&#33616;&#65288;SRS&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#24212;&#29992;&#30340;&#25216;&#26415;&#22522;&#30784;&#65292;&#20854;&#30446;&#26631;&#26159;&#22522;&#20110;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#26469;&#25512;&#33616;&#19979;&#19968;&#20010;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#39034;&#24207;&#25512;&#33616;&#32463;&#24120;&#38754;&#20020;&#25968;&#25454;&#31232;&#30095;&#30340;&#38382;&#39064;&#65292;&#36825;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29992;&#25143;&#21482;&#19982;&#23569;&#25968;&#39033;&#30446;&#36827;&#34892;&#20132;&#20114;&#65292;&#20294;&#29616;&#26377;&#30340;SRS&#27169;&#22411;&#36890;&#24120;&#24615;&#33021;&#19981;&#20339;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#38271;&#23614;&#29992;&#25143;&#38382;&#39064;&#65292;&#20173;&#24453;&#35299;&#20915;&#12290;&#25968;&#25454;&#22686;&#24378;&#26159;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#21046;&#36896;&#35757;&#32451;&#31574;&#30053;&#25110;&#21463;&#21040;&#36136;&#37327;&#19981;&#20339;&#30340;&#29983;&#25104;&#20132;&#20114;&#30340;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#30340;&#25193;&#25955;&#22686;&#24378;&#65288;DiffuASR&#65289;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#12290;DiffuASR&#36890;&#36807;&#25193;&#25955;&#20135;&#29983;&#30340;&#22686;&#24378;&#25968;&#25454;&#38598;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#35757;&#32451;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#65292;&#20813;&#21435;&#20102;&#22797;&#26434;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation (SRS) has become the technical foundation in many applications recently, which aims to recommend the next item based on the user's historical interactions. However, sequential recommendation often faces the problem of data sparsity, which widely exists in recommender systems. Besides, most users only interact with a few items, but existing SRS models often underperform these users. Such a problem, named the long-tail user problem, is still to be resolved. Data augmentation is a distinct way to alleviate these two problems, but they often need fabricated training strategies or are hindered by poor-quality generated interactions. To address these problems, we propose a Diffusion Augmentation for Sequential Recommendation (DiffuASR) for a higher quality generation. The augmented dataset by DiffuASR can be used to train the sequential recommendation models directly, free from complex training procedures. To make the best of the generation ability of the diffusion 
&lt;/p&gt;</description></item><item><title>&#36817;&#20284;&#35745;&#31639;&#20316;&#20026;&#20302;&#25104;&#26412;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#20854;&#20013;&#65292;&#35774;&#35745;&#29305;&#23450;&#20110;&#24179;&#21488;&#30340;&#36817;&#20284;&#31639;&#26415;&#36816;&#31639;&#31526;&#25104;&#20026;&#20027;&#35201;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20195;&#29702;&#20989;&#25968;&#39044;&#27979;&#24615;&#33021;&#21644;&#34892;&#20026;&#24433;&#21709;&#65292;&#32570;&#20047;&#26356;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.12830</link><description>&lt;p&gt;
AxOCS: &#20351;&#29992;&#37197;&#32622;&#36229;&#37319;&#26679;&#26469;&#25193;&#23637;&#22522;&#20110;FPGA&#30340;&#36817;&#20284;&#36816;&#31639;&#31526;
&lt;/p&gt;
&lt;p&gt;
AxOCS: Scaling FPGA-based Approximate Operators using Configuration Supersampling. (arXiv:2309.12830v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12830
&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#35745;&#31639;&#20316;&#20026;&#20302;&#25104;&#26412;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#20854;&#20013;&#65292;&#35774;&#35745;&#29305;&#23450;&#20110;&#24179;&#21488;&#30340;&#36817;&#20284;&#31639;&#26415;&#36816;&#31639;&#31526;&#25104;&#20026;&#20027;&#35201;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20195;&#29702;&#20989;&#25968;&#39044;&#27979;&#24615;&#33021;&#21644;&#34892;&#20026;&#24433;&#21709;&#65292;&#32570;&#20047;&#26356;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22788;&#29702;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#20351;&#29992;&#26085;&#30410;&#22686;&#21152;&#65292;&#21152;&#21095;&#20102;&#23545;&#20302;&#25104;&#26412;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#30340;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#32780;&#35328;&#12290;&#20026;&#27492;&#65292;&#36817;&#20284;&#35745;&#31639;&#20316;&#20026;&#19968;&#31181;&#25506;&#32034;&#21151;&#32791;&#12289;&#24615;&#33021;&#12289;&#38754;&#31215;&#65288;PPA&#65289;&#21644;&#34892;&#20026;&#20934;&#30830;&#24615;&#65288;BEHAV&#65289;&#20043;&#38388;&#26435;&#34913;&#30340;&#26041;&#27861;&#65292;&#24050;&#32463;&#25104;&#20026;&#22312;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#23454;&#29616;&#30340;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30001;&#20110;MAC&#25805;&#20316;&#30340;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#35774;&#35745;&#29305;&#23450;&#20110;&#24179;&#21488;&#30340;&#36817;&#20284;&#31639;&#26415;&#36816;&#31639;&#31526;&#24418;&#25104;&#20102;&#36817;&#20284;&#35745;&#31639;&#20013;&#30340;&#20027;&#35201;&#30740;&#31350;&#38382;&#39064;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#22522;&#20110;AI/ML&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#25216;&#26415;&#34987;&#29992;&#20110;&#23454;&#29616;&#36817;&#20284;&#36816;&#31639;&#31526;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20165;&#38480;&#20110;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20195;&#29702;&#20989;&#25968;&#26469;&#39044;&#27979;&#19968;&#32452;&#30456;&#20851;&#35774;&#35745;&#20915;&#31574;&#30340;PPA&#21644;BEHAV&#24433;&#21709;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22238;&#24402;&#33021;&#21147;&#65292;&#20294;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#26356;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rising usage of AI and ML-based processing across application domains has exacerbated the need for low-cost ML implementation, specifically for resource-constrained embedded systems. To this end, approximate computing, an approach that explores the power, performance, area (PPA), and behavioral accuracy (BEHAV) trade-offs, has emerged as a possible solution for implementing embedded machine learning. Due to the predominance of MAC operations in ML, designing platform-specific approximate arithmetic operators forms one of the major research problems in approximate computing. Recently there has been a rising usage of AI/ML-based design space exploration techniques for implementing approximate operators. However, most of these approaches are limited to using ML-based surrogate functions for predicting the PPA and BEHAV impact of a set of related design decisions. While this approach leverages the regression capabilities of ML methods, it does not exploit the more advanced approaches i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20998;&#21106;&#27169;&#22411;&#65288;VLSM&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#21512;&#25104;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#20998;&#21106;&#27169;&#22411;&#30340;&#25351;&#26631;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.12829</link><description>&lt;p&gt;
&#21512;&#25104;&#25552;&#21319;&#65306;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography. (arXiv:2309.12829v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20998;&#21106;&#27169;&#22411;&#65288;VLSM&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#21512;&#25104;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#20998;&#21106;&#27169;&#22411;&#30340;&#25351;&#26631;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20998;&#21106;&#23545;&#20110;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#24515;&#34880;&#31649;&#30142;&#30149;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36229;&#22768;&#22270;&#20687;&#30340;&#21464;&#24322;&#24615;&#21644;&#22266;&#26377;&#25361;&#25112;&#38459;&#30861;&#20102;&#31934;&#30830;&#30340;&#20998;&#21106;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#32852;&#21512;&#34920;&#31034;&#65292;&#35270;&#35273;-&#35821;&#35328;&#20998;&#21106;&#27169;&#22411;&#65288;VLSM&#65289;&#21487;&#20197;&#34701;&#20837;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#32570;&#20047;&#29616;&#25104;&#30340;&#25968;&#25454;&#38459;&#30861;&#20102;VLSM&#30340;&#35757;&#32451;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#35821;&#20041;&#25193;&#25955;&#27169;&#22411;&#65288;SDM&#65289;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#30340;VLSM&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#12289;&#20998;&#21106;&#25513;&#27169;&#21644;&#20803;&#25968;&#25454;&#20013;&#33258;&#21160;&#25552;&#21462;&#30340;&#22810;&#20010;&#23646;&#24615;&#23548;&#20986;&#30340;&#19971;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#25552;&#31034;&#26469;&#35780;&#20272;&#20004;&#20010;&#27969;&#34892;&#30340;VLSM&#27169;&#22411;&#65288;CLIPSeg&#21644;CRIS&#65289;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#39044;&#35757;&#32451;VLSM&#26102;&#65292;&#36716;&#25442;&#21644;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate segmentation is essential for echocardiography-based assessment of cardiovascular diseases (CVDs). However, the variability among sonographers and the inherent challenges of ultrasound images hinder precise segmentation. By leveraging the joint representation of image and text modalities, Vision-Language Segmentation Models (VLSMs) can incorporate rich contextual information, potentially aiding in accurate and explainable segmentation. However, the lack of readily available data in echocardiography hampers the training of VLSMs. In this study, we explore using synthetic datasets from Semantic Diffusion Models (SDMs) to enhance VLSMs for echocardiography segmentation. We evaluate results for two popular VLSMs (CLIPSeg and CRIS) using seven different kinds of language prompts derived from several attributes, automatically extracted from echocardiography images, segmentation masks, and their metadata. Our results show improved metrics and faster convergence when pretraining VLSMs
&lt;/p&gt;</description></item><item><title>OmniDrones&#26159;&#19968;&#20010;&#19987;&#20026;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#28789;&#27963;&#24179;&#21488;&#65292;&#37319;&#29992;&#33258;&#19979;&#32780;&#19978;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#19968;&#31995;&#21015;&#22522;&#20934;&#20219;&#21153;&#21644;&#26080;&#20154;&#26426;&#23398;&#20064;&#24037;&#20855;&#12290;&#36825;&#20010;&#24179;&#21488;&#26377;&#21161;&#20110;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#23454;&#38469;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2309.12825</link><description>&lt;p&gt;
OmniDrones&#65306;&#19968;&#31181;&#29992;&#20110;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#30340;&#39640;&#25928;&#28789;&#27963;&#30340;&#24378;&#21270;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
OmniDrones: An Efficient and Flexible Platform for Reinforcement Learning in Drone Control. (arXiv:2309.12825v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12825
&lt;/p&gt;
&lt;p&gt;
OmniDrones&#26159;&#19968;&#20010;&#19987;&#20026;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#28789;&#27963;&#24179;&#21488;&#65292;&#37319;&#29992;&#33258;&#19979;&#32780;&#19978;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#19968;&#31995;&#21015;&#22522;&#20934;&#20219;&#21153;&#21644;&#26080;&#20154;&#26426;&#23398;&#20064;&#24037;&#20855;&#12290;&#36825;&#20010;&#24179;&#21488;&#26377;&#21161;&#20110;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#23454;&#38469;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OmniDrones&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#28789;&#27963;&#30340;&#24179;&#21488;&#65292;&#24314;&#31435;&#22312;Nvidia&#30340;Omniverse Isaac Sim&#19978;&#12290;&#23427;&#37319;&#29992;&#33258;&#19979;&#32780;&#19978;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#22320;&#35774;&#35745;&#21644;&#23454;&#39564;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#22312;GPU&#24182;&#34892;&#21270;&#20223;&#30495;&#20043;&#19978;&#36827;&#34892;&#27169;&#25311;&#12290;&#23427;&#36824;&#25552;&#20379;&#19968;&#31995;&#21015;&#22522;&#20934;&#20219;&#21153;&#65292;&#28085;&#30422;&#21333;&#20010;&#26080;&#20154;&#26426;&#24748;&#20572;&#21040;&#22810;&#39537;&#21160;&#31995;&#32479;&#36319;&#36394;&#31561;&#21508;&#31181;&#25361;&#25112;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#26080;&#20154;&#26426;&#20223;&#30495;&#24179;&#21488;&#65292;&#37197;&#22791;&#20102;&#19968;&#22871;&#29992;&#20110;&#26080;&#20154;&#26426;&#23398;&#20064;&#30340;&#24037;&#20855;&#12290;&#23427;&#21253;&#25324;4&#20010;&#26080;&#20154;&#26426;&#27169;&#22411;&#65292;5&#31181;&#20256;&#24863;&#22120;&#27169;&#24335;&#65292;4&#31181;&#25511;&#21046;&#27169;&#24335;&#65292;10&#22810;&#20010;&#22522;&#20934;&#20219;&#21153;&#20197;&#21450;&#19968;&#20123;&#24120;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;OmniDrones&#30340;&#33021;&#21147;&#24182;&#25903;&#25345;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#36825;&#20123;&#22522;&#20934;&#20219;&#21153;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#24179;&#21488;&#33021;&#22815;&#20419;&#36827;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#23454;&#38469;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce OmniDrones, an efficient and flexible platform tailored for reinforcement learning in drone control, built on Nvidia's Omniverse Isaac Sim. It employs a bottom-up design approach that allows users to easily design and experiment with various application scenarios on top of GPU-parallelized simulations. It also offers a range of benchmark tasks, presenting challenges ranging from single-drone hovering to over-actuated system tracking. In summary, we propose an open-sourced drone simulation platform, equipped with an extensive suite of tools for drone learning. It includes 4 drone models, 5 sensor modalities, 4 control modes, over 10 benchmark tasks, and a selection of widely used RL baselines. To showcase the capabilities of OmniDrones and to support future research, we also provide preliminary results on these benchmark tasks. We hope this platform will encourage further studies on applying RL to practical drone systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20809;&#35889;&#29702;&#35770;&#65292;&#29992;&#20110;&#29702;&#35299;&#31070;&#32463;&#39044;&#27979;&#21644;&#23545;&#40784;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#28608;&#27963;&#30340;&#20809;&#35889;&#20559;&#24046;&#21644;&#31070;&#32463;&#21709;&#24212;&#19982;&#21487;&#23398;&#20064;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#23646;&#24615;&#65292;&#24182;&#21306;&#20998;&#22312;&#31070;&#32463;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#30456;&#21516;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.12821</link><description>&lt;p&gt;
&#31070;&#32463;&#39044;&#27979;&#21644;&#23545;&#40784;&#30340;&#20809;&#35889;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Spectral Theory of Neural Prediction and Alignment. (arXiv:2309.12821v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20809;&#35889;&#29702;&#35770;&#65292;&#29992;&#20110;&#29702;&#35299;&#31070;&#32463;&#39044;&#27979;&#21644;&#23545;&#40784;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#28608;&#27963;&#30340;&#20809;&#35889;&#20559;&#24046;&#21644;&#31070;&#32463;&#21709;&#24212;&#19982;&#21487;&#23398;&#20064;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#23646;&#24615;&#65292;&#24182;&#21306;&#20998;&#22312;&#31070;&#32463;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#30456;&#21516;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#32463;&#24120;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#21709;&#24212;&#19982;&#29983;&#29289;&#31995;&#32479;&#27979;&#24471;&#30340;&#21709;&#24212;&#36827;&#34892;&#22238;&#24402;&#26469;&#36827;&#34892;&#27604;&#36739;&#12290;&#35768;&#22810;&#19981;&#21516;&#30340;&#20808;&#36827;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#20102;&#31867;&#20284;&#30340;&#31070;&#32463;&#39044;&#27979;&#65292;&#20294;&#20173;&#28982;&#19981;&#28165;&#26970;&#22914;&#20309;&#21306;&#20998;&#22312;&#39044;&#27979;&#31070;&#32463;&#21709;&#24212;&#26041;&#38754;&#34920;&#29616;&#30456;&#21516;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#36817;&#26399;&#25552;&#20986;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#19982;&#27169;&#22411;&#28608;&#27963;&#30340;&#20809;&#35889;&#20559;&#24046;&#21644;&#31070;&#32463;&#21709;&#24212;&#19982;&#21487;&#23398;&#20064;&#23376;&#31354;&#38388;&#20043;&#38388;&#30340;&#23545;&#40784;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#29702;&#35770;&#25193;&#23637;&#21040;&#20102;&#27169;&#22411;&#28608;&#27963;&#21644;&#31070;&#32463;&#21709;&#24212;&#20043;&#38388;&#30340;&#22238;&#24402;&#24773;&#20917;&#65292;&#24182;&#23450;&#20041;&#20102;&#25551;&#36848;&#35823;&#24046;&#23884;&#20837;&#20960;&#20309;&#24615;&#36136;&#30340;&#20960;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#22823;&#37327;&#39044;&#27979;&#35270;&#35273;&#30382;&#23618;&#27963;&#21160;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#31181;&#20960;&#20309;&#24418;&#29366;&#23548;&#33268;&#20302;&#31070;&#32463;&#39044;&#27979;&#35823;&#24046;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representations of neural networks are often compared to those of biological systems by performing regression between the neural network responses and those measured from biological systems. Many different state-of-the-art deep neural networks yield similar neural predictions, but it remains unclear how to differentiate among models that perform equally well at predicting neural responses. To gain insight into this, we use a recent theoretical framework that relates the generalization error from regression to the spectral bias of the model activations and the alignment of the neural responses onto the learnable subspace of the model. We extend this theory to the case of regression between model activations and neural responses, and define geometrical properties describing the error embedding geometry. We test a large number of deep neural networks that predict visual cortical activity and show that there are multiple types of geometries that result in low neural prediction error as
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#36974;&#30422;&#25805;&#20316;&#24341;&#20837;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#36974;&#30422;&#25805;&#20316;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#21103;&#20316;&#29992;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12757</link><description>&lt;p&gt;
&#23545;&#20110;ConvNets&#26469;&#35828;&#65292;&#36974;&#30422;&#65288;masking&#65289;&#33021;&#25913;&#21892;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#32780;&#26174;&#33879;&#24615;&#21578;&#35785;&#20320;&#20309;&#22788;&#12290;&#65288;arXiv:2309.12757v1 [cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
Masking Improves Contrastive Self-Supervised Learning for ConvNets, and Saliency Tells You Where. (arXiv:2309.12757v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#36974;&#30422;&#25805;&#20316;&#24341;&#20837;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#36974;&#30422;&#25805;&#20316;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#21103;&#20316;&#29992;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#25968;&#25454;&#24320;&#22987;&#21463;&#30410;&#20110;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#24314;&#31435;&#22312;&#36974;&#30422;&#21644;&#33258;&#37325;&#26500;&#30446;&#26631;&#20043;&#19978;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#20196;&#29260;&#21270;&#31243;&#24207;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#39592;&#24178;&#32467;&#26500;&#30340;&#24341;&#20837;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#22270;&#20687;&#25968;&#25454;&#30340;&#21478;&#19968;&#31181;&#37325;&#35201;&#19988;&#24191;&#27867;&#37319;&#29992;&#30340;&#26550;&#26500;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23613;&#31649;&#20855;&#26377;&#39537;&#21160;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20173;&#28982;&#38754;&#20020;&#23558;&#36825;&#31181;&#30452;&#25509;&#32780;&#36890;&#29992;&#30340;&#36974;&#30422;&#25805;&#20316;&#26174;&#33879;&#22320;&#21033;&#29992;&#20110;&#20854;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20943;&#36731;&#23558;&#36974;&#30422;&#25805;&#20316;&#32435;&#20837;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#30340;&#36127;&#25285;&#65292;&#20316;&#20026;&#19968;&#31181;&#39069;&#22806;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;ConvNets&#20013;&#22240;&#36974;&#32617;&#25805;&#20316;&#32780;&#20135;&#29983;&#30340;&#39069;&#22806;&#36793;&#32536;&#65288;&#36974;&#30422;&#21644;&#26410;&#36974;&#30422;&#21306;&#22495;&#20043;&#38388;&#65289;&#20197;&#21450;&#20854;&#20182;&#19981;&#21033;&#24433;&#21709;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#24050;&#32463;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#35752;&#35770;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;
While image data starts to enjoy the simple-but-effective self-supervised learning scheme built upon masking and self-reconstruction objective thanks to the introduction of tokenization procedure and vision transformer backbone, convolutional neural networks as another important and widely-adopted architecture for image data, though having contrastive-learning techniques to drive the self-supervised learning, still face the difficulty of leveraging such straightforward and general masking operation to benefit their learning process significantly. In this work, we aim to alleviate the burden of including masking operation into the contrastive-learning framework for convolutional neural networks as an extra augmentation method. In addition to the additive but unwanted edges (between masked and unmasked regions) as well as other adverse effects caused by the masking operations for ConvNets, which have been discussed by prior works, we particularly identify the potential problem where for 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24037;&#19994;&#24212;&#29992;&#30340;XAI MLOps&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#37096;&#32626;&#21644;&#31649;&#29702;ML&#27169;&#22411;&#20013;&#30340;&#35299;&#37322;&#21644;&#21453;&#39304;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.12756</link><description>&lt;p&gt;
&#38754;&#21521;&#24037;&#19994;&#24212;&#29992;&#30340;XAI MLOps&#26550;&#26500;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards an MLOps Architecture for XAI in Industrial Applications. (arXiv:2309.12756v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24037;&#19994;&#24212;&#29992;&#30340;XAI MLOps&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#37096;&#32626;&#21644;&#31649;&#29702;ML&#27169;&#22411;&#20013;&#30340;&#35299;&#37322;&#21644;&#21453;&#39304;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#32463;&#25104;&#20026;&#24037;&#19994;&#39046;&#22495;&#20013;&#30340;&#19968;&#31181;&#27969;&#34892;&#24037;&#20855;&#65292;&#23427;&#26377;&#21161;&#20110;&#25913;&#21892;&#25805;&#20316;&#12289;&#22686;&#21152;&#25928;&#29575;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#37096;&#32626;&#21644;&#31649;&#29702;ML&#27169;&#22411;&#21487;&#33021;&#20250;&#24456;&#22797;&#26434;&#12290;&#36825;&#23601;&#26159;&#26426;&#22120;&#23398;&#20064;&#36816;&#33829;&#65288;MLOps&#65289;&#30340;&#20316;&#29992;&#25152;&#22312;&#12290;MLOps&#26088;&#22312;&#31616;&#21270;&#37096;&#32626;&#21644;&#31649;&#29702;&#36807;&#31243;&#12290;&#20854;&#20013;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;MLOps&#25361;&#25112;&#26159;&#23545;&#35299;&#37322;&#30340;&#38656;&#27714;&#12290;&#36825;&#20123;&#35299;&#37322;&#23545;&#20110;&#29702;&#35299;ML&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#23545;&#20110;&#20449;&#20219;&#21644;&#25509;&#21463;&#26159;&#20851;&#38190;&#30340;&#12290;&#26356;&#22909;&#22320;&#35782;&#21035;&#38169;&#35823;&#21644;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21482;&#26159;&#20854;&#20013;&#30340;&#20004;&#20010;&#32467;&#26524;&#20248;&#21183;&#12290;&#19968;&#20010;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#20107;&#23454;&#26159;&#65292;&#24403;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#26159;&#21487;&#35299;&#37322;&#24615;&#19981;&#28385;&#36275;&#29992;&#25143;&#26399;&#26395;&#26102;&#65292;&#37096;&#32626;&#30340;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#20250;&#34987;&#32469;&#36807;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MLOps&#36719;&#20214;&#26550;&#26500;&#65292;&#20197;&#35299;&#20915;&#23558;&#35299;&#37322;&#21644;&#21453;&#39304;&#33021;&#21147;&#25972;&#21512;&#21040;ML&#24320;&#21457;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;&#22312;&#39033;&#30446;EXPLAIN&#20013;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#34987;&#24212;&#29992;&#20110;&#24037;&#19994;&#24212;&#29992;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) has become a popular tool in the industrial sector as it helps to improve operations, increase efficiency, and reduce costs. However, deploying and managing ML models in production environments can be complex. This is where Machine Learning Operations (MLOps) comes in. MLOps aims to streamline this deployment and management process. One of the remaining MLOps challenges is the need for explanations. These explanations are essential for understanding how ML models reason, which is key to trust and acceptance. Better identification of errors and improved model accuracy are only two resulting advantages. An often neglected fact is that deployed models are bypassed in practice when accuracy and especially explainability do not meet user expectations. We developed a novel MLOps software architecture to address the challenge of integrating explanations and feedback capabilities into the ML development and deployment processes. In the project EXPLAIN, our architecture is
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;OpenAi&#30340;GPT4&#20316;&#20026;&#32534;&#30721;&#21161;&#25163;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#27979;&#35797;&#21457;&#29616;&#20854;&#22312;&#22238;&#31572;&#38382;&#39064;&#12289;&#29983;&#25104;&#21487;&#38752;&#20195;&#30721;&#21644;&#36129;&#29486;&#20195;&#30721;&#35843;&#35797;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#34920;&#26126;GPT4&#33021;&#22815;&#25552;&#39640;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#24182;&#37325;&#22609;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.12732</link><description>&lt;p&gt;
OpenAi&#30340;GPT4&#20316;&#20026;&#32534;&#30721;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
OpenAi's GPT4 as coding assistant. (arXiv:2309.12732v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;OpenAi&#30340;GPT4&#20316;&#20026;&#32534;&#30721;&#21161;&#25163;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#27979;&#35797;&#21457;&#29616;&#20854;&#22312;&#22238;&#31572;&#38382;&#39064;&#12289;&#29983;&#25104;&#21487;&#38752;&#20195;&#30721;&#21644;&#36129;&#29486;&#20195;&#30721;&#35843;&#35797;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#34920;&#26126;GPT4&#33021;&#22815;&#25552;&#39640;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#24182;&#37325;&#22609;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;GPT4&#34987;&#35748;&#20026;&#26159;Openai&#26368;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;GPT3.5&#21644;GPT4&#20316;&#20026;&#32534;&#30721;&#21161;&#25163;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#36866;&#24403;&#30340;&#27979;&#35797;&#26469;&#26816;&#26597;&#36825;&#20004;&#20010;&#31995;&#32479;&#26159;&#21542;&#33021;&#22815;a)&#22238;&#31572;&#22312;&#20195;&#30721;&#24320;&#21457;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#20856;&#22411;&#38382;&#39064;&#65292;b)&#29983;&#25104;&#21487;&#38752;&#30340;&#20195;&#30721;&#65292;&#20197;&#21450;c)&#23545;&#20195;&#30721;&#35843;&#35797;&#20570;&#20986;&#36129;&#29486;&#12290;&#27979;&#35797;&#32467;&#26524;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#12290;GPT4&#30340;&#24615;&#33021;&#20986;&#33394;&#65292;&#39044;&#31034;&#30528;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#39640;&#20197;&#21450;&#22522;&#20110;&#36825;&#20123;&#26032;&#24037;&#20855;&#30340;&#36719;&#20214;&#24320;&#21457;&#31243;&#24207;&#30340;&#37325;&#26032;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lately, Large Language Models have been widely used in code generation. GPT4 is considered the most potent Large Language Model from Openai. In this paper, we examine GPT3.5 and GPT4 as coding assistants. More specifically, we have constructed appropriate tests to check whether the two systems can a) answer typical questions that can arise during the code development, b) produce reliable code, and c) contribute to code debugging. The test results are impressive. The performance of GPT4 is outstanding and signals an increase in the productivity of programmers and the reorganization of software development procedures based on these new tools.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#24223;&#24323;&#25512;&#29702;&#30340;&#30452;&#35266;&#31526;&#21495;&#21644;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#23436;&#32654;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#21069;&#30340;&#35770;&#35777;&#29702;&#35770;&#24037;&#20316;&#30456;&#20851;&#32852;&#12290;&#36827;&#19968;&#27493;&#24037;&#20316;&#38656;&#35201;&#22312;&#38472;&#36848;&#24615;&#26415;&#35821;&#20013;&#25551;&#36848;&#25512;&#29702;&#31574;&#30053;&#21644;&#25112;&#26415;&#65292;&#24182;&#32467;&#21512;AIF&#26412;&#20307;&#35770;&#30340;&#21551;&#31034;&#12290;&#35770;&#25991;&#36824;&#35266;&#23519;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#31526;&#21495;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.12731</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#30340;&#21487;&#24223;&#24323;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Defeasible Reasoning with Knowledge Graphs. (arXiv:2309.12731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#24223;&#24323;&#25512;&#29702;&#30340;&#30452;&#35266;&#31526;&#21495;&#21644;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#23436;&#32654;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#21069;&#30340;&#35770;&#35777;&#29702;&#35770;&#24037;&#20316;&#30456;&#20851;&#32852;&#12290;&#36827;&#19968;&#27493;&#24037;&#20316;&#38656;&#35201;&#22312;&#38472;&#36848;&#24615;&#26415;&#35821;&#20013;&#25551;&#36848;&#25512;&#29702;&#31574;&#30053;&#21644;&#25112;&#26415;&#65292;&#24182;&#32467;&#21512;AIF&#26412;&#20307;&#35770;&#30340;&#21551;&#31034;&#12290;&#35770;&#25991;&#36824;&#35266;&#23519;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#31526;&#21495;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30340;&#30693;&#35782;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12289;&#19981;&#31934;&#30830;&#24615;&#12289;&#19981;&#23436;&#25972;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#26085;&#24120;&#26415;&#35821;&#30340;&#21547;&#20041;&#21462;&#20915;&#20110;&#19978;&#19979;&#25991;&#12290;&#36825;&#23545;&#35821;&#20041;&#32593;&#26500;&#25104;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#31526;&#21495;&#21644;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#23436;&#32654;&#30693;&#35782;&#30340;&#21487;&#24223;&#24323;&#25512;&#29702;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#21069;&#20851;&#20110;&#35770;&#35777;&#29702;&#35770;&#30340;&#24037;&#20316;&#32852;&#31995;&#36215;&#26469;&#12290;PKN&#19982;N3&#30340;&#20851;&#31995;&#31867;&#20284;&#20110;&#21487;&#24223;&#24323;&#25512;&#29702;&#19982;&#28436;&#32462;&#36923;&#36753;&#30340;&#20851;&#31995;&#12290;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#20197;&#30452;&#35266;&#30340;&#35821;&#27861;&#25551;&#36848;&#38472;&#36848;&#24615;&#26415;&#35821;&#20013;&#30340;&#25512;&#29702;&#31574;&#30053;&#21644;&#25112;&#26415;&#65292;&#20511;&#37492;AIF&#26412;&#20307;&#35770;&#30340;&#21551;&#31034;&#12290;&#35770;&#25991;&#26368;&#21518;&#35266;&#23519;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#31526;&#21495;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human knowledge is subject to uncertainties, imprecision, incompleteness and inconsistencies. Moreover, the meaning of many everyday terms is dependent on the context. That poses a huge challenge for the Semantic Web. This paper introduces work on an intuitive notation and model for defeasible reasoning with imperfect knowledge, and relates it to previous work on argumentation theory. PKN is to N3 as defeasible reasoning is to deductive logic. Further work is needed on an intuitive syntax for describing reasoning strategies and tactics in declarative terms, drawing upon the AIF ontology for inspiration. The paper closes with observations on symbolic approaches in the era of large language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32842;&#22825;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#24178;&#25200;&#38382;&#39064;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#25345;&#32493;&#27969;&#21160;&#30340;&#20449;&#24687;&#20043;&#38388;&#21487;&#33021;&#20250;&#36973;&#21463;&#24178;&#25200;&#65292;&#23548;&#33268;&#36951;&#24536;&#20043;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#38477;&#20302;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12727</link><description>&lt;p&gt;
&#32842;&#22825;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24178;&#25200;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
In-context Interference in Chat-based Large Language Models. (arXiv:2309.12727v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32842;&#22825;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#24178;&#25200;&#38382;&#39064;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#25345;&#32493;&#27969;&#21160;&#30340;&#20449;&#24687;&#20043;&#38388;&#21487;&#33021;&#20250;&#36973;&#21463;&#24178;&#25200;&#65292;&#23548;&#33268;&#36951;&#24536;&#20043;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#38477;&#20302;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#33021;&#21147;&#21644;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#32780;&#23545;&#31038;&#20250;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#12290;&#21019;&#24314;&#20102;&#21508;&#31181;&#24212;&#29992;&#21644;&#24037;&#20855;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20197;&#40657;&#30418;&#22330;&#26223;&#19982;&#36825;&#20123;&#27169;&#22411;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22330;&#26223;&#30340;&#38480;&#21046;&#20043;&#19968;&#26159;&#29992;&#25143;&#26080;&#27861;&#20462;&#25913;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#65292;&#28155;&#21152;&#25110;&#20462;&#25913;&#20869;&#37096;&#30693;&#35782;&#30340;&#21807;&#19968;&#26041;&#27861;&#26159;&#22312;&#24403;&#21069;&#20132;&#20114;&#36807;&#31243;&#20013;&#26126;&#30830;&#25552;&#21450;&#12290;&#36825;&#31181;&#23398;&#20064;&#36807;&#31243;&#31216;&#20026;&#19978;&#19979;&#25991;&#35757;&#32451;&#65292;&#25351;&#30340;&#26159;&#38480;&#23450;&#22312;&#29992;&#25143;&#24403;&#21069;&#20250;&#35805;&#25110;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#30340;&#35757;&#32451;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#65292;&#20294;&#20063;&#23384;&#22312;&#24456;&#23569;&#30740;&#31350;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35828;&#26126;&#20102;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#21463;&#22312;&#19978;&#19979;&#25991;&#20013;&#19981;&#26029;&#27969;&#21160;&#30340;&#20449;&#24687;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#20174;&#32780;&#23548;&#33268;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#38500;&#20102;&#23637;&#31034;&#38382;&#39064;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have had a huge impact on society due to their impressive capabilities and vast knowledge of the world. Various applications and tools have been created that allow users to interact with these models in a black-box scenario. However, one limitation of this scenario is that users cannot modify the internal knowledge of the model, and the only way to add or modify internal knowledge is by explicitly mentioning it to the model during the current interaction. This learning process is called in-context training, and it refers to training that is confined to the user's current session or context. In-context learning has significant applications, but also has limitations that are seldom studied. In this paper, we present a study that shows how the model can suffer from interference between information that continually flows in the context, causing it to forget previously learned knowledge, which can reduce the model's performance. Along with showing the problem, w
&lt;/p&gt;</description></item><item><title>H2O+&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#28151;&#21512;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#30495;&#23454;&#21644;&#27169;&#25311;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#24046;&#36317;&#65292;&#21516;&#26102;&#21033;&#29992;&#26377;&#38480;&#30340;&#31163;&#32447;&#25968;&#25454;&#21644;&#19981;&#23436;&#32654;&#30340;&#27169;&#25311;&#22120;&#36827;&#34892;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12716</link><description>&lt;p&gt;
H2O+: &#19968;&#31181;&#25913;&#36827;&#30340;&#28151;&#21512;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#21147;&#23398;&#24046;&#36317;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
H2O+: An Improved Framework for Hybrid Offline-and-Online RL with Dynamics Gaps. (arXiv:2309.12716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12716
&lt;/p&gt;
&lt;p&gt;
H2O+&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#28151;&#21512;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#30495;&#23454;&#21644;&#27169;&#25311;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#24046;&#36317;&#65292;&#21516;&#26102;&#21033;&#29992;&#26377;&#38480;&#30340;&#31163;&#32447;&#25968;&#25454;&#21644;&#19981;&#23436;&#32654;&#30340;&#27169;&#25311;&#22120;&#36827;&#34892;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#39640;&#31934;&#24230;&#27169;&#25311;&#29615;&#22659;&#25110;&#22823;&#37327;&#31163;&#32447;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#23454;&#38469;&#22797;&#26434;&#20219;&#21153;&#21487;&#33021;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#38750;&#23436;&#32654;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#22312;&#32447;RL&#20195;&#29702;&#21487;&#33021;&#20250;&#21463;&#21040;&#20005;&#37325;&#30340;&#27169;&#25311;&#19982;&#29616;&#23454;&#38382;&#39064;&#12290;&#34429;&#28982;&#31163;&#32447;RL&#26041;&#27861;&#21487;&#20197;&#32469;&#36807;&#23545;&#27169;&#25311;&#22120;&#30340;&#38656;&#27714;&#65292;&#20294;&#24448;&#24448;&#23545;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#36136;&#37327;&#25552;&#20986;&#20102;&#33499;&#21051;&#30340;&#35201;&#27714;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#28151;&#21512;&#31163;&#32447;&#21644;&#22312;&#32447;RL&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#20351;&#29992;&#26377;&#38480;&#30340;&#31163;&#32447;&#25968;&#25454;&#21644;&#19981;&#23436;&#32654;&#30340;&#27169;&#25311;&#22120;&#36827;&#34892;&#21487;&#36716;&#31227;&#31574;&#30053;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;H2O+&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#26725;&#25509;&#19981;&#21516;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#30340;&#21516;&#26102;&#65292;&#20063;&#32771;&#34385;&#20102;&#30495;&#23454;&#21644;&#27169;&#25311;&#29615;&#22659;&#20043;&#38388;&#30340;&#21160;&#21147;&#23398;&#24046;&#36317;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;H2O+&#22312;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#19978;&#20248;&#20110;&#20808;&#36827;&#30340;&#36328;&#22495;&#22312;&#32447;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Solving real-world complex tasks using reinforcement learning (RL) without high-fidelity simulation environments or large amounts of offline data can be quite challenging. Online RL agents trained in imperfect simulation environments can suffer from severe sim-to-real issues. Offline RL approaches although bypass the need for simulators, often pose demanding requirements on the size and quality of the offline datasets. The recently emerged hybrid offline-and-online RL provides an attractive framework that enables joint use of limited offline data and imperfect simulator for transferable policy learning. In this paper, we develop a new algorithm, called H2O+, which offers great flexibility to bridge various choices of offline and online learning methods, while also accounting for dynamics gaps between the real and simulation environment. Through extensive simulation and real-world robotics experiments, we demonstrate superior performance and flexibility over advanced cross-domain online
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#20854;&#20182;&#28216;&#25103;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#25913;&#36827;Holophrasm&#23450;&#29702;&#35777;&#26126;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12711</link><description>&lt;p&gt;
&#25968;&#23398;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
The Mathematical Game. (arXiv:2309.12711v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12711
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#20854;&#20182;&#28216;&#25103;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#25913;&#36827;Holophrasm&#23450;&#29702;&#35777;&#26126;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21487;&#29992;&#20110;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#12290;Holophrasm&#26159;&#19968;&#20010;&#20351;&#29992;MCTS&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#31574;&#30053;&#21644;&#35780;&#20272;&#30340;&#31070;&#32463;&#23450;&#29702;&#35777;&#26126;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20854;&#20182;&#28216;&#25103;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#25913;&#36827;Holophrasm&#23450;&#29702;&#35777;&#26126;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo Tree Search can be used for automated theorem proving. Holophrasm is a neural theorem prover using MCTS combined with neural networks for the policy and the evaluation. In this paper we propose to improve the performance of the Holophrasm theorem prover using other game tree search algorithms.
&lt;/p&gt;</description></item><item><title>PointSSC&#26159;&#31532;&#19968;&#20010;&#20026;&#20102;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#32780;&#24341;&#20837;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;&#65292;&#20855;&#22791;&#38271;&#36317;&#31163;&#24863;&#30693;&#21644;&#26368;&#23567;&#36974;&#25377;&#12290;&#36890;&#36807;&#20351;&#29992;Segment Anything&#36827;&#34892;&#33258;&#21160;&#21270;&#27880;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#34917;&#20840;&#21644;&#20998;&#21106;&#30340;&#21512;&#20316;&#27169;&#22359;&#65292;&#26469;&#25512;&#21160;&#35821;&#20041;&#28857;&#20113;&#34917;&#20840;&#22312;&#30495;&#23454;&#19990;&#30028;&#23548;&#33322;&#20013;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.12708</link><description>&lt;p&gt;
PointSSC&#65306;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion. (arXiv:2309.12708v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12708
&lt;/p&gt;
&lt;p&gt;
PointSSC&#26159;&#31532;&#19968;&#20010;&#20026;&#20102;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#32780;&#24341;&#20837;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;&#65292;&#20855;&#22791;&#38271;&#36317;&#31163;&#24863;&#30693;&#21644;&#26368;&#23567;&#36974;&#25377;&#12290;&#36890;&#36807;&#20351;&#29992;Segment Anything&#36827;&#34892;&#33258;&#21160;&#21270;&#27880;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#34917;&#20840;&#21644;&#20998;&#21106;&#30340;&#21512;&#20316;&#27169;&#22359;&#65292;&#26469;&#25512;&#21160;&#35821;&#20041;&#28857;&#20113;&#34917;&#20840;&#22312;&#30495;&#23454;&#19990;&#30028;&#23548;&#33322;&#20013;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#26088;&#22312;&#20026;&#22797;&#26434;&#30340;3D&#22330;&#26223;&#29983;&#25104;&#31354;&#38388;&#21344;&#29992;&#21644;&#35821;&#20041;&#26631;&#31614;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#27169;&#22411;&#37117;&#38598;&#20013;&#22312;&#20307;&#32032;&#34920;&#31034;&#19978;&#65292;&#23545;&#20110;&#22823;&#22411;&#23460;&#22806;&#31354;&#38388;&#26469;&#35828;&#23384;&#22312;&#20869;&#23384;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#28857;&#20113;&#25552;&#20379;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#29616;&#26377;&#22522;&#20934;&#32570;&#20047;&#24102;&#26377;&#35821;&#20041;&#26631;&#31614;&#30340;&#23460;&#22806;&#28857;&#20113;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;PointSSC&#12290;&#36825;&#20123;&#22330;&#26223;&#20855;&#26377;&#38271;&#36317;&#31163;&#24863;&#30693;&#21644;&#26368;&#23567;&#30340;&#36974;&#25377;&#12290;&#25105;&#20204;&#21033;&#29992;Segment Anything&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27880;&#37322;&#27969;&#31243;&#65292;&#20197;&#39640;&#25928;&#22320;&#20998;&#37197;&#35821;&#20041;&#26631;&#31614;&#12290;&#20026;&#20102;&#35780;&#20272;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#31354;&#38388;&#24863;&#30693;&#21464;&#25442;&#22120;&#29992;&#20110;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#65292;&#20197;&#21450;&#19968;&#20010;&#34917;&#20840;&#21644;&#20998;&#21106;&#21512;&#20316;&#27169;&#22359;&#29992;&#20110;&#32852;&#21512;&#34917;&#20840;&#21644;&#20998;&#21106;&#12290;PointSSC&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#25512;&#21160;&#20102;&#35821;&#20041;&#28857;&#20113;&#34917;&#20840;&#22312;&#30495;&#23454;&#19990;&#30028;&#23548;&#33322;&#20013;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic Scene Completion (SSC) aims to jointly generate space occupancies and semantic labels for complex 3D scenes. Most existing SSC models focus on volumetric representations, which are memory-inefficient for large outdoor spaces. Point clouds provide a lightweight alternative but existing benchmarks lack outdoor point cloud scenes with semantic labels. To address this, we introduce PointSSC, the first cooperative vehicle-infrastructure point cloud benchmark for semantic scene completion. These scenes exhibit long-range perception and minimal occlusion. We develop an automated annotation pipeline leveraging Segment Anything to efficiently assign semantics. To benchmark progress, we propose a LiDAR-based model with a Spatial-Aware Transformer for global and local feature extraction and a Completion and Segmentation Cooperative Module for joint completion and segmentation. PointSSC provides a challenging testbed to drive advances in semantic point cloud completion for real-world navi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#26631;&#31614;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#20272;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#21487;&#36776;&#35782;&#24615;&#65292;&#24182;&#32467;&#21512;&#26631;&#31614;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#22312;&#22122;&#22768;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#23454;&#29616;&#32479;&#35745;&#19968;&#33268;&#24615;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.12706</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#20272;&#35745;&#19982;&#26631;&#31614;&#30456;&#20851;&#24615;&#65306;&#29702;&#35770;&#19982;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Label Noise Transition Matrix Estimation with Label Correlations: Theory and Algorithm. (arXiv:2309.12706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#26631;&#31614;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#20272;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#21487;&#36776;&#35782;&#24615;&#65292;&#24182;&#32467;&#21512;&#26631;&#31614;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#22312;&#22122;&#22768;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#23454;&#29616;&#32479;&#35745;&#19968;&#33268;&#24615;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25910;&#38598;&#22823;&#35268;&#27169;&#20934;&#30830;&#26631;&#31614;&#30340;&#25361;&#25112;&#65292;&#22122;&#22768;&#22810;&#26631;&#31614;&#23398;&#20064;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20351;&#24471;&#22122;&#22768;&#26631;&#31614;&#25104;&#20026;&#26356;&#23454;&#38469;&#30340;&#36873;&#25321;&#12290;&#21463;&#21040;&#22122;&#22768;&#22810;&#31867;&#21035;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#24341;&#20837;&#36716;&#31227;&#30697;&#38453;&#21487;&#20197;&#24110;&#21161;&#24314;&#27169;&#22810;&#26631;&#31614;&#22122;&#22768;&#65292;&#24182;&#23454;&#29616;&#23545;&#22122;&#22768;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#31639;&#27861;&#30340;&#24320;&#21457;&#12290;&#28982;&#32780;&#65292;&#20272;&#35745;&#22810;&#26631;&#31614;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#22312;&#22122;&#22768;&#22810;&#31867;&#21035;&#23398;&#20064;&#20013;&#30340;&#22823;&#22810;&#25968;&#29616;&#26377;&#20272;&#35745;&#22120;&#20381;&#36182;&#20110;&#38170;&#28857;&#21644;&#20934;&#30830;&#25311;&#21512;&#22122;&#22768;&#31867;&#21518;&#39564;&#27010;&#29575;&#65292;&#32780;&#22312;&#22122;&#22768;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#24456;&#38590;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#22312;&#22122;&#22768;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#22522;&#20110;&#31867;&#21035;&#30340;&#36716;&#31227;&#30697;&#38453;&#30340;&#21487;&#36776;&#35782;&#24615;&#12290;&#22312;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#21033;&#29992;&#26631;&#31614;&#30456;&#20851;&#24615;&#65292;&#32780;&#26080;&#38656;&#38170;&#28857;&#25110;&#31934;&#30830;&#25311;&#21512;&#22122;&#22768;&#31867;&#21518;&#39564;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noisy multi-label learning has garnered increasing attention due to the challenges posed by collecting large-scale accurate labels, making noisy labels a more practical alternative. Motivated by noisy multi-class learning, the introduction of transition matrices can help model multi-label noise and enable the development of statistically consistent algorithms for noisy multi-label learning. However, estimating multi-label noise transition matrices remains a challenging task, as most existing estimators in noisy multi-class learning rely on anchor points and accurate fitting of noisy class posteriors, which is hard to satisfy in noisy multi-label learning. In this paper, we address this problem by first investigating the identifiability of class-dependent transition matrices in noisy multi-label learning. Building upon the identifiability results, we propose a novel estimator that leverages label correlations without the need for anchor points or precise fitting of noisy class posterior
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#21453;&#20107;&#23454;&#20445;&#23432;Q&#23398;&#20064;&#23454;&#29616;&#20445;&#23432;&#30340;&#20215;&#20540;&#20272;&#35745;&#65292;&#22312;&#35299;&#20915;&#31163;&#32447;&#20998;&#24067;&#20559;&#31227;&#21644;&#39640;&#32500;&#38382;&#39064;&#30340;&#21516;&#26102;&#65292;&#30830;&#20445;&#34892;&#20026;&#30340;&#20998;&#24067;&#21644;&#20215;&#20540;&#30340;&#20272;&#35745;&#19981;&#36807;&#39640;&#12290;</title><link>http://arxiv.org/abs/2309.12696</link><description>&lt;p&gt;
&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#22686;&#24378;&#23398;&#20064;&#30340;&#21453;&#20107;&#23454;&#20445;&#23432;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Conservative Q Learning for Offline Multi-agent Reinforcement Learning. (arXiv:2309.12696v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#21453;&#20107;&#23454;&#20445;&#23432;Q&#23398;&#20064;&#23454;&#29616;&#20445;&#23432;&#30340;&#20215;&#20540;&#20272;&#35745;&#65292;&#22312;&#35299;&#20915;&#31163;&#32447;&#20998;&#24067;&#20559;&#31227;&#21644;&#39640;&#32500;&#38382;&#39064;&#30340;&#21516;&#26102;&#65292;&#30830;&#20445;&#34892;&#20026;&#30340;&#20998;&#24067;&#21644;&#20215;&#20540;&#30340;&#20272;&#35745;&#19981;&#36807;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#22686;&#24378;&#23398;&#20064;&#38754;&#20020;&#30528;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#21644;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#39640;&#32500;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#23548;&#33268;&#34892;&#20026;&#30340;&#36229;&#20986;&#20998;&#24067;&#21644;&#20215;&#20540;&#30340;&#36807;&#39640;&#20272;&#35745;&#29616;&#35937;&#36807;&#20110;&#20005;&#37325;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;&#21453;&#20107;&#23454;&#20445;&#23432;Q&#23398;&#20064;&#65288;CFCQL&#65289;&#65292;&#20197;&#36827;&#34892;&#20445;&#23432;&#30340;&#20215;&#20540;&#20272;&#35745;&#12290;CFCQL&#24182;&#19981;&#23558;&#25152;&#26377;&#26234;&#33021;&#20307;&#35270;&#20026;&#19968;&#20010;&#39640;&#32500;&#21333;&#29420;&#30340;&#26234;&#33021;&#20307;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;&#65292;&#32780;&#26159;&#20197;&#21453;&#20107;&#23454;&#30340;&#26041;&#24335;&#20998;&#21035;&#20026;&#27599;&#20010;&#26234;&#33021;&#20307;&#35745;&#31639;&#20445;&#23432;&#30340;&#27491;&#21017;&#21270;&#65292;&#24182;&#32447;&#24615;&#32452;&#21512;&#23427;&#20204;&#20197;&#23454;&#29616;&#25972;&#20307;&#20445;&#23432;&#20215;&#20540;&#20272;&#35745;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20173;&#28982;&#20855;&#26377;&#21333;&#19968;&#26234;&#33021;&#20307;&#20445;&#23432;&#26041;&#27861;&#30340;&#20302;&#20272;&#24615;&#36136;&#21644;&#24615;&#33021;&#20445;&#35777;&#65292;&#20294;&#25152;&#24341;&#36215;&#30340;&#27491;&#21017;&#21270;&#21644;&#23433;&#20840;&#31574;&#30053;&#25913;&#36827;&#30028;&#38480;&#37117;&#26159;&#29420;&#31435;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline multi-agent reinforcement learning is challenging due to the coupling effect of both distribution shift issue common in offline setting and the high dimension issue common in multi-agent setting, making the action out-of-distribution (OOD) and value overestimation phenomenon excessively severe. Tomitigate this problem, we propose a novel multi-agent offline RL algorithm, named CounterFactual Conservative Q-Learning (CFCQL) to conduct conservative value estimation. Rather than regarding all the agents as a high dimensional single one and directly applying single agent methods to it, CFCQL calculates conservative regularization for each agent separately in a counterfactual way and then linearly combines them to realize an overall conservative value estimation. We prove that it still enjoys the underestimation property and the performance guarantee as those single agent conservative methods do, but the induced regularization and safe policy improvement bound are independent of the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26412;&#22320;&#21644;&#20113;&#35745;&#31639;&#22686;&#24378;&#29615;&#22659;&#30340;&#22270;&#34920;&#31034;&#65292;&#25552;&#20379;&#35821;&#20041;&#21270;&#30340;&#26426;&#22120;&#20154;&#29615;&#22659;&#34920;&#31034;&#65292;&#28040;&#38500;&#20102;&#23545;&#29305;&#23450;&#24037;&#20855;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.12692</link><description>&lt;p&gt;
&#36890;&#36807;&#26412;&#22320;&#21644;&#20113;&#35745;&#31639;&#22686;&#24378;&#29615;&#22659;&#30340;&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Graph Representation of the Environment through Local and Cloud Computation. (arXiv:2309.12692v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12692
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26412;&#22320;&#21644;&#20113;&#35745;&#31639;&#22686;&#24378;&#29615;&#22659;&#30340;&#22270;&#34920;&#31034;&#65292;&#25552;&#20379;&#35821;&#20041;&#21270;&#30340;&#26426;&#22120;&#20154;&#29615;&#22659;&#34920;&#31034;&#65292;&#28040;&#38500;&#20102;&#23545;&#29305;&#23450;&#24037;&#20855;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20016;&#23500;&#26426;&#22120;&#20154;&#23545;&#25805;&#20316;&#29615;&#22659;&#30340;&#34920;&#31034;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#24357;&#21512;&#20302;&#32423;&#20256;&#24863;&#22120;&#35835;&#25968;&#19982;&#39640;&#32423;&#35821;&#20041;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35201;&#25317;&#26377;&#20016;&#23500;&#30340;&#34920;&#31034;&#36890;&#24120;&#38656;&#35201;&#35745;&#31639;&#38656;&#27714;&#39640;&#30340;&#20307;&#31995;&#32467;&#26500;&#21644;&#32431;&#28857;&#20113;&#30340;&#26816;&#27979;&#31995;&#32479;&#65292;&#32780;&#22312;&#22788;&#29702;&#26426;&#22120;&#20154;&#24517;&#39035;&#22788;&#29702;&#30340;&#26085;&#24120;&#29289;&#20307;&#26102;&#24448;&#24448;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#20379;&#26426;&#22120;&#20154;&#29615;&#22659;&#30340;&#35821;&#20041;&#34920;&#31034;&#26469;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#12290;&#20107;&#23454;&#19978;&#65292;&#20026;&#20102;&#20174;&#29615;&#22659;&#20013;&#33719;&#21462;&#20449;&#24687;&#65292;&#35813;&#26694;&#26550;&#23558;&#20256;&#32479;&#35745;&#31639;&#26426;&#35270;&#35273;&#24037;&#20855;&#19982;&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#20113;&#26381;&#21153;&#30456;&#32467;&#21512;&#65292;&#30830;&#20445;&#22312;&#26426;&#36733;&#30828;&#20214;&#19978;&#20855;&#22791;&#35745;&#31639;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#23558;800&#22810;&#20010;&#30446;&#26631;&#31867;&#21035;&#30340;&#26412;&#20307;&#23618;&#27425;&#32467;&#26500;&#32435;&#20837;&#20854;&#20013;&#65292;&#35813;&#26694;&#26550;&#23454;&#29616;&#20102;&#36328;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#65292;&#28040;&#38500;&#20102;&#29615;&#22659;&#29305;&#23450;&#24037;&#20855;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enriching the robot representation of the operational environment is a challenging task that aims at bridging the gap between low-level sensor readings and high-level semantic understanding. Having a rich representation often requires computationally demanding architectures and pure point cloud based detection systems that struggle when dealing with everyday objects that have to be handled by the robot. To overcome these issues, we propose a graph-based representation that addresses this gap by providing a semantic representation of robot environments from multiple sources. In fact, to acquire information from the environment, the framework combines classical computer vision tools with modern computer vision cloud services, ensuring computational feasibility on onboard hardware. By incorporating an ontology hierarchy with over 800 object classes, the framework achieves cross-domain adaptability, eliminating the need for environment-specific tools. The proposed approach allows us to han
&lt;/p&gt;</description></item><item><title>QAL-BP&#26159;&#19968;&#31181;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#37327;&#23376;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;&#35013;&#31665;&#38382;&#39064;&#12290;&#23427;&#21033;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#23558;&#35013;&#31665;&#32422;&#26463;&#21152;&#20837;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#20272;&#35745;&#21551;&#21457;&#24335;&#20056;&#25968;&#65292;&#28040;&#38500;&#20102;&#38656;&#35201;&#26681;&#25454;&#23454;&#20363;&#35745;&#31639;Lagrangian&#31995;&#25968;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.12678</link><description>&lt;p&gt;
QAL-BP:&#19968;&#31181;&#29992;&#20110;&#35013;&#31665;&#38382;&#39064;&#30340;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#37327;&#23376;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QAL-BP: An Augmented Lagrangian Quantum Approach for Bin Packing Problem. (arXiv:2309.12678v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12678
&lt;/p&gt;
&lt;p&gt;
QAL-BP&#26159;&#19968;&#31181;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#37327;&#23376;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;&#35013;&#31665;&#38382;&#39064;&#12290;&#23427;&#21033;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#23558;&#35013;&#31665;&#32422;&#26463;&#21152;&#20837;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#20272;&#35745;&#21551;&#21457;&#24335;&#20056;&#25968;&#65292;&#28040;&#38500;&#20102;&#38656;&#35201;&#26681;&#25454;&#23454;&#20363;&#35745;&#31639;Lagrangian&#31995;&#25968;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35013;&#31665;&#38382;&#39064;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;NP-Hard&#38382;&#39064;&#65292;&#23547;&#25214;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#30456;&#21453;&#65292;&#37327;&#23376;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#31034;&#20986;&#22312;&#26576;&#20123;&#38382;&#39064;&#31867;&#21035;&#65288;&#22914;&#32452;&#21512;&#20248;&#21270;&#65289;&#20013;&#23454;&#29616;&#22823;&#24133;&#35745;&#31639;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;QAL-BP&#65292;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#35013;&#31665;&#38382;&#39064;&#35774;&#35745;&#30340;&#26032;&#22411;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;&#65288;QUBO&#65289;&#20844;&#24335;&#65292;&#36866;&#29992;&#20110;&#37327;&#23376;&#35745;&#31639;&#12290;QAL-BP&#37319;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#23558;&#35013;&#31665;&#32422;&#26463;&#23884;&#20837;&#21040;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#24182;&#20415;&#20110;&#23545;&#21551;&#21457;&#24335;&#20056;&#25968;&#36827;&#34892;&#20998;&#26512;&#20272;&#35745;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#28789;&#27963;&#21644;&#21487;&#25512;&#24191;&#65292;&#28040;&#38500;&#20102;&#22312;&#20854;&#20182;QUBO&#20844;&#24335;&#20013;&#24120;&#36935;&#21040;&#30340;&#38656;&#35201;&#26681;&#25454;&#23454;&#20363;&#35745;&#31639;Lagrangian&#31995;&#25968;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The bin packing is a well-known NP-Hard problem in the domain of artificial intelligence, posing significant challenges in finding efficient solutions. Conversely, recent advancements in quantum technologies have shown promising potential for achieving substantial computational speedup, particularly in certain problem classes, such as combinatorial optimization. In this study, we introduce QAL-BP, a novel Quadratic Unconstrained Binary Optimization (QUBO) formulation designed specifically for bin packing and suitable for quantum computation. QAL-BP utilizes the augmented Lagrangian method to incorporate the bin packing constraints into the objective function while also facilitating an analytical estimation of heuristic, but empirically robust, penalty multipliers. This approach leads to a more versatile and generalizable model that eliminates the need for empirically calculating instance-dependent Lagrangian coefficients, a requirement commonly encountered in alternative QUBO formulati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Transformer&#27169;&#22411;&#26469;&#25429;&#25417;&#36710;&#36742;&#32676;&#20307;&#20013;&#36712;&#36857;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#20132;&#36890;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35774;&#35745;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#36710;&#36742;&#36712;&#36857;&#30340;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20132;&#36890;&#20219;&#21153;&#30340;&#25968;&#25454;&#32467;&#26500;&#21644;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2309.12677</link><description>&lt;p&gt;
TrTr&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#36890;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#27969;&#37327;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#36710;&#36742;&#32676;&#20307;&#20013;&#30340;&#36712;&#36857;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
TrTr: A Versatile Pre-Trained Large Traffic Model based on Transformer for Capturing Trajectory Diversity in Vehicle Population. (arXiv:2309.12677v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Transformer&#27169;&#22411;&#26469;&#25429;&#25417;&#36710;&#36742;&#32676;&#20307;&#20013;&#36712;&#36857;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#20132;&#36890;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35774;&#35745;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#36710;&#36742;&#36712;&#36857;&#30340;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20132;&#36890;&#20219;&#21153;&#30340;&#25968;&#25454;&#32467;&#26500;&#21644;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#36712;&#36857;&#22810;&#26679;&#24615;&#26159;&#35299;&#20915;&#23454;&#38469;&#20132;&#36890;&#20219;&#21153;&#30340;&#22522;&#26412;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#22823;&#35268;&#27169;&#21442;&#25968;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;&#25429;&#25417;&#36712;&#36857;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#26032;&#20852;&#30340;Transformer&#25216;&#26415;&#20197;&#20854;&#24182;&#34892;&#35745;&#31639;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#21487;&#20197;&#21033;&#29992;&#20855;&#26377;&#25968;&#20159;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20026;&#27492;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;Transformer&#26550;&#26500;&#24212;&#29992;&#20110;&#20132;&#36890;&#20219;&#21153;&#65292;&#26088;&#22312;&#23398;&#20064;&#36710;&#36742;&#32676;&#20307;&#20869;&#30340;&#36712;&#36857;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#21450;&#20854;&#36866;&#24212;&#20132;&#36890;&#20219;&#21153;&#30446;&#26631;&#30340;&#33021;&#21147;&#65292;&#38543;&#21518;&#35774;&#35745;&#20102;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#36866;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#32452;&#19982;&#26102;&#31354;&#38656;&#27714;&#23545;&#24212;&#30340;&#22122;&#22768;&#65292;&#36825;&#20123;&#22122;&#22768;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#34987;&#32435;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding trajectory diversity is a fundamental aspect of addressing practical traffic tasks. However, capturing the diversity of trajectories presents challenges, particularly with traditional machine learning and recurrent neural networks due to the requirement of large-scale parameters. The emerging Transformer technology, renowned for its parallel computation capabilities enabling the utilization of models with hundreds of millions of parameters, offers a promising solution. In this study, we apply the Transformer architecture to traffic tasks, aiming to learn the diversity of trajectories within vehicle populations. We analyze the Transformer's attention mechanism and its adaptability to the goals of traffic tasks, and subsequently, design specific pre-training tasks. To achieve this, we create a data structure tailored to the attention mechanism and introduce a set of noises that correspond to spatio-temporal demands, which are incorporated into the structured data during the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;Transformer&#22312;&#22260;&#26827;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#19982;&#20256;&#32479;&#30340;&#27531;&#24046;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#65292;&#20174;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#32988;&#29575;&#12289;&#20869;&#23384;&#12289;&#36895;&#24230;&#31561;&#22810;&#20010;&#26041;&#38754;&#23637;&#31034;&#20102;&#20854;&#22312;&#22260;&#26827;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.12675</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#22260;&#26827;&#30340;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
Vision Transformers for Computer Go. (arXiv:2309.12675v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;Transformer&#22312;&#22260;&#26827;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#19982;&#20256;&#32479;&#30340;&#27531;&#24046;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#65292;&#20174;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#32988;&#29575;&#12289;&#20869;&#23384;&#12289;&#36895;&#24230;&#31561;&#22810;&#20010;&#26041;&#38754;&#23637;&#31034;&#20102;&#20854;&#22312;&#22260;&#26827;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;Transformer&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#22270;&#20687;&#20998;&#26512;&#31561;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#22260;&#26827;&#20013;&#30340;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;Transformer&#22312;&#35270;&#35273;&#20013;&#30340;&#20998;&#26512;&#12290;&#36890;&#36807;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#32988;&#29575;&#12289;&#20869;&#23384;&#12289;&#36895;&#24230;&#12289;&#22823;&#23567;&#29978;&#33267;&#23398;&#20064;&#29575;&#31561;&#22810;&#20010;&#26041;&#38754;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#31361;&#20986;&#20102;Transformer&#22312;&#22260;&#26827;&#20013;&#21487;&#20197;&#21457;&#25381;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19982;&#20256;&#32479;&#30340;&#27531;&#24046;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#36827;&#34892;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the success of transformers in various fields, such as language understanding and image analysis, this investigation explores their application in the context of the game of Go. In particular, our study focuses on the analysis of the Transformer in Vision. Through a detailed analysis of numerous points such as prediction accuracy, win rates, memory, speed, size, or even learning rate, we have been able to highlight the substantial role that transformers can play in the game of Go. This study was carried out by comparing them to the usual Residual Networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#33021;&#37327;&#20989;&#25968;&#21644;&#31232;&#30095;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#23454;&#29616;&#20102;&#23545;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#30340;&#19968;&#27493;&#36817;&#20284;&#12290;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#65292;&#31232;&#30095;&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#19978;&#30028;&#26356;&#32039;&#20945;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#31232;&#30095;&#20248;&#21183;&#26465;&#20214;&#12290;&#21516;&#26102;&#65292;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#36824;&#20445;&#25345;&#20102;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#30340;&#31283;&#20581;&#29702;&#35770;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2309.12673</link><description>&lt;p&gt;
&#20851;&#20110;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On Sparse Modern Hopfield Model. (arXiv:2309.12673v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#33021;&#37327;&#20989;&#25968;&#21644;&#31232;&#30095;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#23454;&#29616;&#20102;&#23545;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#30340;&#19968;&#27493;&#36817;&#20284;&#12290;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#65292;&#31232;&#30095;&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#19978;&#30028;&#26356;&#32039;&#20945;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#31232;&#30095;&#20248;&#21183;&#26465;&#20214;&#12290;&#21516;&#26102;&#65292;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#36824;&#20445;&#25345;&#20102;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#30340;&#31283;&#20581;&#29702;&#35770;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#20316;&#20026;&#29616;&#20195; Hopfield &#27169;&#22411;&#30340;&#19968;&#31181;&#25193;&#23637;&#12290;&#19982;&#20854;&#23494;&#38598;&#30340;&#23545;&#24212;&#29289;&#19968;&#26679;&#65292;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#20855;&#22791;&#19968;&#31181;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#20854;&#19968;&#27493;&#36817;&#20284;&#23545;&#24212;&#20110;&#31232;&#30095;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#36890;&#36807;&#31232;&#30095;&#29109;&#27491;&#21017;&#21270;&#22120;&#30340;&#20984;&#20849;&#36717;&#23548;&#20986;&#20102;&#23553;&#38381;&#24418;&#24335;&#30340;&#31232;&#30095; Hopfield &#33021;&#37327;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20174;&#31232;&#30095;&#33021;&#37327;&#20989;&#25968;&#20013;&#25512;&#23548;&#20986;&#31232;&#30095;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#19968;&#27493;&#36817;&#20284;&#31561;&#20215;&#20110;&#31232;&#30095;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;&#31232;&#30095;&#24230;&#30340;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#22312;&#35777;&#26126;&#19978;&#35201;&#27604;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#26356;&#32039;&#20945;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#35752;&#35770;&#20102;&#31232;&#30095;&#20248;&#21183;&#20986;&#29616;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#20445;&#25345;&#20102;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#30340;&#31283;&#20581;&#29702;&#35770;&#24615;&#36136;&#65292;&#21253;&#25324;&#24555;&#36895;&#30340;&#22266;&#23450;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the sparse modern Hopfield model as a sparse extension of the modern Hopfield model. Like its dense counterpart, the sparse modern Hopfield model equips a memory-retrieval dynamics whose one-step approximation corresponds to the sparse attention mechanism. Theoretically, our key contribution is a principled derivation of a closed-form sparse Hopfield energy using the convex conjugate of the sparse entropic regularizer. Building upon this, we derive the sparse memory retrieval dynamics from the sparse energy function and show its one-step approximation is equivalent to the sparse-structured attention. Importantly, we provide a sparsity-dependent memory retrieval error bound which is provably tighter than its dense analog. The conditions for the benefits of sparsity to arise are therefore identified and discussed. In addition, we show that the sparse modern Hopfield model maintains the robust theoretical properties of its dense counterpart, including rapid fixed point conver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#25552;&#20379;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#21644;&#36991;&#20813;&#27169;&#22411;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2309.12671</link><description>&lt;p&gt;
&#22914;&#20309;&#24494;&#35843;&#27169;&#22411;&#65306;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization. (arXiv:2309.12671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#25552;&#20379;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#21644;&#36991;&#20813;&#27169;&#22411;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#21644;&#25512;&#23548;&#20986;&#20855;&#26377;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#30340;&#26377;&#25928;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#31639;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#27169;&#22411;&#23398;&#20064;&#21644;&#31574;&#30053;&#20248;&#21270;&#20043;&#38388;&#30340;&#39640;&#32806;&#21512;&#24615;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#38752;&#22238;&#25253;&#24046;&#24322;&#26469;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#65292;&#24573;&#30053;&#20102;&#27169;&#22411;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#36807;&#22810;&#30340;&#27169;&#22411;&#26356;&#26032;&#32780;&#24615;&#33021;&#19979;&#38477;&#12290;&#20854;&#20182;&#26041;&#27861;&#20351;&#29992;&#24615;&#33021;&#24046;&#24322;&#36793;&#30028;&#26469;&#26126;&#30830;&#32771;&#34385;&#27169;&#22411;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#22266;&#23450;&#30340;&#38408;&#20540;&#26469;&#38480;&#21046;&#27169;&#22411;&#20559;&#31227;&#65292;&#23548;&#33268;&#23545;&#38408;&#20540;&#30340;&#20005;&#37325;&#20381;&#36182;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32570;&#20047;&#36866;&#24212;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#19968;&#20010;&#21487;&#20197;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#28982;&#21518;&#21046;&#23450;&#19968;&#20010;&#24494;&#35843;&#36807;&#31243;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#33719;&#24471;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#65292;&#21516;&#26102;&#36991;&#20813;&#27169;&#22411;&#36807;&#25311;&#21512;&#12290;&#22522;&#20110;&#36825;&#20123;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30452;&#35266;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Designing and deriving effective model-based reinforcement learning (MBRL) algorithms with a performance improvement guarantee is challenging, mainly attributed to the high coupling between model learning and policy optimization. Many prior methods that rely on return discrepancy to guide model learning ignore the impacts of model shift, which can lead to performance deterioration due to excessive model updates. Other methods use performance difference bound to explicitly consider model shift. However, these methods rely on a fixed threshold to constrain model shift, resulting in a heavy dependence on the threshold and a lack of adaptability during the training process. In this paper, we theoretically derive an optimization objective that can unify model shift and model bias and then formulate a fine-tuning process. This process adaptively adjusts the model updates to get a performance improvement guarantee while avoiding model overfitting. Based on these, we develop a straightforward 
&lt;/p&gt;</description></item><item><title>&#33258;&#28982;&#20462;&#27491;&#26159;&#19968;&#31181;&#26377;&#26465;&#20214;&#30340;&#20462;&#27491;&#26041;&#24335;&#65292;&#23427;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#20449;&#24565;&#26469;&#34701;&#20837;&#26032;&#20449;&#24687;&#65292;&#24182;&#23558;&#20462;&#27491;&#38480;&#21046;&#22312;&#24403;&#21069;&#26465;&#20214;&#19979;&#12290;</title><link>http://arxiv.org/abs/2309.12655</link><description>&lt;p&gt;
&#33258;&#28982;&#20462;&#27491;&#26159;&#19968;&#31181;&#26377;&#26465;&#20214;&#30340;&#20462;&#27491;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural revision is contingently-conditionalized revision. (arXiv:2309.12655v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12655
&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#20462;&#27491;&#26159;&#19968;&#31181;&#26377;&#26465;&#20214;&#30340;&#20462;&#27491;&#26041;&#24335;&#65292;&#23427;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#20449;&#24565;&#26469;&#34701;&#20837;&#26032;&#20449;&#24687;&#65292;&#24182;&#23558;&#20462;&#27491;&#38480;&#21046;&#22312;&#24403;&#21069;&#26465;&#20214;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#20462;&#27491;&#30475;&#36215;&#26469;&#24456;&#33258;&#28982;&#65306;&#23427;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#20449;&#24565;&#26469;&#34701;&#20837;&#26032;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#21453;&#20363;&#26174;&#31034;&#36825;&#26159;&#38169;&#35823;&#30340;&#12290;&#23427;&#38750;&#24120;&#20445;&#23432;&#65292;&#20174;&#19981;&#23436;&#20840;&#30456;&#20449;&#12290;&#23427;&#21482;&#22312;&#24403;&#21069;&#26465;&#20214;&#19979;&#30456;&#20449;&#12290;&#36825;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#27491;&#30830;&#30340;&#65292;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#26159;&#38169;&#35823;&#30340;&#12290;&#21738;&#31181;&#24773;&#20917;&#26159;&#21738;&#31181;&#24773;&#20917;&#65311;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#38656;&#35201;&#23558;&#33258;&#28982;&#20462;&#27491;&#20174;&#34920;&#36798;&#26222;&#36941;&#30495;&#29702;&#30340;&#31616;&#21333;&#20844;&#24335;&#65288;&#26576;&#29289;&#25104;&#31435;&#65289;&#25193;&#23637;&#21040;&#34920;&#36798;&#26465;&#20214;&#30495;&#29702;&#30340;&#26465;&#20214;&#35821;&#21477;&#65288;&#26576;&#31181;&#24773;&#20917;&#19979;&#25104;&#31435;&#65289;&#12290;&#36825;&#31181;&#25193;&#23637;&#22522;&#20110;&#33258;&#28982;&#20462;&#27491;&#36981;&#24490;&#30340;&#22522;&#26412;&#21407;&#21017;&#65292;&#34987;&#30830;&#23450;&#20026;&#26368;&#23567;&#25913;&#21464;&#12289;&#28448;&#19981;&#20851;&#24515;&#21644;&#22825;&#30495;&#65306;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#20449;&#24565;&#65307;&#40664;&#35748;&#24773;&#20917;&#19979;&#23558;&#22330;&#26223;&#30340;&#21487;&#33021;&#24615;&#35270;&#20026;&#30456;&#31561;&#65307;&#23545;&#25152;&#26377;&#24773;&#20917;&#25345;&#26377;&#20449;&#24565;&#65292;&#30452;&#21040;&#26377;&#30683;&#30462;&#21457;&#29983;&#12290;&#25193;&#23637;&#34920;&#26126;&#33258;&#28982;&#20462;&#27491;&#23558;&#20462;&#27491;&#38480;&#21046;&#22312;&#24403;&#21069;&#26465;&#20214;&#19979;&#12290;&#19982;&#19981;&#21463;&#38480;&#21046;&#30340;&#20462;&#27491;&#36827;&#34892;&#27604;&#36739;&#21487;&#20197;&#30830;&#23450;&#24403;&#21069;&#26465;&#20214;&#12290;&#23427;&#19981;&#26159;&#24403;&#21069;&#34987;&#35748;&#20026;&#26159;&#30495;&#30340;&#65292;&#22914;&#26524;&#19982;&#26032;&#20449;&#24687;&#30456;&#30683;&#30462;&#30340;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural revision seems so natural: it changes beliefs as little as possible to incorporate new information. Yet, some counterexamples show it wrong. It is so conservative that it never fully believes. It only believes in the current conditions. This is right in some cases and wrong in others. Which is which? The answer requires extending natural revision from simple formulae expressing universal truths (something holds) to conditionals expressing conditional truth (something holds in certain conditions). The extension is based on the basic principles natural revision follows, identified as minimal change, indifference and naivety: change beliefs as little as possible; equate the likeliness of scenarios by default; believe all until contradicted. The extension says that natural revision restricts changes to the current conditions. A comparison with an unrestricting revision shows what exactly the current conditions are. It is not what currently considered true if it contradicts the new 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;CT&#25195;&#25551;&#20998;&#31867;&#20013;&#30340;&#32467;&#26524;&#24448;&#24448;&#21482;&#20851;&#27880;&#20934;&#30830;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20844;&#27491;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#20449;&#21644;&#19981;&#36866;&#29992;&#20110;&#30495;&#23454;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2309.12632</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;CT&#25195;&#25551;&#20998;&#31867;&#20013;&#30340;&#32467;&#26524;&#26159;&#21542;&#20844;&#27491;&#21487;&#35299;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?. (arXiv:2309.12632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12632
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;CT&#25195;&#25551;&#20998;&#31867;&#20013;&#30340;&#32467;&#26524;&#24448;&#24448;&#21482;&#20851;&#27880;&#20934;&#30830;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20844;&#27491;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#20449;&#21644;&#19981;&#36866;&#29992;&#20110;&#30495;&#23454;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22270;&#20687;&#21644;&#29289;&#20307;&#20998;&#31867;&#20013;&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20063;&#38754;&#20020;&#30528;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#21160;&#35786;&#26029;&#26696;&#20363;&#30340;&#21387;&#21147;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#23581;&#35797;&#20165;&#20165;&#20851;&#27880;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19981;&#32771;&#34385;&#21487;&#35299;&#37322;&#24615;&#25110;&#32773;&#24739;&#32773;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#20998;&#31163;&#12290;&#20363;&#22914;&#65292;&#22823;&#37096;&#20998;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#32954;&#32467;&#33410;&#20998;&#31867;&#35770;&#25991;&#20250;&#23545;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#27927;&#29260;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#38598;&#65292;&#23548;&#33268;&#19968;&#20010;&#20154;&#30340;CT&#25195;&#25551;&#22270;&#20687;&#20013;&#30340;&#26576;&#20123;&#22270;&#20687;&#20301;&#20110;&#35757;&#32451;&#38598;&#20013;&#65292;&#32780;&#20854;&#20182;&#22270;&#20687;&#21017;&#20301;&#20110;&#39564;&#35777;&#25110;&#27979;&#35797;&#22270;&#20687;&#38598;&#20013;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#20934;&#30830;&#29575;&#25253;&#21578;&#21644;&#23398;&#20064;&#21040;&#30340;&#26080;&#20851;&#29305;&#24449;&#65292;&#26368;&#32456;&#38477;&#20302;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the great success of various deep learning methods in image and object classification, the biomedical image processing society is also overwhelmed with their applications to various automatic diagnosis cases. Unfortunately, most of the deep learning-based classification attempts in the literature solely focus on the aim of extreme accuracy scores, without considering interpretability, or patient-wise separation of training and test data. For example, most lung nodule classification papers using deep learning randomly shuffle data and split it into training, validation, and test sets, causing certain images from the CT scan of a person to be in the training set, while other images of the exact same person to be in the validation or testing image sets. This can result in reporting misleading accuracy rates and the learning of irrelevant features, ultimately reducing the real-life usability of these models. When the deep neural networks trained on the traditional, unfair data sh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#31995;&#32479;Q4FuturePOP&#65292;&#23427;&#21019;&#26032;&#22320;&#21033;&#29992;&#26410;&#26469;&#36164;&#20135;&#20215;&#20540;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#21160;&#20943;&#23569;&#25237;&#36164;&#33539;&#22260;&#30340;&#27169;&#22359;&#12290;&#36890;&#36807;&#23454;&#39564;&#35752;&#35770;&#20102;Q4FuturePOP&#30340;&#21407;&#22411;&#29256;&#26412;&#20013;&#19981;&#21516;&#27169;&#22359;&#30340;&#21021;&#27493;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12627</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#31995;&#32479;&#65306;&#21033;&#29992;&#26410;&#26469;&#36164;&#20135;&#20215;&#20540;&#21644;&#33258;&#21160;&#20943;&#23569;&#25237;&#36164;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
A Quantum Computing-based System for Portfolio Optimization using Future Asset Values and Automatic Reduction of the Investment Universe. (arXiv:2309.12627v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#31995;&#32479;Q4FuturePOP&#65292;&#23427;&#21019;&#26032;&#22320;&#21033;&#29992;&#26410;&#26469;&#36164;&#20135;&#20215;&#20540;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#21160;&#20943;&#23569;&#25237;&#36164;&#33539;&#22260;&#30340;&#27169;&#22359;&#12290;&#36890;&#36807;&#23454;&#39564;&#35752;&#35770;&#20102;Q4FuturePOP&#30340;&#21407;&#22411;&#29256;&#26412;&#20013;&#19981;&#21516;&#27169;&#22359;&#30340;&#21021;&#27493;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#37329;&#34701;&#20013;&#22791;&#21463;&#20851;&#27880;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#38024;&#23545;&#35813;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#25216;&#26415;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Q4FuturePOP&#30340;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#20197;&#19979;&#21019;&#26032;&#35299;&#20915;&#20102;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65306;i&#65289;&#35813;&#24037;&#20855;&#38024;&#23545;&#26410;&#26469;&#36164;&#20135;&#39044;&#27979;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#65307;ii&#65289;Q4FuturePOP&#21253;&#25324;&#19968;&#20010;&#26234;&#33021;&#20943;&#23569;&#38382;&#39064;&#22797;&#26434;&#24615;&#30340;&#33258;&#21160;&#20943;&#23569;&#25237;&#36164;&#33539;&#22260;&#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;Q4FuturePOP&#30340;&#21407;&#22411;&#29256;&#26412;&#30340;&#19981;&#21516;&#27169;&#22359;&#30340;&#21021;&#27493;&#24615;&#33021;&#36827;&#34892;&#20102;&#31616;&#35201;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the problems in quantitative finance that has received the most attention is the portfolio optimization problem. Regarding its solving, this problem has been approached using different techniques, with those related to quantum computing being especially prolific in recent years. In this study, we present a system called Quantum Computing-based System for Portfolio Optimization with Future Asset Values and Automatic Universe Reduction (Q4FuturePOP), which deals with the Portfolio Optimization Problem considering the following innovations: i) the developed tool is modeled for working with future prediction of assets, instead of historical values; and ii) Q4FuturePOP includes an automatic universe reduction module, which is conceived to intelligently reduce the complexity of the problem. We also introduce a brief discussion about the preliminary performance of the different modules that compose the prototypical version of Q4FuturePOP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31569;&#21512;&#21516;&#39118;&#38505;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#22791;&#24314;&#31569;&#21512;&#21516;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#19987;&#23478;&#30340;&#21512;&#21516;&#23457;&#26597;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#26080;&#38656;&#35843;&#25972;&#65292;&#33021;&#22815;&#35782;&#21035;&#24314;&#31569;&#21512;&#21516;&#39118;&#38505;&#65292;&#24182;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12626</link><description>&lt;p&gt;
&#22522;&#20110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31569;&#21512;&#21516;&#39118;&#38505;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Construction contract risk identification based on knowledge-augmented language model. (arXiv:2309.12626v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31569;&#21512;&#21516;&#39118;&#38505;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#22791;&#24314;&#31569;&#21512;&#21516;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#19987;&#23478;&#30340;&#21512;&#21516;&#23457;&#26597;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#26080;&#38656;&#35843;&#25972;&#65292;&#33021;&#22815;&#35782;&#21035;&#24314;&#31569;&#21512;&#21516;&#39118;&#38505;&#65292;&#24182;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24314;&#31569;&#39033;&#30446;&#20013;&#65292;&#21512;&#21516;&#23457;&#26597;&#26159;&#38450;&#27490;&#28508;&#22312;&#25439;&#22833;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29992;&#20110;&#23457;&#26597;&#24314;&#31569;&#21512;&#21516;&#30340;&#26041;&#27861;&#32570;&#20047;&#25928;&#26524;&#21644;&#21487;&#38752;&#24615;&#65292;&#23548;&#33268;&#32791;&#26102;&#21644;&#23481;&#26131;&#20986;&#38169;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25913;&#38761;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#35299;&#20915;&#19987;&#38376;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#22791;&#24314;&#31569;&#21512;&#21516;&#30693;&#35782;&#30340;LLMs&#26469;&#27169;&#25311;&#20154;&#31867;&#19987;&#23478;&#30340;&#21512;&#21516;&#23457;&#26597;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26080;&#35843;&#25972;&#26041;&#27861;&#23558;&#24314;&#31569;&#21512;&#21516;&#39046;&#22495;&#30693;&#35782;&#32467;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#35782;&#21035;&#24314;&#31569;&#21512;&#21516;&#39118;&#38505;&#12290;&#22312;&#26500;&#24314;&#39046;&#22495;&#30693;&#35782;&#24211;&#26102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26377;&#21161;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#30340;&#24314;&#31569;&#21512;&#21516;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#24212;&#29992;&#20110;&#24314;&#31569;&#21512;&#21516;&#30340;&#35782;&#21035;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contract review is an essential step in construction projects to prevent potential losses. However, the current methods for reviewing construction contracts lack effectiveness and reliability, leading to time-consuming and error-prone processes. While large language models (LLMs) have shown promise in revolutionizing natural language processing (NLP) tasks, they struggle with domain-specific knowledge and addressing specialized issues. This paper presents a novel approach that leverages LLMs with construction contract knowledge to emulate the process of contract review by human experts. Our tuning-free approach incorporates construction contract domain knowledge to enhance language models for identifying construction contract risks. The use of a natural language when building the domain knowledge base facilitates practical implementation. We evaluated our method on real construction contracts and achieved solid performance. Additionally, we investigated how large language models employ
&lt;/p&gt;</description></item><item><title>DRG-LLaMA&#26159;&#19968;&#20010;&#36890;&#36807;&#22312;&#20020;&#24202;&#31508;&#35760;&#19978;&#32454;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#20303;&#38498;&#24739;&#32773;&#30340;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;&#39044;&#27979;&#12290;&#22312;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#65292;DRG-LLaMA-7B&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#33021;&#22815;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#22522;&#26412;DRG&#21644;&#24182;&#21457;&#30151;/&#21512;&#24182;&#30151;&#65288;CC&#65289;/&#37325;&#22823;&#24182;&#21457;&#30151;&#25110;&#21512;&#24182;&#30151;&#65288;MCC&#65289;&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2309.12625</link><description>&lt;p&gt;
DRG-LLaMA: &#35843;&#20248;LLaMA&#27169;&#22411;&#20197;&#39044;&#27979;&#20303;&#38498;&#24739;&#32773;&#30340;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;
&lt;/p&gt;
&lt;p&gt;
DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients. (arXiv:2309.12625v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12625
&lt;/p&gt;
&lt;p&gt;
DRG-LLaMA&#26159;&#19968;&#20010;&#36890;&#36807;&#22312;&#20020;&#24202;&#31508;&#35760;&#19978;&#32454;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#20303;&#38498;&#24739;&#32773;&#30340;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;&#39044;&#27979;&#12290;&#22312;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#65292;DRG-LLaMA-7B&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#33021;&#22815;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#22522;&#26412;DRG&#21644;&#24182;&#21457;&#30151;/&#21512;&#24182;&#30151;&#65288;CC&#65289;/&#37325;&#22823;&#24182;&#21457;&#30151;&#25110;&#21512;&#24182;&#30151;&#65288;MCC&#65289;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32654;&#22269;&#20303;&#38498;&#20184;&#36153;&#31995;&#32479;&#20013;&#65292;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;&#65288;DRG&#65289;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#30446;&#21069;&#30340;&#20998;&#32452;&#36807;&#31243;&#32791;&#26102;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DRG-LLaMA&#65292;&#19968;&#20010;&#22312;&#20020;&#24202;&#31508;&#35760;&#19978;&#36827;&#34892;&#32454;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20197;&#25913;&#21892;DRG&#39044;&#27979;&#12290;&#20351;&#29992;Meta&#30340;LLaMA&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;236,192&#20010;MIMIC-IV&#20986;&#38498;&#25688;&#35201;&#19978;&#36827;&#34892;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#20248;&#21270;&#12290;&#22312;&#36755;&#20837;&#20196;&#29260;&#38271;&#24230;&#20026;512&#30340;&#24773;&#20917;&#19979;&#65292;DRG-LLaMA-7B&#23454;&#29616;&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#20026;0.327&#65292;&#39030;&#32423;&#39044;&#27979;&#20934;&#30830;&#24230;&#20026;52.0&#65285;&#65292;&#23439;&#24179;&#22343;AUC&#20026;0.986&#12290;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;DRG-LLaMA-7B&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#20043;&#21069;&#25253;&#36947;&#30340;&#39046;&#20808;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;ClinicalBERT&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;40.3&#65285;&#65292;&#30456;&#23545;&#20110;CAML&#25552;&#39640;&#20102;35.7&#65285;&#12290;&#24403;&#24212;&#29992;DRG-LLaMA&#26469;&#39044;&#27979;&#22522;&#26412;DRG&#21644;&#24182;&#21457;&#30151;/&#21512;&#24182;&#30151;&#65288;CC&#65289;/&#37325;&#22823;&#24182;&#21457;&#30151;&#25110;&#21512;&#24182;&#30151;&#65288;MCC&#65289;&#26102;&#65292;&#22522;&#26412;DRG&#30340;&#39030;&#32423;&#39044;&#27979;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;67.8&#65285;&#65292;&#32780;CC/MCC&#29366;&#24577;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;67.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) plays a key role but its current assignment process is time-consuming. We introduce DRG-LLaMA, a large language model (LLM) fine-tuned on clinical notes for improved DRG prediction. Using Meta's LLaMA as the base model, we optimized it with Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries. With an input token length of 512, DRG-LLaMA-7B achieved a macro-averaged F1 score of 0.327, a top-1 prediction accuracy of 52.0% and a macro-averaged Area Under the Curve (AUC) of 0.986. Impressively, DRG-LLaMA-7B surpassed previously reported leading models on this task, demonstrating a relative improvement in macro-averaged F1 score of 40.3% compared to ClinicalBERT and 35.7% compared to CAML. When DRG-LLaMA is applied to predict base DRGs and complication or comorbidity (CC) / major complication or comorbidity (MCC), the top-1 prediction accuracy reached 67.8% for base DRGs and 67.5% for CC/MCC status. DRG-L
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22253;&#33402;&#39046;&#22495;&#30340;&#22312;&#32447;&#38382;&#31572;&#25968;&#25454;&#26469;&#25552;&#39640;&#20892;&#19994;&#25945;&#32946;&#21644;&#31649;&#29702;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#21644;&#26102;&#38388;&#20998;&#26512;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#38382;&#39064;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.12579</link><description>&lt;p&gt;
&#20174;&#25991;&#23383;&#21040;&#36235;&#21183;&#65306;&#22253;&#33402;&#20998;&#26512;&#35270;&#35282;&#23545;&#29616;&#20195;&#20892;&#19994;&#30340;&#26410;&#26469;&#30340;&#29420;&#29305;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
From Text to Trends: A Unique Garden Analytics Perspective on the Future of Modern Agriculture. (arXiv:2309.12579v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12579
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22253;&#33402;&#39046;&#22495;&#30340;&#22312;&#32447;&#38382;&#31572;&#25968;&#25454;&#26469;&#25552;&#39640;&#20892;&#19994;&#25945;&#32946;&#21644;&#31649;&#29702;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#21644;&#26102;&#38388;&#20998;&#26512;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#38382;&#39064;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#27934;&#23519;&#23545;&#29616;&#20195;&#20892;&#19994;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#25105;&#20204;&#22312;&#22253;&#33402;&#39046;&#22495;&#25945;&#32946;&#21644;&#35302;&#21450;&#20154;&#32676;&#30340;&#26041;&#24335;&#12290;&#35813;&#26694;&#26550;&#20381;&#36182;&#20110;Horticulture Online Help Desk&#65288;HOHD&#65289;&#30340;&#25968;&#25454;&#65292;&#35813;&#25968;&#25454;&#24211;&#26159;&#22253;&#33402;&#29233;&#22909;&#32773;&#21644;&#22823;&#24072;&#22253;&#33402;&#24072;&#39033;&#30446;&#65288;EMGP&#65289;&#30340;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#26377;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#23427;&#20351;&#29992;&#29305;&#27530;&#30340;&#35745;&#31639;&#26426;&#31243;&#24207;&#65288;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65289;&#23558;&#38382;&#39064;&#20998;&#31867;&#12290;&#36825;&#24110;&#21161;&#25105;&#20204;&#24555;&#36895;&#23558;&#27599;&#20010;&#38382;&#39064;&#21457;&#36865;&#32473;&#27491;&#30830;&#30340;&#19987;&#23478;&#65292;&#20197;&#26356;&#24555;&#22320;&#22238;&#31572;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#23427;&#26681;&#25454;&#38382;&#39064;&#30340;&#25552;&#20986;&#26102;&#38388;&#26469;&#29468;&#27979;&#25105;&#20204;&#23558;&#26469;&#21487;&#33021;&#20250;&#25910;&#21040;&#22810;&#23569;&#38382;&#39064;&#20197;&#21450;&#23427;&#20204;&#30340;&#20869;&#23481;&#12290;&#36825;&#24110;&#21161;&#25105;&#20204;&#35745;&#21010;&#23558;&#26469;&#30495;&#27491;&#37325;&#35201;&#30340;&#20027;&#39064;&#12290;&#23601;&#20687;&#30693;&#36947;&#26410;&#26469;&#20960;&#20010;&#26376;&#23558;&#20250;&#26377;&#21738;&#20123;&#28909;&#38376;&#38382;&#39064;&#19968;&#26679;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#26597;&#30475;&#38382;&#39064;&#30340;&#26469;&#28304;&#26469;&#32771;&#34385;&#38382;&#39064;&#30340;&#22320;&#22495;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven insights are essential for modern agriculture. This research paper introduces a machine learning framework designed to improve how we educate and reach out to people in the field of horticulture. The framework relies on data from the Horticulture Online Help Desk (HOHD), which is like a big collection of questions from people who love gardening and are part of the Extension Master Gardener Program (EMGP). This framework has two main parts. First, it uses special computer programs (machine learning models) to sort questions into categories. This helps us quickly send each question to the right expert, so we can answer it faster. Second, it looks at when questions are asked and uses that information to guess how many questions we might get in the future and what they will be about. This helps us plan on topics that will be really important. It's like knowing what questions will be popular in the coming months. We also take into account where the questions come from by looking
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#28436;&#21270;&#30340;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#27491;&#21017;&#21270;&#36827;&#21270;&#31639;&#27861;&#23545;&#27169;&#22411;&#32467;&#26500;&#28436;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#35752;&#35770;&#20102;&#20998;&#24067;&#24335;&#21644;&#32531;&#23384;&#26041;&#38754;&#30340;&#26426;&#20250;&#65292;&#20197;&#21450;&#27169;&#22411;&#26550;&#26500;&#27969;&#34892;&#24230;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.12576</link><description>&lt;p&gt;
&#29702;&#35299;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#28436;&#21270;&#30340;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Understanding Patterns of Deep Learning ModelEvolution in Network Architecture Search. (arXiv:2309.12576v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#28436;&#21270;&#30340;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#27491;&#21017;&#21270;&#36827;&#21270;&#31639;&#27861;&#23545;&#27169;&#22411;&#32467;&#26500;&#28436;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#35752;&#35770;&#20102;&#20998;&#24067;&#24335;&#21644;&#32531;&#23384;&#26041;&#38754;&#30340;&#26426;&#20250;&#65292;&#20197;&#21450;&#27169;&#22411;&#26550;&#26500;&#27969;&#34892;&#24230;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#21644;&#20855;&#20307;&#26469;&#35828;&#26159;&#27491;&#21017;&#21270;&#36827;&#21270;&#26159;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32467;&#26500;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#22312;&#23454;&#38469;&#28436;&#21270;&#30340;&#36807;&#31243;&#20013;&#30693;&#20043;&#29978;&#23569;&#65292;&#36825;&#23545;&#20110;&#35774;&#35745;&#32531;&#23384;&#31574;&#30053;&#12289;&#25913;&#36827;&#29305;&#23450;&#24212;&#29992;&#30340;&#25628;&#32034;&#31639;&#27861;&#20197;&#21450;&#20854;&#20182;&#37325;&#35201;&#29992;&#20363;&#20855;&#26377;&#35774;&#35745;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31639;&#27861;&#20998;&#26512;&#21644;&#23450;&#37327;&#25551;&#36848;&#20102;Candle&#39033;&#30446;&#21644;Nasbench-201&#25628;&#32034;&#31354;&#38388;&#20013;&#19968;&#32452;&#27169;&#22411;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#32467;&#26500;&#30340;&#28436;&#21270;&#21463;&#21040;&#27491;&#21017;&#21270;&#36827;&#21270;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#28436;&#21270;&#27169;&#24335;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#21644;&#32531;&#23384;&#21644;&#25913;&#36827;&#35843;&#24230;&#26041;&#38754;&#30340;&#26426;&#20250;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24433;&#21709;&#29305;&#23450;&#27169;&#22411;&#26550;&#26500;&#20309;&#26102;&#22312;&#27969;&#34892;&#24230;&#19978;&#21319;&#21644;&#19979;&#38477;&#30340;&#26465;&#20214;&#65292;&#36825;&#26159;&#22522;&#20110;&#23427;&#20204;&#22312;&#28369;&#21160;&#31383;&#21475;&#20013;&#20805;&#24403;&#25424;&#29486;&#32773;&#30340;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network Architecture Search and specifically Regularized Evolution is a common way to refine the structure of a deep learning model.However, little is known about how models empirically evolve over time which has design implications for designing caching policies, refining the search algorithm for particular applications, and other important use cases.In this work, we algorithmically analyze and quantitatively characterize the patterns of model evolution for a set of models from the Candle project and the Nasbench-201 search space.We show how the evolution of the model structure is influenced by the regularized evolution algorithm. We describe how evolutionary patterns appear in distributed settings and opportunities for caching and improved scheduling. Lastly, we describe the conditions that affect when particular model architectures rise and fall in popularity based on their frequency of acting as a donor in a sliding window.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#24182;&#21457;&#29616;&#20316;&#23478;&#20204;&#26356;&#20542;&#21521;&#20110;&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#38454;&#27573;&#20013;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2309.12570</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#30340;&#21019;&#36896;&#21147;&#25903;&#25345;: &#19968;&#39033;&#28041;&#21450;&#26032;&#20852;&#20316;&#23478;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers. (arXiv:2309.12570v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#24182;&#21457;&#29616;&#20316;&#23478;&#20204;&#26356;&#20542;&#21521;&#20110;&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#38454;&#27573;&#20013;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#20854;&#33021;&#22815;&#36981;&#24490;&#25351;&#20196;&#24182;&#21442;&#19982;&#23545;&#35805;&#20114;&#21160;&#65292;&#24341;&#21457;&#20102;&#22312;&#21508;&#31181;&#25903;&#25345;&#24037;&#20855;&#20013;&#21033;&#29992;&#23427;&#20204;&#30340;&#20852;&#36259;&#22686;&#21152;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#23454;&#35777;&#29992;&#25143;&#30740;&#31350;&#65288;n=30&#65289;&#25506;&#35752;&#20102;&#29616;&#20195;LLM&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#21512;&#20316;&#20889;&#20316;&#30028;&#38754;&#35774;&#35745;&#22522;&#20110;&#23558;&#20889;&#20316;&#35270;&#20026;&#19968;&#20010;&#30446;&#26631;&#23548;&#21521;&#30340;&#24605;&#32500;&#36807;&#31243;&#30340;&#35748;&#30693;&#36807;&#31243;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#38750;&#32447;&#24615;&#30340;&#35748;&#30693;&#27963;&#21160;&#65306;&#35268;&#21010;&#12289;&#32763;&#35793;&#21644;&#23457;&#26597;&#12290;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#25552;&#20132;&#19968;&#20221;&#21518;&#23436;&#25104;&#35843;&#26597;&#65292;&#20197;&#25552;&#20379;&#20851;&#20110;LLM&#20316;&#20026;&#20889;&#20316;&#21512;&#20316;&#32773;&#28508;&#21147;&#21644;&#38382;&#39064;&#30340;&#21453;&#39304;&#12290;&#36890;&#36807;&#20998;&#26512;&#20316;&#23478;-LLM&#20114;&#21160;,&#25105;&#20204;&#21457;&#29616;&#20316;&#23478;&#22312;&#19977;&#31181;&#31867;&#22411;&#30340;&#35748;&#30693;&#27963;&#21160;&#20013;&#37117;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#65292;&#20294;&#20182;&#20204;&#21457;&#29616;LLM&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#26041;&#38754;&#26356;&#26377;&#24110;&#21161;&#12290;&#36890;&#36807;&#20998;&#26512;&#20114;&#21160;&#21644;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) capable of following instructions and engaging in conversational interactions sparked increased interest in their utilization across various support tools. We investigate the utility of modern LLMs in assisting professional writers via an empirical user study (n=30). The design of our collaborative writing interface is grounded in the cognitive process model of writing that views writing as a goal-oriented thinking process encompassing non-linear cognitive activities: planning, translating, and reviewing. Participants are asked to submit a post-completion survey to provide feedback on the potential and pitfalls of LLMs as writing collaborators. Upon analyzing the writer-LLM interactions, we find that while writers seek LLM's help across all three types of cognitive activities, they find LLMs more helpful in translation and reviewing. Our findings from analyzing both the interactions and the survey responses highlight future research direc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#32508;&#21512;&#30740;&#31350;&#20102;&#21033;&#29992;&#22810;&#27169;&#24577;&#24863;&#30693;&#23398;&#20064;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#22797;&#26434;&#32780;&#24494;&#22937;&#30340;&#31038;&#20132;&#20114;&#21160;&#65292;&#32780;&#26080;&#38656;&#25163;&#24037;&#21046;&#20316;&#31616;&#21270;&#27169;&#22411;&#25110;&#25104;&#26412;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.12568</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#21033;&#29992;&#22810;&#27169;&#24577;&#24863;&#30693;&#23398;&#20064;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Learning Social Robot Navigation with Multimodal Perception. (arXiv:2309.12568v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#32508;&#21512;&#30740;&#31350;&#20102;&#21033;&#29992;&#22810;&#27169;&#24577;&#24863;&#30693;&#23398;&#20064;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#22797;&#26434;&#32780;&#24494;&#22937;&#30340;&#31038;&#20132;&#20114;&#21160;&#65292;&#32780;&#26080;&#38656;&#25163;&#24037;&#21046;&#20316;&#31616;&#21270;&#27169;&#22411;&#25110;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#38656;&#35201;&#21033;&#29992;&#20854;&#25645;&#36733;&#30340;&#20256;&#24863;&#22120;&#65288;&#22914;LiDAR&#21644;RGB&#25668;&#20687;&#22836;&#65289;&#24863;&#30693;&#29615;&#22659;&#65292;&#28982;&#21518;&#20570;&#20986;&#36866;&#24403;&#30340;&#23548;&#33322;&#20915;&#31574;&#12290;&#20026;&#20102;&#22312;&#20154;&#31867;&#23621;&#20303;&#30340;&#20844;&#20849;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#20219;&#21153;&#65292;&#36825;&#26679;&#30340;&#23548;&#33322;&#20219;&#21153;&#19981;&#20165;&#20165;&#26159;&#36991;&#24320;&#38556;&#30861;&#29289;&#65292;&#36824;&#38656;&#35201;&#32771;&#34385;&#21608;&#22260;&#30340;&#20154;&#21644;&#20182;&#20204;&#30340;&#24847;&#22270;&#65292;&#20197;&#22312;&#23545;&#24213;&#23618;&#31038;&#20250;&#35268;&#33539;&#30340;&#21709;&#24212;&#20013;&#25913;&#21464;&#23548;&#33322;&#34892;&#20026;&#65292;&#21363;&#31038;&#20250;&#21512;&#35268;&#24615;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#26041;&#24335;&#25429;&#25417;&#36825;&#20123;&#22797;&#26434;&#32780;&#24494;&#22937;&#30340;&#31038;&#20132;&#20114;&#21160;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#25163;&#24037;&#21046;&#20316;&#31616;&#21270;&#27169;&#22411;&#25110;&#25104;&#26412;&#20989;&#25968;&#12290;&#32771;&#34385;&#21040;&#22810;&#31181;&#21487;&#29992;&#30340;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#23398;&#20064;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#21033;&#29992;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#22810;&#27169;&#24577;&#24863;&#30693;&#23398;&#20064;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#32508;&#21512;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#22312;&#20840;&#23616;&#21644;&#23616;&#37096;&#23618;&#38754;&#19978;&#25506;&#35752;&#20102;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#20915;&#31574;&#21046;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous mobile robots need to perceive the environments with their onboard sensors (e.g., LiDARs and RGB cameras) and then make appropriate navigation decisions. In order to navigate human-inhabited public spaces, such a navigation task becomes more than only obstacle avoidance, but also requires considering surrounding humans and their intentions to somewhat change the navigation behavior in response to the underlying social norms, i.e., being socially compliant. Machine learning methods are shown to be effective in capturing those complex and subtle social interactions in a data-driven manner, without explicitly hand-crafting simplified models or cost functions. Considering multiple available sensor modalities and the efficiency of learning methods, this paper presents a comprehensive study on learning social robot navigation with multimodal perception using a large-scale real-world dataset. The study investigates social robot navigation decision making on both the global and loca
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22238;&#39038;&#20102;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21069;&#27839;&#25216;&#26415;&#21644;&#26368;&#26032;&#36235;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#38454;&#27573;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.12560</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#19982;&#20808;&#36827;&#26426;&#22120;&#20154;&#25805;&#20316;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Meets Advanced Robotic Manipulation. (arXiv:2309.12560v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22238;&#39038;&#20102;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21069;&#27839;&#25216;&#26415;&#21644;&#26368;&#26032;&#36235;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#38454;&#27573;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#24037;&#19994;&#23548;&#33268;&#39640;&#36136;&#37327;&#29983;&#20135;&#65292;&#38477;&#20302;&#21046;&#36896;&#25104;&#26412;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#20154;&#21147;&#36164;&#28304;&#12290;&#26426;&#22120;&#20154;&#25805;&#32437;&#33218;&#22312;&#33258;&#21160;&#21270;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#32534;&#20889;&#39640;&#25928;&#19988;&#23433;&#20840;&#30340;&#36712;&#36857;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#30340;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26377;&#28508;&#21147;&#22522;&#20110;&#19987;&#23478;&#28436;&#31034;&#26469;&#23398;&#20064;&#36825;&#26679;&#30340;&#25511;&#21046;&#22120;&#12290;&#23613;&#31649;&#26377;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#38656;&#24320;&#21457;&#26356;&#22909;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#38454;&#27573;&#30340;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#22238;&#39038;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21069;&#27839;&#25216;&#26415;&#21644;&#26368;&#26032;&#36235;&#21183;&#12290;&#22312;&#22238;&#39038;&#26426;&#22120;&#23398;&#20064;&#30340;&#30456;&#20851;&#32972;&#26223;&#20043;&#21518;&#65292;&#25991;&#31456;&#20854;&#20313;&#37096;&#20998;&#35814;&#32454;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24037;&#19994;&#12289;&#21307;&#30103;&#12289;&#20892;&#19994;&#12289;&#31354;&#38388;&#12289;&#20891;&#20107;&#21644;&#25628;&#25937;&#31561;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#25991;&#31456;&#26368;&#21518;&#24635;&#32467;&#20102;&#26410;&#26469;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated industries lead to high quality production, lower manufacturing cost and better utilization of human resources. Robotic manipulator arms have major role in the automation process. However, for complex manipulation tasks, hard coding efficient and safe trajectories is challenging and time consuming. Machine learning methods have the potential to learn such controllers based on expert demonstrations. Despite promising advances, better approaches must be developed to improve safety, reliability, and efficiency of ML methods in both training and deployment phases. This survey aims to review cutting edge technologies and recent trends on ML methods applied to real-world manipulation tasks. After reviewing the related background on ML, the rest of the paper is devoted to ML applications in different domains such as industry, healthcare, agriculture, space, military, and search and rescue. The paper is closed with important research directions for future works.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12559</link><description>&lt;p&gt;
&#36890;&#36807;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#36827;&#34892;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37326;&#22806;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#26410;&#30693;&#30340;&#12289;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#27979;&#35797;&#20998;&#24067;&#65292;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26368;&#36817;&#20174;&#22240;&#26524;&#24615;&#24341;&#21457;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;OOD&#27867;&#21270;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#32780;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#23646;&#24615;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#19968;&#20010;&#24517;&#35201;&#20294;&#19981;&#20805;&#20998;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#23545;&#20110;&#20998;&#24067;&#36716;&#25442;&#26159;&#19981;&#21464;&#30340;&#65292;&#20294;&#21487;&#33021;&#27809;&#26377;&#25152;&#38656;&#30340;&#20934;&#30830;&#24230;&#12290;&#30456;&#21453;&#65292;&#19968;&#20010;&#20805;&#20998;&#20294;&#19981;&#24517;&#35201;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#20542;&#21521;&#20110;&#24456;&#22909;&#22320;&#36866;&#24212;&#29305;&#23450;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#25429;&#25417;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#32463;&#20856;&#27010;&#24565;&#8212;&#8212;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#65292;&#23427;&#25351;&#31034;&#20102;&#19968;&#20010;&#22240;&#32032;&#26159;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#30340;&#27010;&#29575;&#12290;&#20026;&#20102;&#23558;PNS&#19982;OOD&#27867;&#21270;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose
&lt;/p&gt;</description></item><item><title>PlanFitting&#26159;&#19968;&#20010;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#24110;&#21161;&#29992;&#25143;&#23450;&#21046;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#65292;&#24182;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#23427;&#29983;&#25104;&#20010;&#24615;&#21270;&#12289;&#21487;&#25805;&#20316;&#21644;&#26377;&#25454;&#21487;&#20381;&#30340;&#36816;&#21160;&#35745;&#21010;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12555</link><description>&lt;p&gt;
PlanFitting&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#21046;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
PlanFitting: Tailoring Personalized Exercise Plans with Large Language Models. (arXiv:2309.12555v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12555
&lt;/p&gt;
&lt;p&gt;
PlanFitting&#26159;&#19968;&#20010;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#24110;&#21161;&#29992;&#25143;&#23450;&#21046;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#65292;&#24182;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#23427;&#29983;&#25104;&#20010;&#24615;&#21270;&#12289;&#21487;&#25805;&#20316;&#21644;&#26377;&#25454;&#21487;&#20381;&#30340;&#36816;&#21160;&#35745;&#21010;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#23545;&#20110;&#30830;&#20445;&#36275;&#22815;&#30340;&#20307;&#32946;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#20154;&#20204;&#30340;&#22797;&#26434;&#26085;&#31243;&#21644;&#32771;&#34385;&#22240;&#32032;&#20197;&#21450;&#35745;&#21010;&#30340;&#21019;&#24314;&#36890;&#24120;&#38656;&#35201;&#19982;&#19987;&#23478;&#30340;&#21453;&#22797;&#27807;&#36890;&#65292;&#36825;&#19968;&#36807;&#31243;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PlanFitting&#65292;&#23427;&#26159;&#19968;&#20010;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#21487;&#20197;&#36741;&#21161;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;PlanFitting&#20351;&#29992;&#25143;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21508;&#31181;&#32422;&#26463;&#21644;&#26597;&#35810;&#65292;&#20174;&#32780;&#20415;&#20110;&#21019;&#24314;&#21644;&#20248;&#21270;&#36866;&#21512;&#20854;&#29305;&#23450;&#24773;&#20917;&#30340;&#27599;&#21608;&#36816;&#21160;&#35745;&#21010;&#65292;&#24182;&#20445;&#25345;&#22522;&#26412;&#21407;&#21017;&#30340;&#25166;&#26681;&#12290;&#36890;&#36807;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#21442;&#19982;&#32773;&#65288;N=18&#65289;&#20351;&#29992;PlanFitting&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#65292;&#32780;&#19987;&#23478;&#35268;&#21010;&#32773;&#65288;N=3&#65289;&#23545;&#36825;&#20123;&#35745;&#21010;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;PlanFitting&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#12289;&#21487;&#25805;&#20316;&#21644;&#26377;&#25454;&#21487;&#20381;&#30340;&#36816;&#21160;&#35745;&#21010;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;AI&#21161;&#25163;&#22312;&#21019;&#24314;&#35745;&#21010;&#26041;&#38754;&#30340;&#26410;&#26469;&#35774;&#35745;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
A personally tailored exercise regimen is crucial to ensuring sufficient physical activities, yet challenging to create as people have complex schedules and considerations and the creation of plans often requires iterations with experts. We present PlanFitting, a conversational AI that assists in personalized exercise planning. Leveraging generative capabilities of large language models, PlanFitting enables users to describe various constraints and queries in natural language, thereby facilitating the creation and refinement of their weekly exercise plan to suit their specific circumstances while staying grounded in foundational principles. Through a user study where participants (N=18) generated a personalized exercise plan using PlanFitting and expert planners (N=3) evaluated these plans, we identified the potential of PlanFitting in generating personalized, actionable, and evidence-based exercise plans. We discuss future design opportunities for AI assistants in creating plans that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROPLACE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20445;&#25345;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#29983;&#25104;&#19981;&#21512;&#29702;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12545</link><description>&lt;p&gt;
&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation. (arXiv:2309.12545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROPLACE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20445;&#25345;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#29983;&#25104;&#19981;&#21512;&#29702;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;(CEs)&#20316;&#20026;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#20027;&#35201;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36890;&#24120;&#65292;CEs&#23545;&#20110;&#36755;&#20837;-&#36755;&#20986;&#23545;&#34987;&#23450;&#20041;&#20026;&#21040;&#36755;&#20837;&#30340;&#26368;&#23567;&#36317;&#31163;&#30340;&#25968;&#25454;&#28857;&#65292;&#20854;&#19982;&#36755;&#20986;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;CEs&#22312;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;(&#27604;&#22914;&#37325;&#26032;&#35757;&#32451;)&#26102;&#24456;&#23481;&#26131;&#34987;&#26080;&#25928;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#22411;&#21442;&#25968;&#21464;&#21270;&#30340;&#33539;&#25968;&#29699;&#30028;&#38480;&#26469;&#35777;&#26126;CEs&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38024;&#23545;&#36825;&#31181;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#19981;&#26159;&#23436;&#20840;&#27491;&#30830;&#30340;&#65292;&#25110;&#32773;&#21487;&#33021;&#29983;&#25104;&#19981;&#21512;&#29702;&#30340;CEs&#65292;&#21363;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#23384;&#22312;&#31163;&#32676;&#20540;&#12290;&#20107;&#23454;&#19978;&#65292;&#30446;&#21069;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#36317;&#31163;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#20445;&#25345;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROPLACE&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations (CEs) have received increasing interest as a major methodology for explaining neural network classifiers. Usually, CEs for an input-output pair are defined as data points with minimum distance to the input that are classified with a different label than the output. To tackle the established problem that CEs are easily invalidated when model parameters are updated (e.g. retrained), studies have proposed ways to certify the robustness of CEs under model parameter changes bounded by a norm ball. However, existing methods targeting this form of robustness are not sound or complete, and they may generate implausible CEs, i.e., outliers wrt the training dataset. In fact, no existing method simultaneously optimises for proximity and plausibility while preserving robustness guarantees. In this work, we propose Provably RObust and PLAusible Counterfactual Explanations (PROPLACE), a method leveraging on robust optimisation techniques to address the aforementioned limi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24418;&#24577;-&#29615;&#22659;&#20849;&#21516;&#36827;&#21270;&#30340;&#26041;&#24335;&#65292;&#20248;&#21270;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#21644;&#24418;&#24577;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#35757;&#32451;&#20855;&#26377;&#27867;&#21270;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#35838;&#31243;&#65292;&#24182;&#33258;&#21160;&#25913;&#21464;&#29615;&#22659;&#21644;&#24418;&#24577;&#12290;</title><link>http://arxiv.org/abs/2309.12529</link><description>&lt;p&gt;
&#36890;&#36807;&#24418;&#24577;-&#29615;&#22659;&#20849;&#21516;&#36827;&#21270;&#30340;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curriculum Reinforcement Learning via Morphology-Environment Co-Evolution. (arXiv:2309.12529v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24418;&#24577;-&#29615;&#22659;&#20849;&#21516;&#36827;&#21270;&#30340;&#26041;&#24335;&#65292;&#20248;&#21270;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#21644;&#24418;&#24577;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#35757;&#32451;&#20855;&#26377;&#27867;&#21270;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#35838;&#31243;&#65292;&#24182;&#33258;&#21160;&#25913;&#21464;&#29615;&#22659;&#21644;&#24418;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26102;&#38388;&#30340;&#28436;&#21270;&#36807;&#31243;&#20013;&#65292;&#33258;&#28982;&#29289;&#31181;&#36890;&#36807;&#36827;&#21270;&#20854;&#36523;&#20307;&#32467;&#26500;&#20197;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#26469;&#23398;&#20250;&#29983;&#23384;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35757;&#32451;&#20855;&#26377;&#22266;&#23450;&#24418;&#24577;&#65288;&#22914;&#39592;&#26550;&#32467;&#26500;&#21644;&#20851;&#33410;&#23646;&#24615;&#65289;&#30340;&#20195;&#29702;&#22312;&#22266;&#23450;&#29615;&#22659;&#20013;&#23398;&#20064;&#65292;&#24456;&#38590;&#25512;&#24191;&#21040;&#21464;&#21270;&#30340;&#29615;&#22659;&#25110;&#26032;&#20219;&#21153;&#12290;&#26412;&#25991;&#36890;&#36807;&#8220;&#24418;&#24577;-&#29615;&#22659;&#20849;&#21516;&#36827;&#21270;&#65288;MECE&#65289;&#8221;&#26469;&#20248;&#21270;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#21644;&#20854;&#24418;&#24577;&#65292;&#20854;&#20013;&#24418;&#24577;&#19981;&#26029;&#26356;&#26032;&#20197;&#36866;&#24212;&#21464;&#21270;&#30340;&#29615;&#22659;&#65292;&#21516;&#26102;&#29615;&#22659;&#36880;&#28176;&#20462;&#25913;&#20197;&#24102;&#26469;&#26032;&#30340;&#25361;&#25112;&#24182;&#20419;&#36827;&#24418;&#24577;&#30340;&#25913;&#21892;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#35757;&#32451;&#20855;&#26377;&#27867;&#21270;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#35838;&#31243;&#65292;&#20854;&#24418;&#24577;&#21644;&#31574;&#30053;&#38024;&#23545;&#19981;&#21516;&#30340;&#29615;&#22659;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#20004;&#31181;&#31574;&#30053;&#26469;&#33258;&#21160;&#25913;&#21464;&#24418;&#24577;&#21644;&#29615;&#22659;&#65292;&#32780;&#19981;&#26159;&#25163;&#24037;&#35774;&#35745;&#35838;&#31243;&#12290;&#20026;&#27492;&#65292;&#65288;1&#65289;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout long history, natural species have learned to survive by evolving their physical structures adaptive to the environment changes. In contrast, current reinforcement learning (RL) studies mainly focus on training an agent with a fixed morphology (e.g., skeletal structure and joint attributes) in a fixed environment, which can hardly generalize to changing environments or new tasks. In this paper, we optimize an RL agent and its morphology through ``morphology-environment co-evolution (MECE)'', in which the morphology keeps being updated to adapt to the changing environment, while the environment is modified progressively to bring new challenges and stimulate the improvement of the morphology. This leads to a curriculum to train generalizable RL, whose morphology and policy are optimized for different environments. Instead of hand-crafting the curriculum, we train two policies to automatically change the morphology and the environment. To this end, (1) we develop two novel and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#30740;&#31350;&#29366;&#24577;&#65292;&#20171;&#32461;&#20102;&#20004;&#20010;&#20027;&#35201;&#20998;&#25903;&#65306;&#22522;&#20110;&#36317;&#31163;&#21644;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#36824;&#35752;&#35770;&#20102;CompoundE&#21644;CompoundE3D&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#30740;&#31350;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.12501</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding: An Overview. (arXiv:2309.12501v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12501
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#30740;&#31350;&#29366;&#24577;&#65292;&#20171;&#32461;&#20102;&#20004;&#20010;&#20027;&#35201;&#20998;&#25903;&#65306;&#22522;&#20110;&#36317;&#31163;&#21644;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#36824;&#35752;&#35770;&#20102;CompoundE&#21644;CompoundE3D&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#30740;&#31350;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25968;&#23398;&#27169;&#22411;&#24050;&#34987;&#21033;&#29992;&#26469;&#35774;&#35745;&#23884;&#20837;&#65292;&#20197;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#21644;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#25968;&#23398;&#21551;&#21457;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#22823;&#22411;KG&#20013;&#36827;&#34892;&#25512;&#29702;&#26102;&#39640;&#24230;&#21487;&#25193;&#23637;&#65292;&#32780;&#19988;&#22312;&#24314;&#27169;&#19981;&#21516;&#20851;&#31995;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#24456;&#22810;&#21487;&#35299;&#37322;&#30340;&#20248;&#21183;&#65292;&#36825;&#20123;&#20248;&#21183;&#21487;&#20197;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#21644;&#32463;&#39564;&#32467;&#26524;&#26469;&#39564;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;KG&#23436;&#25104;&#39046;&#22495;&#30340;&#24403;&#21069;&#30740;&#31350;&#29366;&#24577;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30528;&#37325;&#20171;&#32461;&#20102;KG&#23884;&#20837;&#65288;KGE&#65289;&#35774;&#35745;&#30340;&#20004;&#20010;&#20027;&#35201;&#20998;&#25903;&#65306;1&#65289;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#21644;2&#65289;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#36235;&#21183;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#26126;&#26032;&#39062;&#19988;&#26356;&#26377;&#25928;&#30340;&#27169;&#22411;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20174;2D&#21644;3D&#20223;&#23556;&#25805;&#20316;&#20013;&#27762;&#21462;&#28789;&#24863;&#30340;CompoundE&#21644;CompoundE3D&#12290;&#23427;&#20204;&#28085;&#30422;&#20102;&#21253;&#25324;dis&#22312;&#20869;&#30340;&#24191;&#27867;&#25216;&#26415;&#35859;&#35789;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many mathematical models have been leveraged to design embeddings for representing Knowledge Graph (KG) entities and relations for link prediction and many downstream tasks. These mathematically-inspired models are not only highly scalable for inference in large KGs, but also have many explainable advantages in modeling different relation patterns that can be validated through both formal proofs and empirical results. In this paper, we make a comprehensive overview of the current state of research in KG completion. In particular, we focus on two main branches of KG embedding (KGE) design: 1) distance-based methods and 2) semantic matching-based methods. We discover the connections between recently proposed models and present an underlying trend that might help researchers invent novel and more effective models. Next, we delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D affine operations, respectively. They encompass a broad spectrum of techniques including dis
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#21644;&#23376;&#35789;&#26631;&#35760;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#26159;&#23548;&#33268;&#24615;&#21035;&#20559;&#35265;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#32780;&#23376;&#35789;&#25286;&#20998;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#20998;&#26512;&#23376;&#35789;&#25286;&#20998;&#21487;&#20197;&#24456;&#22909;&#22320;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20165;&#24494;&#35843;&#26631;&#35760;&#23884;&#20837;&#23618;&#21487;&#20197;&#20943;&#23569;&#22899;&#24615;&#21644;&#30007;&#24615;&#20043;&#38388;&#24615;&#21035;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.12491</link><description>&lt;p&gt;
&#25506;&#32034;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#21644;&#23376;&#35789;&#26631;&#35760;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation. (arXiv:2309.12491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12491
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#21644;&#23376;&#35789;&#26631;&#35760;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#26159;&#23548;&#33268;&#24615;&#21035;&#20559;&#35265;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#32780;&#23376;&#35789;&#25286;&#20998;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#20998;&#26512;&#23376;&#35789;&#25286;&#20998;&#21487;&#20197;&#24456;&#22909;&#22320;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20165;&#24494;&#35843;&#26631;&#35760;&#23884;&#20837;&#23618;&#21487;&#20197;&#20943;&#23569;&#22899;&#24615;&#21644;&#30007;&#24615;&#20043;&#38388;&#24615;&#21035;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26631;&#35760;&#21270;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#34987;&#22823;&#22810;&#25968;&#20154;&#24573;&#35270;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#32844;&#19994;&#21517;&#31216;&#30340;&#39057;&#29575;&#12289;&#23427;&#20204;&#22312;&#23376;&#35789;&#26631;&#35760;&#22120;&#35789;&#27719;&#34920;&#20013;&#30340;&#34920;&#31034;&#20197;&#21450;&#24615;&#21035;&#20559;&#35265;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22899;&#24615;&#21644;&#38750;&#21051;&#26495;&#21360;&#35937;&#30340;&#24615;&#21035;&#32844;&#19994;&#21517;&#31216;&#30340;&#21464;&#24418;&#65288;&#20363;&#22914;&#65292;&#35199;&#29677;&#29273;&#35821;&#20013;&#30340;"doctora"&#34920;&#31034;"&#22899;&#21307;&#29983;"&#65289;&#24448;&#24448;&#34987;&#25286;&#20998;&#25104;&#22810;&#20010;&#23376;&#35789;&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#26159;&#23548;&#33268;&#24615;&#21035;&#20559;&#35265;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#20854;&#24433;&#21709;&#22823;&#20110;&#23376;&#35789;&#25286;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#26512;&#23376;&#35789;&#25286;&#20998;&#21487;&#20197;&#24456;&#22909;&#22320;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#35821;&#26009;&#24211;&#19981;&#20844;&#24320;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#20165;&#24494;&#35843;&#26631;&#35760;&#23884;&#20837;&#23618;&#21487;&#20197;&#20943;&#23569;&#22899;&#24615;&#21644;&#30007;&#24615;&#20043;&#38388;&#24615;&#21035;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the effect of tokenization on gender bias in machine translation, an aspect that has been largely overlooked in previous works. Specifically, we focus on the interactions between the frequency of gendered profession names in training data, their representation in the subword tokenizer's vocabulary, and gender bias. We observe that female and non-stereotypical gender inflections of profession names (e.g., Spanish "doctora" for "female doctor") tend to be split into multiple subword tokens. Our results indicate that the imbalance of gender forms in the model's training corpus is a major factor contributing to gender bias and has a greater impact than subword splitting. We show that analyzing subword splits provides good estimates of gender-form imbalance in the training data and can be used even when the corpus is not publicly available. We also demonstrate that fine-tuning just the token embedding layer can decrease the gap in gender prediction accuracy between female and male 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#65292;&#21457;&#29616;LLM&#22312;&#25512;&#29702;&#20013;&#23384;&#22312;&#31867;&#20284;&#20110;&#20154;&#31867;&#21551;&#21457;&#24335;&#25512;&#29702;&#30340;&#38169;&#35823;&#65292;&#20294;&#19982;&#20154;&#31867;&#25512;&#29702;&#26377;&#37325;&#35201;&#24046;&#24322;&#65292;&#26368;&#26032;&#30340;LLM&#29256;&#26412;&#20960;&#20046;&#28040;&#38500;&#20102;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#23545;&#30456;&#21516;&#30340;&#25552;&#31034;&#26041;&#26696;&#30340;&#21453;&#24212;&#19981;&#21516;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#25105;&#20204;&#30340;&#35748;&#35782;&#35770;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.12485</link><description>&lt;p&gt;
&#30740;&#31350;&#21644;&#25913;&#36827;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Studying and improving reasoning in humans and machines. (arXiv:2309.12485v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#65292;&#21457;&#29616;LLM&#22312;&#25512;&#29702;&#20013;&#23384;&#22312;&#31867;&#20284;&#20110;&#20154;&#31867;&#21551;&#21457;&#24335;&#25512;&#29702;&#30340;&#38169;&#35823;&#65292;&#20294;&#19982;&#20154;&#31867;&#25512;&#29702;&#26377;&#37325;&#35201;&#24046;&#24322;&#65292;&#26368;&#26032;&#30340;LLM&#29256;&#26412;&#20960;&#20046;&#28040;&#38500;&#20102;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#23545;&#30456;&#21516;&#30340;&#25552;&#31034;&#26041;&#26696;&#30340;&#21453;&#24212;&#19981;&#21516;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#25105;&#20204;&#30340;&#35748;&#35782;&#35770;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20256;&#32479;&#29992;&#20110;&#30740;&#31350;&#65288;&#26377;&#38480;&#65289;&#29702;&#24615;&#30340;&#35748;&#30693;&#24515;&#29702;&#23398;&#24037;&#20855;&#65292;&#30740;&#31350;&#21644;&#27604;&#36739;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21521;&#20154;&#31867;&#21442;&#19982;&#32773;&#21644;&#19968;&#31995;&#21015;&#39044;&#35757;&#32451;&#30340;LLM&#21576;&#29616;&#20102;&#26032;&#30340;&#32463;&#20856;&#35748;&#30693;&#23454;&#39564;&#30340;&#21464;&#20307;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#20132;&#21449;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#21576;&#29616;&#20986;&#31867;&#20284;&#20110;&#24120;&#35265;&#30340;&#38169;&#35823;&#20542;&#21521;&#20110;&#21551;&#21457;&#24335;&#20154;&#31867;&#25512;&#29702;&#30340;&#25512;&#29702;&#38169;&#35823;&#12290;&#23613;&#31649;&#26377;&#36825;&#31181;&#34920;&#38754;&#19978;&#30340;&#30456;&#20284;&#24615;&#65292;&#20154;&#31867;&#21644;LLM&#20043;&#38388;&#30340;&#28145;&#20837;&#27604;&#36739;&#34920;&#26126;&#20102;&#20154;&#31867;&#26679;&#24335;&#25512;&#29702;&#30340;&#37325;&#35201;&#24046;&#24322;&#65292;&#38543;&#30528;&#26368;&#36817;LLM&#29256;&#26412;&#30340;&#25512;&#20986;&#65292;&#27169;&#22411;&#30340;&#38480;&#21046;&#20960;&#20046;&#23436;&#20840;&#28040;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20986;&#65292;&#34429;&#28982;&#21487;&#33021;&#21046;&#23450;&#31574;&#30053;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#20294;&#20154;&#31867;&#21644;&#26426;&#22120;&#23545;&#30456;&#21516;&#30340;&#25552;&#31034;&#26041;&#26696;&#30340;&#21453;&#24212;&#24182;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#36890;&#36807;&#35752;&#35770;&#36825;&#19968;&#35748;&#35782;&#35770;&#30340;&#24433;&#21709;&#26469;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the present study, we investigate and compare reasoning in large language models (LLM) and humans using a selection of cognitive psychology tools traditionally dedicated to the study of (bounded) rationality. To do so, we presented to human participants and an array of pretrained LLMs new variants of classical cognitive experiments, and cross-compared their performances. Our results showed that most of the included models presented reasoning errors akin to those frequently ascribed to error-prone, heuristic-based human reasoning. Notwithstanding this superficial similarity, an in-depth comparison between humans and LLMs indicated important differences with human-like reasoning, with models limitations disappearing almost entirely in more recent LLMs releases. Moreover, we show that while it is possible to devise strategies to induce better performance, humans and machines are not equally-responsive to the same prompting schemes. We conclude by discussing the epistemological implicat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#38750;AI&#19987;&#23478;&#23545;AI&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#23450;&#20041;&#39034;&#24207;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#8220;&#27010;&#24565;&#8221;&#20197;&#21450;&#25506;&#32034;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#23545;RL agent&#23398;&#20064;&#25928;&#26524;&#21644;&#26368;&#32456;&#29992;&#25143;&#23545;agent&#20915;&#31574;&#29702;&#35299;&#30340;&#21452;&#37325;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.12482</link><description>&lt;p&gt;
State2Explanation:&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#65306;&#26377;&#21033;&#20110;Agent&#23398;&#20064;&#21644;&#29992;&#25143;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding. (arXiv:2309.12482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#38750;AI&#19987;&#23478;&#23545;AI&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#23450;&#20041;&#39034;&#24207;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#8220;&#27010;&#24565;&#8221;&#20197;&#21450;&#25506;&#32034;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#23545;RL agent&#23398;&#20064;&#25928;&#26524;&#21644;&#26368;&#32456;&#29992;&#25143;&#23545;agent&#20915;&#31574;&#29702;&#35299;&#30340;&#21452;&#37325;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38750;AI&#19987;&#23478;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;AI&#31995;&#32479;&#26469;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#21162;&#21147;&#24320;&#21457;&#33021;&#22815;&#20026;&#38750;AI&#19987;&#23478;&#29702;&#35299;&#30340;AI&#20915;&#31574;&#25552;&#20379;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#21033;&#29992;&#39640;&#32423;&#27010;&#24565;&#24182;&#29983;&#25104;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#37117;&#26159;&#20026;&#20998;&#31867;&#25216;&#26415;&#32780;&#24320;&#21457;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#20851;&#20110;&#39034;&#24207;&#20915;&#31574;&#30340;&#26041;&#27861;&#36824;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22312;&#39034;&#24207;&#20915;&#31574;&#35774;&#32622;&#20013;&#23450;&#20041;&#8220;&#27010;&#24565;&#8221;&#30340;&#24895;&#26395;&#12290;&#21463;&#21040;&#8220;Protege&#25928;&#24212;&#8221;&#30340;&#21551;&#21457;&#65292;&#35813;&#25928;&#24212;&#35828;&#26126;&#35299;&#37322;&#30693;&#35782;&#36890;&#24120;&#20250;&#22686;&#24378;&#20010;&#20307;&#30340;&#33258;&#20027;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#23545;RL agent&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#26368;&#32456;&#29992;&#25143;&#23545;agent&#20915;&#31574;&#29702;&#35299;&#30340;&#21452;&#37325;&#22909;&#22788;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;St
&lt;/p&gt;
&lt;p&gt;
With more complex AI systems used by non-AI experts to complete daily tasks, there is an increasing effort to develop methods that produce explanations of AI decision making understandable by non-AI experts. Towards this effort, leveraging higher-level concepts and producing concept-based explanations have become a popular method. Most concept-based explanations have been developed for classification techniques, and we posit that the few existing methods for sequential decision making are limited in scope. In this work, we first contribute a desiderata for defining "concepts" in sequential decision making settings. Additionally, inspired by the Protege Effect which states explaining knowledge often reinforces one's self-learning, we explore the utility of concept-based explanations providing a dual benefit to the RL agent by improving agent learning rate, and to the end-user by improving end-user understanding of agent decision making. To this end, we contribute a unified framework, St
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#23433;&#20840;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#26694;&#26550;&#65292;&#21152;&#36895;&#39564;&#35777;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#22312;&#27979;&#35797;&#20013;&#23481;&#26131;&#24341;&#21457;&#25925;&#38556;&#30340;&#22330;&#26223;&#21442;&#25968;&#20998;&#24067;&#21644;&#33021;&#22815;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#27169;&#25311;&#30340;&#20445;&#30495;&#24230;&#35774;&#32622;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#20445;&#30495;&#24230;&#35774;&#32622;&#20998;&#24067;&#26159;&#21542;&#26377;&#21161;&#20110;&#23545;&#26032;&#22330;&#26223;&#30340;&#22330;&#26223;&#21442;&#25968;&#20998;&#24067;&#36827;&#34892;&#26356;&#24555;&#30340;&#23398;&#20064;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.12474</link><description>&lt;p&gt;
SAVME: &#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#33258;&#21160;&#31995;&#32479;&#30340;&#39640;&#25928;&#23433;&#20840;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
SAVME: Efficient Safety Validation for Autonomous Systems Using Meta-Learning. (arXiv:2309.12474v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#23433;&#20840;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#26694;&#26550;&#65292;&#21152;&#36895;&#39564;&#35777;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#22312;&#27979;&#35797;&#20013;&#23481;&#26131;&#24341;&#21457;&#25925;&#38556;&#30340;&#22330;&#26223;&#21442;&#25968;&#20998;&#24067;&#21644;&#33021;&#22815;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#27169;&#25311;&#30340;&#20445;&#30495;&#24230;&#35774;&#32622;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#20445;&#30495;&#24230;&#35774;&#32622;&#20998;&#24067;&#26159;&#21542;&#26377;&#21161;&#20110;&#23545;&#26032;&#22330;&#26223;&#30340;&#22330;&#26223;&#21442;&#25968;&#20998;&#24067;&#36827;&#34892;&#26356;&#24555;&#30340;&#23398;&#20064;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#21069;&#65292;&#21457;&#29616;&#33258;&#21160;&#31995;&#32479;&#30340;&#28508;&#22312;&#25925;&#38556;&#38750;&#24120;&#37325;&#35201;&#12290;&#34394;&#26500;&#27861;&#24120;&#34987;&#29992;&#26469;&#35780;&#20272;&#27492;&#31867;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#20294;&#36816;&#34892;&#20934;&#30830;&#27169;&#25311;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#23558;&#20803;&#23398;&#20064;&#31574;&#30053;&#19982;&#22810;&#33218;&#36172;&#21338;&#26426;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#21152;&#36895;&#39564;&#35777;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23398;&#20064;&#22312;&#27979;&#35797;&#20013;&#23481;&#26131;&#24341;&#21457;&#25925;&#38556;&#30340;&#22330;&#26223;&#21442;&#25968;&#20998;&#24067;&#65292;&#20197;&#21450;&#33021;&#22815;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#27169;&#25311;&#30340;&#20445;&#30495;&#24230;&#35774;&#32622;&#20998;&#24067;&#12290;&#22312;&#20803;&#23398;&#20064;&#30340;&#31934;&#31070;&#19979;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#20445;&#30495;&#24230;&#35774;&#32622;&#20998;&#24067;&#26159;&#21542;&#26377;&#21161;&#20110;&#23545;&#26032;&#22330;&#26223;&#30340;&#22330;&#26223;&#21442;&#25968;&#20998;&#24067;&#36827;&#34892;&#26356;&#24555;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20808;&#36827;&#30340;3D&#39550;&#39542;&#27169;&#25311;&#22120;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#25972;&#21512;&#20102;16&#31181;&#20445;&#30495;&#24230;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering potential failures of an autonomous system is important prior to deployment. Falsification-based methods are often used to assess the safety of such systems, but the cost of running many accurate simulation can be high. The validation can be accelerated by identifying critical failure scenarios for the system under test and by reducing the simulation runtime. We propose a Bayesian approach that integrates meta-learning strategies with a multi-armed bandit framework. Our method involves learning distributions over scenario parameters that are prone to triggering failures in the system under test, as well as a distribution over fidelity settings that enable fast and accurate simulations. In the spirit of meta-learning, we also assess whether the learned fidelity settings distribution facilitates faster learning of the scenario parameter distributions for new scenarios. We showcase our methodology using a cutting-edge 3D driving simulator, incorporating 16 fidelity settings fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#20132;&#20114;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#33021;&#22815;&#20934;&#30830;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.12460</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#31185;&#23398;&#25104;&#20687;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Multimodal Deep Learning for Scientific Imaging Interpretation. (arXiv:2309.12460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#20132;&#20114;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#33021;&#22815;&#20934;&#30830;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#25104;&#20687;&#39046;&#22495;&#65292;&#35299;&#37322;&#35270;&#35273;&#25968;&#25454;&#24120;&#24120;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;&#23545;&#20027;&#39064;&#26448;&#26009;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#27169;&#25311;&#24182;&#35780;&#20272;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;SEM&#65289;&#22270;&#20687;&#30340;&#20154;&#31867;&#20132;&#20114;&#65292;&#29305;&#21035;&#26159;&#29627;&#29827;&#26448;&#26009;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20174;&#21516;&#34892;&#35780;&#35758;&#30340;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#20511;&#21161; GPT-4 &#30340;&#33021;&#21147;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#23613;&#31649;&#23384;&#22312;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#32454;&#24494;&#30340;&#35299;&#37322;&#21644;&#19987;&#19994;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#22312;&#21046;&#23450;&#20934;&#30830;&#30340;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#65292;&#20351;&#24471;&#36827;&#34892;&#32508;&#21512;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of scientific imaging, interpreting visual data often demands an intricate combination of human expertise and deep comprehension of the subject materials. This study presents a novel methodology to linguistically emulate and subsequently evaluate human-like interactions with Scanning Electron Microscopy (SEM) images, specifically of glass materials. Leveraging a multimodal deep learning framework, our approach distills insights from both textual and visual data harvested from peer-reviewed articles, further augmented by the capabilities of GPT-4 for refined data synthesis and evaluation. Despite inherent challenges--such as nuanced interpretations and the limited availability of specialized datasets--our model (GlassLLaVA) excels in crafting accurate interpretations, identifying key features, and detecting defects in previously unseen SEM images. Moreover, we introduce versatile evaluation metrics, suitable for an array of scientific imaging applications, which allows for
&lt;/p&gt;</description></item><item><title>LongDocFACTScore&#26159;&#19968;&#31181;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#23454;&#35777;&#24615;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#35780;&#20272;&#38271;&#25991;&#26723;&#25688;&#35201;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12455</link><description>&lt;p&gt;
LongDocFACTScore: &#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#30340;&#23454;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation. (arXiv:2309.12455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12455
&lt;/p&gt;
&lt;p&gt;
LongDocFACTScore&#26159;&#19968;&#31181;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#23454;&#35777;&#24615;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#35780;&#20272;&#38271;&#25991;&#26723;&#25688;&#35201;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25345;&#20107;&#23454;&#19968;&#33268;&#24615;&#26159;&#29983;&#25104;&#24615;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#25688;&#35201;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;ROUGE&#24471;&#20998;&#65289;&#26080;&#27861;&#35780;&#20272;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#27979;&#37327;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#25913;&#36827;&#24230;&#37327;&#26631;&#20934;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#26377;&#38480;&#21046;&#24615;&#30340;&#20196;&#29260;&#38480;&#21046;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#26377;&#38480;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#29616;&#26377;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#22312;&#24212;&#29992;&#20110;&#38271;&#25991;&#26723;&#25968;&#25454;&#38598;&#26102;&#26159;&#21542;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#22312;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#21151;&#25928;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;LongDocFACTScore&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#24230;&#37327;&#26631;&#20934;&#25193;&#23637;&#21040;&#20219;&#24847;&#38271;&#24230;&#30340;&#25991;&#26723;&#12290;&#35813;&#26694;&#26550;&#22312;&#19982;&#20154;&#31867;&#20107;&#23454;&#19968;&#33268;&#24615;&#24230;&#37327;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maintaining factual consistency is a critical issue in abstractive text summarisation, however, it cannot be assessed by traditional automatic metrics used for evaluating text summarisation, such as ROUGE scoring. Recent efforts have been devoted to developing improved metrics for measuring factual consistency using pre-trained language models, but these metrics have restrictive token limits, and are therefore not suitable for evaluating long document text summarisation. Moreover, there is limited research evaluating whether existing automatic evaluation metrics are fit for purpose when applied to long document data sets. In this work, we evaluate the efficacy of automatic metrics at assessing factual consistency in long document text summarisation and propose a new evaluation framework LongDocFACTScore. This framework allows metrics to be extended to any length document. This framework outperforms existing state-of-the-art metrics in its ability to correlate with human measures of fac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#36827;&#34892;&#27010;&#29575;&#24615;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#35299;&#32806;&#20102;&#26469;&#33258;&#31995;&#32479;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#21644;&#35299;&#37322;&#39044;&#27979;&#30340;&#20449;&#24515;&#12290;</title><link>http://arxiv.org/abs/2309.12445</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ensemble Neural Networks for Remaining Useful Life (RUL) Prediction. (arXiv:2309.12445v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12445
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#36827;&#34892;&#27010;&#29575;&#24615;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#35299;&#32806;&#20102;&#26469;&#33258;&#31995;&#32479;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#21644;&#35299;&#37322;&#39044;&#27979;&#30340;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#25252;&#35745;&#21010;&#30340;&#26680;&#24515;&#37096;&#20998;&#26159;&#19968;&#20010;&#30417;&#27979;&#31995;&#32479;&#65292;&#23427;&#25552;&#20379;&#20581;&#24247;&#21644;&#36864;&#21270;&#30340;&#33391;&#22909;&#39044;&#27979;&#65292;&#36890;&#24120;&#34987;&#34920;&#31034;&#20026;&#21097;&#20313;&#23551;&#21629;(RUL)&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#30340;&#25968;&#25454;&#39537;&#21160;RUL&#39044;&#27979;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#21333;&#28857;&#39044;&#27979;&#19978;&#12290;&#36825;&#20123;&#28857;&#39044;&#27979;&#26041;&#27861;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#25925;&#38556;&#30340;&#27010;&#29575;&#24615;&#36136;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#23569;&#25968;&#30340;&#27010;&#29575;&#24615;&#26041;&#27861;&#35201;&#20040;&#21253;&#25324;&#26469;&#33258;&#31995;&#32479;&#30340;aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#35201;&#20040;&#21253;&#25324;&#26469;&#33258;&#27169;&#22411;&#21442;&#25968;&#30340;epistemic&#19981;&#30830;&#23450;&#24615;&#65292;&#35201;&#20040;&#21516;&#26102;&#21253;&#21547;&#20004;&#32773;&#20316;&#20026;&#24635;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27010;&#29575;&#24615;RUL&#39044;&#27979;&#65292;&#32771;&#34385;&#20102;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#24182;&#23558;&#20854;&#35299;&#32806;&#12290;&#36825;&#20123;&#35299;&#32806;&#30340;&#19981;&#30830;&#23450;&#24615;&#22312;&#20102;&#35299;&#21644;&#35299;&#37322;&#39044;&#27979;&#30340;&#20449;&#24515;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;NASA&#30340;&#28065;&#21943;&#24335;&#21457;&#21160;&#26426;CMAPSS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#22914;&#20309;&#24314;&#27169;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#22914;&#20309;&#35299;&#24320;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A core part of maintenance planning is a monitoring system that provides a good prognosis on health and degradation, often expressed as remaining useful life (RUL). Most of the current data-driven approaches for RUL prediction focus on single-point prediction. These point prediction approaches do not include the probabilistic nature of the failure. The few probabilistic approaches to date either include the aleatoric uncertainty (which originates from the system), or the epistemic uncertainty (which originates from the model parameters), or both simultaneously as a total uncertainty. Here, we propose ensemble neural networks for probabilistic RUL predictions which considers both uncertainties and decouples these two uncertainties. These decoupled uncertainties are vital in knowing and interpreting the confidence of the predictions. This method is tested on NASA's turbofan jet engine CMAPSS data-set. Our results show how these uncertainties can be modeled and how to disentangle the cont
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#20302;&#36164;&#28304;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#30340;&#21487;&#33021;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#21487;&#20197;&#29992;&#20316;&#20302;&#36164;&#28304;&#35835;&#35299;&#20219;&#21153;&#20013;&#20154;&#24037;&#27880;&#37322;&#32773;&#30340;&#26367;&#20195;&#21697;&#12290;&#36825;&#39033;&#24037;&#20316;&#31361;&#20986;&#20102;LLMs&#20316;&#20026;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#22120;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#21457;&#24067;&#20102;&#22686;&#24378;&#29256;&#26412;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.12426</link><description>&lt;p&gt;
LLMs&#33021;&#22686;&#24378;&#20302;&#36164;&#28304;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#21527;&#65311;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges. (arXiv:2309.12426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#20302;&#36164;&#28304;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#30340;&#21487;&#33021;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#21487;&#20197;&#29992;&#20316;&#20302;&#36164;&#28304;&#35835;&#35299;&#20219;&#21153;&#20013;&#20154;&#24037;&#27880;&#37322;&#32773;&#30340;&#26367;&#20195;&#21697;&#12290;&#36825;&#39033;&#24037;&#20316;&#31361;&#20986;&#20102;LLMs&#20316;&#20026;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#22120;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#21457;&#24067;&#20102;&#22686;&#24378;&#29256;&#26412;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#24191;&#27867;&#30340;NLP&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#33021;&#22815;&#36827;&#34892;&#25512;&#29702;&#21644;&#24212;&#29992;&#24120;&#35782;&#12290;&#19968;&#20010;&#30456;&#20851;&#30340;&#24212;&#29992;&#26159;&#23558;&#23427;&#20204;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#20379;&#21518;&#32493;&#20219;&#21153;&#20351;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;GPT-4&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#25277;&#21462;&#24335;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;&#33258;&#21160;&#21270;&#30340;&#25968;&#25454;&#27880;&#37322;&#36807;&#31243;&#26377;&#28508;&#21147;&#33410;&#30465;&#22823;&#37327;&#26102;&#38388;&#12289;&#37329;&#38065;&#21644;&#31934;&#21147;&#65292;&#36825;&#20123;&#37117;&#26159;&#29992;&#20110;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#24494;&#35843;&#21518;&#30340;&#24615;&#33021;&#20197;&#21450;&#27880;&#37322;&#30340;&#25104;&#26412;&#65292;&#35780;&#20272;&#20102;GPT-4&#20316;&#20026;&#20302;&#36164;&#28304;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#20154;&#24037;&#27880;&#37322;&#26367;&#20195;&#32773;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#23545;LLMs&#20316;&#20026;QA&#31995;&#32479;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#22120;&#30340;&#39318;&#27425;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#29420;&#29305;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#30340;&#22686;&#24378;&#29256;&#26412;&#65292;&#36825;&#23558;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#37325;&#26032;&#35780;&#20272;LLMs&#22312;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive zero shot performance on a wide range of NLP tasks, demonstrating the ability to reason and apply commonsense. A relevant application is to use them for creating high quality synthetic datasets for downstream tasks. In this work, we probe whether GPT-4 can be used to augment existing extractive reading comprehension datasets. Automating data annotation processes has the potential to save large amounts of time, money and effort that goes into manually labelling datasets. In this paper, we evaluate the performance of GPT-4 as a replacement for human annotators for low resource reading comprehension tasks, by comparing performance after fine tuning, and the cost associated with annotation. This work serves to be the first analysis of LLMs as synthetic data augmenters for QA systems, highlighting the unique opportunities and challenges. Additionally, we release augmented versions of low resource datasets, that will allow the researc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#27169;&#22411;EvCBR&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#21033;&#29992;&#31867;&#20284;&#22240;&#26524;&#20107;&#20214;&#26469;&#39044;&#27979;&#26032;&#32467;&#26524;&#20107;&#20214;&#30340;&#23646;&#24615;&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#35757;&#32451;&#27493;&#39588;&#65292;&#36890;&#36807;&#32479;&#35745;&#24230;&#37327;&#21644;&#22522;&#20110;&#36335;&#24452;&#30340;&#39044;&#27979;&#26041;&#27861;&#23454;&#29616;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20107;&#20214;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.12423</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Event Prediction using Case-Based Reasoning over Knowledge Graphs. (arXiv:2309.12423v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#27169;&#22411;EvCBR&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#21033;&#29992;&#31867;&#20284;&#22240;&#26524;&#20107;&#20214;&#26469;&#39044;&#27979;&#26032;&#32467;&#26524;&#20107;&#20214;&#30340;&#23646;&#24615;&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#35757;&#32451;&#27493;&#39588;&#65292;&#36890;&#36807;&#32479;&#35745;&#24230;&#37327;&#21644;&#22522;&#20110;&#36335;&#24452;&#30340;&#39044;&#27979;&#26041;&#27861;&#23454;&#29616;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20107;&#20214;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#38142;&#25509;&#39044;&#27979;&#65288;LP&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#30340;&#20219;&#21153;&#65292;&#22914;&#22240;&#26524;&#20107;&#20214;&#39044;&#27979;&#65292;&#26159;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;LP&#27169;&#22411;&#23545;&#20110;&#27492;&#20219;&#21153;&#24182;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#23545;&#26032;&#30340;&#12289;&#26410;&#35265;&#30340;&#20107;&#20214;&#23454;&#20307;&#36827;&#34892;&#24402;&#32435;&#24335;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#19988;&#22312;&#24213;&#23618;KG&#20013;&#28155;&#21152;&#25110;&#26356;&#25913;&#30693;&#35782;&#26102;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#27169;&#22411;EvCBR&#65292;&#26681;&#25454;KG&#20013;&#23384;&#22312;&#30340;&#31867;&#20284;&#22240;&#26524;&#20107;&#20214;&#26469;&#39044;&#27979;&#20851;&#20110;&#26032;&#30340;&#32467;&#26524;&#20107;&#20214;&#30340;&#23646;&#24615;&#12290;EvCBR&#20351;&#29992;&#32479;&#35745;&#24230;&#37327;&#26469;&#35782;&#21035;&#30456;&#20284;&#20107;&#20214;&#24182;&#36827;&#34892;&#22522;&#20110;&#36335;&#24452;&#30340;&#39044;&#27979;&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#27493;&#39588;&#12290;&#20026;&#20102;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#24191;&#21040;&#20107;&#20214;&#39044;&#27979;&#22495;&#20043;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20219;&#21153;&#26694;&#26550;&#21270;&#20026;&#19968;&#20010;2-hop LP&#20219;&#21153;&#65292;&#20854;&#20013;&#31532;&#19968;&#36339;&#26159;&#23558;&#21407;&#22240;&#20107;&#20214;&#19982;&#26032;&#30340;&#25928;&#26524;&#20107;&#20214;&#36830;&#25509;&#36215;&#26469;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#31532;&#20108;&#36339;&#26159;&#25105;&#20204;&#24076;&#26395;&#39044;&#27979;&#30340;&#26032;&#20107;&#20214;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#26032;&#39062;&#30340;&#26032;&#38395;&#25968;&#25454;&#38598;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying link prediction (LP) methods over knowledge graphs (KG) for tasks such as causal event prediction presents an exciting opportunity. However, typical LP models are ill-suited for this task as they are incapable of performing inductive link prediction for new, unseen event entities and they require retraining as knowledge is added or changed in the underlying KG. We introduce a case-based reasoning model, EvCBR, to predict properties about new consequent events based on similar cause-effect events present in the KG. EvCBR uses statistical measures to identify similar events and performs path-based predictions, requiring no training step. To generalize our methods beyond the domain of event prediction, we frame our task as a 2-hop LP task, where the first hop is a causal relation connecting a cause event to a new effect event and the second hop is a property about the new event which we wish to predict. The effectiveness of our method is demonstrated using a novel dataset of news
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20540;&#20915;&#31574;&#22270;&#30340;&#26032;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#24378;&#32422;&#26463;&#25991;&#26412;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#21477;&#23376;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#31163;&#25955;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22810;&#20540;&#20915;&#31574;&#22270;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#21487;&#20197;&#24471;&#21040;&#35814;&#23613;&#35299;&#38598;&#12290;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#20445;&#30041;&#26368;&#20339;&#21477;&#23376;&#65292;&#24182;&#22312;&#33521;&#35821;&#21644;&#27861;&#35821;&#19978;&#36827;&#34892;&#20102;&#35814;&#32454;&#35752;&#35770;&#12290;&#35813;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;&#30340;&#35270;&#21147;&#31579;&#26597;&#27979;&#35797;&#24102;&#26469;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#24182;&#19988;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12415</link><description>&lt;p&gt;
&#32422;&#26463;&#20248;&#20808;&#65306;&#19968;&#31181;&#22522;&#20110;MDD&#30340;&#29983;&#25104;&#21463;&#32422;&#26463;&#21477;&#23376;&#30340;&#26032;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Constraints First: A New MDD-based Model to Generate Sentences Under Constraints. (arXiv:2309.12415v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20540;&#20915;&#31574;&#22270;&#30340;&#26032;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#24378;&#32422;&#26463;&#25991;&#26412;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#21477;&#23376;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#31163;&#25955;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22810;&#20540;&#20915;&#31574;&#22270;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#21487;&#20197;&#24471;&#21040;&#35814;&#23613;&#35299;&#38598;&#12290;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#20445;&#30041;&#26368;&#20339;&#21477;&#23376;&#65292;&#24182;&#22312;&#33521;&#35821;&#21644;&#27861;&#35821;&#19978;&#36827;&#34892;&#20102;&#35814;&#32454;&#35752;&#35770;&#12290;&#35813;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;&#30340;&#35270;&#21147;&#31579;&#26597;&#27979;&#35797;&#24102;&#26469;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#24182;&#19988;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24378;&#32422;&#26463;&#25991;&#26412;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#29992;&#20110;&#35270;&#21147;&#31579;&#26597;&#30340;&#26631;&#20934;&#21270;&#21477;&#23376;&#29983;&#25104;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#31163;&#25955;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22810;&#20540;&#20915;&#31574;&#22270;(MDD)&#65292;&#36825;&#26159;&#19968;&#31181;&#22788;&#29702;&#32422;&#26463;&#30340;&#33879;&#21517;&#25968;&#25454;&#32467;&#26500;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#20013;&#65292;MDD&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#21487;&#20197;&#35745;&#31639;&#20986;&#19981;&#38656;&#35201;&#36827;&#34892;&#20219;&#20309;&#25628;&#32034;&#30340;&#35814;&#23613;&#35299;&#38598;&#12290;&#19968;&#26086;&#33719;&#24471;&#20102;&#21477;&#23376;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;(GPT-2)&#26469;&#20445;&#30041;&#26368;&#20339;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#33521;&#35821;&#21644;&#27861;&#35821;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#19968;&#20123;&#19968;&#33268;&#21644;&#21464;&#20301;&#35268;&#21017;&#34987;&#35748;&#20026;&#26356;&#22797;&#26434;&#12290;&#26368;&#21518;&#65292;&#20511;&#21161;&#20110;GPT-2&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25968;&#30334;&#20010;&#30495;&#27491;&#30340;&#20505;&#36873;&#21477;&#23376;&#12290;&#19982;&#22312;&#33879;&#21517;&#30340;&#35270;&#21147;&#31579;&#26597;&#27979;&#35797;(MNREAD)&#20013;&#36890;&#24120;&#21487;&#29992;&#30340;&#20960;&#21313;&#20010;&#21477;&#23376;&#30456;&#27604;&#65292;&#36825;&#22312;&#26631;&#20934;&#21270;&#21477;&#23376;&#29983;&#25104;&#39046;&#22495;&#24102;&#26469;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;&#35813;&#26041;&#27861;&#36866;&#24212;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#32422;&#26463;&#65292;&#22240;&#27492;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new approach to generating strongly constrained texts. We consider standardized sentence generation for the typical application of vision screening. To solve this problem, we formalize it as a discrete combinatorial optimization problem and utilize multivalued decision diagrams (MDD), a well-known data structure to deal with constraints. In our context, one key strength of MDD is to compute an exhaustive set of solutions without performing any search. Once the sentences are obtained, we apply a language model (GPT-2) to keep the best ones. We detail this for English and also for French where the agreement and conjugation rules are known to be more complex. Finally, with the help of GPT-2, we get hundreds of bona-fide candidate sentences. When compared with the few dozen sentences usually available in the well-known vision screening test (MNREAD), this brings a major breakthrough in the field of standardized sentence generation. Also, as it can be easily adapted 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCOB&#30340;&#26032;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23383;&#31526;&#32423;&#21035;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#22312;&#32447;&#25991;&#26412;&#28210;&#26579;&#26469;&#26377;&#25928;&#22320;&#39044;&#35757;&#32451;&#25991;&#26723;&#21644;&#22330;&#26223;&#25991;&#26412;&#39046;&#22495;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;SCOB&#36824;&#33021;&#23454;&#29616;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.12382</link><description>&lt;p&gt;
SCOB: &#36890;&#36807;&#23383;&#31526;&#32423;&#21035;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#22312;&#32447;&#25991;&#26412;&#28210;&#26579;&#36827;&#34892;&#39046;&#22495;&#38388;&#24046;&#24322;&#30340;&#36890;&#29992;&#25991;&#26412;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
SCOB: Universal Text Understanding via Character-wise Supervised Contrastive Learning with Online Text Rendering for Bridging Domain Gap. (arXiv:2309.12382v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCOB&#30340;&#26032;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23383;&#31526;&#32423;&#21035;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#22312;&#32447;&#25991;&#26412;&#28210;&#26579;&#26469;&#26377;&#25928;&#22320;&#39044;&#35757;&#32451;&#25991;&#26723;&#21644;&#22330;&#26223;&#25991;&#26412;&#39046;&#22495;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;SCOB&#36824;&#33021;&#23454;&#29616;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#39044;&#35757;&#32451;&#21462;&#24471;&#30340;&#24040;&#22823;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#20013;&#25506;&#32034;&#20102;&#22522;&#20110;LM&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#24314;&#27169;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#12290;&#20854;&#20013;&#65292;&#20174;&#22270;&#20687;&#20013;&#35835;&#21462;&#25152;&#26377;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#39046;&#22495;&#65288;&#22914;&#21253;&#25324;&#35270;&#35273;&#25991;&#26723;&#21644;&#22330;&#26223;&#25991;&#26412;&#22270;&#20687;&#30340;&#39046;&#22495;&#65289;&#26102;&#24448;&#24448;&#20855;&#26377;&#19981;&#31283;&#23450;&#24615;&#29978;&#33267;&#22833;&#36133;&#12290;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#22823;&#38480;&#21046;&#65292;&#22240;&#20026;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#22788;&#29702;&#25991;&#26412;&#22270;&#20687;&#36755;&#20837;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26356;&#24191;&#27867;&#39046;&#22495;&#20013;&#30340;&#26377;&#25928;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;SCOB&#65292;&#23427;&#21033;&#29992;&#23383;&#31526;&#32423;&#21035;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#22312;&#32447;&#25991;&#26412;&#28210;&#26579;&#26469;&#36890;&#36807;&#24357;&#21512;&#39046;&#22495;&#24046;&#24322;&#26377;&#25928;&#22320;&#39044;&#35757;&#32451;&#25991;&#26723;&#21644;&#22330;&#26223;&#25991;&#26412;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;SCOB&#23454;&#29616;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#12290;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#35777;&#26126;&#20102;SCOB&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the great success of language model (LM)-based pre-training, recent studies in visual document understanding have explored LM-based pre-training methods for modeling text within document images. Among them, pre-training that reads all text from an image has shown promise, but often exhibits instability and even fails when applied to broader domains, such as those involving both visual documents and scene text images. This is a substantial limitation for real-world scenarios, where the processing of text image inputs in diverse domains is essential. In this paper, we investigate effective pre-training tasks in the broader domains and also propose a novel pre-training method called SCOB that leverages character-wise supervised contrastive learning with online text rendering to effectively pre-train document and scene text domains by bridging the domain gap. Moreover, SCOB enables weakly supervised learning, significantly reducing annotation costs. Extensive benchmarks demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22797;&#26434;&#21307;&#30103;&#20915;&#31574;&#20013;&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#35774;&#35745;&#35201;&#27714;&#65292;&#20197;&#33043;&#27602;&#30151;&#35786;&#26029;&#20026;&#20363;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#25903;&#25345;&#20020;&#24202;&#19987;&#23478;&#22312;&#20915;&#31574;&#36807;&#31243;&#30340;&#20013;&#38388;&#38454;&#27573;&#21457;&#25381;&#20316;&#29992;&#65288;&#22914;&#29983;&#25104;&#20551;&#35774;&#25110;&#25910;&#38598;&#25968;&#25454;&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#26368;&#32456;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2309.12368</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#21307;&#30103;&#20915;&#31574;&#20013;&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#65306;&#20197;&#33043;&#27602;&#30151;&#35786;&#26029;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis. (arXiv:2309.12368v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22797;&#26434;&#21307;&#30103;&#20915;&#31574;&#20013;&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#35774;&#35745;&#35201;&#27714;&#65292;&#20197;&#33043;&#27602;&#30151;&#35786;&#26029;&#20026;&#20363;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#25903;&#25345;&#20020;&#24202;&#19987;&#23478;&#22312;&#20915;&#31574;&#36807;&#31243;&#30340;&#20013;&#38388;&#38454;&#27573;&#21457;&#25381;&#20316;&#29992;&#65288;&#22914;&#29983;&#25104;&#20551;&#35774;&#25110;&#25910;&#38598;&#25968;&#25454;&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#26368;&#32456;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#30340;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#30740;&#31350;&#35770;&#25991;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#21364;&#38754;&#20020;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#33043;&#27602;&#30151;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#38656;&#35201;&#20020;&#24202;&#21307;&#29983;&#26089;&#26399;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#35786;&#26029;&#30340;&#24613;&#24615;&#33268;&#21629;&#20840;&#36523;&#24615;&#24863;&#26579;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#33021;&#22815;&#25903;&#25345;&#20020;&#24202;&#19987;&#23478;&#20570;&#20986;&#26356;&#22909;&#33043;&#27602;&#30151;&#26089;&#26399;&#35786;&#26029;&#20915;&#31574;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;&#30740;&#31350;&#20174;&#19968;&#20010;&#24418;&#25104;&#24615;&#30740;&#31350;&#24320;&#22987;&#65292;&#35843;&#26597;&#20026;&#20160;&#20040;&#20020;&#24202;&#19987;&#23478;&#22312;&#30005;&#23376;&#30149;&#21382;&#31995;&#32479;&#20013;&#25918;&#24323;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#33043;&#27602;&#30151;&#39044;&#27979;&#27169;&#22359;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38656;&#35201;&#22312;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#30340;&#20013;&#38388;&#38454;&#27573;&#65288;&#22914;&#29983;&#25104;&#20551;&#35774;&#25110;&#25910;&#38598;&#25968;&#25454;&#65289;&#25903;&#25345;&#20154;&#31867;&#19987;&#23478;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#26368;&#32456;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26500;&#24314;&#20102;SepsisLab&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;&#39044;&#27979;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today's AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module in their electrical health record (EHR) system. We argue that a human-centered AI system needs to support human experts in the intermediate stages of a medical decision-making process (e.g., generating hypotheses or gathering data), instead of focusing only on the final decision. Therefore, we build SepsisLab based on a state-of-the-art AI algorithm and extend it to predict the future projection o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;GPT&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23558;&#39046;&#22495;&#30693;&#35782;&#24211;&#19982;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#24211;&#21644;&#35780;&#20272;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#29983;&#21644;&#39046;&#22495;&#19987;&#23478;&#23545;&#20110;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2309.12367</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;GPT&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#30740;&#31350;&#39046;&#22495;&#30693;&#35782;&#24211;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Examining the Influence of Varied Levels of Domain Knowledge Base Inclusion in GPT-based Intelligent Tutors. (arXiv:2309.12367v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;GPT&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23558;&#39046;&#22495;&#30693;&#35782;&#24211;&#19982;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#24211;&#21644;&#35780;&#20272;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#29983;&#21644;&#39046;&#22495;&#19987;&#23478;&#23545;&#20110;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20419;&#36827;&#20102;&#20855;&#26377;&#22797;&#26434;&#23545;&#35805;&#33021;&#21147;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;LLM&#23545;&#26597;&#35810;&#30340;&#22238;&#31572;&#32463;&#24120;&#19981;&#20934;&#30830;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#19982;LLM&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#38598;&#25104;&#20197;&#22686;&#21152;&#22238;&#31572;&#21487;&#38752;&#24615;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#24211;&#65292;&#25945;&#32946;&#30417;&#30563;&#21592;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#35838;&#31243;&#65292;&#35813;&#35838;&#31243;&#20250;&#34987;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#33258;&#21160;&#22788;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#35780;&#20272;&#23454;&#39564;&#65292;&#23398;&#29983;&#21442;&#19982;&#32773;&#38656;&#35201;&#22238;&#31572;&#26377;&#20851;&#20154;&#24037;&#26234;&#33021;&#35838;&#31243;&#30340;&#38382;&#39064;&#12290; GPT-4&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20855;&#26377;&#19981;&#21516;&#23618;&#27425;&#30340;KB&#35775;&#38382;&#26435;&#38480;&#65292;&#24182;&#30001;&#20154;&#31867;&#39046;&#22495;&#19987;&#23478;&#35780;&#20272;&#36825;&#20123;&#22238;&#31572;&#12290;&#26368;&#21518;&#65292;&#23398;&#29983;&#23545;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#19982;&#39046;&#22495;&#19987;&#23478;&#30340;&#20132;&#21449;&#39564;&#35777;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#21508;&#31181;&#25945;&#23398;&#33021;&#21147;&#36827;&#34892;&#20102;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have facilitated the development of chatbots with sophisticated conversational capabilities. However, LLMs exhibit frequent inaccurate responses to queries, hindering applications in educational settings. In this paper, we investigate the effectiveness of integrating a knowledge base (KB) with LLM intelligent tutors to increase response reliability. To achieve this, we design a scaleable KB that affords educational supervisors seamless integration of lesson curricula, which is automatically processed by the intelligent tutoring system. We then detail an evaluation, where student participants were presented with questions about the artificial intelligence curriculum to respond to. GPT-4 intelligent tutors with varying hierarchies of KB access and human domain experts then assessed these responses. Lastly, students cross-examined the intelligent tutors' responses to the domain experts' and ranked their various pedagogical abilities. Res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26234;&#33021;&#24211;&#23384;&#31649;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#26465;&#30721;&#21644;&#20998;&#24067;&#24335;flutter&#24212;&#29992;&#25216;&#26415;&#65292;&#20197;&#21450;&#22823;&#25968;&#25454;&#20998;&#26512;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#65292;&#35299;&#20915;&#20102;&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#20934;&#30830;&#24615;&#12289;&#30417;&#27979;&#24310;&#36831;&#21644;&#36807;&#24230;&#20381;&#36182;&#20027;&#35266;&#32463;&#39564;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.12365</link><description>&lt;p&gt;
&#19968;&#20010;&#39640;&#25928;&#30340;&#26234;&#33021;&#21322;&#33258;&#21160;&#20179;&#24211;&#24211;&#23384;&#30424;&#28857;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Efficient Intelligent Semi-Automated Warehouse Inventory Stocktaking System. (arXiv:2309.12365v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26234;&#33021;&#24211;&#23384;&#31649;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#26465;&#30721;&#21644;&#20998;&#24067;&#24335;flutter&#24212;&#29992;&#25216;&#26415;&#65292;&#20197;&#21450;&#22823;&#25968;&#25454;&#20998;&#26512;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#65292;&#35299;&#20915;&#20102;&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#20934;&#30830;&#24615;&#12289;&#30417;&#27979;&#24310;&#36831;&#21644;&#36807;&#24230;&#20381;&#36182;&#20027;&#35266;&#32463;&#39564;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#20379;&#24212;&#38142;&#31649;&#29702;&#32972;&#26223;&#19979;&#65292;&#39640;&#25928;&#30340;&#24211;&#23384;&#31649;&#29702;&#23545;&#20110;&#20225;&#19994;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25163;&#24037;&#21644;&#32463;&#39564;&#39537;&#21160;&#30340;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#28385;&#36275;&#29616;&#20195;&#24066;&#22330;&#38656;&#27714;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26234;&#33021;&#24211;&#23384;&#31649;&#29702;&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#19982;&#25968;&#25454;&#19981;&#20934;&#30830;&#12289;&#30417;&#27979;&#24310;&#36831;&#21644;&#36807;&#24230;&#20381;&#36182;&#20027;&#35266;&#32463;&#39564;&#30340;&#39044;&#27979;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#26465;&#30721;&#21644;&#20998;&#24067;&#24335; flutter &#24212;&#29992;&#25216;&#26415;&#65292;&#29992;&#20110;&#26234;&#33021;&#24863;&#30693;&#65292;&#24182;&#36890;&#36807;&#20840;&#38754;&#30340;&#22823;&#25968;&#25454;&#20998;&#26512;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#20180;&#32454;&#30340;&#20998;&#26512;&#12289;&#31995;&#32479;&#35774;&#35745;&#12289;&#20851;&#38190;&#25216;&#26415;&#25506;&#32034;&#21644;&#27169;&#25311;&#39564;&#35777;&#65292;&#25104;&#21151;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26234;&#33021;&#31995;&#32479;&#23454;&#29616;&#20102;&#20108;&#32423;&#30417;&#27979;&#12289;&#39640;&#39057;&#26816;&#26597;&#21644;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#33258;&#21160;&#21270;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of evolving supply chain management, the significance of efficient inventory management has grown substantially for businesses. However, conventional manual and experience-based approaches often struggle to meet the complexities of modern market demands. This research introduces an intelligent inventory management system to address challenges related to inaccurate data, delayed monitoring, and overreliance on subjective experience in forecasting. The proposed system integrates bar code and distributed flutter application technologies for intelligent perception, alongside comprehensive big data analytics to enable data-driven decision-making. Through meticulous analysis, system design, critical technology exploration, and simulation validation, the effectiveness of the proposed system is successfully demonstrated. The intelligent system facilitates second-level monitoring, high-frequency checks, and artificial intelligence-driven forecasting, consequently enhancing the au
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#32447;&#36130;&#32463;&#38169;&#35823;&#20449;&#24687;&#21450;&#20854;&#21518;&#26524;&#65292;&#21253;&#25324;&#38169;&#35823;&#20449;&#24687;&#30340;&#31867;&#22411;&#12289;&#26469;&#28304;&#21644;&#24433;&#21709;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#26089;&#26399;&#21457;&#29616;&#21644;&#32531;&#35299;&#31574;&#30053;&#21487;&#33021;&#23545;&#20445;&#25252;&#25237;&#36164;&#32773;&#12289;&#22686;&#24378;&#24066;&#22330;&#36879;&#26126;&#24230;&#21644;&#32500;&#25252;&#37329;&#34701;&#31283;&#23450;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.12363</link><description>&lt;p&gt;
&#35843;&#26597;&#22312;&#32447;&#36130;&#32463;&#38169;&#35823;&#20449;&#24687;&#21450;&#20854;&#21518;&#26524;&#65306;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Investigating Online Financial Misinformation and Its Consequences: A Computational Perspective. (arXiv:2309.12363v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12363
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#32447;&#36130;&#32463;&#38169;&#35823;&#20449;&#24687;&#21450;&#20854;&#21518;&#26524;&#65292;&#21253;&#25324;&#38169;&#35823;&#20449;&#24687;&#30340;&#31867;&#22411;&#12289;&#26469;&#28304;&#21644;&#24433;&#21709;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#26089;&#26399;&#21457;&#29616;&#21644;&#32531;&#35299;&#31574;&#30053;&#21487;&#33021;&#23545;&#20445;&#25252;&#25237;&#36164;&#32773;&#12289;&#22686;&#24378;&#24066;&#22330;&#36879;&#26126;&#24230;&#21644;&#32500;&#25252;&#37329;&#34701;&#31283;&#23450;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#24179;&#21488;&#24555;&#36895;&#20256;&#25773;&#20449;&#24687;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#25105;&#20204;&#33719;&#21462;&#21644;&#28040;&#36153;&#26032;&#38395;&#21644;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#23588;&#20854;&#26159;&#22312;&#37329;&#34701;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#25968;&#23383;&#26102;&#20195;&#20063;&#20652;&#29983;&#20102;&#22823;&#37327;&#21487;&#24597;&#30340;&#36130;&#32463;&#38169;&#35823;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#23545;&#20010;&#20154;&#12289;&#24066;&#22330;&#21644;&#25972;&#20307;&#32463;&#27982;&#37117;&#20250;&#36896;&#25104;&#19981;&#21033;&#24433;&#21709;&#12290;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#26088;&#22312;&#20840;&#38754;&#35843;&#26597;&#22312;&#32447;&#36130;&#32463;&#38169;&#35823;&#20449;&#24687;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#31867;&#22411;&#12289;&#26469;&#28304;&#21644;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36130;&#32463;&#38169;&#35823;&#20449;&#24687;&#30340;&#29305;&#24449;&#21644;&#34920;&#29616;&#65292;&#21253;&#25324;&#34394;&#20551;&#22768;&#26126;&#21644;&#35823;&#23548;&#24615;&#20869;&#23481;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20123;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#35828;&#26126;&#36130;&#32463;&#38169;&#35823;&#20449;&#24687;&#23545;&#32463;&#27982;&#30340;&#26377;&#23475;&#21518;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#26816;&#27979;&#36130;&#32463;&#38169;&#35823;&#20449;&#24687;&#30340;&#28508;&#22312;&#24433;&#21709;&#21644;&#24847;&#20041;&#12290;&#26089;&#26399;&#21457;&#29616;&#21644;&#32531;&#35299;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#20445;&#25252;&#25237;&#36164;&#32773;&#65292;&#22686;&#24378;&#24066;&#22330;&#36879;&#26126;&#24230;&#65292;&#32500;&#25252;&#37329;&#34701;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid dissemination of information through digital platforms has revolutionized the way we access and consume news and information, particularly in the realm of finance. However, this digital age has also given rise to an alarming proliferation of financial misinformation, which can have detrimental effects on individuals, markets, and the overall economy. This research paper aims to provide a comprehensive survey of online financial misinformation, including its types, sources, and impacts. We first discuss the characteristics and manifestations of financial misinformation, encompassing false claims and misleading content. We explore various case studies that illustrate the detrimental consequences of financial misinformation on the economy. Finally, we highlight the potential impact and implications of detecting financial misinformation. Early detection and mitigation strategies can help protect investors, enhance market transparency, and preserve financial stability. We emphasiz
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#22312;&#31070;&#32463;&#30524;&#31185;&#30142;&#30149;&#35786;&#26029;&#20013;&#30340;&#36741;&#21161;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;13&#20010;&#30149;&#20363;&#20013;&#27491;&#30830;&#35786;&#26029;&#20102;59%&#65292;&#32780;ChatGPT Plus&#21644;&#20004;&#21517;&#31070;&#32463;&#30524;&#31185;&#21307;&#29983;&#30340;&#27491;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;&#20102;82%&#21644;86%&#12290;&#36825;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#31070;&#32463;&#30524;&#31185;&#30142;&#30149;&#35786;&#26029;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.12361</link><description>&lt;p&gt;
ChatGPT&#22522;&#20110;&#30149;&#20363;&#25253;&#21578;&#36741;&#21161;&#31070;&#32463;&#30524;&#31185;&#30142;&#30149;&#30340;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Assisting Diagnosis of Neuro-ophthalmology Diseases Based on Case Reports. (arXiv:2309.12361v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#22312;&#31070;&#32463;&#30524;&#31185;&#30142;&#30149;&#35786;&#26029;&#20013;&#30340;&#36741;&#21161;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;13&#20010;&#30149;&#20363;&#20013;&#27491;&#30830;&#35786;&#26029;&#20102;59%&#65292;&#32780;ChatGPT Plus&#21644;&#20004;&#21517;&#31070;&#32463;&#30524;&#31185;&#21307;&#29983;&#30340;&#27491;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;&#20102;82%&#21644;86%&#12290;&#36825;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#31070;&#32463;&#30524;&#31185;&#30142;&#30149;&#35786;&#26029;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#22522;&#20110;&#35814;&#32454;&#30149;&#20363;&#25551;&#36848;&#36741;&#21161;&#31070;&#32463;&#30524;&#31185;&#30142;&#30149;&#35786;&#26029;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20174;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22312;&#32447;&#25968;&#25454;&#24211;&#20013;&#36873;&#25321;&#20102;22&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#30524;&#31185;&#30142;&#30149;&#30149;&#20363;&#25253;&#21578;&#12290;&#36825;&#20123;&#30149;&#20363;&#21253;&#25324;&#31070;&#32463;&#30524;&#31185;&#20122;&#19987;&#31185;&#24120;&#35265;&#30340;&#24930;&#24615;&#21644;&#24613;&#24615;&#30142;&#30149;&#12290;&#25105;&#20204;&#25226;&#27599;&#20010;&#30149;&#20363;&#30340;&#25991;&#26412;&#20316;&#20026;&#26032;&#30340;&#25552;&#31034;&#25554;&#20837;&#21040;ChatGPT v3.5&#21644;ChatGPT Plus v4.0&#20013;&#65292;&#24182;&#35810;&#38382;&#26368;&#26377;&#21487;&#33021;&#30340;&#35786;&#26029;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20934;&#30830;&#30340;&#20449;&#24687;&#25552;&#20379;&#32473;&#20004;&#21517;&#31070;&#32463;&#30524;&#31185;&#21307;&#29983;&#65292;&#24182;&#35760;&#24405;&#20182;&#20204;&#30340;&#35786;&#26029;&#32467;&#26524;&#65292;&#28982;&#21518;&#19982;ChatGPT&#30340;&#22238;&#31572;&#36827;&#34892;&#23545;&#27604;&#12290;&#32467;&#26524;&#65306;ChatGPT v3.5&#12289;ChatGPT Plus v4.0&#21644;&#20004;&#21517;&#31070;&#32463;&#30524;&#31185;&#21307;&#29983;&#22312;22&#20010;&#30149;&#20363;&#20013;&#20998;&#21035;&#36798;&#21040;13&#20010;&#65288;59%&#65289;&#12289;18&#20010;&#65288;82%&#65289;&#12289;19&#20010;&#65288;86%&#65289;&#21644;19&#20010;&#65288;86%&#65289;&#30340;&#27491;&#30830;&#35786;&#26029;&#12290;&#21508;&#31181;&#35786;&#26029;&#26469;&#28304;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#22914;&#19979;&#65306;ChatGPT v3.5&#21644;ChatGPT Plus v4.0&#30340;&#19968;&#33268;&#24615;&#20026;13&#20010;...
&lt;/p&gt;
&lt;p&gt;
Objective: To evaluate the efficiency of large language models (LLMs) such as ChatGPT to assist in diagnosing neuro-ophthalmic diseases based on detailed case descriptions. Methods: We selected 22 different case reports of neuro-ophthalmic diseases from a publicly available online database. These cases included a wide range of chronic and acute diseases that are commonly seen by neuro-ophthalmic sub-specialists. We inserted the text from each case as a new prompt into both ChatGPT v3.5 and ChatGPT Plus v4.0 and asked for the most probable diagnosis. We then presented the exact information to two neuro-ophthalmologists and recorded their diagnoses followed by comparison to responses from both versions of ChatGPT. Results: ChatGPT v3.5, ChatGPT Plus v4.0, and the two neuro-ophthalmologists were correct in 13 (59%), 18 (82%), 19 (86%), and 19 (86%) out of 22 cases, respectively. The agreement between the various diagnostic sources were as follows: ChatGPT v3.5 and ChatGPT Plus v4.0, 13 (5
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25277;&#26679;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#31038;&#20250;&#36873;&#25321;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;NLP&#27169;&#22411;&#20272;&#35745;&#20102;&#25552;&#26696;&#36890;&#36807;&#30340;&#27010;&#29575;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#22810;&#25968;&#26469;&#20915;&#23450;&#25552;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.12360</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25277;&#26679;&#23454;&#29616;&#39640;&#25928;&#30340;&#31038;&#20250;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Efficient Social Choice via NLP and Sampling. (arXiv:2309.12360v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25277;&#26679;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#31038;&#20250;&#36873;&#25321;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;NLP&#27169;&#22411;&#20272;&#35745;&#20102;&#25552;&#26696;&#36890;&#36807;&#30340;&#27010;&#29575;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#22810;&#25968;&#26469;&#20915;&#23450;&#25552;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#24863;&#30693;&#31038;&#20250;&#36873;&#25321;&#35299;&#20915;&#20102;&#19968;&#20123;&#20195;&#29702;&#31038;&#21306;&#38754;&#20020;&#30340;&#22522;&#26412;&#20914;&#31361;&#65292;&#21363;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#21253;&#25324;&#25152;&#26377;&#25104;&#21592;&#30340;&#28212;&#26395;&#19982;&#31038;&#21306;&#25104;&#21592;&#21487;&#25903;&#37197;&#30340;&#26377;&#38480;&#26102;&#38388;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#24863;&#30693;&#31038;&#20250;&#36873;&#25321;&#30340;&#20004;&#31181;&#25216;&#26415;&#32452;&#21512;&#65292;&#21363;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#25277;&#26679;&#12290;&#22522;&#26412;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#20854;&#20013;&#27599;&#20010;&#25913;&#21464;&#29616;&#29366;&#30340;&#27835;&#29702;&#25552;&#26696;&#39318;&#20808;&#21457;&#36865;&#21040;&#35757;&#32451;&#26377;&#32032;&#30340;NLP&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20272;&#35745;&#20102;&#22914;&#26524;&#25152;&#26377;&#31038;&#21306;&#25104;&#21592;&#30452;&#25509;&#23545;&#20854;&#36827;&#34892;&#25237;&#31080;&#65292;&#35813;&#25552;&#26696;&#36890;&#36807;&#30340;&#27010;&#29575;&#65307;&#28982;&#21518;&#65292;&#22522;&#20110;&#36825;&#31181;&#20272;&#35745;&#65292;&#36873;&#25321;&#19968;&#20010;&#30830;&#23450;&#22823;&#23567;&#30340;&#20154;&#32676;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#22810;&#25968;&#20915;&#23450;&#25552;&#26696;&#12290;&#25105;&#20204;&#26681;&#25454;&#19978;&#36848;&#26041;&#26696;&#24320;&#21457;&#20102;&#20960;&#31181;&#20855;&#20307;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#22810;&#20010;&#20998;&#25955;&#33258;&#27835;&#32452;&#32455;&#65288;DAO&#65289;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-Aware Social Choice tackles the fundamental conflict faced by some agent communities between their desire to include all members in the decision making processes and the limited time and attention that are at the disposal of the community members. Here, we investigate a combination of two techniques for attention-aware social choice, namely Natural Language Processing (NLP) and Sampling. Essentially, we propose a system in which each governance proposal to change the status quo is first sent to a trained NLP model that estimates the probability that the proposal would pass if all community members directly vote on it; then, based on such an estimation, a population sample of a certain size is being selected and the proposal is decided upon by taking the sample majority. We develop several concrete algorithms following the scheme described above and evaluate them using various data, including such from several Decentralized Autonomous Organizations (DAOs).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#21644;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#30740;&#31350;&#26032;&#38395;&#23398;&#21644;&#22823;&#20247;&#20256;&#25773;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12290;&#36890;&#36807;&#29702;&#35299;&#27599;&#20010;&#23376;&#39046;&#22495;&#30340;&#25805;&#20316;&#21407;&#29702;&#65292;&#23398;&#32773;&#20204;&#21487;&#20197;&#22312;&#20998;&#26512;&#29305;&#23450;&#30740;&#31350;&#35838;&#39064;&#26102;&#22686;&#24378;&#20182;&#20204;&#20851;&#27880;&#29305;&#23450;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12357</link><description>&lt;p&gt;
&#22312;&#26032;&#38395;&#23398;&#30740;&#31350;&#20013;&#32472;&#21046;&#20154;&#24037;&#26234;&#33021;&#35770;&#28857;
&lt;/p&gt;
&lt;p&gt;
Mapping AI Arguments in Journalism Studies. (arXiv:2309.12357v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#21644;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#30740;&#31350;&#26032;&#38395;&#23398;&#21644;&#22823;&#20247;&#20256;&#25773;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12290;&#36890;&#36807;&#29702;&#35299;&#27599;&#20010;&#23376;&#39046;&#22495;&#30340;&#25805;&#20316;&#21407;&#29702;&#65292;&#23398;&#32773;&#20204;&#21487;&#20197;&#22312;&#20998;&#26512;&#29305;&#23450;&#30740;&#31350;&#35838;&#39064;&#26102;&#22686;&#24378;&#20182;&#20204;&#20851;&#27880;&#29305;&#23450;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#21644;&#24314;&#35758;&#29992;&#20110;&#30740;&#31350;&#26032;&#38395;&#23398;&#21644;&#22823;&#20247;&#20256;&#25773;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20855;&#20307;&#31034;&#20363;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#38416;&#26126;AI&#30340;&#19971;&#20010;&#19981;&#21516;&#23376;&#39046;&#22495;&#65292;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35821;&#38899;&#35782;&#21035;&#12289;&#19987;&#23478;&#31995;&#32479;&#12289;&#35745;&#21010;&#12289;&#35843;&#24230;&#12289;&#20248;&#21270;&#12289;&#26426;&#22120;&#20154;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#32467;&#26500;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#26032;&#38395;&#39046;&#22495;&#30340;AI&#30740;&#31350;&#20154;&#21592;&#12290;&#36890;&#36807;&#29702;&#35299;&#27599;&#20010;&#23376;&#39046;&#22495;&#30340;&#25805;&#20316;&#21407;&#29702;&#65292;&#23398;&#32773;&#20204;&#21487;&#20197;&#22312;&#20998;&#26512;&#29305;&#23450;&#30740;&#31350;&#35838;&#39064;&#26102;&#22686;&#24378;&#20182;&#20204;&#20851;&#27880;&#29305;&#23450;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates and suggests typologies for examining Artificial Intelligence (AI) within the domains of journalism and mass communication research. We aim to elucidate the seven distinct subfields of AI, which encompass machine learning, natural language processing (NLP), speech recognition, expert systems, planning, scheduling, optimization, robotics, and computer vision, through the provision of concrete examples and practical applications. The primary objective is to devise a structured framework that can help AI researchers in the field of journalism. By comprehending the operational principles of each subfield, scholars can enhance their ability to focus on a specific facet when analyzing a particular research topic.
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#24102;&#26469;&#20102;&#24076;&#26395;&#21644;&#21361;&#38505;&#12290;&#28982;&#32780;&#65292;&#26080;&#35770;&#26159;AI&#36824;&#26159;&#20154;&#31867;&#21516;&#34892;&#35780;&#23457;&#31995;&#32479;&#37117;&#23384;&#22312;&#30456;&#20851;&#38382;&#39064;&#65292;&#20363;&#22914;&#20559;&#35265;&#12289;&#28389;&#29992;&#21644;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;&#22240;&#27492;&#65292;AI&#39537;&#21160;&#30340;&#21516;&#34892;&#35780;&#23457;&#30340;&#21512;&#27861;&#24615;&#21462;&#20915;&#20110;&#19982;&#31185;&#23398;&#20262;&#29702;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12356</link><description>&lt;p&gt;
AI&#20013;&#20171;&#21516;&#34892;&#35780;&#23457;&#36947;&#24503;&#30340;&#20851;&#38190;&#23457;&#35270;
&lt;/p&gt;
&lt;p&gt;
A Critical Examination of the Ethics of AI-Mediated Peer Review. (arXiv:2309.12356v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12356
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#24102;&#26469;&#20102;&#24076;&#26395;&#21644;&#21361;&#38505;&#12290;&#28982;&#32780;&#65292;&#26080;&#35770;&#26159;AI&#36824;&#26159;&#20154;&#31867;&#21516;&#34892;&#35780;&#23457;&#31995;&#32479;&#37117;&#23384;&#22312;&#30456;&#20851;&#38382;&#39064;&#65292;&#20363;&#22914;&#20559;&#35265;&#12289;&#28389;&#29992;&#21644;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;&#22240;&#27492;&#65292;AI&#39537;&#21160;&#30340;&#21516;&#34892;&#35780;&#23457;&#30340;&#21512;&#27861;&#24615;&#21462;&#20915;&#20110;&#19982;&#31185;&#23398;&#20262;&#29702;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#24102;&#26469;&#20102;&#24076;&#26395;&#21644;&#21361;&#38505;&#12290;&#19968;&#26041;&#38754;&#65292;AI&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#38271;&#26102;&#38388;&#21457;&#34920;&#24310;&#36831;&#31561;&#38382;&#39064;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#20063;&#24102;&#26469;&#20102;&#20262;&#29702;&#21644;&#31038;&#20250;&#20851;&#27880;&#65292;&#21487;&#33021;&#25439;&#23475;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#21644;&#32467;&#26524;&#30340;&#23436;&#25972;&#24615;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#21516;&#34892;&#35780;&#23457;&#31995;&#32479;&#20063;&#23384;&#22312;&#30456;&#20851;&#38382;&#39064;&#65292;&#22914;&#20559;&#35265;&#12289;&#28389;&#29992;&#21644;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#36825;&#24050;&#32463;&#25439;&#23475;&#20102;&#21487;&#20449;&#24230;&#12290;&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#26159;&#20851;&#20110;AI&#22312;&#21516;&#34892;&#35780;&#23457;&#20013;&#30340;&#20351;&#29992;&#65292;&#20294;&#35752;&#35770;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#26415;&#26399;&#21002;&#20986;&#29256;&#20013;&#30340;&#25220;&#34989;&#21644;&#32626;&#21517;&#38382;&#39064;&#19978;&#65292;&#24573;&#35270;&#20102;&#21516;&#34892;&#35780;&#23457;&#25152;&#22788;&#30340;&#26356;&#24191;&#27867;&#30340;&#35748;&#30693;&#12289;&#31038;&#20250;&#12289;&#25991;&#21270;&#21644;&#31038;&#20250;&#35748;&#30693;&#29615;&#22659;&#12290;AI&#39537;&#21160;&#30340;&#21516;&#34892;&#35780;&#23457;&#30340;&#21512;&#27861;&#24615;&#21462;&#20915;&#20110;&#19982;&#31185;&#23398;&#20262;&#29702;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#23450;&#20041;&#23398;&#26415;&#21327;&#20316;&#20013;&#36866;&#24403;&#34892;&#20026;&#30340;&#36947;&#24503;&#21644;&#35748;&#35782;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in artificial intelligence (AI) systems, including large language models like ChatGPT, offer promise and peril for scholarly peer review. On the one hand, AI can enhance efficiency by addressing issues like long publication delays. On the other hand, it brings ethical and social concerns that could compromise the integrity of the peer review process and outcomes. However, human peer review systems are also fraught with related problems, such as biases, abuses, and a lack of transparency, which already diminish credibility. While there is increasing attention to the use of AI in peer review, discussions revolve mainly around plagiarism and authorship in academic journal publishing, ignoring the broader epistemic, social, cultural, and societal epistemic in which peer review is positioned. The legitimacy of AI-driven peer review hinges on the alignment with the scientific ethos, encompassing moral and epistemic norms that define appropriate conduct in the scholarly co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#21160;&#25512;&#29702;&#31995;&#32479;&#30340;&#21487;&#23457;&#26597;&#24615;&#21644;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#25216;&#26415;&#21644;&#31038;&#20250;&#25514;&#26045;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#22686;&#21152;&#20449;&#20219;&#30340;&#21487;&#33021;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2309.12351</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#25512;&#29702;&#20013;&#24314;&#31435;&#20449;&#20219;
&lt;/p&gt;
&lt;p&gt;
Establishing trust in automated reasoning. (arXiv:2309.12351v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#21160;&#25512;&#29702;&#31995;&#32479;&#30340;&#21487;&#23457;&#26597;&#24615;&#21644;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#25216;&#26415;&#21644;&#31038;&#20250;&#25514;&#26045;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#22686;&#21152;&#20449;&#20219;&#30340;&#21487;&#33021;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#19978;&#19990;&#32426;40&#24180;&#20195;&#24320;&#22987;&#65292;&#35745;&#31639;&#26426;&#30340;&#33258;&#21160;&#25512;&#29702;&#24050;&#32463;&#25104;&#20026;&#31185;&#23398;&#30740;&#31350;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#24037;&#20855;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#33258;&#21160;&#25512;&#29702;&#30340;&#35268;&#21017;&#20027;&#35201;&#30001;&#20154;&#31867;&#20197;&#31243;&#24207;&#28304;&#20195;&#30721;&#30340;&#24418;&#24335;&#21046;&#23450;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#35268;&#21017;&#26159;&#19968;&#31181;&#20114;&#34917;&#30340;&#26041;&#27861;&#65292;&#30446;&#21069;&#27491;&#22312;&#31215;&#26497;&#21457;&#23637;&#20013;&#12290;&#20026;&#20160;&#20040;&#25105;&#20204;&#24212;&#35813;&#20449;&#20219;&#36825;&#20123;&#31995;&#32479;&#20197;&#21450;&#19982;&#20854;&#24110;&#21161;&#19979;&#33719;&#24471;&#30340;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#34987;&#31185;&#23398;&#21746;&#23398;&#23478;&#20204;&#35752;&#35770;&#36807;&#65292;&#20294;&#20174;&#23454;&#36341;&#32773;&#37027;&#37324;&#25910;&#21040;&#30340;&#20851;&#27880;&#36824;&#24456;&#23569;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#29420;&#31435;&#23457;&#26597;&#65292;&#36825;&#26159;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20449;&#20219;&#26469;&#28304;&#65292;&#24182;&#30830;&#23450;&#24433;&#21709;&#20854;&#21487;&#23457;&#26597;&#24615;&#30340;&#33258;&#21160;&#25512;&#29702;&#31995;&#32479;&#30340;&#29305;&#24615;&#12290;&#23427;&#36824;&#35752;&#35770;&#20102;&#36890;&#36807;&#25216;&#26415;&#21644;&#31038;&#20250;&#25514;&#26045;&#30456;&#32467;&#21512;&#26469;&#22686;&#21152;&#21487;&#23457;&#26597;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#21487;&#33021;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its beginnings in the 1940s, automated reasoning by computers has become a tool of ever growing importance in scientific research. So far, the rules underlying automated reasoning have mainly been formulated by humans, in the form of program source code. Rules derived from large amounts of data, via machine learning techniques, are a complementary approach currently under intense development. The question of why we should trust these systems, and the results obtained with their help, has been discussed by philosophers of science but has so far received little attention by practitioners. The present work focuses on independent reviewing, an important source of trust in science, and identifies the characteristics of automated reasoning systems that affect their reviewability. It also discusses possible steps towards increasing reviewability and trustworthiness via a combination of technical and social measures.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#20351;&#29992;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35757;&#32451;&#30340;&#25104;&#26412;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24605;&#32771;&#21307;&#30103;&#26426;&#26500;&#26159;&#21542;&#24212;&#35813;&#35757;&#32451;LLM&#20197;&#21450;&#22914;&#20309;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#36873;&#25321;&#21512;&#36866;LLM&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.12339</link><description>&lt;p&gt;
&#23545;&#21307;&#30103;&#26426;&#26500;&#22312;&#30005;&#23376;&#30149;&#21382;&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#30340;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Considerations for health care institutions training large language models on electronic health records. (arXiv:2309.12339v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#20351;&#29992;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35757;&#32451;&#30340;&#25104;&#26412;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24605;&#32771;&#21307;&#30103;&#26426;&#26500;&#26159;&#21542;&#24212;&#35813;&#35757;&#32451;LLM&#20197;&#21450;&#22914;&#20309;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#36873;&#25321;&#21512;&#36866;LLM&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65288;&#22914;ChatGPT&#65289;&#24341;&#36215;&#20102;&#36328;&#23398;&#31185;&#31185;&#23398;&#23478;&#30340;&#20852;&#36259;&#65307;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#23545;LLM&#22312;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#28508;&#22312;&#24212;&#29992;&#20063;&#24341;&#21457;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#21307;&#30103;&#26426;&#26500;&#26377;&#24847;&#35753;LLM&#22312;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#39318;&#20808;&#24517;&#39035;&#38754;&#23545;&#19968;&#20123;&#26840;&#25163;&#30340;&#38382;&#39064;&#65306;&#20182;&#20204;&#24212;&#35813;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLM&#65292;&#36824;&#26159;&#20174;&#24320;&#28304;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65311;&#23545;&#20110;&#39044;&#20808;&#30830;&#23450;&#39044;&#31639;&#30340;&#21307;&#30103;&#26426;&#26500;&#26469;&#35828;&#65292;&#20182;&#20204;&#21487;&#20197;&#36127;&#25285;&#24471;&#36215;&#30340;&#26368;&#22823;LLM&#26159;&#20160;&#20040;&#65311;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#20351;&#29992;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;LLM&#35757;&#32451;&#30340;&#25104;&#26412;&#26469;&#23545;&#36825;&#20123;&#38382;&#39064;&#36827;&#34892;&#20102;&#21021;&#27493;&#25506;&#35752;&#12290;&#36825;&#20010;&#20998;&#26512;&#20026;&#20174;&#25968;&#25454;&#35268;&#27169;&#12289;&#35745;&#31639;&#35268;&#27169;&#21644;&#35757;&#32451;&#39044;&#31639;&#30340;&#35282;&#24230;&#24605;&#32771;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like ChatGPT have excited scientists across fields; in medicine, one source of excitement is the potential applications of LLMs trained on electronic health record (EHR) data. But there are tough questions we must first answer if health care institutions are interested in having LLMs trained on their own data; should they train an LLM from scratch or fine-tune it from an open-source model? For healthcare institutions with a predefined budget, what are the biggest LLMs they can afford? In this study, we take steps towards answering these questions with an analysis on dataset sizes, model sizes, and costs for LLM training using EHR data. This analysis provides a framework for thinking about these questions in terms of data scale, compute scale, and training budgets.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20135;&#29983;&#30340;&#21019;&#36896;&#24615;&#36755;&#20986;&#19982;&#20154;&#31867;&#23457;&#32654;&#21028;&#26029;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25351;&#20986;&#20154;&#20204;&#23545;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20135;&#29983;&#30340;&#21453;&#24212;&#26159;&#32463;&#30001;&#33402;&#26415;&#21028;&#26029;&#26469;&#35843;&#33410;&#30340;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25552;&#21040;&#20102;&#23545;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#21453;&#24212;&#21487;&#33021;&#28304;&#20110;&#23545;&#20854;&#26410;&#26469;&#21457;&#23637;&#30340;&#25285;&#24551;&#65292;&#24182;&#25506;&#35752;&#20102;&#33402;&#26415;&#20316;&#21697;&#22240;&#20026;&#20854;&#25991;&#21270;&#26465;&#20214;&#21644;&#20154;&#31867;&#29366;&#20917;&#30340;&#21452;&#37325;&#35299;&#35835;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12338</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#23457;&#32654;&#21028;&#26029;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence and Aesthetic Judgment. (arXiv:2309.12338v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12338
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20135;&#29983;&#30340;&#21019;&#36896;&#24615;&#36755;&#20986;&#19982;&#20154;&#31867;&#23457;&#32654;&#21028;&#26029;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25351;&#20986;&#20154;&#20204;&#23545;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20135;&#29983;&#30340;&#21453;&#24212;&#26159;&#32463;&#30001;&#33402;&#26415;&#21028;&#26029;&#26469;&#35843;&#33410;&#30340;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25552;&#21040;&#20102;&#23545;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#21453;&#24212;&#21487;&#33021;&#28304;&#20110;&#23545;&#20854;&#26410;&#26469;&#21457;&#23637;&#30340;&#25285;&#24551;&#65292;&#24182;&#25506;&#35752;&#20102;&#33402;&#26415;&#20316;&#21697;&#22240;&#20026;&#20854;&#25991;&#21270;&#26465;&#20214;&#21644;&#20154;&#31867;&#29366;&#20917;&#30340;&#21452;&#37325;&#35299;&#35835;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20135;&#29983;&#20197;&#20154;&#31867;&#34920;&#36798;&#39118;&#26684;&#20026;&#22522;&#30784;&#30340;&#21019;&#36896;&#24615;&#36755;&#20986;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19982;&#29616;&#20195;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20135;&#20986;&#30456;&#36935;&#26159;&#36890;&#36807;&#32654;&#23398;&#21028;&#26029;&#26469;&#35843;&#33410;&#30340;&#65292;&#36825;&#31181;&#21028;&#26029;&#26041;&#24335;&#19982;&#25105;&#20204;&#19982;&#33402;&#26415;&#21697;&#30340;&#20114;&#21160;&#30456;&#20284;&#12290;&#25105;&#20204;&#22312;&#21338;&#29289;&#39302;&#20013;&#23545;&#33402;&#26415;&#21697;&#30340;&#35299;&#37322;&#36807;&#31243;&#24182;&#19981;&#26159;&#20154;&#31867;&#22266;&#26377;&#30340;&#33021;&#21147;&#65292;&#32780;&#26159;&#36890;&#36807;&#33402;&#26415;&#21490;&#21644;&#33402;&#26415;&#25209;&#35780;&#31561;&#23398;&#31185;&#22312;&#21382;&#21490;&#19978;&#21457;&#23637;&#36215;&#26469;&#26469;&#23436;&#25104;&#26576;&#20123;&#31038;&#20250;&#21151;&#33021;&#30340;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#22312;&#32771;&#34385;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20135;&#29983;&#30340;&#21453;&#24212;&#26102;&#24863;&#21040;&#22256;&#24785;&#65292;&#20197;&#21450;&#25105;&#20204;&#24212;&#35813;&#22914;&#20309;&#25509;&#35302;&#36825;&#31181;&#26032;&#23186;&#20171;&#20197;&#21450;&#20026;&#20160;&#20040;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20284;&#20046;&#24341;&#21457;&#20102;&#23545;&#26410;&#26469;&#30340;&#35768;&#22810;&#24656;&#24807;&#12290;&#25105;&#20204;&#33258;&#28982;&#22320;&#32487;&#25215;&#20102;&#33402;&#26415;&#21490;&#30340;&#22240;&#26524;&#25512;&#26029;&#38590;&#39064;&#65306;&#19968;&#37096;&#20316;&#21697;&#26082;&#21487;&#20197;&#34987;&#35299;&#35835;&#20026;&#21463;&#21040;&#20102;&#24433;&#21709;&#20854;&#21019;&#20316;&#30340;&#25991;&#21270;&#26465;&#20214;&#30340;&#30151;&#29366;&#65292;&#21516;&#26102;&#21448;&#34987;&#26500;&#24314;&#20026;&#19968;&#31181;&#27704;&#24658;&#30340;&#12289;&#20284;&#20046;&#38750;&#22240;&#26524;&#30340;&#23545;&#27704;&#24658;&#20154;&#31867;&#29366;&#20917;&#30340;&#28140;&#21462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#24403;&#36825;&#19968;&#22256;&#22659;&#20986;&#29616;&#26102;&#25152;&#24102;&#26469;&#30340;&#26410;&#35299;&#20915;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AIs produce creative outputs in the style of human expression. We argue that encounters with the outputs of modern generative AI models are mediated by the same kinds of aesthetic judgments that organize our interactions with artwork. The interpretation procedure we use on art we find in museums is not an innate human faculty, but one developed over history by disciplines such as art history and art criticism to fulfill certain social functions. This gives us pause when considering our reactions to generative AI, how we should approach this new medium, and why generative AI seems to incite so much fear about the future. We naturally inherit a conundrum of causal inference from the history of art: a work can be read as a symptom of the cultural conditions that influenced its creation while simultaneously being framed as a timeless, seemingly acausal distillation of an eternal human condition. In this essay, we focus on an unresolved tension when we bring this dilemma to bear 
&lt;/p&gt;</description></item><item><title>ActiveAI&#39033;&#30446;&#20026;&#21021;&#20013;&#23398;&#29983;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;AI4K12&#30693;&#35782;&#26694;&#26550;&#30340;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#23398;&#20064;&#20307;&#39564;&#65292;&#36890;&#36807;&#30446;&#26631;&#24773;&#26223;&#23398;&#20064;&#12289;&#21363;&#26102;&#21453;&#39304;&#21644;&#39033;&#30446;&#23398;&#20064;&#31561;&#26426;&#21046;&#24110;&#21161;&#23398;&#29983;&#26377;&#25928;&#22320;&#19982;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20114;&#21160;&#65292;&#24182;&#22521;&#20859;&#20182;&#20204;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#32467;&#26524;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12337</link><description>&lt;p&gt;
ActiveAI&#65306;&#24341;&#20837;&#22522;&#20110;&#30446;&#26631;&#24773;&#26223;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#25945;&#32946;&#32473;&#21021;&#20013;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
ActiveAI: Introducing AI Literacy for Middle School Learners with Goal-based Scenario Learning. (arXiv:2309.12337v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12337
&lt;/p&gt;
&lt;p&gt;
ActiveAI&#39033;&#30446;&#20026;&#21021;&#20013;&#23398;&#29983;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;AI4K12&#30693;&#35782;&#26694;&#26550;&#30340;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#23398;&#20064;&#20307;&#39564;&#65292;&#36890;&#36807;&#30446;&#26631;&#24773;&#26223;&#23398;&#20064;&#12289;&#21363;&#26102;&#21453;&#39304;&#21644;&#39033;&#30446;&#23398;&#20064;&#31561;&#26426;&#21046;&#24110;&#21161;&#23398;&#29983;&#26377;&#25928;&#22320;&#19982;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20114;&#21160;&#65292;&#24182;&#22521;&#20859;&#20182;&#20204;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#32467;&#26524;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ActiveAI&#39033;&#30446;&#36890;&#36807;&#25552;&#20379;&#22522;&#20110;AI4K12&#30693;&#35782;&#26694;&#26550;&#30340;&#24341;&#20154;&#20837;&#32988;&#30340;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#23398;&#20064;&#20307;&#39564;&#65292;&#26469;&#35299;&#20915;7-9&#24180;&#32423;&#23398;&#29983;&#22312;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#20013;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#35813;&#24212;&#29992;&#21033;&#29992;&#30446;&#26631;&#24773;&#26223;&#12289;&#21363;&#26102;&#21453;&#39304;&#12289;&#39033;&#30446;&#23398;&#20064;&#21644;&#26234;&#33021;&#20195;&#29702;&#31561;&#23398;&#20064;&#31185;&#23398;&#26426;&#21046;&#65292;&#32467;&#21512;&#28369;&#22359;&#12289;&#27493;&#36827;&#22120;&#12289;&#25910;&#38598;&#22120;&#31561;&#22810;&#31181;&#23398;&#20064;&#32773;&#36755;&#20837;&#26041;&#24335;&#65292;&#20197;&#22686;&#24378;&#29702;&#35299;&#33021;&#21147;&#12290;&#22312;&#36825;&#20123;&#35838;&#31243;&#20013;&#65292;&#23398;&#29983;&#20204;&#22788;&#29702;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20013;&#30340;&#24773;&#24863;&#20998;&#26512;&#31561;&#29616;&#23454;&#22330;&#26223;&#65292;&#20174;&#32780;&#24110;&#21161;&#20182;&#20204;&#23398;&#20250;&#26377;&#25928;&#22320;&#19982;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20114;&#21160;&#65292;&#24182;&#22521;&#20859;&#20182;&#20204;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#32467;&#26524;&#30340;&#33021;&#21147;&#12290;&#35813;&#39033;&#30446;&#37319;&#29992;&#20102;&#23398;&#20064;&#24037;&#31243;&#36807;&#31243;&#65288;LEP&#65289;&#26469;&#25351;&#23548;&#39033;&#30446;&#30340;&#21019;&#24314;&#21644;&#25968;&#25454;&#24037;&#20855;&#21270;&#65292;&#32858;&#28966;&#35774;&#35745;&#21644;&#24433;&#21709;&#12290;&#30446;&#21069;&#65292;&#35813;&#39033;&#30446;&#27491;&#22788;&#20110;&#23454;&#26045;&#38454;&#27573;&#65292;&#21033;&#29992;&#20102;&#26234;&#33021;&#23548;&#24072;&#35774;&#35745;&#21407;&#21017;&#36827;&#34892;&#24212;&#29992;&#24320;&#21457;&#12290;&#25193;&#23637;&#25688;&#35201;&#20171;&#32461;&#20102;&#39033;&#30446;&#30340;&#22522;&#30784;&#35774;&#35745;&#21644;&#24320;&#21457;&#65292;&#24182;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ActiveAI project addresses key challenges in AI education for grades 7-9 students by providing an engaging AI literacy learning experience based on the AI4K12 knowledge framework. Utilizing learning science mechanisms such as goal-based scenarios, immediate feedback, project-based learning, and intelligent agents, the app incorporates a variety of learner inputs like sliders, steppers, and collectors to enhance understanding. In these courses, students work on real-world scenarios like analyzing sentiment in social media comments. This helps them learn to effectively engage with AI systems and develop their ability to evaluate AI-generated output. The Learning Engineering Process (LEP) guided the project's creation and data instrumentation, focusing on design and impact. The project is currently in the implementation stage, leveraging the intelligent tutor design principles for app development. The extended abstract presents the foundational design and development, with further eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#28508;&#21147;&#12289;&#24212;&#29992;&#12289;&#38480;&#21046;&#12289;&#20262;&#29702;&#31561;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12332</link><description>&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#25945;&#32946;&#65306;&#32972;&#26223;&#19982;&#26368;&#26032;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Education in the age of Generative AI: Context and Recent Developments. (arXiv:2309.12332v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#28508;&#21147;&#12289;&#24212;&#29992;&#12289;&#38480;&#21046;&#12289;&#20262;&#29702;&#31561;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20010;&#20154;&#21644;&#32452;&#32455;&#24320;&#22987;&#25506;&#32034;&#20854;&#22312;&#21508;&#20010;&#34892;&#19994;&#25552;&#21319;&#29983;&#20135;&#21147;&#21644;&#25913;&#21892;&#20135;&#21697;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;&#25945;&#32946;&#39046;&#22495;&#20063;&#19981;&#20363;&#22806;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#21487;&#20197;&#36861;&#28335;&#21040;20&#19990;&#32426;60&#24180;&#20195;&#12290;&#37492;&#20110;&#36825;&#20010;&#21382;&#21490;&#32972;&#26223;&#65292;&#26412;&#30333;&#30382;&#20070;&#20316;&#20026;&#31995;&#21015;&#30340;&#39318;&#31687;&#65292;&#38416;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#20013;&#30340;&#20316;&#29992;&#12290;&#35813;&#31995;&#21015;&#28145;&#20837;&#25506;&#35752;&#20102;&#28508;&#21147;&#12289;&#25104;&#21151;&#24212;&#29992;&#12289;&#23616;&#38480;&#24615;&#12289;&#20262;&#29702;&#32771;&#34385;&#21644;&#26410;&#26469;&#36235;&#21183;&#31561;&#20027;&#39064;&#12290;&#35813;&#21021;&#22987;&#25991;&#31456;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#31361;&#20986;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of generative artificial intelligence, an increasing number of individuals and organizations have begun exploring its potential to enhance productivity and improve product quality across various sectors. The field of education is no exception. However, it is vital to notice that artificial intelligence adoption in education dates back to the 1960s. In light of this historical context, this white paper serves as the inaugural piece in a four-part series that elucidates the role of AI in education. The series delves into topics such as its potential, successful applications, limitations, ethical considerations, and future trends. This initial article provides a comprehensive overview of the field, highlighting the recent developments within the generative artificial intelligence sphere.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#31038;&#20250;&#27491;&#20041;&#35270;&#35282;&#25506;&#35752;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#12289;&#22266;&#26377;&#30340;&#20559;&#35265;&#20197;&#21450;&#26816;&#27979;AI&#29983;&#25104;&#20889;&#20316;&#20013;&#30340;&#28508;&#22312;&#19981;&#20844;&#27491;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12331</link><description>&lt;p&gt;
&#20197;&#31038;&#20250;&#27491;&#20041;&#35270;&#35282;&#25506;&#32034;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Approaches to Generative Artificial Intelligence, A Social Justice Perspective. (arXiv:2309.12331v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#31038;&#20250;&#27491;&#20041;&#35270;&#35282;&#25506;&#35752;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#12289;&#22266;&#26377;&#30340;&#20559;&#35265;&#20197;&#21450;&#26816;&#27979;AI&#29983;&#25104;&#20889;&#20316;&#20013;&#30340;&#28508;&#22312;&#19981;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;2023-2024&#23398;&#24180;&#65292;ChatGPT&#27599;&#26376;1.6&#20159;&#27425;&#35775;&#38382;&#30340;&#24191;&#27867;&#26222;&#21450;&#65292;&#23558;&#23545;&#23398;&#26415;&#35802;&#20449;&#20135;&#29983;&#24433;&#21709;&#12290;&#39640;&#20013;&#23398;&#29983;&#20013;&#26377;77&#65285;&#26366;&#25253;&#21578;&#21442;&#19982;&#19981;&#35802;&#23454;&#34892;&#20026;&#65292;&#30001;Chan&#65288;arXiv:2306.03358v2&#65289;&#31216;&#20026;&#8220;AI&#25220;&#34989;&#8221;&#30340;AI&#39537;&#21160;&#20889;&#20316;&#36741;&#21161;&#30340;&#20852;&#36215;&#23558;&#20351;&#25220;&#34989;&#26356;&#21152;&#23481;&#26131;&#21644;&#19981;&#26131;&#26816;&#27979;&#12290;&#34429;&#28982;&#36825;&#20123;&#38382;&#39064;&#36843;&#22312;&#30473;&#30571;&#65292;&#20294;&#23427;&#20204;&#20063;&#24341;&#21457;&#20102;&#23545;&#36825;&#39033;&#25216;&#26415;&#38761;&#21629;&#24615;&#36136;&#30340;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#20027;&#24615;&#12289;&#25968;&#25454;&#38544;&#31169;&#12289;&#29256;&#26435;&#21644;&#20844;&#24179;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;&#31038;&#20250;&#27491;&#20041;&#30340;&#35282;&#24230;&#25506;&#35752;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65292;&#32771;&#23519;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#12289;&#22266;&#26377;&#30340;&#20559;&#35265;&#20197;&#21450;&#26816;&#27979;AI&#29983;&#25104;&#20889;&#20316;&#20013;&#30340;&#28508;&#22312;&#19981;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the 2023-2024 academic year, the widespread availability of generative artificial intelligence, exemplified by ChatGPT's 1.6 billion monthly visits, is set to impact academic integrity. With 77% of high school students previously reporting engagement in dishonest behaviour, the rise of AI-driven writing assistance, dubbed 'AI-giarism' by Chan (arXiv:2306.03358v2), will make plagiarism more accessible and less detectable. While these concerns are urgent, they also raise broader questions about the revolutionary nature of this technology, including autonomy, data privacy, copyright, and equity. This paper aims to explore generative AI from a social justice perspective, examining the training of these models, the inherent biases, and the potential injustices in detecting AI-generated writing.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24433;&#21709;&#21307;&#23398;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#22806;&#37096;&#21644;&#20869;&#37096;&#22240;&#32032;&#65292;&#21253;&#25324;&#27861;&#35268;&#38480;&#21046;&#12289;&#19978;&#19979;&#25991;&#12289;&#21830;&#19994;&#21644;&#36816;&#33829;&#21387;&#21147;&#12289;&#35748;&#35782;&#24046;&#24322;&#20197;&#21450;&#26631;&#35760;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12327</link><description>&lt;p&gt;
&#30495;&#23454;&#24615;&#19982;&#25361;&#25112;&#65306;&#24433;&#21709;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Ground Truth Or Dare: Factors Affecting The Creation Of Medical Datasets For Training AI. (arXiv:2309.12327v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12327
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24433;&#21709;&#21307;&#23398;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#22806;&#37096;&#21644;&#20869;&#37096;&#22240;&#32032;&#65292;&#21253;&#25324;&#27861;&#35268;&#38480;&#21046;&#12289;&#19978;&#19979;&#25991;&#12289;&#21830;&#19994;&#21644;&#36816;&#33829;&#21387;&#21147;&#12289;&#35748;&#35782;&#24046;&#24322;&#20197;&#21450;&#26631;&#35760;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#30340;&#26680;&#24515;&#30446;&#26631;&#20043;&#19968;&#26159;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#35768;&#22810;&#30740;&#31350;&#32773;&#25351;&#20986;&#20102;&#27880;&#37322;&#27493;&#39588;&#22312;&#21019;&#24314;&#39640;&#36136;&#37327;&#25968;&#25454;&#26102;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#23545;&#20110;&#20351;&#25968;&#25454;&#27880;&#37322;&#25104;&#20026;&#21487;&#33021;&#30340;&#24037;&#20316;&#21364;&#20184;&#20986;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#24037;&#20316;&#23450;&#20041;&#20026;&#30495;&#23454;&#24615;&#26550;&#26500;&#30340;&#35774;&#35745;&#65292;&#24182;&#25506;&#35752;&#22312;&#21019;&#24314;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#20043;&#21069;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#22312;&#19977;&#20010;&#21355;&#29983;&#25216;&#26415;&#32452;&#32455;&#20013;&#30340;&#24191;&#27867;&#24037;&#20316;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24433;&#21709;&#21307;&#23398;&#25968;&#25454;&#38598;&#21019;&#24314;&#36807;&#31243;&#30340;&#20116;&#20010;&#22806;&#37096;&#21644;&#20869;&#37096;&#22240;&#32032;&#12290;&#19977;&#20010;&#22806;&#37096;&#22240;&#32032;&#21253;&#25324;&#27861;&#35268;&#38480;&#21046;&#12289;&#21019;&#24314;&#21644;&#20351;&#29992;&#30340;&#19978;&#19979;&#25991;&#20197;&#21450;&#21830;&#19994;&#21644;&#36816;&#33829;&#21387;&#21147;&#12290;&#36825;&#20123;&#22240;&#32032;&#24433;&#21709;&#21307;&#23398;&#25968;&#25454;&#37319;&#38598;&#65292;&#24182;&#24418;&#25104;&#30495;&#23454;&#24615;&#26550;&#26500;&#30340;&#35774;&#35745;&#12290;&#20004;&#20010;&#20869;&#37096;&#22240;&#32032;&#21253;&#25324;&#35748;&#35782;&#24046;&#24322;&#21644;&#26631;&#35760;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#30452;&#25509;&#22609;&#36896;&#20102;&#30495;&#23454;&#24615;&#26550;&#26500;&#30340;&#35774;&#35745;&#12290;&#20851;&#20110;&#20160;&#20040;&#26500;&#25104;&#20102;&#30495;&#23454;&#24615;&#26550;&#26500;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the core goals of responsible AI development is ensuring high-quality training datasets. Many researchers have pointed to the importance of the annotation step in the creation of high-quality data, but less attention has been paid to the work that enables data annotation. We define this work as the design of ground truth schema and explore the challenges involved in the creation of datasets in the medical domain even before any annotations are made. Based on extensive work in three health-tech organisations, we describe five external and internal factors that condition medical dataset creation processes. Three external factors include regulatory constraints, the context of creation and use, and commercial and operational pressures. These factors condition medical data collection and shape the ground truth schema design. Two internal factors include epistemic differences and limits of labelling. These directly shape the design of the ground truth schema. Discussions of what const
&lt;/p&gt;</description></item><item><title>FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2309.12325</link><description>&lt;p&gt;
FUTURE-AI&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;&#21644;&#21487;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#30340;&#22269;&#38469;&#20849;&#35782;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare. (arXiv:2309.12325v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12325
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;AI&#25216;&#26415;&#22312;&#29616;&#23454;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#37096;&#32626;&#21644;&#37319;&#29992;&#20173;&#21463;&#38480;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#21307;&#30103;AI&#30340;&#25216;&#26415;&#12289;&#20020;&#24202;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#39118;&#38505;&#25552;&#20986;&#20102;&#20851;&#27880;&#12290;&#20026;&#20102;&#22686;&#21152;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37319;&#29992;&#65292;&#21307;&#30103;AI&#24037;&#20855;&#24517;&#39035;&#24471;&#21040;&#24739;&#32773;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20581;&#24247;&#32452;&#32455;&#21644;&#24403;&#23616;&#30340;&#20449;&#20219;&#21644;&#25509;&#21463;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;FUTURE-AI&#25351;&#21335;&#20316;&#20026;&#31532;&#19968;&#20010;&#29992;&#20110;&#25351;&#23548;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#12290;FUTURE-AI&#32852;&#30431;&#25104;&#31435;&#20110;2021&#24180;&#65292;&#30446;&#21069;&#21253;&#25324;&#26469;&#33258;51&#20010;&#22269;&#23478;&#30340;118&#20301;&#36328;&#23398;&#31185;&#19987;&#23478;&#65292;&#20195;&#34920;&#20102;&#25152;&#26377;&#22823;&#27954;&#65292;&#21253;&#25324;AI&#31185;&#23398;&#23478;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20262;&#29702;&#23398;&#23478;&#21644;&#31038;&#20250;&#31185;&#23398;&#23478;&#12290;&#22312;&#20026;&#26399;&#20004;&#24180;&#30340;&#26102;&#38388;&#37324;&#65292;&#32852;&#30431;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#23450;&#20041;&#20102;&#21487;&#20449;AI&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20854;&#20013;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
Despite major advances in artificial intelligence (AI) for medicine and healthcare, the deployment and adoption of AI technologies remain limited in real-world clinical practice. In recent years, concerns have been raised about the technical, clinical, ethical and legal risks associated with medical AI. To increase real world adoption, it is essential that medical AI tools are trusted and accepted by patients, clinicians, health organisations and authorities. This work describes the FUTURE-AI guideline as the first international consensus framework for guiding the development and deployment of trustworthy AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and currently comprises 118 inter-disciplinary experts from 51 countries representing all continents, including AI scientists, clinicians, ethicists, and social scientists. Over a two-year period, the consortium defined guiding principles and best practices for trustworthy AI through an iterative process comprising a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#24352;&#36890;&#36807;&#27861;&#24459;&#21046;&#24230;&#26469;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#38382;&#39064;&#65292;&#35748;&#20026;&#27861;&#24459;&#26159;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26368;&#20339;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.12321</link><description>&lt;p&gt;
&#36890;&#36807;&#27861;&#24459;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#30340;&#29702;&#35770;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
A Case for AI Safety via Law. (arXiv:2309.12321v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#24352;&#36890;&#36807;&#27861;&#24459;&#21046;&#24230;&#26469;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#38382;&#39064;&#65292;&#35748;&#20026;&#27861;&#24459;&#26159;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26368;&#20339;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#20351;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#23433;&#20840;&#24182;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20542;&#21521;&#20110;&#20381;&#38752;&#20154;&#31867;&#22312;&#19981;&#30830;&#23450;&#24773;&#20917;&#19979;&#30340;&#24178;&#39044;&#65292;&#36890;&#36807;&#35757;&#32451;&#25110;&#35266;&#23519;&#26469;&#23398;&#20064;&#20154;&#31867;&#30340;&#20215;&#20540;&#35266;&#21644;&#24847;&#22270;&#65292;&#25552;&#20379;&#20851;&#38381;&#24320;&#20851;&#65292;&#23454;&#26045;&#38548;&#31163;&#25110;&#27169;&#25311;&#29615;&#22659;&#65292;&#25110;&#25512;&#26029;&#22914;&#26524;&#20154;&#20204;&#26377;&#26356;&#22810;&#30693;&#35782;&#21644;&#26102;&#38388;&#26469;&#24605;&#32771;&#65292;&#20182;&#20204;&#20250;&#24819;&#35201;&#20160;&#20040;&#12290;&#20197;&#27861;&#24459;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861; - &#22914;&#20197;&#33406;&#33832;&#20811;&#183;&#38463;&#35199;&#33707;&#22827;&#20026;&#28789;&#24863; - &#24182;&#19981;&#34987;&#24191;&#27867;&#30475;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#26377;&#25928;&#30340;&#27861;&#24459;&#21046;&#24230;&#26159;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#38382;&#39064;&#30340;&#26368;&#20339;&#36884;&#24452;&#12290;&#27861;&#24459;&#34987;&#23450;&#20041;&#20026;&#23545;&#36866;&#29992;&#20110;&#29305;&#23450;&#20195;&#29702;&#20154;&#22312;&#29305;&#23450;&#39046;&#22495;/&#24773;&#22659;&#20013;&#30340;&#31105;&#27490;&#21644;&#35268;&#23450;&#36827;&#34892;&#32534;&#30721;&#30340;&#20219;&#20309;&#35268;&#21017;&#65292;&#24182;&#21253;&#25324;&#21046;&#23450;&#12289;&#31649;&#29702;&#12289;&#25191;&#34892;&#21644;&#35785;&#35772;&#27492;&#31867;&#35268;&#21017;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to make artificial intelligence (AI) systems safe and aligned with human values is an open research question. Proposed solutions tend toward relying on human intervention in uncertain situations, learning human values and intentions through training or observation, providing off-switches, implementing isolation or simulation environments, or extrapolating what people would want if they had more knowledge and more time to think. Law-based approaches--such as inspired by Isaac Asimov--have not been well regarded. This paper makes a case that effective legal systems are the best way to address AI safety. Law is defined as any rules that codify prohibitions and prescriptions applicable to particular agents in specified domains/contexts and includes processes for enacting, managing, enforcing, and litigating such rules.
&lt;/p&gt;</description></item><item><title>&#26412;&#25253;&#21578;&#25552;&#20379;&#20102;&#19968;&#32452;&#20351;&#29992;&#22330;&#26223;&#21644;&#23454;&#38469;&#20363;&#23376;&#65292;&#26088;&#22312;&#24110;&#21161;&#25945;&#24072;&#22312;&#25945;&#32946;&#20013;&#24341;&#20837;&#20154;&#24037;&#26234;&#33021;&#65292;&#20197;&#28608;&#21457;&#21019;&#36896;&#21147;&#21644;&#25351;&#23548;&#25945;&#23398;&#12290;</title><link>http://arxiv.org/abs/2309.12320</link><description>&lt;p&gt;
&#22312;&#25945;&#32946;&#20013;&#20351;&#29992;&#22330;&#26223;&#21644;&#23454;&#38469;&#20363;&#23376;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Use Scenarios &amp; Practical Examples of AI Use in Education. (arXiv:2309.12320v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#25552;&#20379;&#20102;&#19968;&#32452;&#20351;&#29992;&#22330;&#26223;&#21644;&#23454;&#38469;&#20363;&#23376;&#65292;&#26088;&#22312;&#24110;&#21161;&#25945;&#24072;&#22312;&#25945;&#32946;&#20013;&#24341;&#20837;&#20154;&#24037;&#26234;&#33021;&#65292;&#20197;&#28608;&#21457;&#21019;&#36896;&#21147;&#21644;&#25351;&#23548;&#25945;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#25552;&#20986;&#20102;&#19968;&#32452;&#22522;&#20110;&#29616;&#26377;&#36164;&#28304;&#30340;&#20351;&#29992;&#22330;&#26223;&#65292;&#25945;&#24072;&#21487;&#20197;&#29992;&#20316;&#28789;&#24863;&#26469;&#21019;&#24314;&#33258;&#24049;&#30340;&#22330;&#26223;&#65292;&#26088;&#22312;&#22312;&#19981;&#21516;&#30340;&#20013;&#23567;&#23398;&#23618;&#27425;&#21644;&#30446;&#26631;&#20013;&#24341;&#20837;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12290;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#39046;&#22495;&#65288;AIEd&#65289;&#38750;&#24120;&#27963;&#36291;&#65292;&#19981;&#26029;&#28044;&#29616;&#26032;&#30340;&#36164;&#28304;&#21644;&#24037;&#20855;&#12290;&#26412;&#25991;&#26723;&#20013;&#21253;&#21547;&#30340;&#36164;&#28304;&#24050;&#32463;&#32463;&#36807;&#23398;&#29983;&#27979;&#35797;&#65292;&#24182;&#30001;&#35813;&#39046;&#22495;&#30340;&#19987;&#23478;&#36873;&#25321;&#65292;&#20294;&#23427;&#20204;&#24517;&#39035;&#34987;&#35270;&#20026;&#25351;&#23548;&#21644;&#28608;&#21457;&#25945;&#24072;&#21019;&#36896;&#21147;&#30340;&#23454;&#38469;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report presents a set of use scenarios based on existing resources that teachers can use as inspiration to create their own, with the aim of introducing artificial intelligence (AI) at different pre-university levels, and with different goals. The Artificial Intelligence Education field (AIEd) is very active, with new resources and tools arising continuously. Those included in this document have already been tested with students and selected by experts in the field, but they must be taken just as practical examples to guide and inspire teachers creativity.
&lt;/p&gt;</description></item><item><title>MetaMath&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.12284</link><description>&lt;p&gt;
MetaMath&#65306;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#33258;&#24049;&#30340;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12284
&lt;/p&gt;
&lt;p&gt;
MetaMath&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#26497;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;LLMs&#65288;&#20363;&#22914;LLaMA-2&#65289;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#20173;&#28982;&#36828;&#36828;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#21407;&#22240;&#26159;&#22797;&#26434;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaMath&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27809;&#26377;&#39069;&#22806;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20197;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#20889;&#20837;&#38382;&#39064;&#26469;&#24341;&#23548;&#25968;&#23398;&#38382;&#39064;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;MetaMathQA&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;MetaMathQA&#19978;&#23545;LLaMA-2&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23545;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#20004;&#20010;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#65288;&#21363;GSM8K&#21644;MATH&#65289;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MetaMath&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#19968;&#22871;&#24320;&#28304;LLMs&#12290;&#25105;&#20204;&#30340;MetaMath-7B&#27169;&#22411;&#22312;GSM8K&#19978;&#36798;&#21040;&#20102;66.4&#65285;&#65292;&#22312;MATH&#19978;&#36798;&#21040;&#20102;19.4&#65285;&#65292;&#36229;&#36807;&#20102;&#30456;&#21516;&#35268;&#27169;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose \emph{MetaMath}, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (\ie, GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves $66.4\%$ on GSM8K and $19.4\%$ on MATH, exceeding the state-of-the-art models of the same size by 
&lt;/p&gt;</description></item><item><title>LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11998</link><description>&lt;p&gt;
LMSYS-Chat-1M&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. (arXiv:2309.11998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11998
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#30740;&#31350;&#20154;&#20204;&#22914;&#20309;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19982;&#20854;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LMSYS-Chat-1M&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#19982;25&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#30340;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#25105;&#20204;&#30340;Vicuna&#28436;&#31034;&#21644;Chatbot Arena&#32593;&#31449;&#19978;&#30340;21&#19975;&#20010;&#29420;&#31435;IP&#22320;&#22336;&#20013;&#25910;&#38598;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#20869;&#23481;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#31574;&#21010;&#36807;&#31243;&#12289;&#22522;&#26412;&#32479;&#35745;&#25968;&#25454;&#21644;&#20027;&#39064;&#20998;&#24067;&#65292;&#24378;&#35843;&#20854;&#22810;&#26679;&#24615;&#12289;&#29420;&#29305;&#24615;&#21644;&#35268;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#29992;&#20363;&#23637;&#31034;&#20102;&#23427;&#30340;&#22810;&#26679;&#24615;&#65306;&#24320;&#21457;&#19982;GPT-4&#34920;&#29616;&#30456;&#20284;&#30340;&#20869;&#23481;&#36807;&#28388;&#27169;&#22411;&#12289;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#22522;&#20934;&#12289;&#35757;&#32451;&#19982;Vicuna&#34920;&#29616;&#30456;&#20284;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12289;&#21019;&#24314;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#38382;&#39064;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25105;&#20204;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is pub
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#29366;&#24577;&#34920;&#31034;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#20219;&#21153;&#19978;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#25968;&#23383;&#29366;&#24577;&#30340;&#20195;&#29702;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#25104;&#21151;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11984</link><description>&lt;p&gt;
&#34920;&#31034;&#25277;&#35937;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#28608;&#21169;&#65306;&#22522;&#20110;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Representation Abstractions as Incentives for Reinforcement Learning Agents: A Robotic Grasping Case Study. (arXiv:2309.11984v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#29366;&#24577;&#34920;&#31034;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#20219;&#21153;&#19978;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#25968;&#23383;&#29366;&#24577;&#30340;&#20195;&#29702;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#25104;&#21151;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#19968;&#20010;&#36866;&#24403;&#30340;&#29615;&#22659;&#34920;&#31034;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#20915;&#31574;&#36807;&#31243;&#24182;&#19981;&#24635;&#26159;&#31616;&#21333;&#30340;&#12290;&#29366;&#24577;&#34920;&#31034;&#24212;&#35813;&#36275;&#22815;&#21253;&#23481;&#65292;&#20197;&#20415;&#35753;&#20195;&#29702;&#33021;&#22815;&#20449;&#24687;&#22320;&#20915;&#23450;&#20854;&#34892;&#21160;&#65292;&#24182;&#19988;&#36275;&#22815;&#32039;&#20945;&#65292;&#20197;&#25552;&#39640;&#31574;&#30053;&#35757;&#32451;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#29366;&#24577;&#34920;&#31034;&#23545;&#20195;&#29702;&#22312;&#29305;&#23450;&#26426;&#22120;&#20154;&#20219;&#21153;&#65288;&#23545;&#31216;&#21644;&#24179;&#38754;&#29289;&#20307;&#25235;&#21462;&#65289;&#19978;&#35299;&#20915;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20174;&#20855;&#26377;&#23436;&#25972;&#31995;&#32479;&#30693;&#35782;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#24320;&#22987;&#65292;&#36890;&#36807;&#25163;&#24037;&#25968;&#23383;&#34920;&#31034;&#21040;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#31034;&#65292;&#36880;&#28176;&#20943;&#23569;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#30340;&#24341;&#20837;&#37327;&#65292;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#29366;&#24577;&#34920;&#31034;&#25277;&#35937;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#27599;&#31181;&#34920;&#31034;&#23545;&#20195;&#29702;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35299;&#20915;&#20219;&#21153;&#20197;&#21450;&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#21487;&#36716;&#31227;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25968;&#23383;&#29366;&#24577;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Choosing an appropriate representation of the environment for the underlying decision-making process of the \gls{RL} agent is not always straightforward. The state representation should be inclusive enough to allow the agent to informatively decide on its actions and compact enough to increase sample efficiency for policy training. Given this outlook, this work examines the effect of various state representations in incentivizing the agent to solve a specific robotic task: antipodal and planar object grasping. A continuum of state representation abstractions is defined, starting from a model-based approach with complete system knowledge, through hand-crafted numerical, to image-based representations with decreasing level of induced task-specific knowledge. We examine the effects of each representation in the ability of the agent to solve the task in simulation and the transferability of the learned policy to the real robot. The results show that RL agents using numerical states can per
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26041;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#20511;&#37492;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.11981</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65306;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26410;&#26469;&#24230;&#37327;&#30340;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics. (arXiv:2309.11981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11981
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26041;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#20511;&#37492;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#65292;&#36825;&#20026;&#37325;&#26032;&#23457;&#35270;&#20256;&#32479;&#30340;&#26426;&#22120;&#26234;&#33021;&#24230;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20174;&#20256;&#32479;&#30340;&#22270;&#28789;&#27979;&#35797;&#36716;&#21521;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#24182;&#20511;&#37492;&#20102;&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#28145;&#21463;&#22810;&#20010;&#23398;&#31185;&#30340;&#21331;&#36234;&#24037;&#20316;&#30340;&#24433;&#21709;&#65292;&#25351;&#20986;&#20102;&#20445;&#25345;&#36328;&#23398;&#31185;&#26725;&#26753;&#24320;&#25918;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#21246;&#21202;&#20102;&#19968;&#20010;&#26356;&#21152;&#31283;&#20581;&#21644;&#21487;&#25345;&#32493;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the burgeoning field of artificial intelligence (AI), the unprecedented progress of large language models (LLMs) in natural language processing (NLP) offers an opportunity to revisit the entire approach of traditional metrics of machine intelligence, both in form and content. As the realm of machine cognitive evaluation has already reached Imitation, the next step is an efficient Language Acquisition and Understanding. Our paper proposes a paradigm shift from the established Turing Test towards an all-embracing framework that hinges on language acquisition, taking inspiration from the recent advancements in LLMs. The present contribution is deeply tributary of the excellent work from various disciplines, point out the need to keep interdisciplinary bridges open, and delineates a more robust and sustainable approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11895</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Audio Contrastive based Fine-tuning. (arXiv:2309.11895v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#20998;&#31867;&#22312;&#35821;&#38899;&#21644;&#22768;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#23558;&#27169;&#22411;&#25311;&#21512;&#21040;&#35757;&#32451;&#25968;&#25454;&#65288;&#36991;&#20813;&#36807;&#25311;&#21512;&#65289;&#24182;&#20351;&#20854;&#33021;&#22815;&#33391;&#22909;&#22320;&#27867;&#21270;&#21040;&#26032;&#39046;&#22495;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#30528;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio classification plays a crucial role in speech and sound processing tasks with a wide range of applications. There still remains a challenge of striking the right balance between fitting the model to the training data (avoiding overfitting) and enabling it to generalise well to a new domain. Leveraging the transferability of contrastive learning, we introduce Audio Contrastive-based Fine-tuning (AudioConFit), an efficient approach characterised by robust generalisability. Empirical experiments on a variety of audio classification tasks demonstrate the effectiveness and robustness of our approach, which achieves state-of-the-art results in various settings.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"FluentEditor"&#30340;&#27969;&#30021;&#35821;&#38899;&#32534;&#36753;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#27969;&#30021;&#24230;&#24847;&#35782;&#30340;&#35757;&#32451;&#20934;&#21017;&#23454;&#29616;&#25991;&#26412;&#21270;&#35821;&#38899;&#32534;&#36753;&#12290;&#20854;&#20013;&#65292;&#22768;&#23398;&#19968;&#33268;&#24615;&#32422;&#26463;&#21644;&#38901;&#24459;&#19968;&#33268;&#24615;&#32422;&#26463;&#29992;&#20110;&#20445;&#25345;&#35821;&#38899;&#30340;&#27969;&#30021;&#24615;&#21644;&#25972;&#20307;&#39118;&#26684;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11725</link><description>&lt;p&gt;
FluentEditor: &#32771;&#34385;&#22768;&#23398;&#21644;&#38901;&#24459;&#19968;&#33268;&#24615;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#38899;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
FluentEditor: Text-based Speech Editing by Considering Acoustic and Prosody Consistency. (arXiv:2309.11725v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11725
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"FluentEditor"&#30340;&#27969;&#30021;&#35821;&#38899;&#32534;&#36753;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#27969;&#30021;&#24230;&#24847;&#35782;&#30340;&#35757;&#32451;&#20934;&#21017;&#23454;&#29616;&#25991;&#26412;&#21270;&#35821;&#38899;&#32534;&#36753;&#12290;&#20854;&#20013;&#65292;&#22768;&#23398;&#19968;&#33268;&#24615;&#32422;&#26463;&#21644;&#38901;&#24459;&#19968;&#33268;&#24615;&#32422;&#26463;&#29992;&#20110;&#20445;&#25345;&#35821;&#38899;&#30340;&#27969;&#30021;&#24615;&#21644;&#25972;&#20307;&#39118;&#26684;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21270;&#35821;&#38899;&#32534;&#36753;&#65288;TSE&#65289;&#25216;&#26415;&#26088;&#22312;&#20351;&#29992;&#25143;&#36890;&#36807;&#20462;&#25913;&#36755;&#20837;&#30340;&#25991;&#26412;&#36716;&#24405;&#32780;&#19981;&#26159;&#38899;&#39057;&#26412;&#36523;&#26469;&#32534;&#36753;&#29983;&#25104;&#30340;&#38899;&#39057;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#30340;TSE&#25216;&#26415;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#30340;&#25216;&#26415;&#20027;&#35201;&#20851;&#27880;&#20110;&#20943;&#23567;&#32534;&#36753;&#21306;&#22495;&#20013;&#29983;&#25104;&#30340;&#35821;&#38899;&#29255;&#27573;&#19982;&#21442;&#32771;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24573;&#35270;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#21644;&#21407;&#22987;&#35821;&#21477;&#20013;&#30340;&#23616;&#37096;&#21644;&#25972;&#20307;&#27969;&#30021;&#24615;&#12290;&#20026;&#20102;&#20445;&#25345;&#35821;&#38899;&#30340;&#27969;&#30021;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#30021;&#35821;&#38899;&#32534;&#36753;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;FluentEditor&#8221;&#65292;&#36890;&#36807;&#32771;&#34385;&#27969;&#30021;&#24230;&#24847;&#35782;&#30340;&#35757;&#32451;&#20934;&#21017;&#36827;&#34892;TSE&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#8220;&#22768;&#23398;&#19968;&#33268;&#24615;&#32422;&#26463;&#8221;&#26088;&#22312;&#20351;&#32534;&#36753;&#21306;&#22495;&#19982;&#20854;&#30456;&#37051;&#30340;&#22768;&#23398;&#29255;&#27573;&#20043;&#38388;&#30340;&#36807;&#28193;&#21464;&#24471;&#24179;&#28369;&#24182;&#19982;&#30495;&#23454;&#24773;&#20917;&#20445;&#25345;&#19968;&#33268;&#65292;&#8220;&#38901;&#24459;&#19968;&#33268;&#24615;&#32422;&#26463;&#8221;&#26088;&#22312;&#30830;&#20445;&#32534;&#36753;&#21306;&#22495;&#20869;&#30340;&#38901;&#24459;&#23646;&#24615;&#19982;&#21407;&#22987;&#35821;&#21477;&#30340;&#25972;&#20307;&#39118;&#26684;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based speech editing (TSE) techniques are designed to enable users to edit the output audio by modifying the input text transcript instead of the audio itself. Despite much progress in neural network-based TSE techniques, the current techniques have focused on reducing the difference between the generated speech segment and the reference target in the editing region, ignoring its local and global fluency in the context and original utterance. To maintain the speech fluency, we propose a fluency speech editing model, termed \textit{FluentEditor}, by considering fluency-aware training criterion in the TSE training. Specifically, the \textit{acoustic consistency constraint} aims to smooth the transition between the edited region and its neighboring acoustic segments consistent with the ground truth, while the \textit{prosody consistency constraint} seeks to ensure that the prosody attributes within the edited regions remain consistent with the overall style of the original utterance.
&lt;/p&gt;</description></item><item><title>CFGPT&#26159;&#19968;&#20010;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#22269;&#37329;&#34701;&#21161;&#25163;&#65292;&#21253;&#25324;CFData&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#21450;CFLLM&#29992;&#20110;&#22788;&#29702;&#37329;&#34701;&#25991;&#26412;&#65292;CFAPP&#29992;&#20110;&#23454;&#38469;&#37329;&#34701;&#24212;&#29992;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#21508;&#20010;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10654</link><description>&lt;p&gt;
CFGPT: &#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#22269;&#37329;&#34701;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
CFGPT: Chinese Financial Assistant with Large Language Model. (arXiv:2309.10654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10654
&lt;/p&gt;
&lt;p&gt;
CFGPT&#26159;&#19968;&#20010;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#22269;&#37329;&#34701;&#21161;&#25163;&#65292;&#21253;&#25324;CFData&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#21450;CFLLM&#29992;&#20110;&#22788;&#29702;&#37329;&#34701;&#25991;&#26412;&#65292;CFAPP&#29992;&#20110;&#23454;&#38469;&#37329;&#34701;&#24212;&#29992;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#21508;&#20010;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CFGPT&#30340;&#20013;&#22269;&#37329;&#34701;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#38598;&#65288;CFData&#65289;&#65292;&#29992;&#20110;&#29087;&#32451;&#22788;&#29702;&#37329;&#34701;&#25991;&#26412;&#30340;&#37329;&#34701;LLM&#65288;CFLLM&#65289;&#65292;&#20197;&#21450;&#29992;&#20110;&#23454;&#38469;&#37329;&#34701;&#24212;&#29992;&#30340;&#37096;&#32626;&#26694;&#26550;&#65288;CFAPP&#65289;&#12290;CFData&#21253;&#25324;&#19968;&#20010;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#27719;&#38598;&#20102;&#20013;&#22269;&#37329;&#34701;&#25968;&#25454;&#21644;&#20998;&#26512;&#65292;&#20197;&#21450;&#24635;&#20849;584M&#20010;&#25991;&#20214;&#21644;141B&#20010;&#26631;&#35760;&#30340;&#36739;&#23567;&#30340;&#36890;&#29992;&#25991;&#26412;&#23376;&#38598;&#65292;&#24182;&#19988;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;&#38024;&#23545;&#20845;&#20010;&#19981;&#21516;&#30340;&#37329;&#34701;&#20219;&#21153;&#36827;&#34892;&#20102;&#23450;&#21046;&#65292;&#20869;&#23481;&#28085;&#30422;&#20102;&#37329;&#34701;&#20998;&#26512;&#21644;&#20915;&#31574;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;1.5M&#20010;&#25351;&#20196;&#23545;&#21644;&#24635;&#35745;1.5B&#20010;&#26631;&#35760;&#12290;CFLLM&#22522;&#20110;InternLM-7B&#36827;&#34892;&#20102;&#24179;&#34913;&#27169;&#22411;&#33021;&#21147;&#30340;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated great potential in natural language processing tasks within the financial domain. In this work, we present a Chinese Financial Generative Pre-trained Transformer framework, named CFGPT, which includes a dataset~(CFData) for pre-training and supervised fine-tuning, a financial LLM~(CFLLM) to adeptly manage financial texts, and a deployment framework~(CFAPP) designed to navigate real-world financial applications. The CFData comprising both a pre-training dataset and a supervised fine-tuning dataset, where the pre-training dataset collates Chinese financial data and analytics, alongside a smaller subset of general-purpose text with 584M documents and 141B tokens in total, and the supervised fine-tuning dataset is tailored for six distinct financial tasks, embodying various facets of financial analysis and decision-making with 1.5M instruction pairs and 1.5B tokens in total. The CFLLM, which is based on InternLM-7B to balance the model capabil
&lt;/p&gt;</description></item><item><title>RadOnc-GPT&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#32959;&#30244;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#35843;&#25972;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#29983;&#25104;&#27835;&#30103;&#26041;&#26696;&#12289;&#30830;&#23450;&#30103;&#27861;&#21644;&#25552;&#20379;&#35786;&#26029;&#25551;&#36848;&#31561;&#20851;&#38190;&#20219;&#21153;&#19978;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#36827;&#34892;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#21307;&#30103;&#39046;&#22495;&#20855;&#26377;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10160</link><description>&lt;p&gt;
RadOnc-GPT&#65306;&#19968;&#31181;&#29992;&#20110;&#25918;&#23556;&#32959;&#30244;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RadOnc-GPT: A Large Language Model for Radiation Oncology. (arXiv:2309.10160v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10160
&lt;/p&gt;
&lt;p&gt;
RadOnc-GPT&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#32959;&#30244;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#35843;&#25972;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#29983;&#25104;&#27835;&#30103;&#26041;&#26696;&#12289;&#30830;&#23450;&#30103;&#27861;&#21644;&#25552;&#20379;&#35786;&#26029;&#25551;&#36848;&#31561;&#20851;&#38190;&#20219;&#21153;&#19978;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#36827;&#34892;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#21307;&#30103;&#39046;&#22495;&#20855;&#26377;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RadOnc-GPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#35843;&#25972;&#26041;&#27861;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#32959;&#30244;&#23398;&#12290;RadOnc-GPT&#22312;Mayo Clinic&#30340;&#22823;&#37327;&#25918;&#23556;&#32959;&#30244;&#23398;&#24739;&#32773;&#35760;&#24405;&#21644;&#20020;&#24202;&#31508;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#20219;&#21153;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#21253;&#25324;&#29983;&#25104;&#25918;&#23556;&#27835;&#30103;&#26041;&#26696;&#12289;&#30830;&#23450;&#26368;&#20339;&#25918;&#23556;&#30103;&#27861;&#20197;&#21450;&#22522;&#20110;&#24739;&#32773;&#35786;&#26029;&#32454;&#33410;&#25552;&#20379;&#35786;&#26029;&#25551;&#36848;/ICD&#20195;&#30721;&#12290;&#36890;&#36807;&#23558;&#25918;&#23556;&#32959;&#30244;&#23398;&#21307;&#29983;&#27604;&#36739;RadOnc-GPT&#30340;&#21360;&#35937;&#19982;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21360;&#35937;&#36827;&#34892;&#35780;&#20272;&#65292;&#30740;&#31350;&#34920;&#26126;RadOnc-GPT&#29983;&#25104;&#30340;&#36755;&#20986;&#22312;&#28165;&#26224;&#24230;&#12289;&#29305;&#24322;&#24230;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#12290;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#21033;&#29992;&#20687;RadOnc-GPT&#36825;&#26679;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#36827;&#34892;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#31561;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#21307;&#30103;&#39046;&#22495;&#23454;&#29616;&#21464;&#38761;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents RadOnc-GPT, a large language model specialized for radiation oncology through advanced tuning methods. RadOnc-GPT was finetuned on a large dataset of radiation oncology patient records and clinical notes from the Mayo Clinic. The model employs instruction tuning on three key tasks generating radiotherapy treatment regimens, determining optimal radiation modalities, and providing diagnostic descriptions/ICD codes based on patient diagnostic details. Evaluations conducted by having radiation oncologists compare RadOnc-GPT impressions to general large language model impressions showed that RadOnc-GPT generated outputs with significantly improved clarity, specificity, and clinical relevance. The study demonstrated the potential of using large language models fine-tuned using domain-specific knowledge like RadOnc-GPT to achieve transformational capabilities in highly specialized healthcare fields such as radiation oncology.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.09357</link><description>&lt;p&gt;
Talk2Care: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20419;&#36827;&#24322;&#27493;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#36828;&#31243;&#21307;&#30103;&#24212;&#29992;&#31243;&#24207;&#26469;&#24110;&#21161;&#23478;&#24237;&#20013;&#30340;&#32769;&#24180;&#20154;&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;&#65292;&#20294;&#22522;&#26412;&#30340;&#28040;&#24687;&#21644;&#30005;&#35805;&#20173;&#28982;&#26159;&#26368;&#24120;&#35265;&#30340;&#36890;&#20449;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#26377;&#38480;&#30340;&#21487;&#29992;&#24615;&#12289;&#20449;&#24687;&#20002;&#22833;&#21644;&#27969;&#31243;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#20419;&#36827;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21450;&#20854;&#24378;&#22823;&#30340;&#33258;&#28982;&#23545;&#35805;&#21644;&#25688;&#35201;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLMs&#22312;&#36890;&#20449;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#36824;&#23384;&#22312;&#26377;&#38480;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#20004;&#39033;&#35775;&#35848;&#30740;&#31350;&#65292;&#20998;&#21035;&#19982;&#32769;&#24180;&#20154;(N=10)&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;(N=9)&#36827;&#34892;&#20102;&#20132;&#27969;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#22312;&#24739;&#32773;-&#21307;&#29983;&#24322;&#27493;&#36890;&#20449;&#20013;&#23545;LLMs&#30340;&#38656;&#27714;&#21644;&#26426;&#20250;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;Talk2Care&#65292;&#24182;&#20026;&#20004;&#20010;&#32676;&#20307;&#35774;&#35745;&#20102;&#20132;&#20114;&#32452;&#20214;: (1) &#23545;&#20110;&#32769;&#24180;&#20154;&#65292;&#25105;&#20204;&#21033;&#29992;&#35821;&#38899;&#21161;&#25163;&#30340;&#20415;&#21033;&#24615;&#21644;&#26131;&#20110;&#33719;&#21462;&#24615;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#35821;&#38899;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered VA i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#20998;&#26512;&#20102;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#39046;&#22495;&#20013;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23398;&#26415;&#20889;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20135;&#29983;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08636</link><description>&lt;p&gt;
ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#31185;&#23398;&#20889;&#20316;&#26041;&#38754;&#34920;&#29616;&#22914;&#20309;&#65311;&#65288;&#31532;23&#23395;&#31532;3&#23395;&#65289;&#12290;&#65288;arXiv:2309.08636v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3). (arXiv:2309.08636v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#20998;&#26512;&#20102;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#39046;&#22495;&#20013;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23398;&#26415;&#20889;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20135;&#29983;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21382;&#21490;&#19978;&#65292;&#29087;&#32451;&#30340;&#20889;&#20316;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#36827;&#27493;&#30340;&#20851;&#38190;&#65292;&#21019;&#36896;&#24615;&#34920;&#36798;&#34987;&#35270;&#20026;&#20154;&#31867;&#25104;&#23601;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#26631;&#24535;&#30528;&#36825;&#19968;&#21465;&#20107;&#30340;&#19968;&#20010;&#36716;&#25240;&#28857;&#65292;&#21253;&#25324;&#22312;&#31185;&#23398;&#20889;&#20316;&#26041;&#38754;&#12290;&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#26041;&#38754;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#26041;&#27861;&#22522;&#20110;&#30001;&#20154;&#31867;&#19987;&#23478;&#23545;AI&#29983;&#25104;&#20869;&#23481;&#36827;&#34892;&#23450;&#37327;&#20934;&#30830;&#24615;&#21644;&#23450;&#24615;&#31934;&#30830;&#24615;&#26631;&#35760;&#12290;&#23450;&#37327;&#20934;&#30830;&#24615;&#35780;&#20272;&#20102;&#20107;&#23454;&#30340;&#27491;&#30830;&#24615;&#65292;&#32780;&#23450;&#24615;&#31934;&#30830;&#24615;&#35780;&#20272;&#20102;&#31185;&#23398;&#36129;&#29486;&#12290;&#34429;&#28982;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29305;&#21035;&#26159;ChatGPT-4&#65292;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#24615;&#65292;&#20294;&#22312;&#29983;&#25104;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#22833;&#36133;&#20102;&#12290;&#39034;&#20415;&#25552;&#19968;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#26174;&#31034;&#65292;&#38543;&#30528;ChatGPT-4&#65292;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#24050;&#32463;&#20572;&#28382;&#19981;&#21069;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#22797;&#26434;&#19988;&#21453;&#22797;&#26080;&#24120;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historically, proficient writing was deemed essential for human advancement, with creative expression viewed as one of the hallmarks of human achievement. However, recent advances in generative AI have marked an inflection point in this narrative, including for scientific writing. This article provides a comprehensive analysis of the capabilities and limitations of six AI chatbots in scholarly writing in the humanities and archaeology. The methodology was based on tagging AI generated content for quantitative accuracy and qualitative precision by human experts. Quantitative accuracy assessed the factual correctness, while qualitative precision gauged the scientific contribution. While the AI chatbots, especially ChatGPT-4, demonstrated proficiency in recombining existing knowledge, they failed in generating original scientific content. As a side note, our results also suggest that with ChatGPT-4 the size of the LLMs has plateaued. Furthermore, the paper underscores the intricate and re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#27880;&#20837;&#29289;&#29702;&#30693;&#35782;&#26469;&#35299;&#20915;&#36339;&#36291;&#24335;&#21333;&#33050;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#36825;&#26679;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#21644;&#20462;&#27491;&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2309.07038</link><description>&lt;p&gt;
&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#36339;&#36291;&#24335;&#21333;&#33050;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcement Learning for Jumping Monopods. (arXiv:2309.07038v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#27880;&#20837;&#29289;&#29702;&#30693;&#35782;&#26469;&#35299;&#20915;&#36339;&#36291;&#24335;&#21333;&#33050;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#36825;&#26679;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#21644;&#20462;&#27491;&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#21363;&#20351;&#21333;&#33050;&#26426;&#22120;&#20154;&#33021;&#22815;&#36339;&#21040;&#20219;&#20309;&#26041;&#21521;&#65292;&#20854;&#33050;&#19979;&#30340;&#22320;&#24418;&#21487;&#33021;&#26159;&#19981;&#24179;&#30340;&#65292;&#25105;&#20204;&#35201;&#20351;&#23427;&#36798;&#21040;&#30446;&#26631;&#20301;&#32622;&#12290;&#36825;&#26159;&#19968;&#20010;&#26356;&#22823;&#31867;&#21035;&#38382;&#39064;&#30340;&#27169;&#26495;&#65292;&#20351;&#29992;&#26631;&#20934;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#25216;&#26415;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#35745;&#31639;&#24320;&#38144;&#12290;&#24378;&#21270;&#23398;&#20064; (RL) &#21487;&#33021;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#23436;&#20840;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#22312; RL &#26694;&#26550;&#20013;&#27880;&#20837;&#29289;&#29702;&#30693;&#35782;&#26469;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#24102;&#26469;&#20102;&#24191;&#27867;&#30340;&#22909;&#22788;&#65292;&#22914;&#22823;&#24133;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#65292;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#21644;&#20462;&#27491;&#25191;&#34892;&#36816;&#21160;&#30340;&#20302;&#32423;&#25511;&#21046;&#22120;&#21487;&#33021;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#22522;&#20110;&#20248;&#21270;&#21644;&#31471;&#21040;&#31471; RL &#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we consider the complex control problem of making a monopod reach a target with a jump. The monopod can jump in any direction and the terrain underneath its foot can be uneven. This is a template of a much larger class of problems, which are extremely challenging and computationally expensive to solve using standard optimisation-based techniques. Reinforcement Learning (RL) could be an interesting alternative, but the application of an end-to-end approach in which the controller must learn everything from scratch, is impractical. The solution advocated in this paper is to guide the learning process within an RL framework by injecting physical knowledge. This expedient brings to widespread benefits, such as a drastic reduction of the learning time, and the ability to learn and compensate for possible errors in the low-level controller executing the motion. We demonstrate the advantage of our approach with respect to both optimization-based and end-to-end RL approaches.
&lt;/p&gt;</description></item><item><title>&#24314;&#27169;&#25512;&#33616;&#31995;&#32479;&#29983;&#24577;&#31995;&#32479;&#38656;&#35201;&#32771;&#34385;&#21442;&#19982;&#32773;&#28608;&#21169;&#12289;&#34892;&#20026;&#20197;&#21450;&#31574;&#30053;&#24341;&#21457;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#38271;&#26399;&#20248;&#21270;&#65292;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#26435;&#34913;&#65292;&#24182;&#20943;&#23569;&#20449;&#24687;&#19981;&#23545;&#31216;&#12290;</title><link>http://arxiv.org/abs/2309.06375</link><description>&lt;p&gt;
&#24314;&#27169;&#25512;&#33616;&#31995;&#32479;&#29983;&#24577;&#31995;&#32479;&#65306;&#26426;&#21046;&#35774;&#35745;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#20132;&#21449;&#30740;&#31350;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models. (arXiv:2309.06375v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06375
&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#25512;&#33616;&#31995;&#32479;&#29983;&#24577;&#31995;&#32479;&#38656;&#35201;&#32771;&#34385;&#21442;&#19982;&#32773;&#28608;&#21169;&#12289;&#34892;&#20026;&#20197;&#21450;&#31574;&#30053;&#24341;&#21457;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#38271;&#26399;&#20248;&#21270;&#65292;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#26435;&#34913;&#65292;&#24182;&#20943;&#23569;&#20449;&#24687;&#19981;&#23545;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20301;&#20110;&#28085;&#30422;&#29992;&#25143;&#12289;&#20869;&#23481;&#25552;&#20379;&#21830;&#12289;&#24191;&#21578;&#21830;&#21644;&#20854;&#20182;&#21442;&#19982;&#32773;&#34892;&#20026;&#30340;&#22797;&#26434;&#29983;&#24577;&#31995;&#32479;&#30340;&#26680;&#24515;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#30340;&#37325;&#28857;&#65292;&#20197;&#21450;&#22823;&#22810;&#25968;&#37325;&#35201;&#23454;&#29992;&#25512;&#33616;&#31995;&#32479;&#65292;&#20165;&#38480;&#20110;&#20010;&#21035;&#29992;&#25143;&#25512;&#33616;&#30340;&#23616;&#37096;&#12289;&#30701;&#35270;&#20248;&#21270;&#12290;&#36825;&#32473;&#25512;&#33616;&#31995;&#32479;&#21487;&#33021;&#20026;&#29992;&#25143;&#24102;&#26469;&#30340;&#38271;&#26399;&#25928;&#29992;&#24102;&#26469;&#20102;&#37325;&#22823;&#25104;&#26412;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#35201;&#26368;&#22823;&#21270;&#31995;&#32479;&#23545;&#36825;&#20123;&#21442;&#19982;&#32773;&#30340;&#20215;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#29983;&#24577;&#31995;&#32479;&#30340;&#8220;&#20581;&#24247;&#8221;&#29366;&#20917;&#65292;&#26377;&#24517;&#35201;&#26126;&#30830;&#22320;&#23545;&#31995;&#32479;&#20013;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#28608;&#21169;&#21644;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23545;&#20854;&#31574;&#30053;&#24341;&#21457;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#27492;&#38656;&#35201;&#65306;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31561;&#25216;&#26415;&#36827;&#34892;&#38271;&#26399;&#20248;&#21270;&#65307;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26041;&#27861;&#20026;&#19981;&#21516;&#21442;&#19982;&#32773;&#30340;&#25928;&#29992;&#36827;&#34892;&#19981;&#21487;&#36991;&#20813;&#30340;&#26435;&#34913;&#65307;&#20943;&#23569;&#20449;&#24687;&#19981;&#23545;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommender systems lie at the heart of complex ecosystems that couple the behavior of users, content providers, advertisers, and other actors. Despite this, the focus of the majority of recommender research -- and most practical recommenders of any import -- is on the local, myopic optimization of the recommendations made to individual users. This comes at a significant cost to the long-term utility that recommenders could generate for its users. We argue that explicitly modeling the incentives and behaviors of all actors in the system -- and the interactions among them induced by the recommender's policy -- is strictly necessary if one is to maximize the value the system brings to these actors and improve overall ecosystem "health". Doing so requires: optimization over long horizons using techniques such as reinforcement learning; making inevitable tradeoffs in the utility that can be generated for different actors using the methods of social choice; reducing information asymm
&lt;/p&gt;</description></item><item><title>DSLOT-NN&#26159;&#19968;&#31181;&#25968;&#23383;&#20018;&#34892;&#20174;&#24038;&#21040;&#21491;&#30340;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#65292;&#21487;&#20197;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#36816;&#31639;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#21644;&#32456;&#27490;&#26080;&#25928;&#30340;&#21367;&#31215;&#25805;&#20316;&#23454;&#29616;&#21151;&#32791;&#21644;&#33021;&#37327;&#30340;&#22823;&#35268;&#27169;&#33410;&#30465;&#12290;</title><link>http://arxiv.org/abs/2309.06019</link><description>&lt;p&gt;
DSLOT-NN: &#25968;&#23383;&#20018;&#34892;&#20174;&#24038;&#21040;&#21491;&#30340;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
DSLOT-NN: Digit-Serial Left-to-Right Neural Network Accelerator. (arXiv:2309.06019v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06019
&lt;/p&gt;
&lt;p&gt;
DSLOT-NN&#26159;&#19968;&#31181;&#25968;&#23383;&#20018;&#34892;&#20174;&#24038;&#21040;&#21491;&#30340;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#65292;&#21487;&#20197;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#36816;&#31639;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#21644;&#32456;&#27490;&#26080;&#25928;&#30340;&#21367;&#31215;&#25805;&#20316;&#23454;&#29616;&#21151;&#32791;&#21644;&#33021;&#37327;&#30340;&#22823;&#35268;&#27169;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSLOT-NN&#30340;&#22522;&#20110;&#25968;&#23383;&#20018;&#34892;&#20174;&#24038;&#21040;&#21491;&#65288;DSLOT&#65289;&#31639;&#26415;&#22788;&#29702;&#25216;&#26415;&#65292;&#26088;&#22312;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#30340;&#21367;&#31215;&#36816;&#31639;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#35780;&#20272;&#21644;&#32456;&#27490;&#26080;&#25928;&#30340;&#21367;&#31215;&#25805;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#35268;&#27169;&#30340;&#21151;&#32791;&#21644;&#33021;&#37327;&#33410;&#30465;&#12290;&#22788;&#29702;&#24341;&#25806;&#30001;&#20302;&#24310;&#36831;&#30340;&#26368;&#39640;&#26377;&#25928;&#25968;&#23383;&#20248;&#20808;&#65288;MSDF&#65289;&#65288;&#20063;&#31216;&#20026;&#22312;&#32447;&#65289;&#20056;&#27861;&#22120;&#21644;&#21152;&#27861;&#22120;&#32452;&#25104;&#65292;&#20174;&#24038;&#21040;&#21491;&#22788;&#29702;&#25968;&#25454;&#65292;&#20801;&#35768;&#21518;&#32493;&#25805;&#20316;&#20197;&#25968;&#23383;&#27969;&#27700;&#32447;&#26041;&#24335;&#25191;&#34892;&#12290;&#20351;&#29992;&#22312;&#32447;&#36816;&#31639;&#22120;&#28040;&#38500;&#20102;&#22797;&#26434;&#30340;&#36127;&#28608;&#27963;&#35782;&#21035;&#26426;&#21046;&#30340;&#24320;&#21457;&#38656;&#27714;&#65292;&#22240;&#20026;&#39318;&#20808;&#29983;&#25104;&#20855;&#26377;&#26368;&#39640;&#26435;&#37325;&#20540;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#19968;&#26086;&#29983;&#25104;&#31532;&#19968;&#20010;&#38750;&#38646;&#25968;&#23383;&#65292;&#21363;&#21487;&#35782;&#21035;&#32467;&#26524;&#30340;&#31526;&#21495;&#12290;&#22312;&#32447;&#36816;&#31639;&#22120;&#30340;&#31934;&#24230;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#36827;&#34892;&#35843;&#25972;&#65292;&#20351;&#20854;&#22312;&#21487;&#20197;&#29306;&#29298;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#26497;&#20854;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Digit-Serial Left-tO-righT (DSLOT) arithmetic based processing technique called DSLOT-NN with aim to accelerate inference of the convolution operation in the deep neural networks (DNNs). The proposed work has the ability to assess and terminate the ineffective convolutions which results in massive power and energy savings. The processing engine is comprised of low-latency most-significant-digit-first (MSDF) (also called online) multipliers and adders that processes data from left-to-right, allowing the execution of subsequent operations in digit-pipelined manner. Use of online operators eliminates the need for the development of complex mechanism of identifying the negative activation, as the output with highest weight value is generated first, and the sign of the result can be identified as soon as first non-zero digit is generated. The precision of the online operators can be tuned at run-time, making them extremely useful in situations where accuracy can be compromised 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;boosting&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00723</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#19978;&#19979;&#25991;&#20559;&#20506;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Contextual Biasing of Named-Entities with Large Language Models. (arXiv:2309.00723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;boosting&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#65292;&#21363;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#20026;LLM&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#25171;&#20998;&#26399;&#38388;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;LLM&#36827;&#34892;boosting&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20123;&#25552;&#31034;&#20449;&#24687;&#21253;&#25324;&#20559;&#20506;&#21015;&#34920;&#21644;&#23569;&#26679;&#26412;&#31034;&#20363;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#20551;&#35774;&#24471;&#20998;&#26102;&#20316;&#20026;&#38468;&#21152;&#20449;&#24687;&#12290;&#38500;&#20102;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;LLM&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#20026;&#20102;&#25552;&#39640;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#25928;&#29575;&#24182;&#36991;&#20813;&#36229;&#36807;LLMs&#30340;&#26368;&#22823;&#24207;&#21015;&#38271;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#65292;&#21363;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#39044;&#27979;&#36873;&#25321;&#26368;&#21487;&#33021;&#30340;&#31867;&#21035;&#65292;&#24182;&#20165;&#20351;&#29992;&#36825;&#20010;&#31867;&#21035;&#20013;&#30340;&#23454;&#20307;&#20316;&#20026;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#12290;&#23545;&#20869;&#37096;&#30340;&#21628;&#21483;&#12289;&#28040;&#24687;&#21644;&#21475;&#36848;&#25968;&#25454;&#38598;&#20197;&#21450;SLUE-Voxpopuli&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35789;&#38169;&#35823;&#29575;(WER)&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies contextual biasing with Large Language Models (LLMs), where during second-pass rescoring additional contextual information is provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We propose to leverage prompts for a LLM without fine tuning during rescoring which incorporate a biasing list and few-shot examples to serve as additional information when calculating the score for the hypothesis. In addition to few-shot prompt learning, we propose multi-task training of the LLM to predict both the entity class and the next token. To improve the efficiency for contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we propose dynamic prompting, where we select the most likely class using the class tag prediction, and only use entities in this class as contexts for next token prediction. Word Error Rate (WER) evaluation is performed on i) an internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli dataset. Results
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#38646;&#26679;&#26412;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#26500;&#24314;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#24615;&#33021;&#25351;&#26631;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.16775</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#39044;&#27979;&#30340;&#38646;&#26679;&#26412;NAS&#33539;&#24335;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm. (arXiv:2308.16775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16775
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#38646;&#26679;&#26412;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#26500;&#24314;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#24615;&#33021;&#25351;&#26631;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20013;&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#24471;&#21040;&#30340;&#24615;&#33021;&#25351;&#26631;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;one-hot&#32534;&#30721;&#23558;&#21069;&#39304;&#32467;&#26500;&#34920;&#31034;&#20026;&#32452;&#20214;&#22270;&#30340;&#36825;&#20123;&#25351;&#26631;&#38754;&#20020;&#19968;&#20010;&#38480;&#21046;&#65306;&#26080;&#27861;&#22312;&#19981;&#21516;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#30456;&#21453;&#65292;&#25163;&#24037;&#24615;&#33021;&#25351;&#26631;&#65288;&#38646;&#26679;&#26412;NAS&#65289;&#21487;&#20197;&#22312;&#22810;&#20010;&#25628;&#32034;&#31354;&#38388;&#20013;&#27867;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#26550;&#26500;&#21644;&#38543;&#26426;&#21021;&#22987;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;NAS&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#36827;&#34892;&#21367;&#31215;&#26680;&#30340;&#32534;&#30721;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20854;&#32467;&#26500;&#31867;&#20284;&#20110;&#27491;&#22312;&#35780;&#20272;&#30340;&#26550;&#26500;&#12290;&#36825;&#20123;&#32534;&#30721;&#26159;&#21487;&#23398;&#20064;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#26550;&#26500;&#25299;&#25169;&#20449;&#24687;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#21518;&#65292;&#20276;&#38543;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#23545;&#26550;&#26500;&#36827;&#34892;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
In prediction-based Neural Architecture Search (NAS), performance indicators derived from graph convolutional networks have shown significant success. These indicators, achieved by representing feed-forward structures as component graphs through one-hot encoding, face a limitation: their inability to evaluate architecture performance across varying search spaces. In contrast, handcrafted performance indicators (zero-shot NAS), which use the same architecture with random initialization, can generalize across multiple search spaces. Addressing this limitation, we propose a novel approach for zero-shot NAS using deep learning. Our method employs Fourier sum of sines encoding for convolutional kernels, enabling the construction of a computational feed-forward graph with a structure similar to the architecture under evaluation. These encodings are learnable and offer a comprehensive view of the architecture's topological information. An accompanying multi-layer perceptron (MLP) then ranks t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#26631;&#37327;&#21270;&#26159;&#21542;&#33021;&#22815;&#20805;&#20998;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;&#30340;&#32463;&#39564;&#20248;&#21183;&#30456;&#21453;&#65292;&#26631;&#37327;&#21270;&#26412;&#36136;&#19978;&#26080;&#27861;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#24179;&#34913;&#20102;paren</title><link>http://arxiv.org/abs/2308.13985</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#37327;&#21270;&#65306;&#19968;&#20010;&#29702;&#35770;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Revisiting Scalarization in Multi-Task Learning: A Theoretical Perspective. (arXiv:2308.13985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#26631;&#37327;&#21270;&#26159;&#21542;&#33021;&#22815;&#20805;&#20998;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;&#30340;&#32463;&#39564;&#20248;&#21183;&#30456;&#21453;&#65292;&#26631;&#37327;&#21270;&#26412;&#36136;&#19978;&#26080;&#27861;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#24179;&#34913;&#20102;paren
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26631;&#37327;&#21270;&#65292;&#21363;&#36890;&#36807;&#21152;&#26435;&#24635;&#21644;&#26469;&#32452;&#21512;&#25152;&#26377;&#25439;&#22833;&#20989;&#25968;&#65292;&#33258;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#21019;&#31435;&#20197;&#26469;&#19968;&#30452;&#26159;&#25991;&#29486;&#20013;&#30340;&#40664;&#35748;&#36873;&#25321;&#12290;&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#24320;&#21457;&#19987;&#38376;&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#22120;&#65288;SMTOs&#65289;&#26469;&#22788;&#29702;MTL&#20316;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;SMTOs&#26159;&#21542;&#27604;&#26631;&#37327;&#21270;&#26377;&#26681;&#26412;&#19978;&#30340;&#20248;&#21183;&#12290;&#23454;&#38469;&#19978;&#65292;&#31038;&#21306;&#20013;&#23384;&#22312;&#23545;&#27604;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#28608;&#28872;&#35752;&#35770;&#65292;&#20027;&#35201;&#26159;&#20174;&#32463;&#39564;&#35282;&#24230;&#20986;&#21457;&#12290;&#20026;&#20102;&#22238;&#31572;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#26631;&#37327;&#21270;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#32447;&#24615;MTL&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#26631;&#37327;&#21270;&#26159;&#21542;&#33021;&#22815;&#20805;&#20998;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#37027;&#20123;&#22768;&#31216;&#26631;&#37327;&#21270;&#20855;&#26377;&#32463;&#39564;&#20248;&#21183;&#30340;&#26368;&#36817;&#24037;&#20316;&#30456;&#21453;&#65292;&#26631;&#37327;&#21270;&#26412;&#36136;&#19978;&#26080;&#27861;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#24179;&#34913;&#20102;paren
&lt;/p&gt;
&lt;p&gt;
Linear scalarization, i.e., combining all loss functions by a weighted sum, has been the default choice in the literature of multi-task learning (MTL) since its inception. In recent years, there is a surge of interest in developing Specialized Multi-Task Optimizers (SMTOs) that treat MTL as a multi-objective optimization problem. However, it remains open whether there is a fundamental advantage of SMTOs over scalarization. In fact, heated debates exist in the community comparing these two types of algorithms, mostly from an empirical perspective. To approach the above question, in this paper, we revisit scalarization from a theoretical perspective. We focus on linear MTL models and study whether scalarization is capable of fully exploring the Pareto front. Our findings reveal that, in contrast to recent works that claimed empirical advantages of scalarization, scalarization is inherently incapable of full exploration, especially for those Pareto optimal solutions that strike the balanc
&lt;/p&gt;</description></item><item><title>&#39044;&#38376;&#25511;MoE&#31995;&#32479;&#36890;&#36807;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;MoE&#26550;&#26500;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12066</link><description>&lt;p&gt;
&#39044;&#38376;&#25511;MoE&#65306;&#24555;&#36895;&#19988;&#21487;&#25193;&#23637;&#28151;&#21512;&#19987;&#23478;&#25512;&#29702;&#30340;&#31639;&#27861;&#21644;&#31995;&#32479;&#20849;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference. (arXiv:2308.12066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12066
&lt;/p&gt;
&lt;p&gt;
&#39044;&#38376;&#25511;MoE&#31995;&#32479;&#36890;&#36807;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;MoE&#26550;&#26500;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#22522;&#20110;transformers&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20854;&#25104;&#21151;&#28304;&#20110;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#23613;&#31649;&#31639;&#27861;&#24615;&#33021;&#24456;&#39640;&#65292;&#20294;LLMs&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#19981;&#25104;&#27604;&#20363;&#22320;&#25193;&#22823;&#35745;&#31639;&#38656;&#27714;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#27169;&#22411;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;MoE&#30340;&#39640;&#23384;&#20648;&#38656;&#27714;&#21644;&#31232;&#30095;&#19987;&#23478;&#30340;&#21160;&#24577;&#28608;&#27963;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;MoE&#30340;&#20869;&#23384;&#21344;&#29992;&#39640;&#30340;&#19987;&#23478;&#21442;&#25968;&#36716;&#31227;&#21040;CPU&#20869;&#23384;&#19978;&#65292;&#20294;&#26159;&#20174;CPU&#36801;&#31227;&#24050;&#28608;&#27963;&#30340;&#19987;&#23478;&#21040;GPU&#30340;&#24310;&#36831;&#23548;&#33268;&#20102;&#39640;&#24615;&#33021;&#24320;&#38144;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#38376;&#25511;MoE&#31995;&#32479;&#36890;&#36807;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;MoE&#26550;&#26500;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2308.07706</link><description>&lt;p&gt;
&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25506;&#32034;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25972;&#21512;&#25991;&#26412;&#25351;&#23548;&#20197;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#20173;&#28982;&#26159;&#19968;&#20010;&#36827;&#23637;&#26377;&#38480;&#30340;&#39046;&#22495;&#12290;&#29616;&#26377;&#21033;&#29992;&#25991;&#26412;&#25351;&#23548;&#30340;&#20998;&#21106;&#27169;&#22411;&#20027;&#35201;&#22312;&#24320;&#25918;&#39046;&#22495;&#22270;&#20687;&#19978;&#35757;&#32451;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#30452;&#25509;&#24212;&#29992;&#30340;&#38590;&#39064;&#65292;&#38656;&#35201;&#25163;&#21160;&#20171;&#20837;&#25110;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20174;&#22270;&#20687;&#25551;&#36848;&#21644;&#22270;&#20687;&#20013;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#24471;&#33021;&#22815;&#23545;&#22810;&#26679;&#21270;&#30340;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#12290;&#35813;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#20197;&#35780;&#20272;&#20854;&#20174;&#24320;&#25918;&#39046;&#22495;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#20013;&#20197;&#21069;&#26410;&#35265;&#22270;&#20687;&#30340;&#22270;&#20687;&#25551;&#36848;&#24341;&#20837;&#20102;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.  To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations 
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;MiVOLO&#65292;&#20351;&#29992;&#26368;&#26032;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#36827;&#34892;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#23558;&#38754;&#37096;&#20449;&#24687;&#21644;&#20154;&#29289;&#22270;&#20687;&#25968;&#25454;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#22270;&#20687;&#20013;&#20154;&#33080;&#19981;&#21487;&#35265;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.04616</link><description>&lt;p&gt;
MiVOLO: &#22810;&#36755;&#20837;&#21464;&#25442;&#22120;&#29992;&#20110;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
MiVOLO: Multi-input Transformer for Age and Gender Estimation. (arXiv:2307.04616v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;MiVOLO&#65292;&#20351;&#29992;&#26368;&#26032;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#36827;&#34892;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#23558;&#38754;&#37096;&#20449;&#24687;&#21644;&#20154;&#29289;&#22270;&#20687;&#25968;&#25454;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#22270;&#20687;&#20013;&#20154;&#33080;&#19981;&#21487;&#35265;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#36827;&#34892;&#24180;&#40836;&#21644;&#24615;&#21035;&#35782;&#21035;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65306;&#38500;&#20102;&#26465;&#20214;&#30340;&#21487;&#21464;&#24615;&#12289;&#23039;&#21183;&#30340;&#22797;&#26434;&#24615;&#21644;&#22270;&#20687;&#36136;&#37327;&#30340;&#21464;&#21270;&#20043;&#22806;&#65292;&#36824;&#23384;&#22312;&#20154;&#33080;&#37096;&#20998;&#25110;&#23436;&#20840;&#36974;&#25377;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MiVOLO&#65288;&#22810;&#36755;&#20837;VOLO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#26368;&#26032;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#36827;&#34892;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20004;&#20010;&#20219;&#21153;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#21452;&#36755;&#20837;/&#36755;&#20986;&#27169;&#22411;&#20013;&#65292;&#19981;&#20165;&#21033;&#29992;&#20102;&#38754;&#37096;&#20449;&#24687;&#65292;&#36824;&#21033;&#29992;&#20102;&#20154;&#29289;&#22270;&#20687;&#25968;&#25454;&#12290;&#36825;&#25552;&#39640;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#20854;&#21363;&#20351;&#22312;&#22270;&#20687;&#20013;&#20154;&#33080;&#19981;&#21487;&#35265;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;Open Images&#25968;&#25454;&#38598;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#30495;&#23454;&#27880;&#37322;&#30001;&#35748;&#30495;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Age and gender recognition in the wild is a highly challenging task: apart from the variability of conditions, pose complexities, and varying image quality, there are cases where the face is partially or completely occluded. We present MiVOLO (Multi Input VOLO), a straightforward approach for age and gender estimation using the latest vision transformer. Our method integrates both tasks into a unified dual input/output model, leveraging not only facial information but also person image data. This improves the generalization ability of our model and enables it to deliver satisfactory results even when the face is not visible in the image. To evaluate our proposed model, we conduct experiments on four popular benchmarks and achieve state-of-the-art performance, while demonstrating real-time processing capabilities. Additionally, we introduce a novel benchmark based on images from the Open Images Dataset. The ground truth annotations for this benchmark have been meticulously generated by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.03941</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65306;&#28085;&#20041;&#12289;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions. (arXiv:2307.03941v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#26368;&#21021;&#26159;&#30001;&#35895;&#27468;&#35199;&#29677;&#29273;&#19982;&#22467;&#20811;&#26031;&#20869;&#22612;&#32034;&#22996;&#21592;&#20250;(Mario Costeja Gonz\'alez)&#20043;&#38388;&#30340;&#23448;&#21496;&#32467;&#26524;&#32780;&#30830;&#31435;&#30340;&#65292;&#24182;&#19988;&#21518;&#26469;&#34987;&#20316;&#20026;&#27431;&#27954;&#32852;&#30431;&#19968;&#33324;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#19979;&#30340;&#21024;&#38500;&#26435;&#12290;RTBF&#20801;&#35768;&#20010;&#20154;&#21521;&#32452;&#32455;&#35831;&#27714;&#21024;&#38500;&#20010;&#20154;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25628;&#32034;&#24341;&#25806;&#65292;&#20010;&#20154;&#21487;&#20197;&#21521;&#32452;&#32455;&#21457;&#36865;&#35831;&#27714;&#65292;&#25490;&#38500;&#20182;&#20204;&#30340;&#20449;&#24687;&#22312;&#26597;&#35810;&#32467;&#26524;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#20854;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;LLM&#21551;&#29992;&#30340;&#36719;&#20214;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#34987;&#25490;&#38500;&#22312;RTBF&#20043;&#22806;&#12290;&#30456;&#27604;&#25628;&#32034;&#24341;&#25806;&#20351;&#29992;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;LLMs&#20197;&#19968;&#31181;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#22788;&#29702;&#20449;&#24687;&#65292;&#36825;&#20026;&#31526;&#21512;RTBF&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#20197;&#31526;&#21512;RTBF&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Right to be Forgotten (RTBF) was first established as the result of the ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\'alez, and was later included as the Right to Erasure under the General Data Protection Regulation (GDPR) of European Union to allow individuals the right to request personal data be deleted by organizations. Specifically for search engines, individuals can send requests to organizations to exclude their information from the query results. With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF. In this paper, we explore these challenges and provide our insights on how to implement technical solutions for the RTBF, including the use of machine unle
&lt;/p&gt;</description></item><item><title>&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02484</link><description>&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02484
&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#65292;&#23427;&#26159;&#29616;&#26377;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;DT&#22768;&#31216;&#33021;&#22815;&#29983;&#25104;&#26368;&#20339;&#36712;&#36857;&#65292;&#20294;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#23427;&#22312;&#36712;&#36857;&#25340;&#25509;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36712;&#36857;&#25340;&#25509;&#26159;&#25351;&#20174;&#19968;&#32452;&#27425;&#20248;&#36712;&#36857;&#20013;&#29983;&#25104;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;&#25552;&#20986;&#30340;EDT&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;DT&#20013;&#32500;&#25252;&#30340;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#20174;&#32780;&#20351;&#33258;&#24049;&#19982;&#20247;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#36712;&#36857;&#26159;&#26368;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#38271;&#30340;&#21382;&#21490;&#65292;&#24403;&#24403;&#21069;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#30701;&#30340;&#21382;&#21490;&#26469;&#20248;&#21270;&#36712;&#36857;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#26356;&#20248;&#30340;&#36712;&#36857;&#36827;&#34892;&#8220;&#25340;&#25509;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EDT&#33021;&#22815;&#22635;&#34917;&#22522;&#20110;DT&#21644;&#22522;&#20110;Q-Learning&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#29305;&#21035;&#26159;&#65292;EDT&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25506;&#32034;&#21307;&#23398;&#30740;&#31350;&#20013;&#30340;&#8220;&#20551;&#22914;&#8221;&#22330;&#26223;&#65292;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#29305;&#23450;&#30340;&#35265;&#35299;&#65292;&#25299;&#23637;&#20102;&#25105;&#20204;&#23545;&#29616;&#26377;&#36793;&#30028;&#30340;&#29702;&#35299;&#65292;&#24182;&#22635;&#34917;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32467;&#26524;&#35299;&#37322;&#30340;&#32570;&#22833;&#12290;</title><link>http://arxiv.org/abs/2307.02131</link><description>&lt;p&gt;
&#36229;&#36234;&#24050;&#30693;&#29616;&#23454;&#65306;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#36827;&#34892;&#21307;&#23398;&#30740;&#31350; (arXiv&#65306;2307.02131v2 [cs.AI] &#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research. (arXiv:2307.02131v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25506;&#32034;&#21307;&#23398;&#30740;&#31350;&#20013;&#30340;&#8220;&#20551;&#22914;&#8221;&#22330;&#26223;&#65292;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#29305;&#23450;&#30340;&#35265;&#35299;&#65292;&#25299;&#23637;&#20102;&#25105;&#20204;&#23545;&#29616;&#26377;&#36793;&#30028;&#30340;&#29702;&#35299;&#65292;&#24182;&#22635;&#34917;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32467;&#26524;&#35299;&#37322;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25506;&#32034;&#21307;&#23398;&#30740;&#31350;&#20013;&#30340;&#8220;&#20551;&#22914;&#8221;&#22330;&#26223;&#65292;&#26088;&#22312;&#25299;&#23637;&#25105;&#20204;&#23545;&#29616;&#26377;&#36793;&#30028;&#30340;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#21033;&#29992;&#30913;&#20849;&#25391;&#25104;&#20687;&#29305;&#24449;&#26469;&#35786;&#26029;&#20799;&#31185;&#21518;&#39045;&#31389;&#33041;&#32959;&#30244;&#12290;&#20154;&#24037;&#26234;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#24050;&#32463;&#35265;&#35777;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#21644;&#23398;&#26415;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32467;&#26524;&#30340;&#20154;&#31867;&#21451;&#22909;&#35299;&#37322;&#30340;&#32570;&#20047;&#26174;&#33879;&#38459;&#30861;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#25509;&#21463;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34701;&#20837;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#20026;&#26816;&#26597;&#26367;&#20195;&#20915;&#31574;&#22330;&#26223;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#12290;&#36825;&#20123;&#35299;&#37322;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#29305;&#23450;&#30340;&#35265;&#35299;&#65292;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#39564;&#35777;&#39044;&#27979;&#24182;&#28548;&#28165;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#24046;&#24322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#20445;&#25345;&#20102;&#32479;&#35745;&#23398;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20154;&#31867;&#21487;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study employs counterfactual explanations to explore "what if?" scenarios in medical research, with the aim of expanding our understanding beyond existing boundaries. Specifically, we focus on utilizing MRI features for diagnosing pediatric posterior fossa brain tumors as a case study. The field of artificial intelligence and explainability has witnessed a growing number of studies and increasing scholarly interest. However, the lack of human-friendly interpretations in explaining the outcomes of machine learning algorithms has significantly hindered the acceptance of these methods by clinicians in their clinical practice. To address this, our approach incorporates counterfactual explanations, providing a novel way to examine alternative decision-making scenarios. These explanations offer personalized and context-specific insights, enabling the validation of predictions and clarification of variations under diverse circumstances. Importantly, our approach maintains both statistica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#21521;&#21069;&#26223;&#38598;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#23545;&#20027;&#35201;&#27010;&#24565;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01473</link><description>&lt;p&gt;
&#32531;&#35299;&#20559;&#35265;&#65306;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#35299;&#37322;&#26469;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Mitigating Bias: Enhancing Image Classification by Improving Model Explanations. (arXiv:2307.01473v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#21521;&#21069;&#26223;&#38598;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#23545;&#20027;&#35201;&#27010;&#24565;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#21644;&#27010;&#24565;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20542;&#21521;&#20110;&#36807;&#20998;&#20381;&#36182;&#20110;&#22270;&#29255;&#32972;&#26223;&#20013;&#30340;&#31616;&#21333;&#21644;&#23481;&#26131;&#35782;&#21035;&#30340;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#23427;&#20204;&#26412;&#24212;&#20998;&#31867;&#30340;&#20027;&#35201;&#27010;&#24565;&#25110;&#23545;&#35937;&#12290;&#36825;&#31181;&#29616;&#35937;&#32473;&#22270;&#20687;&#20998;&#31867;&#22120;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#29255;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#21487;&#33021;&#20250;&#34987;&#25513;&#30422;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25913;&#21892;&#22270;&#20687;&#20998;&#31867;&#22120;&#23545;&#20027;&#35201;&#27010;&#24565;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#20998;&#31867;&#20219;&#21153;&#36807;&#31243;&#20013;&#21516;&#26102;&#24341;&#23548;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#21521;&#21069;&#26223;&#38598;&#20013;&#12290;&#36890;&#36807;&#24378;&#35843;&#21069;&#26223;&#65292;&#21363;&#20027;&#35201;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#65292;&#25105;&#20204;&#26088;&#22312;&#23558;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#20174;&#32972;&#26223;&#30340;&#20027;&#23548;&#24433;&#21709;&#19978;&#36716;&#31227;&#24320;&#26469;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26426;&#21046;&#26469;&#40723;&#21169;&#27169;&#22411;&#36275;&#22815;&#22320;&#20998;&#37197;&#27880;&#24847;&#21147;&#32473;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have demonstrated remarkable capabilities in learning complex patterns and concepts from training data. However, recent findings indicate that these models tend to rely heavily on simple and easily discernible features present in the background of images rather than the main concepts or objects they are intended to classify. This phenomenon poses a challenge to image classifiers as the crucial elements of interest in images may be overshadowed. In this paper, we propose a novel approach to address this issue and improve the learning of main concepts by image classifiers. Our central idea revolves around concurrently guiding the model's attention toward the foreground during the classification task. By emphasizing the foreground, which encapsulates the primary objects of interest, we aim to shift the focus of the model away from the dominant influence of the background. To accomplish this, we introduce a mechanism that encourages the model to allocate sufficient att
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37096;&#20998;LLMs&#22312;&#29305;&#23450;&#25552;&#31034;&#37197;&#32622;&#19979;&#27169;&#25311;&#30340;&#20154;&#26684;&#21487;&#38752;&#19988;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#22609;&#36896;&#12290;</title><link>http://arxiv.org/abs/2307.00184</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;
&lt;/p&gt;
&lt;p&gt;
Personality Traits in Large Language Models. (arXiv:2307.00184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37096;&#20998;LLMs&#22312;&#29305;&#23450;&#25552;&#31034;&#37197;&#32622;&#19979;&#27169;&#25311;&#30340;&#20154;&#26684;&#21487;&#38752;&#19988;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#22609;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20351;&#24471;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#19988;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25991;&#26412;&#12290;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#39537;&#21160;&#23545;&#35805;&#20195;&#29702;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#33719;&#24471;&#30340;&#20154;&#26684;&#29305;&#36136;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#30001;&#20110;&#20154;&#26684;&#26159;&#20915;&#23450;&#20132;&#27969;&#25928;&#26524;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#39564;&#35777;&#30340;&#24515;&#29702;&#27979;&#37327;&#27979;&#35797;&#65292;&#24182;&#23545;&#20174;&#24191;&#27867;&#20351;&#29992;&#30340;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#36827;&#34892;&#37327;&#21270;&#12289;&#20998;&#26512;&#21644;&#22609;&#36896;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#26576;&#20123;LLMs&#30340;&#36755;&#20986;&#20013;&#27169;&#25311;&#30340;&#20154;&#26684;&#65288;&#22312;&#29305;&#23450;&#30340;&#25552;&#31034;&#37197;&#32622;&#19979;&#65289;&#26159;&#21487;&#38752;&#21644;&#26377;&#25928;&#30340;&#65307;2&#65289;LLM&#27169;&#25311;&#30340;&#20154;&#26684;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#35777;&#25454;&#23545;&#20110;&#26356;&#22823;&#30340;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#26356;&#24378;&#65307;3&#65289;LLM&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#30340;&#32500;&#24230;&#36827;&#34892;&#22609;&#36896;&#65292;&#20197;&#27169;&#20223;&#29305;&#23450;&#30340;&#20154;&#26684;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant text. As LLMs increasingly power conversational agents, the synthesized personality embedded in these models by virtue of their training on large amounts of human-generated data draws attention. Since personality is an important factor determining the effectiveness of communication, we present a comprehensive method for administering validated psychometric tests and quantifying, analyzing, and shaping personality traits exhibited in text generated from widely-used LLMs. We find that: 1) personality simulated in the outputs of some LLMs (under specific prompting configurations) is reliable and valid; 2) evidence of reliability and validity of LLM-simulated personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific personality profiles. 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#30340;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#36710;&#36742;&#12289;&#29615;&#22659;&#21644;&#39550;&#39542;&#21592;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12290;&#36825;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#36866;&#21512;&#30340;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#30340;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.16784</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#25968;&#25454;&#38598;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Datasets for Decision-making of Autonomous Vehicle. (arXiv:2306.16784v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#30340;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#36710;&#36742;&#12289;&#29615;&#22659;&#21644;&#39550;&#39542;&#21592;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12290;&#36825;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#36866;&#21512;&#30340;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#30340;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#26377;&#26395;&#37325;&#22609;&#26410;&#26469;&#30340;&#20132;&#36890;&#31995;&#32479;&#65292;&#32780;&#20915;&#31574;&#26159;&#23454;&#29616;&#39640;&#32423;&#33258;&#21160;&#39550;&#39542;&#30340;&#20851;&#38190;&#27169;&#22359;&#20043;&#19968;&#12290;&#20026;&#20102;&#20811;&#26381;&#37027;&#20123;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22788;&#29702;&#30340;&#22797;&#26434;&#22330;&#26223;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#26041;&#27861;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29992;&#20110;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#26497;&#22823;&#22320;&#24433;&#21709;&#20915;&#31574;&#24615;&#33021;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#20840;&#38754;&#20102;&#35299;&#29616;&#26377;&#25968;&#25454;&#38598;&#12290;&#20174;&#37319;&#38598;&#26469;&#28304;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#39550;&#39542;&#25968;&#25454;&#21487;&#20197;&#20998;&#20026;&#36710;&#36742;&#12289;&#29615;&#22659;&#21644;&#39550;&#39542;&#21592;&#30456;&#20851;&#30340;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#36825;&#19977;&#31867;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#65292;&#21253;&#25324;&#20351;&#29992;&#30340;&#20256;&#24863;&#22120;&#12289;&#27880;&#37322;&#21644;&#39550;&#39542;&#22330;&#26223;&#12290;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#29305;&#28857;&#65292;&#26412;&#32508;&#36848;&#36824;&#24635;&#32467;&#20102;&#25968;&#25454;&#38598;&#22312;AV&#20915;&#31574;&#21508;&#20010;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24110;&#21161;&#30740;&#31350;&#32773;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles (AV) are expected to reshape future transportation systems, and decision-making is one of the critical modules toward high-level automated driving. To overcome those complicated scenarios that rule-based methods could not cope with well, data-driven decision-making approaches have aroused more and more focus. The datasets to be used in developing data-driven methods dramatically influences the performance of decision-making, hence it is necessary to have a comprehensive insight into the existing datasets. From the aspects of collection sources, driving data can be divided into vehicle, environment, and driver related data. This study compares the state-of-the-art datasets of these three categories and summarizes their features including sensors used, annotation, and driving scenarios. Based on the characteristics of the datasets, this survey also concludes the potential applications of datasets on various aspects of AV decision-making, assisting researchers to find 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20110;&#21033;&#26222;&#24076;&#33576;&#21644; Sobolev &#31354;&#38388;&#20013;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#32771;&#34385;&#20013;&#24515;&#38544;&#31169;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#30452;&#26041;&#22270;&#20272;&#35745;&#22120;&#22312; L2 &#39118;&#38505;&#19979;&#23545;&#20110;&#21033;&#26222;&#24076;&#33576;&#20998;&#24067;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#22312;&#27491;&#24120;&#24046;&#20998;&#38544;&#31169;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#65307;&#21516;&#26102;&#21457;&#29616;&#65292;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#26045;&#21152;&#38544;&#31169;&#20250;&#38477;&#20302;&#23545;&#20110; Sobolev &#23494;&#24230;&#30340;&#27491;&#21017;&#26497;&#23567;&#39118;&#38505;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#21457;&#29616;&#22312;&#32431;&#25237;&#24433;&#20272;&#35745;&#35774;&#23450;&#19979;&#65292;&#25152;&#35859;&#30340;&#25237;&#24433;&#20272;&#35745;&#22120;&#23545;&#20110;&#30456;&#21516;&#31867;&#23494;&#24230;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.14535</link><description>&lt;p&gt;
&#20851;&#20110;&#20013;&#24515;&#38544;&#31169;&#22312;&#23494;&#24230;&#20272;&#35745;&#20013;&#30340;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
About the Cost of Central Privacy in Density Estimation. (arXiv:2306.14535v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20110;&#21033;&#26222;&#24076;&#33576;&#21644; Sobolev &#31354;&#38388;&#20013;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#32771;&#34385;&#20013;&#24515;&#38544;&#31169;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#30452;&#26041;&#22270;&#20272;&#35745;&#22120;&#22312; L2 &#39118;&#38505;&#19979;&#23545;&#20110;&#21033;&#26222;&#24076;&#33576;&#20998;&#24067;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#22312;&#27491;&#24120;&#24046;&#20998;&#38544;&#31169;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#65307;&#21516;&#26102;&#21457;&#29616;&#65292;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#26045;&#21152;&#38544;&#31169;&#20250;&#38477;&#20302;&#23545;&#20110; Sobolev &#23494;&#24230;&#30340;&#27491;&#21017;&#26497;&#23567;&#39118;&#38505;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#21457;&#29616;&#22312;&#32431;&#25237;&#24433;&#20272;&#35745;&#35774;&#23450;&#19979;&#65292;&#25152;&#35859;&#30340;&#25237;&#24433;&#20272;&#35745;&#22120;&#23545;&#20110;&#30456;&#21516;&#31867;&#23494;&#24230;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#21033;&#26222;&#24076;&#33576;&#21644; Sobolev &#31354;&#38388;&#20013;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#65292;&#22312;&#20013;&#24515;&#38544;&#31169;&#26465;&#20214;&#19979;&#36827;&#34892;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#38544;&#31169;&#39044;&#31639;&#19981;&#26159;&#24120;&#25968;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;&#20013;&#24515;&#24046;&#20998;&#38544;&#31169;&#23450;&#20041;&#65292;&#20197;&#21450;&#36739;&#26032;&#30340;&#20013;&#24515;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#27010;&#24565;&#12290;&#25105;&#20204;&#35777;&#23454;&#20102; Barber &amp; Duchi (2014) &#30340;&#32467;&#26524;&#65292;&#21363;&#30452;&#26041;&#22270;&#20272;&#35745;&#22120;&#22312;&#23545;&#20110; L2 &#39118;&#38505;&#19979;&#23545;&#20110;&#21033;&#26222;&#24076;&#33576;&#20998;&#24067;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#22312;&#27491;&#24120;&#24046;&#20998;&#38544;&#31169;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#65292;&#25105;&#20204;&#23558;&#20854;&#25193;&#23637;&#21040;&#20854;&#20182;&#33539;&#25968;&#21644;&#38544;&#31169;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#26356;&#39640;&#31243;&#24230;&#30340;&#20809;&#28369;&#24615;&#65292;&#24471;&#20986;&#20004;&#20010;&#32467;&#35770;&#65306;&#39318;&#20808;&#65292;&#19982;&#24120;&#25968;&#38544;&#31169;&#39044;&#31639;&#38656;&#35201;&#30340;&#24773;&#20917;&#30456;&#21453;&#65288;Wasserman &amp;amp; Zhou, 2010&#65289;&#65292;&#22312; Sobolev &#23494;&#24230;&#19978;&#26045;&#21152;&#38544;&#31169;&#20250;&#38477;&#20302;&#27491;&#21017;&#26497;&#23567;&#39118;&#38505;&#20272;&#35745;&#12290;&#20854;&#27425;&#65292;&#22312;&#36825;&#31181;&#26032;&#30340;&#32431;&#25237;&#24433;&#20272;&#35745;&#35774;&#23450;&#19979;&#65292;&#25152;&#35859;&#30340;&#25237;&#24433;&#20272;&#35745;&#22120;&#23545;&#20110;&#30456;&#21516;&#31867;&#23494;&#24230;&#26159;&#20960;&#20046;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study non-parametric density estimation for densities in Lipschitz and Sobolev spaces, and under central privacy. In particular, we investigate regimes where the privacy budget is not supposed to be constant. We consider the classical definition of central differential privacy, but also the more recent notion of central concentrated differential privacy. We recover the result of Barber \&amp; Duchi (2014) stating that histogram estimators are optimal against Lipschitz distributions for the L2 risk, and under regular differential privacy, and we extend it to other norms and notions of privacy. Then, we investigate higher degrees of smoothness, drawing two conclusions: First, and contrary to what happens with constant privacy budget (Wasserman \&amp; Zhou, 2010), there are regimes where imposing privacy degrades the regular minimax risk of estimation on Sobolev densities. Second, so-called projection estimators are near-optimal against the same classes of densities in this new setup with pure
&lt;/p&gt;</description></item><item><title>&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2306.09633</link><description>&lt;p&gt;
&#34394;&#20551;&#40654;&#26126;&#65306;&#37325;&#26032;&#35780;&#20272;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#23439;&#35266;&#24067;&#23616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09633
&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#26377;&#20851;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#33455;&#29255;&#30340;&#35770;&#25991;&#65292;&#22240;&#20026;&#25152;&#22768;&#31216;&#30340;&#32467;&#26524;&#32570;&#20047;&#20805;&#20998;&#30340;&#25991;&#20214;&#35760;&#24405;&#21644;&#20851;&#38190;&#27493;&#39588;&#30340;&#35828;&#26126;&#65292;&#24341;&#21457;&#20105;&#35758;&#24182;&#21463;&#21040;&#23186;&#20307;&#30340;&#25209;&#35780;&#25253;&#36947;&#12290; &#32780;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#22635;&#34917;&#20102;&#31354;&#30333;&#65292;&#35777;&#26126;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#33853;&#21518;&#20110;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#33853;&#21518;&#20110;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#36824;&#33853;&#21518;&#20110;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#12290;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#34892;&#20026;&#12289;&#20998;&#26512;&#21644;&#25253;&#21578;&#20013;&#30340;&#38169;&#35823;&#65292;&#35813;&#12298;&#33258;&#28982;&#12299;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#21463;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#25152;&#38656;&#30340;&#22806;&#37096;&#30693;&#35782;&#33719;&#21462;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.08129</link><description>&lt;p&gt;
AVIS:&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#35270;&#35273;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
AVIS: Autonomous Visual Information Seeking with Large Language Models. (arXiv:2306.08129v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#25152;&#38656;&#30340;&#22806;&#37096;&#30693;&#35782;&#33719;&#21462;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23454;&#29616;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#30340;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLM&#21160;&#24577;&#22320;&#21046;&#23450;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31574;&#30053;&#65292;&#24182;&#35843;&#26597;&#23427;&#20204;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#33719;&#21462;&#25552;&#20379;&#25152;&#25552;&#20986;&#38382;&#39064;&#25152;&#38656;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#30693;&#35782;&#12290;&#22238;&#31572;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#39064;&#65292;&#22914;&#8220;&#36825;&#24133;&#22270;&#20687;&#25152;&#25551;&#32472;&#30340;&#24314;&#31569;&#29289;&#26159;&#20026;&#20102;&#32426;&#24565;&#21738;&#20010;&#20107;&#20214;&#65311;&#8221;&#65292;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#21576;&#29616;&#20986;&#19968;&#20010;&#32452;&#21512;&#25628;&#32034;&#31354;&#38388;&#65292;&#38656;&#35201;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#21253;&#25324;&#35843;&#29992;API&#12289;&#20998;&#26512;&#23427;&#20204;&#30340;&#21709;&#24212;&#24182;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#25910;&#38598;&#20102;&#20154;&#31867;&#38754;&#23545;&#36825;&#20010;&#20219;&#21153;&#26102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20915;&#31574;&#23454;&#20363;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#30340;&#31995;&#32479;&#65306;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#30830;&#23450;&#19979;&#19968;&#20010;&#35201;&#20351;&#29992;&#30340;&#24037;&#20855;&#65307;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#25512;&#29702;&#22120;&#65292;&#20998;&#26512;&#24182;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as "What event is commemorated by the building depicted in this image?", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08013</link><description>&lt;p&gt;
TopP\&amp;R: &#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#25903;&#25345;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
TopP\&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;Inception Score&#65288;IS&#65289;&#65292;Fr\'echet Inception Distance&#65288;FID&#65289;&#20197;&#21450;Precision and Recall&#65288;P\&amp;R&#65289;&#30340;&#21464;&#20307;&#65292;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#26679;&#26412;&#29305;&#24449;&#20272;&#35745;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35780;&#20272;&#30340;&#36136;&#37327;&#23436;&#20840;&#21462;&#20915;&#20110;&#20854;&#21487;&#38752;&#24615;&#65292;&#20294;&#20854;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#24182;&#27809;&#26377;&#24471;&#21040;&#20005;&#32899;&#30340;&#35752;&#35770;&#65288;&#24182;&#34987;&#24573;&#35270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65288;TopP\&amp;R&#65292;&#21457;&#38899;&#20026;&#8220;topper&#8221;&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25903;&#25345;&#65292;&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#12290;&#36825;&#19981;&#20165;&#20351;TopP\&amp;R&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TopP\&amp;R&#23545;&#20110;&#31163;&#32676;&#20540;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&amp;R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&amp;R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&amp;R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&amp;R is robust to outliers and non-independent and identically distributed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#26469;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.07292</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Urban Spatiotemporal Data Synthesis via Neural Disaggregation. (arXiv:2306.07292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#26469;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#25968;&#25454;&#30340;&#32454;&#33410;&#32423;&#21035;&#24120;&#24120;&#19982;&#20854;&#25152;&#33021;&#25552;&#20379;&#30340;&#23454;&#38469;&#25928;&#30410;&#21457;&#29983;&#20914;&#31361;&#12290;&#36739;&#19981;&#32454;&#21270;&#30340;&#25968;&#25454;&#21487;&#20197;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#65292;&#20294;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#29306;&#29298;&#20102;&#24320;&#25918;&#25968;&#25454;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#21327;&#21161;&#30740;&#31350;&#30340;&#25215;&#35834;&#12290;&#31867;&#20284;&#20110;&#22478;&#24066;&#29615;&#22659;&#20013;&#65292;&#39640;&#23618;&#27425;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#21487;&#33021;&#20250;&#25513;&#30422;&#22478;&#24066;&#21160;&#24577;&#30340;&#24213;&#23618;&#29305;&#24449;&#65292;&#20302;&#32423;&#21035;&#22320;&#29702;&#21333;&#20803;&#30340;&#21464;&#21270;&#21487;&#33021;&#26356;&#20026;&#26126;&#26174;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#65292;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#19968;&#20123;&#20256;&#32479;&#20998;&#35299;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#38382;&#39064;-1) &#25105;&#20204;&#23581;&#35797;&#20102;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#29305;&#24449;&#20043;&#38388;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#31070;&#32463;&#26041;&#27861;&#20063;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The level of granularity of open data often conflicts the benefits it can provide. Less granular data can protect individual privacy, but to certain degrees, sabotage the promise of open data to promote transparency and assist research. Similar in the urban setting, aggregated urban data at high-level geographic units can mask out the underline particularities of city dynamics that may vary at lower areal levels. In this work, we aim to synthesize fine-grained, high resolution urban data, by breaking down aggregated urban data at coarse, low resolution geographic units. The goal is to increase the usability and realize the values as much as possible of highly aggregated urban data. To address the issue of simplicity of some traditional disaggregation methods -- 1) we experimented with numerous neural-based models that are capable of modeling intricate non-linear relationships among features. Neural methods can also leverage both spatial and temporal information concurrently. We showed 
&lt;/p&gt;</description></item><item><title>ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.06446</link><description>&lt;p&gt;
ShiftAddViT&#65306;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#28151;&#21512;&#23454;&#29616;&#39640;&#25928;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. (arXiv:2306.06446v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06446
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#30340;&#32479;&#19968;&#39592;&#24178;&#12290;&#20294;&#26159;&#65292;ViTs&#20013;&#30340;&#27880;&#24847;&#21147;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#30001;&#20110;&#23494;&#38598;&#30340;&#20056;&#27861;&#32780;&#19981;&#22815;&#39640;&#25928;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#20195;&#20215;&#39640;&#26114;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;ViT&#20197;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#65288;&#20363;&#22914;&#20301;&#31227;&#21644;&#21152;&#27861;&#65289;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20840;&#26032;&#31867;&#22411;&#30340;&#20943;&#23569;&#20056;&#27861;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;ShiftAddViT&#65292;&#26088;&#22312;&#23454;&#29616;GPU&#19978;&#30340;&#31471;&#21040;&#31471;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#26597;&#35810;&#21644;&#38190;&#26144;&#23556;&#20026;&#27721;&#26126;&#31354;&#38388;&#20013;&#30340;&#20108;&#36827;&#21046;&#30721;&#20043;&#21518;&#65292;&#37319;&#29992;&#21152;&#27861;&#26680;&#23545;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#20043;&#38388;&#30340;MatMul&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#21097;&#20313;&#30340;MLPs&#25110;&#32447;&#24615;&#23618;&#21017;&#37319;&#29992;&#20301;&#31227;&#26680;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;TVM&#22312;GPU&#19978;&#23454;&#26045;&#24182;&#20248;&#21270;&#36825;&#20123;&#23450;&#21046;&#26680;&#65292;&#20197;&#23454;&#29616;&#23454;&#38469;&#30828;&#20214;&#37096;&#32626;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. But both attention and multi-layer perceptions (MLPs) in ViTs are not efficient enough due to dense multiplications, resulting in costly training and inference. To this end, we propose to reparameterize the pre-trained ViT with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\textbf{ShiftAddViT}$, which aims for end-to-end inference speedups on GPUs without the need of training from scratch. Specifically, all $\texttt{MatMuls}$ among queries, keys, and values are reparameterized by additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized by shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;AI&#31574;&#23637;&#21644;&#35266;&#20247;&#20114;&#21160;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#26500;&#24819;&#20102;&#36203;&#23572;&#36763;&#22522;&#24066;&#33402;&#26415;&#21452;&#24180;&#23637;&#65292;&#20351;&#29992;&#35270;&#35273;-&#25991;&#26412;&#27169;&#22411;&#23558;&#23460;&#20869;&#33402;&#26415;&#21697;&#25918;&#32622;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;360&#33402;&#26415;&#20840;&#26223;&#22270;&#65292;&#20197;&#21019;&#36896;&#20986;&#33402;&#26415;&#21697;&#19982;&#22478;&#24066;&#31354;&#38388;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2306.03753</link><description>&lt;p&gt;
AI&#33402;&#26415;&#31574;&#23637;&#65306;&#37325;&#26032;&#26500;&#24819;&#36203;&#23572;&#36763;&#22522;&#24066;&#33402;&#26415;&#21452;&#24180;&#23637;
&lt;/p&gt;
&lt;p&gt;
AI Art Curation: Re-imagining the city of Helsinki in occasion of its Biennial. (arXiv:2306.03753v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;AI&#31574;&#23637;&#21644;&#35266;&#20247;&#20114;&#21160;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#26500;&#24819;&#20102;&#36203;&#23572;&#36763;&#22522;&#24066;&#33402;&#26415;&#21452;&#24180;&#23637;&#65292;&#20351;&#29992;&#35270;&#35273;-&#25991;&#26412;&#27169;&#22411;&#23558;&#23460;&#20869;&#33402;&#26415;&#21697;&#25918;&#32622;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;360&#33402;&#26415;&#20840;&#26223;&#22270;&#65292;&#20197;&#21019;&#36896;&#20986;&#33402;&#26415;&#21697;&#19982;&#22478;&#24066;&#31354;&#38388;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#31574;&#23637;&#23454;&#36341;&#30340;&#29305;&#28857;&#26159;&#20197;&#30693;&#35782;&#30340;&#26041;&#24335;&#23637;&#31034;&#33402;&#26415;&#25910;&#34255;&#21697;&#12290;&#26426;&#22120;&#36807;&#31243;&#30340;&#29305;&#28857;&#26159;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#21644;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#12290;&#26412;&#25991;&#35774;&#24819;&#20102;AI&#31574;&#23637;&#21644;&#35266;&#20247;&#20114;&#21160;&#65292;&#20197;&#25506;&#32034;&#24403;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#31574;&#23637;&#30028;&#30340;&#24433;&#21709;&#12290;&#35813;&#39033;&#30446;&#26159;&#20026;2023&#24180;&#36203;&#23572;&#36763;&#22522;&#33402;&#26415;&#21452;&#24180;&#23637;&#30340;&#22330;&#21512;&#32780;&#24320;&#21457;&#30340;&#65292;&#39064;&#20026;&#8220;&#21487;&#33021;&#20986;&#29616;&#26032;&#30340;&#26041;&#21521;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#36203;&#23572;&#36763;&#22522;&#33402;&#26415;&#21338;&#29289;&#39302;&#65288;HAM&#65289;&#30340;&#34255;&#21697;&#65292;&#36890;&#36807;&#26426;&#22120;&#24863;&#30693;&#30340;&#35270;&#35282;&#37325;&#26032;&#26500;&#24819;&#20102;&#36203;&#23572;&#36763;&#22522;&#24066;&#12290;&#25105;&#20204;&#20351;&#29992;&#35270;&#35273;-&#25991;&#26412;&#27169;&#22411;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#23637;&#31034;&#23460;&#20869;&#33402;&#26415;&#21697;&#65292;&#26681;&#25454;&#30456;&#20284;&#24615;&#35780;&#20998;&#20998;&#37197;&#34394;&#26500;&#30340;&#22352;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#30340;360&#33402;&#26415;&#20840;&#26223;&#22270;&#26469;&#25913;&#21464;&#27599;&#20214;&#33402;&#26415;&#21697;&#22312;&#22478;&#24066;&#20013;&#30340;&#25152;&#22788;&#31354;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#27599;&#20214;&#33402;&#26415;&#21697;&#20301;&#32622;&#30340;360&#20840;&#26223;&#22270;&#30340;&#28145;&#24230;&#20540;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#33402;&#26415;&#21697;&#25552;&#31034;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#36825;&#20010;&#39033;&#30446;&#30340;&#32467;&#26524;&#23601;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Art curatorial practice is characterized by the presentation of an art collection in a knowledgeable way. Machine processes are characterized by their capacity to manage and analyze large amounts of data. This paper envisages AI curation and audience interaction to explore the implications of contemporary machine learning models for the curatorial world. This project was developed for the occasion of the 2023 Helsinki Art Biennial, entitled New Directions May Emerge. We use the Helsinki Art Museum (HAM) collection to re-imagine the city of Helsinki through the lens of machine perception. We use visual-textual models to place indoor artworks in public spaces, assigning fictional coordinates based on similarity scores. We transform the space that each artwork inhabits in the city by generating synthetic 360 art panoramas. We guide the generation estimating depth values from 360 panoramas at each artwork location, and machine-generated prompts of the artworks. The result of this project i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.03341</link><description>&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#30495;&#23454;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30495;&#23454;&#24615;&#12290;ITI&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27839;&#30528;&#19968;&#32452;&#26041;&#21521;&#31227;&#21160;&#27169;&#22411;&#28608;&#27963;&#65292;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#36825;&#31181;&#24178;&#39044;&#26174;&#30528;&#25552;&#39640;&#20102;LLaMA&#27169;&#22411;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#25351;&#20196;&#24494;&#35843;&#30340;LLaMA Alpaca&#19978;&#65292;ITI&#23558;&#20854;&#30495;&#23454;&#24615;&#20174;32.5&#65285;&#25552;&#39640;&#21040;65.1&#65285;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30495;&#23454;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#24178;&#39044;&#24378;&#24230;&#26469;&#24179;&#34913;&#23427;&#12290;ITI &#21462;&#24471;&#20102;&#26368;&#20302;&#31243;&#24230;&#30340;&#24178;&#25200;&#19988;&#35745;&#31639;&#24265;&#20215;&#12290;&#27492;&#22806;&#65292;&#35813;&#25216;&#26415;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65306;&#34429;&#28982;&#20687;RLHF&#36825;&#26679;&#30340;&#26041;&#27861;&#38656;&#35201;&#24191;&#27867;&#27880;&#37322;&#65292;&#20294;&#26159;ITI&#20165;&#20351;&#29992;&#20102;&#20960;&#30334;&#20010;&#20363;&#23376;&#23601;&#33021;&#23450;&#20301;&#30495;&#23454;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#20855;&#26377;&#26576;&#31181;&#20869;&#37096;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#26576;&#20107;&#26159;&#30495;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20102;&#34394;&#20551;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;WCAM&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#65292;&#30830;&#23450;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#38416;&#26126;&#32553;&#25918;&#22914;&#20309;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14979</link><description>&lt;p&gt;
&#23610;&#24230;&#24456;&#37325;&#35201;&#65306;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;&#35299;&#37322;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scale Matters: Attribution Meets the Wavelet Domain to Explain Model Sensitivity to Image Corruptions. (arXiv:2305.14979v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;WCAM&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#65292;&#30830;&#23450;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#38416;&#26126;&#32553;&#25918;&#22914;&#20309;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#30001;&#20110;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#23646;&#24615;&#26041;&#27861;&#23545;&#20110;&#35299;&#37322;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#26159;&#26080;&#25928;&#30340;&#65292;&#32780;&#24378;&#20581;&#24615;&#39046;&#22495;&#30340;&#25991;&#29486;&#20165;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#20687;&#25439;&#22351;&#30340;&#24773;&#20917;&#19979;&#65292;&#23457;&#26597;&#27169;&#22411;&#30340;&#34892;&#20026;&#33021;&#21147;&#23545;&#20110;&#25552;&#39640;&#29992;&#25143;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Wavelet sCale Attribution Method (WCAM)&#65292;&#23427;&#26159;&#20174;&#20687;&#32032;&#22495;&#21040;&#31354;&#38388;&#23610;&#24230;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;&#30340;&#27010;&#25324;&#12290;&#22312;&#31354;&#38388;&#23610;&#24230;&#22495;&#20013;&#36827;&#34892;&#23646;&#24615;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#20851;&#27880;&#28857;&#21644;&#23610;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;WCAM&#35299;&#37322;&#20102;&#27169;&#22411;&#22312;&#22270;&#20687;&#30772;&#22351;&#19979;&#30340;&#22833;&#25928;&#65292;&#30830;&#23450;&#20102;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#35299;&#37322;&#20102;&#22914;&#20309;&#36890;&#36807;&#32553;&#25918;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have shown remarkable performance in computer vision, but their deployment in real-world scenarios is challenging due to their sensitivity to image corruptions. Existing attribution methods are uninformative for explaining the sensitivity to image corruptions, while the literature on robustness only provides model-based explanations. However, the ability to scrutinize models' behavior under image corruptions is crucial to increase the user's trust. Towards this end, we introduce the Wavelet sCale Attribution Method (WCAM), a generalization of attribution from the pixel domain to the space-scale domain. Attribution in the space-scale domain reveals where and on what scales the model focuses. We show that the WCAM explains models' failures under image corruptions, identifies sufficient information for prediction, and explains how zoom-in increases accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLEN&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;FLEN&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#20013;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.06104</link><description>&lt;p&gt;
N-&#20803;&#20107;&#23454;&#30340;&#23569;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Link Prediction on N-ary Facts. (arXiv:2305.06104v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLEN&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;FLEN&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#20013;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
N-&#20803;&#20107;&#23454;&#30001;&#20027;&#35201;&#19977;&#20803;&#32452;&#65288;&#22836;&#23454;&#20307;&#12289;&#20851;&#31995;&#12289;&#23614;&#23454;&#20307;&#65289;&#21644;&#20219;&#24847;&#25968;&#37327;&#30340;&#36741;&#21161;&#23646;&#24615;&#20540;&#23545;&#32452;&#25104;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#24456;&#24120;&#35265;&#12290;&#23545;&#20110;N-&#20803;&#20107;&#23454;&#30340;&#38142;&#25509;&#39044;&#27979;&#26159;&#39044;&#27979;&#20854;&#20013;&#19968;&#20010;&#20803;&#32032;&#30340;&#32570;&#22833;&#65292;&#22635;&#34917;&#32570;&#22833;&#20803;&#32032;&#26377;&#21161;&#20110;&#20016;&#23500;&#30693;&#35782;&#22270;&#35889;&#24182;&#20419;&#36827;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#29702;&#35299;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#20803;&#32032;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#24573;&#35270;&#20102;&#23569;&#26679;&#26412;&#20851;&#31995;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#21364;&#24456;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#26088;&#22312;&#20351;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#26469;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;&#25105;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;N-&#20803;&#20107;&#23454;&#30340;&#23569;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;FLEN&#65292;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#20851;&#31995;&#23398;&#20064;&#27169;&#22359;&#12289;&#25903;&#25345;&#29305;&#23450;&#35843;&#25972;&#27169;&#22359;&#21644;&#26597;&#35810;&#25512;&#29702;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
N-ary facts composed of a primary triple (head entity, relation, tail entity) and an arbitrary number of auxiliary attribute-value pairs, are prevalent in real-world knowledge graphs (KGs). Link prediction on n-ary facts is to predict a missing element in an n-ary fact. This helps populate and enrich KGs and further promotes numerous downstream applications. Previous studies usually require a substantial amount of high-quality data to understand the elements in n-ary facts. However, these studies overlook few-shot relations, which have limited labeled instances, yet are common in real-world scenarios. Thus, this paper introduces a new task, few-shot link prediction on n-ary facts. It aims to predict a missing entity in an n-ary fact with limited labeled instances. We further propose a model for Few-shot Link prEdict on N-ary facts, thus called FLEN, which consists of three modules: the relation learning, support-specific adjusting, and query inference modules. FLEN captures relation me
&lt;/p&gt;</description></item><item><title>CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.00969</link><description>&lt;p&gt;
CryCeleb: &#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00969
&lt;/p&gt;
&lt;p&gt;
CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;Ubenwa CryCeleb&#25968;&#25454;&#38598;&#8212;&#8212;&#19968;&#20010;&#26631;&#35760;&#30340;&#23156;&#20799;&#21741;&#22768;&#25910;&#38598;&#65292;&#20197;&#21450;&#38468;&#24102;&#30340;CryCeleb 2023&#20219;&#21153;&#8212;&#8212;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#20844;&#20849;&#35828;&#35805;&#20154;&#39564;&#35777;&#25361;&#25112;&#12290;&#25105;&#20204;&#37322;&#25918;&#20986;786&#21517;&#26032;&#29983;&#20799;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#20197;&#40723;&#21169;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the Ubenwa CryCeleb dataset - a labeled collection of infant cries, and the accompanying CryCeleb 2023 task - a public speaker verification challenge based on infant cry sounds. We release for academic usage more than 6 hours of manually segmented cry sounds from 786 newborns to encourage research in infant cry analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22320;&#24418;&#24863;&#30693;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#23398;&#20064;&#19982;&#39044;&#27979;&#25511;&#21046;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#21487;&#38752;&#30340; 6 &#33258;&#30001;&#24230;&#36816;&#21160;&#39044;&#27979;&#65292;&#24182;&#21487;&#22312;&#26080;&#38656;&#35757;&#32451;&#26102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#25509;&#35302;&#21147;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#36234;&#37326;&#22330;&#22320;&#30340;&#23433;&#20840;&#19982;&#40065;&#26834;&#33258;&#20027;&#39550;&#39542;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.00676</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#20027;&#36234;&#37326;&#25289;&#21147;&#36187;&#30340;&#22320;&#24418;&#24863;&#30693;&#36816;&#21160;&#23398;&#27169;&#22411;&#23398;&#20064;&#19982;&#39044;&#27979;&#25511;&#21046;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning Terrain-Aware Kinodynamic Model for Autonomous Off-Road Rally Driving With Model Predictive Path Integral Control. (arXiv:2305.00676v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22320;&#24418;&#24863;&#30693;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#23398;&#20064;&#19982;&#39044;&#27979;&#25511;&#21046;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#21487;&#38752;&#30340; 6 &#33258;&#30001;&#24230;&#36816;&#21160;&#39044;&#27979;&#65292;&#24182;&#21487;&#22312;&#26080;&#38656;&#35757;&#32451;&#26102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#25509;&#35302;&#21147;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#36234;&#37326;&#22330;&#22320;&#30340;&#23433;&#20840;&#19982;&#40065;&#26834;&#33258;&#20027;&#39550;&#39542;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36234;&#37326;&#29615;&#22659;&#19979;&#36827;&#34892;&#39640;&#36895;&#33258;&#20027;&#39550;&#39542;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#20294;&#30001;&#20110;&#36710;&#36742;&#19982;&#22320;&#24418;&#20132;&#20114;&#30340;&#22797;&#26434;&#24615;&#65292;&#20063;&#23384;&#22312;&#19968;&#23450;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#65292;&#36710;&#36742;&#39044;&#27979;&#33258;&#36523;&#36816;&#21160;&#24182;&#26681;&#25454;&#29615;&#22659;&#21464;&#21270;&#65292;&#20363;&#22914;&#22320;&#24418;&#39640;&#24046;&#30340;&#21464;&#21270;&#65292;&#20027;&#21160;&#35843;&#25972;&#20854;&#25511;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23398;&#20064;&#22320;&#24418;&#24863;&#30693;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20197;&#26412;&#20307;&#24863;&#30693;&#21644;&#22806;&#37096;&#24863;&#30693;&#20449;&#24687;&#20026;&#26465;&#20214;&#12290;&#35813;&#27169;&#22411;&#21487;&#29983;&#25104;&#21487;&#38752;&#30340; 6 &#33258;&#30001;&#24230;&#36816;&#21160;&#39044;&#27979;&#65292;&#24182;&#21487;&#22312;&#26080;&#38656;&#35757;&#32451;&#26102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#25509;&#35302;&#21147;&#20272;&#35745;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#36866;&#24403;&#30340;&#20195;&#20215;&#20989;&#25968;&#35774;&#35745;&#21487;&#20197;&#29983;&#25104;&#23433;&#20840;&#19988;&#40065;&#26834;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#24809;&#32602;&#20855;&#26377;&#19981;&#31283;&#23450;&#36816;&#21160;&#12289;&#19981;&#23433;&#20840;&#20132;&#20114;&#21644;&#39640;&#19981;&#30830;&#23450;&#24615;&#34893;&#29983;&#30340;&#26679;&#26412;&#36712;&#36857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-speed autonomous driving in off-road environments has immense potential for various applications, but it also presents challenges due to the complexity of vehicle-terrain interactions. In such environments, it is crucial for the vehicle to predict its motion and adjust its controls proactively in response to environmental changes, such as variations in terrain elevation. To this end, we propose a method for learning terrain-aware kinodynamic model which is conditioned on both proprioceptive and exteroceptive information. The proposed model generates reliable predictions of 6-degree-of-freedom motion and can even estimate contact interactions without requiring ground truth force data during training. This enables the design of a safe and robust model predictive controller through appropriate cost function design which penalizes sampled trajectories with unstable motion, unsafe interactions, and high levels of uncertainty derived from the model. We demonstrate the effectiveness of o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#24418;&#34920;&#31034;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20256;&#36882;&#31639;&#27861;&#35745;&#31639;&#26426;&#26800;&#23646;&#24615;&#20197;&#23454;&#29616;&#21453;&#21521;&#35774;&#35745;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#22810;&#26679;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.05422</link><description>&lt;p&gt;
&#21487;&#21306;&#20998;&#30340;&#22270;&#32467;&#26500;&#27169;&#22411;&#29992;&#20110;&#26230;&#26684;&#26448;&#26009;&#21453;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Differentiable graph-structured models for inverse design of lattice materials. (arXiv:2304.05422v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#24418;&#34920;&#31034;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20256;&#36882;&#31639;&#27861;&#35745;&#31639;&#26426;&#26800;&#23646;&#24615;&#20197;&#23454;&#29616;&#21453;&#21521;&#35774;&#35745;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#22810;&#26679;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#20110;&#28145;&#31354;&#24694;&#21155;&#29615;&#22659;&#20013;&#33021;&#22815;&#26681;&#25454;&#38656;&#35201;&#33258;&#36866;&#24212;&#30340;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#30340;&#26448;&#26009;&#23558;&#22312;&#23450;&#20041;&#26410;&#26469;&#30340;&#31354;&#38388;&#25506;&#32034;&#26041;&#38754;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#33258;&#28982;&#30028;&#20013;&#65292;&#24494;&#22937;&#30340;&#24494;&#35266;&#32467;&#26500;&#21644;&#26684;&#23376;&#20960;&#20309;&#24418;&#29366;&#26159;&#35774;&#35745;&#36866;&#24212;&#20110;&#29305;&#23450;&#29615;&#22659;&#26448;&#26009;&#30340;&#20196;&#20154;&#20852;&#22859;&#30340;&#28789;&#24863;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#31181;&#19981;&#35268;&#21017;&#25299;&#25169;&#35206;&#30422;&#30340;&#24040;&#22823;&#35774;&#35745;&#31354;&#38388;&#65292;&#22312;&#20998;&#26512;&#19978;&#36827;&#34892;&#25506;&#32034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#21512;&#25104;&#26230;&#26684;&#26448;&#26009;&#37117;&#26159;&#22522;&#20110;&#21608;&#26399;&#24615;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#24418;&#34920;&#31034;&#23545;&#35268;&#21017;&#21644;&#19981;&#35268;&#21017;&#26230;&#26684;&#26448;&#26009;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21487;&#24494;&#20998;&#20256;&#36882;&#31639;&#27861;&#35745;&#31639;&#21147;&#23398;&#24615;&#36136;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#26469;&#35843;&#25972;&#21333;&#20010;&#26230;&#26684;&#20803;&#32032;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#23646;&#24615;&#65292;&#20174;&#32780;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#26448;&#26009;&#12290;&#24341;&#20837;&#23545;&#26230;&#26684;&#32467;&#26500;&#21644;&#26448;&#26009;&#23646;&#24615;&#30340;&#38544;&#24335;&#21487;&#23398;&#20064;&#20960;&#20309;&#34920;&#31034;&#65292;&#32467;&#21512;&#21453;&#35774;&#35745;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#22810;&#26679;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#35774;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Materials possessing flexible physico-chemical properties that adapt on-demand to the hostile environmental conditions of deep space will become essential in defining the future of space exploration. A promising venue for inspiration towards the design of environment-specific materials is in the intricate micro-architectures and lattice geometry found throughout nature. However, the immense design space covered by such irregular topologies is challenging to probe analytically. For this reason, most synthetic lattice materials have to date been based on periodic architectures instead. Here, we propose a computational approach using a graph representation for both regular and irregular lattice materials. Our method uses differentiable message passing algorithms to calculate mechanical properties, and therefore allows using automatic differentiation to adjust both the geometric structure and attributes of individual lattice elements to design materials with desired properties. The introdu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.08983</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#22686;&#24378;&#65306;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement. (arXiv:2303.08983v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08983
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#23545;&#29992;&#25143;&#27809;&#26377;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#25968;&#25454;&#38598;&#22686;&#24378;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#31574;&#30053;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;CNN&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#30340;&#20998;&#26512;&#65292;&#20197;&#21450;&#23545;&#24102;&#26377;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#33976;&#39311;&#30740;&#31350;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;ImageDataNet+&#30340;&#22686;&#24378;&#29256;&#26412;&#65292;&#20197;&#21450;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;CIFAR-100+&#65292;Flowers-102+&#21644;Food-101+&#12290;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#20934;&#30830;&#12289;&#26356;&#26377;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#21106;&#21644;&#26816;&#27979;&#65289;&#20855;&#26377;&#24456;&#22909;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;&#22312;ImageDataNet+&#19978;&#27979;&#37327;&#30340;Expected Calibration Error&#65288;ECE&#65289;&#20063;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Dataset Reinforcement, a strategy to improve a dataset once such that the accuracy of any model architecture trained on the reinforced dataset is improved at no additional training cost for users. We propose a Dataset Reinforcement strategy based on data augmentation and knowledge distillation. Our generic strategy is designed based on extensive analysis across CNN- and transformer-based models and performing large-scale study of distillation with state-of-the-art models with various data augmentations. We create a reinforced version of the ImageNet training dataset, called ImageNet+, as well as reinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained with ImageNet+ are more accurate, robust, and calibrated, and transfer well to downstream tasks (e.g., segmentation and detection). As an example, the accuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on ImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the Ima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#65292;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#26469;&#25193;&#20805;&#25968;&#25454;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2303.06614</link><description>&lt;p&gt;
&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#65306;&#26088;&#22312;&#29992;&#25193;&#20805;&#25968;&#25454;&#26469;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Synthetic Experience Replay. (arXiv:2303.06614v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#65292;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#26469;&#25193;&#20805;&#25968;&#25454;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#30340;&#19968;&#20010;&#20851;&#38190;&#20027;&#39064;&#26159;&#65292;&#24403;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#20135;&#29983;&#20196;&#20154;&#24778;&#24322;&#30340;&#32467;&#26524;&#12290;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36825;&#31181;&#33539;&#24335;&#36890;&#24120;&#36890;&#36807;&#32463;&#39564;&#22238;&#25918;&#23454;&#29616;&#65292;&#20854;&#20013;&#36807;&#21435;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#31574;&#30053;&#25110;&#20540;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#25110;&#33258;&#30417;&#30563;&#23398;&#20064;&#19981;&#21516;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24517;&#39035;&#25910;&#38598;&#33258;&#24049;&#30340;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#26159;&#26377;&#38480;&#30340;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#22909;&#22788;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#21363;&#20351;&#26159;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#24320;&#22987;&#26102;&#20063;&#21487;&#33021;&#20986;&#29616;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#29983;&#25104;&#24314;&#27169;&#30340;&#24040;&#22823;&#36827;&#27493;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#65288;SynthER&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#26469;&#28789;&#27963;&#22320;&#19978;&#37319;&#26679;&#20195;&#29702;&#25910;&#38598;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SynthER&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#26080;&#35770;&#26159;&#22312;&#24863;&#30693;&#29615;&#22659;&#36824;&#26159;&#22312;&#20687;&#32032;&#29615;&#22659;&#20013;&#12290;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key theme in the past decade has been that when large neural networks and large datasets combine they can produce remarkable results. In deep reinforcement learning (RL), this paradigm is commonly made possible through experience replay, whereby a dataset of past experiences is used to train a policy or value function. However, unlike in supervised or self-supervised learning, an RL agent has to collect its own data, which is often limited. Thus, it is challenging to reap the benefits of deep learning, and even small neural networks can overfit at the start of training. In this work, we leverage the tremendous recent progress in generative modeling and propose Synthetic Experience Replay (SynthER), a diffusion-based approach to flexibly upsample an agent's collected experience. We show that SynthER is an effective method for training RL agents across offline and online settings, in both proprioceptive and pixel-based environments. In offline settings, we observe drastic improvements 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#24046;&#24322;&#23494;&#24230;&#30340;&#37325;&#26032;&#21152;&#26435;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28145;&#24230;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#38477;&#20302;&#31361;&#21457;&#21464;&#21270;&#24341;&#36215;&#30340;&#25439;&#22833;&#65292;&#22686;&#21152;&#27491;&#24120;&#29366;&#24577;&#24341;&#36215;&#30340;&#25439;&#22833;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2302.13563</link><description>&lt;p&gt;
&#28145;&#24230;&#19981;&#24179;&#34913;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#22522;&#20110;&#23616;&#37096;&#24046;&#24322;&#23494;&#24230;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Imbalanced Time-series Forecasting via Local Discrepancy Density. (arXiv:2302.13563v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#24046;&#24322;&#23494;&#24230;&#30340;&#37325;&#26032;&#21152;&#26435;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28145;&#24230;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#38477;&#20302;&#31361;&#21457;&#21464;&#21270;&#24341;&#36215;&#30340;&#25439;&#22833;&#65292;&#22686;&#21152;&#27491;&#24120;&#29366;&#24577;&#24341;&#36215;&#30340;&#25439;&#22833;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#20250;&#36935;&#21040;&#22312;&#26576;&#20010;&#26102;&#38388;&#27573;&#20869;&#30340;&#31361;&#21457;&#21464;&#21270;&#65292;&#36825;&#20123;&#21464;&#21270;&#36890;&#24120;&#26159;&#30001;&#24847;&#22806;&#20107;&#20214;&#25110;&#26410;&#30693;&#20107;&#20214;&#23548;&#33268;&#30340;&#12290;&#23613;&#31649;&#22312;&#35757;&#32451;&#38598;&#20013;&#21457;&#29983;&#30340;&#27425;&#25968;&#24456;&#23569;&#65292;&#20294;&#31361;&#21457;&#21464;&#21270;&#24341;&#36215;&#30340;&#25439;&#22833;&#20250;&#26174;&#33879;&#24433;&#21709;&#24635;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#20316;&#20026;&#22024;&#26434;&#30340;&#35757;&#32451;&#26679;&#26412;&#38459;&#27490;&#20102;&#27169;&#22411;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#27169;&#24335;&#65292;&#21363;&#27491;&#24120;&#29366;&#24577;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37325;&#26032;&#21152;&#26435;&#30340;&#26694;&#26550;&#65292;&#38477;&#20302;&#31361;&#21457;&#21464;&#21270;&#24341;&#36215;&#30340;&#25439;&#22833;&#65292;&#22686;&#21152;&#27491;&#24120;&#29366;&#24577;&#24341;&#36215;&#30340;&#25439;&#22833;&#12290;&#23545;&#20110;&#37325;&#26032;&#21152;&#26435;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#19968;&#20010;&#31216;&#20026;&#23616;&#37096;&#24046;&#24322;&#24230;&#65288;LD&#65289;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#32473;&#23450;&#26102;&#38388;&#27573;&#20869;&#21464;&#21270;&#30340;&#31361;&#28982;&#31243;&#24230;&#12290;&#30001;&#20110;&#35757;&#32451;&#38598;&#20027;&#35201;&#30001;&#27491;&#24120;&#29366;&#24577;&#32452;&#25104;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#22522;&#20110;LD&#22312;&#35757;&#32451;&#38598;&#20013;&#20986;&#29616;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#39057;&#29575;&#12290;&#25105;&#20204;&#30340;&#37325;&#26032;&#21152;&#26435;&#26694;&#26550;&#36866;&#29992;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26080;&#35770;&#20854;&#26550;&#26500;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series forecasting models often encounter abrupt changes in a given period of time which generally occur due to unexpected or unknown events. Despite their scarce occurrences in the training set, abrupt changes incur loss that significantly contributes to the total loss. Therefore, they act as noisy training samples and prevent the model from learning generalizable patterns, namely the normal states. Based on our findings, we propose a reweighting framework that down-weights the losses incurred by abrupt changes and up-weights those by normal states. For the reweighting framework, we first define a measurement termed Local Discrepancy (LD) which measures the degree of abruptness of a change in a given period of time. Since a training set is mostly composed of normal states, we then consider how frequently the temporal changes appear in the training set based on LD. Our reweighting framework is applicable to existing time-series forecasting models regardless of the architectures. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#29305;&#23450;&#35302;&#21457;&#22120;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26080;&#27861;&#20998;&#26512;&#26410;&#30693;&#29305;&#24449;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.10894</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#21512;&#25104;&#24037;&#20855;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32418;&#38431;&#28436;&#32451;
&lt;/p&gt;
&lt;p&gt;
Red Teaming Deep Neural Networks with Feature Synthesis Tools. (arXiv:2302.10894v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#29305;&#23450;&#35302;&#21457;&#22120;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26080;&#27861;&#20998;&#26512;&#26410;&#30693;&#29305;&#24449;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36890;&#24120;&#26088;&#22312;&#29702;&#35299;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#65288;OOD&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#23613;&#31649;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#21463;&#21040;&#20102;&#20851;&#27880;&#65292;&#20294;&#22312;&#36825;&#20123;&#24037;&#20855;&#20013;&#24456;&#23569;&#26377;&#33021;&#22815;&#21457;&#29616;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#12289;&#20197;&#21069;&#26410;&#30693;&#30340;&#38169;&#35823;&#30340;&#26696;&#20363;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#20849;&#21516;&#29305;&#28857;&#65306;&#23427;&#20204;&#20351;&#29992;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#34429;&#28982;&#36825;&#24456;&#26377;&#29992;&#65292;&#20294;&#36825;&#20123;&#24037;&#20855;&#21482;&#33021;&#20998;&#26512;&#29992;&#25143;&#21487;&#20197;&#20107;&#20808;&#37319;&#26679;&#25110;&#35782;&#21035;&#30340;&#29305;&#24449;&#25152;&#24341;&#21457;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20010;&#19981;&#26029;&#22686;&#21152;&#30340;&#30740;&#31350;&#39046;&#22495;&#28041;&#21450;&#20351;&#29992;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21512;&#25104;&#26041;&#27861;&#26469;&#35299;&#37322;&#27169;&#22411;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#29305;&#23450;&#35302;&#21457;&#22120;&#65288;&#20363;&#22914;&#65292;&#25554;&#20837;&#22270;&#20687;&#30340;&#29305;&#23450;&#34917;&#19969;&#65289;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#65288;&#21363;&#26631;&#31614;&#65289;&#65292;&#28982;&#21518;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable AI tools are often motivated by the goal of understanding model behavior in out-of-distribution (OOD) contexts. Despite the attention this area of study receives, there are comparatively few cases where these tools have identified novel, previously unknown, bugs in models. We argue that this is due, in part, to a common feature of many interpretability methods: they analyze and explain the behavior of a model using a particular dataset. While this is useful, such tools can only analyze behaviors induced by features that the user can sample or identify in advance. To address this, a growing body of research involves interpreting models using feature synthesis methods which do not depend on a dataset.  In this paper, our primary contribution is a benchmark to evaluate interpretability tools. Our key insight is that we can train models that respond to specific triggers (e.g., a specific patch inserted into an image) with specific outputs (i.e. a label) and then evaluate inte
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#30340;Tsallis KL&#25955;&#24230;&#65292;&#25193;&#23637;&#20102;Munchausen&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;Tsallis KL&#65292;&#24403;$q &gt; 1$&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26032;&#30340;&#31574;&#30053;&#20248;&#21270;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2301.11476</link><description>&lt;p&gt;
&#20351;&#29992;Tsallis KL&#25955;&#24230;&#30340;&#24191;&#20041;Munchausen&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence. (arXiv:2301.11476v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11476
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#30340;Tsallis KL&#25955;&#24230;&#65292;&#25193;&#23637;&#20102;Munchausen&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;Tsallis KL&#65292;&#24403;$q &gt; 1$&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26032;&#30340;&#31574;&#30053;&#20248;&#21270;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#37117;&#37319;&#29992;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#21040;&#19978;&#19968;&#20010;&#31574;&#30053;&#65292;&#20197;&#38450;&#27490;&#31574;&#30053;&#21464;&#21270;&#36807;&#24555;&#12290;&#36825;&#20010;&#24819;&#27861;&#26368;&#21021;&#26159;&#22312;Conservative Policy Iteration&#30340;&#19968;&#31687;&#37325;&#35201;&#35770;&#25991;&#20013;&#25552;&#20986;&#30340;&#65292;&#36817;&#20284;&#31639;&#27861;&#22914;TRPO&#21644;Munchausen Value Iteration&#65288;MVI&#65289;&#32473;&#20986;&#20102;&#26377;&#38480;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19968;&#31181;&#24191;&#20041;&#30340;KL&#25955;&#24230; - &#31216;&#20026;Tsallis KL&#25955;&#24230; - &#26469;&#32487;&#32493;&#36825;&#19968;&#24037;&#20316;&#65292;&#23427;&#22312;&#23450;&#20041;&#20013;&#20351;&#29992;&#20102;$q$-&#23545;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#19968;&#31181;&#20005;&#26684;&#30340;&#25512;&#24191;&#65292;&#22240;&#20026;$q = 1$&#23545;&#24212;&#20110;&#26631;&#20934;&#30340;KL&#25955;&#24230;&#65307;$q &gt; 1$&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#36873;&#39033;&#12290;&#25105;&#20204;&#23545;&#22312;Tsallis KL&#19979;&#23398;&#20064;&#30340;&#31574;&#30053;&#31867;&#22411;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#38416;&#36848;&#20102;&#20309;&#26102;$ q &gt; 1 $&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#23558;Tsallis KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;MVI&#65292;&#23427;&#26159;&#19968;&#31181;&#26368;&#31616;&#21333;&#30340;&#21253;&#21547;KL&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24191;&#20041;MVI&#65288;$q$&#65289;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many policy optimization approaches in reinforcement learning incorporate a Kullback-Leilbler (KL) divergence to the previous policy, to prevent the policy from changing too quickly. This idea was initially proposed in a seminal paper on Conservative Policy Iteration, with approximations given by algorithms like TRPO and Munchausen Value Iteration (MVI). We continue this line of work by investigating a generalized KL divergence -- called the Tsallis KL divergence -- which use the $q$-logarithm in the definition. The approach is a strict generalization, as $q = 1$ corresponds to the standard KL divergence; $q &gt; 1$ provides a range of new options. We characterize the types of policies learned under the Tsallis KL, and motivate when $q &gt;1$ could be beneficial. To obtain a practical algorithm that incorporates Tsallis KL regularization, we extend MVI, which is one of the simplest approaches to incorporate KL regularization. We show that this generalized MVI($q$) obtains significant improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;A-NeSI&#30340;&#26032;&#39062;PNL&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#36817;&#20284;&#25512;&#29702;&#65292;&#33021;&#22815;&#20445;&#35777;&#27010;&#29575;&#36923;&#36753;&#35821;&#20041;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;PNL&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#30340;&#28385;&#36275;&#12290;</title><link>http://arxiv.org/abs/2212.12393</link><description>&lt;p&gt;
A-NeSI: &#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#36817;&#20284;&#26041;&#27861;&#29992;&#20110;&#27010;&#29575;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference. (arXiv:2212.12393v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;A-NeSI&#30340;&#26032;&#39062;PNL&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#36817;&#20284;&#25512;&#29702;&#65292;&#33021;&#22815;&#20445;&#35777;&#27010;&#29575;&#36923;&#36753;&#35821;&#20041;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;PNL&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#30340;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#25512;&#29702;&#30456;&#32467;&#21512;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#27010;&#29575;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#65288;PNL&#65289;&#26694;&#26550;&#65292;&#22914;DeepProbLog&#65292;&#25191;&#34892;&#25351;&#25968;&#26102;&#38388;&#30340;&#31934;&#30830;&#25512;&#29702;&#65292;&#38480;&#21046;&#20102;PNL&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36817;&#20284;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#65288;A-NeSI&#65289;&#65306;&#19968;&#31181;&#26032;&#30340;PNL&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#36817;&#20284;&#25512;&#29702;&#12290;A-NeSI 1) &#22312;&#19981;&#25913;&#21464;&#27010;&#29575;&#36923;&#36753;&#35821;&#20041;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#22810;&#39033;&#24335;&#26102;&#38388;&#25191;&#34892;&#36817;&#20284;&#25512;&#29702;&#65307;2) &#20351;&#29992;&#30001;&#32972;&#26223;&#30693;&#35782;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65307;3) &#21487;&#20197;&#29983;&#25104;&#26377;&#20851;&#39044;&#27979;&#30340;&#31526;&#21495;&#35299;&#37322;&#65307;4) &#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#38388;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#30340;&#28385;&#36275;&#65292;&#36825;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;A-NeSI&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#20855;&#26377;&#25351;&#25968;&#32452;&#21512;&#25193;&#23637;&#30340;&#19977;&#31181;&#31070;&#32463;&#31526;&#21495;&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;A-NeSI&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#32780;&#27809;&#26377;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#21457;&#29616;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#22823;&#20844;&#21496;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#21464;&#21270;&#12290;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#35821;&#26009;&#24211;&#22823;&#23567;&#12289;&#36136;&#37327;&#21644;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2212.08390</link><description>&lt;p&gt;
&#20174;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#24471;&#20986;&#30340;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Lessons learned from the evaluation of Spanish Language Models. (arXiv:2212.08390v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08390
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#21457;&#29616;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#22823;&#20844;&#21496;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#21464;&#21270;&#12290;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#35821;&#26009;&#24211;&#22823;&#23567;&#12289;&#36136;&#37327;&#21644;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24433;&#21709;&#65292;&#24050;&#32463;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;&#19968;&#20123;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#35199;&#29677;&#29273;&#35821;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;BERT&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#35201;&#20040;&#26159;&#22312;&#20351;&#29992;&#38750;&#24120;&#22823;&#30340;&#31169;&#26377;&#35821;&#26009;&#24211;&#30340;&#22823;&#22411;&#39033;&#30446;&#20013;&#24320;&#21457;&#30340;&#65292;&#35201;&#20040;&#26159;&#36890;&#36807;&#21033;&#29992;&#20813;&#36153;&#21487;&#29992;&#25968;&#25454;&#30340;&#23567;&#35268;&#27169;&#23398;&#26415;&#24037;&#20316;&#24320;&#21457;&#30340;&#12290;&#26412;&#25991;&#23545;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27604;&#36739;&#65292;&#24471;&#20986;&#20197;&#19979;&#32467;&#26524;&#65306;&#65288;i&#65289;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#22823;&#20844;&#21496;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#21464;&#21270;&#65307;&#65288;ii&#65289;&#21333;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26524;&#24182;&#19981;&#26126;&#30830;&#65292;&#25454;&#35828;&#26356;&#23567;&#19988;&#26356;&#24046;&#30340;&#27169;&#22411;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#23454;&#35777;&#32467;&#26524;&#65292;&#25105;&#20204;&#20027;&#24352;&#38656;&#35201;&#26356;&#22810;&#30340;&#30740;&#31350;&#26469;&#29702;&#35299;&#20854;&#32972;&#21518;&#30340;&#22240;&#32032;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#35821;&#26009;&#24211;&#30340;&#22823;&#23567;&#12289;&#36136;&#37327;&#21644;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#24433;&#21709;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the impact of language models on the field of Natural Language Processing, a number of Spanish encoder-only masked language models (aka BERTs) have been trained and released. These models were developed either within large projects using very large private corpora or by means of smaller scale academic efforts leveraging freely available data. In this paper we present a comprehensive head-to-head comparison of language models for Spanish with the following results: (i) Previously ignored multilingual models from large companies fare better than monolingual models, substantially changing the evaluation landscape of language models in Spanish; (ii) Results across the monolingual models are not conclusive, with supposedly smaller and inferior models performing competitively. Based on these empirical results, we argue for the need of more research to understand the factors underlying them. In this sense, the effect of corpus size, quality and pre-training techniques need to be further
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#32852;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32858;&#31867;&#22120;&#65292;&#21487;&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#65292;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32597;&#35265;&#21644;&#26410;&#30693;&#29289;&#20307;&#31867;&#21035;&#19978;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.11679</link><description>&lt;p&gt;
&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#30340;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mean Shift Mask Transformer for Unseen Object Instance Segmentation. (arXiv:2211.11679v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#32852;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32858;&#31867;&#22120;&#65292;&#21487;&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#65292;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32597;&#35265;&#21644;&#26410;&#30693;&#29289;&#20307;&#31867;&#21035;&#19978;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#23454;&#20363;&#30340;&#20998;&#21106;&#26159;&#26426;&#22120;&#20154;&#38656;&#35201;&#25484;&#25569;&#30340;&#20851;&#38190;&#24863;&#30693;&#25216;&#33021;&#20043;&#19968;&#65292;&#23427;&#26377;&#21161;&#20110;&#26426;&#22120;&#20154;&#25235;&#21462;&#21644;&#25805;&#20316;&#26410;&#30693;&#29289;&#20307;&#12290;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#31639;&#27861;&#19981;&#21487;&#24494;&#20998;&#65292;&#20351;&#20854;&#38590;&#20197;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;&#65288;MSMFormer&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#20307;&#31995;&#32467;&#26500;&#65292;&#27169;&#25311; von Mises-Fisher&#65288;vMF&#65289;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#31639;&#27861;&#65292;&#20801;&#35768;&#32852;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32858;&#31867;&#22120;&#12290;&#20854;&#26680;&#24515;&#32452;&#20214;&#26159;&#36229;&#29699;&#38754;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#22312;&#36229;&#29699;&#38754;&#19978;&#26356;&#26032;&#29289;&#20307;&#26597;&#35810;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;MSMFormer&#24212;&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MSMFormer&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#32597;&#35265;&#21644;&#26410;&#30693;&#29289;&#20307;&#31867;&#21035;&#19978;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmenting unseen objects from images is a critical perception skill that a robot needs to acquire. In robot manipulation, it can facilitate a robot to grasp and manipulate unseen objects. Mean shift clustering is a widely used method for image segmentation tasks. However, the traditional mean shift clustering algorithm is not differentiable, making it difficult to integrate it into an end-to-end neural network training framework. In this work, we propose the Mean Shift Mask Transformer (MSMFormer), a new transformer architecture that simulates the von Mises-Fisher (vMF) mean shift clustering algorithm, allowing for the joint training and inference of both the feature extractor and the clustering. Its central component is a hypersphere attention mechanism, which updates object queries on a hypersphere. To illustrate the effectiveness of our method, we apply MSMFormer to unseen object instance segmentation. Our experiments show that MSMFormer achieves competitive performance compared to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21338;&#24328;&#22343;&#34913;&#27010;&#24565;&#8212;&#8212;&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913;&#65292;&#22312;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#21487;&#20197;&#24110;&#21161;&#21019;&#24314;&#30495;&#27491;&#30340;&#31574;&#30053;&#20195;&#29702;&#12290;&#36825;&#31181;&#22343;&#34913;&#27010;&#24565;&#22312;&#20844;&#24320;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.16506</link><description>&lt;p&gt;
&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913; (Observable Perfect Equilibrium)
&lt;/p&gt;
&lt;p&gt;
Observable Perfect Equilibrium. (arXiv:2210.16506v5 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21338;&#24328;&#22343;&#34913;&#27010;&#24565;&#8212;&#8212;&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913;&#65292;&#22312;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#21487;&#20197;&#24110;&#21161;&#21019;&#24314;&#30495;&#27491;&#30340;&#31574;&#30053;&#20195;&#29702;&#12290;&#36825;&#31181;&#22343;&#34913;&#27010;&#24565;&#22312;&#20844;&#24320;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32435;&#20160;&#22343;&#34913;&#25104;&#20026;&#20102;&#21338;&#24328;&#35770;&#30340;&#26680;&#24515;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#65292;&#35768;&#22810;&#37325;&#35201;&#30340;&#21338;&#24328;&#21253;&#21547;&#22810;&#20010;&#32435;&#20160;&#22343;&#34913;&#65292;&#25105;&#20204;&#24517;&#39035;&#30830;&#23450;&#22914;&#20309;&#22312;&#20854;&#20013;&#36873;&#25321;&#65292;&#20197;&#21019;&#24314;&#30495;&#27491;&#30340;&#31574;&#30053;&#20195;&#29702;&#12290;&#20026;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#25552;&#20986;&#20102;&#20960;&#20010;&#32435;&#20160;&#22343;&#34913;&#32454;&#21270;&#27010;&#24565;&#65292;&#20854;&#20013;&#26368;&#31361;&#20986;&#30340;&#26159;&#39076;&#25238;&#25163;&#23436;&#32654;&#22343;&#34913;&#12289;&#25311;&#23436;&#32654;&#22343;&#34913;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#21333;&#20391;&#25311;&#23436;&#32654;&#22343;&#34913;&#12290;&#36825;&#20123;&#27010;&#24565;&#23545;&#26576;&#20123;&#20219;&#24847;&#23567;&#30340;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#20445;&#35777;&#22987;&#32456;&#23384;&#22312;&#12290;&#20294;&#25105;&#20204;&#35748;&#20026;&#65292;&#23545;&#20110;&#21457;&#23637;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#24378;&#22823;&#30340;&#20195;&#29702;&#20154;&#65292;&#36825;&#20123;&#27010;&#24565;&#37117;&#19981;&#27491;&#30830;&#12290;&#25105;&#20204;&#20026;&#28216;&#25103;&#26641;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#34913;&#27010;&#24565;&#8212;&#8212;&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913;&#65292;&#22312;&#20854;&#20013;&#65292;&#35299;&#20915;&#26041;&#26696;&#22312;&#20844;&#24320;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#65288;&#24182;&#19981;&#19968;&#23450;&#38024;&#23545;&#25152;&#26377;&#21487;&#33021;&#19981;&#21487;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#20855;&#26377;&#40065;&#26834;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Nash equilibrium has emerged as the central game-theoretic solution concept, many important games contain several Nash equilibria and we must determine how to select between them in order to create real strategic agents. Several Nash equilibrium refinement concepts have been proposed and studied for sequential imperfect-information games, the most prominent being trembling-hand perfect equilibrium, quasi-perfect equilibrium, and recently one-sided quasi-perfect equilibrium. These concepts are robust to certain arbitrarily small mistakes, and are guaranteed to always exist; however, we argue that neither of these is the correct concept for developing strong agents in sequential games of imperfect information. We define a new equilibrium refinement concept for extensive-form games called observable perfect equilibrium in which the solution is robust over trembles in publicly-observable action probabilities (not necessarily over all action probabilities that may not be observable by
&lt;/p&gt;</description></item><item><title>FALCON&#26159;&#19968;&#20010;&#22522;&#20110;ALC&#26412;&#20307;&#30340;&#27169;&#31946;&#31070;&#32463;&#25512;&#29702;&#22120;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#20010;&#27169;&#22411;&#32467;&#26500;&#35745;&#31639;&#24544;&#23454;&#30340;&#35821;&#20041;&#34164;&#28085;&#65292;&#36171;&#20104;&#31070;&#32463;&#32593;&#32476;&#19990;&#30028;&#27169;&#22411;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#23454;&#29616;&#20102;&#36817;&#20284;&#25512;&#29702;&#12289;&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;ALC&#34920;&#36798;&#30340;&#30693;&#35782;&#25913;&#36827;&#20102;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2208.07628</link><description>&lt;p&gt;
FALCON&#65306;&#22522;&#20110;ALC&#26412;&#20307;&#30340;&#24544;&#23454;&#31070;&#32463;&#35821;&#20041;&#34164;&#28085;
&lt;/p&gt;
&lt;p&gt;
FALCON: Faithful Neural Semantic Entailment over ALC Ontologies. (arXiv:2208.07628v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07628
&lt;/p&gt;
&lt;p&gt;
FALCON&#26159;&#19968;&#20010;&#22522;&#20110;ALC&#26412;&#20307;&#30340;&#27169;&#31946;&#31070;&#32463;&#25512;&#29702;&#22120;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#20010;&#27169;&#22411;&#32467;&#26500;&#35745;&#31639;&#24544;&#23454;&#30340;&#35821;&#20041;&#34164;&#28085;&#65292;&#36171;&#20104;&#31070;&#32463;&#32593;&#32476;&#19990;&#30028;&#27169;&#22411;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#23454;&#29616;&#20102;&#36817;&#20284;&#25512;&#29702;&#12289;&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;ALC&#34920;&#36798;&#30340;&#30693;&#35782;&#25913;&#36827;&#20102;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26412;&#20307;&#65292;&#21363;&#25551;&#36848;&#36923;&#36753;&#65288;DL&#65289;&#30693;&#35782;&#24211;&#65292;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#25552;&#20379;&#20851;&#20110;&#21508;&#31181;&#39046;&#22495;&#30340;&#20016;&#23500;&#30693;&#35782;&#65292;&#24182;&#19988;&#20854;&#20013;&#24456;&#22810;&#22522;&#20110;ALC&#65292;&#21363;&#21407;&#22411;&#21644;&#34920;&#36798;&#21147;&#24378;&#30340;DL&#65292;&#25110;&#20854;&#25193;&#23637;&#12290;&#25506;&#32034;ALC&#26412;&#20307;&#30340;&#20027;&#35201;&#20219;&#21153;&#26159;&#35745;&#31639;&#35821;&#20041;&#34164;&#28085;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;FALCON&#65292;&#19968;&#20010;&#27169;&#31946;&#30340;ALC&#26412;&#20307;&#31070;&#32463;&#25512;&#29702;&#22120;&#65292;&#23427;&#20351;&#29992;&#27169;&#31946;&#36923;&#36753;&#36816;&#31639;&#31526;&#20026;&#20219;&#24847;ALC&#26412;&#20307;&#29983;&#25104;&#27169;&#22411;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#32467;&#26500;&#35745;&#31639;&#24544;&#23454;&#30340;&#35821;&#20041;&#34164;&#28085;&#12290;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;FALCON&#33021;&#22815;&#24544;&#23454;&#22320;&#36817;&#20284;&#35745;&#31639;ALC&#26412;&#20307;&#19978;&#30340;&#35821;&#20041;&#34164;&#28085;&#65292;&#20174;&#32780;&#36171;&#20104;&#31070;&#32463;&#32593;&#32476;&#19990;&#30028;&#27169;&#22411;&#21644;&#23545;&#20854;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FALCON&#33021;&#22815;&#23454;&#29616;&#36817;&#20284;&#25512;&#29702;&#65292;&#36741;&#21161;&#25512;&#29702;&#65288;&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#25512;&#29702;&#65289;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#20197;ALC&#34920;&#36798;&#30340;&#30693;&#35782;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many ontologies, i.e., Description Logic (DL) knowledge bases, have been developed to provide rich knowledge about various domains, and a lot of them are based on ALC, i.e., a prototypical and expressive DL, or its extensions. The main task that explores ALC ontologies is to compute semantic entailment. We developed FALCON, a Fuzzy ALC Ontology Neural reasoner, which uses fuzzy logic operators to generate model structures for arbitrary ALC ontologies, and uses multiple model structures to compute faithful semantic entailments. Theoretical results show that FALCON faithfully approximates semantic entailment over ALC ontologies and therefore endows neural networks with world models and the ability to reason over them. Experimental results show that FALCON enables approximate reasoning, paraconsistent reasoning (reasoning with inconsistencies), and improves machine learning in the biomedical domain by incorporating knowledge expressed in ALC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#32422;&#26463;&#32534;&#31243;&#21644;&#37327;&#23376;&#36864;&#28779;&#26041;&#27861;&#22312;&#20248;&#21270;&#26426;&#36710;&#32534;&#32452;&#20998;&#37197;&#19982;&#32500;&#25252;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#20004;&#31181;&#26041;&#27861;&#22312;&#24403;&#21069;&#21457;&#23637;&#38454;&#27573;&#30340;&#37327;&#23376;&#36864;&#28779;&#26426;&#22120;&#19978;&#20135;&#29983;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2109.07212</link><description>&lt;p&gt;
&#20248;&#21270;&#21253;&#25324;&#32500;&#25252;&#22312;&#20869;&#30340;&#26426;&#36710;&#32534;&#32452;&#35745;&#21010;&#30340;&#32422;&#26463;&#32534;&#31243;&#19982;&#37327;&#23376;&#36864;&#28779;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimising Rolling Stock Planning including Maintenance with Constraint Programming and Quantum Annealing. (arXiv:2109.07212v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.07212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#32422;&#26463;&#32534;&#31243;&#21644;&#37327;&#23376;&#36864;&#28779;&#26041;&#27861;&#22312;&#20248;&#21270;&#26426;&#36710;&#32534;&#32452;&#20998;&#37197;&#19982;&#32500;&#25252;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#20004;&#31181;&#26041;&#27861;&#22312;&#24403;&#21069;&#21457;&#23637;&#38454;&#27573;&#30340;&#37327;&#23376;&#36864;&#28779;&#26426;&#22120;&#19978;&#20135;&#29983;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#20351;&#29992;&#32422;&#26463;&#32534;&#31243;(CP)&#21644;&#37327;&#23376;&#36864;&#28779;(QA)&#26041;&#27861;&#26469;&#20248;&#21270;&#32771;&#34385;&#24517;&#35201;&#32500;&#25252;&#20219;&#21153;&#30340;&#26426;&#36710;&#32534;&#32452;&#20998;&#37197;&#12290;&#22312;CP&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;AllDifferent&#32422;&#26463;&#12289;Element&#32422;&#26463;&#30340;&#25193;&#23637;&#20197;&#21450;&#36923;&#36753;&#34164;&#21547;&#31561;&#26469;&#24314;&#27169;&#38382;&#39064;&#12290;&#23545;&#20110;QA&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;(QUBO)&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#24503;&#22269;&#38081;&#36335;&#30495;&#23454;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;D-Wave&#30340;&#30495;&#23454;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36816;&#34892;QA&#26041;&#27861;&#12290;&#32463;&#20856;&#35745;&#31639;&#26426;&#29992;&#20110;&#35780;&#20272;CP&#26041;&#27861;&#20197;&#21450;QUBO&#27169;&#22411;&#20013;&#30340;&#31105;&#24524;&#25628;&#32034;&#12290;&#22312;&#24403;&#21069;&#29289;&#29702;&#37327;&#23376;&#36864;&#28779;&#26426;&#22120;&#30340;&#24320;&#21457;&#38454;&#27573;&#65292;&#25105;&#20204;&#21457;&#29616;&#20004;&#31181;&#26041;&#27861;&#24448;&#24448;&#20135;&#29983;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose and compare Constraint Programming (CP) and Quantum Annealing (QA) approaches for rolling stock assignment optimisation considering necessary maintenance tasks. In the CP approach, we model the problem with an Alldifferent constraint, extensions of the Element constraint, and logical implications, among others. For the QA approach, we develop a quadratic unconstrained binary optimisation (QUBO) model. For evaluation, we use data sets based on real data from Deutsche Bahn and run the QA approach on real quantum computers from D-Wave. Classical computers are used to evaluate the CP approach as well as tabu search for the QUBO model. At the current development stage of the physical quantum annealers, we find that both approaches tend to produce comparable results.
&lt;/p&gt;</description></item></channel></rss>