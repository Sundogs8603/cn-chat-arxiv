<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>TEDDY&#26159;&#19968;&#31181;&#21033;&#29992;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#24615;&#25805;&#20316;&#23454;&#29616;&#36793;&#32536;&#31232;&#30095;&#21270;&#65292;&#36827;&#32780;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;&#36825;&#26159;&#19968;&#20010;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01261</link><description>&lt;p&gt;
TEDDY: &#22522;&#20110;&#24230;&#37327;&#21028;&#21035;&#31574;&#30053;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TEDDY: Trimming Edges with Degree-based Discrimination strategY
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01261
&lt;/p&gt;
&lt;p&gt;
TEDDY&#26159;&#19968;&#31181;&#21033;&#29992;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#24615;&#25805;&#20316;&#23454;&#29616;&#36793;&#32536;&#31232;&#30095;&#21270;&#65292;&#36827;&#32780;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;&#36825;&#26159;&#19968;&#20010;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Chen&#31561;&#20154;&#22312;2021&#24180;&#25552;&#20986;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#20197;&#26469;&#65292;&#23547;&#25214;&#22270;&#25277;&#22870;&#31080;&#65288;GLT&#65289;&#30340;&#30740;&#31350;&#24050;&#25104;&#20026;GNN&#31038;&#21306;&#30340;&#37325;&#35201;&#20851;&#27880;&#28857;&#20043;&#19968;&#65292;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#23454;&#29616;&#19982;&#21407;&#22987;&#23494;&#38598;&#32593;&#32476;&#30456;&#24403;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#26356;&#31232;&#30095;&#30340;GLT&#12290;&#21516;&#26102;&#65292;&#22270;&#32467;&#26500;&#20316;&#20026;GNN&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#20063;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#24471;&#21040;&#20102;&#26368;&#36817;&#20960;&#39033;&#30740;&#31350;&#30340;&#38416;&#26126;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#20851;&#20110;GLT&#30340;&#30740;&#31350;&#36890;&#24120;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22270;&#32467;&#26500;&#20013;&#30340;&#20869;&#22312;&#36335;&#24452;&#65292;&#24182;&#20197;&#36845;&#20195;&#26041;&#24335;&#35782;&#21035;&#31080;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#32791;&#26102;&#19988;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;TEDDY&#65292;&#19968;&#31181;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#24182;&#25972;&#21512;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#19968;&#27425;&#24615;&#36793;&#32536;&#31232;&#30095;&#21270;&#26694;&#26550;&#12290;&#22312;&#36827;&#34892;&#36793;&#32536;&#31232;&#30095;&#21270;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks. In parallel, the graph structure has gained substantial attention as a crucial factor in GNN training dynamics, also elucidated by several recent studies. Despite this, contemporary studies on GLT, in general, have not fully exploited inherent pathways in the graph structure and identified tickets in an iterative manner, which is time-consuming and inefficient. To address these limitations, we introduce TEDDY, a one-shot edge sparsification framework that leverages structural information by incorporating edge-degree information. Following edge sparsification, we encourage the parameter sparsity during training via simple projected gradient desc
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#21035;&#20013;&#65292;&#25945;&#24072;&#24378;&#21046;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#31532;&#19968;&#26102;&#38388;&#23398;&#20064;&#21040;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#19968;&#33324;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.06963</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
The pitfalls of next-token prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06963
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#21035;&#20013;&#65292;&#25945;&#24072;&#24378;&#21046;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#31532;&#19968;&#26102;&#38388;&#23398;&#20064;&#21040;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#19968;&#33324;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#20851;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#25285;&#24551;&#65306;&#19968;&#20010;&#20165;&#20165;&#22522;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#24544;&#23454;&#22320;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#32463;&#24120;&#28151;&#28102;&#30340;&#20004;&#20010;&#38454;&#27573; -- &#33258;&#22238;&#24402;&#25512;&#26029;&#21644;&#25945;&#24072;&#24378;&#21046;&#35757;&#32451; -- &#24517;&#39035;&#34987;&#21306;&#21035;&#23545;&#24453;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#19968;&#33324;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#25945;&#24072;&#24378;&#21046;&#22914;&#20309;&#22833;&#36133;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#35745;&#21010;&#20219;&#21153;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;Transformer&#21644;Mamba&#26550;&#26500;&#22312;&#23454;&#36341;&#20013;&#20197;&#36825;&#31181;&#26041;&#24335;&#22833;&#36133; -- &#23613;&#31649;&#20219;&#21153;&#26412;&#36523;&#24456;&#23481;&#26131;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06963v1 Announce Type: cross  Abstract: Can a mere next-token predictor faithfully model human intelligence? We crystallize this intuitive concern, which is fragmented in the literature. As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn. We provide preliminary
&lt;/p&gt;</description></item><item><title>SELMA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#22810;&#25216;&#33021;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#25216;&#33021;&#29305;&#23450;&#19987;&#23478;&#23398;&#20064;&#21644;&#21512;&#24182;&#65292;&#20174;&#32780;&#25913;&#36827;T2I&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.06952</link><description>&lt;p&gt;
SELMA&#65306;&#23398;&#20064;&#21644;&#21512;&#24182;&#20855;&#26377;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#30340;&#25216;&#33021;&#29305;&#23450;&#25991;&#26412;&#21040;&#22270;&#20687;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06952
&lt;/p&gt;
&lt;p&gt;
SELMA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#22810;&#25216;&#33021;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#25216;&#33021;&#29305;&#23450;&#19987;&#23478;&#23398;&#20064;&#21644;&#21512;&#24182;&#65292;&#20174;&#32780;&#25913;&#36827;T2I&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#23637;&#31034;&#20102;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21019;&#24314;&#22270;&#20687;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;T2I&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#31934;&#30830;&#21305;&#37197;&#25991;&#26412;&#36755;&#20837;&#32454;&#33410;&#30340;&#22270;&#20687;&#26041;&#38754;&#32463;&#24120;&#34920;&#29616;&#19981;&#20339;&#65292;&#27604;&#22914;&#19981;&#27491;&#30830;&#30340;&#31354;&#38388;&#20851;&#31995;&#25110;&#32570;&#22833;&#23545;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SELMA&#65306;&#20855;&#26377;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#30340;&#25216;&#33021;&#29305;&#23450;&#19987;&#23478;&#23398;&#20064;&#21644;&#21512;&#24182;&#65292;&#36825;&#26159;&#19968;&#31181;&#25913;&#36827;T2I&#27169;&#22411;&#24544;&#23454;&#24230;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#22810;&#25216;&#33021;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#25216;&#33021;&#29305;&#23450;&#19987;&#23478;&#23398;&#20064;&#21644;&#21512;&#24182;&#12290;&#39318;&#20808;&#65292;SELMA&#21033;&#29992;LLM&#30340;&#29615;&#22659;&#23398;&#20064;&#33021;&#21147;&#29983;&#25104;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#25945;&#25480;&#19981;&#21516;&#30340;&#25216;&#33021;&#65292;&#28982;&#21518;&#22522;&#20110;&#25552;&#31034;&#20351;&#29992;T2I&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#12290;&#25509;&#19979;&#26469;&#65292;SELMA&#36890;&#36807;&#23398;&#20064;&#22810;&#20010;&#21333;&#25216;&#33021;&#30340;LoRA&#65288;&#20302;&#31209;&#35843;&#25972;&#65289;&#19987;&#23478;&#35843;&#25972;T2I&#27169;&#22411;&#21040;&#26032;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06952v1 Announce Type: cross  Abstract: Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLM's in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts follow
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#20219;&#21153;CFKGR&#65292;&#26412;&#25991;&#23558;&#30693;&#35782;&#22270;&#34917;&#20840;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#32852;&#31995;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#20551;&#35774;&#21069;&#25552;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;COULDD&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#34920;&#26126;KGEs&#21487;&#20197;&#23398;&#20064;&#22270;&#20013;&#30340;&#27169;&#24335;&#65292;&#26816;&#27979;&#20986;&#21512;&#29702;&#30340;&#21453;&#20107;&#23454;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.06936</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#22270;&#23884;&#20837;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Reasoning with Knowledge Graph Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06936
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#20219;&#21153;CFKGR&#65292;&#26412;&#25991;&#23558;&#30693;&#35782;&#22270;&#34917;&#20840;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#32852;&#31995;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#20551;&#35774;&#21069;&#25552;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;COULDD&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#34920;&#26126;KGEs&#21487;&#20197;&#23398;&#20064;&#22270;&#20013;&#30340;&#27169;&#24335;&#65292;&#26816;&#27979;&#20986;&#21512;&#29702;&#30340;&#21453;&#20107;&#23454;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGEs&#65289;&#26368;&#21021;&#26159;&#20026;&#20102;&#25512;&#26029;&#19981;&#23436;&#25972;&#30693;&#35782;&#24211;&#20013;&#32570;&#22833;&#30340;&#30495;&#23454;&#20107;&#23454;&#32780;&#24320;&#21457;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#20219;&#21153;CFKGR&#23558;&#30693;&#35782;&#22270;&#34917;&#20840;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#23558;&#21407;&#22987;&#19990;&#30028;&#29366;&#24577;&#24314;&#27169;&#20026;&#30693;&#35782;&#22270;&#65292;&#20551;&#35774;&#24773;&#26223;&#20026;&#28155;&#21152;&#21040;&#22270;&#20013;&#30340;&#36793;&#65292;&#23545;&#22270;&#30340;&#21512;&#29702;&#21464;&#21270;&#20026;&#36923;&#36753;&#35268;&#21017;&#25512;&#29702;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#30456;&#24212;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#20551;&#35774;&#24773;&#26223;&#21450;&#23545;&#21407;&#22987;&#30693;&#35782;&#22270;&#30340;&#21512;&#29702;&#25913;&#21464;&#20197;&#21450;&#24212;&#35813;&#20445;&#30041;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;COULDD&#65292;&#19968;&#31181;&#38024;&#23545;&#29305;&#23450;&#20551;&#35774;&#21069;&#25552;&#35843;&#25972;&#29616;&#26377;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;KGEs&#21487;&#20197;&#22312;&#27809;&#26377;&#26174;&#24335;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20174;&#22270;&#20013;&#23398;&#20064;&#27169;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#65292;&#36890;&#36807;COULDD&#35843;&#25972;&#21518;&#30340;KGEs&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#20986;&#36981;&#24490;&#36923;&#36753;&#35268;&#21017;&#30340;&#22270;&#30340;&#21512;&#29702;&#21453;&#20107;&#23454;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06936v1 Announce Type: cross  Abstract: Knowledge graph embeddings (KGEs) were originally developed to infer true but missing facts in incomplete knowledge repositories. In this paper, we link knowledge graph completion and counterfactual reasoning via our new task CFKGR. We model the original world state as a knowledge graph, hypothetical scenarios as edges added to the graph, and plausible changes to the graph as inferences from logical rules. We create corresponding benchmark datasets, which contain diverse hypothetical scenarios with plausible changes to the original knowledge graph and facts that should be retained. We develop COULDD, a general method for adapting existing knowledge graph embeddings given a hypothetical premise, and evaluate it on our benchmark. Our results indicate that KGEs learn patterns in the graph without explicit training. We further observe that KGEs adapted with COULDD solidly detect plausible counterfactual changes to the graph that follow the
&lt;/p&gt;</description></item><item><title>Transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#20855;&#26377;&#20302;&#25935;&#24863;&#24615;&#65292;&#36825;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#26377;&#21161;&#20110;&#35299;&#37322;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06925</link><description>&lt;p&gt;
Transformers&#23398;&#20064;&#20302;&#25935;&#24863;&#24615;&#20989;&#25968;&#30340;&#31616;&#21333;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Simplicity Bias of Transformers to Learn Low Sensitivity Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06925
&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#20855;&#26377;&#20302;&#25935;&#24863;&#24615;&#65292;&#36825;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#26377;&#21161;&#20110;&#35299;&#37322;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20294;&#23545;&#23427;&#20204;&#20855;&#26377;&#30340;&#24402;&#32435;&#20559;&#24046;&#20197;&#21450;&#36825;&#20123;&#20559;&#24046;&#22914;&#20309;&#19982;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19981;&#21516;&#30340;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#23545;&#36755;&#20837;&#20013;&#30340;&#38543;&#26426;&#26356;&#25913;&#30340;&#25935;&#24863;&#24615;&#27010;&#24565;&#21270;&#20026;&#19968;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#30340;&#27010;&#24565;&#65292;&#36825;&#20026;&#35299;&#37322;transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#30340;&#31616;&#21333;&#24615;&#21644;&#35889;&#20559;&#24046;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;transformers&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#27604;&#20854;&#20182;&#26367;&#20195;&#26550;&#26500;&#65288;&#22914;LSTMs&#12289;MLPs&#21644;CNNs&#65289;&#20855;&#26377;&#26356;&#20302;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20302;&#25935;&#24863;&#24615;&#20559;&#24046;&#19982;&#25913;&#36827;&#24615;&#33021;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06925v1 Announce Type: cross  Abstract: Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of the inductive biases that they have and how those biases are different from other neural network architectures remains elusive. Various neural network architectures such as fully connected networks have been found to have a simplicity bias towards simple functions of the data; one version of this simplicity bias is a spectral bias to learn simple functions in the Fourier space. In this work, we identify the notion of sensitivity of the model to random changes in the input as a notion of simplicity bias which provides a unified metric to explain the simplicity and spectral bias of transformers across different data modalities. We show that transformers have lower sensitivity than alternative architectures, such as LSTMs, MLPs and CNNs, across both vision and language tasks. We also show that low-sensitivity bias correlates with impro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Meta dEmonstratioN Distillation (MEND)&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25552;&#39640;MEND&#21644;LLM&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.06914</link><description>&lt;p&gt;
MEND&#65306;&#20803;&#28436;&#31034;&#33976;&#39311;&#29992;&#20110;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06914
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Meta dEmonstratioN Distillation (MEND)&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#25552;&#39640;MEND&#21644;LLM&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#33021;&#21147;&#65292;&#20854;&#20013;LLM&#20026;&#32473;&#23450;&#30340;&#27979;&#35797;&#36755;&#20837;&#21644;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#23545;(&#28436;&#31034;)&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#28436;&#31034;&#30340;&#21152;&#20837;&#23548;&#33268;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#24320;&#38144;&#21576;&#20108;&#27425;&#22686;&#21152;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#23581;&#35797;&#23558;&#20887;&#38271;&#30340;&#28436;&#31034;&#33976;&#39311;&#25104;&#32039;&#20945;&#30340;&#21521;&#37327;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#29306;&#29298;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta dEmonstratioN Distillation (MEND)&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#23558;&#20219;&#20309;&#20887;&#38271;&#28436;&#31034;&#33976;&#39311;&#20026;&#21521;&#37327;&#65292;&#32780;&#26080;&#38656;&#20026;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#22686;&#24378;MEND&#21644;LLM&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;MEND&#20855;&#26377;&#33976;&#39311;&#28436;&#31034;&#30340;&#20803;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06914v1 Announce Type: cross  Abstract: Large Language models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities, where a LLM makes predictions for a given test input together with a few input-output pairs (demonstrations). Nevertheless, the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the self-attention mechanism. Existing solutions attempt to distill lengthy demonstrations into compact vectors. However, they often require task-specific retraining or compromise LLM's in-context learning performance. To mitigate these challenges, we present Meta dEmonstratioN Distillation (MEND), where a language model learns to distill any lengthy demonstrations into vectors without retraining for a new downstream task. We exploit the knowledge distillation to enhance alignment between MEND and LLM, achieving both efficiency and effectiveness simultaneously. MEND is endowed with the meta-knowledge of distilling demonstrat
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#38754;&#32479;&#19968;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;&#38416;&#26126;&#20102;&#24403;&#21069;&#23545;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#29702;&#35299;&#65292;&#26088;&#22312;&#24110;&#21161;&#31435;&#27861;&#32773;&#21644;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#22312;AI&#30417;&#31649;&#39046;&#22495;&#20570;&#20986;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.06910</link><description>&lt;p&gt;
&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Responsible Artificial Intelligence: A Structured Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06910
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#38754;&#32479;&#19968;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;&#38416;&#26126;&#20102;&#24403;&#21069;&#23545;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#29702;&#35299;&#65292;&#26088;&#22312;&#24110;&#21161;&#31435;&#27861;&#32773;&#21644;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#22312;AI&#30417;&#31649;&#39046;&#22495;&#20570;&#20986;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06910v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#25105;&#20204;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25512;&#36827;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#27010;&#24565;&#65292;&#22312;&#27431;&#30431;&#25919;&#31574;&#35752;&#35770;&#20013;&#26085;&#30410;&#37325;&#35201;&#12290;&#27431;&#30431;&#26368;&#36817;&#21457;&#24067;&#20102;&#20960;&#20221;&#24378;&#35843;AI&#20449;&#20219;&#24517;&#35201;&#24615;&#30340;&#20986;&#29256;&#29289;&#65292;&#24378;&#35843;AI&#20316;&#20026;&#26377;&#30410;&#24037;&#20855;&#21644;&#28508;&#22312;&#27494;&#22120;&#30340;&#21452;&#37325;&#24615;&#36136;&#12290;&#36825;&#31181;&#20108;&#20803;&#24615;&#31361;&#26174;&#20102;&#22269;&#38469;&#30417;&#31649;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#38656;&#35201;&#25351;&#23548;&#20844;&#21496;&#22312;AI&#21457;&#23637;&#20013;&#36981;&#23432;&#36825;&#20123;&#35268;&#23450;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#24110;&#21161;&#31435;&#27861;&#32773;&#21644;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#22312;AI&#30417;&#31649;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#23548;&#33322;&#65292;&#30830;&#23450;&#26410;&#26469;&#20851;&#27880;&#30340;&#28966;&#28857;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#36127;&#36131;&#20219;AI&#30340;&#20840;&#38754;&#19988;&#25454;&#25105;&#20204;&#25152;&#30693;&#31532;&#19968;&#20010;&#32479;&#19968;&#23450;&#20041;&#12290;&#36890;&#36807;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#23545;&#36127;&#36131;&#20219;AI&#30340;&#24403;&#21069;&#29702;&#35299;&#12290;&#20511;&#37492;&#36825;&#19968;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06910v1 Announce Type: new  Abstract: Our research endeavors to advance the concept of responsible artificial intelligence (AI), a topic of increasing importance within EU policy discussions. The EU has recently issued several publications emphasizing the necessity of trust in AI, underscoring the dual nature of AI as both a beneficial tool and a potential weapon. This dichotomy highlights the urgent need for international regulation. Concurrently, there is a need for frameworks that guide companies in AI development, ensuring compliance with such regulations. Our research aims to assist lawmakers and machine learning practitioners in navigating the evolving landscape of AI regulation, identifying focal areas for future attention. This paper introduces a comprehensive and, to our knowledge, the first unified definition of responsible AI. Through a structured literature review, we elucidate the current understanding of responsible AI. Drawing from this analysis, we propose an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#12289;&#24182;&#21457;&#39044;&#27979;&#21644;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.06906</link><description>&lt;p&gt;
&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22312;&#32771;&#34385;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#25512;&#36831;&#22810;&#20301;&#19987;&#23478;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06906
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#12289;&#24182;&#21457;&#39044;&#27979;&#21644;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25512;&#36831;&#65288;L2D&#65289;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#31995;&#32479;&#20013;&#23558;&#20915;&#31574;&#25512;&#36831;&#32473;&#20154;&#31867;&#65292;&#20174;&#32780;&#22312;&#20154;&#31867;&#26356;&#26377;&#21487;&#33021;&#27491;&#30830;&#26102;&#25512;&#36831;&#20915;&#31574;&#12290;&#29616;&#26377;L2D&#30740;&#31350;&#24573;&#35270;&#20102;&#38459;&#30861;&#20854;&#23454;&#38469;&#37319;&#29992;&#30340;&#30495;&#23454;&#31995;&#32479;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21363;&#65306;&#24573;&#35270;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#65292;&#20854;&#20013;&#31532;1&#31867;&#21644;&#31532;2&#31867;&#38169;&#35823;&#30340;&#25104;&#26412;&#19981;&#21516;&#65307;&#35201;&#27714;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#24182;&#21457;&#20154;&#31867;&#39044;&#27979;&#65307;&#19981;&#22788;&#29702;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#12290;DeCCaF&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;L2D&#26041;&#27861;&#65292;&#37319;&#29992;&#30417;&#30563;&#23398;&#20064;&#26469;&#24314;&#27169;&#20154;&#31867;&#38169;&#35823;&#30340;&#27010;&#29575;&#65292;&#20943;&#23569;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#65292;&#24182;&#20351;&#29992;&#32422;&#26463;&#32534;&#31243;&#26469;&#20840;&#23616;&#26368;&#23567;&#21270;&#38169;&#35823;&#25104;&#26412;&#65292;&#21516;&#26102;&#32771;&#34385;&#24037;&#20316;&#37327;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#31995;&#21015;&#20013;&#27979;&#35797;&#20102;DeCCaF
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06906v1 Announce Type: cross  Abstract: Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier. Existing research in L2D overlooks key aspects of real-world systems that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type 1 and type 2 errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset and iii) not dealing with human work capacity constraints. To address these issues, we propose the deferral under cost and capacity constraints framework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised learning to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost subject to workload limitations. We test DeCCaF in a series 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#29289;&#21147;&#23398;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;LIBR+&#26041;&#27861;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26415;&#20013;&#32925;&#33039;&#37197;&#20934;&#30340;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#26415;&#20013;&#27979;&#37327;&#30340;&#31232;&#30095;&#24615;&#21644;&#21464;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06901</link><description>&lt;p&gt;
LIBR+: &#36890;&#36807;&#23398;&#20064;&#22522;&#20110;&#29983;&#29289;&#21147;&#23398;&#21464;&#24418;&#30340;&#24046;&#20540;&#26469;&#25913;&#36827;&#26415;&#20013;&#32925;&#33039;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
LIBR+: Improving Intraoperative Liver Registration by Learning the Residual of Biomechanics-Based Deformable Registration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06901
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#29289;&#21147;&#23398;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;LIBR+&#26041;&#27861;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26415;&#20013;&#32925;&#33039;&#37197;&#20934;&#30340;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#26415;&#20013;&#27979;&#37327;&#30340;&#31232;&#30095;&#24615;&#21644;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#29615;&#22659;&#23545;&#26415;&#20013;&#22120;&#23448;&#24418;&#29366;&#19982;&#26415;&#21069;&#25104;&#20687;&#20960;&#20309;&#24418;&#29366;&#30340;&#37197;&#20934;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#29983;&#29289;&#21147;&#23398;&#27169;&#22411;&#30340;&#37197;&#20934;&#20173;&#28982;&#24456;&#27969;&#34892;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#21463;&#21040;&#26415;&#20013;&#27979;&#37327;&#30340;&#31232;&#30095;&#24615;&#21644;&#21464;&#24322;&#24615;&#20197;&#21450;&#25163;&#26415;&#36807;&#31243;&#20013;&#21487;&#20197;&#33719;&#24471;&#30340;&#26377;&#38480;&#22120;&#23448;&#22320;&#38754;&#30495;&#23454;&#21464;&#24418;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#28151;&#21512;&#8221;&#37197;&#20934;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#32447;&#24615;&#24377;&#24615;&#29983;&#29289;&#21147;&#23398;&#30340;&#32447;&#24615;&#21270;&#36845;&#20195;&#36793;&#30028;&#37325;&#24314;&#65288;LIBR&#65289;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20854;&#27531;&#24046;&#20197;&#33719;&#21462;&#22320;&#38754;&#30495;&#23454;&#21464;&#24418;&#65288;LIBR+&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21046;&#23450;&#20102;&#19968;&#20010;&#21452;&#20998;&#25903;&#26679;&#26465;&#27531;&#24046;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;SR-GCN&#65289;&#26469;&#21560;&#25910;&#31232;&#30095;&#21644;&#21464;&#37327;&#26415;&#20013;&#27979;&#37327;&#30340;&#20449;&#24687;&#65292;&#24182;&#26377;&#25928;&#22320;&#36890;&#36807;3D&#22120;&#23448;&#30340;&#20960;&#20309;&#24418;&#29366;&#20256;&#25773;&#12290;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06901v1 Announce Type: cross  Abstract: The surgical environment imposes unique challenges to the intraoperative registration of organ shapes to their preoperatively-imaged geometry. Biomechanical model-based registration remains popular, while deep learning solutions remain limited due to the sparsity and variability of intraoperative measurements and the limited ground-truth deformation of an organ that can be obtained during the surgery. In this paper, we propose a novel \textit{hybrid} registration approach that leverage a linearized iterative boundary reconstruction (LIBR) method based on linear elastic biomechanics, and use deep neural networks to learn its residual to the ground-truth deformation (LIBR+). We further formulate a dual-branch spline-residual graph convolutional neural network (SR-GCN) to assimilate information from sparse and variable intraoperative measurements and effectively propagate it through the geometry of the 3D organ. Experiments on a large int
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#22914;&#20309;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#65292;&#29305;&#21035;&#26159;&#21457;&#29616;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#31232;&#30095;&#36716;&#23494;&#38598;&#65288;S2D&#65289;&#36716;&#25442;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06880</link><description>&lt;p&gt;
&#25581;&#31034;&#21463;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#22312;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06880
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#22914;&#20309;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#65292;&#29305;&#21035;&#26159;&#21457;&#29616;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#31232;&#30095;&#36716;&#23494;&#38598;&#65288;S2D&#65289;&#36716;&#25442;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24188;&#20799;&#20174;&#31232;&#30095;&#21453;&#39304;&#30340;&#33258;&#30001;&#25506;&#32034;&#36880;&#28176;&#21457;&#23637;&#20026;&#21033;&#29992;&#20808;&#21069;&#32463;&#39564;&#36827;&#34892;&#20197;&#30446;&#26631;&#20026;&#23548;&#21521;&#30340;&#23398;&#20064;&#65292;&#33719;&#24471;&#26356;&#23494;&#38598;&#22870;&#21169;&#12290;&#21463;&#27492;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#19981;&#21516;&#22870;&#21169;&#36716;&#25442;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#30340;&#24847;&#20041;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#20174;&#31232;&#30095;&#21040;&#22522;&#20110;&#28508;&#22312;&#30340;&#23494;&#38598;&#22870;&#21169;&#30340;&#36716;&#25442;&#65292;&#36825;&#20004;&#32773;&#20849;&#20139;&#26080;&#35770;&#22870;&#21169;&#21464;&#21270;&#22343;&#20026;&#26368;&#20339;&#31574;&#30053;&#12290;&#36890;&#36807;&#21253;&#25324;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#23548;&#33322;&#21644;&#26426;&#26800;&#33218;&#25805;&#20316;&#20219;&#21153;&#22312;&#20869;&#30340;&#21508;&#31181;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#24403;&#30340;&#22870;&#21169;&#36716;&#25442;&#26174;&#33879;&#24433;&#21709;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#12290;&#29305;&#21035;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#21463;&#24188;&#20799;&#21551;&#21457;&#30340;&#31232;&#30095;&#36716;&#23494;&#38598;&#65288;S2D&#65289;&#36716;&#25442;&#30340;&#26377;&#25928;&#24615;&#12290;&#38500;&#20102;&#36825;&#20123;&#24615;&#33021;&#25351;&#26631;&#22806;&#65292;&#20351;&#29992;&#20132;&#21449;&#23494;&#24230;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36716;&#25442;&#65292;&#29305;&#21035;&#26159;S2D&#65292;&#20351;&#31574;&#30053;&#25439;&#22833;&#26223;&#35266;&#26356;&#21152;&#24179;&#28369;&#65292;&#20419;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06880v1 Announce Type: cross  Abstract: Toddlers evolve from free exploration with sparse feedback to exploiting prior experiences for goal-directed learning with denser rewards. Drawing inspiration from this Toddler-Inspired Reward Transition, we set out to explore the implications of varying reward transitions when incorporated into Reinforcement Learning (RL) tasks. Central to our inquiry is the transition from sparse to potential-based dense rewards, which share optimal strategies regardless of reward changes. Through various experiments, including those in egocentric navigation and robotic arm manipulation tasks, we found that proper reward transitions significantly influence sample efficiency and success rates. Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense (S2D) transition. Beyond these performance metrics, using Cross-Density Visualizer technique, we observed that transitions, especially the S2D, smooth the policy loss landscape, promoting
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;MESc&#26694;&#26550;&#25506;&#32034;&#22823;&#22411;&#27861;&#24459;&#25991;&#20214;&#30340;&#20998;&#31867;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#25991;&#20214;&#37096;&#20998;&#30340;&#23884;&#20837;&#24182;&#20351;&#29992;&#32858;&#31867;&#36817;&#20284;&#32467;&#26500;&#65292;&#36827;&#32780;&#39044;&#27979;&#21028;&#20915;&#12290;</title><link>https://arxiv.org/abs/2403.06872</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20998;&#23618;&#26694;&#26550;&#29992;&#20110;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#27861;&#24459;&#25991;&#20214;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06872
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;MESc&#26694;&#26550;&#25506;&#32034;&#22823;&#22411;&#27861;&#24459;&#25991;&#20214;&#30340;&#20998;&#31867;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#25991;&#20214;&#37096;&#20998;&#30340;&#23884;&#20837;&#24182;&#20351;&#29992;&#32858;&#31867;&#36817;&#20284;&#32467;&#26500;&#65292;&#36827;&#32780;&#39044;&#27979;&#21028;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#21463;&#38271;&#36798;&#25968;&#19975;&#23383;&#30340;&#26696;&#20363;&#25991;&#20214;&#21644;&#38750;&#22343;&#21248;&#32467;&#26500;&#30340;&#38382;&#39064;&#22256;&#25200;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#27809;&#26377;&#32467;&#26500;&#26631;&#27880;&#30340;&#25991;&#20214;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#23618;&#26694;&#26550;(MESc)&#65292;&#21363;&#8220;&#22522;&#20110;&#22810;&#38454;&#27573;&#32534;&#30721;&#22120;&#30340;&#24102;&#32858;&#31867;&#30340;&#30417;&#30563;&#23398;&#20064;&#8221;&#65292;&#26469;&#25506;&#32034;&#36825;&#20123;&#22823;&#22411;&#27861;&#24459;&#25991;&#20214;&#30340;&#20998;&#31867;&#21644;&#23427;&#20204;&#32570;&#20047;&#32467;&#26500;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;&#29992;&#20110;&#21028;&#20915;&#39044;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25991;&#20214;&#20998;&#25104;&#37096;&#20998;&#65292;&#20174;&#33258;&#23450;&#20041;&#31934;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#21518;&#22235;&#23618;&#20013;&#25552;&#21462;&#23427;&#20204;&#30340;&#23884;&#20837;&#65292;&#24182;&#23581;&#35797;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#26469;&#36817;&#20284;&#23427;&#20204;&#30340;&#32467;&#26500;&#12290;&#28982;&#21518;&#22312;&#21478;&#19968;&#32452;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#23618;&#20013;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#26469;&#23398;&#20064;&#37096;&#20998;&#38388;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36866;&#24212;&#24615;(GPT-Neo)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06872v1 Announce Type: cross  Abstract: Legal judgment prediction suffers from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents becomes a challenging task, more so on documents with no structural annotation. We explore the classification of these large legal documents and their lack of structural information with a deep-learning-based hierarchical framework which we call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We analyze the adaptability of Large Language Models (LLMs) with multi-billion parameters (GPT-Neo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.06869</link><description>&lt;p&gt;
&#22312;&#26377;&#22122;&#22768;&#22522;&#30784;&#27169;&#22411;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#35843;&#25972;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#24448;&#24448;&#26080;&#27861;&#33719;&#21462;&#25110;&#25104;&#26412;&#36807;&#39640;&#65292;&#21487;&#33021;&#21253;&#21547;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36896;&#25104;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#24102;&#26469;&#24847;&#24819;&#19981;&#21040;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#26159;&#39318;&#20010;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22122;&#22768;&#24615;&#36136;&#65292;&#24182;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#26377;&#22122;&#22768;&#30340;ImageNet-1K&#12289;YFCC15M&#21644;CC12M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23436;&#20840;&#30417;&#30563;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#20351;&#21516;&#39046;&#22495;&#65288;ID&#65289;&#24615;&#33021;&#21463;&#30410;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20849;&#20139;&#31867;&#20284;&#20998;&#24067;&#65292;&#20294;&#23427;&#24635;&#26159;&#20250;&#30772;&#22351;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#24615;&#33021;&#65292;&#22312;&#37027;&#37324;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#26126;&#26174;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06869v1 Announce Type: cross  Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are signific
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23454;&#38469;&#25968;&#25454;&#20013;&#35782;&#21035;&#26032;&#29983;&#20799;&#20998;&#23081;&#20107;&#20214;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#26088;&#22312;&#35774;&#35745;&#20986;&#19968;&#27454;&#29992;&#25143;&#21451;&#22909;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#25552;&#39640;&#23545;&#39640;&#39118;&#38505;&#24739;&#32773;&#30340;&#35782;&#21035;&#29575;&#21644;&#24178;&#39044;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.06843</link><description>&lt;p&gt;
&#38754;&#21521;&#25903;&#25345;&#26032;&#29983;&#20799;&#31185;&#21307;&#29983;&#30340;&#20998;&#23081;&#23460;&#25945;&#32946;&#24037;&#20855;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards an educational tool for supporting neonatologists in the delivery room
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06843
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23454;&#38469;&#25968;&#25454;&#20013;&#35782;&#21035;&#26032;&#29983;&#20799;&#20998;&#23081;&#20107;&#20214;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#26088;&#22312;&#35774;&#35745;&#20986;&#19968;&#27454;&#29992;&#25143;&#21451;&#22909;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#25552;&#39640;&#23545;&#39640;&#39118;&#38505;&#24739;&#32773;&#30340;&#35782;&#21035;&#29575;&#21644;&#24178;&#39044;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#26377;&#35777;&#25454;&#34920;&#26126;&#20960;&#20010;&#22240;&#32032;&#21487;&#33021;&#22686;&#21152;&#23156;&#20799;&#22312;&#20986;&#29983;&#26102;&#38656;&#35201;&#31283;&#23450;&#25110;&#22797;&#33487;&#25805;&#20316;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39118;&#38505;&#22240;&#32032;&#23578;&#26410;&#23436;&#20840;&#30693;&#26195;&#65292;&#24182;&#19988;&#30446;&#21069;&#23578;&#26080;&#36866;&#29992;&#20110;&#39044;&#27979;&#39640;&#39118;&#38505;&#24773;&#20917;&#30340;&#26222;&#36941;&#27169;&#22411;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#38480;&#21046;&#21644;&#20986;&#29983;&#26102;&#38656;&#35201;&#36827;&#34892;&#22797;&#33487;&#30340;&#38656;&#27714;&#30456;&#23545;&#32597;&#35265;&#65292;&#26377;&#24517;&#35201;&#23545;&#36127;&#36131;&#20998;&#23081;&#23460;&#26032;&#29983;&#20799;&#25252;&#29702;&#30340;&#21307;&#25252;&#20154;&#21592;&#36827;&#34892;&#23450;&#26399;&#22521;&#35757;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23454;&#38469;&#25968;&#25454;&#20013;&#35782;&#21035;&#39118;&#38505;&#22240;&#32032;&#21450;&#20854;&#23545;&#20998;&#23081;&#20107;&#20214;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#20197;&#24110;&#21161;&#20154;&#21592;&#36880;&#27493;&#22686;&#21152;&#21644;&#26356;&#26032;&#20182;&#20204;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#33021;&#22815;&#25552;&#39640;&#39640;&#39118;&#38505;&#24739;&#32773;&#30340;&#35782;&#21035;&#29575;&#21644;&#35268;&#21010;&#36866;&#24403;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06843v1 Announce Type: new  Abstract: Nowadays, there is evidence that several factors may increase the risk, for an infant, to require stabilisation or resuscitation manoeuvres at birth. However, this risk factors are not completely known, and a universally applicable model for predicting high-risk situations is not available yet. Considering both these limitations and the fact that the need for resuscitation at birth is a rare event, periodic training of the healthcare personnel responsible for newborn caring in the delivery room is mandatory.   In this paper, we propose a machine learning approach for identifying risk factors and their impact on the birth event from real data, which can be used by personnel to progressively increase and update their knowledge. Our final goal will be the one of designing a user-friendly mobile application, able to improve the recognition rate and the planning of the appropriate interventions on high-risk patients.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36845;&#20195;&#33258;&#21453;&#39304;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#22312;&#25351;&#23450;&#20219;&#21153;&#30340;&#29305;&#23450;&#22330;&#26223;&#20013;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.06840</link><description>&lt;p&gt;
RA-ISF: &#36890;&#36807;&#36845;&#20195;&#33258;&#21453;&#39304;&#23398;&#20064;&#26816;&#32034;&#22686;&#24378;&#20197;&#22238;&#31572;&#21644;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06840
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#33258;&#21453;&#39304;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#22312;&#25351;&#23450;&#20219;&#21153;&#30340;&#29305;&#23450;&#22330;&#26223;&#20013;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#23384;&#20648;&#22312;&#20854;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26816;&#32034;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#30693;&#35782;&#26469;&#22238;&#31572;&#20197;&#21069;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26816;&#32034;&#22686;&#24378;&#36845;&#20195;&#33258;&#21453;&#39304;(RA-ISF)&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#23376;&#27169;&#22359;&#36845;&#20195;&#20998;&#35299;&#20219;&#21153;&#24182;&#22788;&#29702;&#23427;&#20204;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#22312;&#35832;&#22914;GPT3.5&#12289;Llama2&#20043;&#31867;&#30340;&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06840v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate exceptional performance in numerous tasks but still heavily rely on knowledge stored in their parameters. Moreover, updating this knowledge incurs high training costs. Retrieval-augmented generation (RAG) methods address this issue by integrating external knowledge. The model can answer questions it couldn't previously by retrieving knowledge relevant to the query. This approach improves performance in certain scenarios for specific tasks. However, if irrelevant texts are retrieved, it may impair model performance. In this paper, we propose Retrieval Augmented Iterative Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and processes them in three submodules to enhance the model's problem-solving capabilities. Experiments show that our method outperforms existing benchmarks, performing well on models like GPT3.5, Llama2, significantly enhancing factual reasoning capabilities a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32454;&#31890;&#24230;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#21644;&#35299;&#21078;&#30149;&#29702;&#25552;&#31034;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#24230;&#35814;&#32454;&#21644;&#20934;&#30830;&#30340;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.06835</link><description>&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#21644;&#35299;&#21078;&#30149;&#29702;&#25552;&#31034;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Medical Image Synthesis via Fine-Grained Image-Text Alignment and Anatomy-Pathology Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06835
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#21644;&#35299;&#21078;&#30149;&#29702;&#25552;&#31034;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#24230;&#35814;&#32454;&#21644;&#20934;&#30830;&#30340;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#38480;&#21046;&#20102;&#39640;&#36136;&#37327;&#21307;&#23398;&#22270;&#20687;&#20379;&#20844;&#20247;&#20351;&#29992;&#30340;&#21487;&#29992;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#24120;&#24120;&#38590;&#20197;&#20934;&#30830;&#25429;&#25417;&#35814;&#32454;&#35299;&#21078;&#32467;&#26500;&#21644;&#30149;&#29702;&#26465;&#20214;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#32454;&#31890;&#24230;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#21644;&#35299;&#21078;&#30149;&#29702;&#25552;&#31034;&#29983;&#25104;&#39640;&#24230;&#35814;&#32454;&#21644;&#20934;&#30830;&#30340;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#19982;&#22270;&#20687;&#29983;&#25104;&#24314;&#27169;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#25551;&#36848;&#24615;&#25991;&#26412;&#25552;&#31034;&#19982;&#21512;&#25104;&#22270;&#20687;&#30340;&#35299;&#21078;&#21644;&#30149;&#29702;&#32454;&#33410;&#20043;&#38388;&#30340;&#31934;&#30830;&#23545;&#40784;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;&#35299;&#21078;&#30149;&#29702;&#25552;&#31034;&#27169;&#22359;&#21644;&#22522;&#20110;&#32454;&#31890;&#24230;&#23545;&#40784;&#30340;&#21512;&#25104;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06835v1 Announce Type: cross  Abstract: Data scarcity and privacy concerns limit the availability of high-quality medical images for public use, which can be mitigated through medical image synthesis. However, current medical image synthesis methods often struggle to accurately capture the complexity of detailed anatomical structures and pathological conditions. To address these challenges, we propose a novel medical image synthesis model that leverages fine-grained image-text alignment and anatomy-pathology prompts to generate highly detailed and accurate synthetic medical images. Our method integrates advanced natural language processing techniques with image generative modeling, enabling precise alignment between descriptive text prompts and the synthesized images' anatomical and pathological details. The proposed approach consists of two key components: an anatomy-pathology prompting module and a fine-grained alignment-based synthesis module. The anatomy-pathology prompt
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#25513;&#27169;&#30340;Transformer-based&#26550;&#26500;SNAG&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#20013;&#23454;&#20307;&#23884;&#20837;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.06832</link><description>&lt;p&gt;
&#22122;&#22768;&#30340;&#21147;&#37327;&#65306;&#26397;&#30528;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06832
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#25513;&#27169;&#30340;Transformer-based&#26550;&#26500;SNAG&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#20013;&#23454;&#20307;&#23884;&#20837;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#36827;&#23637;&#20984;&#26174;&#20986;&#40065;&#26834;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#65288;MMKG&#65289;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#30340;&#24517;&#35201;&#24615;&#12290;&#27492;&#26694;&#26550;&#23545;&#20110;&#22312;&#35268;&#27169;&#19978;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#25972;&#21512;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26088;&#22312;&#20943;&#36731;&#30693;&#35782;&#35823;&#35299;&#21644;&#22810;&#27169;&#24577;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#20934;&#30830;&#23884;&#20837;MMKG&#20013;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20219;&#21153;&#65306;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;MKGC&#65289;&#21644;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SNAG&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#37197;&#22791;&#20102;&#27169;&#24577;&#32423;&#22122;&#22768;&#25513;&#27169;&#65292;&#20197;&#22312;&#30693;&#35782;&#22270;&#20013;&#40065;&#26834;&#22320;&#38598;&#25104;&#22810;&#27169;&#24577;&#23454;&#20307;&#29305;&#24449;&#12290;&#36890;&#36807;&#20026;MKGC&#21644;MMEA&#37117;&#24341;&#20837;&#29305;&#23450;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24635;&#20849;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#65288;&#19977;&#20010;&#29992;&#20110;MKGC&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06832v1 Announce Type: cross  Abstract: The advancement of Multi-modal Pre-training highlights the necessity for a robust Multi-Modal Knowledge Graph (MMKG) representation learning framework. This framework is crucial for integrating structured knowledge into multi-modal Large Language Models (LLMs) at scale, aiming to alleviate issues like knowledge misconceptions and multi-modal hallucinations. In this work, to evaluate models' ability to accurately embed entities within MMKGs, we focus on two widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG method that utilizes a Transformer-based architecture equipped with modality-level noise masking for the robust integration of multi-modal entity features in KGs. By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets (three for MKGC and 
&lt;/p&gt;</description></item><item><title>NeuPAN &#26159;&#19968;&#31181;&#23454;&#26102;&#12289;&#39640;&#24230;&#20934;&#30830;&#12289;&#26080;&#22320;&#22270;&#12289;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#19988;&#23545;&#29615;&#22659;&#19981;&#21464;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#22823;&#30340;&#21019;&#26032;&#22312;&#20110;&#23558;&#21407;&#22987;&#28857;&#30452;&#25509;&#26144;&#23556;&#21040;&#23398;&#20064;&#21040;&#30340;&#22810;&#24103;&#36317;&#31163;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#31471;&#21040;&#31471;&#27169;&#22411;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2403.06828</link><description>&lt;p&gt;
NeuPAN:&#30452;&#25509;&#28857;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06828
&lt;/p&gt;
&lt;p&gt;
NeuPAN &#26159;&#19968;&#31181;&#23454;&#26102;&#12289;&#39640;&#24230;&#20934;&#30830;&#12289;&#26080;&#22320;&#22270;&#12289;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#19988;&#23545;&#29615;&#22659;&#19981;&#21464;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#22823;&#30340;&#21019;&#26032;&#22312;&#20110;&#23558;&#21407;&#22987;&#28857;&#30452;&#25509;&#26144;&#23556;&#21040;&#23398;&#20064;&#21040;&#30340;&#22810;&#24103;&#36317;&#31163;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#31471;&#21040;&#31471;&#27169;&#22411;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#23545;&#38750;&#20840;&#21521;&#26426;&#22120;&#20154;&#36827;&#34892;&#23548;&#33322;&#38656;&#35201;&#26497;&#20854;&#31934;&#30830;&#30340;&#24863;&#30693;&#21644;&#36816;&#21160;&#20197;&#36991;&#20813;&#30896;&#25758;&#12290;&#26412;&#25991;&#25552;&#20986;NeuPAN&#65306;&#19968;&#31181;&#23454;&#26102;&#12289;&#39640;&#24230;&#20934;&#30830;&#12289;&#26080;&#22320;&#22270;&#12289;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#65292;&#19988;&#23545;&#29615;&#22659;&#19981;&#21464;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#35299;&#20915;&#26041;&#26696;&#12290;NeuPAN&#37319;&#29992;&#32039;&#32806;&#21512;&#30340;&#24863;&#30693;-&#36816;&#21160;&#26694;&#26550;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#26377;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#65306;1&#65289;&#23427;&#30452;&#25509;&#23558;&#21407;&#22987;&#28857;&#26144;&#23556;&#21040;&#23398;&#20064;&#21040;&#30340;&#22810;&#24103;&#36317;&#31163;&#31354;&#38388;&#65292;&#36991;&#20813;&#20102;&#20174;&#24863;&#30693;&#21040;&#25511;&#21046;&#30340;&#35823;&#24046;&#20256;&#25773;&#65307;2&#65289;&#20174;&#31471;&#21040;&#31471;&#22522;&#20110;&#27169;&#22411;&#23398;&#20064;&#30340;&#35282;&#24230;&#36827;&#34892;&#35299;&#37322;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#25910;&#25947;&#12290;NeuPAN&#30340;&#20851;&#38190;&#22312;&#20110;&#21033;&#29992;&#25554;&#25300;&#24335;&#65288;PnP&#65289;&#20132;&#26367;&#26368;&#23567;&#21270;&#20256;&#24863;&#22120;&#65288;PAN&#65289;&#32593;&#32476;&#35299;&#39640;&#32500;&#31471;&#21040;&#31471;&#25968;&#23398;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#28857;&#32423;&#32422;&#26463;&#65292;&#20351;NeuPAN&#33021;&#22815;&#30452;&#25509;&#29983;&#25104;&#23454;&#26102;&#12289;&#31471;&#21040;&#31471;&#12289;&#29289;&#29702;&#21487;&#35299;&#37322;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06828v1 Announce Type: cross  Abstract: Navigating a nonholonomic robot in a cluttered environment requires extremely accurate perception and locomotion for collision avoidance. This paper presents NeuPAN: a real-time, highly-accurate, map-free, robot-agnostic, and environment-invariant robot navigation solution. Leveraging a tightly-coupled perception-locomotion framework, NeuPAN has two key innovations compared to existing approaches: 1) it directly maps raw points to a learned multi-frame distance space, avoiding error propagation from perception to control; 2) it is interpretable from an end-to-end model-based learning perspective, enabling provable convergence. The crux of NeuPAN is to solve a high-dimensional end-to-end mathematical model with various point-level constraints using the plug-and-play (PnP) proximal alternating-minimization network (PAN) with neurons in the loop. This allows NeuPAN to generate real-time, end-to-end, physically-interpretable motions direct
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;In-context Exploration-Exploitation (ICEE)&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;Transformer&#27169;&#22411;&#20869;&#37096;&#36827;&#34892;&#25506;&#32034;-&#21033;&#29992;&#26435;&#34913;&#65292;&#25552;&#39640;&#20102;&#22312;-context&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.06826</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25506;&#32034;-&#21033;&#29992;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-context Exploration-Exploitation for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06826
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;In-context Exploration-Exploitation (ICEE)&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;Transformer&#27169;&#22411;&#20869;&#37096;&#36827;&#34892;&#25506;&#32034;-&#21033;&#29992;&#26435;&#34913;&#65292;&#25552;&#39640;&#20102;&#22312;-context&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;-context&#23398;&#20064;&#26159;&#22312;&#32447;&#31574;&#30053;&#23398;&#20064;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#23454;&#29616;&#65292;&#26080;&#38656;&#26799;&#24230;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#25910;&#38598;&#22823;&#37327;&#35757;&#32451;&#36712;&#36857;&#38598;&#24182;&#35757;&#32451;&#22823;&#22411;Transformer&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#25152;&#24102;&#26469;&#30340;&#26174;&#33879;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;In-context Exploration-Exploitation&#65288;ICEE&#65289;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#35813;&#31639;&#27861;&#26088;&#22312;&#20248;&#21270;&#22312;-context&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;ICEE&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#22312;Transformer&#27169;&#22411;&#20013;&#25191;&#34892;&#25506;&#32034;-&#21033;&#29992;&#26435;&#34913;&#65292;&#19981;&#38656;&#35201;&#26174;&#24335;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#22240;&#27492;&#65292;ICEE&#21487;&#20197;&#20687;&#39640;&#26031;&#36807;&#31243;&#20559;&#24046;&#26041;&#27861;&#37027;&#26679;&#26377;&#25928;&#22320;&#35299;&#20915;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#65292;&#20294;&#26102;&#38388;&#26174;&#30528;&#36739;&#30701;&#12290;&#36890;&#36807;&#22312;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;ICEE&#33021;&#22815;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;R
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06826v1 Announce Type: cross  Abstract: In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new R
&lt;/p&gt;</description></item><item><title>GNN&#30340;&#26680;&#24515;&#26550;&#26500;&#26377;&#20004;&#20010;&#29256;&#26412;&#65292;&#31532;&#19968;&#20010;&#29256;&#26412;&#28040;&#24687;&#20165;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#30340;&#29366;&#24577;&#65292;&#32780;&#31532;&#20108;&#20010;&#29256;&#26412;&#28040;&#24687;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#21644;&#30446;&#26631;&#39030;&#28857;&#30340;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.06817</link><description>&lt;p&gt;
&#30446;&#26631;&#20449;&#24687;&#26356;&#26377;&#25928;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Targeted Messages More Effective?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06817
&lt;/p&gt;
&lt;p&gt;
GNN&#30340;&#26680;&#24515;&#26550;&#26500;&#26377;&#20004;&#20010;&#29256;&#26412;&#65292;&#31532;&#19968;&#20010;&#29256;&#26412;&#28040;&#24687;&#20165;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#30340;&#29366;&#24577;&#65292;&#32780;&#31532;&#20108;&#20010;&#29256;&#26412;&#28040;&#24687;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#21644;&#30446;&#26631;&#39030;&#28857;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#29992;&#20110;&#22270;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#26412;&#36136;&#19978;&#65292;GNN&#26159;&#19968;&#20010;&#20998;&#24067;&#24335;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#20854;&#21463;&#21040;&#20174;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#21442;&#25968;&#30340;&#25511;&#21046;&#12290;&#23427;&#22312;&#22270;&#30340;&#39030;&#28857;&#19978;&#25805;&#20316;&#65306;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#39030;&#28857;&#22312;&#27599;&#20010;&#20256;&#20837;&#36793;&#19978;&#25509;&#25910;&#19968;&#26465;&#28040;&#24687;&#65292;&#32858;&#21512;&#36825;&#20123;&#28040;&#24687;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#24403;&#21069;&#30340;&#29366;&#24577;&#21644;&#32858;&#21512;&#30340;&#28040;&#24687;&#26356;&#26032;&#23427;&#20204;&#30340;&#29366;&#24577;&#12290;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#21487;&#20197;&#29992;&#24102;&#35745;&#25968;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#26576;&#20123;&#29255;&#27573;&#21644;Weisfeiler-Lehman&#31639;&#27861;&#26469;&#25551;&#36848;&#12290;GNN&#30340;&#26680;&#24515;&#26550;&#26500;&#26377;&#20004;&#20010;&#19981;&#21516;&#30340;&#29256;&#26412;&#12290;&#22312;&#31532;&#19968;&#20010;&#29256;&#26412;&#20013;&#65292;&#28040;&#24687;&#20165;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#30340;&#29366;&#24577;&#65292;&#32780;&#22312;&#31532;&#20108;&#20010;&#29256;&#26412;&#20013;&#65292;&#28040;&#24687;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#21644;&#30446;&#26631;&#39030;&#28857;&#30340;&#29366;&#24577;&#12290;&#23454;&#38469;&#19978;&#65292;&#36825;&#20004;&#20010;&#29256;&#26412;&#37117;&#34987;&#20351;&#29992;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;GNN&#30340;&#29702;&#35770;&#22823;&#22810;&#38598;&#20013;&#22312;&#31532;&#19968;&#20010;&#29256;&#26412;&#19978;&#12290;&#22312;&#36923;&#36753;&#26041;&#38754;&#65292;&#36825;&#20004;&#20010;&#29256;&#26412;&#23545;&#24212;&#30528;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06817v1 Announce Type: cross  Abstract: Graph neural networks (GNN) are deep learning architectures for graphs. Essentially, a GNN is a distributed message passing algorithm, which is controlled by parameters learned from data. It operates on the vertices of a graph: in each iteration, vertices receive a message on each incoming edge, aggregate these messages, and then update their state based on their current state and the aggregated messages. The expressivity of GNNs can be characterised in terms of certain fragments of first-order logic with counting and the Weisfeiler-Lehman algorithm.   The core GNN architecture comes in two different versions. In the first version, a message only depends on the state of the source vertex, whereas in the second version it depends on the states of the source and target vertices. In practice, both of these versions are used, but the theory of GNNs so far mostly focused on the first one. On the logical side, the two versions correspond to 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#21487;&#35299;&#37322;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#32467;&#21512;&#39044;&#27979;&#29305;&#23450;&#22686;&#24378;&#31574;&#30053;&#22312;Sim-to-Real&#35774;&#32622;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;GeneticAugment&#30340;&#36951;&#20256;&#35268;&#21010;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.06786</link><description>&lt;p&gt;
&#29992;&#36951;&#20256;&#23398;&#20064;&#35774;&#35745;Sim-to-Real&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Genetic Learning for Designing Sim-to-Real Data Augmentations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06786
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#21487;&#35299;&#37322;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#32467;&#21512;&#39044;&#27979;&#29305;&#23450;&#22686;&#24378;&#31574;&#30053;&#22312;Sim-to-Real&#35774;&#32622;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;GeneticAugment&#30340;&#36951;&#20256;&#35268;&#21010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#22312;&#35757;&#32451;&#21512;&#25104;&#25968;&#25454;&#26102;&#29992;&#20110;&#24357;&#21512;Sim-to-Real&#39046;&#22495;&#24046;&#36317;&#20013;&#26159;&#26377;&#29992;&#30340;&#12290;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#25193;&#23637;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#40723;&#21169;&#27169;&#22411;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#35768;&#22810;&#22270;&#20687;&#22686;&#24378;&#25216;&#26415;&#23384;&#22312;&#65292;&#30001;&#19981;&#21516;&#35774;&#32622;&#21442;&#25968;&#21270;&#65292;&#27604;&#22914;&#24378;&#24230;&#21644;&#27010;&#29575;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#19981;&#21516;&#21487;&#33021;&#22686;&#24378;&#31574;&#30053;&#30340;&#22823;&#31354;&#38388;&#12290;&#23545;&#20110;&#20811;&#26381;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;Sim-to-Real&#24046;&#36317;&#65292;&#19968;&#20123;&#31574;&#30053;&#27604;&#20854;&#20182;&#31574;&#30053;&#26356;&#26377;&#25928;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#21407;&#22240;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#21487;&#35299;&#37322;&#24230;&#24230;&#37327;&#65292;&#21487;&#20197;&#32467;&#21512;&#36215;&#26469;&#39044;&#27979;&#26576;&#31181;&#29305;&#23450;&#22686;&#24378;&#31574;&#30053;&#22312;&#29305;&#23450;Sim-to-Real&#35774;&#32622;&#20013;&#30340;&#24037;&#20316;&#25928;&#26524;&#65292;&#37325;&#28857;&#25918;&#22312;&#30446;&#26631;&#26816;&#27979;&#19978;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#35768;&#22810;&#20855;&#26377;&#19981;&#21516;&#22686;&#24378;&#31574;&#30053;&#30340;&#27169;&#22411;&#24182;&#23637;&#31034;&#19982;&#30495;&#23454;&#25968;&#25454;&#34920;&#29616;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GeneticAugment&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06786v1 Announce Type: cross  Abstract: Data augmentations are useful in closing the sim-to-real domain gap when training on synthetic data. This is because they widen the training data distribution, thus encouraging the model to generalize better to other domains. Many image augmentation techniques exist, parametrized by different settings, such as strength and probability. This leads to a large space of different possible augmentation policies. Some policies work better than others for overcoming the sim-to-real gap for specific datasets, and it is unclear why. This paper presents two different interpretable metrics that can be combined to predict how well a certain augmentation policy will work for a specific sim-to-real setting, focusing on object detection. We validate our metrics by training many models with different augmentation policies and showing a strong correlation with performance on real data. Additionally, we introduce GeneticAugment, a genetic programming me
&lt;/p&gt;</description></item><item><title>FastV&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#24182;&#22312;&#21518;&#32493;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06764</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#29255;&#22312;&#31532;&#20108;&#23618;&#20043;&#21518;&#20215;&#20540;1/2&#20195;&#24065;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#25512;&#29702;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06764
&lt;/p&gt;
&lt;p&gt;
FastV&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#24182;&#22312;&#21518;&#32493;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#20013;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#23384;&#22312;&#20302;&#25928;&#29616;&#35937;&#65292;&#23588;&#20854;&#26159;&#22312;&#30693;&#21517;&#27169;&#22411;&#22914;LLaVA-1.5&#12289;QwenVL-Chat&#21644;Video-LLaVA&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27969;&#34892;&#30340;LVLMs&#30340;&#28145;&#23618;&#20013;&#65292;&#23545;&#35270;&#35273;&#20195;&#24065;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26497;&#20854;&#20302;&#25928;&#65292;&#26263;&#31034;&#30456;&#36739;&#20110;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#65292;&#38656;&#35201;&#26356;&#31232;&#30095;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FastV&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#26089;&#26399;&#23618;&#20013;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#21644;&#22312;&#38543;&#21518;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#26469;&#20248;&#21270;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;FastV&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;LLaVA-1.5-13B&#30340;FLOP&#20943;&#23569;&#20102;45%&#65289;&#65292;&#32780;&#19981;&#20250;&#22312;&#24191;&#27867;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#29306;&#29298;&#24615;&#33021;&#12290;FastV&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#26435;&#34913;&#26159;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#65292;&#24182;&#19988;&#26159;&#24085;&#32047;&#25176;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06764v1 Announce Type: cross  Abstract: In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones. Our evaluations demonstrate FastV's ability to dramatically reduce computational costs (e.g., a 45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks. The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient. It can compress t
&lt;/p&gt;</description></item><item><title>ALaRM&#26159;&#31532;&#19968;&#20010;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24314;&#27169;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#31934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.06754</link><description>&lt;p&gt;
ALaRM: &#36890;&#36807;&#20998;&#23618;&#22870;&#21169;&#24314;&#27169;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ALaRM: Align Language Models via Hierarchical Rewards Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06754
&lt;/p&gt;
&lt;p&gt;
ALaRM&#26159;&#31532;&#19968;&#20010;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24314;&#27169;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#31934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;ALaRM&#65292;&#31532;&#19968;&#20010;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20174;&#20154;&#31867;&#21453;&#39304;&#27169;&#22411;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24403;&#21069;&#23545;&#40784;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#20154;&#31867;&#30417;&#30563;&#20449;&#21495;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#36890;&#36807;&#23558;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#25972;&#21512;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#26356;&#21152;&#31934;&#30830;&#21644;&#19968;&#33268;&#22320;&#25351;&#23548;&#26397;&#30528;&#26399;&#26395;&#30340;&#32467;&#26524;&#21069;&#36827;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#21644;&#24320;&#25918;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#21644;&#32452;&#21512;&#22810;&#20010;&#22870;&#21169;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26426;&#21046;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#23545;&#40784;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#38271;&#31687;&#38382;&#39064;&#22238;&#31572;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#20351;&#29992;gpt-3.5-turbo&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06754v1 Announce Type: cross  Abstract: We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demon
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#20986;&#29616;&#30340;&#31163;&#38774;&#38382;&#39064;&#30340;&#26032;&#22411;&#30417;&#30563;&#24494;&#35843;&#26426;&#21046;ACT-MNMT Auto-Constriction Turning&#65292;&#19982;&#20256;&#32479;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#27491;&#20132;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#26500;&#24314;&#21463;&#38480;&#27169;&#26495;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06745</link><description>&lt;p&gt;
ACT-MNMT&#33258;&#21160;&#25910;&#32553;&#36716;&#21521;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#20986;&#29616;&#30340;&#31163;&#38774;&#38382;&#39064;&#30340;&#26032;&#22411;&#30417;&#30563;&#24494;&#35843;&#26426;&#21046;ACT-MNMT Auto-Constriction Turning&#65292;&#19982;&#20256;&#32479;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#27491;&#20132;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#26500;&#24314;&#21463;&#38480;&#27169;&#26495;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#38646;/&#23569;shot&#25552;&#31034;&#25110;&#25552;&#31034;&#35843;&#25972;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;LLM&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#28151;&#21512;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#65292;&#22522;&#20110;LLM&#30340;&#32763;&#35793;&#27169;&#22411;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#20013;&#38754;&#20020;&#20102;&#31163;&#38774;&#38382;&#39064;&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#29616;&#35937;&#65292;&#21363;&#25351;&#20196;&#35823;&#35299;&#12289;&#38169;&#35823;&#35821;&#35328;&#32763;&#35793;&#21644;&#36807;&#24230;&#29983;&#25104;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;\model&#65289;&#30340;\textbf{\underline{A}}uto-\textbf{\underline{C}}onstriction \textbf{\underline{T}}urning&#26426;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#30417;&#30563;&#24494;&#35843;&#26426;&#21046;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#27491;&#20132;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;\model&#36890;&#36807;&#22312;&#30446;&#26631;&#31471;&#28155;&#21152;&#35302;&#21457;&#20196;&#29260;&#33258;&#21160;&#26500;&#36896;&#21463;&#38480;&#27169;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06745v1 Announce Type: cross  Abstract: Large language model (LLM) has achieved promising performance in multilingual machine translation tasks through zero/few-shot prompts or prompt-tuning. However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation. For this issue, this paper introduces an \textbf{\underline{A}}uto-\textbf{\underline{C}}onstriction \textbf{\underline{T}}urning mechanism for \textbf{\underline{M}}ultilingual \textbf{\underline{N}}eural \textbf{\underline{M}}achine \textbf{\underline{T}}ranslation (\model), which is a novel supervised fine-tuning mechanism and orthogonal to the traditional prompt-based methods. In this method, \model automatically constructs a constrained template in the target side by adding trigger tokens ahead
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#24182;&#24341;&#20837;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#22270;&#20687;&#26631;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06735</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#22686;&#24378;&#22270;&#20687;&#26631;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Image Caption Generation Using Reinforcement Learning with Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06735
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#24182;&#24341;&#20837;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#22270;&#20687;&#26631;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#20197;&#20135;&#29983;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#36755;&#20986;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#19978;&#65292;&#29305;&#21035;&#26159;&#20026;&#22270;&#20687;&#29983;&#25104;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#26631;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#30417;&#30563;&#23398;&#20064;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29983;&#25104;&#20154;&#31867;&#21916;&#27426;&#30340;&#26631;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;Flickr8k&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#20248;&#21270;&#27169;&#22411;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#30340;&#31616;&#26126;&#27010;&#36848;&#65292;&#24076;&#26395;&#20026;&#20154;&#31867;&#20559;&#22909;&#29983;&#25104;AI&#27169;&#22411;&#39046;&#22495;&#30340;&#25345;&#32493;&#36827;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06735v1 Announce Type: cross  Abstract: Research on generative models to produce human-aligned / human-preferred outputs has seen significant recent contributions. Between text and image-generative models, we narrowed our focus to text-based generative models, particularly to produce captions for images that align with human preferences. In this research, we explored a potential method to amplify the performance of the Deep Neural Network Model to generate captions that are preferred by humans. This was achieved by integrating Supervised Learning and Reinforcement Learning with Human Feedback (RLHF) using the Flickr8k dataset. Also, a novel loss function that is capable of optimizing the model based on human feedback is introduced. In this paper, we provide a concise sketch of our approach and results, hoping to contribute to the ongoing advances in the field of human-aligned generative AI models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CognitiveEMS&#65292;&#19968;&#20010;&#23454;&#26102;&#22810;&#27169;&#24577;&#35748;&#30693;&#21161;&#25163;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#26032;&#39062;&#32452;&#20214;&#35299;&#20915;&#20102;&#23454;&#26102;&#35748;&#30693;&#36741;&#21161;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#65292;&#20026;&#24613;&#25937;&#26381;&#21153;&#25552;&#20379;&#20851;&#38190;&#30340;&#36741;&#21161;&#12290;</title><link>https://arxiv.org/abs/2403.06734</link><description>&lt;p&gt;
&#24613;&#25937;&#26381;&#21153;&#30340;&#23454;&#26102;&#22810;&#27169;&#24577;&#35748;&#30693;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Real-Time Multimodal Cognitive Assistant for Emergency Medical Services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CognitiveEMS&#65292;&#19968;&#20010;&#23454;&#26102;&#22810;&#27169;&#24577;&#35748;&#30693;&#21161;&#25163;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#26032;&#39062;&#32452;&#20214;&#35299;&#20915;&#20102;&#23454;&#26102;&#35748;&#30693;&#36741;&#21161;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#65292;&#20026;&#24613;&#25937;&#26381;&#21153;&#25552;&#20379;&#20851;&#38190;&#30340;&#36741;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#25937;&#26381;&#21153;&#65288;EMS&#65289;&#21709;&#24212;&#32773;&#32463;&#24120;&#22312;&#26102;&#38388;&#32039;&#36843;&#30340;&#26465;&#20214;&#19979;&#24037;&#20316;&#65292;&#38754;&#20020;&#35748;&#30693;&#36807;&#36733;&#21644;&#22266;&#26377;&#39118;&#38505;&#65292;&#38656;&#35201;&#20855;&#22791;&#20851;&#38190;&#24605;&#32500;&#21644;&#24555;&#36895;&#20915;&#31574;&#30340;&#22522;&#26412;&#25216;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CognitiveEMS&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#21487;&#31359;&#25140;&#35748;&#30693;&#21161;&#25163;&#31995;&#32479;&#65292;&#21487;&#20197;&#20805;&#24403;&#21327;&#20316;&#34394;&#25311;&#20249;&#20276;&#65292;&#23454;&#26102;&#33719;&#21462;&#21644;&#20998;&#26512;&#24613;&#25937;&#29616;&#22330;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#26234;&#33021;&#30524;&#38236;&#19982;EMS&#21709;&#24212;&#32773;&#20114;&#21160;&#12290;CognitiveEMS&#23454;&#26102;&#22788;&#29702;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#65292;&#24182;&#21033;&#29992;&#36793;&#32536;&#35745;&#31639;&#26469;&#25552;&#20379;EMS&#21327;&#35758;&#36873;&#25321;&#21644;&#24178;&#39044;&#35782;&#21035;&#30340;&#36741;&#21161;&#12290;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#26032;&#39062;&#32452;&#20214;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23454;&#26102;&#35748;&#30693;&#36741;&#21161;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#65306;&#65288;i&#65289;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#38024;&#23545;&#27169;&#25311;&#30340;EMS&#38899;&#39057;&#35760;&#24405;&#36827;&#34892;&#23454;&#38469;&#24613;&#35786;&#23545;&#35805;&#65292;&#22686;&#24378;wit
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06734v1 Announce Type: new  Abstract: Emergency Medical Services (EMS) responders often operate under time-sensitive conditions, facing cognitive overload and inherent risks, requiring essential skills in critical thinking and rapid decision-making. This paper presents CognitiveEMS, an end-to-end wearable cognitive assistant system that can act as a collaborative virtual partner engaging in the real-time acquisition and analysis of multimodal data from an emergency scene and interacting with EMS responders through Augmented Reality (AR) smart glasses. CognitiveEMS processes the continuous streams of data in real-time and leverages edge computing to provide assistance in EMS protocol selection and intervention recognition. We address key technical challenges in real-time cognitive assistance by introducing three novel components: (i) a Speech Recognition model that is fine-tuned for real-world medical emergency conversations using simulated EMS audio recordings, augmented wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.06725</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#37325;&#35201;&#24615;&#26426;&#21046;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#26088;&#22312;&#22522;&#20110;&#23398;&#29983;&#30340;&#21382;&#21490;&#20114;&#21160;&#26469;&#20272;&#35745;&#20182;&#20204;&#30340;&#30693;&#35782;&#25484;&#25569;&#31243;&#24230;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;KT&#65288;DLKT&#65289;&#26041;&#27861;&#22312;KT&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#22914;&#39044;&#31639;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35266;&#23519;&#21040;&#30340;&#20114;&#21160;&#38750;&#24120;&#26377;&#38480;&#65292;&#21363;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#12290;&#30452;&#25509;&#22312;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;DLKT&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#24456;&#38590;&#36873;&#25321;&#36866;&#24403;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;KT&#26694;&#26550;&#26469;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;&#21463;&#30427;&#34892;&#30340;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06725v1 Announce Type: cross  Abstract: Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subseque
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22312;&#40654;&#26364;&#20248;&#21270;&#20013;&#37319;&#29992;&#27010;&#29575;&#26799;&#24230;&#35745;&#31639;&#35302;&#21457;&#22120;&#30340;Loopless SVRG&#21644;PAGE&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#35777;&#26126;&#12289;&#25552;&#39640;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#25928;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#23574;&#38160;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.06677</link><description>&lt;p&gt;
&#27969;&#30021;&#30340;&#40654;&#26364;&#39046;&#22495;&#65306;&#26080;&#29615;&#26041;&#24046;&#20943;&#23569;&#30340;&#39640;&#25928;&#40654;&#26364;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Streamlining in the Riemannian Realm: Efficient Riemannian Optimization with Loopless Variance Reduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06677
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22312;&#40654;&#26364;&#20248;&#21270;&#20013;&#37319;&#29992;&#27010;&#29575;&#26799;&#24230;&#35745;&#31639;&#35302;&#21457;&#22120;&#30340;Loopless SVRG&#21644;PAGE&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#35777;&#26126;&#12289;&#25552;&#39640;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#25928;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#23574;&#38160;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#27431;&#20960;&#37324;&#24471;&#21644;&#40654;&#26364;&#35774;&#32622;&#20013;&#20351;&#29992;&#30340;&#20851;&#38190;&#26041;&#24046;&#20943;&#23569;&#26426;&#21046;&#12290;&#40654;&#26364;&#26041;&#24046;&#20943;&#23569;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#21452;&#24490;&#29615;&#32467;&#26500;&#65292;&#22312;&#27599;&#20010;&#24490;&#29615;&#30340;&#24320;&#22987;&#35745;&#31639;&#23436;&#25972;&#26799;&#24230;&#12290;&#30830;&#23450;&#26368;&#20339;&#20869;&#24490;&#29615;&#38271;&#24230;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#21462;&#20915;&#20110;&#24378;&#20984;&#24615;&#25110;&#20809;&#28369;&#24230;&#24120;&#25968;&#65292;&#36825;&#20123;&#24120;&#25968;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#25110;&#38590;&#20197;&#20272;&#35745;&#12290;&#21463;&#27431;&#20960;&#37324;&#24471;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26080;&#29615;&#40654;&#26364;SVRG&#65288;R-LSVRG&#65289;&#21644;PAGE&#65288;R-PAGE&#65289;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#29992;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;&#30828;&#24065;&#32763;&#36716;&#35302;&#21457;&#30340;&#27010;&#29575;&#26799;&#24230;&#35745;&#31639;&#26367;&#25442;&#20102;&#22806;&#24490;&#29615;&#65292;&#30830;&#20445;&#20102;&#26356;&#31616;&#21333;&#30340;&#35777;&#26126;&#12289;&#39640;&#25928;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#23574;&#38160;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#23558;R-PAGE&#20316;&#20026;&#38750;&#20984;&#40654;&#26364;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#37325;&#35201;&#35774;&#32622;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06677v1 Announce Type: cross  Abstract: In this study, we investigate stochastic optimization on Riemannian manifolds, focusing on the crucial variance reduction mechanism used in both Euclidean and Riemannian settings. Riemannian variance-reduced methods usually involve a double-loop structure, computing a full gradient at the start of each loop. Determining the optimal inner loop length is challenging in practice, as it depends on strong convexity or smoothness constants, which are often unknown or hard to estimate. Motivated by Euclidean methods, we introduce the Riemannian Loopless SVRG (R-LSVRG) and PAGE (R-PAGE) methods. These methods replace the outer loop with probabilistic gradient computation triggered by a coin flip in each iteration, ensuring simpler proofs, efficient hyperparameter selection, and sharp convergence guarantees. Using R-PAGE as a framework for non-convex Riemannian optimization, we demonstrate its applicability to various important settings. For ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31456;&#38024;&#23545;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#38754;&#20020;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#20013;&#27880;&#20837;&#27602;&#32032;&#26469;&#29983;&#25104;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#65292;&#24182;&#23545;&#20854;&#23545;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.06675</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#20462;&#22797;&#20195;&#30721;&#23545;&#31243;&#24207;&#36827;&#34892;&#27602;&#21270;&#65306;AI&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#23433;&#20840;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#38024;&#23545;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#38754;&#20020;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#20013;&#27880;&#20837;&#27602;&#32032;&#26469;&#29983;&#25104;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#65292;&#24182;&#23545;&#20854;&#23545;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#22312;&#21327;&#21161;&#24320;&#21457;&#20154;&#21592;&#20174;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#24320;&#22987;&#32534;&#20889;&#36719;&#20214;&#26041;&#38754;&#21457;&#25381;&#20102;&#26681;&#26412;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#22312;&#20174;&#19981;&#21487;&#38752;&#30340;&#22312;&#32447;&#26469;&#28304;&#65288;&#20363;&#22914;GitHub&#65292;Hugging Face&#65289;&#25910;&#38598;&#30340;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;AI&#27169;&#22411;&#25104;&#20026;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#30340;&#23481;&#26131;&#30446;&#26631;&#65292;&#21363;&#25915;&#20987;&#32773;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#20013;&#27880;&#20837;&#23569;&#37327;&#27602;&#32032;&#65288;&#21363;&#24039;&#22937;&#21046;&#20316;&#30340;&#24694;&#24847;&#26679;&#26412;&#65289;&#26469;&#30772;&#22351;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#19968;&#31181;&#23548;&#33268;&#29983;&#25104;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#30340;&#26032;&#22411;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#26469;&#35752;&#35770;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#23433;&#20840;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#25915;&#20987;&#22914;&#20309;&#24433;&#21709;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#26368;&#26032;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35299;&#20915;&#36825;&#19968;&#23041;&#32961;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06675v1 Announce Type: cross  Abstract: AI-based code generators have gained a fundamental role in assisting developers in writing software starting from natural language (NL). However, since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples. In this position paper, we address the security of AI code generators by identifying a novel data poisoning attack that results in the generation of vulnerable code. Next, we devise an extensive evaluation of how these attacks impact state-of-the-art models for code generation. Lastly, we discuss potential solutions to overcome this threat.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340;&#34917;&#19969;&#23545;&#34917;&#19969;SimCLR&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#27773;&#36710;&#25439;&#20260;&#26816;&#27979;&#30340;&#22270;&#20687;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#27773;&#36710;&#25439;&#20260;&#26816;&#27979;&#21644;&#22270;&#20687;&#23545;&#40784;&#20004;&#22823;&#32452;&#20214;&#30340;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.06674</link><description>&lt;p&gt;
&#27773;&#36710;&#25439;&#20260;&#26816;&#27979;&#21450;&#34917;&#19969;&#21040;&#34917;&#19969;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Car Damage Detection and Patch-to-Patch Self-supervised Image Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340;&#34917;&#19969;&#23545;&#34917;&#19969;SimCLR&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#27773;&#36710;&#25439;&#20260;&#26816;&#27979;&#30340;&#22270;&#20687;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#27773;&#36710;&#25439;&#20260;&#26816;&#27979;&#21644;&#22270;&#20687;&#23545;&#40784;&#20004;&#22823;&#32452;&#20214;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#26088;&#22312;&#35782;&#21035;&#22330;&#26223;&#20013;&#30340;&#20687;&#32032;&#65292;&#24182;&#23558;&#23427;&#20204;&#29992;&#20110;&#21508;&#31181;&#30446;&#30340;&#12290;&#20854;&#20013;&#19968;&#20010;&#26377;&#36259;&#30340;&#24212;&#29992;&#26159;&#20445;&#38505;&#20844;&#21496;&#30340;&#27773;&#36710;&#25439;&#20260;&#26816;&#27979;&#65292;&#23427;&#20542;&#21521;&#20110;&#36890;&#36807;&#27604;&#36739;&#34892;&#31243;&#21069;&#21518;&#30340;&#22270;&#29255;&#26469;&#26816;&#27979;&#25152;&#26377;&#27773;&#36710;&#25439;&#20260;&#65292;&#29978;&#33267;&#38656;&#35201;&#20004;&#20010;&#32452;&#20214;&#65306;(i) &#27773;&#36710;&#25439;&#20260;&#26816;&#27979;&#65307;(ii) &#22270;&#20687;&#23545;&#40784;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;Mask R-CNN&#27169;&#22411;&#26469;&#26816;&#27979;&#33258;&#23450;&#20041;&#22270;&#29255;&#19978;&#30340;&#27773;&#36710;&#25439;&#20260;&#12290;&#32780;&#23545;&#20110;&#22270;&#20687;&#23545;&#40784;&#37096;&#20998;&#65292;&#25105;&#20204;&#29305;&#21035;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#33258;&#30417;&#30563;&#21551;&#21457;&#30340;&#34917;&#19969;&#23545;&#34917;&#19969;SimCLR&#23545;&#40784;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#33258;&#23450;&#20041;&#34892;&#31243;&#21069;/&#34892;&#31243;&#21518;&#27773;&#36710;&#31199;&#36161;&#22270;&#29255;&#20043;&#38388;&#30340;&#36879;&#35270;&#21464;&#25442;&#65292;&#38500;&#20102;&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06674v1 Announce Type: cross  Abstract: Most computer vision applications aim to identify pixels in a scene and use them for diverse purposes. One intriguing application is car damage detection for insurance carriers which tends to detect all car damages by comparing both pre-trip and post-trip images, even requiring two components: (i) car damage detection; (ii) image alignment. Firstly, we implemented a Mask R-CNN model to detect car damages on custom images. Whereas for the image alignment section, we especially propose a novel self-supervised Patch-to-Patch SimCLR inspired alignment approach to find perspective transformations between custom pre/post car rental images except for traditional computer vision methods.
&lt;/p&gt;</description></item><item><title>CEAT&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#31034;&#33539;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#26032;&#26550;&#26500;&#65292;&#36890;&#36807;&#25345;&#32493;&#25193;&#23637;&#21644;&#21560;&#25910;&#21442;&#25968;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#21487;&#22609;&#24615;-&#31283;&#23450;&#24615;&#22256;&#22659;&#21644;&#20998;&#31867;&#22120;&#20559;&#24046;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.06670</link><description>&lt;p&gt;
CEAT&#65306;&#29992;&#20110;&#38750;&#31034;&#33539;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#25345;&#32493;&#25193;&#23637;&#21644;&#21560;&#25910;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar Class-Incremental Learnin
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06670
&lt;/p&gt;
&lt;p&gt;
CEAT&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#31034;&#33539;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#26032;&#26550;&#26500;&#65292;&#36890;&#36807;&#25345;&#32493;&#25193;&#23637;&#21644;&#21560;&#25910;&#21442;&#25968;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#21487;&#22609;&#24615;-&#31283;&#23450;&#24615;&#22256;&#22659;&#21644;&#20998;&#31867;&#22120;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#21160;&#24577;&#22330;&#26223;&#35201;&#27714;&#27169;&#22411;&#20855;&#22791;&#19981;&#26029;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#24536;&#35760;&#26087;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#32463;&#39564;&#37325;&#25918;&#26041;&#27861;&#23384;&#20648;&#19968;&#37096;&#20998;&#26087;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#22312;&#26356;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#65292;&#23384;&#20648;&#26087;&#22270;&#20687;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#36825;&#23548;&#33268;&#20102;&#26356;&#20026;&#20005;&#37325;&#30340;&#21487;&#22609;&#24615;-&#31283;&#23450;&#24615;&#22256;&#22659;&#21644;&#20998;&#31867;&#22120;&#20559;&#24046;&#12290;&#20026;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#25345;&#32493;&#25193;&#23637;&#21644;&#21560;&#25910;&#21464;&#21387;&#22120;&#65288;CEAT&#65289;&#12290;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23558;&#25193;&#23637;-&#34701;&#21512;&#23618;&#19982;&#20923;&#32467;&#21069;&#26399;&#21442;&#25968;&#24182;&#34892;&#25193;&#23637;&#26469;&#23398;&#20064;&#26032;&#30693;&#35782;&#12290;&#20219;&#21153;&#32467;&#26463;&#21518;&#65292;&#25105;&#20204;&#26080;&#25439;&#22320;&#21560;&#25910;&#25193;&#23637;&#30340;&#21442;&#25968;&#21040;&#20027;&#24178;&#65292;&#20197;&#30830;&#20445;&#21442;&#25968;&#25968;&#37327;&#20445;&#25345;&#24658;&#23450;&#12290;&#20026;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21407;&#22411;&#23545;&#27604;&#25439;&#22833;&#65292;&#20197;&#20943;&#23569;&#26087;&#31867;&#21644;&#26032;&#31867;&#20043;&#38388;&#30340;&#37325;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06670v1 Announce Type: cross  Abstract: In real-world applications, dynamic scenarios require the models to possess the capability to learn new tasks continuously without forgetting the old knowledge. Experience-Replay methods store a subset of the old images for joint training. In the scenario of more strict privacy protection, storing the old images becomes infeasible, which leads to a more severe plasticity-stability dilemma and classifier bias. To meet the above challenges, we propose a new architecture, named continual expansion and absorption transformer~(CEAT). The model can learn the novel knowledge by extending the expanded-fusion layers in parallel with the frozen previous parameters. After the task ends, we losslessly absorb the extended parameters into the backbone to ensure that the number of parameters remains constant. To improve the learning ability of the model, we designed a novel prototype contrastive loss to reduce the overlap between old and new classes 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#26234;&#33021;&#26102;&#23578;&#20998;&#26512;&#21644;&#25253;&#21578;&#31995;&#32479; GPT-FAR&#65292;&#26088;&#22312;&#36890;&#36807;&#26377;&#25928;&#30340;&#31168;&#22330;&#20998;&#26512;&#26469;&#29983;&#25104;&#26102;&#23578;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2403.06660</link><description>&lt;p&gt;
FashionReGen: &#22522;&#20110;LLM&#30340;&#26102;&#23578;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FashionReGen: LLM-Empowered Fashion Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06660
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#26234;&#33021;&#26102;&#23578;&#20998;&#26512;&#21644;&#25253;&#21578;&#31995;&#32479; GPT-FAR&#65292;&#26088;&#22312;&#36890;&#36807;&#26377;&#25928;&#30340;&#31168;&#22330;&#20998;&#26512;&#26469;&#29983;&#25104;&#26102;&#23578;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#23578;&#20998;&#26512;&#26159;&#25351;&#23457;&#26597;&#21644;&#35780;&#20272;&#26102;&#23578;&#34892;&#19994;&#20869;&#30340;&#36235;&#21183;&#12289;&#39118;&#26684;&#21644;&#20803;&#32032;&#30340;&#36807;&#31243;&#65292;&#20197;&#29702;&#35299;&#21644;&#35299;&#37322;&#20854;&#24403;&#21069;&#29366;&#24577;&#65292;&#24182;&#29983;&#25104;&#26102;&#23578;&#25253;&#21578;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#26234;&#33021;&#26102;&#23578;&#20998;&#26512;&#21644;&#25253;&#21578;&#31995;&#32479;&#65292;&#31216;&#20026;GPT-FAR&#65292;&#20197;&#24212;&#23545;&#26102;&#23578;&#25253;&#21578;&#29983;&#25104;&#65288;FashionReGen&#65289;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#35797;&#22270;&#36890;&#36807;&#26377;&#25928;&#30340;&#31168;&#22330;&#20998;&#26512;&#26469;&#36827;&#34892;FashionReGen&#65292;&#35813;&#20998;&#26512;&#31995;&#32479;&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#36807;&#31243;&#65292;&#21363;&#31168;&#22330;&#29702;&#35299;&#12289;&#38598;&#20307;&#32452;&#32455;&#21644;&#20998;&#26512;&#65292;&#20197;&#21450;&#25253;&#21578;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06660v1 Announce Type: cross  Abstract: Fashion analysis refers to the process of examining and evaluating trends, styles, and elements within the fashion industry to understand and interpret its current state, generating fashion reports. It is traditionally performed by fashion professionals based on their expertise and experience, which requires high labour cost and may also produce biased results for relying heavily on a small group of people. In this paper, to tackle the Fashion Report Generation (FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting system based the advanced Large Language Models (LLMs), debbed as GPT-FAR. Specifically, it tries to deliver FashionReGen based on effective catwalk analysis, which is equipped with several key procedures, namely, catwalk understanding, collective organization and analysis, and report generation. By posing and exploring such an open-ended, complex and domain-specific task of FashionReGen, it is able t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23545;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#26469;&#21033;&#29992;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;</title><link>https://arxiv.org/abs/2403.06659</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#21644;&#27979;&#35797;&#26102;&#20020;&#24202;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06659
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23545;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#26469;&#21033;&#29992;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#20020;&#24202;&#23454;&#36341;&#20013;&#29992;&#20110;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#30142;&#30149;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#12290;&#22312;&#26410;&#32463;&#27880;&#37322;&#30340;ECG&#25968;&#25454;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;eSSL&#65289;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#34920;&#24449;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#21487;&#20197;&#22312;&#25253;&#21578;&#20013;&#25214;&#21040;&#30340;&#20020;&#24202;&#30693;&#35782;&#12290;&#26412;&#25991;&#36890;&#36807;&#22810;&#27169;&#24577;&#23398;&#20064;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#65292;&#25552;&#20986;&#20102;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#38646;&#26679;&#26412;ECG&#20998;&#31867;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21033;&#29992;&#22806;&#37096;&#19987;&#23478;&#39564;&#35777;&#30340;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#65292;&#29983;&#25104;&#26356;&#22810;&#20851;&#20110;&#24739;&#32773;&#30149;&#21490;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06659v1 Announce Type: cross  Abstract: Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice. While ECG Self-supervised Learning (eSSL) methods show promise in representation learning from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports. This oversight and the requirement for annotated samples for downstream tasks limit eSSL's versatility. In this work, we address these issues with the Multimodal ECG Representation Learning (MERL}) framework. Through multimodal learning on ECG records and associated reports, MERL is capable of performing zero-shot ECG classification with text prompts, eliminating the need for training data in downstream tasks. At test time, we propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach, which uses Large Language Models (LLMs) to exploit external expert-verified clinical knowledge databases, generating mo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24110;&#21161;&#29983;&#25104;&#30495;&#23454;&#21487;&#29992;&#30340;&#25991;&#26412;&#65292;&#24182;&#21253;&#25324;&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.06642</link><description>&lt;p&gt;
KELLMRec: &#30693;&#35782;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06642
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24110;&#21161;&#29983;&#25104;&#30495;&#23454;&#21487;&#29992;&#30340;&#25991;&#26412;&#65292;&#24182;&#21253;&#25324;&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#26088;&#22312;&#34917;&#20805;&#20027;&#27969;&#22522;&#20110;ID&#30340;&#26041;&#27861;&#30340;&#32570;&#22833;&#37096;&#20998;&#12290;&#38543;&#30528;LLM&#30340;&#20852;&#36215;&#65292;&#23427;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#33021;&#21147;&#21644;&#25512;&#29702;&#33021;&#21147;&#20026;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20351;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#25104;&#20026;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20351;&#29992;LLM&#26469;&#22788;&#29702;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#26159;&#19981;&#21487;&#38752;&#21644;&#27425;&#20248;&#30340;&#65292;&#30001;&#20110;&#23384;&#22312;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24110;&#21161;LLM&#29983;&#25104;&#30495;&#23454;&#21487;&#29992;&#30340;&#25991;&#26412;&#12290;&#21463;&#20197;&#19978;&#21160;&#26426;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;LLMRec&#26041;&#27861;&#12290;&#38500;&#20102;&#22312;&#25552;&#31034;&#20013;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#29992;&#20110;&#35757;&#32451;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#20225;&#19994;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06642v1 Announce Type: cross  Abstract: The utilization of semantic information is an important research problem in the field of recommender systems, which aims to complement the missing parts of mainstream ID-based approaches. With the rise of LLM, its ability to act as a knowledge base and its reasoning capability have opened up new possibilities for this research area, making LLM-based recommendation an emerging research direction. However, directly using LLM to process semantic information for recommendation scenarios is unreliable and sub-optimal due to several problems such as hallucination. A promising way to cope with this is to use external knowledge to aid LLM in generating truthful and usable text. Inspired by the above motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to using external knowledge in prompts, the proposed method also includes a knowledge-based contrastive learning scheme for training. Experiments on public datasets and in-enter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#36827;&#34892;&#29289;&#20307;&#26816;&#27979;&#30340;&#33021;&#25928;&#24615;&#65292;&#36890;&#36807;&#24494;&#35843;&#26041;&#27861;&#38477;&#20302;&#20102;&#33021;&#28304;&#28040;&#32791;&#65292;&#24182;&#23545;&#27169;&#22411;&#22312;&#24037;&#19994;&#29615;&#22659;&#25968;&#25454;&#38598;&#19978;&#30340;&#33021;&#28304;&#38656;&#27714;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.06631</link><description>&lt;p&gt;
&#35780;&#20272;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#29289;&#20307;&#26816;&#27979;&#20013;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#33021;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Energy Efficiency of Few-Shot Learning for Object Detection in Industrial Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#36827;&#34892;&#29289;&#20307;&#26816;&#27979;&#30340;&#33021;&#25928;&#24615;&#65292;&#36890;&#36807;&#24494;&#35843;&#26041;&#27861;&#38477;&#20302;&#20102;&#33021;&#28304;&#28040;&#32791;&#65292;&#24182;&#23545;&#27169;&#22411;&#22312;&#24037;&#19994;&#29615;&#22659;&#25968;&#25454;&#38598;&#19978;&#30340;&#33021;&#28304;&#38656;&#27714;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#21457;&#23637;&#30340;&#26102;&#20195;&#65292;&#27169;&#22411;&#24615;&#33021;&#19968;&#30452;&#26159;&#39537;&#21160;&#21019;&#26032;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#23548;&#33268;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#22312;&#24403;&#20195;&#24037;&#19994;&#29615;&#22659;&#20013;&#65292;&#21487;&#25345;&#32493;&#24615;&#21644;&#33021;&#25928;&#24615;&#19968;&#30452;&#26159;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#22240;&#27492;&#38656;&#35201;&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#25968;&#25454;&#39640;&#25928;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24494;&#35843;&#26041;&#27861;&#26469;&#20943;&#36731;&#27169;&#22411;&#35757;&#32451;&#30340;&#36127;&#25285;&#65292;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#65292;&#23558;&#26631;&#20934;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#35843;&#25972;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#38543;&#21518;&#65292;&#23545;&#24320;&#21457;&#30340;&#27169;&#22411;&#22312;&#19981;&#31283;&#23450;&#24037;&#19994;&#29615;&#22659;&#20013;&#30340;&#29289;&#20307;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#33021;&#28304;&#38656;&#27714;&#36827;&#34892;&#20102;&#28145;&#20837;&#26696;&#20363;&#30740;&#31350;&#21644;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#24494;&#35843;&#31574;&#30053;&#20197;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#36741;&#21161;&#35780;&#20272;&#25968;&#25454;&#65292;&#20197;&#21450;&#24615;&#33021;&#21644;&#33021;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06631v1 Announce Type: cross  Abstract: In the ever-evolving era of Artificial Intelligence (AI), model performance has constituted a key metric driving innovation, leading to an exponential growth in model size and complexity. However, sustainability and energy efficiency have been critical requirements during deployment in contemporary industrial settings, necessitating the use of data-efficient approaches such as few-shot learning. In this paper, to alleviate the burden of lengthy model training and minimize energy consumption, a finetuning approach to adapt standard object detection models to downstream tasks is examined. Subsequently, a thorough case study and evaluation of the energy demands of the developed models, applied in object detection benchmark datasets from volatile industrial environments is presented. Specifically, different finetuning strategies as well as utilization of ancillary evaluation data during training are examined, and the trade-off between perf
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#33322;&#31354;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#33258;&#28982;&#29615;&#22659;&#30340;&#30495;&#23454;&#21644;&#34394;&#25311;&#24405;&#20687;&#65292;&#20351;&#29992;&#23494;&#38598;&#27880;&#37322;&#30340;&#35821;&#20041;&#20998;&#21106;&#26631;&#31614;&#21644;&#28145;&#24230;&#22320;&#22270;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#25293;&#25668;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#26862;&#26519;&#24033;&#26816;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06621</link><description>&lt;p&gt;
&#38024;&#23545;&#33322;&#31354;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#20272;&#35745;&#30340;&#26862;&#26519;&#24033;&#26816;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Forest Inspection Dataset for Aerial Semantic Segmentation and Depth Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06621
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#33322;&#31354;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#33258;&#28982;&#29615;&#22659;&#30340;&#30495;&#23454;&#21644;&#34394;&#25311;&#24405;&#20687;&#65292;&#20351;&#29992;&#23494;&#38598;&#27880;&#37322;&#30340;&#35821;&#20041;&#20998;&#21106;&#26631;&#31614;&#21644;&#28145;&#24230;&#22320;&#22270;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#25293;&#25668;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#26862;&#26519;&#24033;&#26816;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20351;&#29992;&#26080;&#20154;&#26426;&#26469;&#30417;&#27979;&#26862;&#26519;&#29615;&#22659;&#30340;&#21464;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#37325;&#37327;&#36731;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#30417;&#35270;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20449;&#24687;&#24182;&#19981;&#36275;&#20197;&#23637;&#31034;&#36275;&#22815;&#30340;&#32454;&#33410;&#65292;&#36825;&#23545;&#20110;&#35780;&#20272;&#26862;&#26519;&#30733;&#20240;&#31243;&#24230;&#26159;&#24517;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#33322;&#31354;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#33258;&#28982;&#29615;&#22659;&#30340;&#30495;&#23454;&#21644;&#34394;&#25311;&#24405;&#24433;&#65292;&#20855;&#26377;&#23494;&#38598;&#27880;&#37322;&#30340;&#35821;&#20041;&#20998;&#21106;&#26631;&#31614;&#21644;&#28145;&#24230;&#22320;&#22270;&#65292;&#25293;&#25668;&#20110;&#19981;&#21516;&#30340;&#29031;&#26126;&#26465;&#20214;&#65292;&#19981;&#21516;&#39640;&#24230;&#21644;&#35760;&#24405;&#35282;&#24230;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#20004;&#20010;&#22810;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#65288;HRNet&#21644;PointFlow&#32593;&#32476;&#65289;&#65292;&#30740;&#31350;&#20102;&#21508;&#31181;&#33719;&#21462;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06621v1 Announce Type: cross  Abstract: Humans use UAVs to monitor changes in forest environments since they are lightweight and provide a large variety of surveillance data. However, their information does not present enough details for understanding the scene which is needed to assess the degree of deforestation. Deep learning algorithms must be trained on large amounts of data to output accurate interpretations, but ground truth recordings of annotated forest imagery are not available. To solve this problem, we introduce a new large aerial dataset for forest inspection which contains both real-world and virtual recordings of natural environments, with densely annotated semantic segmentation labels and depth maps, taken in different illumination conditions, at various altitudes and recording angles. We test the performance of two multi-scale neural networks for solving the semantic segmentation task (HRNet and PointFlow network), studying the impact of the various acquisit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;&#27169;&#22359;&#21644;&#20869;&#37096;&#20020;&#24202;&#36335;&#24452;&#32534;&#30721;&#65292;MedKP&#26694;&#26550;&#22312;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.06611</link><description>&lt;p&gt;
MedKP&#65306;&#21307;&#23398;&#23545;&#35805;&#19982;&#30693;&#35782;&#22686;&#24378;&#19982;&#20020;&#24202;&#36335;&#24452;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06611
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;&#27169;&#22359;&#21644;&#20869;&#37096;&#20020;&#24202;&#36335;&#24452;&#32534;&#30721;&#65292;MedKP&#26694;&#26550;&#22312;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36866;&#24403;&#30340;&#25968;&#25454;&#36873;&#25321;&#21644;&#35757;&#32451;&#25216;&#26415;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#21307;&#23398;&#32771;&#35797;&#21644;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290; &#28982;&#32780;&#65292;LLMs&#22312;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#39046;&#22495;&#30340;&#24212;&#29992;--&#36825;&#20010;&#20219;&#21153;&#26356;&#21152;&#36148;&#36817;&#23454;&#38469;&#21307;&#23398;&#23454;&#36341;--&#21364;&#40092;&#26377;&#25506;&#35752;&#12290; &#36825;&#19968;&#24046;&#36317;&#24402;&#22240;&#20110;LLMs&#21307;&#23398;&#30693;&#35782;&#19981;&#36275;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#21307;&#23398;&#22238;&#22797;&#20986;&#29616;&#19981;&#20934;&#30830;&#21644;&#24187;&#35273;&#20449;&#24687;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21307;&#23398;&#23545;&#35805;&#19982;&#30693;&#35782;&#22686;&#24378;&#21644;&#20020;&#24202;&#36335;&#24452;&#32534;&#30721;&#65288;MedKP&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21307;&#23398;&#30693;&#35782;&#22270;&#21644;&#21307;&#30103;&#23454;&#20307;&#20197;&#21450;&#21307;&#24072;&#34892;&#21160;&#30340;&#20869;&#37096;&#20020;&#24202;&#36335;&#24452;&#32534;&#30721;&#65292;&#38598;&#25104;&#20102;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;&#27169;&#22359;&#12290; &#36890;&#36807;&#20840;&#38754;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#12289;&#30495;&#23454;&#19990;&#30028;&#30340;&#22312;&#32447;&#21307;&#23398;&#21672;&#35810;&#25968;&#25454;&#38598;&#65288;MedDG&#21644;KaMed&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06611v1 Announce Type: cross  Abstract: With appropriate data selection and training techniques, Large Language Models (LLMs) have demonstrated exceptional success in various medical examinations and multiple-choice questions. However, the application of LLMs in medical dialogue generation-a task more closely aligned with actual medical practice-has been less explored. This gap is attributed to the insufficient medical knowledge of LLMs, which leads to inaccuracies and hallucinated information in the generated medical responses. In this work, we introduce the Medical dialogue with Knowledge enhancement and clinical Pathway encoding (MedKP) framework, which integrates an external knowledge enhancement module through a medical knowledge graph and an internal clinical pathway encoding via medical entities and physician actions. Evaluated with comprehensive metrics, our experiments on two large-scale, real-world online medical consultation datasets (MedDG and KaMed) demonstrate 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25512;&#29702;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#21644;&#19982;&#21307;&#29983;&#20915;&#31574;&#36335;&#24452;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.06609</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#31181;&#23376;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06609
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25512;&#29702;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#21644;&#19982;&#21307;&#29983;&#20915;&#31574;&#36335;&#24452;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#25512;&#29702;&#26159;&#21307;&#29983;&#22312;&#35780;&#20272;&#21644;&#31649;&#29702;&#24739;&#32773;&#26102;&#37319;&#29992;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#28041;&#21450;&#24314;&#35758;&#24517;&#35201;&#30340;&#26816;&#26597;&#65292;&#35786;&#26029;&#24739;&#32773;&#30142;&#30149;&#65292;&#24182;&#20915;&#23450;&#36866;&#24403;&#30340;&#27835;&#30103;&#31561;&#12290;&#20934;&#30830;&#30340;&#20020;&#24202;&#25512;&#29702;&#38656;&#35201;&#24191;&#27867;&#30340;&#21307;&#23398;&#30693;&#35782;&#21644;&#20016;&#23500;&#30340;&#20020;&#24202;&#32463;&#39564;&#65292;&#20026;&#21307;&#29983;&#35774;&#32622;&#20102;&#24456;&#39640;&#30340;&#38376;&#27099;&#12290;&#26368;&#36817;&#65292;&#20687;ChatGPT&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#31034;&#20986;&#22312;&#20020;&#24202;&#25512;&#29702;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#38382;&#39064;&#65292;&#32780;LLMs&#30340;&#25512;&#29702;&#36807;&#31243;&#21487;&#33021;&#19982;&#21307;&#29983;&#30340;&#20020;&#24202;&#20915;&#31574;&#36335;&#24452;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06609v1 Announce Type: cross  Abstract: Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patients' diseases, and deciding on appropriate therapies, etc. Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches. Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning. However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians. In this study, we introduce a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#24418;&#36716;&#25442;&#22120;&#30340;&#36328;&#22495;&#21644;&#36328;&#32500;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#27491;&#21017;&#21270;&#36793;&#32536;&#37319;&#26679;&#25439;&#22833;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#21644;&#31616;&#21333;&#30340;&#25237;&#24433;&#20989;&#25968;&#65292;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06601</link><description>&lt;p&gt;
&#22270;&#20687;&#21040;&#22270;&#24418;&#21464;&#25442;&#20013;&#30340;&#36328;&#22495;&#21644;&#36328;&#32500;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-domain and Cross-dimension Learning for Image-to-Graph Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06601
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#24418;&#36716;&#25442;&#22120;&#30340;&#36328;&#22495;&#21644;&#36328;&#32500;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#27491;&#21017;&#21270;&#36793;&#32536;&#37319;&#26679;&#25439;&#22833;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#21644;&#31616;&#21333;&#30340;&#25237;&#24433;&#20989;&#25968;&#65292;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#30340;&#22270;&#20687;&#21040;&#22270;&#24418;&#36716;&#25442;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#22312;&#21333;&#20010;&#27169;&#22411;&#20013;&#35299;&#20915;&#20102;&#30446;&#26631;&#26816;&#27979;&#21644;&#20851;&#31995;&#39044;&#27979;&#12290;&#30001;&#20110;&#36825;&#20010;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24456;&#38590;&#25214;&#21040;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#24471;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#31181;&#25968;&#25454;&#31232;&#30095;&#24615;&#38656;&#35201;&#24314;&#31435;&#31867;&#20284;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#21040;&#22270;&#24418;&#36716;&#25442;&#22120;&#30340;&#36328;&#22495;&#21644;&#36328;&#32500;&#24230;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;(1) &#27491;&#21017;&#21270;&#36793;&#32536;&#37319;&#26679;&#25439;&#22833;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#37319;&#26679;&#26368;&#20339;&#25968;&#37327;&#30340;&#30446;&#26631;&#20851;&#31995;(&#36793;&#32536;)&#65292;(2) &#19968;&#31181;&#22270;&#20687;&#21040;&#22270;&#24418;&#36716;&#25442;&#22120;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#40784;&#19981;&#21516;&#39046;&#22495;&#30340;&#29305;&#24449;&#65292;&#21644;(3) &#19968;&#31181;&#31616;&#21333;&#30340;&#25237;&#24433;&#20989;&#25968;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20108;&#32500;&#36755;&#20837;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#19977;&#32500;&#36716;&#25442;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36328;&#22495;&#21644;&#36328;&#32500;&#24230;&#19979;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06601v1 Announce Type: cross  Abstract: Direct image-to-graph transformation is a challenging task that solves object detection and relationship prediction in a single model. Due to the complexity of this task, large training datasets are rare in many domains, which makes the training of large networks challenging. This data sparsity necessitates the establishment of pre-training strategies akin to the state-of-the-art in computer vision. In this work, we introduce a set of methods enabling cross-domain and cross-dimension transfer learning for image-to-graph transformers. We propose (1) a regularized edge sampling loss for sampling the optimal number of object relationships (edges) across domains, (2) a domain adaptation framework for image-to-graph transformers that aligns features from different domains, and (3) a simple projection function that allows us to pretrain 3D transformers on 2D input data. We demonstrate our method's utility in cross-domain and cross-dimension 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26679;&#24335;&#28508;&#22312;&#27969;&#36827;&#34892;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26679;&#24335;&#28508;&#22312;&#21521;&#37327;&#22312;&#29983;&#25104;&#35270;&#39057;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#26469;&#26816;&#27979;&#20551;&#35270;&#39057;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;StyleGRU&#27169;&#22359;&#21644;&#26679;&#24335;&#27880;&#24847;&#27169;&#22359;&#30340;&#32452;&#21512;&#65292;&#33021;&#26377;&#25928;&#26816;&#27979;&#35270;&#35273;&#21644;&#26102;&#38388;&#24322;&#24120;&#12290;</title><link>https://arxiv.org/abs/2403.06592</link><description>&lt;p&gt;
&#21033;&#29992;&#26679;&#24335;&#28508;&#22312;&#27969;&#26469;&#25512;&#24191;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exploiting Style Latent Flows for Generalizing Deepfake Detection Video Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06592
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26679;&#24335;&#28508;&#22312;&#27969;&#36827;&#34892;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26679;&#24335;&#28508;&#22312;&#21521;&#37327;&#22312;&#29983;&#25104;&#35270;&#39057;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#26469;&#26816;&#27979;&#20551;&#35270;&#39057;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;StyleGRU&#27169;&#22359;&#21644;&#26679;&#24335;&#27880;&#24847;&#27169;&#22359;&#30340;&#32452;&#21512;&#65292;&#33021;&#26377;&#25928;&#26816;&#27979;&#35270;&#35273;&#21644;&#26102;&#38388;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#26512;&#26679;&#24335;&#28508;&#22312;&#21521;&#37327;&#21450;&#20854;&#22312;&#29983;&#25104;&#35270;&#39057;&#30340;&#26102;&#38388;&#21464;&#21270;&#20013;&#24322;&#24120;&#34892;&#20026;&#30340;&#26816;&#27979;&#20551;&#35270;&#39057;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#29983;&#25104;&#30340;&#38754;&#37096;&#35270;&#39057;&#22312;&#26679;&#24335;&#28508;&#22312;&#21521;&#37327;&#30340;&#26102;&#38388;&#21464;&#21270;&#20013;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#29420;&#29305;&#24615;&#65292;&#36825;&#22312;&#29983;&#25104;&#20855;&#26377;&#21508;&#31181;&#38754;&#37096;&#34920;&#24773;&#21644;&#20960;&#20309;&#21464;&#25442;&#30340;&#26102;&#38388;&#31283;&#23450;&#35270;&#39057;&#26102;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;StyleGRU&#27169;&#22359;&#26469;&#34920;&#31034;&#26679;&#24335;&#28508;&#22312;&#21521;&#37327;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26679;&#24335;&#27880;&#24847;&#27169;&#22359;&#65292;&#23558;StyleGRU&#29983;&#25104;&#30340;&#29305;&#24449;&#19982;&#22522;&#20110;&#20869;&#23481;&#30340;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#23545;&#35270;&#35273;&#21644;&#26102;&#38388;&#24322;&#24120;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#21508;&#31181;&#22522;&#20934;&#24773;&#26223;&#19979;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#36328;&#25968;&#25454;&#38598;&#21644;&#36328;&#25805;&#20316;&#24773;&#26223;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06592v1 Announce Type: cross  Abstract: This paper presents a new approach for the detection of fake videos, based on the analysis of style latent vectors and their abnormal behavior in temporal changes in the generated videos. We discovered that the generated facial videos suffer from the temporal distinctiveness in the temporal changes of style latent vectors, which are inevitable during the generation of temporally stable videos with various facial expressions and geometric transformations. Our framework utilizes the StyleGRU module, trained by contrastive learning, to represent the dynamic properties of style latent vectors. Additionally, we introduce a style attention module that integrates StyleGRU-generated features with content-based features, enabling the detection of visual and temporal artifacts. We demonstrate our approach across various benchmark scenarios in deepfake detection, showing its superiority in cross-dataset and cross-manipulation scenarios. Through f
&lt;/p&gt;</description></item><item><title>&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24120;&#35782;&#30693;&#35782;&#26377;&#25928;&#22320;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06586</link><description>&lt;p&gt;
ContextGPT: &#23558;LLMs&#30693;&#35782;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06586
&lt;/p&gt;
&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24120;&#35782;&#30693;&#35782;&#26377;&#25928;&#22320;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#31227;&#21160;&#35745;&#31639;&#20013;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25991;&#29486;&#20013;&#26368;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#38656;&#35201;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#12290;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65288;NeSy&#65289;&#20026;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#23558;&#20851;&#20110;&#20154;&#31867;&#27963;&#21160;&#21450;&#20854;&#21487;&#33021;&#21457;&#29983;&#30340;&#32972;&#26223;&#30340;&#24120;&#35782;&#30693;&#35782;&#27880;&#20837;HAR&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;HAR&#30340;NeSy&#26041;&#27861;&#20381;&#36182;&#20110;&#36923;&#36753;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#26412;&#20307;&#35770;&#65289;&#65292;&#20854;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#32500;&#25252;&#20197;&#25429;&#25417;&#26032;&#27963;&#21160;&#21644;&#19978;&#19979;&#25991;&#38656;&#35201;&#26174;&#33879;&#30340;&#20154;&#21147;&#24037;&#31243;&#21162;&#21147;&#12289;&#25216;&#26415;&#30693;&#35782;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#20851;&#20110;&#20154;&#31867;&#27963;&#21160;&#30340;&#24120;&#35782;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06586v1 Announce Type: cross  Abstract: Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on supervised deep learning models. However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers. Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise. Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20219;&#24847;&#24615;&#33021;&#20998;&#26512;&#26174;&#31034;&#20986;MaxSAT&#26412;&#22320;&#25628;&#32034;&#27714;&#35299;&#22120;&#22312;&#19981;&#21516;&#26102;&#38388;&#39044;&#31639;&#19979;&#30340;&#24615;&#33021;&#20248;&#21155;&#21464;&#21270;</title><link>https://arxiv.org/abs/2403.06568</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#24847;&#24615;&#33021;&#20998;&#26512;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#37197;&#32622;MaxSAT&#26412;&#22320;&#25628;&#32034;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Better Understandings and Configurations in MaxSAT Local Search Solvers via Anytime Performance Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06568
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20219;&#24847;&#24615;&#33021;&#20998;&#26512;&#26174;&#31034;&#20986;MaxSAT&#26412;&#22320;&#25628;&#32034;&#27714;&#35299;&#22120;&#22312;&#19981;&#21516;&#26102;&#38388;&#39044;&#31639;&#19979;&#30340;&#24615;&#33021;&#20248;&#21155;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29992;&#20110;MaxSAT&#38382;&#39064;&#30340;&#27714;&#35299;&#22120;&#65292;&#24182;&#19988;&#35832;&#22914;MaxSAT Evaluations&#20043;&#31867;&#30340;&#22522;&#20934;&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#21488;&#65292;&#29992;&#20110;&#27604;&#36739;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;&#65292;&#20294;&#29616;&#26377;&#30340;&#35780;&#20272;&#36890;&#24120;&#26159;&#22522;&#20110;&#22312;&#32473;&#23450;&#36816;&#34892;&#26102;&#38388;&#39044;&#31639;&#20869;&#33719;&#24471;&#30340;&#26368;&#20339;&#35299;&#30340;&#36136;&#37327;&#26469;&#35780;&#20272;&#30340;&#12290;&#28982;&#32780;&#65292;&#20165;&#32771;&#34385;&#29305;&#23450;&#26102;&#38388;&#39044;&#31639;&#20869;&#26368;&#32456;&#33719;&#24471;&#30340;&#35299;&#21487;&#33021;&#20250;&#38480;&#21046;&#25105;&#20204;&#29702;&#35299;&#27714;&#35299;&#22120;&#22312;&#25910;&#25947;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#32463;&#39564;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#21487;&#29992;&#20110;&#27604;&#36739;MaxSAT&#26412;&#22320;&#25628;&#32034;&#27714;&#35299;&#22120;&#22312;&#22810;&#20010;&#38382;&#39064;&#23454;&#20363;&#21644;&#19981;&#21516;&#26102;&#38388;&#39044;&#31639;&#19979;&#30340;&#20219;&#24847;&#24615;&#33021;&#12290;&#35780;&#20272;&#25581;&#31034;&#20102;&#27714;&#35299;&#22120;&#24615;&#33021;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#31034;&#20986;&#27714;&#35299;&#22120;&#30340;&#65288;&#19981;&#65289;&#20248;&#21183;&#38543;&#30528;&#19981;&#21516;&#36816;&#34892;&#26102;&#38388;&#30340;&#35843;&#25972;&#12290;&#36825;&#39033;&#24037;&#20316;&#36824;&#23637;&#31034;&#20102;&#23450;&#37327;&#21644;&#39640;&#26041;&#24046;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06568v1 Announce Type: new  Abstract: Though numerous solvers have been proposed for the MaxSAT problem, and the benchmark environment such as MaxSAT Evaluations provides a platform for the comparison of the state-of-the-art solvers, existing assessments were usually evaluated based on the quality, e.g., fitness, of the best-found solutions obtained within a given running time budget. However, concerning solely the final obtained solutions regarding specific time budgets may restrict us from comprehending the behavior of the solvers along the convergence process. This paper demonstrates that Empirical Cumulative Distribution Functions can be used to compare MaxSAT local search solvers' anytime performance across multiple problem instances and various time budgets. The assessment reveals distinctions in solvers' performance and displays that the (dis)advantages of solvers adjust along different running times. This work also exhibits that the quantitative and high variance ass
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24418;&#24577;&#29305;&#23450;&#30340;IHC&#26579;&#33394;&#20998;&#35299;&#20026;&#21333;&#29420;&#30340;&#22270;&#20687;&#36890;&#36947;&#65292;&#29983;&#25104;&#20102;in-silico&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#22270;&#20687;&#65292;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#32454;&#32990;&#26680;&#20998;&#21106;&#27169;&#22411;&#26102;&#22312;&#36136;&#37327;&#21644;&#25968;&#37327;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.06545</link><description>&lt;p&gt;
ReStainGAN:&#21033;&#29992;IHC&#21040;IF&#26579;&#33394;&#22495;&#36716;&#25442;&#36827;&#34892;in-silico&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06545
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24418;&#24577;&#29305;&#23450;&#30340;IHC&#26579;&#33394;&#20998;&#35299;&#20026;&#21333;&#29420;&#30340;&#22270;&#20687;&#36890;&#36947;&#65292;&#29983;&#25104;&#20102;in-silico&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#22270;&#20687;&#65292;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#32454;&#32990;&#26680;&#20998;&#21106;&#27169;&#22411;&#26102;&#22312;&#36136;&#37327;&#21644;&#25968;&#37327;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06545v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#36890;&#36807;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#25193;&#23637;&#29616;&#26377;&#27880;&#37322;&#30340;&#23454;&#29992;&#24615;&#21040;&#20855;&#26377;&#19981;&#21516;&#26579;&#33394;&#27169;&#24335;&#30340;&#26032;&#39046;&#22495;&#20013;&#65292;&#21487;&#20197;&#21019;&#36896;in-silico&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#36825;&#26377;&#21487;&#33021;&#22823;&#24133;&#38477;&#20302;&#26500;&#24314;&#35757;&#32451;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#22411;&#19988;&#20687;&#32032;&#31934;&#30830;&#30340;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20813;&#30123;&#33639;&#20809;&#65288;IF&#65289;&#22270;&#20687;&#20013;&#23558;&#24418;&#24577;&#29305;&#23450;&#30340;IHC&#26579;&#33394;&#20998;&#31163;&#25104;&#21333;&#29420;&#30340;&#22270;&#20687;&#36890;&#36947;&#65292;&#29983;&#25104;in-silico&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#22270;&#20687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21019;&#24314;&#30340;in-silico&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#35757;&#32451;&#32454;&#32990;&#26680;&#20998;&#21106;&#27169;&#22411;&#22312;&#36136;&#37327;&#21644;&#25968;&#37327;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06545v1 Announce Type: cross  Abstract: The creation of in-silico datasets can expand the utility of existing annotations to new domains with different staining patterns in computational pathology. As such, it has the potential to significantly lower the cost associated with building large and pixel precise datasets needed to train supervised deep learning models. We propose a novel approach for the generation of in-silico immunohistochemistry (IHC) images by disentangling morphology specific IHC stains into separate image channels in immunofluorescence (IF) images. The proposed approach qualitatively and quantitatively outperforms baseline methods as proven by training nucleus segmentation models on the created in-silico datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DeLAMA&#65292;&#19968;&#31181;&#20855;&#26377;&#21160;&#24577;&#21327;&#20316;&#22270;&#30340;&#20998;&#25955;&#22810;&#26234;&#33021;&#20307;&#32456;&#36523;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#20027;&#35782;&#21035;&#21327;&#20316;&#20851;&#31995;&#21644;&#36866;&#24212;&#21160;&#24577;&#20219;&#21153;&#26469;&#22686;&#24378;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.06535</link><description>&lt;p&gt;
&#20998;&#25955;&#21644;&#32456;&#36523;&#33258;&#36866;&#24212;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DeLAMA&#65292;&#19968;&#31181;&#20855;&#26377;&#21160;&#24577;&#21327;&#20316;&#22270;&#30340;&#20998;&#25955;&#22810;&#26234;&#33021;&#20307;&#32456;&#36523;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#20027;&#35782;&#21035;&#21327;&#20316;&#20851;&#31995;&#21644;&#36866;&#24212;&#21160;&#24577;&#20219;&#21153;&#26469;&#22686;&#24378;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#21644;&#32456;&#36523;&#33258;&#36866;&#24212;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23398;&#20064;&#26088;&#22312;&#22686;&#24378;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#19981;&#38656;&#35201;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#38543;&#26102;&#38388;&#35299;&#20915;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#21327;&#20316;&#65292;&#26234;&#33021;&#20307;&#24212;&#35813;&#65306;i) &#22312;&#20998;&#25955;&#30340;&#26041;&#24335;&#19979;&#33258;&#20027;&#35782;&#21035;&#26377;&#30410;&#30340;&#21327;&#20316;&#20851;&#31995;&#65307;ii) &#36866;&#24212;&#21160;&#24577;&#21464;&#21270;&#30340;&#20219;&#21153;&#35266;&#23519;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DeLAMA&#65292;&#19968;&#31181;&#20855;&#26377;&#21160;&#24577;&#21327;&#20316;&#22270;&#30340;&#20998;&#25955;&#22810;&#26234;&#33021;&#20307;&#32456;&#36523;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#20419;&#36827;&#33258;&#20027;&#21327;&#20316;&#20851;&#31995;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#65292;&#28040;&#38500;&#20102;&#23545;&#22806;&#37096;&#20808;&#39564;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#20419;&#36827;&#36866;&#24212;&#21160;&#24577;&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23384;&#20648;&#21333;&#20803;&#26469;&#25429;&#33719;&#26234;&#33021;&#20307;&#31215;&#32047;&#30340;&#23398;&#20064;&#21382;&#21490;&#21644;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#38480;&#30340;&#23384;&#20648;&#28040;&#32791;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#31995;&#32479;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06535v1 Announce Type: cross  Abstract: Decentralized and lifelong-adaptive multi-agent collaborative learning aims to enhance collaboration among multiple agents without a central server, with each agent solving varied tasks over time. To achieve efficient collaboration, agents should: i) autonomously identify beneficial collaborative relationships in a decentralized manner; and ii) adapt to dynamically changing task observations. In this paper, we propose DeLAMA, a decentralized multi-agent lifelong collaborative learning algorithm with dynamic collaboration graphs. To promote autonomous collaboration relationship learning, we propose a decentralized graph structure learning algorithm, eliminating the need for external priors. To facilitate adaptation to dynamic tasks, we design a memory unit to capture the agents' accumulated learning history and knowledge, while preserving finite storage consumption. To further augment the system's expressive capabilities and computation
&lt;/p&gt;</description></item><item><title>SARDet-100K&#26159;&#31532;&#19968;&#20010;COCO&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035;SAR&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;SAR&#29289;&#20307;&#26816;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#26174;&#33879;&#24046;&#24322;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.06534</link><description>&lt;p&gt;
SARDet-100K: &#38754;&#21521;&#22823;&#35268;&#27169;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798; SAR &#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#28304;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06534
&lt;/p&gt;
&lt;p&gt;
SARDet-100K&#26159;&#31532;&#19968;&#20010;COCO&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035;SAR&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;SAR&#29289;&#20307;&#26816;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#26174;&#33879;&#24046;&#24322;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#29289;&#20307;&#26816;&#27979;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#20854;&#19981;&#21487;&#26367;&#20195;&#30340;&#20840;&#22825;&#20505;&#25104;&#20687;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#20027;&#35201;&#21253;&#21547; &lt;2K &#24352;&#22270;&#20687;&#65292;&#19988;&#20165;&#21253;&#21547;&#21333;&#31867;&#21035;&#29289;&#20307;&#65289;&#21644;&#28304;&#20195;&#30721;&#19981;&#21487;&#35775;&#38382;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#38024;&#23545;&#22823;&#35268;&#27169; SAR &#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#28304;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598; SARDet-100K &#32467;&#26524;&#26159;&#23545; 10 &#20010;&#29616;&#26377; SAR &#26816;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#20837;&#35843;&#30740;&#12289;&#25910;&#38598;&#21644;&#26631;&#20934;&#21270;&#30340;&#20135;&#29289;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SARDet-100K &#26159;&#26377;&#21490;&#20197;&#26469;&#31532;&#19968;&#20010;&#36798;&#21040; COCO &#27700;&#24179;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035; SAR &#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#20973;&#20511;&#36825;&#19968;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#24182;&#25581;&#31034;&#20102; SAR &#29289;&#20307;&#26816;&#27979;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06534v1 Announce Type: cross  Abstract: Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising &lt;2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#20840;&#38754;&#36816;&#33829;&#25104;&#26412;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20248;&#21270;&#33258;&#20027;&#21345;&#36710;&#30340;&#25112;&#26415;&#20915;&#31574;&#65292;&#23558;&#39640;&#32423;&#20915;&#31574;&#19982;&#20302;&#32423;&#25511;&#21046;&#20998;&#31163;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#25216;&#24039;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06524</link><description>&lt;p&gt;
&#22522;&#20110;&#20840;&#38754;&#36816;&#33829;&#25104;&#26412;&#22870;&#21169;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#33258;&#20027;&#21345;&#36710;&#25112;&#26415;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06524
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20840;&#38754;&#36816;&#33829;&#25104;&#26412;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20248;&#21270;&#33258;&#20027;&#21345;&#36710;&#30340;&#25112;&#26415;&#20915;&#31574;&#65292;&#23558;&#39640;&#32423;&#20915;&#31574;&#19982;&#20302;&#32423;&#25511;&#21046;&#20998;&#31163;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#25216;&#24039;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#33258;&#20027;&#21345;&#36710;&#30340;&#25112;&#26415;&#20915;&#31574;&#21046;&#23450;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#39640;&#36895;&#20844;&#36335;&#22330;&#26223;&#20013;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#21644;&#21464;&#36947;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#21644;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#20302;&#32423;&#25511;&#21046;&#22120;&#20043;&#38388;&#20998;&#31163;&#39640;&#32423;&#20915;&#31574;&#36807;&#31243;&#21644;&#20302;&#32423;&#25511;&#21046;&#21160;&#20316;&#26159;&#26377;&#30410;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#22312;&#21345;&#36710;&#30340;&#20840;&#38754;&#36816;&#33829;&#25104;&#26412;&#65288;TCOP&#65289;&#20026;&#22522;&#30784;&#30340;&#22810;&#30446;&#26631;&#22870;&#21169;&#20989;&#25968;&#20248;&#21270;&#24615;&#33021;&#30340;&#19981;&#21516;&#26041;&#27861;&#65307;&#36890;&#36807;&#20026;&#22870;&#21169;&#20998;&#37327;&#28155;&#21152;&#26435;&#37325;&#65292;&#36890;&#36807;&#23545;&#22870;&#21169;&#20998;&#37327;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06524v1 Announce Type: cross  Abstract: We develop a deep reinforcement learning framework for tactical decision making in an autonomous truck, specifically for Adaptive Cruise Control (ACC) and lane change maneuvers in a highway scenario. Our results demonstrate that it is beneficial to separate high-level decision-making processes and low-level control actions between the reinforcement learning agent and the low-level controllers based on physical models. In the following, we study optimizing the performance with a realistic and multi-objective reward function based on Total Cost of Operation (TCOP) of the truck using different approaches; by adding weights to reward components, by normalizing the reward components and by using curriculum learning techniques.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#24120;&#35782;&#30693;&#35782;&#26469;&#24110;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#31995;&#32479;&#29702;&#35299;&#21629;&#21517;&#23454;&#20307;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25551;&#36848;&#22270;&#20687;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.06520</link><description>&lt;p&gt;
&#22914;&#20309;&#29702;&#35299;&#21629;&#21517;&#23454;&#20307;&#65306;&#22312;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#20013;&#20351;&#29992;&#24120;&#35782;
&lt;/p&gt;
&lt;p&gt;
How to Understand Named Entities: Using Common Sense for News Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#24120;&#35782;&#30693;&#35782;&#26469;&#24110;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#31995;&#32479;&#29702;&#35299;&#21629;&#21517;&#23454;&#20307;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25551;&#36848;&#22270;&#20687;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26088;&#22312;&#20351;&#29992;&#26032;&#38395;&#25991;&#31456;&#20027;&#20307;&#25551;&#36848;&#22270;&#20687;&#12290;&#23427;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#19968;&#32452;&#26816;&#27979;&#21040;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#20154;&#29289;&#12289;&#32452;&#32455;&#21644;&#22320;&#28857;&#12290;&#26412;&#25991;&#21033;&#29992;&#24120;&#35782;&#30693;&#35782;&#26469;&#29702;&#35299;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#36890;&#36807;&#8220;&#29702;&#35299;&#8221;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#23558;&#26032;&#38395;&#20869;&#23481;&#19982;&#24120;&#35782;&#32852;&#31995;&#36215;&#26469;&#65292;&#24110;&#21161;&#20195;&#29702;&#20154;&#21306;&#20998;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#21033;&#29992;&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#22806;&#30340;&#35789;&#35821;&#25551;&#36848;&#21629;&#21517;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06520v1 Announce Type: cross  Abstract: News captioning aims to describe an image with its news article body as input. It greatly relies on a set of detected named entities, including real-world people, organizations, and places. This paper exploits commonsense knowledge to understand named entities for news captioning. By ``understand'', we mean correlating the news content with common sense in the wild, which helps an agent to 1) distinguish semantically similar named entities and 2) describe named entities using words outside of training corpora. Our approach consists of three modules: (a) Filter Module aims to clarify the common sense concerning a named entity from two aspects: what does it mean? and what is it related to?, which divide the common sense into explanatory knowledge and relevant knowledge, respectively. (b) Distinguish Module aggregates explanatory knowledge from node-degree, dependency, and distinguish three aspects to distinguish semantically similar name
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ActGen&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#35757;&#32451;&#24863;&#30693;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#22270;&#20687;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;ActGen&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#30340;&#29702;&#24565;&#65292;&#29983;&#25104;&#31867;&#20284;&#20110;&#25361;&#25112;&#24615;&#25110;&#34987;&#35823;&#20998;&#31867;&#26679;&#26412;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#35757;&#32451;&#38598;&#20013;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06517</link><description>&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#30340;&#20027;&#21160;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Active Generation for Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06517
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ActGen&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#35757;&#32451;&#24863;&#30693;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#22270;&#20687;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;ActGen&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#30340;&#29702;&#24565;&#65292;&#29983;&#25104;&#31867;&#20284;&#20110;&#25361;&#25112;&#24615;&#25110;&#34987;&#35823;&#20998;&#31867;&#26679;&#26412;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#35757;&#32451;&#38598;&#20013;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#19981;&#26029;&#22686;&#24378;&#30340;&#33021;&#21147;&#31361;&#26174;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#35201;&#27714;&#29983;&#25104;&#30340;&#22270;&#20687;&#25968;&#37327;&#36828;&#36828;&#36229;&#36807;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#32780;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#21482;&#26377;&#26497;&#23567;&#30340;&#25913;&#36827;&#12290;&#36825;&#31181;&#35745;&#31639;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#36807;&#31243;&#38459;&#30861;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#19987;&#27880;&#20110;&#27169;&#22411;&#30340;&#20855;&#20307;&#38656;&#27714;&#21644;&#29305;&#24449;&#26469;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;ActGen&#20197;&#20027;&#21160;&#23398;&#20064;&#20026;&#20013;&#24515;&#21407;&#21017;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#38024;&#23545;&#35757;&#32451;&#24863;&#30693;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;&#23427;&#26088;&#22312;&#21019;&#24314;&#31867;&#20284;&#20110;&#24403;&#21069;&#27169;&#22411;&#36935;&#21040;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#25110;&#34987;&#35823;&#20998;&#31867;&#26679;&#26412;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#36825;&#20123;&#29983;&#25104;&#30340;&#22270;&#20687;&#32435;&#20837;&#35757;&#32451;&#38598;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06517v1 Announce Type: cross  Abstract: Recently, the growing capabilities of deep generative models have underscored their potential in enhancing image classification accuracy. However, existing methods often demand the generation of a disproportionately large number of images compared to the original dataset, while having only marginal improvements in accuracy. This computationally expensive and time-consuming process hampers the practicality of such approaches. In this paper, we propose to address the efficiency of image generation by focusing on the specific needs and characteristics of the model. With a central tenet of active learning, our method, named ActGen, takes a training-aware approach to image generation. It aims to create images akin to the challenging or misclassified samples encountered by the current model and incorporates these generated images into the training set to augment model performance. ActGen introduces an attentive image guidance technique, usin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#22270;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#21033;&#29992;GNN&#26469;&#36827;&#34892;&#39640;&#25928;&#30340;&#22270;&#32534;&#36753;&#36317;&#31163;&#35745;&#31639;&#65292;&#36890;&#36807;&#22330;&#26223;&#22270;&#24418;&#24335;&#65292;&#32469;&#36807;NP&#22256;&#38590;&#30340;&#22270;&#30456;&#20284;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#26356;&#20855;&#25551;&#36848;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.06514</link><description>&lt;p&gt;
&#26500;&#24314;&#25968;&#25454;&#32467;&#26500;&#65306;&#36208;&#21521;&#35821;&#20041;&#22270;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Structure Your Data: Towards Semantic Graph Counterfactuals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06514
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#22270;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#21033;&#29992;GNN&#26469;&#36827;&#34892;&#39640;&#25928;&#30340;&#22270;&#32534;&#36753;&#36317;&#31163;&#35745;&#31639;&#65292;&#36890;&#36807;&#22330;&#26223;&#22270;&#24418;&#24335;&#65292;&#32469;&#36807;NP&#22256;&#38590;&#30340;&#22270;&#30456;&#20284;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#26356;&#20855;&#25551;&#36848;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#26159;&#32771;&#34385;&#26367;&#20195;&#24773;&#26223;&#20197;&#20102;&#35299;&#21738;&#20123;&#39640;&#32423;&#35821;&#20041;&#29305;&#24449;&#23545;&#29305;&#23450;&#27169;&#22411;&#39044;&#27979;&#20570;&#20986;&#20102;&#36129;&#29486;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20276;&#38543;&#36755;&#20837;&#25968;&#25454;&#30340;&#35821;&#20041;&#22270;&#30340;CEs&#65292;&#20197;&#23454;&#29616;&#26356;&#20855;&#25551;&#36848;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#35299;&#37322;&#12290;&#20511;&#37492;&#26368;&#20808;&#36827;&#30340;&#27010;&#24565;&#23581;&#35797;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#21033;&#29992;GNN&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#22270;&#32534;&#36753;&#36317;&#31163;&#65288;GED&#65289;&#35745;&#31639;&#12290;&#25105;&#20204;&#23558;&#22270;&#24418;&#32467;&#26500;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#65292;&#23558;&#22270;&#20687;&#34920;&#31034;&#20026;&#22330;&#26223;&#22270;&#65292;&#24182;&#33719;&#24471;&#23427;&#20204;&#30340;GNN&#23884;&#20837;&#20197;&#32469;&#36807;&#35299;&#20915;&#25152;&#26377;&#36755;&#20837;&#23545;&#30340;NP&#22256;&#38590;&#22270;&#30456;&#20284;&#24615;&#38382;&#39064;&#65292;&#36825;&#26159;CE&#35745;&#31639;&#36807;&#31243;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#38590;&#24230;&#21644;&#35821;&#20041;&#27880;&#37322;&#21487;&#29992;&#24615;&#30340;&#22522;&#20934;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#12290;&#22312;&#21508;&#31181;&#20998;&#31867;&#22120;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;CEs&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06514v1 Announce Type: cross  Abstract: Counterfactual explanations (CEs) based on concepts are explanations that consider alternative scenarios to understand which high-level semantic features contributed to particular model predictions. In this work, we propose CEs based on the semantic graphs accompanying input data to achieve more descriptive, accurate, and human-aligned explanations. Building upon state-of-the-art (SoTA) conceptual attempts, we adopt a model-agnostic edit-based approach and introduce leveraging GNNs for efficient Graph Edit Distance (GED) computation. With a focus on the visual domain, we represent images as scene graphs and obtain their GNN embeddings to bypass solving the NP-hard graph similarity problem for all input pairs, an integral part of the CE computation process. We apply our method to benchmark and real-world datasets with varying difficulty and availability of semantic annotations. Testing on diverse classifiers, we find that our CEs outper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25490;&#21015;&#36136;&#37327;&#20989;&#25968;&#30340;&#21542;&#23450;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#25910;&#25947;&#24615;&#65292;&#30740;&#31350;&#20102;&#21542;&#23450;&#25805;&#20316;&#21518;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#30456;&#20284;&#24615;&#21464;&#21270;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.06483</link><description>&lt;p&gt;
&#25490;&#21015;&#36136;&#37327;&#20989;&#25968;&#30340;&#21542;&#23450;
&lt;/p&gt;
&lt;p&gt;
The negation of permutation mass function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25490;&#21015;&#36136;&#37327;&#20989;&#25968;&#30340;&#21542;&#23450;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#25910;&#25947;&#24615;&#65292;&#30740;&#31350;&#20102;&#21542;&#23450;&#25805;&#20316;&#21518;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#30456;&#20284;&#24615;&#21464;&#21270;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21542;&#23450;&#26159;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#35270;&#35282;&#12290;&#29616;&#26377;&#30340;&#21542;&#23450;&#26041;&#27861;&#20027;&#35201;&#24212;&#29992;&#20110;&#27010;&#29575;&#35770;&#12289;&#35777;&#25454;&#29702;&#35770;&#21644;&#22797;&#26434;&#35777;&#25454;&#29702;&#35770;&#12290;&#20316;&#20026;&#35777;&#25454;&#29702;&#35770;&#30340;&#19968;&#31181;&#27867;&#21270;&#65292;&#38543;&#26426;&#25490;&#21015;&#38598;&#21512;&#29702;&#35770;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#34920;&#31034;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#21542;&#23450;&#30340;&#27010;&#24565;&#24212;&#29992;&#20110;&#38543;&#26426;&#25490;&#21015;&#38598;&#21512;&#29702;&#35770;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25490;&#21015;&#36136;&#37327;&#20989;&#25968;&#30340;&#21542;&#23450;&#12290;&#27492;&#22806;&#65292;&#22312;&#21542;&#23450;&#36807;&#31243;&#20013;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#21542;&#23450;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#30740;&#31350;&#20102;&#27599;&#20010;&#21542;&#23450;&#25805;&#20316;&#21518;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#30456;&#20284;&#24615;&#30340;&#36235;&#21183;&#12290;&#25968;&#20540;&#20363;&#23376;&#34987;&#29992;&#26469;&#35777;&#26126;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06483v1 Announce Type: new  Abstract: Negation is a important perspective of knowledge representation. Existing negation methods are mainly applied in probability theory, evidence theory and complex evidence theory. As a generalization of evidence theory, random permutation sets theory may represent information more precisely. However, how to apply the concept of negation to random permutation sets theory has not been studied. In this paper, the negation of permutation mass function is proposed. Moreover, in the negation process, the convergence of proposed negation method is verified. The trends of uncertainty and dissimilarity after each negation operation are investigated. Numerical examples are used to demonstrate the rationality of the proposed method.
&lt;/p&gt;</description></item><item><title>Ada-Tracker&#21033;&#29992;&#20809;&#27969;&#25429;&#25417;&#20687;&#32032;&#32423;&#32452;&#32455;&#21464;&#24418;&#24182;&#33258;&#36866;&#24212;&#32416;&#27491;&#36319;&#36394;&#27169;&#26495;&#65292;&#21516;&#26102;&#32467;&#21512;&#24103;&#38388;&#21305;&#37197;&#21644;&#33258;&#36866;&#24212;&#27169;&#26495;&#21305;&#37197;&#65292;&#35299;&#20915;&#20102;&#22312;&#25163;&#26415;&#22330;&#26223;&#20013;&#36719;&#32452;&#32455;&#36319;&#36394;&#38754;&#20020;&#30340;&#24418;&#29366;&#21644;&#22806;&#35266;&#25913;&#21464;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06479</link><description>&lt;p&gt;
Ada-Tracker&#65306;&#36890;&#36807;&#24103;&#38388;&#21644;&#33258;&#36866;&#24212;&#27169;&#26495;&#21305;&#37197;&#36827;&#34892;&#36719;&#32452;&#32455;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Ada-Tracker: Soft Tissue Tracking via Inter-Frame and Adaptive-Template Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06479
&lt;/p&gt;
&lt;p&gt;
Ada-Tracker&#21033;&#29992;&#20809;&#27969;&#25429;&#25417;&#20687;&#32032;&#32423;&#32452;&#32455;&#21464;&#24418;&#24182;&#33258;&#36866;&#24212;&#32416;&#27491;&#36319;&#36394;&#27169;&#26495;&#65292;&#21516;&#26102;&#32467;&#21512;&#24103;&#38388;&#21305;&#37197;&#21644;&#33258;&#36866;&#24212;&#27169;&#26495;&#21305;&#37197;&#65292;&#35299;&#20915;&#20102;&#22312;&#25163;&#26415;&#22330;&#26223;&#20013;&#36719;&#32452;&#32455;&#36319;&#36394;&#38754;&#20020;&#30340;&#24418;&#29366;&#21644;&#22806;&#35266;&#25913;&#21464;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#32452;&#32455;&#36319;&#36394;&#23545;&#35745;&#31639;&#26426;&#36741;&#21161;&#25163;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#27169;&#26495;&#21644;&#35270;&#39057;&#20013;&#25552;&#21462;&#36776;&#21035;&#29305;&#24449;&#26469;&#24674;&#22797;&#30456;&#24212;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#22312;&#25163;&#26415;&#22330;&#26223;&#20013;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#32452;&#32455;&#22312;&#25972;&#20010;&#25163;&#26415;&#36807;&#31243;&#20013;&#20250;&#25913;&#21464;&#24418;&#29366;&#21644;&#22806;&#35266;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#20809;&#27969;&#26469;&#33258;&#28982;&#25429;&#25417;&#20687;&#32032;&#32423;&#32452;&#32455;&#21464;&#24418;&#65292;&#24182;&#33258;&#36866;&#24212;&#24615;&#22320;&#32416;&#27491;&#36319;&#36394;&#27169;&#26495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23454;&#29616;&#19968;&#20010;&#24103;&#38388;&#21305;&#37197;&#26426;&#21046;&#65292;&#22522;&#20110;&#36830;&#32493;&#24103;&#20043;&#38388;&#30340;&#20809;&#27969;&#25552;&#21462;&#19968;&#20010;&#31895;&#30053;&#24863;&#20852;&#36259;&#21306;&#22495;&#12290;&#20026;&#20102;&#36866;&#24212;&#22806;&#35266;&#21464;&#21270;&#21644;&#20943;&#36731;&#28418;&#31227;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27169;&#26495;&#21305;&#37197;&#26041;&#27861;&#65292;&#26681;&#25454;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#26356;&#26032;&#36319;&#36394;&#27169;&#26495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;Ada-Tracker&#36890;&#36807;&#25429;&#25417;&#23616;&#37096;&#21464;&#24418;&#26469;&#20139;&#21463;&#30701;&#26399;&#21160;&#24577;&#24314;&#27169;&#21644;&#38271;&#26399;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06479v1 Announce Type: cross  Abstract: Soft tissue tracking is crucial for computer-assisted interventions. Existing approaches mainly rely on extracting discriminative features from the template and videos to recover corresponding matches. However, it is difficult to adopt these techniques in surgical scenes, where tissues are changing in shape and appearance throughout the surgery. To address this problem, we exploit optical flow to naturally capture the pixel-wise tissue deformations and adaptively correct the tracked template. Specifically, we first implement an inter-frame matching mechanism to extract a coarse region of interest based on optical flow from consecutive frames. To accommodate appearance change and alleviate drift, we then propose an adaptive-template matching method, which updates the tracked template based on the reliability of the estimates. Our approach, Ada-Tracker, enjoys both short-term dynamics modeling by capturing local deformations and long-ter
&lt;/p&gt;</description></item><item><title>RL-MSA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#32447;&#36335;&#20844;&#20132;&#36710;&#35843;&#24230;&#26041;&#27861;&#65292;&#23558;&#22810;&#32447;&#36335;&#20844;&#20132;&#36710;&#35843;&#24230;&#38382;&#39064;&#24314;&#27169;&#20026;MDP&#65292;&#39318;&#27425;&#22312;&#31163;&#32447;&#38454;&#27573;&#23558;&#30452;&#34892;&#36710;&#20915;&#31574;&#25972;&#21512;&#20837;&#20844;&#20132;&#36710;&#36873;&#25321;&#20915;&#31574;&#65292;&#26377;&#25928;&#31616;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#32447;&#38454;&#27573;&#36890;&#36807;&#26102;&#38388;&#31383;&#21475;&#26426;&#21046;&#36827;&#34892;&#30452;&#34892;&#36710;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.06466</link><description>&lt;p&gt;
RL-MSA&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#32447;&#36335;&#20844;&#20132;&#36710;&#35843;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RL-MSA: a Reinforcement Learning-based Multi-line bus Scheduling Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06466
&lt;/p&gt;
&lt;p&gt;
RL-MSA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#32447;&#36335;&#20844;&#20132;&#36710;&#35843;&#24230;&#26041;&#27861;&#65292;&#23558;&#22810;&#32447;&#36335;&#20844;&#20132;&#36710;&#35843;&#24230;&#38382;&#39064;&#24314;&#27169;&#20026;MDP&#65292;&#39318;&#27425;&#22312;&#31163;&#32447;&#38454;&#27573;&#23558;&#30452;&#34892;&#36710;&#20915;&#31574;&#25972;&#21512;&#20837;&#20844;&#20132;&#36710;&#36873;&#25321;&#20915;&#31574;&#65292;&#26377;&#25928;&#31616;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#32447;&#38454;&#27573;&#36890;&#36807;&#26102;&#38388;&#31383;&#21475;&#26426;&#21046;&#36827;&#34892;&#30452;&#34892;&#36710;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32447;&#36335;&#20844;&#20132;&#36710;&#35843;&#24230;&#38382;&#39064;&#65288;MLBSP&#65289;&#23545;&#20110;&#33410;&#30465;&#20844;&#20132;&#20844;&#21496;&#36816;&#33829;&#25104;&#26412;&#21644;&#20445;&#35777;&#20056;&#23458;&#26381;&#21153;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20197;&#31163;&#32447;&#26041;&#24335;&#29983;&#25104;&#20844;&#20132;&#36710;&#35843;&#24230;&#26041;&#26696;&#65292;&#28982;&#21518;&#26681;&#25454;&#35813;&#26041;&#26696;&#23433;&#25490;&#20844;&#20132;&#36710;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#35832;&#22914;&#20132;&#36890;&#25317;&#22581;&#20043;&#31867;&#30340;&#19981;&#30830;&#23450;&#20107;&#20214;&#32463;&#24120;&#21457;&#29983;&#65292;&#36825;&#21487;&#33021;&#20351;&#20107;&#20808;&#30830;&#23450;&#30340;&#20844;&#20132;&#36710;&#35843;&#24230;&#26041;&#26696;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#26412;&#25991;&#23558;MLBSP&#24314;&#27169;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#32447;&#36335;&#20844;&#20132;&#36710;&#35843;&#24230;&#26041;&#27861;&#65288;RL-MSA&#65289;&#65292;&#29992;&#20110;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#38454;&#27573;&#36827;&#34892;&#20844;&#20132;&#36710;&#35843;&#24230;&#12290;&#22312;&#31163;&#32447;&#38454;&#27573;&#65292;&#23558;&#30452;&#34892;&#36710;&#20915;&#31574;&#25972;&#21512;&#21040;&#39318;&#27425;&#20986;&#29616;&#30340;&#20844;&#20132;&#36710;&#36873;&#25321;&#20915;&#31574;&#20013;&#65292;&#20197;&#31616;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#22312;&#32447;&#38454;&#27573;&#65292;&#36890;&#36807;&#22522;&#20110;&#31163;&#32447;&#38454;&#27573;&#23398;&#20250;&#30340;&#31574;&#30053;&#36827;&#34892;&#30452;&#34892;&#36710;&#20915;&#31574;&#65292;&#37319;&#29992;&#26102;&#38388;&#31383;&#21475;&#26426;&#21046;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#20010;&#26032;&#30340;&#26377;&#29992;&#29366;&#24577;&#29305;&#24449;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06466v1 Announce Type: cross  Abstract: Multiple Line Bus Scheduling Problem (MLBSP) is vital to save operational cost of bus company and guarantee service quality for passengers. Existing approaches typically generate a bus scheduling scheme in an offline manner and then schedule buses according to the scheme. In practice, uncertain events such as traffic congestion occur frequently, which may make the pre-determined bus scheduling scheme infeasible. In this paper, MLBSP is modeled as a Markov Decision Process (MDP). A Reinforcement Learning-based Multi-line bus Scheduling Approach (RL-MSA) is proposed for bus scheduling at both the offline and online phases. At the offline phase, deadhead decision is integrated into bus selection decision for the first time to simplify the learning problem. At the online phase, deadhead decision is made through a time window mechanism based on the policy learned at the offline phase. We develop several new and useful state features includi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;RecAI&#65292;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#30340;&#23454;&#29992;&#24037;&#20855;&#21253;&#65292;&#26032;&#19968;&#20195;&#30001;LLMs&#36171;&#33021;&#30340;&#25512;&#33616;&#31995;&#32479;&#23558;&#26356;&#21152;&#22810;&#25165;&#22810;&#33402;&#12289;&#21487;&#35299;&#37322;&#12289;&#23545;&#35805;&#24335;&#21644;&#21487;&#25511;&#65292;&#20026;&#26356;&#26234;&#33021;&#21644;&#29992;&#25143;&#20013;&#24515;&#30340;&#25512;&#33616;&#20307;&#39564;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.06465</link><description>&lt;p&gt;
RecAI&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#19979;&#19968;&#20195;&#25512;&#33616;&#31995;&#32479;&#22686;&#28155;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RecAI&#65292;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#30340;&#23454;&#29992;&#24037;&#20855;&#21253;&#65292;&#26032;&#19968;&#20195;&#30001;LLMs&#36171;&#33021;&#30340;&#25512;&#33616;&#31995;&#32479;&#23558;&#26356;&#21152;&#22810;&#25165;&#22810;&#33402;&#12289;&#21487;&#35299;&#37322;&#12289;&#23545;&#35805;&#24335;&#21644;&#21487;&#25511;&#65292;&#20026;&#26356;&#26234;&#33021;&#21644;&#29992;&#25143;&#20013;&#24515;&#30340;&#25512;&#33616;&#20307;&#39564;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RecAI&#65292;&#19968;&#20010;&#23454;&#29992;&#30340;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#33021;&#21147;&#26469;&#22686;&#24378;&#29978;&#33267;&#38761;&#26032;&#25512;&#33616;&#31995;&#32479;&#12290;RecAI&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#24037;&#20855;&#65292;&#21253;&#25324;&#25512;&#33616;AI&#20195;&#29702;&#12289;&#38754;&#21521;&#25512;&#33616;&#30340;&#35821;&#35328;&#27169;&#22411;&#12289;&#30693;&#35782;&#25554;&#20214;&#12289;&#25512;&#33616;&#35299;&#37322;&#22120;&#21644;&#35780;&#20272;&#22120;&#65292;&#20197;&#22810;&#35282;&#24230;&#20419;&#36827;LLMs&#34701;&#20837;&#25512;&#33616;&#31995;&#32479;&#12290;LLMs&#36171;&#33021;&#30340;&#26032;&#19968;&#20195;&#25512;&#33616;&#31995;&#32479;&#39044;&#35745;&#23558;&#26356;&#21152;&#22810;&#25165;&#22810;&#33402;&#12289;&#21487;&#35299;&#37322;&#12289;&#23545;&#35805;&#24335;&#21644;&#21487;&#25511;&#65292;&#20026;&#26356;&#26234;&#33021;&#21644;&#29992;&#25143;&#20013;&#24515;&#30340;&#25512;&#33616;&#20307;&#39564;&#38138;&#24179;&#36947;&#36335;&#12290;&#25105;&#20204;&#24076;&#26395;RecAI&#30340;&#24320;&#28304;&#33021;&#22815;&#21152;&#36895;&#26032;&#19968;&#20195;&#20808;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#28436;&#36827;&#12290;RecAI&#30340;&#28304;&#20195;&#30721;&#21487;&#22312; \url{https://github.com/microsoft/RecAI} &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06465v1 Announce Type: cross  Abstract: This paper introduces RecAI, a practical toolkit designed to augment or even revolutionize recommender systems with the advanced capabilities of Large Language Models (LLMs). RecAI provides a suite of tools, including Recommender AI Agent, Recommendation-oriented Language Models, Knowledge Plugin, RecExplainer, and Evaluator, to facilitate the integration of LLMs into recommender systems from multifaceted perspectives. The new generation of recommender systems, empowered by LLMs, are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric recommendation experiences. We hope the open-source of RecAI can help accelerate evolution of new advanced recommender systems. The source code of RecAI is available at \url{https://github.com/microsoft/RecAI}.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#36827;&#34892;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.06448</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#30340;&#26080;&#30417;&#30563;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#36827;&#34892;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#26159;&#25351;&#20135;&#29983;&#36830;&#36143;&#20294;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#20013;&#24187;&#35273;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MIND&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#20869;&#37096;&#29366;&#24577;&#36827;&#34892;&#23454;&#26102;&#24187;&#35273;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;HELM&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#20010;LLMs&#24187;&#35273;&#26816;&#27979;&#30340;&#26032;&#22522;&#20934;&#65292;&#22312;LLMs&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;LLM&#36755;&#20986;&#21644;&#20869;&#37096;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06448v1 Announce Type: cross  Abstract: Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM's inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments d
&lt;/p&gt;</description></item><item><title>CoRAL&#24341;&#20837;&#20102;&#21327;&#20316;&#26816;&#32034;&#22686;&#24378;LLMs&#65292;&#23558;&#21327;&#20316;&#35777;&#25454;&#30452;&#25509;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#20043;&#38388;&#30340;&#20559;&#22909;&#65292;&#25913;&#36827;&#38271;&#23614;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.06447</link><description>&lt;p&gt;
CoRAL: &#21327;&#20316;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#38271;&#23614;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06447
&lt;/p&gt;
&lt;p&gt;
CoRAL&#24341;&#20837;&#20102;&#21327;&#20316;&#26816;&#32034;&#22686;&#24378;LLMs&#65292;&#23558;&#21327;&#20316;&#35777;&#25454;&#30452;&#25509;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#20043;&#38388;&#30340;&#20559;&#22909;&#65292;&#25913;&#36827;&#38271;&#23614;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#25512;&#33616;&#23545;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#22797;&#26434;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#24110;&#21161;&#22522;&#20110;&#38750;&#24120;&#23569;&#30340;&#20808;&#21069;&#20132;&#20114;&#26469;&#25512;&#26029;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#20165;&#20381;&#36182;&#20110;&#29289;&#21697;&#30340;&#35821;&#20041;&#21547;&#20041;&#20316;&#20026;&#25512;&#29702;&#30340;&#21807;&#19968;&#35777;&#25454;&#65292;&#24573;&#30053;&#20102;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#21327;&#20316;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;LLM&#30340;&#25512;&#29702;&#19982;&#25968;&#25454;&#38598;&#30340;&#20219;&#21153;&#29305;&#23450;&#21327;&#20316;&#20449;&#24687;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#23558;LLMs&#30340;&#25512;&#29702;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30693;&#35782;&#30456;&#19968;&#33268;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21327;&#20316;&#26816;&#32034;&#22686;&#24378;LLMs&#65292;&#31216;&#20026;CoRAL&#65292;&#30452;&#25509;&#23558;&#21327;&#20316;&#35777;&#25454;&#32435;&#20837;&#20132;&#20114;&#20013;&#12290;&#22522;&#20110;&#26816;&#32034;&#21040;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#65292;LLMs&#21487;&#20197;&#20998;&#26512;&#29992;&#25143;&#20043;&#38388;&#30340;&#20849;&#20139;&#21644;&#19981;&#21516;&#20559;&#22909;&#65292;&#24182;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06447v1 Announce Type: cross  Abstract: The long-tail recommendation is a challenging task for traditional recommender systems, due to data sparsity and data imbalance issues. The recent development of large language models (LLMs) has shown their abilities in complex reasoning, which can help to deduce users' preferences based on very few previous interactions. However, since most LLM-based systems rely on items' semantic meaning as the sole evidence for reasoning, the collaborative information of user-item interactions is neglected, which can cause the LLM's reasoning to be misaligned with task-specific collaborative information of the dataset. To further align LLMs' reasoning to task-specific user-item interaction knowledge, we introduce collaborative retrieval-augmented LLMs, CoRAL, which directly incorporate collaborative evidence into the prompts. Based on the retrieved user-item interactions, the LLM can analyze shared and distinct preferences among users, and summariz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#32454;&#31890;&#24230;&#26609;&#29305;&#24449;&#32534;&#30721;&#65288;FG-PFE&#65289;&#30340;&#26032;&#22411;&#26609;&#32534;&#30721;&#26550;&#26500;&#65292;&#21033;&#29992;&#26102;&#31354;&#34394;&#25311;&#65288;STV&#65289;&#32593;&#26684;&#25429;&#25417;&#27599;&#20010;&#26609;&#20869;&#28857;&#20113;&#30340;&#32454;&#31890;&#24230;&#20998;&#24067;</title><link>https://arxiv.org/abs/2403.06433</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#31354;&#34394;&#25311;&#32593;&#26684;&#36827;&#34892;&#32454;&#31890;&#24230;&#26609;&#29305;&#24449;&#32534;&#30721;&#29992;&#20110;3D&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fine-Grained Pillar Feature Encoding Via Spatio-Temporal Virtual Grid for 3D Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#32454;&#31890;&#24230;&#26609;&#29305;&#24449;&#32534;&#30721;&#65288;FG-PFE&#65289;&#30340;&#26032;&#22411;&#26609;&#32534;&#30721;&#26550;&#26500;&#65292;&#21033;&#29992;&#26102;&#31354;&#34394;&#25311;&#65288;STV&#65289;&#32593;&#26684;&#25429;&#25417;&#27599;&#20010;&#26609;&#20869;&#28857;&#20113;&#30340;&#32454;&#31890;&#24230;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#39640;&#24615;&#33021;&#12289;&#23454;&#26102;&#30340;&#22522;&#20110;LiDAR&#30340;3D&#29289;&#20307;&#26816;&#27979;&#22120;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#25104;&#21151;&#21830;&#19994;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#26609;&#30340;&#26041;&#27861;&#30001;&#20110;&#20854;&#35745;&#31639;&#25928;&#29575;&#32780;&#25104;&#20026;&#26426;&#36733;&#37096;&#32626;&#30340;&#23454;&#29992;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#25928;&#29575;&#65292;&#36825;&#20123;&#26041;&#27861;&#26377;&#26102;&#20250;&#23545;&#27604;&#20307;&#32032;&#32534;&#30721;&#25110;PointNet++&#31561;&#26367;&#20195;&#28857;&#32534;&#30721;&#25216;&#26415;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#22522;&#20110;&#26609;&#30340;&#26041;&#27861;&#24182;&#26410;&#20805;&#20998;&#25429;&#25417;&#27599;&#20010;&#26609;&#32467;&#26500;&#20869;LiDAR&#28857;&#30340;&#32454;&#31890;&#24230;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#26609;&#29305;&#24449;&#32534;&#30721;&#20013;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#32454;&#31890;&#24230;&#26609;&#29305;&#24449;&#32534;&#30721;&#65288;FG-PFE&#65289;&#30340;&#26032;&#22411;&#26609;&#32534;&#30721;&#26550;&#26500;&#12290;FG-PFE&#21033;&#29992;&#26102;&#31354;&#34394;&#25311;&#65288;STV&#65289;&#32593;&#26684;&#26469;&#25429;&#25417;&#27599;&#20010;&#26609;&#20869;&#28857;&#20113;&#30340;&#20998;&#24067;&#65292;&#36328;&#22402;&#30452;&#12289;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06433v1 Announce Type: cross  Abstract: Developing high-performance, real-time architectures for LiDAR-based 3D object detectors is essential for the successful commercialization of autonomous vehicles. Pillar-based methods stand out as a practical choice for onboard deployment due to their computational efficiency. However, despite their efficiency, these methods can sometimes underperform compared to alternative point encoding techniques such as Voxel-encoding or PointNet++. We argue that current pillar-based methods have not sufficiently captured the fine-grained distributions of LiDAR points within each pillar structure. Consequently, there exists considerable room for improvement in pillar feature encoding. In this paper, we introduce a novel pillar encoding architecture referred to as Fine-Grained Pillar Feature Encoding (FG-PFE). FG-PFE utilizes Spatio-Temporal Virtual (STV) grids to capture the distribution of point clouds within each pillar across vertical, temporal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#20998;&#20960;&#20309;&#35270;&#35282;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27969;&#24418;&#19978;&#30340;&#24179;&#28369;&#26354;&#32447;&#27169;&#25311;&#22270;&#28436;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20998;&#24067;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06425</link><description>&lt;p&gt;
&#22522;&#20110;&#24494;&#20998;&#20960;&#20309;&#35270;&#35282;&#30340;&#22270;&#28436;&#21270;&#19978;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Differential Geometric View and Explainability of GNN on Evolving Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06425
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#20998;&#20960;&#20309;&#35270;&#35282;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27969;&#24418;&#19978;&#30340;&#24179;&#28369;&#26354;&#32447;&#27169;&#25311;&#22270;&#28436;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20998;&#24067;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#29983;&#29289;&#21270;&#23398;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#29992;&#20110;&#39044;&#27979;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#22270;&#21487;&#33021;&#26159;&#28436;&#21270;&#30340;&#65292;&#24418;&#24335;&#21270;&#24314;&#27169;&#24182;&#29702;&#35299;&#35757;&#32451;&#21518;&#30340;GNN&#22914;&#20309;&#21709;&#24212;&#22270;&#28436;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#20844;&#29702;&#24402;&#22240;&#23545;GNN&#39044;&#27979;&#20998;&#24067;&#36827;&#34892;&#24179;&#28369;&#21442;&#25968;&#21270;&#65292;&#20854;&#20013;&#20998;&#24067;&#20301;&#20110;&#39640;&#32500;&#23884;&#20837;&#31354;&#38388;&#20869;&#30340;&#20302;&#32500;&#27969;&#24418;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#24494;&#20998;&#20960;&#20309;&#35270;&#35282;&#23558;&#20998;&#24067;&#28436;&#21270;&#24314;&#27169;&#20026;&#27969;&#24418;&#19978;&#30340;&#24179;&#28369;&#26354;&#32447;&#12290;&#25105;&#20204;&#37325;&#26032;&#21442;&#25968;&#21270;&#27969;&#24418;&#19978;&#30340;&#26354;&#32447;&#26063;&#65292;&#24182;&#35774;&#35745;&#19968;&#20010;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#25214;&#21040;&#31616;&#27905;&#22320;&#36817;&#20284;&#20998;&#24067;&#28436;&#21270;&#20197;&#20379;&#20154;&#31867;&#35299;&#37322;&#30340;&#21807;&#19968;&#26354;&#32447;&#12290;&#22312;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#36825;&#20123;&#20219;&#21153;&#28041;&#21450;&#21040;&#28436;&#21270;&#30340;&#22270;&#65292;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#12289;&#24544;&#23454;&#24230;&#21644;&#30452;&#35266;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06425v1 Announce Type: cross  Abstract: Graphs are ubiquitous in social networks and biochemistry, where Graph Neural Networks (GNN) are the state-of-the-art models for prediction. Graphs can be evolving and it is vital to formally model and understand how a trained GNN responds to graph evolution. We propose a smooth parameterization of the GNN predicted distributions using axiomatic attribution, where the distributions are on a low-dimensional manifold within a high-dimensional embedding space. We exploit the differential geometric viewpoint to model distributional evolution as smooth curves on the manifold. We reparameterize families of curves on the manifold and design a convex optimization problem to find a unique curve that concisely approximates the distributional evolution for human interpretation. Extensive experiments on node classification, link prediction, and graph classification tasks with evolving graphs demonstrate the better sparsity, faithfulness, and intui
&lt;/p&gt;</description></item><item><title>RLingua&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.06420</link><description>&lt;p&gt;
RLingua&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#21892;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06420
&lt;/p&gt;
&lt;p&gt;
RLingua&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#20294;&#20197;&#20854;&#20302;&#26679;&#26412;&#25928;&#29575;&#32780;&#22768;&#21517;&#29436;&#34249;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RLingua&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20869;&#37096;&#30693;&#35782;&#26469;&#20943;&#23569;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;RL&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#26694;&#26550;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25552;&#21462;LLMs&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20174;&#32780;&#29983;&#25104;&#29305;&#23450;&#20219;&#21153;&#30340;&#21021;&#27493;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#12290;&#23613;&#31649;&#19981;&#23436;&#32654;&#65292;LLM&#29983;&#25104;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#34987;&#29992;&#20110;&#22312;rollout&#26102;&#20197;&#34928;&#20943;&#27010;&#29575;&#29983;&#25104;&#21160;&#20316;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;RL&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;&#24182;&#20462;&#25913;&#20102;&#28436;&#21592;&#25439;&#22833;&#65292;&#20197;&#20351;&#31574;&#30053;&#23398;&#20064;&#26397;&#30528;LLM&#29983;&#25104;&#30340;&#25511;&#21046;&#22120;&#35268;&#33539;&#21270;&#12290;RLingua&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#25913;&#21892;&#19981;&#23436;&#32654;&#30340;LLM&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RLing
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06420v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has demonstrated its capability in solving various tasks but is notorious for its low sample efficiency. In this paper, we propose RLingua, a framework that can leverage the internal knowledge of large language models (LLMs) to reduce the sample complexity of RL in robotic manipulations. To this end, we first present how to extract the prior knowledge of LLMs by prompt engineering so that a preliminary rule-based robot controller for a specific task can be generated. Despite being imperfect, the LLM-generated robot controller is utilized to produce action samples during rollouts with a decaying probability, thereby improving RL's sample efficiency. We employ the actor-critic framework and modify the actor loss to regularize the policy learning towards the LLM-generated controller. RLingua also provides a novel method of improving the imperfect LLM-generated robot controllers by RL. We demonstrated that RLing
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36923;&#36753;&#27169;&#24335;&#35760;&#24518;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LMPM&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#22806;&#37096;&#23384;&#20648;&#32467;&#26500;&#23398;&#20064;&#21644;&#23384;&#20648;&#36923;&#36753;&#27169;&#24335;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#29983;&#25104;&#36923;&#36753;&#19968;&#33268;&#30340;&#32467;&#35770;&#65292;&#24182;&#24341;&#20837;&#23454;&#20307;&#25277;&#35937;&#26041;&#27861;&#26469;&#20943;&#23569;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#20013;&#30340;&#36923;&#36753;&#26080;&#20851;&#39046;&#22495;&#30693;&#35782;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.06410</link><description>&lt;p&gt;
&#29992;&#20110;&#34164;&#28085;&#26641;&#29983;&#25104;&#30340;&#36923;&#36753;&#27169;&#24335;&#35760;&#24518;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Logical Pattern Memory Pre-trained Model for Entailment Tree Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06410
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36923;&#36753;&#27169;&#24335;&#35760;&#24518;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LMPM&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#22806;&#37096;&#23384;&#20648;&#32467;&#26500;&#23398;&#20064;&#21644;&#23384;&#20648;&#36923;&#36753;&#27169;&#24335;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#29983;&#25104;&#36923;&#36753;&#19968;&#33268;&#30340;&#32467;&#35770;&#65292;&#24182;&#24341;&#20837;&#23454;&#20307;&#25277;&#35937;&#26041;&#27861;&#26469;&#20943;&#23569;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#20013;&#30340;&#36923;&#36753;&#26080;&#20851;&#39046;&#22495;&#30693;&#35782;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#29983;&#25104;&#36830;&#36143;&#21487;&#20449;&#30340;&#35299;&#37322;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#28145;&#20837;&#30740;&#31350;&#20102;&#21033;&#29992;&#34164;&#28085;&#26641;&#26469;&#25551;&#36848;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36825;&#23637;&#31034;&#20102;&#19968;&#20010;&#20551;&#35774;&#22914;&#20309;&#20174;&#25903;&#25345;&#20107;&#23454;&#20013;&#25512;&#23548;&#20986;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#24573;&#35270;&#20102;&#20174;&#32473;&#23450;&#20107;&#23454;&#20013;&#29983;&#25104;&#20855;&#26377;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#20013;&#38388;&#32467;&#35770;&#30340;&#37325;&#35201;&#24615;&#65292;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#32467;&#35770;&#65292;&#21066;&#24369;&#20102;&#34164;&#28085;&#26641;&#30340;&#25972;&#20307;&#21487;&#20449;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36923;&#36753;&#27169;&#24335;&#35760;&#24518;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LMPM&#65289;&#12290;LMPM&#32467;&#21512;&#20102;&#22806;&#37096;&#23384;&#20648;&#32467;&#26500;&#65292;&#23398;&#20064;&#21644;&#23384;&#20648;&#36923;&#36753;&#27169;&#24335;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#29983;&#25104;&#36923;&#36753;&#19968;&#33268;&#30340;&#32467;&#35770;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#20013;&#36923;&#36753;&#26080;&#20851;&#39046;&#22495;&#30693;&#35782;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23454;&#20307;&#25277;&#35937;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06410v1 Announce Type: cross  Abstract: Generating coherent and credible explanations remains a significant challenge in the field of AI. In recent years, researchers have delved into the utilization of entailment trees to depict explanations, which exhibit a reasoning process of how a hypothesis is deduced from the supporting facts. However, existing models often overlook the importance of generating intermediate conclusions with logical consistency from the given facts, leading to inaccurate conclusions and undermining the overall credibility of entailment trees. To address this limitation, we propose the logical pattern memory pre-trained model (LMPM). LMPM incorporates an external memory structure to learn and store the latent representations of logical patterns, which aids in generating logically consistent conclusions. Furthermore, to mitigate the influence of logically irrelevant domain knowledge in the Wikipedia-based data, we introduce an entity abstraction approach
&lt;/p&gt;</description></item><item><title>&#37327;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22256;&#38590;&#20027;&#35201;&#34920;&#29616;&#22312;&#22914;&#20309;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#36890;&#36807;&#23545;&#19981;&#21516;&#20154;&#20026;&#25200;&#21160;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#20102;&#25200;&#21160;&#29305;&#24615;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#25913;&#21892;&#37327;&#21270;&#31283;&#20581;&#24615;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.06408</link><description>&lt;p&gt;
&#37327;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22256;&#38590;&#22312;&#21738;&#37324;&#65311;&#22522;&#20110;&#25200;&#21160;&#35270;&#35282;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06408
&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22256;&#38590;&#20027;&#35201;&#34920;&#29616;&#22312;&#22914;&#20309;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#36890;&#36807;&#23545;&#19981;&#21516;&#20154;&#20026;&#25200;&#21160;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#20102;&#25200;&#21160;&#29305;&#24615;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#25913;&#21892;&#37327;&#21270;&#31283;&#20581;&#24615;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#65292;&#20294;&#20851;&#20110;&#37327;&#21270;&#19982;LLM&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#26377;&#24456;&#22810;&#24453;&#25506;&#32034;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#31181;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#26032;&#35270;&#35282;&#65292;&#23558;&#20854;&#35270;&#20026;&#28155;&#21152;&#21040;LLMs&#26435;&#37325;&#21644;&#28608;&#27963;&#19978;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;&#8220;&#25200;&#21160;&#35270;&#35282;&#8221;&#12290;&#21033;&#29992;&#36825;&#19968;&#35270;&#35282;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#20154;&#20026;&#25200;&#21160;&#30340;&#23454;&#39564;&#65292;&#25506;&#35752;&#23427;&#20204;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#25200;&#21160;&#30340;&#29305;&#24615;&#19982;LLM&#24615;&#33021;&#20043;&#38388;&#30340;&#20960;&#20010;&#32852;&#31995;&#65292;&#20026;&#22343;&#21248;&#37327;&#21270;&#30340;&#22833;&#36133;&#26696;&#20363;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#24182;&#26263;&#31034;&#20102;&#25913;&#21892;LLM&#37327;&#21270;&#31283;&#20581;&#24615;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06408v1 Announce Type: cross  Abstract: Quantization has emerged as a promising technique for improving the memory and computational efficiency of large language models (LLMs). Though the trade-off between performance and efficiency is well-known, there is still much to be learned about the relationship between quantization and LLM performance. To shed light on this relationship, we propose a new perspective on quantization, viewing it as perturbations added to the weights and activations of LLMs. We call this approach "the lens of perturbation". Using this lens, we conduct experiments with various artificial perturbations to explore their impact on LLM performance. Our findings reveal several connections between the properties of perturbations and LLM performance, providing insights into the failure cases of uniform quantization and suggesting potential solutions to improve the robustness of LLM quantization. To demonstrate the significance of our findings, we implement a s
&lt;/p&gt;</description></item><item><title>&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#65292;&#24182;&#19988;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#33539;&#22260;&#20869;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.06398</link><description>&lt;p&gt;
&#20851;&#20110;&#25345;&#32493;&#23398;&#20064;&#20013;&#23485;&#24230;&#36882;&#20943;&#22238;&#25253;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Diminishing Returns of Width for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06398
&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#65292;&#24182;&#19988;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#33539;&#22260;&#20869;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#25353;&#39034;&#24207;&#35757;&#32451;&#26032;&#20219;&#21153;&#26102;&#32463;&#24120;&#20986;&#29616;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#12290; &#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#20943;&#23569;&#65292;&#20294;&#23578;&#26410;&#20934;&#30830;&#21051;&#30011;&#23485;&#24230;&#21644;&#25345;&#32493;&#23398;&#20064;&#20043;&#38388;&#30340;&#30830;&#20999;&#20851;&#31995;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20854;&#20013;&#19968;&#20010;&#26368;&#26089;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#25345;&#32493;&#23398;&#20064;&#29702;&#35770;&#65292;&#24182;&#35777;&#26126;&#23485;&#24230;&#19982;&#21069;&#39304;&#32593;&#32476;&#65288;FFN&#65289;&#20013;&#30340;&#36951;&#24536;&#30452;&#25509;&#30456;&#20851;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#19978;&#32463;&#39564;&#24615;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35770;&#26029;&#65292;&#32467;&#26524;&#26174;&#31034;&#36882;&#20943;&#22238;&#25253;&#22914;&#25105;&#20204;&#30340;&#29702;&#35770;&#25152;&#39044;&#27979;&#30340;&#37027;&#26679;&#28165;&#26224;&#21487;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06398v1 Announce Type: cross  Abstract: While deep neural networks have demonstrated groundbreaking performance in various settings, these models often suffer from \emph{catastrophic forgetting} when trained on new tasks in sequence. Several works have empirically demonstrated that increasing the width of a neural network leads to a decrease in catastrophic forgetting but have yet to characterize the exact relationship between width and continual learning. We design one of the first frameworks to analyze Continual Learning Theory and prove that width is directly related to forgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that increasing network widths to reduce forgetting yields diminishing returns. We empirically verify our claims at widths hitherto unexplored in prior studies where the diminishing returns are clearly observed as predicted by our theory.
&lt;/p&gt;</description></item><item><title>DeepSafeMPC&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#39044;&#27979;&#22810;&#26234;&#20307;&#29615;&#22659;&#30340;&#22797;&#26434;&#21160;&#24577;&#65292;&#24182;&#24212;&#29992;MARL&#21407;&#21017;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.06397</link><description>&lt;p&gt;
DeepSafeMPC: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23433;&#20840;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06397
&lt;/p&gt;
&lt;p&gt;
DeepSafeMPC&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#39044;&#27979;&#22810;&#26234;&#20307;&#29615;&#22659;&#30340;&#22797;&#26434;&#21160;&#24577;&#65292;&#24182;&#24212;&#29992;MARL&#21407;&#21017;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;safe MARL&#65289;&#22312;&#26368;&#36817;&#20960;&#24180;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#65292;&#24378;&#35843;&#20102;&#26234;&#20307;&#19981;&#20165;&#38656;&#35201;&#20248;&#21270;&#20840;&#23616;&#22238;&#25253;&#65292;&#36824;&#38656;&#35201;&#36890;&#36807;&#34892;&#20026;&#32422;&#26463;&#36981;&#23432;&#23433;&#20840;&#35201;&#27714;&#30340;&#24517;&#35201;&#24615;&#12290;&#36817;&#26399;&#19968;&#20123;&#24037;&#20316;&#23558;&#25511;&#21046;&#29702;&#35770;&#19982;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#24212;&#29992;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26041;&#27861;&#30340;&#24212;&#29992;&#38750;&#24120;&#26377;&#38480;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22810;&#26234;&#20307;&#29615;&#22659;&#20013;&#22797;&#26434;&#19988;&#38544;&#24335;&#21160;&#24577;&#30340;&#29305;&#24615;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23433;&#20840;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;DeepSafeMPC&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;DeepSafeMPC &#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#21033;&#29992;&#38598;&#20013;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24456;&#22909;&#22320;&#39044;&#27979;&#29615;&#22659;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;MARL&#21407;&#21017;&#26469;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06397v1 Announce Type: cross  Abstract: Safe Multi-agent reinforcement learning (safe MARL) has increasingly gained attention in recent years, emphasizing the need for agents to not only optimize the global return but also adhere to safety requirements through behavioral constraints. Some recent work has integrated control theory with multi-agent reinforcement learning to address the challenge of ensuring safety. However, there have been only very limited applications of Model Predictive Control (MPC) methods in this domain, primarily due to the complex and implicit dynamics characteristic of multi-agent environments. To bridge this gap, we propose a novel method called Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning (DeepSafeMPC). The key insight of DeepSafeMPC is leveraging a entralized deep learning model to well predict environmental dynamics. Our method applies MARL principles to search for optimal solutions. Through the employme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Fennec&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25152;&#26377;&#27169;&#22411;&#21644;&#21382;&#21490;&#20219;&#21153;&#26144;&#23556;&#21040;&#19968;&#20010;&#36801;&#31227;&#30456;&#20851;&#30340;&#23376;&#31354;&#38388;&#65292;&#20197;&#20415;&#25512;&#26029;&#26032;&#20219;&#21153;&#22312;&#36801;&#31227;&#31354;&#38388;&#20013;&#30340;&#34920;&#24449;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.06382</link><description>&lt;p&gt;
&#38024;&#23545;&#19979;&#28216;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Pre-Trained Model Recommendation for Downstream Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Fennec&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25152;&#26377;&#27169;&#22411;&#21644;&#21382;&#21490;&#20219;&#21153;&#26144;&#23556;&#21040;&#19968;&#20010;&#36801;&#31227;&#30456;&#20851;&#30340;&#23376;&#31354;&#38388;&#65292;&#20197;&#20415;&#25512;&#26029;&#26032;&#20219;&#21153;&#22312;&#36801;&#31227;&#31354;&#38388;&#20013;&#30340;&#34920;&#24449;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#27169;&#22411;&#36873;&#25321;&#26088;&#22312;&#23545;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;&#65292;&#24182;&#36873;&#25321;&#26368;&#36866;&#21512;&#26032;&#30446;&#26631;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#36890;&#24120;&#22312;&#33539;&#22260;&#19978;&#21463;&#38480;&#65292;&#24182;&#20542;&#21521;&#20110;&#24573;&#35270;&#27169;&#22411;&#19982;&#20219;&#21153;&#20043;&#38388;&#24494;&#22937;&#30340;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21153;&#23454;&#30340;&#26694;&#26550; Fennec&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#24211;&#65292;&#21516;&#26102;&#32454;&#33268;&#32771;&#34385;&#20102;&#20219;&#21153;&#19982;&#27169;&#22411;&#20043;&#38388;&#30340;&#22797;&#26434;&#32852;&#31995;&#12290;&#20851;&#38190;&#27934;&#35265;&#22312;&#20110;&#23558;&#25152;&#26377;&#27169;&#22411;&#21644;&#21382;&#21490;&#20219;&#21153;&#26144;&#23556;&#21040;&#19968;&#20010;&#19982;&#36801;&#31227;&#30456;&#20851;&#30340;&#23376;&#31354;&#38388;&#20013;&#65292;&#27169;&#22411;&#21521;&#37327;&#21644;&#20219;&#21153;&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#20195;&#34920;&#20102;&#21487;&#36801;&#31227;&#24615;&#30340;&#22823;&#23567;&#12290;&#19968;&#20010;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#20154;&#65292;&#22312;&#36801;&#31227;&#31354;&#38388;&#20013;&#25512;&#26029;&#26032;&#20219;&#21153;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#36991;&#24320;&#20102;&#36827;&#34892;&#22823;&#37327;&#21069;&#21521;&#20256;&#25773;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#27169;&#22411;&#22266;&#26377;&#24402;&#32435;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06382v1 Announce Type: cross  Abstract: As a fundamental problem in transfer learning, model selection aims to rank off-the-shelf pre-trained models and select the most suitable one for the new target task. Existing model selection techniques are often constrained in their scope and tend to overlook the nuanced relationships between models and tasks. In this paper, we present a pragmatic framework \textbf{Fennec}, delving into a diverse, large-scale model repository while meticulously considering the intricate connections between tasks and models. The key insight is to map all models and historical tasks into a transfer-related subspace, where the distance between model vectors and task vectors represents the magnitude of transferability. A large vision model, as a proxy, infers a new task's representation in the transfer space, thereby circumventing the computational burden of extensive forward passes. We also investigate the impact of the inherent inductive bias of models 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26032;&#30340;&#32599;&#39532;&#23612;&#20122;&#21517;&#35789;&#22797;&#21512;&#35789;&#20851;&#31995;&#38598;&#65292;&#36890;&#36807;&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#27979;&#35797;&#21518;&#21457;&#29616;&#65292;&#32593;&#32476;&#30340;&#39044;&#27979;&#19982;&#20154;&#31867;&#21028;&#26029;&#23384;&#22312;&#19968;&#33268;&#65292;&#21363;&#20351;&#26159;&#22312;&#20154;&#31867;&#19968;&#33268;&#29575;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#12290;&#38656;&#35201;&#19968;&#20010;&#26356;&#22909;&#30340;&#20851;&#31995;&#28165;&#21333;&#12290;</title><link>https://arxiv.org/abs/2403.06360</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#33258;&#21160;&#35299;&#37322;&#32599;&#39532;&#23612;&#20122;&#21517;&#35789;&#22797;&#21512;&#35789;
&lt;/p&gt;
&lt;p&gt;
Human and Automatic Interpretation of Romanian Noun Compounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06360
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26032;&#30340;&#32599;&#39532;&#23612;&#20122;&#21517;&#35789;&#22797;&#21512;&#35789;&#20851;&#31995;&#38598;&#65292;&#36890;&#36807;&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#27979;&#35797;&#21518;&#21457;&#29616;&#65292;&#32593;&#32476;&#30340;&#39044;&#27979;&#19982;&#20154;&#31867;&#21028;&#26029;&#23384;&#22312;&#19968;&#33268;&#65292;&#21363;&#20351;&#26159;&#22312;&#20154;&#31867;&#19968;&#33268;&#29575;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#12290;&#38656;&#35201;&#19968;&#20010;&#26356;&#22909;&#30340;&#20851;&#31995;&#28165;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#31867;&#20284;"&#38795;&#23376;&#38144;&#21806;"&#21644;"&#28779;&#28798;&#22823;&#29993;&#21334;"&#36825;&#26679;&#30340;&#21517;&#35789;&#22797;&#21512;&#35789;&#22312;&#29305;&#23450;&#35821;&#22659;&#20013;&#30340;&#39044;&#26399;&#21547;&#20041;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32780;&#35328;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#25429;&#25417;&#22797;&#21512;&#35789;&#25104;&#21592;&#38388;&#19981;&#21516;&#21547;&#20041;&#30340;&#35821;&#20041;&#20851;&#31995;&#28165;&#21333;&#12290;&#38024;&#23545;&#32599;&#39532;&#23612;&#20122;&#30340;&#22797;&#21512;&#35789;&#65292;&#20854;&#24418;&#24577;&#21477;&#27861;&#19982;&#33521;&#35821;&#30340;&#23545;&#24212;&#29289;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20851;&#31995;&#38598;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#27880;&#37322;&#32773;&#21644;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#32593;&#32476;&#30340;&#39044;&#27979;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#23384;&#22312;&#19968;&#33268;&#65292;&#21363;&#20351;&#22312;&#20154;&#31867;&#19968;&#33268;&#29575;&#36739;&#20302;&#30340;&#22320;&#26041;&#20063;&#26159;&#22914;&#27492;&#12290;&#19968;&#33268;&#24615;&#19982;&#25152;&#36873;&#20851;&#31995;&#30340;&#39057;&#29575;&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#19981;&#21463;&#32467;&#26500;&#24046;&#24322;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#26368;&#24120;&#36873;&#25321;&#30340;&#20851;&#31995;&#19981;&#23646;&#20110;&#21313;&#20845;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#20851;&#31995;&#20043;&#19968;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#19968;&#20010;&#26356;&#22909;&#30340;&#20851;&#31995;&#28165;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06360v1 Announce Type: cross  Abstract: Determining the intended, context-dependent meanings of noun compounds like "shoe sale" and "fire sale" remains a challenge for NLP. Previous work has relied on inventories of semantic relations that capture the different meanings between compound members. Focusing on Romanian compounds, whose morphosyntax differs from that of their English counterparts, we propose a new set of relations and test it with human annotators and a neural net classifier. Results show an alignment of the network's predictions and human judgments, even where the human agreement rate is low. Agreement tracks with the frequency of the selected relations, regardless of structural differences. However, the most frequently selected relation was none of the sixteen labeled semantic relations, indicating the need for a better relation inventory.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#39057;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#22235;&#20010;&#27169;&#22359;&#30340;&#24212;&#29992;&#20248;&#21270;&#35270;&#39057;&#24103;&#20013;&#32972;&#26223;&#21644;&#21069;&#26223;&#30340;&#19968;&#33268;&#24615;&#65292;&#29983;&#25104;&#30340;&#35270;&#39057;&#36136;&#37327;&#39640;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.06356</link><description>&lt;p&gt;
&#35270;&#39057;&#29983;&#25104;&#19982;&#19968;&#33268;&#24615;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
Video Generation with Consistency Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06356
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#39057;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#22235;&#20010;&#27169;&#22359;&#30340;&#24212;&#29992;&#20248;&#21270;&#35270;&#39057;&#24103;&#20013;&#32972;&#26223;&#21644;&#21069;&#26223;&#30340;&#19968;&#33268;&#24615;&#65292;&#29983;&#25104;&#30340;&#35270;&#39057;&#36136;&#37327;&#39640;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#21508;&#31181;&#30740;&#31350;&#37117;&#22312;&#25506;&#32034;&#29983;&#25104;&#38271;&#35270;&#39057;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35270;&#39057;&#20013;&#29983;&#25104;&#30340;&#24103;&#32463;&#24120;&#20986;&#29616;&#25238;&#21160;&#21644;&#22122;&#38899;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#29983;&#25104;&#27809;&#26377;&#36825;&#20123;&#22122;&#38899;&#30340;&#35270;&#39057;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#30001;&#22235;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#29420;&#31435;&#35843;&#33410;&#27169;&#22359;&#12289;&#24179;&#22343;&#34701;&#21512;&#27169;&#22359;&#12289;&#32508;&#21512;&#35843;&#33410;&#27169;&#22359;&#21644;&#24103;&#38388;&#19968;&#33268;&#24615;&#27169;&#22359;&#12290;&#36890;&#36807;&#36880;&#27493;&#24212;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26032;&#27169;&#22359;&#65292;&#20248;&#21270;&#20102;&#27599;&#20010;&#35270;&#39057;&#24103;&#20013;&#32972;&#26223;&#21644;&#21069;&#26223;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#35270;&#39057;&#22312;&#36136;&#37327;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06356v1 Announce Type: cross  Abstract: Currently, various studies have been exploring generation of long videos. However, the generated frames in these videos often exhibit jitter and noise. Therefore, in order to generate the videos without these noise, we propose a novel framework composed of four modules: separate tuning module, average fusion module, combined tuning module, and inter-frame consistency module. By applying our newly proposed modules subsequently, the consistency of the background and foreground in each video frames is optimized. Besides, the experimental results demonstrate that videos generated by our method exhibit a high quality in comparison of the state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22806;&#37096;&#31639;&#26415;&#22359;&#65288;MOAB&#65289;&#65292;&#29992;&#20110;&#23558;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#21644;&#36951;&#20256;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#39044;&#27979;&#33041;&#32959;&#30244;&#30340;&#31561;&#32423;&#12290;</title><link>https://arxiv.org/abs/2403.06349</link><description>&lt;p&gt;
MOAB: &#33041;&#32959;&#30244;&#20998;&#32423;&#20013;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#21644;&#36951;&#20256;&#25968;&#25454;&#34701;&#21512;&#30340;&#22810;&#27169;&#24577;&#22806;&#37096;&#31639;&#26415;&#22359;
&lt;/p&gt;
&lt;p&gt;
MOAB: Multi-Modal Outer Arithmetic Block For Fusion Of Histopathological Images And Genetic Data For Brain Tumor Grading
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22806;&#37096;&#31639;&#26415;&#22359;&#65288;MOAB&#65289;&#65292;&#29992;&#20110;&#23558;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#21644;&#36951;&#20256;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#39044;&#27979;&#33041;&#32959;&#30244;&#30340;&#31561;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32959;&#30244;&#26159;&#22823;&#33041;&#32454;&#32990;&#24322;&#24120;&#22686;&#38271;&#12290;&#26681;&#25454;&#23427;&#20204;&#30340;&#29983;&#38271;&#65292;&#21487;&#20197;&#23558;&#20854;&#20998;&#31867;&#20026;&#19981;&#21516;&#31561;&#32423;&#12290;&#36890;&#24120;&#26681;&#25454;&#32452;&#32455;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#32423;&#65292;&#36825;&#26159;&#24739;&#32773;&#39044;&#21518;&#30340;&#26368;&#37325;&#35201;&#39044;&#27979;&#22240;&#32032;&#20043;&#19968;&#65292;&#31561;&#32423;&#36234;&#39640;&#65292;&#32959;&#30244;&#36234;&#20855;&#20405;&#30053;&#24615;&#12290;&#32959;&#30244;&#31561;&#32423;&#30340;&#27491;&#30830;&#35786;&#26029;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#32452;&#32455;&#30149;&#29702;&#23398;&#20998;&#32423;&#20855;&#26377;&#39044;&#21518;&#20215;&#20540;&#65292;&#20294;&#32467;&#26524;&#20250;&#21463;&#21040;&#24178;&#39044;&#32773;&#20043;&#38388;&#30340;&#21464;&#24322;&#24615;&#24433;&#21709;&#65292;&#21363;&#20351;&#26159;&#32463;&#39564;&#20016;&#23500;&#30340;&#30149;&#29702;&#23398;&#23478;&#20043;&#38388;&#20063;&#26159;&#22914;&#27492;&#12290;&#26368;&#36817;&#65292;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#25253;&#21578;&#31216;&#65292;&#20998;&#23376;&#36951;&#20256;&#23398;&#26041;&#38754;&#30340;&#36827;&#23637;&#24050;&#32463;&#25913;&#21892;&#20102;&#32959;&#30244;&#20998;&#31867;&#12290;&#26412;&#25991;&#26088;&#22312;&#25972;&#21512;&#32452;&#32455;&#23398;&#22270;&#20687;&#21644;&#36951;&#20256;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#25913;&#36827;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31639;&#26415;&#36816;&#31639;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#22806;&#37096;&#31639;&#26415;&#22359;&#65288;MOAB&#65289;&#65292;&#29992;&#20110;&#32467;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20197;&#39044;&#27979;&#32959;&#30244;&#31561;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06349v1 Announce Type: cross  Abstract: Brain tumors are an abnormal growth of cells in the brain. They can be classified into distinct grades based on their growth. Often grading is performed based on a histological image and is one of the most significant predictors of a patients prognosis, the higher the grade, the more aggressive the tumor. Correct diagnosis of a tumor grade remains challenging. Though histopathological grading has been shown to be prognostic, results are subject to interobserver variability, even among experienced pathologists. Recently, the World Health Organization reported that advances in molecular genetics have led to improvements in tumor classification. This paper seeks to integrate histological images and genetic data for improved computer-aided diagnosis. We propose a novel Multi-modal Outer Arithmetic Block (MOAB) based on arithmetic operations to combine latent representations of the different modalities for predicting the tumor grade (Grade 
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22312;&#28145;&#21270;&#31038;&#20250;&#19981;&#24179;&#31561;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#21093;&#21066;&#36793;&#32536;&#32676;&#20307;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#26426;&#21046;&#22914;&#38646;&#24037;&#32463;&#27982;&#21171;&#24037;&#30340;&#28389;&#29992;&#12289;&#26377;&#20559;&#35265;&#30340;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#21644;&#23545;&#36825;&#20123;&#31038;&#32676;&#26045;&#21152;&#30340;&#19981;&#25104;&#27604;&#20363;&#30340;&#24515;&#29702;&#20581;&#24247;&#36127;&#25285;&#65292;&#21152;&#21095;&#20102;&#29616;&#26377;&#30340;&#19981;&#24179;&#31561;&#12290;</title><link>https://arxiv.org/abs/2403.06332</link><description>&lt;p&gt;
&#21033;&#29992;&#21033;&#28070;&#31354;&#38388;&#65306;&#36164;&#26412;&#20027;&#20041;&#22914;&#20309;&#20197;&#21093;&#21066;&#23569;&#25968;&#32676;&#20307;&#20026;&#20195;&#20215;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Exploiting the Margin: How Capitalism Fuels AI at the Expense of Minoritized Groups
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06332
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#28145;&#21270;&#31038;&#20250;&#19981;&#24179;&#31561;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#21093;&#21066;&#36793;&#32536;&#32676;&#20307;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#26426;&#21046;&#22914;&#38646;&#24037;&#32463;&#27982;&#21171;&#24037;&#30340;&#28389;&#29992;&#12289;&#26377;&#20559;&#35265;&#30340;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#21644;&#23545;&#36825;&#20123;&#31038;&#32676;&#26045;&#21152;&#30340;&#19981;&#25104;&#27604;&#20363;&#30340;&#24515;&#29702;&#20581;&#24247;&#36127;&#25285;&#65292;&#21152;&#21095;&#20102;&#29616;&#26377;&#30340;&#19981;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#36164;&#26412;&#20027;&#20041;&#12289;&#31181;&#26063;&#21387;&#36843;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#20803;&#32032;&#22914;&#20309;&#20849;&#21516;&#21152;&#28145;&#31038;&#20250;&#19981;&#24179;&#31561;&#12290;&#36890;&#36807;&#36861;&#28335;&#21382;&#21490;&#19978;&#23545;&#36793;&#32536;&#31038;&#32676;&#30340;&#21093;&#21066;&#65292;&#35813;&#30740;&#31350;&#34920;&#26126;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19981;&#20165;&#21453;&#26144;&#20102;&#31038;&#20250;&#20559;&#35265;&#65292;&#32780;&#19988;&#21152;&#21095;&#20102;&#31181;&#26063;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06332v1 Announce Type: cross  Abstract: This article investigates the complex nexus of capitalism, racial oppression, and artificial intelligence (AI), revealing how these elements coalesce to deepen social inequities. By tracing the historical exploitation of marginalized communities through capitalist practices, the study demonstrates how AI technologies not only reflect but also amplify societal biases, particularly in exacerbating racial disparities. Through a focused analysis, the paper presents how AI's development and application exploit marginalized groups via mechanisms such as gig economy labor abuses, biased facial recognition technologies, and the disproportionate mental health burdens placed on these communities. These examples underscore the critical role of AI in reinforcing and intensifying existing inequalities. Concluding that unregulated AI significantly threatens to compound current oppressions, the article calls for a concerted effort towards responsible
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ACT&#26694;&#26550;&#65292;&#36890;&#36807;&#32422;&#26463;&#39564;&#35777;&#22120;&#33258;&#21160;&#35745;&#31639;&#27599;&#20010;&#21709;&#24212;&#30340;&#32422;&#26463;&#28385;&#24847;&#29575;&#65292;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#19982;&#33258;&#21160;&#32422;&#26463;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.06326</link><description>&lt;p&gt;
&#20174;&#25351;&#20196;&#21040;&#32422;&#26463;&#65306;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#19982;&#33258;&#21160;&#32422;&#26463;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06326
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ACT&#26694;&#26550;&#65292;&#36890;&#36807;&#32422;&#26463;&#39564;&#35777;&#22120;&#33258;&#21160;&#35745;&#31639;&#27599;&#20010;&#21709;&#24212;&#30340;&#32422;&#26463;&#28385;&#24847;&#29575;&#65292;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#19982;&#33258;&#21160;&#32422;&#26463;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23545;&#40784;&#23545;&#20110;&#23558;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35843;&#25972;&#20026;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#20026;&#25152;&#26377;&#31867;&#22411;&#30340;&#25351;&#20196;&#25552;&#20379;&#20154;&#31867;&#27880;&#37322;&#65292;&#29305;&#21035;&#26159;&#20855;&#26377;&#23450;&#21046;&#32422;&#26463;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29992;&#25143;&#25351;&#20196;&#36890;&#24120;&#21253;&#21547;&#32422;&#26463;&#26465;&#20214;&#12290;&#34429;&#28982;&#35780;&#20272;&#25972;&#20010;&#25351;&#20196;&#30340;&#21709;&#24212;&#36136;&#37327;&#36890;&#24120;&#25104;&#26412;&#39640;&#26114;&#65292;&#20294;&#39640;&#25928;&#22320;&#35780;&#20272;&#32422;&#26463;&#26465;&#20214;&#30340;&#28385;&#24847;&#29575;&#26159;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;NLP&#20219;&#21153;&#20013;&#30340;&#24120;&#35265;&#32422;&#26463;&#26465;&#20214;&#65292;&#23558;&#23427;&#20204;&#22522;&#20110;&#20854;&#21442;&#25968;&#31867;&#22411;&#20998;&#31867;&#20026;&#19977;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;ACT&#65288;Aligning to ConsTraints&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#20026;&#24102;&#32422;&#26463;&#29992;&#25143;&#23545;&#40784;&#29983;&#25104;&#30417;&#30563;&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ACT&#20351;&#29992;&#32422;&#26463;&#39564;&#35777;&#22120;&#65292;&#36825;&#20123;&#39564;&#35777;&#22120;&#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#26131;&#20110;&#23454;&#29616;&#65292;&#26469;&#35745;&#31639;&#27599;&#20010;&#21709;&#24212;&#30340;&#32422;&#26463;&#28385;&#24847;&#29575;&#65288;CSR&#65289;&#12290;&#23427;&#20026;&#27599;&#20010;&#25552;&#31034;&#21462;&#26679;&#22810;&#20010;&#21709;&#24212;&#24182;&#25910;&#38598;&#20559;&#22909;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06326v1 Announce Type: cross  Abstract: User alignment is crucial for adapting general-purpose language models (LMs) to downstream tasks, but human annotations are often not available for all types of instructions, especially those with customized constraints. We observe that user instructions typically contain constraints. While assessing response quality in terms of the whole instruction is often costly, efficiently evaluating the satisfaction rate of constraints is feasible. We investigate common constraints in NLP tasks, categorize them into three classes based on the types of their arguments, and propose a unified framework, ACT (Aligning to ConsTraints), to automatically produce supervision signals for user alignment with constraints. Specifically, ACT uses constraint verifiers, which are typically easy to implement in practice, to compute constraint satisfaction rate (CSR) of each response. It samples multiple responses for each prompt and collect preference labels ba
&lt;/p&gt;</description></item><item><title>&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20013;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#29616;&#19981;&#23384;&#22312;&#30340;&#35780;&#20272;&#21644;&#22686;&#24378;&#29616;&#26377;&#35780;&#20272;&#30340;&#39057;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#24037;&#20316;&#20154;&#21592;&#24037;&#20316;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.06322</link><description>&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#20013;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#26816;&#26597;&#25506;&#35270;&#21644;&#27963;&#21160;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Leveraging Computer Vision in the Intensive Care Unit (ICU) for Examining Visitation and Mobility
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06322
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20013;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#29616;&#19981;&#23384;&#22312;&#30340;&#35780;&#20272;&#21644;&#22686;&#24378;&#29616;&#26377;&#35780;&#20272;&#30340;&#39057;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#24037;&#20316;&#20154;&#21592;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23494;&#20999;&#30417;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#24739;&#32773;&#30340;&#37325;&#35201;&#24615;&#65292;&#30001;&#20110;&#21307;&#25252;&#20154;&#21592;&#38754;&#20020;&#30340;&#26102;&#38388;&#38480;&#21046;&#65292;&#35768;&#22810;&#26041;&#38754;&#20173;&#28982;&#21463;&#21040;&#38480;&#21046;&#35780;&#20272;&#12290;&#36807;&#24230;&#30340;&#25506;&#35270;&#21487;&#33021;&#22312;&#20241;&#24687;&#26102;&#38388;&#21152;&#21095;&#24490;&#29615;&#33410;&#24459;&#32010;&#20081;&#21644;&#35893;&#22916;&#30340;&#39118;&#38505;&#65292;&#20294;&#22312;ICU&#20013;&#24182;&#26410;&#34987;&#25429;&#25417;&#12290;&#21516;&#26679;&#65292;&#27963;&#21160;&#33021;&#21147;&#21487;&#20197;&#26159;ICU&#24739;&#32773;&#24247;&#22797;&#25110;&#24694;&#21270;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#20294;&#21482;&#34987;&#38646;&#26143;&#22320;&#25429;&#25417;&#25110;&#26681;&#26412;&#19981;&#34987;&#25429;&#25417;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#25214;&#21040;&#20102;&#24212;&#29992;&#65292;&#20943;&#36731;&#20102;&#20154;&#21147;&#36127;&#25285;&#12290;&#22312;ICU&#20013;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#20063;&#26377;&#21487;&#33021;&#23454;&#29616;&#19981;&#23384;&#22312;&#30340;&#35780;&#20272;&#25110;&#22686;&#24378;&#29616;&#26377;&#35780;&#20272;&#30340;&#39057;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#24037;&#20316;&#20154;&#21592;&#30340;&#24037;&#20316;&#37327;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#25104;&#20687;&#30340;&#26368;&#26032;&#38750;&#20405;&#20837;&#24335;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06322v1 Announce Type: cross  Abstract: Despite the importance of closely monitoring patients in the Intensive Care Unit (ICU), many aspects are still assessed in a limited manner due to the time constraints imposed on healthcare providers. For example, although excessive visitations during rest hours can potentially exacerbate the risk of circadian rhythm disruption and delirium, it is not captured in the ICU. Likewise, while mobility can be an important indicator of recovery or deterioration in ICU patients, it is only captured sporadically or not captured at all. In the past few years, the computer vision field has found application in many domains by reducing the human burden. Using computer vision systems in the ICU can also potentially enable non-existing assessments or enhance the frequency and accuracy of existing assessments while reducing the staff workload. In this study, we leverage a state-of-the-art noninvasive computer vision system based on depth imaging to c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#24418;&#34920;&#31034;&#20026;&#32593;&#26684;&#24314;&#31435;&#21487;&#24494;&#35843;&#30340;&#24418;&#29366;&#23545;&#24212;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#29983;&#25104;&#36924;&#30495;&#30340;&#21512;&#25104;&#24418;&#29366;&#65292;&#24182;&#25193;&#23637;&#20026;&#32852;&#21512;&#24418;&#29366;&#29983;&#25104;-&#32858;&#31867;&#22810;&#22270;&#35889;&#26694;&#26550;&#20197;&#22686;&#21152;&#21464;&#24322;&#24615;&#21644;&#20445;&#30041;&#29983;&#25104;&#24418;&#29366;&#20013;&#30340;&#26356;&#22810;&#32454;&#33410;</title><link>https://arxiv.org/abs/2403.06317</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21487;&#24494;&#35843;&#24418;&#29366;&#21305;&#37197;&#21644;&#29983;&#25104;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An End-to-End Deep Learning Generative Framework for Refinable Shape Matching and Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#24418;&#34920;&#31034;&#20026;&#32593;&#26684;&#24314;&#31435;&#21487;&#24494;&#35843;&#30340;&#24418;&#29366;&#23545;&#24212;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#29983;&#25104;&#36924;&#30495;&#30340;&#21512;&#25104;&#24418;&#29366;&#65292;&#24182;&#25193;&#23637;&#20026;&#32852;&#21512;&#24418;&#29366;&#29983;&#25104;-&#32858;&#31867;&#22810;&#22270;&#35889;&#26694;&#26550;&#20197;&#22686;&#21152;&#21464;&#24322;&#24615;&#21644;&#20445;&#30041;&#29983;&#25104;&#24418;&#29366;&#20013;&#30340;&#26356;&#22810;&#32454;&#33410;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#30340;&#29983;&#25104;&#24314;&#27169;&#26159;&#20307;&#20869;&#20020;&#24202;&#35797;&#39564;&#65288;ISCTs&#65289;&#30340;&#20808;&#20915;&#26465;&#20214;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#35299;&#21078;&#24418;&#29366;&#65288;&#36890;&#24120;&#34920;&#31034;&#20026;3D&#34920;&#38754;&#32593;&#26684;&#65289;&#20197;&#32463;&#27982;&#26377;&#25928;&#30340;&#26041;&#24335;&#39564;&#35777;&#21307;&#30103;&#35774;&#22791;&#24178;&#39044;&#25514;&#26045;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#33021;&#22815;&#29983;&#25104;&#19982;&#30495;&#23454;&#32593;&#26684;&#26679;&#26412;&#23494;&#20999;&#31867;&#20284;&#30340;&#24418;&#29366;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#22312;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#21487;&#21464;&#30340;&#39030;&#28857;&#35745;&#25968;&#12289;&#36830;&#25509;&#24615;&#20197;&#21450;&#32570;&#20047;&#23494;&#38598;&#30340;&#39030;&#28857;&#23545;&#24212;&#12290;&#36890;&#36807;&#20351;&#29992;&#32593;&#26684;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#24314;&#31435;&#28508;&#22312;&#31354;&#38388;&#20013;&#21487;&#24494;&#35843;&#30340;&#24418;&#29366;&#23545;&#24212;&#65292;&#26500;&#24314;&#22522;&#20110;&#20154;&#21475;&#25968;&#25454;&#30340;&#22270;&#35889;&#24182;&#29983;&#25104;&#36924;&#30495;&#30340;&#21512;&#25104;&#24418;&#29366;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#30784;&#27169;&#22411;&#25193;&#23637;&#20026;&#32852;&#21512;&#24418;&#29366;&#29983;&#25104;-&#32858;&#31867;&#22810;&#22270;&#35889;&#26694;&#26550;&#65292;&#20197;&#22686;&#21152;&#26356;&#22810;&#30340;&#21464;&#24322;&#24615;&#24182;&#22312;&#29983;&#25104;&#30340;&#24418;&#29366;&#20013;&#20445;&#30041;&#26356;&#22810;&#32454;&#33410;&#12290;&#21033;&#29992;&#32925;&#33039;&#21644;&#24038;&#24515;&#38745;&#33033;&#20316;&#20026;&#23454;&#39564;&#23545;&#35937;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06317v1 Announce Type: cross  Abstract: Generative modelling for shapes is a prerequisite for In-Silico Clinical Trials (ISCTs), which aim to cost-effectively validate medical device interventions using synthetic anatomical shapes, often represented as 3D surface meshes. However, constructing AI models to generate shapes closely resembling the real mesh samples is challenging due to variable vertex counts, connectivities, and the lack of dense vertex-wise correspondences across the training data. Employing graph representations for meshes, we develop a novel unsupervised geometric deep-learning model to establish refinable shape correspondences in a latent space, construct a population-derived atlas and generate realistic synthetic shapes. We additionally extend our proposed base model to a joint shape generative-clustering multi-atlas framework to incorporate further variability and preserve more details in the generated shapes. Experimental results using liver and left-ven
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;$L_0$&#33539;&#25968;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#20248;&#31574;&#30053;&#31232;&#30095;&#21270;&#21644;&#20302;&#31209;&#20998;&#35299;</title><link>https://arxiv.org/abs/2403.06313</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#20248;&#31574;&#30053;&#31232;&#30095;&#21270;&#21644;&#20302;&#31209;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06313
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;$L_0$&#33539;&#25968;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#20248;&#31574;&#30053;&#31232;&#30095;&#21270;&#21644;&#20302;&#31209;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#31561;&#22810;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#32791;&#36153;&#20102;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23548;&#33268;&#23494;&#38598;&#31574;&#30053;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#23494;&#38598;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#25512;&#29702;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36793;&#32536;&#35745;&#31639;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#38480;&#21046;&#36807;&#25311;&#21512;&#21644;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#65292;&#30740;&#31350;&#32773;&#24050;&#32463;&#20351;&#29992;&#20102;&#20687;&#21098;&#26525;&#21644;&#22855;&#24322;&#20540;&#20998;&#35299;&#36825;&#26679;&#30340;&#25216;&#26415;&#26469;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#31232;&#30095;&#21270;&#21644;&#27169;&#22411;&#21387;&#32553;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#23548;&#33268;&#20102;&#24615;&#33021;&#27425;&#20248;&#65292;&#22312;&#22870;&#21169;&#26041;&#38754;&#20986;&#29616;&#26174;&#33879;&#30340;&#20943;&#24369;&#12290;&#22312;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#24320;&#21457;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;$L_1$&#21644;$L_2$&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#23454;&#29616;&#23578;&#19981;&#26126;&#26174;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$L_0$&#33539;&#25968;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26368;&#20248;&#31232;&#30095;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06313v1 Announce Type: cross  Abstract: Deep reinforcement learning(DRL) has shown significant promise in a wide range of applications including computer games and robotics. Yet, training DRL policies consume extraordinary computing resources resulting in dense policies which are prone to overfitting. Moreover, inference with dense DRL policies limit their practical applications, especially in edge computing. Techniques such as pruning and singular value decomposition have been used with deep learning models to achieve sparsification and model compression to limit overfitting and reduce memory consumption. However, these techniques resulted in sub-optimal performance with notable decay in rewards. $L_1$ and $L_2$ regularization techniques have been proposed for neural network sparsification and sparse auto-encoder development, but their implementation in DRL environments has not been apparent. We propose a novel $L_0$-norm-regularization technique using an optimal sparsity m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20105;&#35770;&#26041;&#26696;&#30340;&#33258;&#25105;&#35770;&#35777;&#36845;&#20195;&#21644;&#26500;&#24314;&#20105;&#35770;&#36807;&#31243;&#65292;ArgMed-Agents&#23454;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#21487;&#35299;&#37322;&#20020;&#24202;&#20915;&#31574;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#20020;&#24202;&#20915;&#31574;&#30340;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2403.06294</link><description>&lt;p&gt;
ArgMed-Agents: &#20351;&#29992;&#20105;&#35758;&#26041;&#26696;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#20020;&#24202;&#20915;&#31574;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06294
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20105;&#35770;&#26041;&#26696;&#30340;&#33258;&#25105;&#35770;&#35777;&#36845;&#20195;&#21644;&#26500;&#24314;&#20105;&#35770;&#36807;&#31243;&#65292;ArgMed-Agents&#23454;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#21487;&#35299;&#37322;&#20020;&#24202;&#20915;&#31574;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#20020;&#24202;&#20915;&#31574;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06294v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20020;&#24202;&#25512;&#29702;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#39318;&#20808;&#65292;&#34429;&#28982;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#35268;&#21010;&#26041;&#38754;&#30340;&#34920;&#29616;&#21364;&#19981;&#23613;&#20154;&#24847;&#12290;&#20854;&#27425;&#65292;LLMs&#20351;&#29992;&#19981;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#65292;&#36825;&#19982;&#20020;&#24202;&#21307;&#29983;&#30340;&#35748;&#30693;&#36807;&#31243;&#26412;&#36136;&#19978;&#19981;&#21516;&#65292;&#23548;&#33268;&#29992;&#25143;&#19981;&#20449;&#20219;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ArgMed-Agents&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20132;&#20114;&#20351;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#33021;&#22815;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#20020;&#24202;&#20915;&#31574;&#25512;&#29702;&#12290;ArgMed-Agents&#36890;&#36807;&#20020;&#24202;&#20915;&#31574;&#35770;&#25454;&#65288;&#19968;&#31181;&#27169;&#25311;&#20020;&#24202;&#20915;&#31574;&#35748;&#30693;&#36807;&#31243;&#30340;&#25512;&#29702;&#26426;&#21046;&#65289;&#25191;&#34892;&#33258;&#35770;&#35777;&#36845;&#20195;&#65292;&#28982;&#21518;&#23558;&#20105;&#35770;&#36807;&#31243;&#26500;&#24314;&#20026;&#34920;&#31034;&#20914;&#31361;&#20851;&#31995;&#30340;&#26377;&#21521;&#22270;&#12290;&#26368;&#32456;&#65292;Reasoner&#65288;&#19968;&#31181;&#31526;&#21495;&#27714;&#35299;&#22120;&#65289;&#35782;&#21035;&#20986;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06294v1 Announce Type: new  Abstract: There are two main barriers to using large language models (LLMs) in clinical reasoning. Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician's cognitive processes. This leads to user distrust. In this paper, we present a multi-agent framework called ArgMed-Agents, which aims to enable LLM-based agents to make explainable clinical decision reasoning through interaction. ArgMed-Agents performs self-argumentation iterations via Argumentation Scheme for Clinical Decision (a reasoning mechanism for modeling cognitive processes in clinical reasoning), and then constructs the argumentation process as a directed graph representing conflicting relationships. Ultimately, Reasoner(a symbolic solver) identify a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#20154;&#24037;&#26631;&#27880;&#35823;&#24046;&#19981;&#20165;&#19982;&#21512;&#25104;&#26631;&#31614;&#38169;&#35823;&#26377;&#26174;&#33879;&#19981;&#21516;&#65292;&#32780;&#19988;&#22312;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#26500;&#25104;&#20102;&#29420;&#29305;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20154;&#24037;&#26631;&#27880;&#35823;&#24046;&#30340;SCL&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.06289</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#20943;&#36731;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#20154;&#24037;&#26631;&#27880;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Understanding and Mitigating Human-Labelling Errors in Supervised Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#20154;&#24037;&#26631;&#27880;&#35823;&#24046;&#19981;&#20165;&#19982;&#21512;&#25104;&#26631;&#31614;&#38169;&#35823;&#26377;&#26174;&#33879;&#19981;&#21516;&#65292;&#32780;&#19988;&#22312;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#26500;&#25104;&#20102;&#29420;&#29305;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20154;&#24037;&#26631;&#27880;&#35823;&#24046;&#30340;SCL&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;arXiv:2403.06289v1&#20844;&#24320;&#30340;&#20132;&#21449;&#31867;&#22411;&#30340;&#25991;&#25688;&#21487;&#20197;&#24471;&#30693;&#65292;&#20154;&#24037;&#26631;&#27880;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#21253;&#21547;&#19968;&#37096;&#20998;&#20154;&#24037;&#26631;&#27880;&#38169;&#35823;&#30340;&#31034;&#20363;&#12290;&#23613;&#31649;&#36825;&#31867;&#26631;&#27880;&#38169;&#35823;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#36127;&#38754;&#24433;&#21709;&#24050;&#32463;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#23545;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;SCL&#65289;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#30456;&#23545;&#26410;&#30693;&#30340;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20154;&#24037;&#26631;&#27880;&#38169;&#35823;&#19981;&#20165;&#19982;&#21512;&#25104;&#26631;&#31614;&#38169;&#35823;&#26174;&#33879;&#19981;&#21516;&#65292;&#32780;&#19988;&#22312;SCL&#20013;&#26500;&#25104;&#29420;&#29305;&#25361;&#25112;&#65292;&#19982;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#25361;&#25112;&#26377;&#25152;&#19981;&#21516;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#23427;&#20204;&#20316;&#20026;&#35823;&#25253;&#26679;&#26412;&#20986;&#29616;&#26102;&#65292;&#23427;&#20204;&#20250;&#23545;&#23398;&#20064;&#36807;&#31243;&#36896;&#25104;&#22823;&#32422;99%&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#24050;&#26377;&#30340;&#22122;&#22768;&#32531;&#35299;&#26041;&#27861;&#20027;&#35201;&#20391;&#37325;&#20110;&#21512;&#25104;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#22788;&#29702;&#38750;&#24120;&#39640;&#21512;&#25104;&#22122;&#22768;&#29575;&#65288;40-80%&#65289;&#30340;&#19981;&#20999;&#23454;&#38469;&#35774;&#32622;&#65292;&#20294;&#30001;&#20110;&#36807;&#24230;&#25311;&#21512;&#65292;&#23427;&#20204;&#22312;&#26222;&#36890;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#24448;&#24448;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20154;&#24037;&#26631;&#27880;&#40065;&#26834;&#24615;&#30340;SCL&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06289v1 Announce Type: cross  Abstract: Human-annotated vision datasets inevitably contain a fraction of human mislabelled examples. While the detrimental effects of such mislabelling on supervised learning are well-researched, their influence on Supervised Contrastive Learning (SCL) remains largely unexplored. In this paper, we show that human-labelling errors not only differ significantly from synthetic label errors, but also pose unique challenges in SCL, different to those in traditional supervised learning methods. Specifically, our results indicate they adversely impact the learning process in the ~99% of cases when they occur as false positive samples. Existing noise-mitigating methods primarily focus on synthetic label errors and tackle the unrealistic setting of very high synthetic noise rates (40-80%), but they often underperform on common image datasets due to overfitting. To address this issue, we introduce a novel SCL objective with robustness to human-labelling
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNICORN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#21644;&#33258;&#36866;&#24212;&#23454;&#29616;&#36229;&#22768;&#32435;&#21345;&#21152;&#31859;&#25104;&#20687;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#21644;&#20998;&#36776;&#29575;&#36136;&#37327;&#19978;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.06275</link><description>&lt;p&gt;
UNICORN: &#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#21644;&#33258;&#36866;&#24212;&#23454;&#29616;&#30340;&#36229;&#22768;&#32435;&#21345;&#21152;&#31859;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
UNICORN: Ultrasound Nakagami Imaging via Score Matching and Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06275
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNICORN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#21644;&#33258;&#36866;&#24212;&#23454;&#29616;&#36229;&#22768;&#32435;&#21345;&#21152;&#31859;&#25104;&#20687;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#21644;&#20998;&#36776;&#29575;&#36136;&#37327;&#19978;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Nakagami&#25104;&#20687;&#22312;&#36229;&#22768;&#27874;&#20013;&#21487;&#35270;&#21270;&#21644;&#37327;&#21270;&#32452;&#32455;&#25955;&#23556;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#22312;&#32959;&#30244;&#35786;&#26029;&#21644;&#33026;&#32938;&#20998;&#25968;&#20272;&#35745;&#31561;&#39046;&#22495;&#26377;&#28508;&#22312;&#24212;&#29992;&#65292;&#32780;&#36825;&#20123;&#39046;&#22495;&#24456;&#38590;&#36890;&#36807;&#20256;&#32479;&#36229;&#22768;B&#27169;&#24335;&#22270;&#20687;&#20998;&#36776;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#36873;&#25321;&#26368;&#20248;&#31383;&#21475;&#22823;&#23567;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#19988;&#30001;&#20110;&#20272;&#35745;&#19981;&#31283;&#23450;&#24615;&#32780;&#23548;&#33268;&#20998;&#36776;&#29575;&#38477;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNICORN&#65288;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#21644;&#33258;&#36866;&#24212;&#23454;&#29616;&#30340;&#36229;&#22768;&#32435;&#21345;&#21152;&#31859;&#25104;&#20687;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#20934;&#30830;&#30340;&#65292;&#23553;&#38381;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#36229;&#22768;&#21253;&#32476;&#30340;&#20998;&#25968;&#20989;&#25968;&#26469;&#20272;&#35745;&#21345;&#21152;&#31859;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#36229;&#22768;RF&#25968;&#25454;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;UNICORN&#22312;&#20934;&#30830;&#24615;&#21644;&#20998;&#36776;&#29575;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06275v1 Announce Type: cross  Abstract: Nakagami imaging holds promise for visualizing and quantifying tissue scattering in ultrasound waves, with potential applications in tumor diagnosis and fat fraction estimation which are challenging to discern by conventional ultrasound B-mode images. Existing methods struggle with optimal window size selection and suffer from estimator instability, leading to degraded resolution images. To address this, here we propose a novel method called UNICORN (Ultrasound Nakagami Imaging via Score Matching and Adaptation), that offers an accurate, closed-form estimator for Nakagami parameter estimation in terms of the score function of ultrasonic envelope. Extensive experiments using simulation and real ultrasound RF data demonstrate UNICORN's superiority over conventional approaches in accuracy and resolution quality.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29289;&#29702;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36712;&#36857;&#25968;&#25454;&#20013;&#35782;&#21035;&#24322;&#24120;&#38388;&#38553;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#31038;&#20250;&#24212;&#29992;&#21644;&#25216;&#26415;&#38590;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.06268</link><description>&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#30340;&#24322;&#24120;&#36712;&#36857;&#38388;&#38553;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Physics-Guided Abnormal Trajectory Gap Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06268
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36712;&#36857;&#25968;&#25454;&#20013;&#35782;&#21035;&#24322;&#24120;&#38388;&#38553;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#31038;&#20250;&#24212;&#29992;&#21644;&#25216;&#26415;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#38388;&#38553;&#65288;&#21363;&#32570;&#22833;&#25968;&#25454;&#65289;&#30340;&#36712;&#36857;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#35782;&#21035;&#36712;&#36857;&#20013;&#30340;&#24322;&#24120;&#38388;&#38553;&#30340;&#31639;&#27861;&#65292;&#36825;&#31181;&#24322;&#24120;&#38388;&#38553;&#22312;&#19968;&#20010;&#31227;&#21160;&#29289;&#20307;&#27809;&#26377;&#25253;&#21578;&#20854;&#20301;&#32622;&#26102;&#21457;&#29983;&#65292;&#20294;&#21516;&#19968;&#22320;&#29702;&#21306;&#22495;&#30340;&#20854;&#20182;&#31227;&#21160;&#29289;&#20307;&#21608;&#26399;&#24615;&#22320;&#25253;&#21578;&#20301;&#32622;&#12290;&#35813;&#38382;&#39064;&#30001;&#20110;&#20854;&#23545;&#31038;&#20250;&#30340;&#37325;&#35201;&#24212;&#29992;&#32780;&#21464;&#24471;&#37325;&#35201;&#65292;&#22914;&#25913;&#21892;&#28023;&#19978;&#23433;&#20840;&#21644;&#23545;&#20840;&#29699;&#23433;&#20840;&#38382;&#39064;&#65288;&#22914;&#38750;&#27861;&#25429;&#40060;&#12289;&#38750;&#27861;&#36755;&#27833;&#21644;&#36716;&#36816;&#27963;&#21160;&#65289;&#30340;&#30417;&#31649;&#25191;&#34892;&#12290;&#35813;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#24456;&#38590;&#38480;&#23450;&#22312;&#36712;&#36857;&#38388;&#38553;&#26399;&#38388;&#31227;&#21160;&#29289;&#20307;&#30340;&#21487;&#33021;&#20301;&#32622;&#65292;&#24182;&#19988;&#26816;&#27979;&#22914;&#27492;&#22823;&#37327;&#20301;&#32622;&#25968;&#25454;&#20013;&#30340;&#38388;&#38553;&#30340;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#39640;&#12290;&#30446;&#21069;&#20851;&#20110;&#24322;&#24120;&#36712;&#36857;&#26816;&#27979;&#30340;&#25991;&#29486;&#20551;&#35774;&#22312;&#38388;&#38553;&#20869;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#26816;&#27979;&#21040;&#24322;&#24120;&#38388;&#38553;&#65292;&#22240;&#20026;&#22312;&#32473;&#23450;&#21306;&#22495;&#20869;&#30340;&#29289;&#20307;&#21487;&#33021;&#24050;&#32463;&#20559;&#31163;&#26368;&#30701;&#36335;&#24452;&#12290;&#22312;&#21021;&#27493;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06268v1 Announce Type: cross  Abstract: Given trajectories with gaps (i.e., missing data), we investigate algorithms to identify abnormal gaps in trajectories which occur when a given moving object did not report its location, but other moving objects in the same geographic region periodically did. The problem is important due to its societal applications, such as improving maritime safety and regulatory enforcement for global security concerns such as illegal fishing, illegal oil transfers, and trans-shipments. The problem is challenging due to the difficulty of bounding the possible locations of the moving object during a trajectory gap, and the very high computational cost of detecting gaps in such a large volume of location data. The current literature on anomalous trajectory detection assumes linear interpolation within gaps, which may not be able to detect abnormal gaps since objects within a given region may have traveled away from their shortest path. In preliminary 
&lt;/p&gt;</description></item><item><title>FARPLS&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#24449;&#22686;&#24378;&#30340;&#26426;&#22120;&#20154;&#36712;&#36857;&#20559;&#22909;&#26631;&#27880;&#31995;&#32479;&#65292;&#29992;&#20110;&#24110;&#21161;&#20154;&#31867;&#26631;&#27880;&#32773;&#22312;&#20559;&#22909;&#33719;&#21462;&#36807;&#31243;&#20013;&#36991;&#20813;&#24573;&#35270;&#37325;&#35201;&#29305;&#24449;&#12289;&#24418;&#25104;&#20559;&#35265;&#26631;&#20934;&#21644;&#22240;&#27604;&#36739;&#36807;&#22810;&#32780;&#23548;&#33268;&#26631;&#27880;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06267</link><description>&lt;p&gt;
FARPLS: &#19968;&#20010;&#29305;&#24449;&#22686;&#24378;&#30340;&#26426;&#22120;&#20154;&#36712;&#36857;&#20559;&#22909;&#26631;&#27880;&#31995;&#32479;&#65292;&#29992;&#20110;&#21327;&#21161;&#20154;&#31867;&#26631;&#27880;&#32773;&#30340;&#20559;&#22909;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
FARPLS: A Feature-Augmented Robot Trajectory Preference Labeling System to Assist Human Labelers' Preference Elicitation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06267
&lt;/p&gt;
&lt;p&gt;
FARPLS&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#24449;&#22686;&#24378;&#30340;&#26426;&#22120;&#20154;&#36712;&#36857;&#20559;&#22909;&#26631;&#27880;&#31995;&#32479;&#65292;&#29992;&#20110;&#24110;&#21161;&#20154;&#31867;&#26631;&#27880;&#32773;&#22312;&#20559;&#22909;&#33719;&#21462;&#36807;&#31243;&#20013;&#36991;&#20813;&#24573;&#35270;&#37325;&#35201;&#29305;&#24449;&#12289;&#24418;&#25104;&#20559;&#35265;&#26631;&#20934;&#21644;&#22240;&#27604;&#36739;&#36807;&#22810;&#32780;&#23548;&#33268;&#26631;&#27880;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#23398;&#20064;&#26088;&#22312;&#20351;&#26426;&#22120;&#20154;&#30340;&#20219;&#21153;&#30446;&#26631;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#25512;&#26029;&#20154;&#31867;&#20559;&#22909;&#30340;&#26368;&#24120;&#35265;&#26041;&#27861;&#20043;&#19968;&#26159;&#36890;&#36807;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#36712;&#36857;&#30340;&#20004;&#20004;&#27604;&#36739;&#12290;&#20256;&#32479;&#22522;&#20110;&#27604;&#36739;&#30340;&#20559;&#22909;&#26631;&#27880;&#31995;&#32479;&#24456;&#23569;&#25903;&#25345;&#26631;&#27880;&#32773;&#28040;&#21270;&#21644;&#30830;&#23450;&#35270;&#39057;&#20013;&#35760;&#24405;&#30340;&#22797;&#26434;&#36712;&#36857;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#24418;&#25104;&#24615;&#30740;&#31350;&#65288;N = 12&#65289;&#34920;&#26126;&#65292;&#20010;&#20307;&#21487;&#33021;&#20250;&#24573;&#35270;&#38750;&#26174;&#33879;&#30340;&#20219;&#21153;&#29305;&#24449;&#65292;&#24182;&#22312;&#20559;&#22909;&#33719;&#21462;&#36807;&#31243;&#20013;&#24314;&#31435;&#26377;&#20559;&#35265;&#30340;&#20559;&#22909;&#26631;&#20934;&#65292;&#22240;&#20026;&#23384;&#22312;&#37096;&#20998;&#35266;&#23519;&#12290;&#27492;&#22806;&#65292;&#24403;&#32473;&#23450;&#35768;&#22810;&#37197;&#23545;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#20182;&#20204;&#21487;&#33021;&#20250;&#24863;&#21040;&#31934;&#31070;&#30130;&#21171;&#65292;&#23548;&#33268;&#26631;&#27880;&#36136;&#37327;&#19979;&#38477;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FARPLS&#65292;&#19968;&#20010;&#29305;&#24449;&#22686;&#24378;&#30340;&#26426;&#22120;&#20154;&#36712;&#36857;&#20559;&#22909;&#26631;&#27880;&#31995;&#32479;&#12290;FARPLS&#31361;&#20986;&#26174;&#31034;&#19982;&#20154;&#31867;&#30456;&#20851;&#30340;&#21508;&#31181;&#20219;&#21153;&#29305;&#24449;&#20013;&#30340;&#28508;&#22312;&#24322;&#24120;&#20540;&#65292;&#24182;&#25552;&#21462;&#30456;&#24212;&#30340;&#35270;&#39057;&#20851;&#38190;&#24103;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06267v1 Announce Type: cross  Abstract: Preference-based learning aims to align robot task objectives with human values. One of the most common methods to infer human preferences is by pairwise comparisons of robot task trajectories. Traditional comparison-based preference labeling systems seldom support labelers to digest and identify critical differences between complex trajectories recorded in videos. Our formative study (N = 12) suggests that individuals may overlook non-salient task features and establish biased preference criteria during their preference elicitation process because of partial observations. In addition, they may experience mental fatigue when given many pairs to compare, causing their label quality to deteriorate. To mitigate these issues, we propose FARPLS, a Feature-Augmented Robot trajectory Preference Labeling System. FARPLS highlights potential outliers in a wide variety of task features that matter to humans and extracts the corresponding video ke
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21387;&#32553;&#22312;&#20998;&#35789;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#35777;&#26126;&#20102;&#21387;&#32553;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#25104;&#21151;&#20043;&#38388;&#30340;&#23454;&#35777;&#37325;&#35201;&#24615;&#65292;&#24182;&#34920;&#26126;&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#19982;&#27169;&#22411;&#30340;&#24615;&#33021;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06265</link><description>&lt;p&gt;
&#25286;&#35299;&#20998;&#35789;&#65306;&#35780;&#20272;&#25991;&#26412;&#21387;&#32553;&#21450;&#20854;&#19982;&#27169;&#22411;&#24615;&#33021;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21387;&#32553;&#22312;&#20998;&#35789;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#35777;&#26126;&#20102;&#21387;&#32553;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#25104;&#21151;&#20043;&#38388;&#30340;&#23454;&#35777;&#37325;&#35201;&#24615;&#65292;&#24182;&#34920;&#26126;&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#19982;&#27169;&#22411;&#30340;&#24615;&#33021;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21387;&#32553;&#26159;BPE&#26368;&#24120;&#35265;&#30340;&#20998;&#35789;&#31639;&#27861;&#30340;&#37325;&#35201;&#22522;&#30784;&#65292;&#20294;&#20998;&#35789;&#36807;&#31243;&#20013;&#30340;&#21387;&#32553;&#37325;&#35201;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#35770;&#36848;&#20102;&#21387;&#32553;&#30340;&#29702;&#35770;&#37325;&#35201;&#24615;&#65292;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;0-gram&#35821;&#35328;&#24314;&#27169;&#65292;&#21363;&#20026;&#25152;&#26377;&#26631;&#35760;&#20998;&#37197;&#30456;&#31561;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21387;&#32553;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#25104;&#21151;&#30340;&#23454;&#35777;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#29992;&#25991;&#26723;&#30340;&#25968;&#37327;&#26469;&#25511;&#21046;&#22810;&#20010;BPE&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#33021;&#21147;&#65306;&#20174;100&#19975;&#20010;&#25991;&#26723;&#21040;&#30456;&#24403;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#22522;&#20110;&#23383;&#31526;&#30340;&#20998;&#35789;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#20123;&#20998;&#35789;&#22120;&#39044;&#35757;&#32451;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#19982;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;&#21387;&#32553;&#26159;&#20998;&#35789;&#30340;&#21487;&#38752;&#20869;&#22312;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06265v1 Announce Type: cross  Abstract: Despite it being the cornerstone of BPE, the most common tokenization algorithm, the importance of compression in the tokenization process is still unclear. In this paper, we argue for the theoretical importance of compression, that can be viewed as 0-gram language modeling where equal probability is assigned to all tokens. We also demonstrate the empirical importance of compression for downstream success of pre-trained language models. We control the compression ability of several BPE tokenizers by varying the amount of documents available during their training: from 1 million documents to a character-based tokenizer equivalent to no training data at all. We then pre-train English language models based on those tokenizers and fine-tune them over several tasks. We show that there is a correlation between tokenizers' compression and models' downstream performance, suggesting that compression is a reliable intrinsic indicator of tokeniza
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#33021;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#23450;&#20041;&#65292;&#20294;&#20063;&#21487;&#33021;&#36896;&#25104;LLMs&#20013;&#30456;&#20851;&#23454;&#20363;&#30693;&#35782;&#30340;&#25197;&#26354;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.06259</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Conceptual Knowledge for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#33021;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#23450;&#20041;&#65292;&#20294;&#20063;&#21487;&#33021;&#36896;&#25104;LLMs&#20013;&#30456;&#20851;&#23454;&#20363;&#30693;&#35782;&#30340;&#25197;&#26354;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21644;&#35780;&#20272;&#20165;&#25506;&#35752;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#32534;&#36753;&#65292;&#28982;&#32780;LLMs&#26159;&#21542;&#20855;&#26377;&#20462;&#25913;&#27010;&#24565;&#30340;&#33021;&#21147;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;LLMs&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;ConceptEdit&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#32423;&#21035;&#30340;&#23450;&#20041;&#65292;&#20294;&#23427;&#20204;&#20063;&#26377;&#28508;&#21147;&#25197;&#26354;LLMs&#20013;&#30456;&#20851;&#30340;&#23454;&#20363;&#30693;&#35782;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#26399;&#26395;&#36825;&#21487;&#20197;&#28608;&#21457;&#23545;&#26356;&#22909;&#29702;&#35299;LLMs&#30340;&#36827;&#19968;&#27493;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#20027;&#39029;&#20301;&#20110;https://zjunlp.github.io/project/ConceptEdit&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06259v1 Announce Type: cross  Abstract: Recently, there has been a growing interest in knowledge editing for Large Language Models (LLMs). Current approaches and evaluations merely explore the instance-level editing, while whether LLMs possess the capability to modify concepts remains unclear. This paper pioneers the investigation of editing conceptual knowledge for LLMs, by constructing a novel benchmark dataset ConceptEdit and establishing a suite of new metrics for evaluation. The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance. We anticipate this can inspire further progress in better understanding LLMs. Our project homepage is available at https://zjunlp.github.io/project/ConceptEdit.
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#29983;&#25104;&#31867;&#20284;&#36755;&#20837;&#22270;&#20687;&#30340;&#26080;&#32570;&#38519;&#25968;&#25454;&#22270;&#20687;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#22270;&#20687;&#31526;&#21512;&#26399;&#26395;&#20998;&#24067;&#65292;&#31283;&#23450;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06247</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;&#21464;&#20998;&#22270;&#20687;&#29983;&#25104;&#29992;&#20110;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#21644;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06247
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#29983;&#25104;&#31867;&#20284;&#36755;&#20837;&#22270;&#20687;&#30340;&#26080;&#32570;&#38519;&#25968;&#25454;&#22270;&#20687;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#22270;&#20687;&#31526;&#21512;&#26399;&#26395;&#20998;&#24067;&#65292;&#31283;&#23450;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#24341;&#23548;&#30340;&#21464;&#20998;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24037;&#19994;&#21046;&#36896;&#20013;&#24322;&#24120;&#26816;&#27979;&#30340;&#24178;&#20928;&#25968;&#25454;&#33719;&#21462;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26469;&#33258;&#24191;&#27867;&#25991;&#26412;&#24211;&#25991;&#26723;&#30340;&#20851;&#20110;&#30446;&#26631;&#23545;&#35937;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#29983;&#25104;&#31867;&#20284;&#36755;&#20837;&#22270;&#20687;&#30340;&#26080;&#32570;&#38519;&#25968;&#25454;&#22270;&#20687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30830;&#20445;&#29983;&#25104;&#30340;&#26080;&#32570;&#38519;&#22270;&#20687;&#19982;&#26469;&#33258;&#25991;&#26412;&#21644;&#22270;&#20687;&#30693;&#35782;&#30340;&#39044;&#26399;&#20998;&#24067;&#20445;&#25345;&#19968;&#33268;&#65292;&#30830;&#20445;&#31283;&#23450;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#30340;&#26080;&#32570;&#38519;&#25968;&#25454;&#65292;&#20063;&#33021;&#36229;&#36234;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36328;&#22235;&#20010;&#22522;&#20934;&#27169;&#22411;&#21644;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#27867;&#21270;&#27979;&#35797;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#25928;&#26524;&#30340;&#39069;&#22806;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06247v1 Announce Type: cross  Abstract: We propose a text-guided variational image generation method to address the challenge of getting clean data for anomaly detection in industrial manufacturing. Our method utilizes text information about the target object, learned from extensive text library documents, to generate non-defective data images resembling the input image. The proposed framework ensures that the generated non-defective images align with anticipated distributions derived from textual and image-based knowledge, ensuring stability and generality. Experimental results demonstrate the effectiveness of our approach, surpassing previous methods even with limited non-defective data. Our approach is validated through generalization tests across four baseline models and three distinct datasets. We present an additional analysis to enhance the effectiveness of anomaly detection models by utilizing the generated images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#20998;&#31867;&#19982;&#29702;&#24615;&#21270;&#65288;C2R&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#21644;&#29702;&#24615;&#21270;&#27169;&#22359;&#21327;&#21516;&#24037;&#20316;&#65292;&#25913;&#21892;&#23545;&#20998;&#24067;&#20043;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.06239</link><description>&lt;p&gt;
&#21512;&#20316;&#20998;&#31867;&#19982;&#29702;&#24615;&#21270;&#29992;&#20110;&#22270;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Cooperative Classification and Rationalization for Graph Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#20998;&#31867;&#19982;&#29702;&#24615;&#21270;&#65288;C2R&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#21644;&#29702;&#24615;&#21270;&#27169;&#22359;&#21327;&#21516;&#24037;&#20316;&#65292;&#25913;&#21892;&#23545;&#20998;&#24067;&#20043;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06239v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#30028; &#25688;&#35201;: &#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#38754;&#23545;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#26102;&#24456;&#38590;&#26377;&#25928;&#27867;&#21270;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20854;&#20013;&#20043;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#20462;&#25913;&#25968;&#25454;&#29615;&#22659;&#26469;&#20351;&#21407;&#22987;&#20998;&#31867;&#30340;&#35757;&#32451;&#20998;&#24067;&#22810;&#26679;&#21270;&#65292;&#20294;&#35775;&#38382;&#29615;&#22659;&#20449;&#24687;&#36739;&#20026;&#22797;&#26434;&#12290;&#21478;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#28041;&#21450;&#29702;&#24615;&#21270;&#65292;&#25552;&#21462;&#29992;&#20110;&#39044;&#27979;&#30340;&#19981;&#21464;&#21407;&#29702;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23398;&#20064;&#20449;&#21495;&#26377;&#38480;&#65292;&#25552;&#21462;&#21407;&#29702;&#26159;&#22256;&#38590;&#30340;&#65292;&#23548;&#33268;&#36739;&#19981;&#20934;&#30830;&#30340;&#21407;&#29702;&#21644;&#20943;&#24369;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#20998;&#31867;&#19982;&#29702;&#24615;&#21270;&#65288;C2R&#65289;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#27169;&#22359;&#21644;&#29702;&#24615;&#21270;&#27169;&#22359;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#20551;&#35774;&#20998;&#31867;&#20013;&#23384;&#22312;&#22810;&#20010;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06239v1 Announce Type: cross  Abstract: Graph Neural Networks (GNNs) have achieved impressive results in graph classification tasks, but they struggle to generalize effectively when faced with out-of-distribution (OOD) data. Several approaches have been proposed to address this problem. Among them, one solution is to diversify training distributions in vanilla classification by modifying the data environment, yet accessing the environment information is complex. Besides, another promising approach involves rationalization, extracting invariant rationales for predictions. However, extracting rationales is difficult due to limited learning signals, resulting in less accurate rationales and diminished predictions. To address these challenges, in this paper, we propose a Cooperative Classification and Rationalization (C2R) method, consisting of the classification and the rationalization module. Specifically, we first assume that multiple environments are available in the classif
&lt;/p&gt;</description></item><item><title>PNCs&#23558;&#27010;&#29575;&#30005;&#36335;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#28145;&#23618;&#28151;&#21512;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#21516;&#26102;&#20316;&#20026;&#24378;&#22823;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.06235</link><description>&lt;p&gt;
&#27010;&#29575;&#31070;&#32463;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Neural Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06235
&lt;/p&gt;
&lt;p&gt;
PNCs&#23558;&#27010;&#29575;&#30005;&#36335;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#28145;&#23618;&#28151;&#21512;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#21516;&#26102;&#20316;&#20026;&#24378;&#22823;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#36817;&#24180;&#26469;&#20316;&#20026;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25506;&#35752;&#25903;&#25345;&#21487;&#22788;&#29702;&#26597;&#35810;&#19988;&#36275;&#22815;&#34920;&#36798;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21487;&#22788;&#29702;&#24615;&#26159;&#26377;&#20195;&#20215;&#30340;&#65306;PCs&#27604;&#31070;&#32463;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#26356;&#24369;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27010;&#29575;&#31070;&#32463;&#30005;&#36335;&#65288;PNCs&#65289;&#65292;&#22312;&#21487;&#22788;&#29702;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#22312;PCs&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PNCs&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#28145;&#24230;&#28151;&#21512;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PNCs&#26500;&#25104;&#20102;&#24378;&#22823;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06235v1 Announce Type: cross  Abstract: Probabilistic circuits (PCs) have gained prominence in recent years as a versatile framework for discussing probabilistic models that support tractable queries and are yet expressive enough to model complex probability distributions. Nevertheless, tractability comes at a cost: PCs are less expressive than neural networks. In this paper we introduce probabilistic neural circuits (PNCs), which strike a balance between PCs and neural nets in terms of tractability and expressive power. Theoretically, we show that PNCs can be interpreted as deep mixtures of Bayesian networks. Experimentally, we demonstrate that PNCs constitute powerful function approximators.
&lt;/p&gt;</description></item><item><title>MoST&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#20316;&#39118;&#26684;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#39118;&#26684;&#19982;&#20869;&#23481;&#20998;&#31163;&#65292;&#22312;&#36716;&#31227;&#39118;&#26684;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#19981;&#21516;&#20869;&#23481;&#30340;&#21160;&#20316;&#23545;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.06225</link><description>&lt;p&gt;
MoST: &#22312;&#22810;&#26679;&#21160;&#20316;&#20869;&#23481;&#20043;&#38388;&#36827;&#34892;&#36816;&#21160;&#39118;&#26684;&#36716;&#25442;&#30340;&#21160;&#20316;&#39118;&#26684;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
MoST: Motion Style Transformer between Diverse Action Contents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06225
&lt;/p&gt;
&lt;p&gt;
MoST&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#20316;&#39118;&#26684;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#39118;&#26684;&#19982;&#20869;&#23481;&#20998;&#31163;&#65292;&#22312;&#36716;&#31227;&#39118;&#26684;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#19981;&#21516;&#20869;&#23481;&#30340;&#21160;&#20316;&#23545;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#26377;&#30340;&#21160;&#20316;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#22312;&#30456;&#21516;&#20869;&#23481;&#30340;&#20004;&#20010;&#21160;&#20316;&#20043;&#38388;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#24403;&#22312;&#19981;&#21516;&#20869;&#23481;&#30340;&#21160;&#20316;&#20043;&#38388;&#36716;&#31227;&#39118;&#26684;&#26102;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#21160;&#20316;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#20043;&#38388;&#32570;&#20047;&#26126;&#30830;&#30340;&#20998;&#31163;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#20316;&#39118;&#26684;&#36716;&#25442;&#22120;&#65292;&#26377;&#25928;&#22320;&#23558;&#39118;&#26684;&#19982;&#20869;&#23481;&#20998;&#31163;&#65292;&#24182;&#20174;&#28304;&#21160;&#20316;&#29983;&#25104;&#20855;&#26377;&#36716;&#31227;&#39118;&#26684;&#30340;&#21512;&#29702;&#21160;&#20316;&#12290;&#25105;&#20204;&#23454;&#29616;&#20998;&#31163;&#30446;&#26631;&#30340;&#29420;&#29305;&#26041;&#27861;&#20855;&#26377;&#21452;&#37325;&#24615;&#65306;(1) &#24102;&#26377;&#8220;&#37096;&#20998;&#20851;&#27880;&#39118;&#26684;&#35843;&#21046;&#22120;&#36328;&#36523;&#20307;&#37096;&#20301;&#8221;&#21644;&#8220;&#32534;&#30721;&#22120;&#32534;&#30721;&#39118;&#26684;&#21644;&#20869;&#23481;&#29305;&#24449;&#20998;&#24320;&#8221;&#30340;&#21160;&#20316;&#39118;&#26684;&#36716;&#25442;&#22120;&#30340;&#26032;&#26550;&#26500;&#65307;(2) &#39118;&#26684;&#20998;&#31163;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#34920;&#29616;&#20986;&#24322;&#24120;&#39640;&#30340;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#19981;&#21516;&#20869;&#23481;&#30340;&#21160;&#20316;&#23545;&#20013;&#65292;&#26080;&#38656;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06225v1 Announce Type: cross  Abstract: While existing motion style transfer methods are effective between two motions with identical content, their performance significantly diminishes when transferring style between motions with different contents. This challenge lies in the lack of clear separation between content and style of a motion. To tackle this challenge, we propose a novel motion style transformer that effectively disentangles style from content and generates a plausible motion with transferred style from a source motion. Our distinctive approach to achieving the goal of disentanglement is twofold: (1) a new architecture for motion style transformer with 'part-attentive style modulator across body parts' and 'Siamese encoders that encode style and content features separately'; (2) style disentanglement loss. Our method outperforms existing methods and demonstrates exceptionally high quality, particularly in motion pairs with different contents, without the need fo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;TRAD&#26694;&#26550;&#65292;&#36890;&#36807;&#27493;&#39588;&#24335;&#24605;&#32500;&#26816;&#32034;&#21644;&#23545;&#40784;&#20915;&#31574;&#35299;&#20915;&#20102;&#21033;&#29992;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06221</link><description>&lt;p&gt;
&#29992;&#27493;&#39588;&#24335;&#24605;&#32500;&#26816;&#32034;&#21644;&#23545;&#40784;&#20915;&#31574;&#22686;&#24378;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06221
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;TRAD&#26694;&#26550;&#65292;&#36890;&#36807;&#27493;&#39588;&#24335;&#24605;&#32500;&#26816;&#32034;&#21644;&#23545;&#40784;&#20915;&#31574;&#35299;&#20915;&#20102;&#21033;&#29992;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#32463;&#34987;&#26500;&#24314;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#65292;&#22914;&#32593;&#32476;&#23548;&#33322;&#21644;&#22312;&#32447;&#36141;&#29289;&#65292;&#36825;&#26159;&#22240;&#20026;LLM&#20855;&#26377;&#24191;&#27867;&#30340;&#30693;&#35782;&#21644;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#12290;&#22312;&#36825;&#20123;&#30740;&#31350;&#20013;&#65292;&#35768;&#22810;&#21033;&#29992;&#19978;&#19979;&#25991;&#31034;&#20363;&#26469;&#23454;&#29616;&#27867;&#21270;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#20294;&#23569;&#25968;&#32771;&#34385;&#20102;&#22914;&#20309;&#36873;&#25321;&#21644;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#31034;&#20363;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#36712;&#36857;&#32423;&#26816;&#32034;&#21644;&#20351;&#29992;&#36712;&#36857;&#20316;&#20026;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26041;&#27861;&#24050;&#32463;&#25552;&#20986;&#65292;&#20197;&#25552;&#39640;&#20195;&#29702;&#22312;&#19968;&#20123;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#26816;&#32034;&#20986;&#30340;&#21487;&#20449;&#31034;&#20363;&#32570;&#20047;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#29366;&#24577;&#36716;&#31227;&#21160;&#24577;&#65292;&#19988;&#36755;&#20837;&#38271;&#19988;&#21253;&#21547;&#22823;&#37327;&#26080;&#20851;&#19978;&#19979;&#25991;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65288;TRAD&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;TRAD&#39318;&#20808;&#36827;&#34892;&#24605;&#32500;&#26816;&#32034;&#65292;&#23454;&#29616;&#27493;&#39588;&#32423;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06221v1 Announce Type: new  Abstract: Numerous large language model (LLM) agents have been built for different tasks like web navigation and online shopping due to LLM's wide knowledge and text-understanding ability. Among these works, many of them utilize in-context examples to achieve generalization without the need for fine-tuning, while few of them have considered the problem of how to select and effectively utilize these examples. Recently, methods based on trajectory-level retrieval with task meta-data and using trajectories as in-context examples have been proposed to improve the agent's overall performance in some sequential decision making tasks. However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context. In this paper, we propose a novel framework (TRAD) to address these issues. TRAD first conducts Thought Retrieval, achieving step-level demonstration
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#32422;&#26463;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27491;&#20132;&#25237;&#24433;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#24402;&#19968;&#21270;&#65292;&#33021;&#22815;&#22312;ImageNet&#19978;&#23454;&#29616;&#39640;&#36798;4.4%&#30340;&#30456;&#23545;&#25913;&#36827;&#65292;&#24182;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.06213</link><description>&lt;p&gt;
$V_kD&#65306;&#20351;&#29992;&#27491;&#20132;&#25237;&#24433;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
$V_kD:$ Improving Knowledge Distillation using Orthogonal Projections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06213
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#32422;&#26463;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27491;&#20132;&#25237;&#24433;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#24402;&#19968;&#21270;&#65292;&#33021;&#22815;&#22312;ImageNet&#19978;&#23454;&#29616;&#39640;&#36798;4.4%&#30340;&#30456;&#23545;&#25913;&#36827;&#65292;&#24182;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#35757;&#32451;&#23567;&#22411;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32422;&#26463;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#19968;&#26041;&#27861;&#22312;&#36716;&#31227;&#21040;&#20854;&#20182;&#20219;&#21153;&#12289;&#27169;&#24577;&#25110;&#26550;&#26500;&#26102;&#30340;&#26377;&#25928;&#24615;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#28304;&#20110;&#19968;&#23567;&#32452;&#26680;&#24515;&#21407;&#21017;&#65292;&#21253;&#25324;&#27491;&#20132;&#25237;&#24433;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#24402;&#19968;&#21270;&#65292;&#36890;&#36807;&#36825;&#20004;&#20010;&#32452;&#20214;&#65292;&#25105;&#20204;&#30340;Transformer&#27169;&#22411;&#22312;ImageNet&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#21462;&#24471;&#20102;&#39640;&#36798;4.4%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#21644;&#22270;&#20687;&#29983;&#25104;&#65292;&#24471;&#21040;&#20102;&#25345;&#32493;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#20195;&#30721;&#21644;&#27169;&#22411;&#20844;&#24320;&#21487;&#29992;&#65306;https://github.com/roymiles/
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06213v1 Announce Type: cross  Abstract: Knowledge distillation is an effective method for training small and efficient deep learning models. However, the efficacy of a single method can degenerate when transferring to other tasks, modalities, or even other architectures. To address this limitation, we propose a novel constrained feature distillation method. This method is derived from a small set of core principles, which results in two emerging components: an orthogonal projection and a task-specific normalisation. Equipped with both of these components, our transformer models can outperform all previous methods on ImageNet and reach up to a 4.4% relative improvement over the previous state-of-the-art methods. To further demonstrate the generality of our method, we apply it to object detection and image generation, whereby we obtain consistent and substantial performance improvements over state-of-the-art. Code and models are publicly available: https://github.com/roymiles/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23450;&#20041;&#20102;&#29109;&#20989;&#25968;&#30340;&#21253;&#32476;&#27010;&#24565;&#65292;&#24182;&#25512;&#23548;&#35777;&#26126;&#20102;&#38543;&#26426;&#32622;&#25442;&#38598;&#29109;&#21253;&#32476;&#30340;&#26497;&#38480;&#24418;&#24335;&#20026;$e \times (N!)^2$&#12290;</title><link>https://arxiv.org/abs/2403.06206</link><description>&lt;p&gt;
&#26368;&#22823;&#38543;&#26426;&#32622;&#25442;&#38598;&#29109;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Limit of the Maximum Random Permutation Set Entropy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;&#29109;&#20989;&#25968;&#30340;&#21253;&#32476;&#27010;&#24565;&#65292;&#24182;&#25512;&#23548;&#35777;&#26126;&#20102;&#38543;&#26426;&#32622;&#25442;&#38598;&#29109;&#21253;&#32476;&#30340;&#26497;&#38480;&#24418;&#24335;&#20026;$e \times (N!)^2$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#38543;&#26426;&#32622;&#25442;&#38598;&#65288;RPS&#65289;&#20316;&#20026;&#35777;&#25454;&#29702;&#35770;&#30340;&#27867;&#21270;&#24418;&#24335;&#12290;&#20026;&#20102;&#34913;&#37327;RPS&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;RPS&#30340;&#29109;&#20197;&#21450;&#20854;&#23545;&#24212;&#30340;&#26368;&#22823;&#29109;&#12290;&#25506;&#32034;&#26368;&#22823;&#29109;&#20026;&#29702;&#35299;RPS&#30340;&#29289;&#29702;&#24847;&#20041;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#26041;&#24335;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#29109;&#20989;&#25968;&#30340;&#21253;&#32476;&#30340;&#26032;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25512;&#23548;&#24182;&#35777;&#26126;&#20102;RPS&#29109;&#21253;&#32476;&#30340;&#26497;&#38480;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#35745;&#31639;RPS&#29109;&#21253;&#32476;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#22823;&#22823;&#38477;&#20302;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;$N \to \infty$&#26102;&#65292;RPS&#29109;&#21253;&#32476;&#30340;&#26497;&#38480;&#24418;&#24335;&#25910;&#25947;&#21040;$e \times (N!)^2$&#65292;&#36825;&#19982;&#24120;&#25968;$e$&#21644;&#38454;&#20056;&#23494;&#20999;&#30456;&#20851;&#12290;&#26368;&#21518;&#65292;&#25968;&#20540;&#20363;&#35777;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#21253;&#32476;&#30340;&#25928;&#29575;&#21644;&#31616;&#27905;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06206v1 Announce Type: cross  Abstract: The Random Permutation Set (RPS) is a new type of set proposed recently, which can be regarded as the generalization of evidence theory. To measure the uncertainty of RPS, the entropy of RPS and its corresponding maximum entropy have been proposed. Exploring the maximum entropy provides a possible way of understanding the physical meaning of RPS. In this paper, a new concept, the envelope of entropy function, is defined. In addition, the limit of the envelope of RPS entropy is derived and proved. Compared with the existing method, the computational complexity of the proposed method to calculate the envelope of RPS entropy decreases greatly. The result shows that when $N \to \infty$, the limit form of the envelope of the entropy of RPS converges to $e \times (N!)^2$, which is highly connected to the constant $e$ and factorial. Finally, numerical examples validate the efficiency and conciseness of the proposed envelope, which provides a 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;LLMTrack&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#25552;&#31034;&#25216;&#26415;&#65292;&#32467;&#21512;&#35282;&#33394;&#25198;&#28436;&#21644;&#36880;&#27493;&#24605;&#32771;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#32463;&#22788;&#29702;&#30340;IMU&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#38646;&#23556;&#36712;&#36857;&#35782;&#21035;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26080;&#38656;&#35757;&#32451;&#22312;&#19987;&#38376;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.06201</link><description>&lt;p&gt;
&#24744;&#34987;&#36861;&#36394;&#20102;&#21527;&#65311;&#21457;&#29616;LLMs&#30340;&#38646;&#23556;&#36712;&#36857;&#36319;&#36394;&#30340;&#23041;&#21147;&#65281;
&lt;/p&gt;
&lt;p&gt;
Are You Being Tracked? Discover the Power of Zero-Shot Trajectory Tracing with LLMs!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06201
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;LLMTrack&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#25552;&#31034;&#25216;&#26415;&#65292;&#32467;&#21512;&#35282;&#33394;&#25198;&#28436;&#21644;&#36880;&#27493;&#24605;&#32771;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#32463;&#22788;&#29702;&#30340;IMU&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#38646;&#23556;&#36712;&#36857;&#35782;&#21035;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26080;&#38656;&#35757;&#32451;&#22312;&#19987;&#38376;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#20316;&#20026;&#22522;&#26412;&#32452;&#20214;&#30340;&#35752;&#35770;&#20013;&#65292;&#33021;&#22815;&#26080;&#32541;&#22320;&#34701;&#20837;&#29289;&#32852;&#32593;&#20154;&#24037;&#26234;&#33021;&#65288;AIoT&#65289;&#20013;&#20197;&#35299;&#37322;&#22797;&#26434;&#36712;&#36857;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;LLMTrack&#65292;&#35813;&#27169;&#22411;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#36827;&#34892;&#38646;&#23556;&#36712;&#36857;&#35782;&#21035;&#65292;&#36890;&#36807;&#37319;&#29992;&#23558;&#35282;&#33394;&#25198;&#28436;&#21644;&#36880;&#27493;&#24605;&#32771;&#26041;&#27861;&#19982;&#26410;&#32463;&#22788;&#29702;&#30340;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#25968;&#25454;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#21333;&#25552;&#31034;&#25216;&#26415;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26088;&#22312;&#25361;&#25112;&#23427;&#20197;&#20855;&#26377;&#23460;&#20869;&#21644;&#23460;&#22806;&#24773;&#26223;&#29305;&#24449;&#30340;&#19981;&#21516;&#36712;&#36857;&#12290;&#22312;&#20004;&#31181;&#27979;&#35797;&#24773;&#26223;&#20013;&#65292;LLMTrack &#19981;&#20165;&#28385;&#36275;&#29978;&#33267;&#36229;&#36807;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20197;&#21450;&#24403;&#20195;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35774;&#23450;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#32780;&#19988;&#26080;&#38656;&#23545;&#19987;&#38376;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06201v1 Announce Type: cross  Abstract: There is a burgeoning discussion around the capabilities of Large Language Models (LLMs) in acting as fundamental components that can be seamlessly incorporated into Artificial Intelligence of Things (AIoT) to interpret complex trajectories. This study introduces LLMTrack, a model that illustrates how LLMs can be leveraged for Zero-Shot Trajectory Recognition by employing a novel single-prompt technique that combines role-play and think step-by-step methodologies with unprocessed Inertial Measurement Unit (IMU) data. We evaluate the model using real-world datasets designed to challenge it with distinct trajectories characterized by indoor and outdoor scenarios. In both test scenarios, LLMTrack not only meets but exceeds the performance benchmarks set by traditional machine learning approaches and even contemporary state-of-the-art deep learning models, all without the requirement of training on specialized datasets. The results of our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#20998;&#31867;&#20219;&#21153;&#30340;&#39046;&#22495;&#23545;&#25239;&#20027;&#21160;&#23398;&#20064;&#65288;DAAL&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#25361;&#25112;&#24615;&#26679;&#26412;&#30340;&#39046;&#22495;&#23545;&#25239;&#36873;&#25321;&#26041;&#27861;&#65292;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.06174</link><description>&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#20998;&#31867;&#30340;&#39046;&#22495;&#23545;&#25239;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain Adversarial Active Learning for Domain Generalization Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#20998;&#31867;&#20219;&#21153;&#30340;&#39046;&#22495;&#23545;&#25239;&#20027;&#21160;&#23398;&#20064;&#65288;DAAL&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#25361;&#25112;&#24615;&#26679;&#26412;&#30340;&#39046;&#22495;&#23545;&#25239;&#36873;&#25321;&#26041;&#27861;&#65292;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#27169;&#22411;&#26088;&#22312;&#20174;&#28304;&#39046;&#22495;&#25968;&#25454;&#20013;&#23398;&#20064;&#36328;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#22312;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#19978;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22810;&#26679;&#21270;&#21644;&#20016;&#23500;&#30340;&#28304;&#39046;&#22495;&#26679;&#26412;&#21487;&#20197;&#22686;&#24378;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#35748;&#20026;&#27599;&#20010;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#26159;&#19981;&#21516;&#30340;&#12290;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;&#19968;&#23450;&#27700;&#24179;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#20998;&#31867;&#20219;&#21153;&#30340;&#39046;&#22495;&#23545;&#25239;&#20027;&#21160;&#23398;&#20064;&#65288;DAAL&#65289;&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#21516;&#19968;&#39046;&#22495;&#20869;&#30340;&#31867;&#38388;&#36317;&#31163;&#65292;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#26368;&#23567;&#21270;&#31867;&#20869;&#36317;&#31163;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20248;&#20808;&#32771;&#34385;&#20855;&#26377;&#25361;&#25112;&#24615;&#26679;&#26412;&#30340;&#39046;&#22495;&#23545;&#25239;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06174v1 Announce Type: cross  Abstract: Domain generalization models aim to learn cross-domain knowledge from source domain data, to improve performance on unknown target domains. Recent research has demonstrated that diverse and rich source domain samples can enhance domain generalization capability. This paper argues that the impact of each sample on the model's generalization ability varies. Despite its small scale, a high-quality dataset can still attain a certain level of generalization ability. Motivated by this, we propose a domain-adversarial active learning (DAAL) algorithm for classification tasks in domain generalization. First, we analyze that the objective of tasks is to maximize the inter-class distance within the same domain and minimize the intra-class distance across different domains. To achieve this objective, we design a domain adversarial selection method that prioritizes challenging samples. Second, we posit that even in a converged model, there are sub
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DiffuMatting&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#25216;&#26415;&#23454;&#29616;&#20102;&#8220;&#25248;&#22270;&#20219;&#20309;&#29289;&#20307;&#8221;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#27880;&#37322;&#65292;&#21516;&#26102;&#20860;&#23481;&#31038;&#21306;LoRAs&#25110;&#21508;&#31181;&#26465;&#20214;&#25511;&#21046;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.06168</link><description>&lt;p&gt;
DiffuMatting&#65306;&#20351;&#29992;Matting&#32423;&#21035;&#26631;&#27880;&#21512;&#25104;&#20219;&#24847;&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
DiffuMatting: Synthesizing Arbitrary Objects with Matting-level Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06168
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DiffuMatting&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#25216;&#26415;&#23454;&#29616;&#20102;&#8220;&#25248;&#22270;&#20219;&#20309;&#29289;&#20307;&#8221;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#27880;&#37322;&#65292;&#21516;&#26102;&#20860;&#23481;&#31038;&#21306;LoRAs&#25110;&#21508;&#31181;&#26465;&#20214;&#25511;&#21046;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33719;&#21462;&#39640;&#24230;&#20934;&#30830;&#25110;&#25248;&#22270;&#27880;&#37322;&#30340;&#22256;&#38590;&#21644;&#21171;&#21160;&#23494;&#38598;&#24615;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#39640;&#24230;&#20934;&#30830;&#26631;&#31614;&#25968;&#37327;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;DiffuMatting&#65292;&#23427;&#32487;&#25215;&#20102;&#25193;&#25955;&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#36171;&#20104;&#20102;&#8220;&#25248;&#22270;&#20219;&#20309;&#29289;&#20307;&#8221;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;DiffuMatting&#21487;&#20197;&#65306;1&#65289;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#39640;&#20934;&#30830;&#24230;&#27880;&#37322;&#30340;&#20219;&#24847;&#25248;&#22270;&#24037;&#21378;&#65307;2&#65289;&#19982;&#31038;&#21306;LoRAs&#25110;&#21508;&#31181;&#26465;&#20214;&#25511;&#21046;&#26041;&#27861;&#20860;&#23481;&#65292;&#20197;&#23454;&#29616;&#31038;&#21306;&#21451;&#22909;&#30340;&#33402;&#26415;&#35774;&#35745;&#21644;&#21487;&#25511;&#29983;&#25104;&#12290;&#20855;&#20307;&#22320;&#65292;&#21463;&#32511;&#24149;&#25248;&#20687;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#25945;&#25480;&#25193;&#25955;&#27169;&#22411;&#22312;&#22266;&#23450;&#30340;&#32511;&#24149;&#30011;&#24067;&#19978;&#32472;&#30011;&#12290;&#20026;&#27492;&#65292;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#32511;&#24149;&#25968;&#25454;&#38598;&#65288;Green100K&#65289;&#20316;&#20026;DiffuMatting&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32511;&#33394;&#32972;&#26223;&#25511;&#21046;&#25439;&#22833;&#65292;&#20197;&#20445;&#25345;&#30011;&#24067;&#20026;&#32431;&#32511;&#33394;&#20197;&#36827;&#34892;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06168v1 Announce Type: cross  Abstract: Due to the difficulty and labor-consuming nature of getting highly accurate or matting annotations, there only exists a limited amount of highly accurate labels available to the public. To tackle this challenge, we propose a DiffuMatting which inherits the strong Everything generation ability of diffusion and endows the power of "matting anything". Our DiffuMatting can 1). act as an anything matting factory with high accurate annotations 2). be well-compatible with community LoRAs or various conditional control approaches to achieve the community-friendly art design and controllable generation. Specifically, inspired by green-screen-matting, we aim to teach the diffusion model to paint on a fixed green screen canvas. To this end, a large-scale greenscreen dataset (Green100K) is collected as a training dataset for DiffuMatting. Secondly, a green background control loss is proposed to keep the drawing board as a pure green color to disti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21644;&#35780;&#20998;&#20070;&#38754;&#20316;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#35774;&#35745;&#19981;&#21516;&#25552;&#31034;&#24182;&#22312;ASAP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06149</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#33258;&#21160;&#35780;&#20998;&#20889;&#20316;&#25991;&#31456;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Automatically Score Proficiency of Written Essays?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21644;&#35780;&#20998;&#20070;&#38754;&#20316;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#35774;&#35745;&#19981;&#21516;&#25552;&#31034;&#24182;&#22312;ASAP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#36807;&#21435;50&#24180;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#21160;&#35780;&#20998;&#20316;&#25991;&#65288;AES&#65289;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#25928;&#26524;&#26041;&#38754;&#20173;&#26377;&#35768;&#22810;&#19981;&#36275;&#20043;&#22788;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#27979;&#35797;&#20102;LLMs&#30340;&#33021;&#21147;&#65292;&#37492;&#20110;&#23427;&#20204;&#24378;&#22823;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#26469;&#20998;&#26512;&#21644;&#26377;&#25928;&#35780;&#20998;&#20070;&#38754;&#20316;&#25991;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;LLMs&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#21035;&#26159;ChatGPT&#21644;Llama&#12290;&#25105;&#20204;&#26088;&#22312;&#26816;&#26597;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#20004;&#20010;&#23618;&#38754;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#21363;&#22312;&#25972;&#20307;&#19978;&#21644;&#22312;&#20010;&#20307;&#20889;&#20316;&#29305;&#24449;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#31574;&#30053;&#35774;&#35745;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#30340;&#26368;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#22312;ASAP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20960;&#20010;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06149v1 Announce Type: cross  Abstract: Although several methods were proposed to address the problem of automated essay scoring (AES) in the last 50 years, there is still much to desire in terms of effectiveness. Large Language Models (LLMs) are transformer-based models that demonstrate extraordinary capabilities on various tasks. In this paper, we test the ability of LLMs, given their powerful linguistic knowledge, to analyze and effectively score written essays. We experimented with two popular LLMs, namely ChatGPT and Llama. We aim to check if these models can do this task and, if so, how their performance is positioned among the state-of-the-art (SOTA) models across two levels, holistically and per individual writing trait. We utilized prompt-engineering tactics in designing four different prompts to bring their maximum potential to this task. Our experiments conducted on the ASAP dataset revealed several interesting observations. First, choosing the right prompt depend
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#39318;&#20010;&#21830;&#19994;&#21270;&#21307;&#23398;&#24433;&#20687;&#24179;&#21488;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#26469;&#33258;&#20122;&#27954;&#22320;&#21306;&#30340;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#65292;&#20026;&#21307;&#23398;AI&#30740;&#21457;&#20934;&#22791;&#24182;&#25552;&#20379;&#20102;&#21487;&#30452;&#25509;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;</title><link>https://arxiv.org/abs/2403.06145</link><description>&lt;p&gt;
AI&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20840;&#33021;&#30740;&#21457;&#24179;&#21488;: &#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#12289;&#31579;&#36873;&#12289;&#26631;&#27880;&#21644;&#39044;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
All-in-one platform for AI R&amp;D in medical imaging, encompassing data collection, selection, annotation, and pre-processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06145
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#39318;&#20010;&#21830;&#19994;&#21270;&#21307;&#23398;&#24433;&#20687;&#24179;&#21488;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#26469;&#33258;&#20122;&#27954;&#22320;&#21306;&#30340;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#65292;&#20026;&#21307;&#23398;AI&#30740;&#21457;&#20934;&#22791;&#24182;&#25552;&#20379;&#20102;&#21487;&#30452;&#25509;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27491;&#22312;&#25512;&#21160;&#21307;&#23398;&#24433;&#20687;&#30740;&#31350;&#19982;&#24320;&#21457;&#65288;R&amp;D&#65289;&#65292;&#20419;&#20351;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#30340;&#21307;&#30103;&#35774;&#22791;&#39057;&#32321;&#22312;&#20020;&#24202;&#20013;&#20351;&#29992;&#12290;&#20026;&#20102;&#25512;&#36827;AI R&amp;D&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#39318;&#20010;&#21830;&#19994;&#21270;&#21307;&#23398;&#24433;&#20687;&#24179;&#21488;&#65292;&#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#12289;&#25968;&#25454;&#31579;&#36873;&#12289;&#26631;&#27880;&#21644;&#39044;&#22788;&#29702;&#31561;&#27493;&#39588;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21033;&#29992;&#26469;&#33258;&#26085;&#26412;&#21644;&#24191;&#27867;&#20122;&#27954;&#22320;&#21306;&#30340;&#20302;&#37325;&#22797;&#25968;&#25454;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#12289;&#30913;&#20849;&#25391;&#25104;&#20687;&#21644;&#20840;&#20999;&#29255;&#25104;&#20687;&#25195;&#25551;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06145v1 Announce Type: cross  Abstract: Deep Learning is advancing medical imaging Research and Development (R&amp;D), leading to the frequent clinical use of Artificial Intelligence/Machine Learning (AI/ML)-based medical devices. However, to advance AI R&amp;D, two challenges arise: 1) significant data imbalance, with most data from Europe/America and under 10% from Asia, despite its 60% global population share; and 2) hefty time and investment needed to curate proprietary datasets for commercial use. In response, we established the first commercial medical imaging platform, encompassing steps like: 1) data collection, 2) data selection, 3) annotation, and 4) pre-processing. Moreover, we focus on harnessing under-represented data from Japan and broader Asia, including Computed Tomography, Magnetic Resonance Imaging, and Whole Slide Imaging scans. Using the collected data, we are preparing/providing ready-to-use datasets for medical AI R&amp;D by 1) offering these datasets to AI firms, 
&lt;/p&gt;</description></item><item><title>Fluent&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31169;&#20154;&#32852;&#37030;&#23398;&#20064;&#30340;&#36718;&#27425;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#65292;&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#26377;&#20004;&#20010;&#37325;&#35201;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.06143</link><description>&lt;p&gt;
&#27969;&#30021;&#65306;&#38754;&#21521;&#31169;&#20154;&#32852;&#37030;&#23398;&#20064;&#30340;&#36718;&#27425;&#39640;&#25928;&#23433;&#20840;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Fluent: Round-efficient Secure Aggregation for Private Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06143
&lt;/p&gt;
&lt;p&gt;
Fluent&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31169;&#20154;&#32852;&#37030;&#23398;&#20064;&#30340;&#36718;&#27425;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#65292;&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#26377;&#20004;&#20010;&#37325;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#31169;&#20154;&#32852;&#37030;&#23398;&#20064;&#30340;&#27969;&#30021;(Fluent)&#20171;&#32461;&#20102;&#19968;&#31181;&#36718;&#27425;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#12290;&#19982;Bell&#31561;&#20154;&#65288;CCS 2020&#65289;&#21644;Ma&#31561;&#20154;&#65288;SP 2023&#65289;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;Fluent&#22312;&#20960;&#20010;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#65306;(1)&#36890;&#36807;&#22312;&#22810;&#20010;&#35757;&#32451;&#36845;&#20195;&#20043;&#38388;&#39640;&#25928;&#22320;&#37325;&#29992;&#20221;&#39069;&#32780;&#19981;&#27844;&#38706;&#20219;&#20309;&#31169;&#20154;&#20449;&#24687;&#65292;&#28040;&#38500;&#20102;&#39057;&#32321;&#30340;&#25569;&#25163;&#21644;&#31192;&#23494;&#20849;&#20139;&#25805;&#20316;&#65307;(2)&#22312;&#19968;&#20010;&#36923;&#36753;&#25805;&#20316;&#20013;&#23436;&#25104;&#20102;&#19968;&#33268;&#24615;&#26816;&#26597;&#21644;&#26799;&#24230;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06143v1 Announce Type: cross  Abstract: Federated learning (FL) facilitates collaborative training of machine learning models among a large number of clients while safeguarding the privacy of their local datasets. However, FL remains susceptible to vulnerabilities such as privacy inference and inversion attacks. Single-server secure aggregation schemes were proposed to address these threats. Nonetheless, they encounter practical constraints due to their round and communication complexities. This work introduces Fluent, a round and communication-efficient secure aggregation scheme for private FL. Fluent has several improvements compared to state-of-the-art solutions like Bell et al. (CCS 2020) and Ma et al. (SP 2023): (1) it eliminates frequent handshakes and secret sharing operations by efficiently reusing the shares across multiple training iterations without leaking any private information; (2) it accomplishes both the consistency check and gradient unmasking in one logica
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21512;&#25104;&#27969;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#32467;&#26500;&#29702;&#35299;&#65292;&#23558;&#31232;&#30095;&#29992;&#25143;&#36827;&#34892;&#20998;&#31867;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.06139</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#32467;&#26500;&#29702;&#35299;&#30340;&#32454;&#31890;&#24230;&#21512;&#25104;&#27969;&#25968;&#25454;&#29992;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06139
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21512;&#25104;&#27969;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#32467;&#26500;&#29702;&#35299;&#65292;&#23558;&#31232;&#30095;&#29992;&#25143;&#36827;&#34892;&#20998;&#31867;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#25968;&#25454;&#31232;&#30095;&#65292;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#23545;&#29992;&#25143;&#35780;&#35770;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#36890;&#24120;&#24615;&#33021;&#19981;&#20339;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#26497;&#24230;&#31232;&#30095;&#30340;&#29992;&#25143;&#25968;&#25454;&#25110;&#38271;&#23614;&#26631;&#31614;&#26102;&#12290;&#26368;&#36817;&#65292;LLMs&#30340;&#20986;&#29616;&#36890;&#36807;&#21033;&#29992;&#22270;&#32467;&#26500;&#29983;&#25104;&#36741;&#21161;&#29992;&#25143;&#26723;&#26696;&#65292;&#20026;&#36825;&#20123;&#38382;&#39064;&#24341;&#20837;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#19988;&#38590;&#20197;&#36866;&#24212;&#22797;&#26434;&#30340;&#27969;&#25968;&#25454;&#29615;&#22659;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#27969;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#65292;&#23558;&#31232;&#30095;&#29992;&#25143;&#20998;&#31867;&#20026;&#19977;&#31867;&#65306;&#20013;&#23614;&#12289;&#38271;&#23614;&#21644;&#26497;&#31471;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LLMs&#65292;&#20840;&#38754;&#29702;&#35299;&#27969;&#25968;&#25454;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#22270;&#20803;&#32032;&#65292;&#21253;&#25324;&#23616;&#37096;-&#20840;&#23616;&#22270;&#29702;&#35299;&#12289;&#20108;&#38454;&#20851;&#31995;&#25552;&#21462;&#21644;&#20135;&#21697;&#23646;&#24615;&#29702;&#35299;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06139v1 Announce Type: cross  Abstract: Due to the sparsity of user data, sentiment analysis on user reviews in e-commerce platforms often suffers from poor performance, especially when faced with extremely sparse user data or long-tail labels. Recently, the emergence of LLMs has introduced new solutions to such problems by leveraging graph structures to generate supplementary user profiles. However, previous approaches have not fully utilized the graph understanding capabilities of LLMs and have struggled to adapt to complex streaming data environments. In this work, we propose a fine-grained streaming data synthesis framework that categorizes sparse users into three categories: Mid-tail, Long-tail, and Extreme. Specifically, we design LLMs to comprehensively understand three key graph elements in streaming data, including Local-global Graph Understanding, Second-Order Relationship Extraction, and Product Attribute Understanding, which enables the generation of high-quality
&lt;/p&gt;</description></item><item><title>MACE&#36890;&#36807;&#25104;&#21151;&#23558;&#27010;&#24565;&#28040;&#38500;&#30340;&#33539;&#22260;&#25193;&#23637;&#21040;100&#20010;&#27010;&#24565;&#65292;&#24182;&#22312;&#27867;&#21270;&#24615;&#21644;&#29305;&#24322;&#24615;&#20043;&#38388;&#21462;&#24471;&#26377;&#25928;&#24179;&#34913;&#65292;&#20174;&#32780;&#38450;&#27490;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#25110;&#35823;&#23548;&#24615;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.06135</link><description>&lt;p&gt;
MACE&#65306;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#22823;&#35268;&#27169;&#27010;&#24565;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
MACE: Mass Concept Erasure in Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06135
&lt;/p&gt;
&lt;p&gt;
MACE&#36890;&#36807;&#25104;&#21151;&#23558;&#27010;&#24565;&#28040;&#38500;&#30340;&#33539;&#22260;&#25193;&#23637;&#21040;100&#20010;&#27010;&#24565;&#65292;&#24182;&#22312;&#27867;&#21270;&#24615;&#21644;&#29305;&#24322;&#24615;&#20043;&#38388;&#21462;&#24471;&#26377;&#25928;&#24179;&#34913;&#65292;&#20174;&#32780;&#38450;&#27490;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#25110;&#35823;&#23548;&#24615;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#24555;&#36895;&#25193;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20854;&#22312;&#21019;&#24314;&#26377;&#23475;&#25110;&#35823;&#23548;&#24615;&#20869;&#23481;&#26041;&#38754;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MACE&#30340;&#24494;&#35843;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#27010;&#24565;&#28040;&#38500;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#38450;&#27490;&#27169;&#22411;&#22312;&#25552;&#31034;&#26102;&#29983;&#25104;&#20855;&#26377;&#19981;&#24076;&#26395;&#27010;&#24565;&#30340;&#22270;&#20687;&#12290;&#29616;&#26377;&#30340;&#27010;&#24565;&#28040;&#38500;&#26041;&#27861;&#36890;&#24120;&#21463;&#38480;&#20110;&#21516;&#26102;&#22788;&#29702;&#23569;&#20110;&#20116;&#20010;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#28040;&#38500;&#27010;&#24565;&#30340;&#21516;&#26102;&#38590;&#20197;&#25214;&#21040;&#28040;&#38500;&#27010;&#24565;&#21516;&#20041;&#35789;&#65288;&#27867;&#21270;&#24615;&#65289;&#21644;&#20445;&#25345;&#19981;&#30456;&#20851;&#27010;&#24565;&#65288;&#29305;&#24322;&#24615;&#65289;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;MACE&#36890;&#36807;&#25104;&#21151;&#23558;&#28040;&#38500;&#33539;&#22260;&#25193;&#23637;&#21040;100&#20010;&#27010;&#24565;&#65292;&#24182;&#22312;&#27867;&#21270;&#24615;&#21644;&#29305;&#24322;&#24615;&#20043;&#38388;&#21462;&#24471;&#26377;&#25928;&#24179;&#34913;&#32780;&#19981;&#21516;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#38381;&#29615;&#20132;&#21449;&#27880;&#24847;&#21147;&#32454;&#21270;&#20197;&#21450;LoRA&#24494;&#35843;&#26469;&#23454;&#29616;&#30340;&#65292;&#20849;&#21516;&#28040;&#38500;&#20102;&#19981;&#33391;&#27010;&#24565;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;MACE&#36824;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06135v1 Announce Type: cross  Abstract: The rapid expansion of large-scale text-to-image diffusion models has raised growing concerns regarding their potential misuse in creating harmful or misleading content. In this paper, we introduce MACE, a finetuning framework for the task of mass concept erasure. This task aims to prevent models from generating images that embody unwanted concepts when prompted. Existing concept erasure methods are typically restricted to handling fewer than five concepts simultaneously and struggle to find a balance between erasing concept synonyms (generality) and maintaining unrelated concepts (specificity). In contrast, MACE differs by successfully scaling the erasure scope up to 100 concepts and by achieving an effective balance between generality and specificity. This is achieved by leveraging closed-form cross-attention refinement along with LoRA finetuning, collectively eliminating the information of undesirable concepts. Furthermore, MACE int
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#31639;&#27861;FedPIT&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#21442;&#25968;&#38548;&#31163;&#35757;&#32451;&#26469;&#38450;&#27490;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.06131</link><description>&lt;p&gt;
FedPIT&#65306;&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#21644;&#23569;&#26679;&#26412;&#32852;&#37030;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#31639;&#27861;FedPIT&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#21442;&#25968;&#38548;&#31163;&#35757;&#32451;&#26469;&#38450;&#27490;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#23545;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#21709;&#24212;&#26041;&#38754;&#30340;&#24615;&#33021;&#24050;&#34987;&#35777;&#26126;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#35843;&#25972;&#36807;&#31243;&#20013;&#25910;&#38598;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#38544;&#31169;&#30340;&#39046;&#22495;&#12290;&#32852;&#37030;&#25351;&#20196;&#35843;&#25972;&#65288;FedIT&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25351;&#20196;&#25968;&#25454;&#26377;&#38480;&#20197;&#21450;&#23481;&#26131;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#23427;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;FedPIT&#65292;&#23427;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#33258;&#21160;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#30340;&#29305;&#23450;&#20219;&#21153;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#21442;&#25968;&#38548;&#31163;&#35757;&#32451;&#26469;&#32500;&#25252;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20840;&#23616;&#21442;&#25968;&#21644;&#22312;&#22686;&#24378;&#26412;&#22320;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26412;&#22320;&#21442;&#25968;&#65292;&#26377;&#25928;&#22320;&#38450;&#27490;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06131v1 Announce Type: cross  Abstract: Instruction tuning has proven essential for enhancing the performance of large language models (LLMs) in generating human-aligned responses. However, collecting diverse, high-quality instruction data for tuning poses challenges, particularly in privacy-sensitive domains. Federated instruction tuning (FedIT) has emerged as a solution, leveraging federated learning from multiple data owners while preserving privacy. Yet, it faces challenges due to limited instruction data and vulnerabilities to training data extraction attacks. To address these issues, we propose a novel federated algorithm, FedPIT, which utilizes LLMs' in-context learning capability to self-generate task-specific synthetic data for training autonomously. Our method employs parameter-isolated training to maintain global parameters trained on synthetic data and local parameters trained on augmented local data, effectively thwarting data extraction attacks. Extensive exper
&lt;/p&gt;</description></item><item><title>FMPAF&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22238;&#24402;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#32654;&#32852;&#20648;&#20027;&#24109;&#26032;&#38395;&#21457;&#24067;&#20250;&#27807;&#36890;&#23545;&#37329;&#34701;&#24066;&#22330;&#24433;&#21709;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.06115</link><description>&lt;p&gt;
FMPAF&#65306;&#32852;&#37030;&#20648;&#22791;&#20027;&#24109;&#22914;&#20309;&#24433;&#21709;&#37329;&#34701;&#24066;&#22330;&#65311;&#20851;&#20110;&#20182;&#20204;&#35821;&#35328;&#30340;&#32454;&#31890;&#24230;&#36135;&#24065;&#25919;&#31574;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained Monetary Policy Analysis Framework on Their Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06115
&lt;/p&gt;
&lt;p&gt;
FMPAF&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22238;&#24402;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#32654;&#32852;&#20648;&#20027;&#24109;&#26032;&#38395;&#21457;&#24067;&#20250;&#27807;&#36890;&#23545;&#37329;&#34701;&#24066;&#22330;&#24433;&#21709;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#22830;&#38134;&#34892;&#27807;&#36890;&#30340;&#26377;&#25928;&#24615;&#26159;&#36135;&#24065;&#25919;&#31574;&#20256;&#23548;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;&#32654;&#32852;&#20648;&#20027;&#24109;&#30340;&#25919;&#31574;&#27807;&#36890;&#23545;&#21508;&#31181;&#37329;&#34701;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#20294;&#22823;&#37096;&#20998;&#25991;&#29486;&#20381;&#36182;&#20110;&#22522;&#20110;&#35268;&#21017;&#25110;&#35789;&#20856;&#30340;&#26041;&#27861;&#26469;&#35299;&#26512;&#20027;&#24109;&#30340;&#35821;&#35328;&#65292;&#23548;&#33268;&#23545;&#21253;&#21547;&#22312;&#38750;&#35328;&#35821;&#24773;&#32490;&#20013;&#30340;&#25919;&#31574;&#31435;&#22330;&#30340;&#24494;&#22937;&#20449;&#24687;&#32570;&#20047;&#20998;&#26512;&#12290;&#22312;&#24403;&#21069;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#36135;&#24065;&#25919;&#31574;&#20998;&#26512;&#26694;&#26550;&#65288;FMPAF&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22238;&#24402;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#20840;&#38754;&#20998;&#26512;&#32654;&#32852;&#20648;&#20027;&#24109;&#26032;&#38395;&#21457;&#24067;&#20250;&#27807;&#36890;&#23545;&#37329;&#34701;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27169;&#22411;&#24615;&#33021;&#22312;&#19981;&#21516;&#32454;&#31890;&#24230;&#12289;&#27169;&#24577;&#21644;&#27807;&#36890;&#22330;&#26223;&#19979;&#30340;&#24191;&#27867;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06115v1 Announce Type: cross  Abstract: The effectiveness of central bank communication is a crucial aspect of monetary policy transmission. While recent research has examined the influence of policy communication by the chairs of the Federal Reserve on various financial variables, much of the literature relies on rule-based or dictionary-based methods in parsing the language of the chairs, leaving nuanced information about policy stance contained in nonverbal emotion out of the analysis. In the current study, we propose the Fine-Grained Monetary Policy Analysis Framework (FMPAF), a novel approach that integrates large language models (LLMs) with regression analysis to provide a comprehensive analysis of the impact of the press-conference communications of chairs of the Federal Reserve on financial markets. We conduct extensive comparisons of model performance under different levels of granularity, modalities, and communication scenarios. Based on our preferred specification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32454;&#31890;&#24230;&#24773;&#24863;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#25991;&#26412;&#20013;&#26816;&#27979;&#24494;&#22937;&#24773;&#24863;&#30340;&#25361;&#25112;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.06108</link><description>&lt;p&gt;
&#22312;&#21253;&#21547;&#25968;&#25454;&#22686;&#24378;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32454;&#31890;&#24230;&#24773;&#24863;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#25991;&#26412;&#20013;&#26816;&#27979;&#24494;&#22937;&#24773;&#24863;&#30340;&#25361;&#25112;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;GoEmotions&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#24773;&#24863;&#26816;&#27979;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22411;&#12289;&#25163;&#21160;&#27880;&#37322;&#30340;&#25991;&#26412;&#24773;&#24863;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35299;&#20915;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24494;&#22937;&#24773;&#24863;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#35299;&#20915;&#25991;&#26412;&#24773;&#24863;&#26816;&#27979;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#21253;&#25324;&#21512;&#25104;&#35813;&#39046;&#22495;&#21508;&#20010;&#25968;&#25454;&#38598;&#19978;&#26041;&#27861;&#21644;&#24615;&#33021;&#30340;&#32508;&#36848;&#35770;&#25991;&#30340;&#28508;&#22312;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06108v1 Announce Type: cross  Abstract: This paper delves into enhancing the classification performance on the GoEmotions dataset, a large, manually annotated dataset for emotion detection in text. The primary goal of this paper is to address the challenges of detecting subtle emotions in text, a complex issue in Natural Language Processing (NLP) with significant practical applications. The findings offer valuable insights into addressing the challenges of emotion detection in text and suggest directions for future research, including the potential for a survey paper that synthesizes methods and performances across various datasets in this domain.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;CNER-UAV&#65292;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#32422;&#26377;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;</title><link>https://arxiv.org/abs/2403.06097</link><description>&lt;p&gt;
&#33021;&#21542;&#29992;LLM&#26367;&#20195;&#20154;&#24037;&#26631;&#27880;&#65311; &#26080;&#20154;&#26426;&#20132;&#20184;&#20219;&#21153;&#19979;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22320;&#22336;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;CNER-UAV&#65292;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#32422;&#26377;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CNER-UAV&#65292;&#19968;&#20010;&#19987;&#20026;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#35774;&#35745;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20116;&#20010;&#31867;&#21035;&#65292;&#21487;&#20197;&#20840;&#38754;&#35757;&#32451;&#21644;&#35780;&#20272;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#12290;&#20026;&#26500;&#24314;&#36825;&#19968;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#33719;&#21462;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#30830;&#20445;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#26368;&#32456;&#30340;&#25968;&#25454;&#38598;&#32422;&#21253;&#21547;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#32463;&#36807;&#20154;&#24037;&#19987;&#23478;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312; \url{https://github.com/zhhvvv/CNER-UAV} &#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06097v1 Announce Type: cross  Abstract: We present CNER-UAV, a fine-grained \textbf{C}hinese \textbf{N}ame \textbf{E}ntity \textbf{R}ecognition dataset specifically designed for the task of address resolution in \textbf{U}nmanned \textbf{A}erial \textbf{V}ehicle delivery systems. The dataset encompasses a diverse range of five categories, enabling comprehensive training and evaluation of NER models. To construct this dataset, we sourced the data from a real-world UAV delivery system and conducted a rigorous data cleaning and desensitization process to ensure privacy and data integrity. The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and \textbf{L}arge \textbf{L}anguage \textbf{M}odel annotation. We evaluated classical NER models on our dataset and provided in-depth analysis. The dataset and models are publicly available at \url{https://github.com/zhhvvv/CNER-UAV}.
&lt;/p&gt;</description></item><item><title>RepoHyper&#25552;&#20986;&#20102;Repo&#32423;&#35821;&#20041;&#22270;&#65288;RSG&#65289;&#21644;Expand&#21644;Refine&#26816;&#32034;&#26041;&#27861;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#28385;&#36275;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30340;&#38656;&#27714;</title><link>https://arxiv.org/abs/2403.06095</link><description>&lt;p&gt;
RepoHyper&#65306;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#26159;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06095
&lt;/p&gt;
&lt;p&gt;
RepoHyper&#25552;&#20986;&#20102;Repo&#32423;&#35821;&#20041;&#22270;&#65288;RSG&#65289;&#21644;Expand&#21644;Refine&#26816;&#32034;&#26041;&#27861;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#28385;&#36275;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06095v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CodeLLMs&#65289;&#22312;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#23436;&#20840;&#29702;&#35299;&#39033;&#30446;&#20179;&#24211;&#30340;&#24191;&#27867;&#19978;&#19979;&#25991;&#65292;&#27604;&#22914;&#30456;&#20851;&#25991;&#20214;&#21644;&#31867;&#23618;&#27425;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#34917;&#20840;&#19981;&#22815;&#31934;&#30830;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RepoHyper&#65292;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#19982;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30456;&#20851;&#30340;&#22797;&#26434;&#25361;&#25112;&#30340;&#22810;&#26041;&#38754;&#26694;&#26550;&#12290;RepoHyper&#30340;&#26680;&#24515;&#26159;Repo&#32423;&#35821;&#20041;&#22270;&#65288;RSG&#65289;&#65292;&#19968;&#31181;&#23553;&#35013;&#20195;&#30721;&#20179;&#24211;&#24191;&#27867;&#19978;&#19979;&#25991;&#30340;&#26032;&#39062;&#35821;&#20041;&#22270;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;RepoHyper&#21033;&#29992;&#25193;&#23637;&#21644;&#32454;&#21270;&#26816;&#32034;&#26041;&#27861;&#65292;&#21253;&#25324;&#24212;&#29992;&#20110;RSG&#30340;&#22270;&#25193;&#23637;&#21644;&#38142;&#25509;&#39044;&#27979;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#30456;&#20851;&#20195;&#30721;&#29255;&#27573;&#30340;&#26377;&#25928;&#26816;&#32034;&#21644;&#20248;&#20808;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;RepoHyper&#22312;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06095v1 Announce Type: cross  Abstract: Code Large Language Models (CodeLLMs) have demonstrated impressive proficiency in code completion tasks. However, they often fall short of fully understanding the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies, which can result in less precise completions. To overcome these limitations, we present RepoHyper, a multifaceted framework designed to address the complex challenges associated with repository-level code completion. Central to RepoHyper is the Repo-level Semantic Graph (RSG), a novel semantic graph structure that encapsulates the vast context of code repositories. Furthermore, RepoHyper leverages Expand and Refine retrieval method, including a graph expansion and a link prediction algorithm applied to the RSG, enabling the effective retrieval and prioritization of relevant code snippets. Our evaluations show that RepoHyper markedly outperforms existing techniques in re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#21644;&#19981;&#21516;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#36710;&#36733;&#22810;&#20219;&#21153;&#38754;&#37096;&#23646;&#24615;&#35782;&#21035;&#20013;&#30340;&#25928;&#26524;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.06088</link><description>&lt;p&gt;
&#38754;&#21521;&#36710;&#36733;&#22810;&#20219;&#21153;&#38754;&#37096;&#23646;&#24615;&#35782;&#21035;&#65306;&#30740;&#31350;&#21512;&#25104;&#25968;&#25454;&#21644;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#21644;&#19981;&#21516;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#36710;&#36733;&#22810;&#20219;&#21153;&#38754;&#37096;&#23646;&#24615;&#35782;&#21035;&#20013;&#30340;&#25928;&#26524;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#34028;&#21187;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#38754;&#37096;&#23646;&#24615;&#35782;&#21035;&#65288;&#22914;&#38754;&#37096;&#34920;&#24773;&#12289;&#30524;&#31070;&#12289;&#24180;&#40836;&#31561;&#65289;&#22686;&#24378;&#36710;&#36742;&#39550;&#39542;&#21592;&#30340;&#20132;&#20114;&#23545;&#20110;&#23433;&#20840;&#12289;&#20010;&#24615;&#21270;&#21644;&#25972;&#20307;&#29992;&#25143;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20840;&#38754;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#23545;&#35757;&#32451;&#31283;&#20581;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#25991;&#29486;&#36890;&#24120;&#24573;&#35270;&#20102;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#28508;&#21147;&#20197;&#21450;&#22312;&#36825;&#31181;&#21463;&#38480;&#29615;&#22659;&#20013;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#27604;&#36739;&#21151;&#25928;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#26469;&#35757;&#32451;&#35782;&#21035;&#36710;&#36742;&#20056;&#23458;&#30340;&#38754;&#37096;&#23646;&#24615;&#65288;&#22914;&#30524;&#31070;&#26041;&#21521;&#12289;&#24180;&#40836;&#21644;&#38754;&#37096;&#34920;&#24773;&#65289;&#30340;&#22797;&#26434;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Vision Transformer&#65288;ViT&#65289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65288;ResNet&#65289;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#26469;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06088v1 Announce Type: cross  Abstract: In the burgeoning field of intelligent transportation systems, enhancing vehicle-driver interaction through facial attribute recognition, such as facial expression, eye gaze, age, etc., is of paramount importance for safety, personalization, and overall user experience. However, the scarcity of comprehensive large-scale, real-world datasets poses a significant challenge for training robust multi-task models. Existing literature often overlooks the potential of synthetic datasets and the comparative efficacy of state-of-the-art vision foundation models in such constrained settings. This paper addresses these gaps by investigating the utility of synthetic datasets for training complex multi-task models that recognize facial attributes of passengers of a vehicle, such as gaze plane, age, and facial expression. Utilizing transfer learning techniques with both pre-trained Vision Transformer (ViT) and Residual Network (ResNet) models, we exp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Goal-based Neural Variational Agent (GNeVA)&#27169;&#22411;&#65292;&#36890;&#36807;&#21464;&#20998;&#39640;&#26031;&#28151;&#21512;&#20272;&#31639;&#38271;&#26399;&#30446;&#30340;&#22320;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#39537;&#21160;&#30340;&#36816;&#21160;&#39044;&#27979;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06086</link><description>&lt;p&gt;
&#36890;&#21521;&#21487;&#27867;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#36816;&#21160;&#39044;&#27979;&#65306;&#19968;&#31181;&#28145;&#24230;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizable and Interpretable Motion Prediction: A Deep Variational Bayes Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Goal-based Neural Variational Agent (GNeVA)&#27169;&#22411;&#65292;&#36890;&#36807;&#21464;&#20998;&#39640;&#26031;&#28151;&#21512;&#20272;&#31639;&#38271;&#26399;&#30446;&#30340;&#22320;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#39537;&#21160;&#30340;&#36816;&#21160;&#39044;&#27979;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06086v1 &#20844;&#21578;&#31867;&#22411;:&#26032; &#25688;&#35201;:&#20272;&#35745;&#21608;&#22260;&#20154;&#39550;&#39542;&#36710;&#36742;&#30340;&#28508;&#22312;&#34892;&#20026;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#28151;&#21512;&#20132;&#36890;&#27969;&#20013;&#30340;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31471;&#21040;&#31471;&#27169;&#22411;&#36890;&#24120;&#26159;&#40657;&#31665;&#65292;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#24369;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30446;&#26631;&#30340;&#31070;&#32463;&#21464;&#20998;&#20195;&#29702;&#65288;GNeVA&#65289;&#65292;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#20855;&#26377;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#20197;&#22788;&#29702;&#20998;&#24067;&#20043;&#22806;&#24773;&#20917;&#30340;&#36816;&#21160;&#39044;&#27979;&#12290;&#20026;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20272;&#31639;&#38271;&#26399;&#30446;&#30340;&#22320;&#30340;&#31354;&#38388;&#20998;&#24067;&#26469;&#23454;&#29616;&#30446;&#26631;&#39537;&#21160;&#30340;&#36816;&#21160;&#39044;&#27979;&#65292;&#37319;&#29992;&#39640;&#26031;&#21464;&#20998;&#28151;&#21512;&#12290;&#25105;&#20204;&#35782;&#21035;&#22320;&#22270;&#21644;&#20195;&#29702;&#21382;&#21490;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#25512;&#23548;&#21464;&#20998;&#21518;&#39564;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36816;&#21160;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25311;&#21512;&#30340;&#27169;&#22411;&#26082;&#21487;&#35299;&#37322;&#21448;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#21487;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06086v1 Announce Type: new  Abstract: Estimating the potential behavior of the surrounding human-driven vehicles is crucial for the safety of autonomous vehicles in a mixed traffic flow. Recent state-of-the-art achieved accurate prediction using deep neural networks. However, these end-to-end models are usually black boxes with weak interpretability and generalizability. This paper proposes the Goal-based Neural Variational Agent (GNeVA), an interpretable generative model for motion prediction with robust generalizability to out-of-distribution cases. For interpretability, the model achieves target-driven motion prediction by estimating the spatial distribution of long-term destinations with a variational mixture of Gaussians. We identify a causal structure among maps and agents' histories and derive a variational posterior to enhance generalizability. Experiments on motion prediction datasets validate that the fitted model can be interpretable and generalizable and can achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06064</link><description>&lt;p&gt;
L$^2$GC: &#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#29992;&#20110;&#23545;&#22270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32447;&#24615;GCN&#27169;&#22411;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#25191;&#34892;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#65292;&#36825;&#24182;&#27809;&#26377;&#26126;&#30830;&#25429;&#25417;&#21040;&#20316;&#20026;&#22270;&#27169;&#22411;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#21576;&#29616;&#20986;&#30340;&#31867;&#20284;&#26641;&#29366;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#26412;&#25991;&#23581;&#35797;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;GCN&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22270;&#33410;&#28857;&#30340;&#23398;&#20064;&#29305;&#24449;&#26144;&#23556;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#36827;&#34892;&#27931;&#20262;&#20857;&#32447;&#24615;&#29305;&#24449;&#21464;&#25442;&#65292;&#20197;&#25429;&#33719;&#25968;&#25454;&#30340;&#28508;&#22312;&#26641;&#29366;&#32467;&#26500;&#12290;&#22312;&#26631;&#20934;&#24341;&#25991;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Citeseer&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;74.7%&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#22312;PubMed&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;81.3%&#30340;&#20934;&#30830;&#24230;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35757;&#32451;&#33267;&#23569;&#36798;&#21040;2&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06064v1 Announce Type: cross  Abstract: Linear Graph Convolutional Networks (GCNs) are used to classify the node in the graph data. However, we note that most existing linear GCN models perform neural network operations in Euclidean space, which do not explicitly capture the tree-like hierarchical structure exhibited in real-world datasets that modeled as graphs. In this paper, we attempt to introduce hyperbolic space into linear GCN and propose a novel framework for Lorentzian linear GCN. Specifically, we map the learned features of graph nodes into hyperbolic space, and then perform a Lorentzian linear feature transformation to capture the underlying tree-like structure of data. Experimental results on standard citation networks datasets with semi-supervised learning show that our approach yields new state-of-the-art results of accuracy 74.7$\%$ on Citeseer and 81.3$\%$ on PubMed datasets. Furthermore, we observe that our approach can be trained up to two orders of magnitu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#29983;&#25104;&#30340;&#30446;&#26631;&#21463;&#38480;&#21452;&#21521;&#35268;&#21010;&#65288;TRIP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21069;&#21521;&#21644;&#21518;&#21521;&#35268;&#21010;&#29983;&#25104;&#23545;&#35805;&#36335;&#24452;&#65292;&#20419;&#36827;&#23545;&#35805;&#21521;&#39044;&#23450;&#30446;&#26631;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.06063</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#20027;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#30446;&#26631;&#21463;&#38480;&#21452;&#21521;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Target-constrained Bidirectional Planning for Generation of Target-oriented Proactive Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06063
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#29983;&#25104;&#30340;&#30446;&#26631;&#21463;&#38480;&#21452;&#21521;&#35268;&#21010;&#65288;TRIP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21069;&#21521;&#21644;&#21518;&#21521;&#35268;&#21010;&#29983;&#25104;&#23545;&#35805;&#36335;&#24452;&#65292;&#20419;&#36827;&#23545;&#35805;&#21521;&#39044;&#23450;&#30446;&#26631;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#30340;&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#24341;&#23548;&#23545;&#35805;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#21521;&#39044;&#23450;&#30446;&#26631;&#21457;&#23637;&#65292;&#27604;&#22914;&#22312;&#25351;&#23450;&#39033;&#30446;&#19978;&#36827;&#34892;&#25512;&#33616;&#25110;&#20171;&#32461;&#26032;&#30340;&#29305;&#23450;&#20027;&#39064;&#12290;&#20026;&#27492;&#65292;&#23545;&#35805;&#31995;&#32479;&#20851;&#38190;&#22312;&#20110;&#35745;&#21010;&#21512;&#29702;&#30340;&#34892;&#21160;&#20197;&#20027;&#21160;&#25512;&#21160;&#23545;&#35805;&#65292;&#24182;&#21516;&#26102;&#35745;&#21010;&#36866;&#24403;&#30340;&#20027;&#39064;&#20197;&#39034;&#21033;&#25512;&#36827;&#23545;&#35805;&#21040;&#30446;&#26631;&#35805;&#39064;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#20110;&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#29983;&#25104;&#30340;&#26377;&#25928;&#23545;&#35805;&#35268;&#21010;&#12290;&#21463;&#35748;&#30693;&#31185;&#23398;&#20013;&#20915;&#31574;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30446;&#26631;&#21463;&#38480;&#21452;&#21521;&#35268;&#21010;&#65288;TRIP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21069;&#21521;&#21644;&#21518;&#21521;&#35268;&#21010;&#19968;&#20010;&#36866;&#24403;&#30340;&#23545;&#35805;&#36335;&#24452;&#12290;&#36890;&#36807;&#23558;&#35268;&#21010;&#24418;&#24335;&#21270;&#20026;&#19968;&#39033;&#29983;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;TRIP&#21452;&#21521;&#29983;&#25104;&#20102;&#19968;&#20010;&#30001;&#19968;&#31995;&#21015;pair&#32452;&#25104;&#30340;&#23545;&#35805;&#36335;&#24452;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;Transformer&#35299;&#30721;&#22120;&#12290;&#23427;&#20204;&#34987;&#39044;&#26399;&#29992;&#20110;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06063v1 Announce Type: cross  Abstract: Target-oriented proactive dialogue systems aim to lead conversations from a dialogue context toward a pre-determined target, such as making recommendations on designated items or introducing new specific topics. To this end, it is critical for such dialogue systems to plan reasonable actions to drive the conversation proactively, and meanwhile, to plan appropriate topics to move the conversation forward to the target topic smoothly. In this work, we mainly focus on effective dialogue planning for target-oriented dialogue generation. Inspired by decision-making theories in cognitive science, we propose a novel target-constrained bidirectional planning (TRIP) approach, which plans an appropriate dialogue path by looking ahead and looking back. By formulating the planning as a generation task, our TRIP bidirectionally generates a dialogue path consisting of a sequence of  pairs using two Transformer decoders. They are expected to supervis
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.06054</link><description>&lt;p&gt;
&#20855;&#26377;&#25193;&#25955;&#20928;&#21270;&#30340;&#20998;&#31163;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Decoupled Data Consistency with Diffusion Purification for Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06054
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#31867;&#21035;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#22320;&#24314;&#27169;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#35299;&#20915;&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#65292;&#35768;&#22810;&#29616;&#26377;&#25216;&#26415;&#36890;&#36807;&#23558;&#39069;&#22806;&#30340;&#20284;&#28982;&#26799;&#24230;&#27493;&#39588;&#32435;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#21453;&#21521;&#37319;&#26679;&#36807;&#31243;&#20013;&#26469;&#23454;&#29616;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#26799;&#24230;&#27493;&#39588;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#25512;&#29702;&#26102;&#38388;&#12290;&#24403;&#20351;&#29992;&#21152;&#36895;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#22120;&#26102;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#27493;&#39588;&#36824;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#22256;&#38590;&#65292;&#22240;&#20026;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#30340;&#25968;&#37327;&#21463;&#38480;&#20110;&#21453;&#21521;&#37319;&#26679;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#23558;&#21453;&#21521;&#36807;&#31243;&#19982;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#20998;&#31163;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MATRIX&#30340;&#23398;&#20064;&#22522;&#30784;&#33258;&#21160;&#36712;&#36857;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#32972;&#26223;&#20013;&#29983;&#25104;&#20132;&#20114;&#24335;&#20154;&#31867;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.06041</link><description>&lt;p&gt;
MATRIX: &#20855;&#26377;&#22810;&#26679;&#21270;&#32972;&#26223;&#30340;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MATRIX&#30340;&#23398;&#20064;&#22522;&#30784;&#33258;&#21160;&#36712;&#36857;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#32972;&#26223;&#20013;&#29983;&#25104;&#20132;&#20114;&#24335;&#20154;&#31867;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#24314;&#27169;&#22797;&#26434;&#20154;&#31867;&#34892;&#20026;&#21160;&#24577;&#21644;&#22788;&#29702;&#35768;&#22810;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#24102;&#26631;&#27880;&#30340;&#30495;&#23454;&#19990;&#30028;&#20154;&#31867;&#25968;&#25454;&#36890;&#24120;&#26159;&#19968;&#39033;&#36153;&#21147;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#24230;&#20132;&#20114;&#24335;&#30340;&#22330;&#26223;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#31639;&#27861;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#21463;&#21040;&#27169;&#22411;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#20351;&#20854;&#26080;&#27861;&#20026;&#21508;&#31181;&#24212;&#29992;&#29992;&#25143;&#25552;&#20379;&#25152;&#38656;&#30340;&#29616;&#23454;&#21644;&#22810;&#26679;&#21270;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#25110;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#30340;&#36712;&#36857;&#32423;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#30784;&#30340;&#33258;&#21160;&#36712;&#36857;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20855;&#26377;&#22810;&#26679;&#21270;&#32972;&#26223;&#30340;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#29983;&#25104;&#65288;MATRIX&#65289;&#12290;MATRIX&#33021;&#22815;&#22312;&#29616;&#23454;&#22810;&#26679;&#21270;&#30340;&#32972;&#26223;&#20013;&#29983;&#25104;&#20132;&#20114;&#24335;&#20154;&#31867;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#27169;&#26174;&#24335;&#21644;&#21487;&#35299;&#37322;&#30340;&#30446;&#26631;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20351;MATRIX&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06041v1 Announce Type: cross  Abstract: Data-driven methods have great advantages in modeling complicated human behavioral dynamics and dealing with many human-robot interaction applications. However, collecting massive and annotated real-world human datasets has been a laborious task, especially for highly interactive scenarios. On the other hand, algorithmic data generation methods are usually limited by their model capacities, making them unable to offer realistic and diverse data needed by various application users. In this work, we study trajectory-level data generation for multi-human or human-robot interaction scenarios and propose a learning-based automatic trajectory generation model, which we call Multi-Agent TRajectory generation with dIverse conteXts (MATRIX). MATRIX is capable of generating interactive human behaviors in realistic diverse contexts. We achieve this goal by modeling the explicit and interpretable objectives so that MATRIX can generate human motion
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#25506;&#35752;&#20102;YouTuber&#22312;&#20869;&#23481;&#21019;&#20316;&#20013;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#21644;&#29983;&#25104;&#30340;&#20869;&#23481;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.06039</link><description>&lt;p&gt;
&#23545;YouTuber&#22312;&#20869;&#23481;&#21019;&#20316;&#20013;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#21021;&#27493;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
A Preliminary Exploration of YouTubers' Use of Generative-AI in Content Creation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06039
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#25506;&#35752;&#20102;YouTuber&#22312;&#20869;&#23481;&#21019;&#20316;&#20013;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#21644;&#29983;&#25104;&#30340;&#20869;&#23481;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23481;&#21019;&#20316;&#32773;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;Gen-AI&#65289;&#22312;YouTube&#12289;TikTok&#12289;Instagram&#21644;&#21508;&#31181;&#21338;&#23458;&#32593;&#31449;&#31561;&#24179;&#21488;&#19978;&#29983;&#25104;&#24819;&#35937;&#21147;&#21313;&#36275;&#30340;&#22270;&#20687;&#12289;AI&#29983;&#25104;&#35270;&#39057;&#21644;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25991;&#31456;&#12290;&#23613;&#31649;&#20854;&#26085;&#30410;&#26222;&#21450;&#65292;AI&#29983;&#25104;&#20869;&#23481;&#27491;&#22312;&#24212;&#29992;&#30340;&#20855;&#20307;&#39046;&#22495;&#20197;&#21450;&#20869;&#23481;&#21019;&#20316;&#32773;&#22312;&#21019;&#20316;&#36807;&#31243;&#20013;&#20351;&#29992;Gen-AI&#24037;&#20855;&#30340;&#26041;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#34987;&#28145;&#20837;&#25506;&#35752;&#30340;&#39046;&#22495;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#23637;&#31034;Gen-AI&#20351;&#29992;&#30340;68&#20010;YouTube&#35270;&#39057;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#65292;&#26368;&#21021;&#25506;&#35752;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#35782;&#21035;&#22312;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#32972;&#26223;&#19979;&#30001;Gen-AI&#29983;&#25104;&#30340;&#20869;&#23481;&#39046;&#22495;&#12289;&#20351;&#29992;&#30340;&#24037;&#20855;&#31181;&#31867;&#12289;&#25191;&#34892;&#30340;&#27963;&#21160;&#20197;&#21450;&#29983;&#25104;&#30340;&#26368;&#32456;&#20135;&#21697;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06039v1 Announce Type: cross  Abstract: Content creators increasingly utilize generative artificial intelligence (Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging sites to produce imaginative images, AI-generated videos, and articles using Large Language Models (LLMs). Despite its growing popularity, there remains an underexplored area concerning the specific domains where AI-generated content is being applied, and the methodologies content creators employ with Gen-AI tools during the creation process. This study initially explores this emerging area through a qualitative analysis of 68 YouTube videos demonstrating Gen-AI usage. Our research focuses on identifying the content domains, the variety of tools used, the activities performed, and the nature of the final products generated by Gen-AI in the context of user-generated content.
&lt;/p&gt;</description></item><item><title>FairTargetSim&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#27169;&#25311;&#22120;&#65292;&#23637;&#31034;&#20102;&#30446;&#26631;&#21464;&#37327;&#23450;&#20041;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#36866;&#29992;&#20110;&#31639;&#27861;&#24320;&#21457;&#32773;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#38750;&#25216;&#26415;&#21033;&#30410;&#30456;&#20851;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.06031</link><description>&lt;p&gt;
FairTargetSim&#65306;&#29992;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#30446;&#26631;&#21464;&#37327;&#23450;&#20041;&#20844;&#24179;&#24615;&#24433;&#21709;&#30340;&#20132;&#20114;&#24335;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
FairTargetSim: An Interactive Simulator for Understanding and Explaining the Fairness Effects of Target Variable Definition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06031
&lt;/p&gt;
&lt;p&gt;
FairTargetSim&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#27169;&#25311;&#22120;&#65292;&#23637;&#31034;&#20102;&#30446;&#26631;&#21464;&#37327;&#23450;&#20041;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#36866;&#29992;&#20110;&#31639;&#27861;&#24320;&#21457;&#32773;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#38750;&#25216;&#26415;&#21033;&#30410;&#30456;&#20851;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#20026;&#39044;&#27979;&#25110;&#20915;&#31574;&#23450;&#20041;&#30446;&#26631;&#21464;&#37327;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#23545;&#20844;&#24179;&#24615;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#65306;&#20559;&#35265;&#36890;&#24120;&#24050;&#32463;&#34987;&#32534;&#30721;&#22312;&#30446;&#26631;&#21464;&#37327;&#23450;&#20041;&#26412;&#36523;&#20013;&#65292;&#32780;&#19981;&#26159;&#22312;&#20219;&#20309;&#25968;&#25454;&#25910;&#38598;&#25110;&#35757;&#32451;&#20043;&#21069;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#27169;&#25311;&#22120;&#65292;FairTargetSim (FTS)&#65292;&#23637;&#31034;&#20102;&#30446;&#26631;&#21464;&#37327;&#23450;&#20041;&#22914;&#20309;&#24433;&#21709;&#20844;&#24179;&#24615;&#12290;FTS&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#36866;&#29992;&#20110;&#31639;&#27861;&#24320;&#21457;&#32773;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#38750;&#25216;&#26415;&#21033;&#30410;&#30456;&#20851;&#32773;&#12290;FTS&#20351;&#29992;&#20102;&#31639;&#27861;&#25307;&#32856;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#21644;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#21464;&#37327;&#12290;FTS&#26159;&#24320;&#28304;&#30340;&#65292;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;http://tinyurl.com/ftsinterface&#12290;&#26412;&#25991;&#38468;&#24102;&#30340;&#35270;&#39057;&#32593;&#22336;&#20026;&#65306;http://tinyurl.com/ijcaifts&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06031v1 Announce Type: cross  Abstract: Machine learning requires defining one's target variable for predictions or decisions, a process that can have profound implications on fairness: biases are often encoded in target variable definition itself, before any data collection or training. We present an interactive simulator, FairTargetSim (FTS), that illustrates how target variable definition impacts fairness. FTS is a valuable tool for algorithm developers, researchers, and non-technical stakeholders. FTS uses a case study of algorithmic hiring, using real-world data and user-defined target variables. FTS is open-source and available at: http://tinyurl.com/ftsinterface. The video accompanying this paper is here: http://tinyurl.com/ijcaifts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20513;&#23548;&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26500;&#24314;&#36890;&#29992;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#29305;&#23450;&#34920;&#31034;&#26080;&#27861;&#36328;&#36234;&#19981;&#21516;&#32452;&#21512;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06026</link><description>&lt;p&gt;
&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26500;&#24314;&#36890;&#29992;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Towards a Generic Representation of Cominatorial Problems for Learning-Based Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20513;&#23548;&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26500;&#24314;&#36890;&#29992;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#29305;&#23450;&#34920;&#31034;&#26080;&#27861;&#36328;&#36234;&#19981;&#21516;&#32452;&#21512;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#20135;&#29983;&#20102;&#20852;&#36259;&#65292;&#26080;&#35770;&#26159;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36824;&#26159;&#19982;&#20256;&#32479;&#20248;&#21270;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#26223;&#19979;&#65292;&#25361;&#25112;&#22312;&#20110;&#23558;&#30446;&#26631;&#32452;&#21512;&#38382;&#39064;&#32534;&#30721;&#25104;&#36866;&#29992;&#20110;&#23398;&#20064;&#31639;&#27861;&#30340;&#32467;&#26500;&#12290;&#35768;&#22810;&#29616;&#26377;&#20316;&#21697;&#25552;&#20986;&#20102;&#29305;&#23450;&#20110;&#38382;&#39064;&#30340;&#34920;&#31034;&#65292;&#36890;&#24120;&#20197;&#22270;&#30340;&#24418;&#24335;&#65292;&#20197;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#65292;&#22240;&#20026;&#34920;&#31034;&#19981;&#33021;&#36731;&#26131;&#20174;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#23581;&#35797;&#21435;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#21482;&#25552;&#20379;&#20102;&#37096;&#20998;&#27867;&#21270;&#24615;&#12290;&#37492;&#20110;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#20513;&#23548;&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26397;&#30528;&#23436;&#20840;&#36890;&#29992;&#30340;&#34920;&#31034;&#26041;&#24335;&#36808;&#36827;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06026v1 Announce Type: cross  Abstract: In recent years, there has been a growing interest in using learning-based approaches for solving combinatorial problems, either in an end-to-end manner or in conjunction with traditional optimization algorithms. In both scenarios, the challenge lies in encoding the targeted combinatorial problems into a structure compatible with the learning algorithm. Many existing works have proposed problem-specific representations, often in the form of a graph, to leverage the advantages of \textit{graph neural networks}. However, these approaches lack generality, as the representation cannot be easily transferred from one combinatorial problem to another one. While some attempts have been made to bridge this gap, they still offer a partial generality only. In response to this challenge, this paper advocates for progress toward a fully generic representation of combinatorial problems for learning-based approaches. The approach we propose involves 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20174;&#22320;&#19979;&#20648;&#23384;&#31354;&#38388;&#20960;&#20309;&#22270;&#20687;&#20013;&#39044;&#27979;&#38470;&#22320;&#34920;&#38754;&#20301;&#31227;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#30899;&#25429;&#38598;&#21644;&#23553;&#23384;&#39033;&#30446;&#20013;&#30340;&#20915;&#31574;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.06025</link><description>&lt;p&gt;
CarbonNet: &#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#27668;&#20505;&#21464;&#21270;&#20013;&#30340;&#20316;&#29992;&#26159;&#20160;&#20040;&#65311; &#24212;&#29992;&#65306;&#23398;&#20064;&#20174;&#22320;&#19979;&#20648;&#23384;&#31354;&#38388;&#20960;&#20309;&#24418;&#29366;&#20013;&#20943;&#32531;&#20840;&#29699;&#21464;&#26262;&#30340;&#22320;&#36136;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
CarbonNet: How Computer Vision Plays a Role in Climate Change? Application: Learning Geomechanics from Subsurface Geometry of CCS to Mitigate Global Warming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06025
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20174;&#22320;&#19979;&#20648;&#23384;&#31354;&#38388;&#20960;&#20309;&#22270;&#20687;&#20013;&#39044;&#27979;&#38470;&#22320;&#34920;&#38754;&#20301;&#31227;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#30899;&#25429;&#38598;&#21644;&#23553;&#23384;&#39033;&#30446;&#20013;&#30340;&#20915;&#31574;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20174;&#22320;&#19979;&#20648;&#23384;&#31354;&#38388;&#20960;&#20309;&#22270;&#20687;&#20013;&#39044;&#27979;&#38470;&#22320;&#34920;&#38754;&#20301;&#31227;&#65292;&#20197;&#24212;&#29992;&#20110;&#30899;&#25429;&#38598;&#21644;&#23553;&#23384;&#65288;CCS&#65289;&#12290;CCS&#24050;&#34987;&#35777;&#26126;&#26159;&#30899;&#20013;&#21644;&#31038;&#20250;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#31185;&#23398;&#23478;&#21457;&#29616;&#23384;&#22312;&#25361;&#25112;&#65292;&#21253;&#25324;&#30001;&#20110;&#22823;&#27169;&#22411;&#23610;&#24230;&#32780;&#23548;&#33268;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#20197;&#21450;&#38590;&#20197;&#27867;&#21270;&#20855;&#26377;&#22797;&#26434;&#29289;&#29702;&#23398;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#20174;&#22320;&#19979;&#20648;&#23384;&#31354;&#38388;&#20960;&#20309;&#22270;&#20687;&#35757;&#32451;&#27169;&#22411;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29702;&#35299;&#30001;&#30899;&#27880;&#20837;&#23548;&#33268;&#30340;&#38470;&#22320;&#34920;&#38754;&#20301;&#31227;&#21709;&#24212;&#65292;&#24182;&#21033;&#29992;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#20026;CCS&#39033;&#30446;&#30340;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06025v1 Announce Type: cross  Abstract: We introduce a new approach using computer vision to predict the land surface displacement from subsurface geometry images for Carbon Capture and Sequestration (CCS). CCS has been proved to be a key component for a carbon neutral society. However, scientists see there are challenges along the way including the high computational cost due to the large model scale and limitations to generalize a pre-trained model with complex physics. We tackle those challenges by training models directly from the subsurface geometry images. The goal is to understand the respons of land surface displacement due to carbon injection and utilize our trained models to inform decision making in CCS projects.   We implement multiple models (CNN, ResNet, and ResNetUNet) for static mechanics problem, which is a image prediction problem. Next, we use the LSTM and transformer for transient mechanics scenario, which is a video prediction problem. It shows ResNetUNe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22914;&#20309;&#23558;&#20855;&#26377;70&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;PLM LLaMa &#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25552;&#31034;&#65292;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#36866;&#24212;&#25552;&#31034;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06018</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#29992;&#20110;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22914;&#20309;&#23558;&#20855;&#26377;70&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;PLM LLaMa &#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25552;&#31034;&#65292;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#36866;&#24212;&#25552;&#31034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22788;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#23637;&#30340;&#21069;&#27839;&#12290;PLMs&#30340;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#26159;&#8220;&#25552;&#31034;&#8221; - &#25110;&#19978;&#19979;&#25991;&#23398;&#20064; - &#29992;&#25143;&#22312;&#25552;&#31034;PLM&#23545;&#26032;&#31034;&#20363;&#25191;&#34892;&#20219;&#21153;&#20043;&#21069;&#21521;PLM&#25552;&#20379;&#20219;&#21153;&#25551;&#36848;&#21644;&#19968;&#20123;&#23436;&#25104;&#30340;&#31034;&#20363;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;&#30446;&#21069;&#21482;&#26377;&#26368;&#22823;&#12289;&#26368;&#26377;&#33021;&#21147;&#30340;PLMs&#25165;&#33021;&#26377;&#25928;&#22320;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26159;&#36890;&#36807;&#20027;&#35201;&#20197;&#33521;&#35821;&#20026;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#65292;&#20854;&#20182;&#25152;&#26377;&#35821;&#35328;&#37117;&#33853;&#21518;&#12290;&#22823;&#22810;&#25968;&#35821;&#35328;&#30340;&#25968;&#25454;&#38480;&#21046;&#38459;&#30861;&#20102;&#35757;&#32451;&#20855;&#26377;&#25552;&#31034;&#33021;&#21147;&#30340;&#35821;&#35328;&#29305;&#23450;PLMs&#12290;&#23613;&#31649;&#22312;&#25552;&#31034;&#35774;&#32622;&#26041;&#38754;&#30340;&#24037;&#20316;&#28608;&#22686;&#65292;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#22914;&#20309;&#23558;PLMs&#19987;&#38376;&#29992;&#20110;&#36328;&#35821;&#35328;&#36866;&#24212;&#25552;&#31034;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36866;&#24212;LLaMa&#30340;&#21487;&#33021;&#26041;&#27861;&#65292;LLaMa&#26159;&#19968;&#20010;&#20027;&#35201;&#22312;&#33521;&#35821;&#20013;&#35757;&#32451;&#30340;&#20855;&#26377;70&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;PLM&#65292;&#29992;&#20110;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#36827;&#34892;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06018v1 Announce Type: cross  Abstract: Large pre-trained language models (PLMs) are at the forefront of advances in Natural Language Processing. One widespread use case of PLMs is "prompting" - or in-context learning - where a user provides a description of a task and some completed examples of the task to a PLM as context before prompting the PLM to perform the task on a new example. Only the largest, most capable PLMs are able to perform in-context learning effectively, and these models are typically trained with a predominantly English corpus, leaving all other languages behind. The data limitations in most languages preclude the training of language-specific PLMs capable of prompting. Albeit the surge in work of prompting settings, it is still unclear how PLMs should be adapted cross-lingually specifically for prompting. We evaluate the possible methods to adapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for prompting in low-resource languages, nam
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30828;&#26631;&#31614;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25915;&#20987;&#30340;&#26597;&#35810;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.06014</link><description>&lt;p&gt;
&#22522;&#20110;&#30828;&#26631;&#31614;&#30340;&#23567;&#26597;&#35810;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Hard-label based Small Query Black-box Adversarial Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06014
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30828;&#26631;&#31614;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25915;&#20987;&#30340;&#26597;&#35810;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#30828;&#26631;&#31614;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#35774;&#32622;&#65292;&#20165;&#35266;&#23519;&#26469;&#33258;&#30446;&#26631;&#27169;&#22411;&#30340;&#39044;&#27979;&#31867;&#21035;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#22823;&#22810;&#25968;&#25915;&#20987;&#26041;&#27861;&#37117;&#38656;&#35201;&#19981;&#20999;&#23454;&#38469;&#25968;&#37327;&#30340;&#26597;&#35810;&#25165;&#33021;&#23454;&#29616;&#25104;&#21151;&#25915;&#20987;&#12290;&#35299;&#20915;&#36825;&#19968;&#32570;&#28857;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#21033;&#29992;&#30333;&#30418;&#26367;&#20195;&#27169;&#22411;&#19982;&#40657;&#30418;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#23545;&#25239;&#20256;&#36882;&#24615;&#12290;&#28982;&#32780;&#65292;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#36719;&#26631;&#31614;&#30340;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#12290;&#19982;&#20027;&#27969;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#29992;&#30340;&#22522;&#20110;&#30828;&#26631;&#31614;&#30340;&#25915;&#20987;&#35774;&#32622;&#65292;&#20854;&#20248;&#21270;&#36807;&#31243;&#30001;&#39044;&#35757;&#32451;&#30340;&#26367;&#20195;&#27169;&#22411;&#25351;&#23548;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#21508;&#31181;&#30446;&#26631;&#27169;&#22411;&#26550;&#26500;&#19978;&#30340;&#22522;&#20110;&#30828;&#26631;&#31614;&#30340;&#40657;&#30418;&#25915;&#20987;&#30340;&#26597;&#35810;&#25928;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#32422;5&#20493;&#26356;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06014v1 Announce Type: cross  Abstract: We consider the hard label based black box adversarial attack setting which solely observes predicted classes from the target model. Most of the attack methods in this setting suffer from impractical number of queries required to achieve a successful attack. One approach to tackle this drawback is utilising the adversarial transferability between white box surrogate models and black box target model. However, the majority of the methods adopting this approach are soft label based to take the full advantage of zeroth order optimisation. Unlike mainstream methods, we propose a new practical setting of hard label based attack with an optimisation process guided by a pretrained surrogate model. Experiments show the proposed method significantly improves the query efficiency of the hard label based black-box attack across various target model architectures. We find the proposed method achieves approximately 5 times higher attack success rat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21487;&#20197;&#25429;&#25417;&#22870;&#21169;&#20989;&#25968;&#30456;&#20284;&#24615;&#23450;&#20041;&#30340;&#26694;&#26550;&#65292;&#20248;&#21270;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.06003</link><description>&lt;p&gt;
&#19968;&#31181;&#25512;&#24191;&#30340;&#29992;&#20110;&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#23398;&#20064;&#30340;&#25910;&#30410;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Generalized Acquisition Function for Preference-based Reward Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06003
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21487;&#20197;&#25429;&#25417;&#22870;&#21169;&#20989;&#25968;&#30456;&#20284;&#24615;&#23450;&#20041;&#30340;&#26694;&#26550;&#65292;&#20248;&#21270;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#23398;&#20064;&#26159;&#19968;&#31181;&#29992;&#20110;&#25945;&#23548;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#22914;&#20309;&#25191;&#34892;&#20219;&#21153;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290; &#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31215;&#26497;&#21512;&#25104;&#20559;&#22909;&#26597;&#35810;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#22870;&#21169;&#20989;&#25968;&#21442;&#25968;&#30340;&#20449;&#24687;&#22686;&#30410;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12290; &#20449;&#24687;&#22686;&#30410;&#26631;&#20934;&#20391;&#37325;&#20110;&#31934;&#30830;&#35782;&#21035;&#22870;&#21169;&#20989;&#25968;&#30340;&#25152;&#26377;&#21442;&#25968;&#12290; &#36825;&#21487;&#33021;&#26159;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#35768;&#22810;&#21442;&#25968;&#21487;&#33021;&#23548;&#33268;&#30456;&#21516;&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#35768;&#22810;&#22870;&#21169;&#21487;&#33021;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#30456;&#21516;&#34892;&#20026;&#12290; &#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20248;&#21270;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30452;&#21040;&#34892;&#20026;&#31561;&#25928;&#31867;&#65292;&#20363;&#22914;&#35825;&#23548;&#20986;&#30456;&#21516;&#30340;&#34892;&#20026;&#25490;&#24207;&#65292;&#36873;&#25321;&#20998;&#24067;&#65292;&#25110;&#20854;&#20182;&#30456;&#20851;&#23450;&#20041;&#20351;&#24471;&#20004;&#20010;&#22870;&#21169;&#30475;&#36215;&#26469;&#30456;&#20284;&#12290; &#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#20197;&#25429;&#25417;&#36825;&#31181;&#30456;&#20284;&#24615;&#23450;&#20041;&#30340;&#21487;&#22788;&#29702;&#26694;&#26550;&#12290; &#25105;&#20204;&#22312;&#19968;&#20010;.synthetic env&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06003v1 Announce Type: cross  Abstract: Preference-based reward learning is a popular technique for teaching robots and autonomous systems how a human user wants them to perform a task. Previous works have shown that actively synthesizing preference queries to maximize information gain about the reward function parameters improves data efficiency. The information gain criterion focuses on precisely identifying all parameters of the reward function. This can potentially be wasteful as many parameters may result in the same reward, and many rewards may result in the same behavior in the downstream tasks. Instead, we show that it is possible to optimize for learning the reward function up to a behavioral equivalence class, such as inducing the same ranking over behaviors, distribution over choices, or other related definitions of what makes two rewards similar. We introduce a tractable framework that can capture such definitions of similarity. Our experiments in a synthetic env
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21078;&#26512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39318;&#35201;&#20559;&#24046;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#22823;&#37327;&#26356;&#26032;&#27604;&#20363;&#19979;&#65292;&#20215;&#20540;&#39640;&#20272;&#26159;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#26681;&#26412;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05996</link><description>&lt;p&gt;
&#29992;&#39640;&#26356;&#26032;&#27604;&#20363;&#21078;&#26512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#24212;&#23545;&#20215;&#20540;&#39640;&#20272;&#21644;&#21457;&#25955;
&lt;/p&gt;
&lt;p&gt;
Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21078;&#26512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39318;&#35201;&#20559;&#24046;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#22823;&#37327;&#26356;&#26032;&#27604;&#20363;&#19979;&#65292;&#20215;&#20540;&#39640;&#20272;&#26159;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#26681;&#26412;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35774;&#32622;&#20013;&#21487;&#20197;&#22312;&#26799;&#24230;&#26356;&#26032;&#27425;&#25968;&#22823;&#22823;&#36229;&#36807;&#29615;&#22659;&#26679;&#26412;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#23398;&#20064;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#32622;&#32593;&#32476;&#21442;&#25968;&#12290;&#22312;&#36825;&#31181;&#22823;&#37327;&#26356;&#26032;&#19982;&#25968;&#25454;&#27604;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23612;&#22522;&#36763;&#31561;&#20154; (2022) &#30340;&#26368;&#36817;&#19968;&#39033;&#30740;&#31350;&#25351;&#20986;&#20102;&#19968;&#20010;&#39318;&#35201;&#20559;&#24046;&#30340;&#20986;&#29616;&#65292;&#21363;&#20195;&#29702;&#22312;&#26089;&#26399;&#20132;&#20114;&#20013;&#36807;&#25311;&#21512;&#24182;&#28129;&#21270;&#21518;&#32493;&#32463;&#39564;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#20854;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#35299;&#26512;&#20102;&#23548;&#33268;&#39318;&#35201;&#20559;&#24046;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#24212;&#35813;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#26159;&#38271;&#26399;&#20197;&#26469;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#20215;&#20540;&#39640;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;Q&#20540;&#19981;&#20165;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#34987;&#39640;&#20272;&#65292;&#32780;&#19988;&#22312;&#20998;&#24067;&#20869;&#25968;&#25454;&#19978;&#20063;&#26159;&#22914;&#27492;&#65292;&#21487;&#20197;&#36861;&#28335;&#21040;&#30001;&#20248;&#21270;&#22120;&#21160;&#37327;&#25512;&#21160;&#30340;&#26410;&#35265;&#30340;&#21160;&#20316;&#39044;&#27979;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21333;&#20301;&#29699;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#26356;&#26032;&#27604;&#20363;&#19979;&#23454;&#29616;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05996v1 Announce Type: cross  Abstract: We show that deep reinforcement learning can maintain its ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples. Under such large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we dissect the phenomena underlying the primacy bias. We inspect the early stages of training that ought to cause the failure to learn and find that a fundamental challenge is a long-standing acquaintance: value overestimation. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be traced to unseen action prediction propelled by optimizer momentum. We employ a simple unit-ball normalization that enables learning under large update ratios
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;APRICOT&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35774;&#32622;&#32622;&#20449;&#30446;&#26631;&#24182;&#35757;&#32451;&#39069;&#22806;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.05973</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#29983;&#25104;&#26469;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Calibrating Large Language Models Using Their Generations Only
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05973
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;APRICOT&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35774;&#32622;&#32622;&#20449;&#30446;&#26631;&#24182;&#35757;&#32451;&#39069;&#22806;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#37096;&#32626;&#22312;&#38754;&#21521;&#29992;&#25143;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#36890;&#36807;&#20934;&#30830;&#37327;&#21270;&#27169;&#22411;&#23545;&#20854;&#39044;&#27979;&#30340;&#20449;&#24515;&#26469;&#24314;&#31435;&#20449;&#20219;&#24182;&#20445;&#25345;&#23433;&#20840;&#24615;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26657;&#20934;LLMs - &#23588;&#20854;&#26159;&#24403;&#19982;&#27169;&#22411;&#30340;&#21807;&#19968;&#25509;&#21475;&#26159;&#23427;&#20204;&#29983;&#25104;&#30340;&#25991;&#26412;&#26102; - &#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;APRICOT&#65288;&#36741;&#21161;&#39044;&#27979;&#32622;&#20449;&#30446;&#26631;&#65289;&#65306;&#19968;&#31181;&#36890;&#36807;&#20165;&#20351;&#29992;&#20854;&#25991;&#26412;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35774;&#32622;&#32622;&#20449;&#30446;&#26631;&#24182;&#35757;&#32451;&#19968;&#20010;&#39069;&#22806;&#27169;&#22411;&#26469;&#39044;&#27979;LLM&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#20960;&#20010;&#20248;&#28857;&#65306;&#27010;&#24565;&#19978;&#31616;&#21333;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#36229;&#20986;&#20854;&#36755;&#20986;&#65292;&#19981;&#24178;&#25200;&#35821;&#35328;&#29983;&#25104;&#65292;&#24182;&#19988;&#26377;&#22810;&#31181;&#28508;&#22312;&#29992;&#36884;&#65292;&#20363;&#22914;&#36890;&#36807;&#35328;&#35821;&#21270;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#25110;&#26681;&#25454;&#32622;&#20449;&#24230;&#35843;&#25972;&#32473;&#23450;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05973v1 Announce Type: cross  Abstract: As large language models (LLMs) are increasingly deployed in user-facing applications, building trust and maintaining safety by accurately quantifying a model's confidence in its prediction becomes even more important. However, finding effective ways to calibrate LLMs - especially when the only interface to the models is their generated text - remains a challenge. We propose APRICOT (auxiliary prediction of confidence targets): A method to set confidence targets and train an additional model that predicts an LLM's confidence based on its textual input and output alone. This approach has several advantages: It is conceptually simple, does not require access to the target model beyond its output, does not interfere with the language generation, and has a multitude of potential usages, for instance by verbalizing the predicted confidence or adjusting the given answer based on the confidence. We show how our approach performs competitively
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GRU&#21644;LSTM&#28151;&#21512;&#26041;&#27861;&#36827;&#34892;3D&#28857;&#20113;&#20013;&#29289;&#20307;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#23545;&#22810;&#20010;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.05950</link><description>&lt;p&gt;
&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;3D&#28857;&#20113;&#20013;&#23545;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#65306;&#19968;&#31181;GRU LSTM&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A GRU LSTM Hybrid Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GRU&#21644;LSTM&#28151;&#21512;&#26041;&#27861;&#36827;&#34892;3D&#28857;&#20113;&#20013;&#29289;&#20307;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#23545;&#22810;&#20010;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#23548;&#33322;&#21644;&#22686;&#24378;/&#34394;&#25311;&#29616;&#23454;&#22330;&#26223;&#31561;&#22810;&#20010;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#23545;3D&#28857;&#20113;&#20013;&#30340;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#24050;&#25104;&#20026;&#30740;&#31350;&#28909;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#29616;&#23454;&#20013;&#30340;3D&#29289;&#20307;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#26159;GRU&#21644;LSTM&#30340;&#32452;&#21512;&#12290;LSTM&#32593;&#32476;&#33021;&#22815;&#24456;&#22909;&#22320;&#23398;&#20064;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#30001;&#20110;&#38376;&#25968;&#37327;&#36739;&#22810;&#65292;&#35757;&#32451;&#26102;&#38388;&#36739;&#38271;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;GRU&#32593;&#32476;&#24615;&#33021;&#36739;&#24369;&#20110;LSTM&#65292;&#20294;&#20854;&#35757;&#32451;&#36895;&#24230;&#36828;&#39640;&#20110;LSTM&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#38376;&#25968;&#37327;&#36739;&#23569;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36825;&#20004;&#31181;&#32593;&#32476;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#32452;&#21512;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21253;&#21547;&#20843;&#20010;&#31867;&#21035;&#65288;&#26410;&#26631;&#35760;&#12289;&#20154;&#36896;&#22320;&#24418;&#12289;&#33258;&#28982;&#22320;&#24418;&#12289;&#39640;&#26893;&#34987;&#12289;&#20302;&#26893;&#34987;&#12289;&#24314;&#31569;&#12289;&#30828;&#26223;&#35266;&#65289;&#30340;4,499,0641&#20010;&#28857;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;0.99&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05950v1 Announce Type: cross  Abstract: Accurate classification of objects in 3D point clouds is a significant problem in several applications, such as autonomous navigation and augmented/virtual reality scenarios, which has become a research hot spot. In this paper, we presented a deep learning strategy for 3D object classification in augmented reality. The proposed approach is a combination of the GRU and LSTM. LSTM networks learn longer dependencies well, but due to the number of gates, it takes longer to train; on the other hand, GRU networks have a weaker performance than LSTM, but their training speed is much higher than GRU, which is The speed is due to its fewer gates. The proposed approach used the combination of speed and accuracy of these two networks. The proposed approach achieved an accuracy of 0.99 in the 4,499,0641 points dataset, which includes eight classes (unlabeled, man-made terrain, natural terrain, high vegetation, low vegetation, buildings, hardscape,
&lt;/p&gt;</description></item><item><title>&#35813;&#27169;&#22411;&#35774;&#35745;&#20102;&#19968;&#20010;&#23398;&#20064;-based &#27169;&#22411;&#65288;ProbCT&#65289;&#65292;&#36890;&#36807;&#22024;&#26434;&#30340;&#22810;&#35270;&#35282;&#33322;&#22825;&#22270;&#20687;&#23454;&#29616;&#20102;&#23545;&#20113;&#30340;&#19977;&#32500;CT&#65292;&#39318;&#27425;&#25512;&#26029;&#20986;&#27599;&#20010;3D&#20301;&#32622;&#30340;&#21518;&#39564;&#27010;&#29575;&#20998;&#24067;&#65292;&#20135;&#29983;&#20102;&#20855;&#26377;&#20215;&#20540;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.05932</link><description>&lt;p&gt;
&#23398;&#20064;&#19977;&#32500;&#20113;&#20307;&#31215;&#24674;&#22797;&#21450;&#20854;&#22312;&#27668;&#20505;&#20998;&#26512;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learned 3D volumetric recovery of clouds and its uncertainty for climate analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05932
&lt;/p&gt;
&lt;p&gt;
&#35813;&#27169;&#22411;&#35774;&#35745;&#20102;&#19968;&#20010;&#23398;&#20064;-based &#27169;&#22411;&#65288;ProbCT&#65289;&#65292;&#36890;&#36807;&#22024;&#26434;&#30340;&#22810;&#35270;&#35282;&#33322;&#22825;&#22270;&#20687;&#23454;&#29616;&#20102;&#23545;&#20113;&#30340;&#19977;&#32500;CT&#65292;&#39318;&#27425;&#25512;&#26029;&#20986;&#27599;&#20010;3D&#20301;&#32622;&#30340;&#21518;&#39564;&#27010;&#29575;&#20998;&#24067;&#65292;&#20135;&#29983;&#20102;&#20855;&#26377;&#20215;&#20540;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#39044;&#27979;&#21644;&#20113;&#29289;&#29702;&#23398;&#20013;&#30340;&#37325;&#35201;&#19981;&#30830;&#23450;&#24615;&#19982;&#20851;&#20110;&#31232;&#30095;&#25955;&#23556;&#20113;&#30340;&#35266;&#27979;&#31354;&#38553;&#30456;&#20851;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#36828;&#31243;&#24863;&#27979;&#23427;&#20204;&#30340;&#19977;&#32500;&#65288;3D&#65289;&#24322;&#36136;&#20307;&#31215;&#25955;&#23556;&#20869;&#23481;&#12290;&#36825;&#38656;&#35201;&#36827;&#34892;&#34987;&#21160;&#25955;&#23556;&#35745;&#31639;&#26029;&#23618;&#25668;&#24433;&#65288;CT&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#65288;ProbCT&#65289;&#65292;&#36890;&#36807;&#22024;&#26434;&#30340;&#22810;&#35270;&#35282;&#33322;&#22825;&#22270;&#20687;&#23454;&#29616;&#36825;&#31867;&#20113;&#30340;CT&#12290;ProbCT&#39318;&#27425;&#25512;&#26029;&#20986;&#27599;&#20010;3D&#20301;&#32622;&#30340;&#24322;&#36136;&#28040;&#20809;&#31995;&#25968;&#30340;&#21518;&#39564;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#20135;&#29983;&#20102;&#20219;&#24847;&#26377;&#20215;&#20540;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20363;&#22914;&#26368;&#21487;&#33021;&#30340;&#28040;&#20809;&#30340;3D&#22330;&#21644;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;ProbCT&#20351;&#29992;&#31070;&#32463;&#22330;&#34920;&#31034;&#65292;&#23454;&#36136;&#19978;&#21487;&#20197;&#36827;&#34892;&#23454;&#26102;&#25512;&#26029;&#12290;ProbCT&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#24102;&#26631;&#31614;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#22810;&#31867;&#25968;&#25454;&#24211;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#21253;&#25324;&#20113;&#30340;&#20307;&#31215;&#22330;&#21644;&#30456;&#24212;&#30340;&#22270;&#20687;&#65292;&#20197;&#25913;&#21892;&#20854;&#36234;&#30028;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05932v1 Announce Type: cross  Abstract: Significant uncertainty in climate prediction and cloud physics is tied to observational gaps relating to shallow scattered clouds. Addressing these challenges requires remote sensing of their three-dimensional (3D) heterogeneous volumetric scattering content. This calls for passive scattering computed tomography (CT). We design a learning-based model (ProbCT) to achieve CT of such clouds, based on noisy multi-view spaceborne images. ProbCT infers - for the first time - the posterior probability distribution of the heterogeneous extinction coefficient, per 3D location. This yields arbitrary valuable statistics, e.g., the 3D field of the most probable extinction and its uncertainty. ProbCT uses a neural-field representation, making essentially real-time inference. ProbCT undergoes supervised training by a new labeled multi-class database of physics-based volumetric fields of clouds and their corresponding images. To improve out-of-distr
&lt;/p&gt;</description></item><item><title>OntoChat&#26159;&#19968;&#20010;&#25903;&#25345;&#23545;&#35805;&#26412;&#20307;&#24037;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#23545;&#35805;&#20195;&#29702;&#20114;&#21160;&#65292;&#29992;&#25143;&#21487;&#20197;&#24341;&#23548;&#29992;&#25143;&#25925;&#20107;&#30340;&#21019;&#24314;&#21644;&#33021;&#21147;&#38382;&#39064;&#30340;&#25552;&#21462;&#65292;&#21516;&#26102;&#25509;&#25910;&#35745;&#31639;&#25903;&#25345;&#20197;&#20998;&#26512;&#25972;&#20307;&#38656;&#27714;&#24182;&#36827;&#34892;&#26089;&#26399;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2403.05921</link><description>&lt;p&gt;
OntoChat: &#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#26412;&#20307;&#24037;&#31243;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OntoChat: a Framework for Conversational Ontology Engineering using Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05921
&lt;/p&gt;
&lt;p&gt;
OntoChat&#26159;&#19968;&#20010;&#25903;&#25345;&#23545;&#35805;&#26412;&#20307;&#24037;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#23545;&#35805;&#20195;&#29702;&#20114;&#21160;&#65292;&#29992;&#25143;&#21487;&#20197;&#24341;&#23548;&#29992;&#25143;&#25925;&#20107;&#30340;&#21019;&#24314;&#21644;&#33021;&#21147;&#38382;&#39064;&#30340;&#25552;&#21462;&#65292;&#21516;&#26102;&#25509;&#25910;&#35745;&#31639;&#25903;&#25345;&#20197;&#20998;&#26512;&#25972;&#20307;&#38656;&#27714;&#24182;&#36827;&#34892;&#26089;&#26399;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39033;&#30446;&#20013;&#30340;&#26412;&#20307;&#24037;&#31243;(OE)&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#20854;&#20013;&#30340;&#25361;&#25112;&#28304;&#20110;&#21508;&#26041;&#21033;&#30410;&#30456;&#20851;&#32773;&#12289;&#39046;&#22495;&#19987;&#23478;&#21450;&#20854;&#19982;&#26412;&#20307;&#35774;&#35745;&#32773;&#30340;&#22797;&#26434;&#20114;&#21160;&#32972;&#26223;&#30340;&#24322;&#36136;&#24615;&#12290;&#36825;&#31181;&#22810;&#26041;&#20114;&#21160;&#32463;&#24120;&#20250;&#22312;&#26412;&#20307;&#38656;&#27714;&#30340;&#24341;&#30003;&#20013;&#20135;&#29983;&#31995;&#32479;&#24615;&#30340;&#27495;&#20041;&#21644;&#20559;&#35265;&#65292;&#30452;&#25509;&#24433;&#21709;&#35774;&#35745;&#12289;&#35780;&#20272;&#24182;&#21487;&#33021;&#21361;&#21450;&#30446;&#26631;&#30340;&#37325;&#22797;&#20351;&#29992;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30446;&#21069;&#30340;OE&#26041;&#27861;&#24378;&#28872;&#20381;&#36182;&#20110;&#25163;&#24037;&#27963;&#21160;&#65288;&#22914;&#37319;&#35775;&#12289;&#35752;&#35770;&#39029;&#38754;&#65289;&#12290;&#22312;&#25910;&#38598;&#20102;&#20851;&#20110;&#26368;&#20851;&#38190;OE&#27963;&#21160;&#30340;&#35777;&#25454;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;OntoChat&#65292;&#36825;&#26159;&#19968;&#20010;&#25903;&#25345;&#38656;&#27714;&#24341;&#30003;&#12289;&#20998;&#26512;&#21644;&#27979;&#35797;&#30340;&#23545;&#35805;&#26412;&#20307;&#24037;&#31243;&#26694;&#26550;&#12290;&#36890;&#36807;&#19982;&#23545;&#35805;&#20195;&#29702;&#36827;&#34892;&#20114;&#21160;&#65292;&#29992;&#25143;&#21487;&#20197;&#24341;&#23548;&#29992;&#25143;&#25925;&#20107;&#30340;&#21019;&#24314;&#21644;&#33021;&#21147;&#38382;&#39064;&#30340;&#25552;&#21462;&#65292;&#21516;&#26102;&#25509;&#25910;&#35745;&#31639;&#25903;&#25345;&#20197;&#20998;&#26512;&#25972;&#20307;&#38656;&#27714;&#24182;&#36827;&#34892;&#26089;&#26399;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05921v1 Announce Type: new  Abstract: Ontology engineering (OE) in large projects poses a number of challenges arising from the heterogeneous backgrounds of the various stakeholders, domain experts, and their complex interactions with ontology designers. This multi-party interaction often creates systematic ambiguities and biases from the elicitation of ontology requirements, which directly affect the design, evaluation and may jeopardise the target reuse. Meanwhile, current OE methodologies strongly rely on manual activities (e.g., interviews, discussion pages). After collecting evidence on the most crucial OE activities, we introduce OntoChat, a framework for conversational ontology engineering that supports requirement elicitation, analysis, and testing. By interacting with a conversational agent, users can steer the creation of user stories and the extraction of competency questions, while receiving computational support to analyse the overall requirements and test early
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#28151;&#21512;NLP&#27169;&#22411;&#30340;&#32467;&#21512;&#21487;&#22312;&#39640;&#20934;&#30830;&#29575;&#19979;&#23545;&#21307;&#24072;&#31508;&#35760;&#36827;&#34892;&#39640;&#21534;&#21520;&#37327;&#34920;&#22411;&#35782;&#21035;&#65292;&#26377;&#26395;&#25104;&#20026;&#26410;&#26469;&#39318;&#36873;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.05920</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#28151;&#21512;NLP&#27169;&#22411;&#30340;&#21307;&#24072;&#31508;&#35760;&#39640;&#21534;&#21520;&#37327;&#34920;&#22411;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
High Throughput Phenotyping of Physician Notes with Large Language and Hybrid NLP Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05920
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#28151;&#21512;NLP&#27169;&#22411;&#30340;&#32467;&#21512;&#21487;&#22312;&#39640;&#20934;&#30830;&#29575;&#19979;&#23545;&#21307;&#24072;&#31508;&#35760;&#36827;&#34892;&#39640;&#21534;&#21520;&#37327;&#34920;&#22411;&#35782;&#21035;&#65292;&#26377;&#26395;&#25104;&#20026;&#26410;&#26469;&#39318;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#34920;&#22411;&#26159;&#21033;&#29992;&#26412;&#20307;&#35770;&#20013;&#30340;&#27010;&#24565;&#23545;&#24739;&#32773;&#20307;&#24449;&#21644;&#30151;&#29366;&#36827;&#34892;&#35814;&#32454;&#25551;&#36848;&#12290;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#22823;&#37327;&#21307;&#24072;&#31508;&#35760;&#30340;&#28145;&#24230;&#34920;&#22411;&#38656;&#35201;&#39640;&#21534;&#21520;&#37327;&#30340;&#26041;&#27861;&#12290;&#22312;&#36807;&#21435;&#30340;&#19977;&#21313;&#24180;&#37324;&#65292;&#21462;&#24471;&#20102;&#20351;&#39640;&#21534;&#21520;&#37327;&#34920;&#22411;&#25104;&#20026;&#21487;&#33021;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19968;&#20010;&#28151;&#21512;NLP&#27169;&#22411;&#65288;&#32467;&#21512;&#20102;&#35789;&#21521;&#37327;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65289;&#21487;&#20197;&#22312;&#39640;&#20934;&#30830;&#29575;&#19979;&#23545;&#21307;&#24072;&#31508;&#35760;&#36827;&#34892;&#39640;&#21534;&#21520;&#37327;&#34920;&#22411;&#35782;&#21035;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24456;&#21487;&#33021;&#20250;&#25104;&#20026;&#21307;&#24072;&#31508;&#35760;&#39640;&#21534;&#21520;&#37327;&#28145;&#24230;&#34920;&#22411;&#35782;&#21035;&#30340;&#39318;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05920v1 Announce Type: cross  Abstract: Deep phenotyping is the detailed description of patient signs and symptoms using concepts from an ontology. The deep phenotyping of the numerous physician notes in electronic health records requires high throughput methods. Over the past thirty years, progress toward making high throughput phenotyping feasible. In this study, we demonstrate that a large language model and a hybrid NLP model (combining word vectors with a machine learning classifier) can perform high throughput phenotyping on physician notes with high accuracy. Large language models will likely emerge as the preferred method for high throughput deep phenotyping of physician notes.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#32463;&#20856;&#36807;&#37319;&#26679;&#26041;&#27861;&#21644;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#30340;&#27169;&#24335;&#22604;&#38519;&#19982;&#35757;&#32451;&#19981;&#31283;&#23450;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05918</link><description>&lt;p&gt;
&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#30340;&#25193;&#25955;&#24314;&#27169;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05918
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#32463;&#20856;&#36807;&#37319;&#26679;&#26041;&#27861;&#21644;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#30340;&#27169;&#24335;&#22604;&#38519;&#19982;&#35757;&#32451;&#19981;&#31283;&#23450;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36890;&#24120;&#20351;&#29992;&#30340;&#20998;&#31867;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#26080;&#27861;&#26377;&#25928;&#23398;&#20064;&#12290;&#20026;&#20102;&#24179;&#34913;&#27169;&#22411;&#35757;&#32451;&#21069;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#36890;&#24120;&#20351;&#29992;&#36807;&#37319;&#26679;&#26041;&#27861;&#20026;&#23569;&#25968;&#31867;&#29983;&#25104;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#20998;&#31867;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#32463;&#20856;&#30340;&#36807;&#37319;&#26679;&#26041;&#27861;&#22522;&#20110;SMOTE&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20165;&#20851;&#27880;&#25968;&#25454;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#22240;&#27492;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#33021;&#23384;&#22312;&#19981;&#22815;&#36924;&#30495;&#30340;&#38382;&#39064;&#12290;&#22312;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#30340;&#24403;&#21069;&#36807;&#37319;&#26679;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#33719;&#25968;&#25454;&#30340;&#30495;&#23454;&#20998;&#24067;&#65292;&#20294;&#35757;&#32451;&#20013;&#23384;&#22312;&#27169;&#24335;&#23849;&#28291;&#21644;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#65307;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#36807;&#37319;&#26679;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;U-Net&#30340;&#36870;&#25193;&#25955;&#36807;&#31243;&#31070;&#32463;&#32593;&#32476;&#19981;&#36866;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05918v1 Announce Type: cross  Abstract: In the field of data mining and machine learning, commonly used classification models cannot effectively learn in unbalanced data. In order to balance the data distribution before model training,oversamplingmethods are often used to generate data for a small number of classes to solve the problem of classifying unbalanced data. Most of the classical oversampling methods are based on theSMOTE technique, which only focuses on the local information of the data, and therefore the generated data may have the problem of not being realistic enough. In the current oversampling methods based on generative networks, the methods based on GANs can capture the true distribution of data, but there is the problem of pattern collapse and training instability in training; in the oversampling methods based on denoising diffusion probability models, the neural network of the inverse diffusion process using the U-Net is not applicable to tabular data, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;GPT4&#22312;&#35270;&#35273;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20854;&#22312;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#35782;&#21035;&#21644;&#24494;&#34920;&#24773;&#26816;&#27979;&#26041;&#38754;&#20934;&#30830;&#24615;&#39640;&#65292;&#20294;&#19968;&#33324;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#24615;&#33021;&#19981;&#20339;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;&#24494;&#34920;&#24773;&#35782;&#21035;&#30340;&#25361;&#25112;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05916</link><description>&lt;p&gt;
GPT&#20316;&#20026;&#24515;&#29702;&#23398;&#23478;&#65311;GPT-4V&#22312;&#35270;&#35273;&#24773;&#24863;&#35745;&#31639;&#19978;&#30340;&#21021;&#27493;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;GPT4&#22312;&#35270;&#35273;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20854;&#22312;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#35782;&#21035;&#21644;&#24494;&#34920;&#24773;&#26816;&#27979;&#26041;&#38754;&#20934;&#30830;&#24615;&#39640;&#65292;&#20294;&#19968;&#33324;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#24615;&#33021;&#19981;&#20339;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;&#24494;&#34920;&#24773;&#35782;&#21035;&#30340;&#25361;&#25112;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#26088;&#22312;&#22788;&#29702;&#21644;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#25991;&#26412;&#12289;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#12290;&#23613;&#31649;&#22312;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#23545;&#20110;&#26356;&#22909;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;MLMs&#22312;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#65292;&#36328;&#36234;&#35270;&#35273;&#24773;&#24863;&#20219;&#21153;&#21644;&#25512;&#29702;&#20219;&#21153;&#30340;5&#20010;&#20851;&#38190;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT4&#22312;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#35782;&#21035;&#21644;&#24494;&#34920;&#24773;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#65292;&#32780;&#20854;&#19968;&#33324;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#24615;&#33021;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#23454;&#29616;&#32454;&#31890;&#24230;&#24494;&#34920;&#24773;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#19982;&#20219;&#21153;&#30456;&#20851;&#20195;&#29702;&#32467;&#21512;&#23637;&#31034;&#20102;GPT4&#22788;&#29702;&#24773;&#24863;&#35782;&#21035;&#21644;&#30456;&#20851;&#39046;&#22495;&#39640;&#32423;&#20219;&#21153;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#28508;&#21147;&#65292;&#20197;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05916v1 Announce Type: cross  Abstract: Multimodal language models (MLMs) are designed to process and integrate information from multiple sources, such as text, speech, images, and videos. Despite its success in language understanding, it is critical to evaluate the performance of downstream tasks for better human-centric applications. This paper assesses the application of MLMs with 5 crucial abilities for affective computing, spanning from visual affective tasks and reasoning tasks. The results show that GPT4 has high accuracy in facial action unit recognition and micro-expression detection while its general facial expression recognition performance is not accurate. We also highlight the challenges of achieving fine-grained micro-expression recognition and the potential for further study and demonstrate the versatility and potential of GPT4 for handling advanced tasks in emotion recognition and related fields by integrating with task-related agents for more complex tasks, 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#20154;&#31867;&#20013;&#24515;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#36866;&#24403;&#31867;&#22411;&#30340;&#20915;&#31574;&#25903;&#25345;&#65292;&#38024;&#23545;&#29305;&#23450;&#20154;&#21592;&#12289;&#22312;&#36866;&#24403;&#26102;&#38388;&#65292;&#26469;&#20248;&#21270;&#20915;&#31574;&#20934;&#30830;&#24615;&#21644;&#20154;&#31867;&#23398;&#20064;&#33021;&#21147;&#36825;&#20004;&#20010;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.05911</link><description>&lt;p&gt;
&#20248;&#21270;&#20154;&#31867;&#20013;&#24515;&#30446;&#26631;&#65306;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Optimizing Human-Centric Objectives in AI-Assisted Decision-Making With Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#20154;&#31867;&#20013;&#24515;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#36866;&#24403;&#31867;&#22411;&#30340;&#20915;&#31574;&#25903;&#25345;&#65292;&#38024;&#23545;&#29305;&#23450;&#20154;&#21592;&#12289;&#22312;&#36866;&#24403;&#26102;&#38388;&#65292;&#26469;&#20248;&#21270;&#20915;&#31574;&#20934;&#30830;&#24615;&#21644;&#20154;&#31867;&#23398;&#20064;&#33021;&#21147;&#36825;&#20004;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#28183;&#36879;&#21040;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#20248;&#21270;&#20154;&#31867;&#20013;&#24515;&#30446;&#26631;&#65292;&#36229;&#36234;&#20915;&#31574;&#20934;&#30830;&#24615;&#65292;&#22914;&#37027;&#20123;&#19982;&#36825;&#20123;&#31995;&#32479;&#20114;&#21160;&#30340;&#20010;&#20307;&#30340;&#25216;&#33021;&#25552;&#21319;&#25110;&#20219;&#21153;&#20139;&#21463;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#24895;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20316;&#20026;&#24314;&#27169;&#20154;&#26426;&#20915;&#31574;&#20197;&#20248;&#21270;&#36825;&#20123;&#20154;&#31867;&#20013;&#24515;&#30446;&#26631;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#28789;&#27963;&#22320;&#20026;&#20154;&#31867;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#26469;&#20248;&#21270;&#19981;&#21516;&#30340;&#30446;&#26631;--&#22312;&#27491;&#30830;&#30340;&#26102;&#38388;&#12289;&#21521;&#27491;&#30830;&#30340;&#20154;&#25552;&#20379;&#27491;&#30830;&#31867;&#22411;&#30340;&#24110;&#21161;&#12290;&#25105;&#20204;&#29992;&#20004;&#20010;&#30446;&#26631;&#23454;&#20363;&#21270;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#20154;&#24037;&#26234;&#33021;&#22312;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#20154;&#31867;&#23545;&#35813;&#20219;&#21153;&#30340;&#23398;&#20064;&#65292;&#24182;&#20174;&#20808;&#21069;&#30340;&#20154;&#26426;&#20132;&#20114;&#25968;&#25454;&#20013;&#23398;&#20064;&#20248;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#20248;&#21270;&#30340;&#31574;&#30053;&#19982;&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#30340;&#21508;&#31181;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#65288;N = 316 &#21644; N
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05911v1 Announce Type: cross  Abstract: As AI assistance is increasingly infused into decision-making processes, we may seek to optimize human-centric objectives beyond decision accuracy, such as skill improvement or task enjoyment of individuals interacting with these systems. With this aspiration in mind, we propose offline reinforcement learning (RL) as a general approach for modeling human-AI decision-making to optimize such human-centric objectives. Our approach seeks to optimize different objectives by adaptively providing decision support to humans -- the right type of assistance, to the right person, at the right time. We instantiate our approach with two objectives: human-AI accuracy on the decision-making task and human learning about the task, and learn policies that optimize these two objectives from previous human-AI interaction data. We compare the optimized policies against various baselines in AI-assisted decision-making. Across two experiments (N = 316 and N
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#26415;&#23398;&#20064;&#31574;&#30053;&#65292;&#37325;&#28857;&#32771;&#34385;&#26368;&#20302;&#26377;&#25928;&#25968;&#23383;&#30340;&#36755;&#20986;&#65292;&#37325;&#26032;&#35780;&#20272;&#25968;&#23383;&#39034;&#24207;&#65292;&#24182;&#32467;&#21512;&#36880;&#27493;&#26041;&#27861;&#22823;&#24133;&#20943;&#23569;&#20102;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.05845</link><description>&lt;p&gt;
&#25968;&#23383;&#21453;&#36716;&#65281;&#31639;&#26415;&#23398;&#20064;&#20013;&#39034;&#24207;&#35299;&#30721;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Reverse That Number! Decoding Order Matters in Arithmetic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#26415;&#23398;&#20064;&#31574;&#30053;&#65292;&#37325;&#28857;&#32771;&#34385;&#26368;&#20302;&#26377;&#25928;&#25968;&#23383;&#30340;&#36755;&#20986;&#65292;&#37325;&#26032;&#35780;&#20272;&#25968;&#23383;&#39034;&#24207;&#65292;&#24182;&#32467;&#21512;&#36880;&#27493;&#26041;&#27861;&#22823;&#24133;&#20943;&#23569;&#20102;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#39044;&#35757;&#32451;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#26377;&#25928;&#23398;&#20064;&#31639;&#26415;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25215;&#35748;&#25968;&#23383;&#39034;&#24207;&#22312;&#31639;&#26415;&#35745;&#31639;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#39034;&#24207;&#12289;&#36880;&#27493;&#30340;&#26041;&#27861;&#26469;&#25945;&#25480;LLMs&#31639;&#26415;&#65292;&#23548;&#33268;&#32467;&#35770;&#26159;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#28041;&#21450;&#21040;&#31934;&#32454;&#30340;&#36880;&#27493;&#25805;&#20316;&#12290;&#19982;&#20256;&#32479;&#36335;&#24452;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#19981;&#20165;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#20174;&#26368;&#20302;&#26377;&#25928;&#25968;&#23383;&#36755;&#20986;&#26469;&#37325;&#26032;&#35780;&#20272;&#25968;&#23383;&#39034;&#24207;&#65292;&#36824;&#32467;&#21512;&#36880;&#27493;&#26041;&#27861;&#22823;&#24133;&#20943;&#23569;&#20102;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#24050;&#32463;&#22312;&#20840;&#38754;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#20013;&#24320;&#21457;&#24182;&#24212;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#24635;&#20307;&#20934;&#30830;&#24230;&#30340;&#25552;&#39640;&#65292;&#21516;&#26102;&#20165;&#38656;&#35201;&#19977;&#20998;&#20043;&#19968;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05845v1 Announce Type: cross  Abstract: Recent advancements in pretraining have demonstrated that modern Large Language Models (LLMs) possess the capability to effectively learn arithmetic operations. However, despite acknowledging the significance of digit order in arithmetic computation, current methodologies predominantly rely on sequential, step-by-step approaches for teaching LLMs arithmetic, resulting in a conclusion where obtaining better performance involves fine-grained step-by-step. Diverging from this conventional path, our work introduces a novel strategy that not only reevaluates the digit order by prioritizing output from the least significant digit but also incorporates a step-by-step methodology to substantially reduce complexity. We have developed and applied this method in a comprehensive set of experiments. Compared to the previous state-of-the-art (SOTA) method, our findings reveal an overall improvement of in accuracy while requiring only a third of the 
&lt;/p&gt;</description></item><item><title>Hufu&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;&#65292;&#21033;&#29992;Transformer&#30340;&#32622;&#25442;&#31561;&#21464;&#24615;&#36136;&#65292;&#23454;&#29616;&#20102;&#22312;&#27169;&#22411;&#20013;&#23884;&#20837;&#27700;&#21360;&#24182;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.05842</link><description>&lt;p&gt;
Hufu&#65306;&#19968;&#31181;&#36890;&#36807;&#32622;&#25442;&#31561;&#21464;&#24615;&#23545;&#39044;&#35757;&#32451;&#30340;Transformer&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Hufu: A Modality-Agnositc Watermarking System for Pre-Trained Transformers via Permutation Equivariance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05842
&lt;/p&gt;
&lt;p&gt;
Hufu&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;&#65292;&#21033;&#29992;Transformer&#30340;&#32622;&#25442;&#31561;&#21464;&#24615;&#36136;&#65292;&#23454;&#29616;&#20102;&#22312;&#27169;&#22411;&#20013;&#23884;&#20837;&#27700;&#21360;&#24182;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#26381;&#21153;&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#20445;&#25252;&#23453;&#36149;&#30340;&#27169;&#22411;&#21442;&#25968;&#20813;&#21463;&#30423;&#31363;&#24050;&#25104;&#20026;&#19968;&#39033;&#36843;&#20999;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#27700;&#21360;&#25216;&#26415;&#34987;&#35748;&#20026;&#26159;&#25152;&#26377;&#26435;&#39564;&#35777;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#26041;&#26696;&#38024;&#23545;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#23450;&#21046;&#65292;&#38590;&#20197;&#20316;&#20026;&#38598;&#25104;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26381;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Hufu&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;&#65292;&#20381;&#36182;&#20110;Transformer&#30340;&#32622;&#25442;&#31561;&#21464;&#24615;&#36136;&#12290;Hufu&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#32622;&#25442;&#30340;&#19968;&#32452;&#25968;&#25454;&#26679;&#26412;&#19978;&#23884;&#20837;&#27700;&#21360;&#65292;&#23884;&#20837;&#30340;&#27169;&#22411;&#22522;&#26412;&#19978;&#21253;&#21547;&#20004;&#32452;&#26435;&#37325; -- &#19968;&#32452;&#29992;&#20110;&#27491;&#24120;&#20351;&#29992;&#65292;&#21478;&#19968;&#32452;&#29992;&#20110;&#27700;&#21360;&#25552;&#21462;&#65292;&#35302;&#21457;&#26465;&#20214;&#26159;&#32463;&#36807;&#32622;&#25442;&#30340;&#36755;&#20837;&#12290;&#32622;&#25442;&#31561;&#21464;&#24615;&#30830;&#20445;&#36825;&#20004;&#32452;&#27169;&#22411;&#26435;&#37325;&#20043;&#38388;&#30340;&#26368;&#23567;&#24178;&#25200;&#65292;&#20174;&#32780;&#22312;&#27700;&#21360;&#25552;&#21462;&#26102;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05842v1 Announce Type: cross  Abstract: With the blossom of deep learning models and services, it has become an imperative concern to safeguard the valuable model parameters from being stolen. Watermarking is considered an important tool for ownership verification. However, current watermarking schemes are customized for different models and tasks, hard to be integrated as an integrated intellectual protection service. We propose Hufu, a modality-agnostic watermarking system for pre-trained Transformer-based models, relying on the permutation equivariance property of Transformers. Hufu embeds watermark by fine-tuning the pre-trained model on a set of data samples specifically permuted, and the embedded model essentially contains two sets of weights -- one for normal use and the other for watermark extraction which is triggered on permuted inputs. The permutation equivariance ensures minimal interference between these two sets of model weights and thus high fidelity on downst
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38271;&#26399;&#21644;&#22823;&#35268;&#27169;&#30340;&#24103;&#20107;&#20214;&#21333;&#30446;&#26631;&#36319;&#36394;&#25968;&#25454;&#38598;FELT&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;15&#20010;&#22522;&#20934;&#36319;&#36394;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;&#20851;&#32852;&#35760;&#24518;Transformer&#32593;&#32476;&#26469;&#35299;&#20915;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05839</link><description>&lt;p&gt;
&#38271;&#26399;&#24103;&#20107;&#20214;&#35270;&#35273;&#36319;&#36394;&#65306;&#22522;&#20934;&#25968;&#25454;&#38598;&#19982;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05839
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38271;&#26399;&#21644;&#22823;&#35268;&#27169;&#30340;&#24103;&#20107;&#20214;&#21333;&#30446;&#26631;&#36319;&#36394;&#25968;&#25454;&#38598;FELT&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;15&#20010;&#22522;&#20934;&#36319;&#36394;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;&#20851;&#32852;&#35760;&#24518;Transformer&#32593;&#32476;&#26469;&#35299;&#20915;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22522;&#20110;&#20107;&#20214;/&#24103;&#20107;&#20214;&#30340;&#36319;&#36394;&#22120;&#22312;&#30701;&#26399;&#36319;&#36394;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#28982;&#32780;&#65292;&#23545;&#20110;&#30495;&#23454;&#22330;&#26223;&#30340;&#36319;&#36394;&#28041;&#21450;&#38271;&#26399;&#36319;&#36394;&#65292;&#29616;&#26377;&#36319;&#36394;&#31639;&#27861;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38271;&#26399;&#21644;&#22823;&#35268;&#27169;&#30340;&#24103;&#20107;&#20214;&#21333;&#30446;&#26631;&#36319;&#36394;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;FELT&#12290;&#23427;&#21253;&#21547;742&#20010;&#35270;&#39057;&#21644;1,594,474&#20010;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#23545;&#65292;&#24182;&#24050;&#25104;&#20026;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#24103;&#20107;&#20214;&#36319;&#36394;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37325;&#26032;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;15&#20010;&#22522;&#20934;&#36319;&#36394;&#22120;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#20197;&#20379;&#26410;&#26469;&#30740;&#31350;&#36827;&#34892;&#27604;&#36739;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#30001;&#20110;&#25361;&#25112;&#22240;&#32032;&#30340;&#24433;&#21709;&#21644;&#31354;&#38388;&#31232;&#30095;&#30340;&#20107;&#20214;&#27969;&#32780;&#33258;&#28982;&#19981;&#23436;&#25972;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#32852;&#35760;&#24518;Transformer&#32593;&#32476;&#20316;&#20026;&#32479;&#19968;&#39592;&#24178;&#65292;&#36890;&#36807;&#23558;&#29616;&#20195;Hopfield&#23618;&#24341;&#20837;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#26469;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05839v1 Announce Type: cross  Abstract: Current event-/frame-event based trackers undergo evaluation on short-term tracking datasets, however, the tracking of real-world scenarios involves long-term tracking, and the performance of existing tracking algorithms in these scenarios remains unclear. In this paper, we first propose a new long-term and large-scale frame-event single object tracking dataset, termed FELT. It contains 742 videos and 1,594,474 RGB frames and event stream pairs and has become the largest frame-event tracking dataset to date. We re-train and evaluate 15 baseline trackers on our dataset for future works to compare. More importantly, we find that the RGB frames and event streams are naturally incomplete due to the influence of challenging factors and spatially sparse event flow. In response to this, we propose a novel associative memory Transformer network as a unified backbone by introducing modern Hopfield layers into multi-head self-attention blocks to
&lt;/p&gt;</description></item><item><title>&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#24037;&#20316;&#27969;&#26550;&#26500;QCQ&#23454;&#29616;&#20102;&#37327;&#23376;&#27169;&#25311;&#20013;&#30340;&#21019;&#26032;&#65292;&#36890;&#36807;&#22312;QPUs&#19978;&#36816;&#34892;VQE&#31639;&#27861;&#21644;&#22312;&#32463;&#20856;&#30828;&#20214;&#19978;&#36827;&#34892;&#37327;&#23376;&#24577;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;cuQuantum SDK&#21644;PennyLane's Lightning plugin&#65292;&#20026;&#26448;&#26009;&#21644;&#20957;&#32858;&#24577;&#29289;&#29702;&#39046;&#22495;&#30340;&#35745;&#31639;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.05828</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;GPU&#21551;&#29992;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#24037;&#20316;&#27969;&#30340;Quantum-HPC&#26694;&#26550;&#65306;&#22312;&#37327;&#23376;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantum-HPC Framework with multi-GPU-Enabled Hybrid Quantum-Classical Workflow: Applications in Quantum Simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05828
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#24037;&#20316;&#27969;&#26550;&#26500;QCQ&#23454;&#29616;&#20102;&#37327;&#23376;&#27169;&#25311;&#20013;&#30340;&#21019;&#26032;&#65292;&#36890;&#36807;&#22312;QPUs&#19978;&#36816;&#34892;VQE&#31639;&#27861;&#21644;&#22312;&#32463;&#20856;&#30828;&#20214;&#19978;&#36827;&#34892;&#37327;&#23376;&#24577;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;cuQuantum SDK&#21644;PennyLane's Lightning plugin&#65292;&#20026;&#26448;&#26009;&#21644;&#20957;&#32858;&#24577;&#29289;&#29702;&#39046;&#22495;&#30340;&#35745;&#31639;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#23376;&#31995;&#32479;&#19978;&#23454;&#29616;&#39640;&#24615;&#33021;&#35745;&#31639;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#24357;&#21512;&#37327;&#23376;&#30828;&#20214;&#21644;&#32463;&#20856;&#35745;&#31639;&#36164;&#28304;&#20043;&#38388;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#24067;&#24863;&#30693;&#30340;Quantum-Classical-Quantum (QCQ)&#26550;&#26500;&#65292;&#23558;&#21069;&#27839;&#30340;&#37327;&#23376;&#36719;&#20214;&#26694;&#26550;&#19982;&#39640;&#24615;&#33021;&#32463;&#20856;&#35745;&#31639;&#36164;&#28304;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#35299;&#20915;&#26448;&#26009;&#21644;&#20957;&#32858;&#24577;&#29289;&#29702;&#39046;&#22495;&#30340;&#37327;&#23376;&#27169;&#25311;&#25361;&#25112;&#12290;&#35813;&#26550;&#26500;&#30340;&#26680;&#24515;&#26159;&#22312;QPUs&#19978;&#36816;&#34892;VQE&#31639;&#27861;&#23454;&#29616;&#39640;&#25928;&#37327;&#23376;&#24577;&#20934;&#22791;&#65292;&#22312;&#32463;&#20856;&#30828;&#20214;&#19978;&#36827;&#34892;&#37327;&#23376;&#24577;&#20998;&#31867;&#30340;Tensor Network states&#21644;QCNNs&#30340;&#26080;&#32541;&#38598;&#25104;&#12290; &#20026;&#20102;&#23545;&#37327;&#23376;&#27169;&#25311;&#22120;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;QCQ&#26550;&#26500;&#21033;&#29992;cuQuantum SDK&#21033;&#29992;&#22810;GPU&#21152;&#36895;&#65292;&#38598;&#25104;&#20102;PennyLane&#30340;Lightning&#25554;&#20214;&#65292;&#23637;&#31034;&#20102;&#35745;&#31639;&#36895;&#24230;&#22686;&#21152;&#22810;&#36798;&#21313;&#20493;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05828v1 Announce Type: cross  Abstract: Achieving high-performance computation on quantum systems presents a formidable challenge that necessitates bridging the capabilities between quantum hardware and classical computing resources. This study introduces an innovative distribution-aware Quantum-Classical-Quantum (QCQ) architecture, which integrates cutting-edge quantum software framework works with high-performance classical computing resources to address challenges in quantum simulation for materials and condensed matter physics. At the heart of this architecture is the seamless integration of VQE algorithms running on QPUs for efficient quantum state preparation, Tensor Network states, and QCNNs for classifying quantum states on classical hardware.   For benchmarking quantum simulators, the QCQ architecture utilizes the cuQuantum SDK to leverage multi-GPU acceleration, integrated with PennyLane's Lightning plugin, demonstrating up to tenfold increases in computational spe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#21160;&#21270;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;MP2D&#65292;&#36890;&#36807;&#26144;&#23556;&#23545;&#35805;&#20013;&#35805;&#39064;&#30340;&#27969;&#21160;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#20154;&#31867;&#23545;&#35805;&#30340;&#21160;&#24577;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#20855;&#26377;&#33258;&#28982;&#35805;&#39064;&#36716;&#25442;&#30340;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05814</link><description>&lt;p&gt;
MP2D:&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#21160;&#35805;&#39064;&#36716;&#31227;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05814
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#21160;&#21270;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;MP2D&#65292;&#36890;&#36807;&#26144;&#23556;&#23545;&#35805;&#20013;&#35805;&#39064;&#30340;&#27969;&#21160;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#20154;&#31867;&#23545;&#35805;&#30340;&#21160;&#24577;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#20855;&#26377;&#33258;&#28982;&#35805;&#39064;&#36716;&#25442;&#30340;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38754;&#21521;&#29305;&#23450;&#35805;&#39064;&#30340;&#23545;&#35805;&#31995;&#32479;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#23545;&#35805;&#20013;&#26377;&#25928;&#22320;&#31649;&#29702;&#35805;&#39064;&#36716;&#31227;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#25361;&#25112;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multi-Passage to Dialogue (MP2D)&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#21019;&#24314;&#20855;&#26377;&#33258;&#28982;&#35805;&#39064;&#36716;&#25442;&#30340;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;MP2D&#26144;&#23556;&#23545;&#35805;&#20013;&#35805;&#39064;&#30340;&#27969;&#21160;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#30340;&#21160;&#24577;&#12290;&#23427;&#26816;&#32034;&#19982;&#35805;&#39064;&#23545;&#24212;&#30340;&#30456;&#20851;&#27573;&#33853;&#65292;&#24182;&#36890;&#36807;&#27573;&#33853;&#21040;&#23545;&#35805;&#30340;&#26041;&#27861;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#23545;&#35805;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MP2D&#22312;&#29983;&#25104;&#20855;&#26377;&#33258;&#28982;&#35805;&#39064;&#36716;&#25442;&#30340;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35805;&#39064;&#36716;&#31227;&#23545;&#35805;&#22522;&#20934;&#65292;TS-WikiDialog&#12290;&#21033;&#29992;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05814v1 Announce Type: cross  Abstract: Despite advancements in on-topic dialogue systems, effectively managing topic shifts within dialogues remains a persistent challenge, largely attributed to the limited availability of training datasets. To address this issue, we propose Multi-Passage to Dialogue (MP2D), a data generation framework that automatically creates conversational question-answering datasets with natural topic transitions. By leveraging the relationships between entities in a knowledge graph, MP2D maps the flow of topics within a dialogue, effectively mirroring the dynamics of human conversation. It retrieves relevant passages corresponding to the topics and transforms them into dialogues through the passage-to-dialogue method. Through quantitative and qualitative experiments, we demonstrate MP2D's efficacy in generating dialogue with natural topic shifts. Furthermore, this study introduces a novel benchmark for topic shift dialogues, TS-WikiDialog. Utilizing t
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#31639;&#27861;&#27599;8&#20010;&#26376;&#20960;&#20046;&#20943;&#21322;&#19968;&#27425;&#25152;&#38656;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#22823;&#22823;&#24555;&#20110;&#25705;&#23572;&#23450;&#24459;&#30340;&#30828;&#20214;&#22686;&#30410;&#65292;&#23613;&#31649;&#31639;&#27861;&#36827;&#23637;&#36895;&#24230;&#24555;&#65292;&#20294;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#23545;&#25972;&#20307;&#24615;&#33021;&#25913;&#21892;&#30340;&#36129;&#29486;&#26356;&#22823;&#12290;</title><link>https://arxiv.org/abs/2403.05812</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#31639;&#27861;&#36827;&#23637;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Algorithmic progress in language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05812
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#31639;&#27861;&#27599;8&#20010;&#26376;&#20960;&#20046;&#20943;&#21322;&#19968;&#27425;&#25152;&#38656;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#22823;&#22823;&#24555;&#20110;&#25705;&#23572;&#23450;&#24459;&#30340;&#30828;&#20214;&#22686;&#30410;&#65292;&#23613;&#31649;&#31639;&#27861;&#36827;&#23637;&#36895;&#24230;&#24555;&#65292;&#20294;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#23545;&#25972;&#20307;&#24615;&#33021;&#25913;&#21892;&#30340;&#36129;&#29486;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#33258;&#28145;&#24230;&#23398;&#20064;&#38382;&#19990;&#20197;&#26469;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#25913;&#21892;&#36895;&#24230;&#12290;&#20351;&#29992;&#36328;&#36234;2012&#24180;&#33267;2023&#24180;&#30340;Wikitext&#21644;Penn Treebank&#19978;&#30340;200&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#36798;&#21040;&#19968;&#23450;&#24615;&#33021;&#38408;&#20540;&#25152;&#38656;&#30340;&#35745;&#31639;&#26102;&#38388;&#22823;&#32422;&#27599;8&#20010;&#26376;&#20943;&#21322;&#65292;95%&#30340;&#32622;&#20449;&#21306;&#38388;&#32422;&#20026;5&#33267;14&#20010;&#26376;&#65292;&#36828;&#36828;&#24555;&#20110;&#25705;&#23572;&#23450;&#24459;&#30340;&#30828;&#20214;&#22686;&#30410;&#12290;&#25105;&#20204;&#20272;&#35745;&#20102;&#22686;&#24378;&#25193;&#23637;&#35268;&#24459;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#30830;&#23450;&#27169;&#22411;&#25193;&#23637;&#19982;&#35757;&#32451;&#31639;&#27861;&#21019;&#26032;&#30340;&#30456;&#23545;&#36129;&#29486;&#12290;&#23613;&#31649;&#31639;&#27861;&#36827;&#23637;&#36895;&#24230;&#24555;&#65292;&#19988;&#20986;&#29616;&#26032;&#30340;&#26550;&#26500;&#22914;Transformer&#65292;&#20294;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#22312;&#36825;&#27573;&#26102;&#38388;&#20869;&#65292;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#23545;&#25972;&#20307;&#24615;&#33021;&#25913;&#21892;&#30340;&#36129;&#29486;&#26356;&#22823;&#12290;&#23613;&#31649;&#21463;&#21040;&#22024;&#26434;&#30340;&#22522;&#20934;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#31639;&#27861;&#30340;&#36827;&#23637;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#30340;&#22686;&#38271;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05812v1 Announce Type: cross  Abstract: We investigate the rate at which algorithms for pre-training language models have improved since the advent of deep learning. Using a dataset of over 200 language model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we find that the compute required to reach a set performance threshold has halved approximately every 8 months, with a 95% confidence interval of around 5 to 14 months, substantially faster than hardware gains per Moore's Law. We estimate augmented scaling laws, which enable us to quantify algorithmic progress and determine the relative contributions of scaling models versus innovations in training algorithms. Despite the rapid pace of algorithmic progress and the development of new architectures such as the transformer, our analysis reveals that the increase in compute made an even larger contribution to overall performance improvements over this time period. Though limited by noisy benchmark data, our analy
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#24490;&#29615;&#23545;&#40784;&#32593;&#32476;&#65288;RAN&#65289;&#26469;&#26368;&#23567;&#21270;&#39046;&#22495;&#24046;&#24322;&#65292;&#36890;&#36807;&#24490;&#29615;&#23545;&#40784;&#31574;&#30053;&#26377;&#25928;&#22320;&#22312;&#26102;&#38388;-&#29366;&#24577;&#21644;&#26102;&#38388;-&#24207;&#21015;&#32423;&#21035;&#23545;&#40784;&#36712;&#36857;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#24191;&#20041;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.05810</link><description>&lt;p&gt;
&#29992;&#20110;&#24191;&#20041;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#30340;&#24490;&#29615;&#23545;&#40784;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Recurrent Aligned Network for Generalized Pedestrian Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05810
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#24490;&#29615;&#23545;&#40784;&#32593;&#32476;&#65288;RAN&#65289;&#26469;&#26368;&#23567;&#21270;&#39046;&#22495;&#24046;&#24322;&#65292;&#36890;&#36807;&#24490;&#29615;&#23545;&#40784;&#31574;&#30053;&#26377;&#25928;&#22320;&#22312;&#26102;&#38388;-&#29366;&#24577;&#21644;&#26102;&#38388;-&#24207;&#21015;&#32423;&#21035;&#23545;&#40784;&#36712;&#36857;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#24191;&#20041;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#26159;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#32780;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#30446;&#26631;&#39046;&#22495;&#30340;&#37096;&#20998;&#36712;&#36857;&#25968;&#25454;&#26469;&#35843;&#25972;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#36825;&#20123;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#19981;&#22826;&#21487;&#33021;&#20174;&#25152;&#26377;&#28508;&#22312;&#30340;&#30446;&#26631;&#39046;&#22495;&#25910;&#38598;&#36712;&#36857;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#39033;&#21517;&#20026;&#24191;&#20041;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#27169;&#22411;&#25512;&#24191;&#21040;&#30475;&#19981;&#35265;&#30340;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#23427;&#20204;&#30340;&#36712;&#36857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24490;&#29615;&#23545;&#40784;&#32593;&#32476;&#65288;RAN&#65289;&#26469;&#36890;&#36807;&#39046;&#22495;&#23545;&#40784;&#26469;&#26368;&#23567;&#21270;&#39046;&#22495;&#24046;&#24322;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24490;&#29615;&#23545;&#40784;&#27169;&#22359;&#65292;&#36890;&#36807;&#24490;&#29615;&#23545;&#40784;&#31574;&#30053;&#26377;&#25928;&#22320;&#22312;&#26102;&#38388;-&#29366;&#24577;&#21644;&#26102;&#38388;-&#24207;&#21015;&#32423;&#21035;&#23545;&#40784;&#36712;&#36857;&#29305;&#24449;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05810v1 Announce Type: cross  Abstract: Pedestrian trajectory prediction is a crucial component in computer vision and robotics, but remains challenging due to the domain shift problem. Previous studies have tried to tackle this problem by leveraging a portion of the trajectory data from the target domain to adapt the model. However, such domain adaptation methods are impractical in real-world scenarios, as it is infeasible to collect trajectory data from all potential target domains. In this paper, we study a task named generalized pedestrian trajectory prediction, with the aim of generalizing the model to unseen domains without accessing their trajectories. To tackle this task, we introduce a Recurrent Aligned Network~(RAN) to minimize the domain gap through domain alignment. Specifically, we devise a recurrent alignment module to effectively align the trajectory feature spaces at both time-state and time-sequence levels by the recurrent alignment strategy.Furthermore, we 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;BERT&#23884;&#20837;&#20197;&#21450;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#22312;&#22810;&#36339;&#30693;&#35782;&#22270;&#25512;&#29702;&#20013;&#22788;&#29702;&#30693;&#35782;&#22270;&#30340;&#22266;&#26377;&#19981;&#23436;&#25972;&#24615;&#65292;&#25552;&#39640;&#20102;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.05801</link><description>&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#22686;&#24378;&#22810;&#36339;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-Hop Knowledge Graph Reasoning through Reward Shaping Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05801
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;BERT&#23884;&#20837;&#20197;&#21450;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#22312;&#22810;&#36339;&#30693;&#35782;&#22270;&#25512;&#29702;&#20013;&#22788;&#29702;&#30693;&#35782;&#22270;&#30340;&#22266;&#26377;&#19981;&#23436;&#25972;&#24615;&#65292;&#25552;&#39640;&#20102;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#30693;&#35782;&#34920;&#31034;&#39046;&#22495;&#65292;&#30693;&#35782;&#22270;&#25512;&#29702;&#65288;KG-R&#65289;&#22788;&#20110;&#20419;&#36827;&#21508;&#20010;&#39046;&#22495;&#20043;&#38388;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#30340;&#21069;&#27839;&#12290;&#26412;&#30740;&#31350;&#30340;&#35201;&#28857;&#22312;&#20110;&#38416;&#26126;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31574;&#30053;&#65292;&#29305;&#21035;&#26159;REINFORCE&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#36339;KG-R&#20013;&#22266;&#26377;&#22797;&#26434;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#20026;&#20016;&#23500;&#21644;&#31232;&#30095;&#23376;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;BERT&#23884;&#20837;&#21644;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#23545;&#25913;&#36827;&#22870;&#21169;&#22609;&#36896;&#36807;&#31243;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#22810;&#36339;KG-R&#30340;&#31934;&#24230;&#65292;&#36824;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05801v1 Announce Type: new  Abstract: In the realm of computational knowledge representation, Knowledge Graph Reasoning (KG-R) stands at the forefront of facilitating sophisticated inferential capabilities across multifarious domains. The quintessence of this research elucidates the employment of reinforcement learning (RL) strategies, notably the REINFORCE algorithm, to navigate the intricacies inherent in multi-hop KG-R. This investigation critically addresses the prevalent challenges introduced by the inherent incompleteness of Knowledge Graphs (KGs), which frequently results in erroneous inferential outcomes, manifesting as both false negatives and misleading positives. By partitioning the Unified Medical Language System (UMLS) benchmark dataset into rich and sparse subsets, we investigate the efficacy of pre-trained BERT embeddings and Prompt Learning methodologies to refine the reward shaping process. This approach not only enhances the precision of multi-hop KG-R but 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;HE-Diffusion&#65292;&#36890;&#36807;&#26368;&#23567;&#22833;&#30495;&#26041;&#27861;&#21644;&#31232;&#30095;&#24352;&#37327;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;500&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.05794</link><description>&lt;p&gt;
&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Diffusion Model Using Homomorphic Encryption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;HE-Diffusion&#65292;&#36890;&#36807;&#26368;&#23567;&#22833;&#30495;&#26041;&#27861;&#21644;&#31232;&#30095;&#24352;&#37327;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;500&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#31283;&#23450;&#25193;&#25955;&#26694;&#26550;&#65292;&#31216;&#20026;HE-Diffusion&#65292;&#20027;&#35201;&#20851;&#27880;&#20110;&#20445;&#25252;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#21435;&#22122;&#38454;&#27573;&#12290;HE-Diffusion&#26159;&#19968;&#20010;&#37327;&#36523;&#23450;&#21046;&#30340;&#21152;&#23494;&#26694;&#26550;&#65292;&#19987;&#38376;&#35774;&#35745;&#20197;&#19982;&#31283;&#23450;&#25193;&#25955;&#30340;&#29420;&#29305;&#26550;&#26500;&#30456;&#21305;&#37197;&#65292;&#30830;&#20445;&#38544;&#31169;&#21644;&#21151;&#33021;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#22266;&#26377;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#23567;&#22833;&#30495;&#26041;&#27861;&#65292;&#20351;&#24471;&#37096;&#20998;&#22270;&#20687;&#21152;&#23494;&#26356;&#21152;&#39640;&#25928;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24320;&#38144;&#32780;&#19981;&#25439;&#23475;&#27169;&#22411;&#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31232;&#30095;&#24352;&#37327;&#34920;&#31034;&#26469;&#21152;&#36895;&#35745;&#31639;&#25805;&#20316;&#65292;&#25552;&#39640;&#20102;&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#36807;&#31243;&#30340;&#25972;&#20307;&#25928;&#29575;&#12290;&#25105;&#20204;&#25104;&#21151;&#23454;&#29616;&#20102;&#22522;&#20110;HE&#30340;&#38544;&#31169;&#20445;&#25252;&#31283;&#23450;&#25193;&#25955;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HE-Diffusion&#30456;&#27604;&#20043;&#19979;&#23454;&#29616;&#20102;500&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05794v1 Announce Type: cross  Abstract: In this paper, we introduce a privacy-preserving stable diffusion framework leveraging homomorphic encryption, called HE-Diffusion, which primarily focuses on protecting the denoising phase of the diffusion process. HE-Diffusion is a tailored encryption framework specifically designed to align with the unique architecture of stable diffusion, ensuring both privacy and functionality. To address the inherent computational challenges, we propose a novel min-distortion method that enables efficient partial image encryption, significantly reducing the overhead without compromising the model's output quality. Furthermore, we adopt a sparse tensor representation to expedite computational operations, enhancing the overall efficiency of the privacy-preserving diffusion process. We successfully implement HE-based privacy-preserving stable diffusion inference. The experimental results show that HE-Diffusion achieves 500 times speedup compared wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#28436;&#32462;&#36890;&#36807;&#24402;&#32435;&#65288;ItD&#65289;&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#28436;&#32462;&#33258;&#23398;&#24402;&#32435;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24402;&#32435;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05789</link><description>&lt;p&gt;
ItD&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#28436;&#32462;&#33258;&#23398;&#24402;&#32435;
&lt;/p&gt;
&lt;p&gt;
ItD: Large Language Models Can Teach Themselves Induction through Deduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#28436;&#32462;&#36890;&#36807;&#24402;&#32435;&#65288;ItD&#65289;&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#28436;&#32462;&#33258;&#23398;&#24402;&#32435;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24402;&#32435;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#23427;&#20204;&#22312;&#36827;&#34892;&#24402;&#32435;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697;&#20027;&#35201;&#37319;&#29992;&#8220;&#21518;&#22788;&#29702;&#8221;&#33539;&#24335;&#26469;&#25552;&#39640;LLMs&#22312;&#24402;&#32435;&#26041;&#38754;&#30340;&#34920;&#29616;&#65288;&#22914;&#20551;&#35774;&#25628;&#32034;&#21644;&#32454;&#21270;&#26041;&#27861;&#65289;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#21463;&#38480;&#20110;LLMs&#30340;&#22266;&#26377;&#24402;&#32435;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#28436;&#32462;&#36890;&#36807;&#24402;&#32435;&#65288;ItD&#65289;&#65292;&#20197;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#28436;&#32462;&#33258;&#23398;&#24402;&#32435;&#12290;ItD&#26694;&#26550;&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#28436;&#32462;&#25968;&#25454;&#29983;&#25104;&#27169;&#22359;&#29992;&#20110;&#29983;&#25104;&#24402;&#32435;&#25968;&#25454;&#65292;&#20197;&#21450;&#26420;&#32032;&#36125;&#21494;&#26031;&#24402;&#32435;&#27169;&#22359;&#29992;&#20110;&#20248;&#21270;LLMs&#30340;&#24494;&#35843;&#21644;&#35299;&#30721;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;ItD&#22312;&#20004;&#20010;&#24402;&#32435;&#22522;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#24615;&#33021;&#25552;&#21319;&#20998;&#21035;&#20026;36%&#21644;1%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05789v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt ``post processes'' paradigms to improve the performance of LLMs on induction (e.g., the hypothesis search &amp; refinement methods), but their performance is still constrained by the inherent inductive capability of the LLMs. In this paper, we propose a novel framework, Induction through Deduction (ItD), to enable the LLMs to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the fine-tuning and decoding of LLMs. Our empirical results showcase the effectiveness of ItD on two induction benchmarks, achieving relative performance improvement of 36% and 1
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32454;&#31890;&#24230;NLL&#25439;&#22833;&#21644;fi&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#20107;&#23454;&#24615;&#65292;&#25913;&#36827;&#20102;&#32454;&#31890;&#24230;&#25439;&#22833;&#25130;&#26029;&#23545;&#20110;&#25688;&#35201;&#20013;&#20107;&#23454;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.05788</link><description>&lt;p&gt;
&#23545;&#32454;&#31890;&#24230;&#25439;&#22833;&#25130;&#26029;&#25928;&#30410;&#30340;&#30740;&#31350;&#65306;&#20197;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#24615;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
On the Benefits of Fine-Grained Loss Truncation: A Case Study on Factuality in Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05788
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;NLL&#25439;&#22833;&#21644;fi&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#20107;&#23454;&#24615;&#65292;&#25913;&#36827;&#20102;&#32454;&#31890;&#24230;&#25439;&#22833;&#25130;&#26029;&#23545;&#20110;&#25688;&#35201;&#20013;&#20107;&#23454;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05788v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39033; &#25688;&#35201;&#65306;&#25991;&#26412;&#25688;&#35201;&#21644;&#31616;&#21270;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#36825;&#20123;&#20219;&#21153;&#24320;&#21457;&#30340;&#27169;&#22411;&#24448;&#24448;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#22312;&#26410;&#23545;&#40784;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#26159;&#25439;&#22833;&#25130;&#26029;&#65288;LT&#65289;&#65288;Kang&#21644;Hashimoto&#65292;2020&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20462;&#25913;&#26631;&#20934;&#23545;&#25968;&#25439;&#22833;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#21435;&#38500;&#22024;&#26434;&#31034;&#20363;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20351;&#29992;LT&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20250;&#20135;&#29983;&#22823;&#37327;&#24187;&#35273;&#23454;&#20307;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20107;&#23454;&#21644;&#38750;&#20107;&#23454;&#31034;&#20363;&#20043;&#38388;&#22522;&#30784;&#25439;&#22833;&#30340;&#34892;&#20026;&#65292;&#20197;&#20102;&#35299;&#24182;&#25913;&#36827;LT&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#22024;&#26434;&#30446;&#26631;&#20855;&#26377;&#36739;&#39640;NLL&#25439;&#22833;&#30340;&#22522;&#30784;&#20551;&#35774;&#19981;&#34987;&#28385;&#36275;&#26102;&#65292;LT&#30340;&#24615;&#33021;&#26159;&#26377;&#38480;&#30340;&#65292;&#24182;&#21457;&#29616;&#23454;&#20307;&#20043;&#38388;&#30340;&#21333;&#35789;&#32423;NLL&#20026;&#21306;&#20998;&#20107;&#23454;&#24615;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#20449;&#21495;&#12290;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#28857;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;NLL&#25439;&#22833;&#21644;fi
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05788v1 Announce Type: cross  Abstract: Text summarization and simplification are among the most widely used applications of AI. However, models developed for such tasks are often prone to hallucination, which can result from training on unaligned data. One efficient approach to address this issue is Loss Truncation (LT) (Kang and Hashimoto, 2020), an approach to modify the standard log loss to adaptively remove noisy examples during training. However, we find that LT alone yields a considerable number of hallucinated entities on various datasets. We study the behavior of the underlying losses between factual and non-factual examples, to understand and refine the performance of LT. We demonstrate that LT's performance is limited when the underlying assumption that noisy targets have higher NLL loss is not satisfied, and find that word-level NLL among entities provides better signal for distinguishing factuality. We then leverage this to propose a fine-grained NLL loss and fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28176;&#36827;&#25200;&#21160;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#65288;PROPER&#65289;&#30340;&#35757;&#32451;&#33539;&#24335;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#26234;&#33021;&#20307;&#23545;&#21508;&#31181;&#24178;&#25200;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#20559;&#31163;&#40065;&#26834;&#23548;&#33322;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.05770</link><description>&lt;p&gt;
&#36890;&#36807;&#25200;&#21160;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#20559;&#31163;&#40065;&#26834;&#26234;&#33021;&#20307;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28176;&#36827;&#25200;&#21160;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#65288;PROPER&#65289;&#30340;&#35757;&#32451;&#33539;&#24335;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#26234;&#33021;&#20307;&#23545;&#21508;&#31181;&#24178;&#25200;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#20559;&#31163;&#40065;&#26834;&#23548;&#33322;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05770v1 &#21457;&#34920;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#35201;&#27714;&#26234;&#33021;&#20307;&#26681;&#25454;&#32473;&#23450;&#30340;&#35821;&#35328;&#25351;&#20196;&#22312;&#30495;&#23454;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20256;&#32479;&#30340;VLN&#26234;&#33021;&#20307;&#36890;&#24120;&#22312;&#26080;&#24178;&#25200;&#30340;&#29615;&#22659;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#33021;&#20250;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36731;&#26131;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#30693;&#36947;&#22914;&#20309;&#22788;&#29702;&#21508;&#31181;&#21487;&#33021;&#30340;&#24178;&#25200;&#65292;&#20363;&#22914;&#31361;&#28982;&#30340;&#38556;&#30861;&#29289;&#25110;&#20154;&#31867;&#20013;&#26029;&#65292;&#36825;&#20123;&#24178;&#25200;&#24191;&#27867;&#23384;&#22312;&#24182;&#36890;&#24120;&#20250;&#23548;&#33268;&#24847;&#22806;&#30340;&#36335;&#24452;&#20559;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28176;&#36827;&#25200;&#21160;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#65288;PROPER&#65289;&#30340;&#27169;&#22411;&#26080;&#20851;&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;VLN&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#35201;&#27714;&#23427;&#20204;&#26397;&#21521;&#20855;&#26377;&#20559;&#31163;&#40065;&#26834;&#24615;&#23548;&#33322;&#30340;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36335;&#24452;&#25200;&#21160;&#26041;&#26696;&#26469;&#23454;&#29616;&#36335;&#32447;&#20559;&#31163;&#65292;&#26234;&#33021;&#20307;&#38656;&#35201;&#32487;&#32493;&#25104;&#21151;&#22320;&#25353;&#29031;&#21407;&#22987;&#25351;&#20196;&#36827;&#34892;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05770v1 Announce Type: cross  Abstract: Vision-and-language navigation (VLN) asks an agent to follow a given language instruction to navigate through a real 3D environment. Despite significant advances, conventional VLN agents are trained typically under disturbance-free environments and may easily fail in real-world scenarios, since they are unaware of how to deal with various possible disturbances, such as sudden obstacles or human interruptions, which widely exist and may usually cause an unexpected route deviation. In this paper, we present a model-agnostic training paradigm, called Progressive Perturbation-aware Contrastive Learning (PROPER) to enhance the generalization ability of existing VLN agents, by requiring them to learn towards deviation-robust navigation. Specifically, a simple yet effective path perturbation scheme is introduced to implement the route deviation, with which the agent is required to still navigate successfully following the original instruction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#24212;&#29992;&#20110;&#24191;&#27867;&#25216;&#33021;&#21644;&#22810;&#31181;&#34892;&#20026;&#30340;&#21151;&#25928;&#65292;&#24182;&#21457;&#29616;&#23548;&#21521;&#24191;&#27867;&#25216;&#33021;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#20013;&#21516;&#26102;&#27880;&#20837;&#20010;&#20307;&#23548;&#21521;&#21521;&#37327;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.05767</link><description>&lt;p&gt;
&#23558;&#28608;&#27963;&#23548;&#21521;&#25193;&#23637;&#21040;&#24191;&#27867;&#25216;&#33021;&#19982;&#22810;&#31181;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Extending Activation Steering to Broad Skills and Multiple Behaviours
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#24212;&#29992;&#20110;&#24191;&#27867;&#25216;&#33021;&#21644;&#22810;&#31181;&#34892;&#20026;&#30340;&#21151;&#25928;&#65292;&#24182;&#21457;&#29616;&#23548;&#21521;&#24191;&#27867;&#25216;&#33021;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#20013;&#21516;&#26102;&#27880;&#20837;&#20010;&#20307;&#23548;&#21521;&#21521;&#37327;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21361;&#38505;&#30340;&#33021;&#21147;&#65292;&#36825;&#24456;&#21487;&#33021;&#22312;&#26410;&#26469;&#21464;&#24471;&#26356;&#21152;&#26840;&#25163;&#12290;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#21487;&#29992;&#20110;&#20943;&#23569;&#36825;&#20123;&#33021;&#21147;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28608;&#27963;&#23548;&#21521;&#22312;&#24191;&#27867;&#25216;&#33021;&#21644;&#22810;&#31181;&#34892;&#20026;&#20013;&#30340;&#21151;&#25928;&#12290;&#36890;&#36807;&#27604;&#36739;&#20943;&#23569;&#23545;&#19968;&#33324;&#32534;&#30721;&#33021;&#21147;&#21644;Python&#29305;&#23450;&#33021;&#21147;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23548;&#21521;&#26356;&#24191;&#27867;&#25216;&#33021;&#19982;&#23548;&#21521;&#36739;&#31364;&#25216;&#33021;&#31454;&#20105;&#28608;&#28872;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#25110;&#26356;&#23569;&#36817;&#35270;&#21644;&#23547;&#27714;&#36130;&#23500;&#65292;&#20197;&#21450;&#20854;&#20182;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#23558;&#22810;&#31181;&#19981;&#21516;&#34892;&#20026;&#30340;&#23548;&#21521;&#21521;&#37327;&#32452;&#21512;&#20026;&#19968;&#20010;&#23548;&#21521;&#21521;&#37327;&#36890;&#24120;&#19981;&#25104;&#21151;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#20013;&#19981;&#21516;&#20301;&#32622;&#27880;&#20837;&#20010;&#20307;&#23548;&#21521;&#21521;&#37327;&#26159;&#26377;&#21069;&#36884;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05767v1 Announce Type: cross  Abstract: Current large language models have dangerous capabilities, which are likely to become more problematic in the future. Activation steering techniques can be used to reduce risks from these capabilities. In this paper, we investigate the efficacy of activation steering for broad skills and multiple behaviours. First, by comparing the effects of reducing performance on general coding ability and Python-specific ability, we find that steering broader skills is competitive to steering narrower skills. Second, we steer models to become more or less myopic and wealth-seeking, among other behaviours. In our experiments, combining steering vectors for multiple different behaviours into one steering vector is largely unsuccessful. On the other hand, injecting individual steering vectors at different places in a model simultaneously is promising.
&lt;/p&gt;</description></item><item><title>&#24182;&#34892;&#37327;&#23376;&#36864;&#28779;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#22312;&#21333;&#20010;&#36864;&#28779;&#21608;&#26399;&#20013;&#35299;&#20915;&#22810;&#20010;&#29420;&#31435;&#38382;&#39064;&#26469;&#20248;&#21270;&#21487;&#29992;&#37327;&#23376;&#20301;&#30340;&#21033;&#29992;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#39034;&#24207;&#22788;&#29702;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#38386;&#32622;&#37327;&#23376;&#20301;&#65292;&#24182;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#21152;&#36895;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05764</link><description>&lt;p&gt;
&#25506;&#35752;&#24182;&#34892;&#37327;&#23376;&#36864;&#28779;&#22312;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#38382;&#39064;&#26041;&#38754;&#30340;&#28508;&#21147;&#65306;&#19968;&#39033;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigation into the Potential of Parallel Quantum Annealing for Simultaneous Optimization of Multiple Problems: A Comprehensive Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05764
&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#37327;&#23376;&#36864;&#28779;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#22312;&#21333;&#20010;&#36864;&#28779;&#21608;&#26399;&#20013;&#35299;&#20915;&#22810;&#20010;&#29420;&#31435;&#38382;&#39064;&#26469;&#20248;&#21270;&#21487;&#29992;&#37327;&#23376;&#20301;&#30340;&#21033;&#29992;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#39034;&#24207;&#22788;&#29702;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#38386;&#32622;&#37327;&#23376;&#20301;&#65292;&#24182;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#21152;&#36895;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#37327;&#23376;&#36864;&#28779;&#26159;&#19968;&#31181;&#35299;&#20915;&#22810;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#25216;&#26415;&#12290;&#23427;&#26088;&#22312;&#36890;&#36807;&#22312;&#21333;&#20010;&#36864;&#28779;&#21608;&#26399;&#20013;&#22788;&#29702;&#22810;&#20010;&#29420;&#31435;&#38382;&#39064;&#26469;&#20248;&#21270;&#37327;&#23376;&#25299;&#25169;&#19978;&#21487;&#29992;&#37327;&#23376;&#20301;&#30340;&#21033;&#29992;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#31181;&#24182;&#34892;&#21270;&#26041;&#27861;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#20102;&#21508;&#31181;&#38382;&#39064;&#32500;&#24230;&#65292;&#21253;&#25324;&#20351;&#29992;&#29305;&#23450;&#26041;&#27861;&#36827;&#34892;&#26631;&#20934;&#21270;&#25216;&#26415;&#65292;&#22914;&#20351;&#29992;&#20855;&#26377;&#40664;&#35748;&#23884;&#20837;&#30340;DWaveSampler&#12289;&#20351;&#29992;&#33258;&#23450;&#20041;&#23884;&#20837;&#30340;DWaveSampler&#21644;LeapHybridSampler&#12290;&#36825;&#31181;&#26041;&#27861;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#38386;&#32622;&#37327;&#23376;&#20301;&#65292;&#24182;&#22312;&#35299;&#20915;&#26041;&#26696;&#26102;&#38388;&#65288;TTS&#65289;&#24230;&#37327;&#26631;&#20934;&#26041;&#38754;&#26174;&#31034;&#20986;&#19982;&#20256;&#32479;&#37327;&#23376;&#36864;&#28779;&#30456;&#27604;&#30340;&#24040;&#22823;&#21152;&#36895;&#28508;&#21147;&#65292;&#20256;&#32479;&#37327;&#23376;&#36864;&#28779;&#35299;&#20915;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#39034;&#24207;&#22788;&#29702;&#65292;&#21487;&#33021;&#23548;&#33268;&#26410;&#21033;&#29992;&#30340;&#37327;&#23376;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05764v1 Announce Type: cross  Abstract: Parallel Quantum Annealing is a technique to solve multiple optimization problems simultaneously. Parallel quantum annealing aims to optimize the utilization of available qubits on a quantum topology by addressing multiple independent problems in a single annealing cycle. This study provides insights into the potential and the limitations of this parallelization method. The experiments consisting of two different problems are integrated, and various problem dimensions are explored including normalization techniques using specific methods such as DWaveSampler with Default Embedding, DWaveSampler with Custom Embedding and LeapHybridSampler. This method minimizes idle qubits and holds promise for substantial speed-up, as indicated by the Time-to-Solution (TTS) metric, compared to traditional quantum annealing, which solves problems sequentially and may leave qubits unutilized.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32500;&#35745;&#31639;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#29992;&#20110;&#26356;&#39640;&#25928;&#21644;&#21152;&#36895;&#30340;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.05763</link><description>&lt;p&gt;
HDReason&#65306;&#36229;&#32500;&#30693;&#35782;&#22270;&#25512;&#29702;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
HDReason: Algorithm-Hardware Codesign for Hyperdimensional Knowledge Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32500;&#35745;&#31639;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#29992;&#20110;&#26356;&#39640;&#25928;&#21644;&#21152;&#36895;&#30340;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20026;&#22270;&#23398;&#20064;&#24212;&#29992;&#22914;&#39030;&#28857;&#20998;&#31867;&#21644;&#22270;&#20998;&#31867;&#25552;&#20986;&#20102;&#22823;&#37327;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#24456;&#23569;&#20851;&#27880;&#30693;&#35782;&#22270;&#34917;&#20840;&#65288;KGC&#65289;&#65292;&#36825;&#26159;&#19968;&#39033;&#20197;&#20854;&#26174;&#33879;&#26356;&#39640;&#31639;&#27861;&#22797;&#26434;&#24615;&#32780;&#38395;&#21517;&#30340;&#20219;&#21153;&#12290;&#22522;&#20110;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#26368;&#20808;&#36827;KGC&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#24191;&#27867;&#30340;&#39030;&#28857;/&#20851;&#31995;&#23884;&#20837;&#26356;&#26032;&#21644;&#22797;&#26434;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#36825;&#23545;&#21152;&#36895;&#26469;&#35828;&#26159;&#22256;&#38590;&#30340;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#21152;&#36895;&#22120;&#35774;&#35745;&#19981;&#20877;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#38656;&#35201;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#26469;&#36827;&#34892;KG&#25512;&#29702;&#12290;&#26368;&#36817;&#65292;&#21463;&#33041;&#21551;&#21457;&#30340;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#34987;&#24341;&#20837;&#20316;&#20026;&#36731;&#37327;&#32423;&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22270;&#23398;&#20064;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;HDC&#26469;&#23454;&#29616;&#19968;&#20010;&#22266;&#26377;&#26356;&#39640;&#25928;&#19988;&#36866;&#21512;&#21152;&#36895;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05763v1 Announce Type: cross  Abstract: In recent times, a plethora of hardware accelerators have been put forth for graph learning applications such as vertex classification and graph classification. However, previous works have paid little attention to Knowledge Graph Completion (KGC), a task that is well-known for its significantly higher algorithm complexity. The state-of-the-art KGC solutions based on graph convolution neural network (GCN) involve extensive vertex/relation embedding updates and complicated score functions, which are inherently cumbersome for acceleration. As a result, existing accelerator designs are no longer optimal, and a novel algorithm-hardware co-design for KG reasoning is needed.   Recently, brain-inspired HyperDimensional Computing (HDC) has been introduced as a promising solution for lightweight machine learning, particularly for graph learning applications. In this paper, we leverage HDC for an intrinsically more efficient and acceleration-fri
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24314;&#31435;&#22312;&#32473;&#23450;&#30340;&#26368;&#22823;&#26080;&#21521;&#22242;&#22823;&#23567;($s$)&#26041;&#38754;&#30340;&#19979;&#30028;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#29420;&#31435;&#26597;&#35810;&#39044;&#35328;&#32773;&#22312;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#20013;&#36827;&#34892;&#25104;&#21592;&#27979;&#35797;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05759</link><description>&lt;p&gt;
&#36890;&#36807;&#29420;&#31435;&#26597;&#35810;&#39044;&#35328;&#32773;&#22312;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#20013;&#36827;&#34892;&#25104;&#21592;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Membership Testing in Markov Equivalence Classes via Independence Query Oracles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05759
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24314;&#31435;&#22312;&#32473;&#23450;&#30340;&#26368;&#22823;&#26080;&#21521;&#22242;&#22823;&#23567;($s$)&#26041;&#38754;&#30340;&#19979;&#30028;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#29420;&#31435;&#26597;&#35810;&#39044;&#35328;&#32773;&#22312;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#20013;&#36827;&#34892;&#25104;&#21592;&#27979;&#35797;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#26159;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#24191;&#27867;&#24433;&#21709;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#25237;&#20837;&#20102;&#22823;&#37327;&#30740;&#31350;&#26469;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#20294;&#20854;&#34917;&#20805;&#27010;&#24565;&#8212;&#8212;&#27979;&#35797;&#22240;&#26524;&#20851;&#31995;&#21364;&#22522;&#26412;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#22312;&#32473;&#23450;MEC(Markov&#31561;&#20215;&#31867;)&#30340;&#26368;&#22823;&#26080;&#21521;&#22242;&#30340;&#22823;&#23567;($s$)&#26041;&#38754;&#30340;&#19979;&#30028;&#65292;&#25506;&#35752;&#22522;&#20110;&#32422;&#26463;&#30340;&#27979;&#35797;&#26041;&#27861;&#12290;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;$\exp(\Omega(s))$&#20010;&#29420;&#31435;&#24615;&#27979;&#35797;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05759v1 Announce Type: cross  Abstract: Understanding causal relationships between variables is a fundamental problem with broad impact in numerous scientific fields. While extensive research has been dedicated to learning causal graphs from data, its complementary concept of testing causal relationships has remained largely unexplored. While learning involves the task of recovering the Markov equivalence class (MEC) of the underlying causal graph from observational data, the testing counterpart addresses the following critical question: Given a specific MEC and observational data from some causal graph, can we determine if the data-generating causal graph belongs to the given MEC?   We explore constraint-based testing methods by establishing bounds on the required number of conditional independence tests. Our bounds are in terms of the size of the maximum undirected clique ($s$) of the given MEC. In the worst case, we show a lower bound of $\exp(\Omega(s))$ independence tes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;TOSG&#25552;&#21462;&#30340;&#26041;&#27861;KG-TOSA&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38754;&#21521;&#20219;&#21153;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#20197;&#20943;&#36731;&#23545;&#22823;&#22411;KG&#30340;&#36807;&#22810;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2403.05752</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#30693;&#35782;&#22270;&#19978;&#35757;&#32451;&#38754;&#21521;&#20219;&#21153;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20934;&#30830;&#39640;&#25928;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;TOSG&#25552;&#21462;&#30340;&#26041;&#27861;KG-TOSA&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38754;&#21521;&#20219;&#21153;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#20197;&#20943;&#36731;&#23545;&#22823;&#22411;KG&#30340;&#36807;&#22810;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#26159;&#19968;&#31181;&#21253;&#21547;&#21508;&#31181;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#30340;&#24322;&#26500;&#22270;&#12290;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#36890;&#24120;&#29992;&#20110;&#22312;KG&#19978;&#35757;&#32451;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#31561;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;HGNN&#26041;&#27861;&#21463;KG&#30340;&#22823;&#23567;&#12289;&#23494;&#24230;&#20197;&#21450;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#25968;&#37327;&#30340;&#24433;&#21709;&#65292;&#34920;&#29616;&#20986;&#36807;&#22810;&#30340;&#22797;&#26434;&#24615;&#12290;AI&#20174;&#19994;&#32773;&#25163;&#24037;&#35774;&#35745;&#20986;&#19968;&#20010;&#19982;&#29305;&#23450;&#20219;&#21153;&#30456;&#20851;&#30340;KG G&#30340;&#23376;&#22270;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#38754;&#21521;&#20219;&#21153;&#30340;&#23376;&#22270;&#65288;TOSG&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;G&#20013;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#30340;&#23376;&#38598;&#12290;&#20351;&#29992;TOSG&#32780;&#19981;&#26159;G&#26469;&#35757;&#32451;&#20219;&#21153;&#21487;&#20197;&#20943;&#36731;&#23545;&#22823;&#22411;KG&#25152;&#38656;&#30340;&#36807;&#22810;&#35745;&#31639;&#12290;&#35774;&#35745;TOSG&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;KG&#30340;&#32467;&#26500;&#21644;&#20219;&#21153;&#30340;&#30446;&#26631;&#65292;&#22240;&#27492;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;KG-TOSA&#65292;&#19968;&#31181;&#33258;&#21160;&#21270;TOSG&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;KG&#19978;&#36827;&#34892;&#38754;&#21521;&#20219;&#21153;&#30340;HGNN&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05752v1 Announce Type: cross  Abstract: A Knowledge Graph (KG) is a heterogeneous graph encompassing a diverse range of node and edge types. Heterogeneous Graph Neural Networks (HGNNs) are popular for training machine learning tasks like node classification and link prediction on KGs. However, HGNN methods exhibit excessive complexity influenced by the KG's size, density, and the number of node and edge types. AI practitioners handcraft a subgraph of a KG G relevant to a specific task. We refer to this subgraph as a task-oriented subgraph (TOSG), which contains a subset of task-related node and edge types in G. Training the task using TOSG instead of G alleviates the excessive computation required for a large KG. Crafting the TOSG demands a deep understanding of the KG's structure and the task's objectives. Hence, it is challenging and time-consuming. This paper proposes KG-TOSA, an approach to automate the TOSG extraction for task-oriented HGNN training on a large KG. In KG
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MG-TSD&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#20869;&#22312;&#31890;&#24230;&#27700;&#24179;&#20316;&#20026;&#30446;&#26631;&#26469;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#29366;&#24577;-of-the-art&#30340;&#39044;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.05751</link><description>&lt;p&gt;
MG-TSD&#65306;&#20855;&#26377;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#30340;&#22810;&#31890;&#24230;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05751
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MG-TSD&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#20869;&#22312;&#31890;&#24230;&#27700;&#24179;&#20316;&#20026;&#30446;&#26631;&#26469;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#29366;&#24577;-of-the-art&#30340;&#39044;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30001;&#20110;&#20854;&#29983;&#25104;&#39640;&#20445;&#30495;&#26679;&#26412;&#30340;&#26174;&#33879;&#33021;&#21147;&#32780;&#22312;&#29983;&#25104;&#24335;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24341;&#36215;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#38543;&#26426;&#29305;&#24615;&#24102;&#26469;&#30340;&#19981;&#31283;&#23450;&#24615;&#25361;&#25112;&#65292;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#22312;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24378;&#22823;&#24314;&#27169;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#31890;&#24230;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#65288;MG-TSD&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#20869;&#22312;&#30340;&#31890;&#24230;&#27700;&#24179;&#20316;&#20026;&#20013;&#38388;&#25193;&#25955;&#27493;&#39588;&#30340;&#32473;&#23450;&#30446;&#26631;&#26469;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05751v1 Announce Type: cross  Abstract: Recently, diffusion probabilistic models have attracted attention in generative time series forecasting due to their remarkable capacity to generate high-fidelity samples. However, the effective utilization of their strong modeling ability in the probabilistic time series forecasting task remains an open question, partially due to the challenge of instability arising from their stochastic nature. To address this challenge, we introduce a novel Multi-Granularity Time Series Diffusion (MG-TSD) model, which achieves state-of-the-art predictive performance by leveraging the inherent granularity levels within the data as given targets at intermediate diffusion steps to guide the learning process of diffusion models. The way to construct the targets is motivated by the observation that the forward process of the diffusion model, which sequentially corrupts the data distribution to a standard normal distribution, intuitively aligns with the p
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.05750</link><description>&lt;p&gt;
&#35299;&#35835;AI&#31508;: &#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#25216;&#26415;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05750
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#23637;&#31034;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#24443;&#24213;&#39072;&#35206;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;(NLG)&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24191;&#27867;&#30340;&#24212;&#29992;&#24102;&#26469;&#25361;&#25112;&#65292;&#38656;&#35201;&#28145;&#20837;&#23457;&#26597;&#12289;&#20262;&#29702;&#23457;&#26597;&#21644;&#36127;&#36131;&#20219;&#30340;&#23454;&#36341;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#25506;&#32034;&#20102;&#29616;&#26377;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#37325;&#28857;&#26159;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#20316;&#20026;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#35282;&#24230;&#35780;&#20272;&#20102;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24403;&#21069;&#39046;&#22495;&#38480;&#21046;&#30340;&#26032;&#39062;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05750v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20445;&#23432;DDPG&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;$Q$-&#30446;&#26631;&#21644;&#34892;&#20026;&#20811;&#38534;&#25439;&#22833;&#24809;&#32602;&#26469;&#35299;&#20915;DDPG&#20013;&#30340;&#39640;&#20272;&#20559;&#24046;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#38598;&#25104;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#23454;&#29616;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.05732</link><description>&lt;p&gt;
&#20445;&#23432;DDPG - &#26080;&#38598;&#25104;&#30340;&#24754;&#35266;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conservative DDPG -- Pessimistic RL without Ensemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20445;&#23432;DDPG&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;$Q$-&#30446;&#26631;&#21644;&#34892;&#20026;&#20811;&#38534;&#25439;&#22833;&#24809;&#32602;&#26469;&#35299;&#20915;DDPG&#20013;&#30340;&#39640;&#20272;&#20559;&#24046;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#38598;&#25104;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#23454;&#29616;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DDPG&#21463;&#21040;&#39640;&#20272;&#20559;&#24046;&#38382;&#39064;&#30340;&#38459;&#30861;&#65292;&#20854;&#20013;&#20854;$Q$-&#20272;&#35745;&#20542;&#21521;&#20110;&#22840;&#22823;&#23454;&#38469;$Q$&#20540;&#12290;&#20256;&#32479;&#35299;&#20915;&#36825;&#19968;&#20559;&#35265;&#30340;&#26041;&#27861;&#28041;&#21450;&#22522;&#20110;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#25110;&#32773;&#22522;&#20110;&#22797;&#26434;&#23545;&#25968;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#38590;&#20197;&#29702;&#35299;&#21644;&#23454;&#26045;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;$Q$-&#30446;&#26631;&#24182;&#32467;&#21512;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#25439;&#22833;&#24809;&#32602;&#12290;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#29992;&#36739;&#23569;&#30340;&#20195;&#30721;&#23454;&#29616;&#65292;&#32780;&#26080;&#38656;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#24378;&#28872;&#25903;&#25345;&#20445;&#23432;DDPG&#22312;&#21508;&#31181;MuJoCo&#21644;Bullet&#20219;&#21153;&#19978;&#20248;&#20110;DDPG&#12290;&#25105;&#20204;&#22987;&#32456;&#35266;&#23519;&#21040;&#22312;&#25152;&#26377;&#35780;&#20272;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#22312;&#19982;TD3&#21644;TD7&#30456;&#27604;&#24615;&#33021;&#26356;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#20248;&#36234;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#26159;&#20197;&#26174;&#33879;&#38477;&#20302;&#30340;&#35745;&#31639;&#35201;&#27714;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05732v1 Announce Type: new  Abstract: DDPG is hindered by the overestimation bias problem, wherein its $Q$-estimates tend to overstate the actual $Q$-values. Traditional solutions to this bias involve ensemble-based methods, which require significant computational resources, or complex log-policy-based approaches, which are difficult to understand and implement. In contrast, we propose a straightforward solution using a $Q$-target and incorporating a behavioral cloning (BC) loss penalty. This solution, acting as an uncertainty measure, can be easily implemented with minimal code and without the need for an ensemble. Our empirical findings strongly support the superiority of Conservative DDPG over DDPG across various MuJoCo and Bullet tasks. We consistently observe better performance in all evaluated tasks and even competitive or superior performance compared to TD3 and TD7, all achieved with significantly reduced computational requirements.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.05720</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05720
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#25688;&#35201;&#26159;&#36890;&#36807;&#24635;&#32467;&#20020;&#24202;&#35760;&#24405;&#32780;&#29983;&#25104;&#30340;&#24120;&#35265;&#20020;&#24202;&#25991;&#20214;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#21270;&#23454;&#38469;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21307;&#30103;&#24212;&#29992;&#65288;&#22914;BHC&#21512;&#25104;&#65289;&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#23637;&#31034;&#12290;&#20026;&#20102;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;BHC&#21512;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;MIMIC-IV&#35760;&#24405;&#20013;&#25552;&#21462;&#30340;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#23553;&#35013;&#20102;&#20020;&#24202;&#35760;&#24405;&#21644;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#23545;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;&#36890;&#29992;LLMs&#21644;&#19977;&#20010;&#21307;&#30103;&#39046;&#22495;&#36866;&#24212;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#25913;&#36827;&#20174;&#20020;&#24202;&#35760;&#24405;&#29983;&#25104;BHC&#12290;&#25105;&#20204;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;BHC&#65292;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#65288;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#21644;&#22522;&#20110;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#26469;&#24212;&#29992;&#20110;&#19977;&#20010;&#24320;&#28304;LLMs&#65288;Clinical-T5-Large&#65292;Llama2-13B&#65292;FLAN-UL2&#65289;&#21644;&#20004;&#20010;&#19987;&#26377;LLMs&#65288;GPT-3.5&#65292;GPT-4&#65289;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05720v1 Announce Type: cross  Abstract: Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose LLMs and three healthcare-adapted LLMs to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We quantitatively evaluate the performa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#32593;&#32476;&#29289;&#29702;&#20154;&#31995;&#32479;&#20013;&#20154;&#24037;&#26234;&#33021;&#25512;&#33616;&#21644;&#20154;&#31867;&#20915;&#31574;&#32773;&#20043;&#38388;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#32771;&#34385;&#20154;&#31867;&#21487;&#33021;&#19981;&#21516;&#20110;AI&#24179;&#21488;&#30340;&#24863;&#30693;&#21644;&#35299;&#37322;&#26041;&#24335;&#65292;&#24314;&#31435;&#20102;&#26368;&#20339;&#25512;&#33616;&#31574;&#30053;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#36817;&#20284;&#20154;&#31867;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#24615;&#30028;&#38480;&#21644;&#25968;&#20540;&#31034;&#20363;&#39564;&#35777;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.05715</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#22312;&#32593;&#32476;&#29289;&#29702;&#20154;&#31995;&#32479;&#20013;&#36827;&#34892;&#26377;&#25928;&#20154;&#24037;&#26234;&#33021;&#25512;&#33616;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Effective AI Recommendations in Cyber-Physical-Human Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05715
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#32593;&#32476;&#29289;&#29702;&#20154;&#31995;&#32479;&#20013;&#20154;&#24037;&#26234;&#33021;&#25512;&#33616;&#21644;&#20154;&#31867;&#20915;&#31574;&#32773;&#20043;&#38388;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#32771;&#34385;&#20154;&#31867;&#21487;&#33021;&#19981;&#21516;&#20110;AI&#24179;&#21488;&#30340;&#24863;&#30693;&#21644;&#35299;&#37322;&#26041;&#24335;&#65292;&#24314;&#31435;&#20102;&#26368;&#20339;&#25512;&#33616;&#31574;&#30053;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#36817;&#20284;&#20154;&#31867;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#24615;&#30028;&#38480;&#21644;&#25968;&#20540;&#31034;&#20363;&#39564;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32593;&#32476;&#29289;&#29702;&#20154;&#31995;&#32479;&#65288;CPHS&#65289;&#28041;&#21450;&#21040;&#19968;&#20010;&#20154;&#31867;&#20915;&#31574;&#32773;&#65292;&#20182;&#21487;&#33021;&#20250;&#20174;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24179;&#21488;&#25509;&#25910;&#25512;&#33616;&#65292;&#21516;&#26102;&#21448;&#25345;&#26377;&#26368;&#32456;&#20915;&#31574;&#30340;&#36131;&#20219;&#12290;&#22312;&#36825;&#31867;CPHS&#24212;&#29992;&#20013;&#65292;&#20154;&#31867;&#20915;&#31574;&#32773;&#21487;&#33021;&#20250;&#22240;&#20026;&#21508;&#31181;&#21407;&#22240;&#32780;&#20559;&#31163;&#26368;&#20339;&#25512;&#33616;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#23454;&#26045;&#21478;&#19968;&#20010;&#12290;&#22312;&#36825;&#23553;&#20449;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#26694;&#26550;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#20154;&#31867;&#21487;&#33021;&#20250;&#22240;&#20026;&#24863;&#30693;&#21644;&#35299;&#37322;&#31995;&#32479;&#29366;&#24577;&#30340;&#26041;&#24335;&#19982;AI&#24179;&#21488;&#19981;&#21516;&#32780;&#20559;&#31163;AI&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#26368;&#20339;&#25512;&#33616;&#31574;&#30053;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;AI&#20351;&#29992;&#30340;&#36817;&#20284;&#20154;&#31867;&#27169;&#22411;&#65288;AHM&#65289;&#12290;&#25105;&#20204;&#20026;&#30001;AHM&#20135;&#29983;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#25552;&#20379;&#20102;&#29702;&#35770;&#30028;&#38480;&#65292;&#24182;&#22312;&#19968;&#20010;&#25968;&#20540;&#31034;&#20363;&#20013;&#35828;&#26126;&#20102;&#25105;&#20204;&#32467;&#26524;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05715v1 Announce Type: cross  Abstract: Many cyber-physical-human systems (CPHS) involve a human decision-maker who may receive recommendations from an artificial intelligence (AI) platform while holding the ultimate responsibility of making decisions. In such CPHS applications, the human decision-maker may depart from an optimal recommended decision and instead implement a different one for various reasons. In this letter, we develop a rigorous framework to overcome this challenge. In our framework, we consider that humans may deviate from AI recommendations as they perceive and interpret the system's state in a different way than the AI platform. We establish the structural properties of optimal recommendation strategies and develop an approximate human model (AHM) used by the AI. We provide theoretical bounds on the optimality gap that arises from an AHM and illustrate the efficacy of our results in a numerical example.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26426;&#20114;&#21160;&#20013;&#26159;&#21542;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#20204;&#30340;&#34892;&#20026;&#21028;&#26029;&#21644;&#27807;&#36890;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#22312;&#29983;&#25104;&#31038;&#20250;&#21487;&#25509;&#21463;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.05701</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#19982;&#20154;&#20204;&#30340;&#31038;&#20132;&#30452;&#35273;&#30456;&#19968;&#33268;&#65292;&#29992;&#20110;&#20154;&#26426;&#20114;&#21160;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Aligned with People's Social Intuitions for Human-Robot Interactions?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05701
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26426;&#20114;&#21160;&#20013;&#26159;&#21542;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#20204;&#30340;&#34892;&#20026;&#21028;&#26029;&#21644;&#27807;&#36890;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#22312;&#29983;&#25104;&#31038;&#20250;&#21487;&#25509;&#21463;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#39640;&#23618;&#27425;&#30340;&#34892;&#21160;&#35268;&#21010;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35768;&#22810;&#26426;&#22120;&#20154;&#24212;&#29992;&#28041;&#21450;&#20154;&#31867;&#30417;&#30563;&#21592;&#25110;&#21512;&#20316;&#32773;&#12290;&#22240;&#27492;&#65292;&#23545;LLMs&#29983;&#25104;&#19982;&#20154;&#20204;&#20559;&#22909;&#21644;&#20215;&#20540;&#35266;&#30456;&#19968;&#33268;&#30340;&#31038;&#20250;&#21487;&#25509;&#21463;&#34892;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;LLMs&#26159;&#21542;&#25429;&#25417;&#21040;&#20154;&#20204;&#22312;&#20154;&#26426;&#20114;&#21160;&#65288;HRI&#65289;&#22330;&#26223;&#20013;&#34892;&#20026;&#21028;&#26029;&#21644;&#27807;&#36890;&#20559;&#22909;&#26041;&#38754;&#30340;&#30452;&#35273;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#37325;&#29616;&#20102;&#19977;&#20010;HRI&#29992;&#25143;&#30740;&#31350;&#65292;&#23558;LLMs&#30340;&#36755;&#20986;&#19982;&#30495;&#23454;&#21442;&#19982;&#32773;&#30340;&#36755;&#20986;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-4&#22312;&#38750;&#24120;&#20986;&#33394;&#22320;&#34920;&#29616;&#65292;&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;&#20004;&#39033;&#30740;&#31350;&#30340;&#29992;&#25143;&#31572;&#26696;&#20855;&#26377;&#24456;&#24378;&#30456;&#20851;&#24615;&#8212;&#8212;&#31532;&#19968;&#39033;&#30740;&#31350;&#28041;&#21450;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#27807;&#36890;&#20030;&#21160;&#32473;&#26426;&#22120;&#20154;&#65288;$r_s$ = 0.82&#65289;&#65292;&#31532;&#20108;&#39033;&#28041;&#21450;&#21028;&#26029;&#34892;&#20026;&#30340;&#21487;&#21462;&#24615;&#12289;&#24847;&#22270;&#24615;&#21644;&#20196;&#20154;&#24778;&#35766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05701v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly used in robotics, especially for high-level action planning. Meanwhile, many robotics applications involve human supervisors or collaborators. Hence, it is crucial for LLMs to generate socially acceptable actions that align with people's preferences and values. In this work, we test whether LLMs capture people's intuitions about behavior judgments and communication preferences in human-robot interaction (HRI) scenarios. For evaluation, we reproduce three HRI user studies, comparing the output of LLMs with that of real participants. We find that GPT-4 strongly outperforms other models, generating answers that correlate strongly with users' answers in two studies $\unicode{x2014}$ the first study dealing with selecting the most appropriate communicative act for a robot in various situations ($r_s$ = 0.82), and the second with judging the desirability, intentionality, and surprisingness of beh
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#65288;DFL&#65289;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#20013;&#30340;&#24212;&#29992;&#25552;&#39640;&#20102;&#24178;&#39044;&#30340;&#31934;&#20934;&#24230;&#65292;&#34429;&#28982;&#23384;&#22312;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#33021;&#20248;&#21270;&#26377;&#38480;&#30340;&#24178;&#39044;&#36164;&#28304;&#20351;&#29992;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.05683</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20998;&#35299;&#30340;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#30340;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Efficient Public Health Intervention Planning Using Decomposition-Based Decision-Focused Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05683
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#65288;DFL&#65289;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#20013;&#30340;&#24212;&#29992;&#25552;&#39640;&#20102;&#24178;&#39044;&#30340;&#31934;&#20934;&#24230;&#65292;&#34429;&#28982;&#23384;&#22312;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#33021;&#20248;&#21270;&#26377;&#38480;&#30340;&#24178;&#39044;&#36164;&#28304;&#20351;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#21355;&#29983;&#35745;&#21010;&#20013;&#21463;&#30410;&#32773;&#21442;&#19982;&#24230;&#19979;&#38477;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#20026;&#20102;&#25913;&#21892;&#20445;&#30041;&#29575;&#65292;&#20581;&#24247;&#24037;&#20316;&#32773;&#20250;&#23545;&#26377;&#36749;&#23398;&#39118;&#38505;&#30340;&#21463;&#30410;&#32773;&#36827;&#34892;&#24178;&#39044;&#65292;&#28982;&#32780;&#65292;&#20581;&#24247;&#24037;&#20316;&#32773;&#30340;&#21487;&#29992;&#24615;&#21644;&#26102;&#38388;&#26159;&#26377;&#38480;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#26377;&#30740;&#31350;&#33268;&#21147;&#20110;&#20351;&#29992;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#26426;&#26469;&#20248;&#21270;&#36825;&#20123;&#26377;&#38480;&#30340;&#24178;&#39044;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#36825;&#19968;&#26694;&#26550;&#30340;&#20851;&#38190;&#25216;&#26415;&#38556;&#30861;&#22312;&#20110;&#38656;&#35201;&#20174;&#21382;&#21490;&#25968;&#25454;&#20013;&#20272;&#31639;&#21463;&#30410;&#32773;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05683v1 Announce Type: new  Abstract: The declining participation of beneficiaries over time is a key concern in public health programs. A popular strategy for improving retention is to have health workers `intervene' on beneficiaries at risk of dropping out. However, the availability and time of these health workers are limited resources. As a result, there has been a line of research on optimizing these limited intervention resources using Restless Multi-Armed Bandits (RMABs). The key technical barrier to using this framework in practice lies in the need to estimate the beneficiaries' RMAB parameters from historical data. Recent research has shown that Decision-Focused Learning (DFL), which focuses on maximizing the beneficiaries' adherence rather than predictive accuracy, improves the performance of intervention targeting using RMABs. Unfortunately, these gains come at a high computational cost because of the need to solve and evaluate the RMAB in each DFL training step. 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#26469;&#20445;&#25252;&#22312;&#29615;&#22659;&#20013;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.05681</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;ially Private Tabular Data&#36827;&#34892;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;DP-TabICL
&lt;/p&gt;
&lt;p&gt;
DP-TabICL: In-Context Learning with Differentially Private Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05681
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#26469;&#20445;&#25252;&#22312;&#29615;&#22659;&#20013;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL)&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#22312;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#31034;&#33539;&#26465;&#20214;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#24182;&#19988;&#23427;&#24050;&#32463;&#34920;&#29616;&#20986;&#19982;&#26114;&#36149;&#30340;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#21644;&#24494;&#35843;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;ICL&#24050;&#34987;&#25193;&#23637;&#65292;&#20801;&#35768;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#20316;&#20026;&#31034;&#33539;&#31034;&#20363;&#65292;&#26041;&#27861;&#26159;&#23558;&#21333;&#20010;&#35760;&#24405;&#20018;&#34892;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#34920;&#26126;LLM&#21487;&#33021;&#20250;&#27844;&#38706;&#25552;&#31034;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#65292;&#32780;&#19988;&#30001;&#20110;&#34920;&#26684;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65292;&#22240;&#27492;&#20102;&#35299;&#22914;&#20309;&#20445;&#25252;ICL&#20013;&#20351;&#29992;&#30340;&#22522;&#30784;&#34920;&#26684;&#25968;&#25454;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#20316;&#20026;&#23545;&#22914;&#20309;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#36827;&#34892;&#21021;&#22987;&#25506;&#32034;&#30340;&#30740;&#31350;--&#24046;&#20998;&#38544;&#31169;&#26159;&#25968;&#25454;&#38544;&#31169;&#21644;&#21311;&#21517;&#21270;&#30340;&#38271;&#26399;&#37329;&#26631;&#20934;--&#20197;&#20445;&#25252;ICL&#20013;&#20351;&#29992;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#25968;&#25454;&#31169;&#26377;&#21270;&#26426;&#21046;&#22312;&#31169;&#26377;&#34920;&#26684;ICL&#20013;&#24212;&#29992;DP&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05681v1 Announce Type: cross  Abstract: In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks by conditioning on demonstrations of question-answer pairs and it has been shown to have comparable performance to costly model retraining and fine-tuning. Recently, ICL has been extended to allow tabular data to be used as demonstration examples by serializing individual records into natural language formats. However, it has been shown that LLMs can leak information contained in prompts, and since tabular data often contain sensitive information, understanding how to protect the underlying tabular data used in ICL is a critical area of research. This work serves as an initial investigation into how to use differential privacy (DP) -- the long-established gold standard for data privacy and anonymization -- to protect tabular data used in ICL. Specifically, we investigate the application of DP mechanisms for private tabular ICL via data privatization pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;LLMs&#22312;&#29983;&#25104;CT&#24322;&#24120;&#30340;&#20934;&#30830;&#25688;&#35201;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05680</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#23545;&#22522;&#20110;&#35270;&#35273;&#30340;LLM&#39044;&#27979;&#36827;&#34892;&#20998;&#35299;&#20197;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05680
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;LLMs&#22312;&#29983;&#25104;CT&#24322;&#24120;&#30340;&#20934;&#30830;&#25688;&#35201;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CT&#26816;&#26597;&#30340;&#25968;&#37327;&#27599;&#24180;&#37117;&#22312;&#22686;&#21152;&#65292;&#36825;&#23548;&#33268;&#25918;&#23556;&#31185;&#21307;&#29983;&#30130;&#21171;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#20943;&#36731;&#20182;&#20204;&#30340;&#36127;&#25285;&#65292;&#20294;&#20854;&#22312;&#20020;&#24202;&#20013;&#30340;&#37319;&#29992;&#21462;&#20915;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#20449;&#20219;&#21644;&#29983;&#25104;&#20869;&#23481;&#30340;&#31616;&#21333;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;LLMs&#22312;&#29983;&#25104;CT&#24322;&#24120;&#30340;&#20934;&#30830;&#25688;&#35201;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05680v1 Announce Type: new  Abstract: The volume of CT exams being done in the world has been rising every year, which has led to radiologist burn-out. Large Language Models (LLMs) have the potential to reduce their burden, but their adoption in the clinic depends on radiologist trust, and easy evaluation of generated content. Presently, many automated methods are available to evaluate the reports generated for chest radiographs, but such an approach is not available for CT presently. In this paper, we propose a novel evaluation framework to judge the capabilities of vision-language LLMs in generating accurate summaries of CT-based abnormalities. CT slices containing an abnormality (e.g., lesion) were input to a vision-based LLM (GPT-4V, LLaVA-Med, and RadFM), and it generated a free-text summary of the predicted characteristics of the abnormality. Next, a GPT-4 model decomposed the summary into specific aspects (body part, location, type, and attributes), automatically eval
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#28608;&#27963;&#26041;&#27861;&#22312;CNN&#27169;&#22411;&#22270;&#20687;&#20998;&#31867;&#39044;&#27979;&#35299;&#37322;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Feature CAM&#25216;&#26415;&#65292;&#29992;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05658</link><description>&lt;p&gt;
Feature CAM: &#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#21487;&#35299;&#37322;AI
&lt;/p&gt;
&lt;p&gt;
Feature CAM: Interpretable AI in Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#28608;&#27963;&#26041;&#27861;&#22312;CNN&#27169;&#22411;&#22270;&#20687;&#20998;&#31867;&#39044;&#27979;&#35299;&#37322;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Feature CAM&#25216;&#26415;&#65292;&#29992;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24120;&#34987;&#31216;&#20026;&#40657;&#30418;&#23376;&#65292;&#22240;&#20026;&#20854;&#22797;&#26434;&#30340;&#28145;&#23618;&#32467;&#26500;&#21644;&#20869;&#37096;&#23618;&#30340;&#19981;&#36879;&#26126;&#24615;&#12290;&#20154;&#20204;&#22312;&#23558;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#23433;&#20840;&#12289;&#37329;&#34701;&#12289;&#20581;&#24247;&#21644;&#21046;&#36896;&#19994;&#31561;&#20851;&#38190;&#21644;&#39640;&#31934;&#24230;&#39046;&#22495;&#26102;&#24120;&#24120;&#32570;&#20047;&#20449;&#20219;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#28608;&#27963;&#26041;&#27861;&#65288;ABM&#65289;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;CNN&#27169;&#22411;&#23545;&#22270;&#20687;&#20998;&#31867;&#24212;&#29992;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#21516;&#26679;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20843;&#31181;&#22522;&#20110;CNN&#30340;&#20307;&#31995;&#32467;&#26500;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;CAM&#25216;&#26415;&#65292;&#23427;&#23646;&#20110;&#25200;&#21160;&#28608;&#27963;&#32452;&#21512;&#65292;&#29992;&#20110;&#21019;&#24314;&#32454;&#31890;&#24230;&#30340;&#12289;&#20855;&#26377;&#31867;&#21035;&#21306;&#20998;&#24615;&#30340;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05658v1 Announce Type: cross  Abstract: Deep Neural Networks have often been called the black box because of the complex, deep architecture and non-transparency presented by the inner layers. There is a lack of trust to use Artificial Intelligence in critical and high-precision fields such as security, finance, health, and manufacturing industries. A lot of focused work has been done to provide interpretable models, intending to deliver meaningful insights into the thoughts and behavior of neural networks. In our research, we compare the state-of-the-art methods in the Activation-based methods (ABM) for interpreting predictions of CNN models, specifically in the application of Image Classification. We then extend the same for eight CNN-based architectures to compare the differences in visualization and thus interpretability. We introduced a novel technique Feature CAM, which falls in the perturbation-activation combination, to create fine-grained, class-discriminative visual
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>https://arxiv.org/abs/2403.05652</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
What is different between these datasets?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05652
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05652v1 Announce Type: cross  Abstract: The performance of machine learning models heavily depends on the quality of input data, yet real-world applications often encounter various data-related challenges. One such challenge could arise when curating training data or deploying the model in the real world - two comparable datasets in the same domain may have different distributions. While numerous techniques exist for detecting distribution shifts, the literature lacks comprehensive approaches for explaining dataset differences in a human-understandable manner. To address this gap, we propose a suite of interpretable methods (toolbox) for comparing two datasets. We demonstrate the versatility of our approach across diverse data modalities, including tabular data, language, images, and signals in both low and high-dimensional settings. Our methods not only outperform comparable and related approaches in terms of explanation quality and correctness, but also provide actionable,
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30456;&#31354;&#38388;&#30340;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;BCI&#35299;&#30721;&#65292;&#25552;&#20379;&#20102;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#20013;&#21487;&#38752;&#31639;&#27861;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#33298;&#36866;&#24230;&#24182;&#20419;&#36827;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.05645</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#31354;&#38388;&#30340;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;BCI&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Geometric Neural Network based on Phase Space for BCI decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05645
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30456;&#31354;&#38388;&#30340;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;BCI&#35299;&#30721;&#65292;&#25552;&#20379;&#20102;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#20013;&#21487;&#38752;&#31639;&#27861;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#33298;&#36866;&#24230;&#24182;&#20419;&#36827;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Deep Learning(DL)&#31639;&#27861;&#19982;&#33041;&#20449;&#21495;&#20998;&#26512;&#30340;&#25972;&#21512;&#20173;&#22788;&#20110;&#33804;&#33469;&#38454;&#27573;&#65292;&#30456;&#27604;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;(BCI)&#39046;&#22495;&#23588;&#20026;&#31361;&#20986;&#65292;BCI&#36890;&#36807;&#35299;&#30721;&#22823;&#33041;&#27963;&#21160;&#25511;&#21046;&#22806;&#37096;&#35774;&#22791;&#32780;&#26080;&#38656;&#32908;&#32905;&#25511;&#21046;&#12290;&#33041;&#30005;&#22270;(EEG)&#26159;&#35774;&#35745;BCI&#31995;&#32479;&#30340;&#24191;&#27867;&#36873;&#25321;&#65292;&#22240;&#20854;&#26080;&#21019;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#20986;&#33394;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#20294;&#32570;&#23569;&#35757;&#32451;&#25968;&#25454;&#12289;&#20449;&#22122;&#27604;&#20302;&#12289;&#20197;&#21450;&#22312;&#20010;&#20307;&#38388;&#21644;&#20869;&#37096;&#30340;&#22823;&#37327;&#21464;&#21270;&#12290; &#26368;&#21518;&#65292;&#20351;&#29992;&#22810;&#20010;&#30005;&#26497;&#35774;&#32622;BCI&#31995;&#32479;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#65292;&#38459;&#30861;&#21487;&#38752;DL&#26550;&#26500;&#22312;&#30740;&#31350;&#23454;&#39564;&#23460;&#20043;&#22806;&#30340;BCI&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290; &#20026;&#20102;&#25552;&#39640;&#37319;&#32435;&#29575;&#65292;&#25105;&#20204;&#38656;&#35201;&#25913;&#21892;&#29992;&#25143;&#33298;&#36866;&#24230;&#65292;&#20363;&#22914;&#20351;&#29992;&#23569;&#37327;&#30005;&#26497;&#25805;&#20316;&#30340;&#21487;&#38752;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05645v1 Announce Type: cross  Abstract: The integration of Deep Learning (DL) algorithms on brain signal analysis is still in its nascent stages compared to their success in fields like Computer Vision, especially in Brain-Computer Interface (BCI), where the brain activity is decoded to control external devices without requiring muscle control. Electroencephalography (EEG) is a widely adopted choice for designing BCI systems due to its non-invasive and cost-effective nature and excellent temporal resolution. Still, it comes at the expense of limited training data, poor signal-to-noise, and a large variability across and within-subject recordings. Finally, setting up a BCI system with many electrodes takes a long time, hindering the widespread adoption of reliable DL architectures in BCIs outside research laboratories. To improve adoption, we need to improve user comfort using, for instance, reliable algorithms that operate with few electrodes. \textbf{Approach:} Our research
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#20811;&#26381;&#24403;&#21069;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#21644;&#25512;&#26029;&#20219;&#21153;&#35268;&#21017;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25506;&#32034;&#20154;&#31867;&#22312;&#31526;&#21495;&#21644;&#36830;&#32493;&#24863;&#30693;&#34920;&#31034;&#20043;&#38388;&#28789;&#27963;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05641</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#29305;&#24449;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#24863;&#30693;&#21644;&#25277;&#35937;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Feature-based Generalizable Prediction Model for Both Perceptual and Abstract Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#20811;&#26381;&#24403;&#21069;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#21644;&#25512;&#26029;&#20219;&#21153;&#35268;&#21017;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25506;&#32034;&#20154;&#31867;&#22312;&#31526;&#21495;&#21644;&#36830;&#32493;&#24863;&#30693;&#34920;&#31034;&#20043;&#38388;&#28789;&#27963;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#24935;&#30340;&#19968;&#20010;&#29305;&#28857;&#26159;&#33021;&#22815;&#20174;&#26377;&#38480;&#30340;&#32463;&#39564;&#20013;&#25512;&#26029;&#20986;&#25277;&#35937;&#35268;&#21017;&#65292;&#24182;&#23558;&#36825;&#20123;&#35268;&#21017;&#24212;&#29992;&#20110;&#38476;&#29983;&#24773;&#22659;&#12290;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#30340;&#22810;&#39033;&#36827;&#23637;&#24102;&#26469;&#20102;&#22810;&#20010;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20154;&#31867;&#33021;&#22815;&#22312;&#24456;&#23569;&#25110;&#27809;&#26377;&#25509;&#35302;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#21644;&#34920;&#36798;&#36825;&#20123;&#20219;&#21153;&#32972;&#21518;&#30340;&#35268;&#21017;&#65292;&#24403;&#20195;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#22522;&#20110;&#27169;&#24335;&#30340;&#35757;&#32451;&#65292;&#26080;&#27861;&#34920;&#36798;&#25110;&#25512;&#26029;&#20986;&#20219;&#21153;&#20013;&#30340;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22823;&#37096;&#20998;Raven's Progressive Matrices&#25110;&#31867;&#20284;&#20219;&#21153;&#20351;&#29992;&#20102;&#31526;&#21495;&#34920;&#31034;&#65292;&#32780;&#20154;&#31867;&#21487;&#20197;&#22312;&#31526;&#21495;&#21644;&#36830;&#32493;&#24863;&#30693;&#34920;&#31034;&#20043;&#38388;&#28789;&#27963;&#20999;&#25442;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#26816;&#27979;&#30340;&#31639;&#27861;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#24212;&#29992;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05641v1 Announce Type: new  Abstract: A hallmark of human intelligence is the ability to infer abstract rules from limited experience and apply these rules to unfamiliar situations. This capacity is widely studied in the visual domain using the Raven's Progressive Matrices. Recent advances in deep learning have led to multiple artificial neural network models matching or even surpassing human performance. However, while humans can identify and express the rule underlying these tasks with little to no exposure, contemporary neural networks often rely on massive pattern-based training and cannot express or extrapolate the rule inferred from the task. Furthermore, most Raven's Progressive Matrices or Raven-like tasks used for neural network training used symbolic representations, whereas humans can flexibly switch between symbolic and continuous perceptual representations. In this work, we present an algorithmic approach to rule detection and application using feature detection
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20803;&#35748;&#30693;&#26041;&#27861;&#65292;&#21517;&#20026;CLEAR&#65292;&#26088;&#22312;&#20026;LLMs&#25552;&#20379;&#33258;&#25105;&#24847;&#35782;&#30340;&#38169;&#35823;&#35782;&#21035;&#21644;&#32416;&#27491;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.05636</link><description>&lt;p&gt;
&#26080;&#38656;&#35843;&#21442;&#30340;LLM&#37096;&#32626;&#36127;&#36131;&#24178;&#39044;--&#19968;&#31181;&#20803;&#35748;&#30693;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tuning-Free Accountable Intervention for LLM Deployment -- A Metacognitive Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05636
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20803;&#35748;&#30693;&#26041;&#27861;&#65292;&#21517;&#20026;CLEAR&#65292;&#26088;&#22312;&#20026;LLMs&#25552;&#20379;&#33258;&#25105;&#24847;&#35782;&#30340;&#38169;&#35823;&#35782;&#21035;&#21644;&#32416;&#27491;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#23569;&#37327;&#25110;&#38646;-shot&#25552;&#31034;&#22312;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20652;&#29983;&#20102;&#21464;&#38761;&#24615;&#36827;&#23637;&#65292;&#32469;&#36807;&#20102;&#21442;&#25968;&#35843;&#25972;&#30340;&#24517;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20415;&#21033;&#30340;&#25805;&#20316;&#26041;&#24335;&#21152;&#21095;&#20102;&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#23427;&#20204;&#24222;&#22823;&#27169;&#22411;&#35268;&#27169;&#32972;&#21518;&#30340;&#31070;&#31192;&#8220;&#40657;&#21283;&#23376;&#8221;&#24615;&#36136;&#12290;&#36825;&#20123;&#25285;&#24551;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#65289;&#20013;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#19981;&#36127;&#36131;&#20219;&#30340;&#20915;&#31574;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#20915;&#31574;&#20381;&#36182;&#20110;&#24494;&#22937;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#22914;&#36890;&#36807;&#27010;&#24565;&#29702;&#35299;&#24863;&#30693;&#21644;&#33258;&#36866;&#24212;&#22320;&#32416;&#27491;&#38169;&#35823;&#21028;&#26029;&#30340;&#33021;&#21147;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20803;&#35748;&#30693;&#26041;&#27861;&#65292;&#31216;&#20026;CLEAR&#65292;&#20026;LLMs&#25552;&#20379;&#33258;&#25105;&#24847;&#35782;&#30340;&#38169;&#35823;&#35782;&#21035;&#21644;&#32416;&#27491;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#21161;&#20110;&#26500;&#24314;co
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05636v1 Announce Type: new  Abstract: Large Language Models (LLMs) have catalyzed transformative advances across a spectrum of natural language processing tasks through few-shot or zero-shot prompting, bypassing the need for parameter tuning. While convenient, this modus operandi aggravates ``hallucination'' concerns, particularly given the enigmatic ``black-box'' nature behind their gigantic model sizes. Such concerns are exacerbated in high-stakes applications (e.g., healthcare), where unaccountable decision errors can lead to devastating consequences. In contrast, human decision-making relies on nuanced cognitive processes, such as the ability to sense and adaptively correct misjudgments through conceptual understanding. Drawing inspiration from human cognition, we propose an innovative \textit{metacognitive} approach, dubbed \textbf{CLEAR}, to equip LLMs with capabilities for self-aware error identification and correction. Our framework facilitates the construction of co
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#33258;&#25105;&#21338;&#24328;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#30830;&#23450;&#24615;&#36718;&#27969;&#38646;&#21644;&#28216;&#25103;&#65288;DTZG&#65289;&#65292;&#22914;&#22269;&#38469;&#35937;&#26827;&#21644;&#22260;&#26827;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.05632</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29609;&#28216;&#25103;&#21527;&#65311;&#19968;&#31181;&#33258;&#25105;&#21338;&#24328;&#26041;&#27861;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Play Games? A Case Study of A Self-Play Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#33258;&#25105;&#21338;&#24328;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#30830;&#23450;&#24615;&#36718;&#27969;&#38646;&#21644;&#28216;&#25103;&#65288;DTZG&#65289;&#65292;&#22914;&#22269;&#38469;&#35937;&#26827;&#21644;&#22260;&#26827;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21033;&#29992;&#26469;&#33258;&#20114;&#32852;&#32593;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#23384;&#20648;&#24191;&#27867;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#34429;&#28982;LLMs&#24050;&#34987;&#35777;&#26126;&#26377;&#21161;&#20110;&#20915;&#31574;&#36741;&#21161;&#65292;&#20294;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21463;&#21040;&#25512;&#29702;&#33021;&#21147;&#30340;&#38480;&#21046;&#12289;&#24187;&#35273;&#29616;&#35937;&#31561;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#26159;&#19968;&#31181;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#23637;&#24320;&#21644;&#33258;&#25105;&#21338;&#24328;&#25552;&#20379;&#21487;&#38752;&#30340;&#20915;&#31574;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#20915;&#31574;&#22330;&#26223;&#20013;&#65292;MCTS&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#20462;&#21098;&#21644;&#22806;&#37096;&#20540;&#20989;&#25968;&#12290;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;MCTS&#33258;&#25105;&#21338;&#24328;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#35299;&#20915;&#30830;&#23450;&#24615;&#36718;&#27969;&#38646;&#21644;&#28216;&#25103;&#65288;DTZG&#65289;&#65292;&#22914;&#22269;&#38469;&#35937;&#26827;&#21644;&#22260;&#26827;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#20316;&#20026;&#34892;&#21160;&#21098;&#26525;&#22120;&#21644;&#20540;&#20989;&#25968;&#30340;&#20195;&#29702;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05632v1 Announce Type: new  Abstract: Large Language Models (LLMs) harness extensive data from the Internet, storing a broad spectrum of prior knowledge. While LLMs have proven beneficial as decision-making aids, their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on. On the other hand, Monte-Carlo Tree Search (MCTS) is a heuristic search algorithm that provides reliable decision-making solutions, achieved through recursive rollouts and self-play. However, the effectiveness of MCTS relies heavily on heuristic pruning and external value functions, particularly in complex decision scenarios. This work introduces an innovative approach that bolsters LLMs with MCTS self-play to efficiently resolve deterministic turn-based zero-sum games (DTZG), such as chess and go, without the need for additional training. Specifically, we utilize LLMs as both action pruners and proxies for value functions without the need for additional training. We theo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#35843;&#25972;&#24494;&#35843;&#31034;&#20363;&#30340;&#30417;&#30563;&#26469;&#25511;&#21046;&#20854;&#23545;&#19981;&#29087;&#24713;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#65292;&#26356;&#21487;&#38752;&#22320;&#20943;&#36731;&#20102;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.05612</link><description>&lt;p&gt;
&#19981;&#29087;&#24713;&#30340;&#24494;&#35843;&#31034;&#20363;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Unfamiliar Finetuning Examples Control How Language Models Hallucinate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#35843;&#25972;&#24494;&#35843;&#31034;&#20363;&#30340;&#30417;&#30563;&#26469;&#25511;&#21046;&#20854;&#23545;&#19981;&#29087;&#24713;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#65292;&#26356;&#21487;&#38752;&#22320;&#20943;&#36731;&#20102;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20542;&#21521;&#20110;&#29983;&#25104;&#21548;&#36215;&#26469;&#20196;&#20154;&#20449;&#26381;&#20294;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#29305;&#21035;&#26159;&#24403;&#22312;&#19981;&#29087;&#24713;&#30340;&#27010;&#24565;&#19978;&#36827;&#34892;&#26597;&#35810;&#26102;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35843;&#25972;&#21518;&#30340;LLMs&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#27169;&#24335;&#65306;&#38543;&#30528;&#36755;&#20837;&#21464;&#24471;&#26356;&#19981;&#29087;&#24713;&#65292;LLMs&#30340;&#36755;&#20986;&#20542;&#21521;&#20110;&#40664;&#35748;&#20026;"&#21547;&#31946;&#20854;&#35789;"&#30340;&#39044;&#27979;&#65292;&#20854;&#24418;&#24335;&#21463;&#24494;&#35843;&#25968;&#25454;&#20013;&#19981;&#29087;&#24713;&#31034;&#20363;&#30417;&#30563;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#20462;&#25913;&#36825;&#20123;&#31034;&#20363;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;LLM&#23545;&#19981;&#29087;&#24713;&#36755;&#20837;&#30340;&#39044;&#27979;&#65288;&#20363;&#22914;&#65292;&#25945;&#20250;&#23427;&#20204;&#35828;&#8220;&#25105;&#19981;&#30693;&#36947;&#8221;&#65289;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;RL&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#22870;&#21169;&#27169;&#22411;&#24187;&#35273;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#26356;&#21487;&#38752;&#22320;&#20943;&#36731;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MMLU&#19978;&#30340;&#22810;&#36873;QA&#20013;&#36827;&#34892;&#19968;&#31995;&#21015;&#21463;&#25511;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05612v1 Announce Type: cross  Abstract: Large language models (LLMs) have a tendency to generate plausible-sounding yet factually incorrect responses, especially when queried on unfamiliar concepts. In this work, we explore the underlying mechanisms that govern how finetuned LLMs hallucinate. Our investigation reveals an interesting pattern: as inputs become more unfamiliar, LLM outputs tend to default towards a ``hedged'' prediction, whose form is determined by how the unfamiliar examples in the finetuning data are supervised. Thus, by strategically modifying these examples' supervision, we can control LLM predictions for unfamiliar inputs (e.g., teach them to say ``I don't know''). Based on these principles, we develop an RL approach that more reliably mitigates hallucinations for long-form generation tasks, by tackling the challenges presented by reward model hallucinations. We validate our findings with a series of controlled experiments in multiple-choice QA on MMLU, as
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#29992;&#20110;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#35786;&#26029;&#33033;&#32476;&#33180;&#32959;&#30244;&#65292;&#20419;&#36827;&#20102;&#23545;&#32597;&#35265;&#30142;&#30149;&#30340;&#35786;&#26029;&#65292;&#24182;&#22312;&#20020;&#24202;&#23454;&#36341;&#21644;&#21307;&#23398;&#25945;&#32946;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;</title><link>https://arxiv.org/abs/2403.05606</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#29992;&#20110;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#35786;&#26029;&#33033;&#32476;&#33180;&#32959;&#30244;
&lt;/p&gt;
&lt;p&gt;
A Concept-based Interpretable Model for the Diagnosis of Choroid Neoplasias using Multimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05606
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#29992;&#20110;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#35786;&#26029;&#33033;&#32476;&#33180;&#32959;&#30244;&#65292;&#20419;&#36827;&#20102;&#23545;&#32597;&#35265;&#30142;&#30149;&#30340;&#35786;&#26029;&#65292;&#24182;&#22312;&#20020;&#24202;&#23454;&#36341;&#21644;&#21307;&#23398;&#25945;&#32946;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35786;&#26029;&#32597;&#35265;&#30142;&#30149;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#38754;&#20020;&#30528;&#20849;&#21516;&#25361;&#25112;&#65292;&#38656;&#35201;&#19987;&#23478;&#30340;&#19987;&#19994;&#30693;&#35782;&#25165;&#33021;&#20934;&#30830;&#35782;&#21035;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#36825;&#31867;&#25216;&#26415;&#30340;&#21457;&#23637;&#21463;&#21040;&#32597;&#35265;&#29366;&#20917;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#38656;&#35201;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#36182;&#24615;&#30340;&#27169;&#22411;&#30340;&#38656;&#27714;&#30340;&#38459;&#30861;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#20855;&#26377;&#20154;&#31867;&#21487;&#35835;&#36755;&#20986;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#20419;&#36827;&#20020;&#24202;&#21307;&#29983;&#30340;&#39564;&#35777;&#24182;&#20419;&#36827;&#21307;&#23398;&#25945;&#32946;&#12290;&#22312;&#24403;&#21069;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#33033;&#32476;&#33180;&#32959;&#30244;&#65292;&#36825;&#26159;&#25104;&#20154;&#20013;&#26368;&#24120;&#35265;&#30340;&#30524;&#30555;&#30284;&#30151;&#24418;&#24335;&#65292;&#23613;&#31649;&#32597;&#35265;&#65292;&#32633;&#24739;&#29575;&#20026;&#27599;&#30334;&#19975;&#20154;5.1&#20363;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#20849;&#21253;&#25324;750&#21517;&#24739;&#32773;&#65292;&#28085;&#30422;&#20102;&#20174;2004&#24180;&#33267;2022&#24180;&#25910;&#38598;&#30340;&#19977;&#31181;&#19981;&#21516;&#25104;&#20687;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#21487;&#21306;&#20998;&#19977;&#31181;&#31867;&#22411;&#30340;&#33033;&#32476;&#33180;&#32959;&#30244;&#65292;&#34701;&#21512;&#20102;&#19968;&#20123;&#19981;&#22826;&#37325;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05606v1 Announce Type: cross  Abstract: Diagnosing rare diseases presents a common challenge in clinical practice, necessitating the expertise of specialists for accurate identification. The advent of machine learning offers a promising solution, while the development of such technologies is hindered by the scarcity of data on rare conditions and the demand for models that are both interpretable and trustworthy in a clinical context. Interpretable AI, with its capacity for human-readable outputs, can facilitate validation by clinicians and contribute to medical education. In the current work, we focus on choroid neoplasias, the most prevalent form of eye cancer in adults, albeit rare with 5.1 per million. We built the so-far largest dataset consisting of 750 patients, incorporating three distinct imaging modalities collected from 2004 to 2022. Our work introduces a concept-based interpretable model that distinguishes between three types of choroidal tumors, integrating insig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26725;&#25509;&#37327;&#23376;&#21147;&#23398;&#21407;&#29702;&#19982;&#31038;&#20250;&#31995;&#32479;&#22797;&#26434;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22522;&#20110;&#30898;&#32435;&#31859;&#39063;&#31890;&#21644;&#30707;&#22696;&#28911;&#32467;&#26500;&#29305;&#24449;&#19982;&#31038;&#20250;&#32676;&#20307;&#34892;&#20026;&#27169;&#24335;&#30340;&#38544;&#21947;&#23545;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.05593</link><description>&lt;p&gt;
&#24341;&#20837;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#65306;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#29992;&#20110;&#32676;&#20307;&#21160;&#24577;&#21644;&#22312;TeNP-&#38142;&#31038;&#20250;&#21160;&#24577;&#27169;&#25311;&#20013;&#26550;&#35774;&#31038;&#20250;&#29616;&#35937;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Introducing First-Principles Calculations: New Approach to Group Dynamics and Bridging Social Phenomena in TeNP-Chain Based Social Dynamics Simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26725;&#25509;&#37327;&#23376;&#21147;&#23398;&#21407;&#29702;&#19982;&#31038;&#20250;&#31995;&#32479;&#22797;&#26434;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22522;&#20110;&#30898;&#32435;&#31859;&#39063;&#31890;&#21644;&#30707;&#22696;&#28911;&#32467;&#26500;&#29305;&#24449;&#19982;&#31038;&#20250;&#32676;&#20307;&#34892;&#20026;&#27169;&#24335;&#30340;&#38544;&#21947;&#23545;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#23398;&#31185;&#26041;&#27861;&#35770;&#65292;&#23427;&#26550;&#36215;&#20102;&#37327;&#23376;&#21147;&#23398;&#22522;&#26412;&#21407;&#29702;&#19982;&#26448;&#26009;&#30740;&#31350;&#65288;&#22914;&#30898;&#32435;&#31859;&#39063;&#31890;&#21644;&#30707;&#22696;&#28911;&#65289;&#20197;&#21450;&#31038;&#20250;&#31995;&#32479;&#22797;&#26434;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22522;&#30784;&#22312;&#20110;&#30898;&#32435;&#31859;&#39063;&#31890;&#21644;&#30707;&#22696;&#28911;&#30340;&#32467;&#26500;&#29305;&#24449;&#19982;&#31038;&#20250;&#32676;&#20307;&#34892;&#20026;&#27169;&#24335;&#20043;&#38388;&#30340;&#38544;&#21947;&#23545;&#24212;&#12290;&#30898;&#32435;&#31859;&#39063;&#31890;&#20855;&#26377;&#29420;&#29305;&#30340;&#23646;&#24615;&#65292;&#22914;&#25552;&#39640;&#30898;&#38142;&#20013;&#20849;&#20215;&#38190;&#30340;&#24378;&#21270;&#21644;&#24341;&#21457;&#27425;&#29983;&#32467;&#26500;&#30772;&#22351;&#23548;&#33268;&#36825;&#20123;&#38142;&#30340;&#20998;&#31163;&#12290;&#36825;&#31867;&#20284;&#20110;&#31038;&#20250;&#32676;&#20307;&#20869;&#37096;&#20957;&#32858;&#21147;&#21152;&#24378;&#21644;&#19981;&#21516;&#20998;&#32452;&#20043;&#38388;&#20449;&#24687;&#27969;&#21160;&#30340;&#24178;&#25200;&#12290;&#21516;&#26679;&#65292;&#30707;&#22696;&#28911;&#30340;&#20986;&#33394;&#29305;&#24615;&#65292;&#22914;&#39640;&#30005;&#23548;&#24615;&#12289;&#24378;&#24230;&#21644;&#26580;&#38887;&#24615;&#65292;&#20026;&#29702;&#35299;&#31038;&#20250;&#32676;&#20307;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#26356;&#22810;&#23618;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05593v1 Announce Type: cross  Abstract: This note considers an innovative interdisciplinary methodology that bridges the gap between the fundamental principles of quantum mechanics applied to the study of materials such as tellurium nanoparticles (TeNPs) and graphene and the complex dynamics of social systems. The basis for this approach lies in the metaphorical parallels drawn between the structural features of TeNPs and graphene and the behavioral patterns of social groups in the face of misinformation. TeNPs exhibit unique properties such as the strengthening of covalent bonds within telluric chains and the disruption of secondary structure leading to the separation of these chains. This is analogous to increased cohesion within social groups and disruption of information flow between different subgroups, respectively. . Similarly, the outstanding properties of graphene, such as high electrical conductivity, strength, and flexibility, provide additional aspects for unders
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#20986;&#29616;&#65292;&#20294;&#22312;&#22788;&#29702;&#36825;&#20123;&#20154;&#24037;&#22823;&#33041;&#26102;&#65292;&#22914;&#20309;&#20445;&#25252;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05592</link><description>&lt;p&gt;
&#26426;&#26800;&#22836;&#33041;&#30340;&#27704;&#24658;&#38451;&#20809;&#65306;&#26426;&#22120;&#23398;&#20064;&#19982;&#34987;&#36951;&#24536;&#26435;&#30340;&#19981;&#21487;&#35843;&#21644;&#24615;
&lt;/p&gt;
&lt;p&gt;
Eternal Sunshine of the Mechanical Mind: The Irreconcilability of Machine Learning and the Right to be Forgotten
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05592
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#20986;&#29616;&#65292;&#20294;&#22312;&#22788;&#29702;&#36825;&#20123;&#20154;&#24037;&#22823;&#33041;&#26102;&#65292;&#22914;&#20309;&#20445;&#25252;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25105;&#20204;&#36805;&#36895;&#36808;&#21521;&#20154;&#24037;&#26234;&#33021;&#25104;&#20026;&#22823;&#22810;&#25968;&#20154;&#19981;&#26029;&#21457;&#29983;&#21644;&#35268;&#33539;&#21270;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#20063;&#24517;&#39035;&#24847;&#35782;&#21040;&#36825;&#19968;&#24895;&#26223;&#21644;&#36825;&#19968;&#36827;&#27493;&#34164;&#21547;&#30528;&#20160;&#20040;&#12290;&#36890;&#36807;&#39318;&#20808;&#36817;&#20284;&#35745;&#31639;&#26426;&#30005;&#36335;&#20013;&#30340;&#31070;&#32463;&#36830;&#25509;&#21644;&#27963;&#21160;&#65292;&#28982;&#21518;&#21019;&#24314;&#36234;&#26469;&#36234;&#22797;&#26434;&#29256;&#26412;&#30340;&#36825;&#31181;&#31895;&#31961;&#36817;&#20284;&#65292;&#25105;&#20204;&#29616;&#22312;&#27491;&#38754;&#20020;&#19968;&#20010;&#26410;&#26469;&#26102;&#20195;&#65292;&#29616;&#20195;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#34987;&#31216;&#20026;&#24605;&#32771;&#26426;&#22120;&#65292;&#26377;&#26102;&#29978;&#33267;&#22240;&#20854;&#26032;&#29983;&#34892;&#20026;&#21644;&#40657;&#30418;&#26041;&#27861;&#32780;&#21463;&#21040;&#31216;&#36190;&#12290;&#20294;&#38543;&#30528;&#25105;&#20204;&#21019;&#36896;&#26356;&#24378;&#22823;&#30340;&#30005;&#23376;&#22823;&#33041;&#65292;&#20855;&#26377;&#25968;&#21313;&#20159;&#30340;&#31070;&#32463;&#36830;&#25509;&#21644;&#21442;&#25968;&#65292;&#25105;&#20204;&#33021;&#30830;&#20445;&#36825;&#20123;&#30001;&#20154;&#24037;&#31070;&#32463;&#26500;&#24314;&#30340;&#24222;&#28982;&#22823;&#29289;&#33021;&#22815;&#36951;&#24536;&#25105;&#20204;&#22312;&#20854;&#20013;&#23384;&#20648;&#30340;&#25968;&#25454;&#21527;&#65311;&#22914;&#26524;&#23427;&#20204;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#20687;&#22823;&#33041;&#65292;&#37027;&#20040;&#22312;&#24212;&#23545;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#26102;&#65292;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#20173;&#33021;&#24471;&#21040;&#20445;&#25252;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05592v1 Announce Type: cross  Abstract: As we keep rapidly advancing toward an era where artificial intelligence is a constant and normative experience for most of us, we must also be aware of what this vision and this progress entail. By first approximating neural connections and activities in computer circuits and then creating more and more sophisticated versions of this crude approximation, we are now facing an age to come where modern deep learning-based artificial intelligence systems can rightly be called thinking machines, and they are sometimes even lauded for their emergent behavior and black-box approaches. But as we create more powerful electronic brains, with billions of neural connections and parameters, can we guarantee that these mammoths built of artificial neurons will be able to forget the data that we store in them? If they are at some level like a brain, can the right to be forgotten still be protected while dealing with these AIs? The essential gap betw
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#22522;&#20110;&#20154;&#20307;&#27979;&#37327;&#30340;&#23478;&#20855;&#23610;&#23544;&#36866;&#21512;&#22823;&#23398;&#29983;&#65292;&#20197;&#25913;&#21892;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#20154;&#20307;&#24037;&#31243;&#23398;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#19982;&#29616;&#26377;&#23478;&#20855;&#23610;&#23544;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#24182;&#26356;&#21152;&#20860;&#23481;</title><link>https://arxiv.org/abs/2403.05589</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#23398;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#20154;&#20307;&#24037;&#31243;&#23398;&#65306;&#19968;&#20010;&#20851;&#20110;&#20154;&#20307;&#27979;&#37327;&#12289;&#23478;&#20855;&#35774;&#35745;&#21644;ANOVA&#27979;&#35797;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimizing Computer Lab Ergonomics in Universities: A Study on Anthropometric Measurements, Furniture Design, and ANOVA Test
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#22522;&#20110;&#20154;&#20307;&#27979;&#37327;&#30340;&#23478;&#20855;&#23610;&#23544;&#36866;&#21512;&#22823;&#23398;&#29983;&#65292;&#20197;&#25913;&#21892;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#20154;&#20307;&#24037;&#31243;&#23398;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#19982;&#29616;&#26377;&#23478;&#20855;&#23610;&#23544;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#24182;&#26356;&#21152;&#20860;&#23481;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#20307;&#24037;&#31243;&#23398;&#35774;&#35745;&#30340;&#23478;&#20855;&#33021;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#21644;&#36523;&#24515;&#20581;&#24247;&#12290;&#38543;&#30528;&#35745;&#31639;&#26426;&#25104;&#20026;&#23398;&#29983;&#23398;&#26415;&#29983;&#27963;&#30340;&#19968;&#37096;&#20998;&#65292;&#23427;&#20204;&#22312;&#26410;&#26469;&#23558;&#36827;&#19968;&#27493;&#26222;&#21450;&#12290;&#25105;&#20204;&#25552;&#20986;&#22522;&#20110;&#20154;&#20307;&#27979;&#37327;&#30340;&#23478;&#20855;&#23610;&#23544;&#65292;&#36866;&#21512;&#22823;&#23398;&#29983;&#20197;&#25913;&#21892;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#30340;&#20154;&#20307;&#24037;&#31243;&#23398;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;380&#21517;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#65292;&#20998;&#26512;&#20102;11&#39033;&#20154;&#20307;&#27979;&#37327;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;11&#39033;&#23478;&#20855;&#23610;&#23544;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#23478;&#20855;&#65306;&#38750;&#21487;&#35843;&#26885;&#23376;&#19982;&#38750;&#21487;&#35843;&#26700;&#23376;&#65292;&#20197;&#21450;&#21487;&#35843;&#26885;&#23376;&#19982;&#38750;&#21487;&#35843;&#26700;&#23376;&#12290;&#19981;&#21305;&#37197;&#35745;&#31639;&#26174;&#31034;&#23478;&#20855;&#23610;&#23544;&#19982;&#20154;&#20307;&#27979;&#37327;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#26174;&#33879;&#27700;&#24179;&#20026;5%&#30340;&#21333;&#22240;&#32032;&#26041;&#24046;&#20998;&#26512;&#27979;&#35797;&#36824;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#21644;&#29616;&#26377;&#30340;&#23478;&#20855;&#23610;&#23544;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#23610;&#23544;&#26356;&#21152;&#20860;&#23481;&#65292;&#20943;&#23569;&#20102;&#19981;&#21305;&#37197;&#30334;&#20998;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05589v1 Announce Type: cross  Abstract: Many studies have shown how ergonomically designed furniture improves productivity and well-being. As computers have become a part of students' academic lives, they will grow further in the future. We propose anthropometric-based furniture dimensions suitable for university students to improve computer laboratory ergonomics. We collected data from 380 participants and analyzed 11 anthropometric measurements, correlating them to 11 furniture dimensions. Two types of furniture were studied: a non-adjustable chair with a non-adjustable table and an adjustable chair with a non-adjustable table. The mismatch calculation showed a significant difference between furniture dimensions and anthropometric measurements. The one-way ANOVA test with a significance level of 5% also showed a significant difference between proposed and existing furniture dimensions. The proposed dimensions were found to be more compatible and reduced mismatch percentage
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#23396;&#23376;&#35299;&#26041;&#27861;&#30740;&#31350;&#20102;&#20551;&#26032;&#38395;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#25506;&#35752;&#20102;&#21508;&#31867;&#21160;&#32773;&#22312;&#31995;&#32479;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26088;&#22312;&#29702;&#35299;&#21644;&#38450;&#27490;&#20551;&#26032;&#38395;&#25193;&#25955;&#12290;</title><link>https://arxiv.org/abs/2403.05585</link><description>&lt;p&gt;
&#24748;&#28014;&#22797;&#20849;&#25391;&#27169;&#22411;&#65306;&#20351;&#29992;&#23396;&#23376;&#35299;&#30740;&#31350;&#31532;&#19977;&#26041;&#24178;&#39044;&#19979;&#38750;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#20551;&#26032;&#38395;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Plasmon Resonance Model: Investigation of Analysis of Fake News Diffusion Model with Third Mover Intervention Using Soliton Solution in Non-Complete Information Game under Repeated Dilemma Condition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05585
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#23396;&#23376;&#35299;&#26041;&#27861;&#30740;&#31350;&#20102;&#20551;&#26032;&#38395;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#25506;&#35752;&#20102;&#21508;&#31867;&#21160;&#32773;&#22312;&#31995;&#32479;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26088;&#22312;&#29702;&#35299;&#21644;&#38450;&#27490;&#20551;&#26032;&#38395;&#25193;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#30740;&#31350;&#35828;&#26126;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#27169;&#25311;&#34394;&#20551;&#26032;&#38395;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#26694;&#26550;&#20869;&#30340;&#25193;&#25955;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#26469;&#34920;&#31034;&#24748;&#28014;&#22797;&#20849;&#25391;&#29616;&#35937;&#65292;&#20854;&#20013;&#34394;&#20551;&#26032;&#38395;&#30340;&#25193;&#25955;&#22312;&#29305;&#23450;&#31038;&#20132;&#32676;&#20307;&#25110;&#36890;&#20449;&#32593;&#32476;&#20869;&#34987;&#36805;&#36895;&#25918;&#22823;&#65292;&#24182;&#36890;&#36807;&#23396;&#23376;&#35299;&#26041;&#27861;&#20998;&#26512;&#20854;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#22836;&#37096;&#21160;&#32773;&#12289;&#27425;&#22836;&#21160;&#32773;&#21644;&#31532;&#19977;&#26041;&#24178;&#39044;&#31574;&#30053;&#22914;&#20309;&#22312;&#36825;&#20010;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23545;&#34394;&#20551;&#26032;&#38395;&#30340;&#25193;&#25955;&#36215;&#21040;&#25918;&#22823;&#25110;&#25233;&#21046;&#30340;&#20316;&#29992;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#29702;&#35299;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#26426;&#21046;&#65292;&#24182;&#25552;&#20379;&#38450;&#27490;&#25110;&#25171;&#20987;&#20854;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#32467;&#21512;&#31038;&#20250;&#31185;&#23398;&#21644;&#33258;&#28982;&#31185;&#23398;&#30340;&#27010;&#24565;&#65292;&#26412;&#30740;&#31350;&#35797;&#22270;&#20026;&#24403;&#20170;&#30340;&#20551;&#26032;&#38395;&#38382;&#39064;&#24320;&#21457;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05585v1 Announce Type: cross  Abstract: In this research note, we propose a new approach to model the fake news diffusion process within the framework of incomplete information games. In particular, we use nonlinear partial differential equations to represent the phenomenon of plasmon resonance, in which the diffusion of fake news is rapidly amplified within a particular social group or communication network, and analyze its dynamics through a soliton solution approach. In addition, we consider how first mover, second mover, and third mover strategies interact within this nonlinear system and contribute to the amplification or suppression of fake news diffusion. The model aims to understand the mechanisms of fake news proliferation and provide insights into how to prevent or combat it. By combining concepts from the social sciences and the physical sciences, this study attempts to develop a new theoretical framework for the contemporary problem of fake news.
&lt;/p&gt;</description></item><item><title>Time2Stop&#24320;&#21457;&#20102;&#19968;&#31181;&#26234;&#33021;&#12289;&#33258;&#36866;&#24212;&#21644;&#21487;&#35299;&#37322;&#30340;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#31995;&#32479;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30830;&#23450;&#26368;&#20339;&#24178;&#39044;&#26102;&#26426;&#65292;&#24341;&#20837;&#36879;&#26126;&#30340;AI&#35299;&#37322;&#36827;&#34892;&#24178;&#39044;&#65292;&#24182;&#36890;&#36807;&#25910;&#38598;&#29992;&#25143;&#21453;&#39304;&#24314;&#31435;&#20154;-AI&#24490;&#29615;&#12290;</title><link>https://arxiv.org/abs/2403.05584</link><description>&lt;p&gt;
Time2Stop&#65306;&#33258;&#36866;&#24212;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#24490;&#29615;&#31995;&#32479;&#65292;&#29992;&#20110;&#24178;&#39044;&#26234;&#33021;&#25163;&#26426;&#36807;&#24230;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Time2Stop: Adaptive and Explainable Human-AI Loop for Smartphone Overuse Intervention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05584
&lt;/p&gt;
&lt;p&gt;
Time2Stop&#24320;&#21457;&#20102;&#19968;&#31181;&#26234;&#33021;&#12289;&#33258;&#36866;&#24212;&#21644;&#21487;&#35299;&#37322;&#30340;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#31995;&#32479;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30830;&#23450;&#26368;&#20339;&#24178;&#39044;&#26102;&#26426;&#65292;&#24341;&#20837;&#36879;&#26126;&#30340;AI&#35299;&#37322;&#36827;&#34892;&#24178;&#39044;&#65292;&#24182;&#36890;&#36807;&#25910;&#38598;&#29992;&#25143;&#21453;&#39304;&#24314;&#31435;&#20154;-AI&#24490;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#24178;&#39044;&#26234;&#33021;&#25163;&#26426;&#36807;&#24230;&#20351;&#29992;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#65292;&#20294;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#65288;JITAI&#65289;&#26041;&#27861;&#20173;&#28982;&#32570;&#20047;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;Time2Stop&#65292;&#36825;&#26159;&#19968;&#20010;&#26234;&#33021;&#12289;&#33258;&#36866;&#24212;&#21644;&#21487;&#35299;&#37322;&#30340;JITAI&#31995;&#32479;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#30830;&#23450;&#26368;&#20339;&#24178;&#39044;&#26102;&#26426;&#65292;&#24341;&#20837;&#36879;&#26126;&#30340;AI&#35299;&#37322;&#36827;&#34892;&#24178;&#39044;&#65292;&#24182;&#25910;&#38598;&#29992;&#25143;&#21453;&#39304;&#20197;&#24314;&#31435;&#20154;&#24037;&#26234;&#33021;&#24490;&#29615;&#65292;&#24182;&#38543;&#26102;&#38388;&#35843;&#25972;&#24178;&#39044;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20026;&#26399;8&#21608;&#30340;&#29616;&#22330;&#23454;&#39564;&#65288;N=71&#65289;&#26469;&#35780;&#20272;Time2Stop&#30340;&#33258;&#36866;&#24212;&#21644;&#35299;&#37322;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#22312;&#24178;&#39044;&#20934;&#30830;&#24615;&#65288;&gt;32.8\%&#30456;&#23545;&#65289;&#21644;&#25509;&#21463;&#24230;&#65288;&gt;8.0\%&#65289;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#32435;&#20837;&#35299;&#37322;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#65292;&#20998;&#21035;&#20026;&#20934;&#30830;&#24615;&#21644;&#25509;&#21463;&#24230;&#25552;&#39640;&#20102;53.8\%&#21644;11.4\%&#12290;&#27492;&#22806;&#65292;Time2Stop
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05584v1 Announce Type: cross  Abstract: Despite a rich history of investigating smartphone overuse intervention techniques, AI-based just-in-time adaptive intervention (JITAI) methods for overuse reduction are lacking. We develop Time2Stop, an intelligent, adaptive, and explainable JITAI system that leverages machine learning to identify optimal intervention timings, introduces interventions with transparent AI explanations, and collects user feedback to establish a human-AI loop and adapt the intervention model over time. We conducted an 8-week field experiment (N=71) to evaluate the effectiveness of both the adaptation and explanation aspects of Time2Stop. Our results indicate that our adaptive models significantly outperform the baseline methods on intervention accuracy (&gt;32.8\% relatively) and receptivity (&gt;8.0\%). In addition, incorporating explanations further enhances the effectiveness by 53.8\% and 11.4\% on accuracy and receptivity, respectively. Moreover, Time2Stop
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#36328;&#27169;&#24577;&#23545;&#40784;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Multimodal Orofacial Neural Audio&#31995;&#32479;&#65292;&#25104;&#21151;&#20943;&#23569;&#38745;&#40664;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#35789;&#38169;&#35823;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.05583</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;LLM&#22686;&#24378;&#35782;&#21035;&#30340;&#36328;&#27169;&#24577;&#38745;&#40664;&#35821;&#38899;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Cross-Modal Approach to Silent Speech with LLM-Enhanced Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05583
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#36328;&#27169;&#24577;&#23545;&#40784;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Multimodal Orofacial Neural Audio&#31995;&#32479;&#65292;&#25104;&#21151;&#20943;&#23569;&#38745;&#40664;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#35789;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38745;&#40664;&#35821;&#38899;&#30028;&#38754;&#65288;SSIs&#65289;&#20026;&#26080;&#22768;&#21475;&#22836;&#20132;&#27969;&#25552;&#20379;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#30456;&#36739;&#20110;&#33041;&#26426;&#25509;&#21475;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Multimodal Orofacial Neural Audio&#65288;MONA&#65289;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#8212;&#8212;&#36328;&#23545;&#27604;&#65288;crossCon&#65289;&#21644;&#30417;&#30563;&#26102;&#38388;&#23545;&#27604;&#65288;supTcon&#65289;&#21033;&#29992;&#36328;&#27169;&#24577;&#23545;&#40784;&#26469;&#35757;&#32451;&#20855;&#26377;&#20849;&#20139;&#28508;&#22312;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#36825;&#31181;&#26550;&#26500;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#31867;&#20284;LibriSpeech&#30340;&#20165;&#38899;&#39057;&#25968;&#25454;&#38598;&#26469;&#25913;&#21892;&#38745;&#40664;&#35821;&#38899;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25972;&#21512;&#35780;&#20998;&#35843;&#25972;&#65288;LISA&#65289;&#26174;&#33879;&#25552;&#39640;&#20102;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#32508;&#21512;&#32780;&#35328;&#65292;MONA LISA&#22312;Gaddy&#65288;2020&#24180;&#65289;&#38745;&#40664;&#35821;&#38899;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23558;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20174;28.8%&#38477;&#33267;12.2%&#65292;&#24182;&#19988;&#22312;&#24320;&#25918;&#35789;&#27719;&#34920;&#19978;&#36827;&#34892;&#20102;&#38745;&#40664;&#35821;&#38899;&#30340;&#25913;&#36827;&#12290;&#23545;&#20110;&#22768;&#38899;EMG&#35760;&#24405;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#26368;&#20808;&#36827;&#30340;&#35782;&#21035;&#29575;&#20174;23.3%&#25552;&#39640;&#21040;3.7% WER&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05583v1 Announce Type: cross  Abstract: Silent Speech Interfaces (SSIs) offer a noninvasive alternative to brain-computer interfaces for soundless verbal communication. We introduce Multimodal Orofacial Neural Audio (MONA), a system that leverages cross-modal alignment through novel loss functions--cross-contrast (crossCon) and supervised temporal contrast (supTcon)--to train a multimodal model with a shared latent representation. This architecture enables the use of audio-only datasets like LibriSpeech to improve silent speech recognition. Additionally, our introduction of Large Language Model (LLM) Integrated Scoring Adjustment (LISA) significantly improves recognition accuracy. Together, MONA LISA reduces the state-of-the-art word error rate (WER) from 28.8% to 12.2% in the Gaddy (2020) benchmark dataset for silent speech on an open vocabulary. For vocal EMG recordings, our method improves the state-of-the-art from 23.3% to 3.7% WER. In the Brain-to-Text 2024 competition,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24067;&#23616;&#26159;&#21542;&#20250;&#24433;&#21709;&#21442;&#19982;&#32773;&#23545;&#21253;&#21547;&#20167;&#24680;&#35328;&#35770;&#21477;&#23376;&#30340;&#35780;&#20215;&#65292;&#32467;&#26524;&#34920;&#26126;&#35299;&#37322;&#24067;&#23616;&#22312;&#35302;&#21457;&#21442;&#19982;&#32773;&#25552;&#20379;&#32416;&#27491;&#24615;&#21453;&#39304;&#21644;&#35780;&#20272;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;</title><link>https://arxiv.org/abs/2403.05581</link><description>&lt;p&gt;
&#35299;&#37322;&#24067;&#23616;&#21487;&#20197;&#24433;&#21709;&#20154;&#23545;&#20882;&#29359;&#24615;&#21477;&#23376;&#30340;&#24863;&#30693;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Interpretability Layouts Influence Human Perception of Offensive Sentences?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24067;&#23616;&#26159;&#21542;&#20250;&#24433;&#21709;&#21442;&#19982;&#32773;&#23545;&#21253;&#21547;&#20167;&#24680;&#35328;&#35770;&#21477;&#23376;&#30340;&#35780;&#20215;&#65292;&#32467;&#26524;&#34920;&#26126;&#35299;&#37322;&#24067;&#23616;&#22312;&#35302;&#21457;&#21442;&#19982;&#32773;&#25552;&#20379;&#32416;&#27491;&#24615;&#21453;&#39304;&#21644;&#35780;&#20272;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36827;&#34892;&#20102;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35299;&#37322;&#24067;&#23616;&#26159;&#21542;&#20250;&#24433;&#21709;&#21442;&#19982;&#32773;&#35780;&#20272;&#21253;&#21547;&#20167;&#24680;&#35328;&#35770;&#30340;&#21477;&#23376;&#26102;&#30340;&#35266;&#28857;&#65292;&#37325;&#28857;&#20851;&#27880;&#8220;&#21388;&#24694;&#22899;&#24615;&#8221;&#21644;&#8220;&#31181;&#26063;&#20027;&#20041;&#8221;&#20004;&#31867;&#12290;&#37492;&#20110;&#25991;&#29486;&#20013;&#23384;&#22312;&#20998;&#27495;&#30340;&#32467;&#35770;&#65292;&#25105;&#20204;&#36890;&#36807;&#32479;&#35745;&#21644;&#23450;&#24615;&#20998;&#26512;&#38382;&#21367;&#35843;&#26597;&#22238;&#24212;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#25506;&#35752;&#22312;&#22312;&#32447;&#31038;&#21306;&#20013;&#20351;&#29992;ML&#35299;&#37322;&#24615;&#30340;&#20248;&#21183;&#12290;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#20272;&#35745;&#21442;&#19982;&#32773;&#30340;&#35780;&#32423;&#65292;&#34701;&#21512;&#20102;&#32452;&#20869;&#35774;&#35745;&#21644;&#32452;&#38388;&#35774;&#35745;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#32479;&#35745;&#20998;&#26512;&#34920;&#26126;&#65292;&#27809;&#26377;&#20219;&#20309;&#35299;&#37322;&#24067;&#23616;&#26174;&#33879;&#24433;&#21709;&#21442;&#19982;&#32773;&#30340;&#35266;&#28857;&#65292;&#20294;&#25105;&#20204;&#30340;&#23450;&#24615;&#20998;&#26512;&#34920;&#26126;ML&#35299;&#37322;&#24615;&#30340;&#20248;&#21183;&#65306;1&#65289;&#35302;&#21457;&#21442;&#19982;&#32773;&#22312;&#20182;&#20204;&#30340;&#35266;&#28857;&#19982;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#26102;&#25552;&#20379;&#32416;&#27491;&#24615;&#21453;&#39304;&#65292;2&#65289;&#25552;&#20379;&#35780;&#20272;&#27169;&#22411;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05581v1 Announce Type: cross  Abstract: This paper conducts a user study to assess whether three machine learning (ML) interpretability layouts can influence participants' views when evaluating sentences containing hate speech, focusing on the "Misogyny" and "Racism" classes. Given the existence of divergent conclusions in the literature, we provide empirical evidence on using ML interpretability in online communities through statistical and qualitative analyses of questionnaire responses. The Generalized Additive Model estimates participants' ratings, incorporating within-subject and between-subject designs. While our statistical analysis indicates that none of the interpretability layouts significantly influences participants' views, our qualitative analysis demonstrates the advantages of ML interpretability: 1) triggering participants to provide corrective feedback in case of discrepancies between their views and the model, and 2) providing insights to evaluate a model's 
&lt;/p&gt;</description></item><item><title>XAI&#30740;&#31350;&#26222;&#36941;&#24573;&#35270;&#20102;&#28508;&#22312;&#30340;&#25991;&#21270;&#24046;&#24322;&#65292;&#36807;&#20110;&#20551;&#35774;&#35199;&#26041;&#35299;&#37322;&#38656;&#27714;&#22312;&#20840;&#29699;&#36890;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.05579</link><description>&lt;p&gt;
Explainable AI&#30740;&#31350;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cultural Bias in Explainable AI Research: A Systematic Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05579
&lt;/p&gt;
&lt;p&gt;
XAI&#30740;&#31350;&#26222;&#36941;&#24573;&#35270;&#20102;&#28508;&#22312;&#30340;&#25991;&#21270;&#24046;&#24322;&#65292;&#36807;&#20110;&#20551;&#35774;&#35199;&#26041;&#35299;&#37322;&#38656;&#27714;&#22312;&#20840;&#29699;&#36890;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#26356;&#21152;&#26377;&#25928;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#36890;&#24120;&#38656;&#35201;&#21521;&#20154;&#31867;&#35299;&#37322;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31995;&#32479;&#36890;&#24120;&#22312;&#20154;&#31867;&#29992;&#25143;&#30740;&#31350;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;XAI&#30740;&#31350;&#20154;&#21592;&#26159;&#21542;&#32771;&#34385;&#20102;&#20154;&#31867;&#35299;&#37322;&#38656;&#27714;&#20013;&#30340;&#25991;&#21270;&#24046;&#24322;&#20173;&#26410;&#34987;&#25506;&#35752;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#39033;&#24515;&#29702;&#30740;&#31350;&#21457;&#29616;&#65292;&#26222;&#36941;&#26469;&#33258;&#35199;&#26041;&#20010;&#20154;&#20027;&#20041;&#22269;&#23478;&#30340;&#20154;&#20204;&#21644;&#32463;&#24120;&#26469;&#33258;&#38750;&#35199;&#26041;&#38598;&#20307;&#20027;&#20041;&#22269;&#23478;&#30340;&#20154;&#20204;&#22312;&#35299;&#37322;&#19978;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;XAI&#30740;&#31350;&#30446;&#21069;&#24573;&#35270;&#20102;&#36825;&#20123;&#24046;&#24322;&#65292;&#24182;&#19988;&#35768;&#22810;&#27969;&#34892;&#30340;XAI&#35774;&#35745;&#38544;&#21547;&#22320;&#19988;&#38382;&#39064;&#22320;&#20551;&#35774;&#35199;&#26041;&#30340;&#35299;&#37322;&#38656;&#27714;&#22312;&#36328;&#25991;&#21270;&#20013;&#26159;&#20849;&#20139;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31995;&#32479;&#24615;&#22320;&#23457;&#26597;&#20102;&#36229;&#36807;200&#20010;XAI&#29992;&#25143;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#30740;&#31350;&#26410;&#32771;&#34385;&#30456;&#20851;&#30340;&#25991;&#21270;&#21464;&#21270;&#65292;&#21482;&#23545;&#35199;&#26041;&#20154;&#21475;&#36827;&#34892;&#20102;&#21462;&#26679;&#65292;&#20294;&#23545;&#20154;&#31867;&#30340;&#32467;&#35770;&#21462;&#24471;&#30340;&#26159;&#20851;&#20110;&#20154;&#31867;&#25972;&#20307;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05579v1 Announce Type: cross  Abstract: For synergistic interactions between humans and artificial intelligence (AI) systems, AI outputs often need to be explainable to people. Explainable AI (XAI) systems are commonly tested in human user studies. However, whether XAI researchers consider potential cultural differences in human explanatory needs remains unexplored. We highlight psychological research that found significant differences in human explanations between many people from Western, commonly individualist countries and people from non-Western, often collectivist countries. We argue that XAI research currently overlooks these variations and that many popular XAI designs implicitly and problematically assume that Western explanatory needs are shared cross-culturally. Additionally, we systematically reviewed over 200 XAI user studies and found that most studies did not consider relevant cultural variations, sampled only Western populations, but drew conclusions about hu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20010;&#24615;&#21270;&#32593;&#39029;&#27178;&#24133;&#65292;&#26681;&#25454;&#29992;&#25143;&#20114;&#21160;&#21160;&#24577;&#20869;&#23481;&#65292;&#24182;&#19988;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;</title><link>https://arxiv.org/abs/2403.05578</link><description>&lt;p&gt;
&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20018;&#32852;&#65306;&#29983;&#25104;&#20010;&#24615;&#21270;&#30005;&#23376;&#21830;&#21153;&#27178;&#24133;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chaining text-to-image and large language model: A novel approach for generating personalized e-commerce banners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20010;&#24615;&#21270;&#32593;&#39029;&#27178;&#24133;&#65292;&#26681;&#25454;&#29992;&#25143;&#20114;&#21160;&#21160;&#24577;&#20869;&#23481;&#65292;&#24182;&#19988;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05578v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#31283;&#23450;&#25193;&#25955;&#31561;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20026;&#29983;&#25104;&#33402;&#26415;&#20316;&#21697;&#24320;&#36767;&#20102;&#22823;&#37327;&#26426;&#20250;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22312;&#22686;&#24378;&#35768;&#22810;&#21019;&#24847;&#33402;&#26415;&#23478;&#24037;&#20316;&#20013;&#30340;&#24212;&#29992;&#12290;&#35768;&#22810;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#37319;&#29992;&#25163;&#21160;&#27969;&#31243;&#29983;&#25104;&#27178;&#24133;&#65292;&#36825;&#26159;&#32791;&#26102;&#30340;&#19988;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#26681;&#25454;&#22312;&#32447;&#36141;&#29289;&#32773;&#30340;&#20114;&#21160;&#29983;&#25104;&#20855;&#26377;&#21160;&#24577;&#20869;&#23481;&#30340;&#20010;&#24615;&#21270;&#32593;&#39029;&#27178;&#24133;&#30340;&#29992;&#36884;&#12290;&#27492;&#26041;&#27861;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#22312;&#27809;&#26377;&#20154;&#20026;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#23558;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#36716;&#25442;&#20026;&#26377;&#24847;&#20041;&#30340;&#25552;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#31995;&#32479;&#22320;&#20174;&#39033;&#30446;&#20803;&#20449;&#24687;&#20013;&#25552;&#21462;&#23646;&#24615;&#20803;&#32452;&#12290;&#28982;&#21518;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#36825;&#20123;&#23646;&#24615;&#20256;&#36882;&#32473;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#27178;&#24133;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#21019;&#24314;&#39640;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05578v1 Announce Type: cross  Abstract: Text-to-image models such as stable diffusion have opened a plethora of opportunities for generating art. Recent literature has surveyed the use of text-to-image models for enhancing the work of many creative artists. Many e-commerce platforms employ a manual process to generate the banners, which is time-consuming and has limitations of scalability. In this work, we demonstrate the use of text-to-image models for generating personalized web banners with dynamic content for online shoppers based on their interactions. The novelty in this approach lies in converting users' interaction data to meaningful prompts without human intervention. To this end, we utilize a large language model (LLM) to systematically extract a tuple of attributes from item meta-information. The attributes are then passed to a text-to-image model via prompt engineering to generate images for the banner. Our results show that the proposed approach can create high-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#25311;&#19981;&#21516;&#28608;&#21169;&#32972;&#26223;&#19979;&#30340;&#20027;&#35266;&#24615;&#22240;&#32032;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#35780;&#20272;&#22270;&#29255;&#26102;&#65292;&#20027;&#35266;&#24615;&#21463;&#22270;&#20687;&#22806;&#35266;&#12289;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#20197;&#21450;&#25991;&#26412;&#20013;&#25552;&#21450;&#23545;&#35937;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#25506;&#31350;&#20027;&#35266;&#24615;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05576</link><description>&lt;p&gt;
&#36890;&#36807;&#28608;&#21169;&#32972;&#26223;&#29702;&#35299;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#28385;&#24847;&#24230;&#20013;&#30340;&#20027;&#35266;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Subjectivity through the Lens of Motivational Context in Model-Generated Image Satisfaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05576
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#19981;&#21516;&#28608;&#21169;&#32972;&#26223;&#19979;&#30340;&#20027;&#35266;&#24615;&#22240;&#32032;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#35780;&#20272;&#22270;&#29255;&#26102;&#65292;&#20027;&#35266;&#24615;&#21463;&#22270;&#20687;&#22806;&#35266;&#12289;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#20197;&#21450;&#25991;&#26412;&#20013;&#25552;&#21450;&#23545;&#35937;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#25506;&#31350;&#20027;&#35266;&#24615;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26377;&#26395;&#25104;&#20026;&#26222;&#36941;&#23384;&#22312;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20154;&#31867;&#30340;&#36136;&#37327;&#35780;&#20215;&#36827;&#34892;&#24494;&#35843;&#21644;&#35780;&#20272;&#65292;&#32780;&#36825;&#20123;&#35780;&#20215;&#20551;&#23450;&#20102;&#19968;&#20010;&#26222;&#36941;&#26631;&#20934;&#65292;&#26410;&#33021;&#32771;&#34385;&#21040;&#36825;&#31867;&#20219;&#21153;&#30340;&#20027;&#35266;&#24615;&#12290;&#20026;&#20102;&#30740;&#31350;&#22914;&#20309;&#37327;&#21270;&#20027;&#35266;&#24615;&#21450;&#20854;&#24433;&#21709;&#35268;&#27169;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#19981;&#21516;&#20351;&#29992;&#24773;&#20917;&#19979;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#35780;&#20272;&#24046;&#24322;&#12290;&#36890;&#36807;&#27169;&#25311;&#27880;&#37322;&#32773;&#20027;&#35266;&#24615;&#21407;&#26412;&#30340;&#28508;&#22312;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#32452;&#21160;&#26426;&#65288;T&#24676;&#22270;&#26696;&#12289;&#28436;&#31034;&#25991;&#31295;&#35270;&#35273;&#21644;&#25163;&#26426;&#32972;&#26223;&#22270;&#20687;&#65289;&#26469;&#20026;&#19968;&#32452;&#20247;&#21253;&#20219;&#21153;&#25552;&#20379;&#35821;&#22659;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#20687;&#30340;&#20154;&#31867;&#35780;&#20272;&#22312;&#19981;&#21516;&#35821;&#22659;&#20013;&#26377;&#25152;&#19981;&#21516;&#65292;&#20197;&#21450;&#19981;&#21516;&#35821;&#22659;&#32452;&#21512;&#20043;&#38388;&#20063;&#26377;&#25152;&#24046;&#24322;&#12290;&#24433;&#21709;&#36825;&#31181;&#20027;&#35266;&#24615;&#30340;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#22270;&#20687;&#22806;&#35266;&#12289;&#22270;&#20687;&#19982;&#25991;&#26412;&#30340;&#23545;&#40784;&#20197;&#21450;&#25991;&#26412;&#20013;&#25552;&#21450;&#30340;&#23545;&#35937;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20984;&#26174;&#20102;&#32771;&#34385;&#20027;&#35266;&#24615;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#33719;&#21462;&#20027;&#35266;&#24615;&#25968;&#25454;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05576v1 Announce Type: cross  Abstract: Image generation models are poised to become ubiquitous in a range of applications. These models are often fine-tuned and evaluated using human quality judgments that assume a universal standard, failing to consider the subjectivity of such tasks. To investigate how to quantify subjectivity, and the scale of its impact, we measure how assessments differ among human annotators across different use cases. Simulating the effects of ordinarily latent elements of annotators subjectivity, we contrive a set of motivations (t-shirt graphics, presentation visuals, and phone background images) to contextualize a set of crowdsourcing tasks. Our results show that human evaluations of images vary within individual contexts and across combinations of contexts. Three key factors affecting this subjectivity are image appearance, image alignment with text, and representation of objects mentioned in the text. Our study highlights the importance of takin
&lt;/p&gt;</description></item><item><title>&#36825;&#19968;&#21019;&#26032;&#24515;&#29702;&#27835;&#30103;&#27169;&#22411;HealMe&#36890;&#36807;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#26694;&#26550;&#30340;&#20849;&#24773;&#23545;&#35805;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26681;&#28145;&#33922;&#22266;&#30340;&#36127;&#38754;&#24605;&#32500;&#65292;&#24182;&#20419;&#36827;&#20102;&#29702;&#24615;&#12289;&#24179;&#34913;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.05574</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#27835;&#30103;&#20013;&#36827;&#34892;&#35748;&#30693;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05574
&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#21019;&#26032;&#24515;&#29702;&#27835;&#30103;&#27169;&#22411;HealMe&#36890;&#36807;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#26694;&#26550;&#30340;&#20849;&#24773;&#23545;&#35805;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26681;&#28145;&#33922;&#22266;&#30340;&#36127;&#38754;&#24605;&#32500;&#65292;&#24182;&#20419;&#36827;&#20102;&#29702;&#24615;&#12289;&#24179;&#34913;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#27835;&#30103;&#20013;&#21487;&#20197;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#29087;&#32451;&#22788;&#29702;&#35748;&#30693;&#37325;&#26500;&#31561;&#20851;&#38190;&#20219;&#21153;&#65292;&#20811;&#26381;&#32670;&#32827;&#12289;&#19981;&#20449;&#20219;&#12289;&#27835;&#30103;&#24072;&#25216;&#33021;&#24046;&#24322;&#21644;&#36164;&#28304;&#31232;&#32570;&#31561;&#25361;&#25112;&#12290;&#22312;&#20808;&#21069;&#30340;&#35748;&#30693;&#37325;&#26500;&#20013;&#65292;&#20027;&#35201;&#23558;&#36127;&#38754;&#24773;&#32490;&#36716;&#21270;&#20026;&#31215;&#26497;&#30340;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#25928;&#26524;&#26377;&#38480;&#65292;&#32463;&#24120;&#19981;&#33021;&#20419;&#36827;&#23458;&#25143;&#33258;&#25105;&#21457;&#29616;&#26367;&#20195;&#35270;&#35282;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24110;&#21161;&#21644;&#36171;&#33021;&#36890;&#36807;&#33258;&#36866;&#24212;&#35821;&#35328;&#22312;&#24515;&#29702;&#22686;&#24378;&#65288;HealMe&#65289;&#27169;&#22411;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#35748;&#30693;&#37325;&#26500;&#30103;&#27861;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26681;&#28145;&#33922;&#22266;&#30340;&#36127;&#38754;&#24819;&#27861;&#65292;&#24182;&#20419;&#36827;&#29702;&#24615;&#12289;&#24179;&#34913;&#30340;&#35270;&#35282;&#12290;HealMe&#19982;&#20256;&#32479;LLM&#26041;&#27861;&#19981;&#21516;&#65292;&#37319;&#29992;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#26694;&#26550;&#30340;&#20849;&#24773;&#23545;&#35805;&#12290;&#23427;&#36890;&#36807;&#31995;&#32479;&#25351;&#23548;&#23458;&#25143;&#21306;&#20998;&#24773;&#22659;&#21644;&#24863;&#21463;&#65292;&#38598;&#24605;&#24191;&#30410;&#23547;&#25214;&#26367;&#20195;&#35270;&#35282;&#65292;&#24182;&#21046;&#23450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05574v1 Announce Type: cross  Abstract: Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the crucial task of cognitive reframing and overcoming challenges such as shame, distrust, therapist skill variability, and resource scarcity. Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients' self-discovery of alternative perspectives. In this paper, we unveil the Helping and Empowering through Adaptive Language in Mental Enhancement (HealMe) model. This novel cognitive reframing therapy method effectively addresses deep-rooted negative thoughts and fosters rational, balanced perspectives. Diverging from traditional LLM methods, HealMe employs empathetic dialogue based on psychotherapeutic frameworks. It systematically guides clients through distinguishing circumstances from feelings, brainstorming alternative viewpoints, and developing 
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#22238;&#24212;&#24773;&#32490;&#22330;&#26223;&#26102;&#27604;&#20154;&#31867;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31227;&#24773;&#33021;&#21147;&#65292;&#24179;&#22343;&#31227;&#24773;&#35780;&#20998;&#39640;&#20110;&#20154;&#31867;&#29983;&#25104;&#30340;&#22238;&#24212;10%&#65307;&#25351;&#31034;ChatGPT&#22312;&#22238;&#24212;&#20013;&#34701;&#20837;&#23545;&#31227;&#24773;&#30340;&#28165;&#26224;&#29702;&#35299;&#20351;&#24471;&#20854;&#22238;&#24212;&#19982;&#39640;&#24230;&#31227;&#24773;&#30340;&#20010;&#20307;&#30340;&#26399;&#26395;&#22823;&#33268;&#25509;&#36817;5&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.05572</link><description>&lt;p&gt;
ChatGPT&#27604;&#20154;&#31867;&#26356;&#20855;&#31227;&#24773;&#33021;&#21147;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT More Empathetic than Humans?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05572
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#22238;&#24212;&#24773;&#32490;&#22330;&#26223;&#26102;&#27604;&#20154;&#31867;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31227;&#24773;&#33021;&#21147;&#65292;&#24179;&#22343;&#31227;&#24773;&#35780;&#20998;&#39640;&#20110;&#20154;&#31867;&#29983;&#25104;&#30340;&#22238;&#24212;10%&#65307;&#25351;&#31034;ChatGPT&#22312;&#22238;&#24212;&#20013;&#34701;&#20837;&#23545;&#31227;&#24773;&#30340;&#28165;&#26224;&#29702;&#35299;&#20351;&#24471;&#20854;&#22238;&#24212;&#19982;&#39640;&#24230;&#31227;&#24773;&#30340;&#20010;&#20307;&#30340;&#26399;&#26395;&#22823;&#33268;&#25509;&#36817;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#20197;&#21450;&#23588;&#20854;&#26159;&#20854;&#26368;&#26032;&#29256;&#26412;GPT-4&#22312;&#22238;&#24212;&#21508;&#31181;&#24773;&#32490;&#22330;&#26223;&#65288;&#21253;&#25324;&#31215;&#26497;&#21644;&#28040;&#26497;&#24773;&#32490;&#65289;&#26102;&#30340;&#31227;&#24773;&#33021;&#21147;&#65292;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#22238;&#24212;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#37319;&#29992;&#20005;&#35880;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#28041;&#21450;600&#21517;&#21442;&#19982;&#32773;&#30340;&#32452;&#38388;&#30740;&#31350;&#26469;&#35780;&#20272;ChatGPT&#29983;&#25104;&#30340;&#22238;&#24212;&#20013;&#30340;&#31227;&#24773;&#27700;&#24179;&#12290;ChatGPT&#34987;&#25552;&#31034;&#30340;&#26041;&#24335;&#26377;&#20004;&#31181;&#65306;&#19968;&#31181;&#26159;&#26631;&#20934;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26126;&#30830;&#35814;&#32454;&#35828;&#26126;&#20102;&#31227;&#24773;&#30340;&#35748;&#30693;&#12289;&#24773;&#24863;&#21644;&#21516;&#24773;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;ChatGPT&#29983;&#25104;&#30340;&#22238;&#24212;&#30340;&#24179;&#22343;&#31227;&#24773;&#35780;&#20998;&#36229;&#36807;&#20102;&#20154;&#31867;&#29983;&#25104;&#30340;&#22238;&#24212;&#32422;10%&#12290;&#27492;&#22806;&#65292;&#25351;&#31034;ChatGPT&#22312;&#20854;&#22238;&#24212;&#20013;&#34701;&#20837;&#23545;&#31227;&#24773;&#30340;&#28165;&#26224;&#29702;&#35299;&#20351;&#24471;&#36825;&#20123;&#22238;&#24212;&#19982;&#39640;&#24230;&#31227;&#24773;&#30340;&#20010;&#20307;&#30340;&#26399;&#26395;&#22823;&#33268;&#25509;&#36817;&#22810;&#36798;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05572v1 Announce Type: cross  Abstract: This paper investigates the empathetic responding capabilities of ChatGPT, particularly its latest iteration, GPT-4, in comparison to human-generated responses to a wide range of emotional scenarios, both positive and negative. We employ a rigorous evaluation methodology, involving a between-groups study with 600 participants, to evaluate the level of empathy in responses generated by humans and ChatGPT. ChatGPT is prompted in two distinct ways: a standard approach and one explicitly detailing empathy's cognitive, affective, and compassionate counterparts. Our findings indicate that the average empathy rating of responses generated by ChatGPT exceeds those crafted by humans by approximately 10%. Additionally, instructing ChatGPT to incorporate a clear understanding of empathy in its responses makes the responses align approximately 5 times more closely with the expectations of individuals possessing a high degree of empathy, compared t
&lt;/p&gt;</description></item><item><title>OpenHEXAI&#26159;&#19968;&#20010;&#29992;&#20110;&#20154;&#31867;&#20013;&#24515;&#35780;&#20272;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#25317;&#26377;&#22810;&#20803;&#21270;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#25143;&#30740;&#31350;Web&#24212;&#29992;&#31243;&#24207;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.05565</link><description>&lt;p&gt;
OpenHEXAI&#65306;&#19968;&#20010;&#29992;&#20110;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20154;&#31867;&#20013;&#24515;&#35780;&#20272;&#30340;&#24320;&#28304;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OpenHEXAI: An Open-Source Framework for Human-Centered Evaluation of Explainable Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05565
&lt;/p&gt;
&lt;p&gt;
OpenHEXAI&#26159;&#19968;&#20010;&#29992;&#20110;&#20154;&#31867;&#20013;&#24515;&#35780;&#20272;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#25317;&#26377;&#22810;&#20803;&#21270;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#25143;&#30740;&#31350;Web&#24212;&#29992;&#31243;&#24207;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#38656;&#35201;&#29702;&#35299;&#39640;&#39118;&#38505;&#24773;&#26223;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#65292;&#35299;&#37322;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#22823;&#24133;&#22686;&#22810;&#12290;&#28982;&#32780;&#65292;&#36866;&#24403;&#35780;&#20272;XAI&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#20154;&#31867;&#20027;&#20307;&#30340;&#21442;&#19982;&#65292;&#24182;&#19988;&#36827;&#34892;&#20154;&#31867;&#20013;&#24515;&#22522;&#20934;&#27979;&#35797;&#22312;&#35768;&#22810;&#26041;&#38754;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#35774;&#35745;&#21644;&#23454;&#26045;&#29992;&#25143;&#30740;&#31350;&#26159;&#22797;&#26434;&#30340;&#65307;&#22312;&#35774;&#35745;&#31354;&#38388;&#20013;&#30340;&#20247;&#22810;&#35774;&#35745;&#36873;&#25321;&#23548;&#33268;&#21487;&#37325;&#22797;&#24615;&#38382;&#39064;&#65307;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#21487;&#33021;&#23545;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#29978;&#33267;&#20196;&#20154;&#26395;&#32780;&#29983;&#30031;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;OpenHEXAI&#65292;&#19968;&#20010;&#29992;&#20110;XAI&#26041;&#27861;&#30340;&#20154;&#31867;&#20013;&#24515;&#35780;&#20272;&#30340;&#24320;&#28304;&#26694;&#26550;&#12290;OpenHEXAI&#20855;&#26377;&#65288;1&#65289;&#19968;&#31995;&#21015;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#65307;&#65288;2&#65289;&#29992;&#20110;&#29992;&#25143;&#30740;&#31350;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;Web&#24212;&#29992;&#31243;&#24207;&#65307;&#65288;3&#65289;&#20840;&#38754;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05565v1 Announce Type: cross  Abstract: Recently, there has been a surge of explainable AI (XAI) methods driven by the need for understanding machine learning model behaviors in high-stakes scenarios. However, properly evaluating the effectiveness of the XAI methods inevitably requires the involvement of human subjects, and conducting human-centered benchmarks is challenging in a number of ways: designing and implementing user studies is complex; numerous design choices in the design space of user study lead to problems of reproducibility; and running user studies can be challenging and even daunting for machine learning researchers. To address these challenges, this paper presents OpenHEXAI, an open-source framework for human-centered evaluation of XAI methods. OpenHEXAI features (1) a collection of diverse benchmark datasets, pre-trained models, and post hoc explanation methods; (2) an easy-to-use web application for user study; (3) comprehensive evaluation metrics for the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19982;&#20256;&#32479;&#38463;&#32852;&#37195;&#22270;&#26696;&#30456;&#34701;&#21512;&#65292;&#22312;SDXL&#27169;&#22411;&#20013;&#21152;&#20837;LoRA&#25216;&#26415;&#29983;&#25104;&#20855;&#26377;&#25991;&#21270;&#24847;&#20041;&#30340;&#19978;&#33394;&#27169;&#26495;&#65292;&#23558;&#19978;&#33394;&#30103;&#27861;&#19982;&#25991;&#21270;&#20849;&#40483;&#30456;&#32467;&#21512;&#65292;&#20316;&#20026;&#27835;&#30103;&#24178;&#39044;&#21644;&#25991;&#21270;&#20445;&#25252;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.05562</link><description>&lt;p&gt;
&#21033;&#29992;LoRA&#23545;SDXL&#36827;&#34892;&#24494;&#35843;&#20197;&#36827;&#34892;&#19978;&#33394;&#30103;&#27861;&#65306;&#21463;&#38463;&#25289;&#20271;&#32852;&#21512;&#37195;&#38271;&#22269;&#25991;&#21270;&#21551;&#21457;&#29983;&#25104;&#22270;&#24418;&#27169;&#26495;
&lt;/p&gt;
&lt;p&gt;
SDXL Finetuned with LoRA for Coloring Therapy: Generating Graphic Templates Inspired by United Arab Emirates Culture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05562
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19982;&#20256;&#32479;&#38463;&#32852;&#37195;&#22270;&#26696;&#30456;&#34701;&#21512;&#65292;&#22312;SDXL&#27169;&#22411;&#20013;&#21152;&#20837;LoRA&#25216;&#26415;&#29983;&#25104;&#20855;&#26377;&#25991;&#21270;&#24847;&#20041;&#30340;&#19978;&#33394;&#27169;&#26495;&#65292;&#23558;&#19978;&#33394;&#30103;&#27861;&#19982;&#25991;&#21270;&#20849;&#40483;&#30456;&#32467;&#21512;&#65292;&#20316;&#20026;&#27835;&#30103;&#24178;&#39044;&#21644;&#25991;&#21270;&#20445;&#25252;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#24515;&#29702;&#20581;&#24247;&#30103;&#27861;&#26041;&#27861;&#34701;&#21512;&#20102;&#25991;&#21270;&#36951;&#20135;&#21644;&#20808;&#36827;&#25216;&#26415;&#30340;&#20132;&#27719;&#28857;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19982;&#20256;&#32479;&#30340;&#38463;&#25289;&#20271;&#32852;&#21512;&#37195;&#38271;&#22269;&#22270;&#26696;&#30456;&#34701;&#21512;&#65292;&#37325;&#28857;&#25918;&#22312;&#38463;&#32852;&#37195;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#22686;&#24378;&#20102;Low-Rank Adaptation (LoRA)&#30340;Stable Diffusion XL (SDXL)&#27169;&#22411;&#65292;&#20197;&#21019;&#36896;&#20855;&#26377;&#25991;&#21270;&#24847;&#20041;&#30340;&#19978;&#33394;&#27169;&#26495;&#65292;&#20854;&#20013;&#21253;&#25324;Al-Sadu&#32534;&#32455;&#22270;&#26696;&#12290;&#36825;&#31181;&#26032;&#39062;&#26041;&#27861;&#21033;&#29992;&#20102;&#19978;&#33394;&#30103;&#27861;&#34987;&#35748;&#21487;&#30340;&#20943;&#21387;&#30410;&#22788;&#65292;&#24182;&#23884;&#20837;&#20102;&#28145;&#21402;&#30340;&#25991;&#21270;&#20849;&#40483;&#65292;&#20351;&#20854;&#25104;&#20026;&#27835;&#30103;&#24178;&#39044;&#21644;&#25991;&#21270;&#20445;&#25252;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#38024;&#23545;&#24191;&#27867;&#24615;&#28966;&#34385;&#38556;&#30861; (GAD)&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#20943;&#23569;&#30456;&#20851;&#30151;&#29366;&#26041;&#38754;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#33394;&#24425;&#21644;&#38899;&#20048;&#30103;&#27861;&#30340;&#26356;&#24191;&#27867;&#24433;&#21709;&#65292;&#24182;&#24378;&#35843;&#20102;&#25991;&#21270;&#23450;&#21046;&#20869;&#23481;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05562v1 Announce Type: cross  Abstract: A transformative approach to mental health therapy lies at the crossroads of cultural heritage and advanced technology. This paper introduces an innovative method that fuses machine learning techniques with traditional Emirati motifs, focusing on the United Arab Emirates (UAE). We utilize the Stable Diffusion XL (SDXL) model, enhanced with Low-Rank Adaptation (LoRA), to create culturally significant coloring templates featuring Al-Sadu weaving patterns. This novel approach leverages coloring therapy for its recognized stress-relieving benefits and embeds deep cultural resonance, making it a potent tool for therapeutic intervention and cultural preservation. Specifically targeting Generalized Anxiety Disorder (GAD), our method demonstrates significant potential in reducing associated symptoms. Additionally, the paper delves into the broader implications of color and music therapy, emphasizing the importance of culturally tailored conten
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24212;&#29992;&#22810;&#28304;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#28151;&#21512;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#39044;&#27979;&#23398;&#29983;&#23398;&#26415;&#34920;&#29616;&#30340;&#26368;&#20339;&#23646;&#24615;&#38598;&#20026;&#29702;&#35770;&#35838;&#19978;&#30340;&#20851;&#27880;&#31243;&#24230;&#12289;Moodle&#27979;&#39564;&#25104;&#32489;&#20197;&#21450;Moodle&#35770;&#22363;&#19978;&#30340;&#27963;&#21160;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.05552</link><description>&lt;p&gt;
&#22810;&#28304;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#22312;&#28151;&#21512;&#24335;&#23398;&#20064;&#22823;&#23398;&#35838;&#31243;&#20013;&#39044;&#27979;&#23398;&#26415;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Multi-source and multimodal data fusion for predicting academic performance in blended learning university courses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05552
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24212;&#29992;&#22810;&#28304;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#28151;&#21512;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#39044;&#27979;&#23398;&#29983;&#23398;&#26415;&#34920;&#29616;&#30340;&#26368;&#20339;&#23646;&#24615;&#38598;&#20026;&#29702;&#35770;&#35838;&#19978;&#30340;&#20851;&#27880;&#31243;&#24230;&#12289;Moodle&#27979;&#39564;&#25104;&#32489;&#20197;&#21450;Moodle&#35770;&#22363;&#19978;&#30340;&#27963;&#21160;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#26469;&#39044;&#27979;&#22823;&#23398;&#29983;&#22312;&#28151;&#21512;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#26368;&#32456;&#23398;&#26415;&#34920;&#29616;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#28304;&#22836;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;&#19981;&#21516;&#26469;&#28304;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#20102;&#20851;&#20110;&#22823;&#19968;&#23398;&#29983;&#30340;&#25968;&#25454;&#65306;&#29702;&#35770;&#35838;&#12289;&#23454;&#36341;&#35838;&#12289;&#22312;&#32447;Moodle&#35838;&#31243;&#20197;&#21450;&#26399;&#26411;&#32771;&#35797;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21457;&#29616;&#21738;&#31181;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#20135;&#29983;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24212;&#29992;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#21644;&#20845;&#31181;&#20998;&#31867;&#31639;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38598;&#25104;&#21644;&#36873;&#25321;&#26368;&#20339;&#23646;&#24615;&#26041;&#27861;&#19982;&#31163;&#25955;&#21270;&#25968;&#25454;&#19968;&#36215;&#20135;&#29983;&#20102;&#26368;&#20339;&#39044;&#27979;&#32467;&#26524;&#12290;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#26174;&#31034;&#65292;&#29702;&#35770;&#35838;&#19978;&#30340;&#20851;&#27880;&#31243;&#24230;&#12289;Moodle&#27979;&#39564;&#25104;&#32489;&#20197;&#21450;Moodle&#35770;&#22363;&#19978;&#30340;&#27963;&#21160;&#27700;&#24179;&#26159;&#39044;&#27979;&#23398;&#29983;&#26368;&#32456;&#34920;&#29616;&#30340;&#26368;&#20339;&#23646;&#24615;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05552v1 Announce Type: cross  Abstract: In this paper we applied data fusion approaches for predicting the final academic performance of university students using multiple-source, multimodal data from blended learning environments. We collected and preprocessed data about first-year university students from different sources: theory classes, practical sessions, on-line Moodle sessions, and a final exam. Our objective was to discover which data fusion approach produced the best results using our data. We carried out experiments by applying four different data fusion approaches and six classification algorithms. The results showed that the best predictions were produced using ensembles and selecting the best attributes approach with discretized data. The best prediction models showed us that the level of attention in theory classes, scores in Moodle quizzes, and the level of activity in Moodle forums were the best set of attributes for predicting students' final performance in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20108;&#20803;&#27169;&#31946;&#35821;&#35328;Delphi&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#35780;&#20272;&#38382;&#21367;&#30340;&#21508;&#20010;&#37096;&#20998;&#26469;&#39564;&#35777;&#25972;&#20010;&#38382;&#21367;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#35299;&#20915;&#35780;&#22996;&#23637;&#31034;&#19981;&#21516;&#19987;&#19994;&#31243;&#24230;&#30340;&#22330;&#26223;&#65292;&#24182;&#26816;&#27979;&#24433;&#21709;&#24037;&#20855;&#36136;&#37327;&#30340;&#39033;&#30446;&#12290;</title><link>https://arxiv.org/abs/2403.05550</link><description>&lt;p&gt;
Teranga Go!&#65306;&#25340;&#36710;&#21327;&#20316;&#28040;&#36153;&#31038;&#21306;&#65292;&#37319;&#29992;&#22810;&#20934;&#21017;&#29369;&#35947;&#27169;&#31946;&#35821;&#35328;&#26415;&#35821;&#38598;&#35266;&#28857;&#26469;&#24314;&#31435;&#20449;&#24515;&#21644;&#20449;&#20219;
&lt;/p&gt;
&lt;p&gt;
Teranga Go!: Carpooling Collaborative Consumption Community with multi-criteria hesitant fuzzy linguistic term set opinions to build confidence and trust
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05550
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20108;&#20803;&#27169;&#31946;&#35821;&#35328;Delphi&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#35780;&#20272;&#38382;&#21367;&#30340;&#21508;&#20010;&#37096;&#20998;&#26469;&#39564;&#35777;&#25972;&#20010;&#38382;&#21367;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#35299;&#20915;&#35780;&#22996;&#23637;&#31034;&#19981;&#21516;&#19987;&#19994;&#31243;&#24230;&#30340;&#22330;&#26223;&#65292;&#24182;&#26816;&#27979;&#24433;&#21709;&#24037;&#20855;&#36136;&#37327;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Classic Delphi&#21644;Fuzzy Delphi&#26041;&#27861;&#29992;&#20110;&#27979;&#35797;&#35832;&#22914;&#38382;&#21367;&#35843;&#26597;&#20043;&#31867;&#30340;&#25968;&#25454;&#37319;&#38598;&#24037;&#20855;&#30340;&#20869;&#23481;&#26377;&#25928;&#24615;&#12290;Fuzzy Delphi&#20174;&#35821;&#35328;&#35282;&#24230;&#22788;&#29702;&#35780;&#22996;&#21592;&#20204;&#21457;&#34920;&#30340;&#35266;&#28857;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#31946;&#25968;&#20540;&#20943;&#23569;&#35266;&#28857;&#30340;&#27495;&#20041;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20108;&#20803;&#27169;&#31946;&#35821;&#35328;Delphi&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#20197;&#22788;&#29702;&#35780;&#22996;&#21592;&#23637;&#31034;&#19981;&#21516;&#19987;&#19994;&#31243;&#24230;&#30340;&#22330;&#26223;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#26415;&#35821;&#30340;&#27169;&#31946;&#22810;&#31890;&#24230;&#35821;&#20041;&#65292;&#24182;&#36890;&#36807;2&#20803;&#35821;&#35328;&#20540;&#24471;&#20986;&#20013;&#38388;&#21644;&#26368;&#32456;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#26696;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#35780;&#20272;&#38382;&#21367;&#30340;&#21508;&#20010;&#37096;&#20998;&#26469;&#39564;&#35777;&#25972;&#20010;&#38382;&#21367;&#30340;&#26377;&#25928;&#24615;&#65292;&#23558;&#27599;&#20010;&#39033;&#30446;&#30340;&#26377;&#25928;&#24615;&#23450;&#20041;&#20026;&#20915;&#31574;&#38382;&#39064;&#12290;&#36890;&#36807;&#19987;&#23478;&#30340;&#24847;&#35265;&#65292;&#25105;&#20204;&#27979;&#37327;&#19968;&#33268;&#24615;&#31243;&#24230;&#12289;&#19968;&#33268;&#24615;&#31243;&#24230;&#21644;&#27599;&#20010;&#39033;&#30446;&#30340;&#35821;&#35328;&#20998;&#25968;&#65292;&#20197;&#20415;&#26816;&#27979;&#37027;&#20123;&#23545;&#24037;&#20855;&#30340;&#36136;&#37327;&#20135;&#29983;&#31215;&#26497;&#25110;&#28040;&#26497;&#24433;&#21709;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05550v1 Announce Type: cross  Abstract: Classic Delphi and Fuzzy Delphi methods are used to test content validity of a data collection tools such as questionnaires. Fuzzy Delphi takes the opinion issued by judges from a linguistic perspective reducing ambiguity in opinions by using fuzzy numbers. We propose an extension named 2-Tuple Fuzzy Linguistic Delphi method to deal with scenarios in which judges show different expertise degrees by using fuzzy multigranular semantics of the linguistic terms and to obtain intermediate and final results expressed by 2-tuple linguistic values. The key idea of our proposal is to validate the full questionnaire by means of the evaluation of its parts, defining the validity of each item as a Decision Making problem. Taking the opinion of experts, we measure the degree of consensus, the degree of consistency, and the linguistic score of each item, in order to detect those items that affect, positively or negatively, the quality of the instrum
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;BERT&#30417;&#27979;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#19978;&#21453;&#29369;&#22826;&#20027;&#20041;&#35805;&#35821;&#28436;&#21464;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#30417;&#27979;&#30340;&#19981;&#21487;&#34892;&#24615;&#65292;&#20026;&#24178;&#39044;&#21644;&#38450;&#27490;&#20167;&#24680;&#21319;&#32423;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.05548</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#30417;&#27979;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#19978;&#21453;&#29369;&#22826;&#20027;&#20041;&#35805;&#35821;&#30340;&#28436;&#21464;
&lt;/p&gt;
&lt;p&gt;
Monitoring the evolution of antisemitic discourse on extremist social media using BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;BERT&#30417;&#27979;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#19978;&#21453;&#29369;&#22826;&#20027;&#20041;&#35805;&#35821;&#28436;&#21464;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#30417;&#27979;&#30340;&#19981;&#21487;&#34892;&#24615;&#65292;&#20026;&#24178;&#39044;&#21644;&#38450;&#27490;&#20167;&#24680;&#21319;&#32423;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#31181;&#26063;&#20027;&#20041;&#21644;&#19981;&#23485;&#23481;&#26377;&#21487;&#33021;&#22312;&#32447;&#19979;&#20135;&#29983;&#20167;&#24680;&#65292;&#26368;&#32456;&#23548;&#33268;&#36523;&#20307;&#26292;&#21147;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#30340;&#26159;&#22312;&#32447;&#21453;&#29369;&#20027;&#20041;&#65292;&#36861;&#36394;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#21453;&#29369;&#20027;&#39064;&#21450;&#20854;&#30456;&#20851;&#26415;&#35821;&#30340;&#28436;&#21464;&#65292;&#26377;&#21161;&#20110;&#30417;&#27979;&#21442;&#19982;&#32773;&#30340;&#24773;&#32490;&#21644;&#28436;&#21464;&#65292;&#24182;&#21487;&#33021;&#25552;&#20379;&#24178;&#39044;&#26041;&#27861;&#65292;&#38450;&#27490;&#20167;&#24680;&#21319;&#32423;&#12290;&#37492;&#20110;&#22312;&#32447;&#27969;&#37327;&#24222;&#22823;&#19988;&#19981;&#26029;&#21464;&#21270;&#65292;&#25163;&#21160;&#30417;&#27979;&#35848;&#35805;&#23454;&#38469;&#19978;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#25552;&#21462;&#21453;&#29369;&#20027;&#39064;&#21644;&#26415;&#35821;&#65292;&#36319;&#36394;&#23427;&#20204;&#30340;&#28436;&#21464;&#12290;&#30001;&#20110;&#30417;&#30563;&#23398;&#20064;&#22312;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#36807;&#20110;&#21463;&#38480;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05548v1 Announce Type: cross  Abstract: Racism and intolerance on social media contribute to a toxic online environment which may spill offline to foster hatred, and eventually lead to physical violence. That is the case with online antisemitism, the specific category of hatred considered in this study. Tracking antisemitic themes and their associated terminology over time in online discussions could help monitor the sentiments of their participants and their evolution, and possibly offer avenues for intervention that may prevent the escalation of hatred. Due to the large volume and constant evolution of online traffic, monitoring conversations manually is impractical. Instead, we propose an automated method that extracts antisemitic themes and terminology from extremist social media over time and captures their evolution. Since supervised learning would be too limited for such a task, we created an unsupervised online machine learning approach that uses large language model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;AI&#30340;&#25945;&#23398;&#35268;&#21010;&#33050;&#26412;&#65292;&#36890;&#36807;&#23558;AI&#27010;&#24565;&#19982;&#30740;&#31350;&#30456;&#20851;&#20027;&#39064;&#32852;&#31995;&#36215;&#26469;&#65292;&#20419;&#36827;&#20102;&#23398;&#29983;&#23545;AI&#28508;&#21147;&#21644;&#39118;&#38505;&#30340;&#29702;&#35299;&#21644;&#20852;&#36259;&#12290;</title><link>https://arxiv.org/abs/2403.05547</link><description>&lt;p&gt;
AI&#23545;&#38750;&#31243;&#24207;&#21592;&#30340;&#24212;&#29992;&#65306;&#24212;&#29992;AI&#22312;&#27809;&#26377;&#32534;&#31243;&#25216;&#33021;&#30340;&#23398;&#29983;&#35838;&#22530;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AI for non-programmers: Applied AI in the lectures for students without programming skills
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;AI&#30340;&#25945;&#23398;&#35268;&#21010;&#33050;&#26412;&#65292;&#36890;&#36807;&#23558;AI&#27010;&#24565;&#19982;&#30740;&#31350;&#30456;&#20851;&#20027;&#39064;&#32852;&#31995;&#36215;&#26469;&#65292;&#20419;&#36827;&#20102;&#23398;&#29983;&#23545;AI&#28508;&#21147;&#21644;&#39118;&#38505;&#30340;&#29702;&#35299;&#21644;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;ChatGPT&#21644;WOMBO Dream&#31561;&#24212;&#29992;&#20351;&#24471;&#21551;&#21457;&#26080;&#32534;&#31243;&#30693;&#35782;&#30340;&#23398;&#29983;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21464;&#24471;&#36731;&#32780;&#26131;&#20030;&#12290;&#37492;&#20110;AI&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#30340;&#26085;&#30410;&#37325;&#35201;&#65292;&#38656;&#35201;&#21019;&#26032;&#31574;&#30053;&#26469;&#25945;&#32946;&#37027;&#20123;&#27809;&#26377;&#32534;&#31243;&#30693;&#35782;&#30340;&#23398;&#29983;&#65292;&#20197;&#20415;&#23558;AI&#20316;&#20026;&#26410;&#26469;&#25216;&#33021;&#34701;&#20837;&#20182;&#20204;&#30340;&#23398;&#20064;&#27169;&#22359;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;AI&#30340;&#25945;&#23398;&#35268;&#21010;&#33050;&#26412;&#12290;&#35813;&#25945;&#23398;&#35268;&#21010;&#33050;&#26412;&#22522;&#20110;AI&#24212;&#29992;&#27969;&#31243;&#65292;&#24182;&#23558;AI&#27010;&#24565;&#19982;&#30740;&#31350;&#30456;&#20851;&#20027;&#39064;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20123;&#32852;&#31995;&#25171;&#24320;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#65292;&#24182;&#20419;&#36827;&#20102;&#23398;&#29983;&#23545;AI&#28508;&#21147;&#21644;&#39118;&#38505;&#30340;&#20852;&#36259;&#21644;&#29702;&#35299;&#12290;&#20197;&#33021;&#28304;&#31649;&#29702;&#30805;&#22763;&#23398;&#29983;&#20026;&#20363;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;AI&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#19987;&#19994;&#35838;&#31243;&#20013;&#12290;&#20026;&#27492;&#65292;&#24212;&#29992;AI&#30340;&#25945;&#23398;&#35268;&#21010;&#33050;&#26412;&#34987;&#35843;&#25972;&#20197;&#36866;&#24212;&#30740;&#31350;&#39033;&#30446;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05547v1 Announce Type: cross  Abstract: Applications such as ChatGPT and WOMBO Dream make it easy to inspire students without programming knowledge to use artificial intelligence (AI). Therefore, given the increasing importance of AI in all disciplines, innovative strategies are needed to educate students in AI without programming knowledge so that AI can be integrated into their study modules as a future skill. This work presents a didactic planning script for applied AI. The didactic planning script is based on the AI application pipeline and links AI concepts with study-relevant topics. These linkages open up a new solution space and promote students' interest in and understanding of the potentials and risks of AI. An example lecture series for master students in energy management shows how AI can be seamlessly integrated into discipline-specific lectures. To this end, the planning script for applied AI is adapted to fit the study programs' topic. This specific teaching s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#35282;&#33394;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20851;&#27880;&#20102;&#20010;&#24615;&#21270;&#23398;&#20064;&#12289;&#20262;&#29702;&#24433;&#21709;&#12289;&#38750;STEM&#23398;&#31185;&#30340;&#36140;&#20540;&#20197;&#21450;&#28508;&#22312;&#23545;&#31070;&#32463;&#35748;&#30693;&#21644;&#31038;&#20250;&#24773;&#24863;&#21151;&#33021;&#30340;&#36716;&#21464;&#24433;&#21709;&#65292;&#20197;&#26399;&#20026;&#26410;&#26469;&#25945;&#32946;&#23454;&#36341;&#21644;&#25919;&#31574;&#25552;&#20379;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.05544</link><description>&lt;p&gt;
&#20174;&#31639;&#27861;&#23815;&#25308;&#21040;&#20154;&#31867;&#23398;&#20064;&#30340;&#33402;&#26415;&#65306;&#20174;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;50&#24180;&#21382;&#31243;&#20013;&#33719;&#24471;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
From Algorithm Worship to the Art of Human Learning: Insights from 50-year journey of AI in Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#35282;&#33394;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20851;&#27880;&#20102;&#20010;&#24615;&#21270;&#23398;&#20064;&#12289;&#20262;&#29702;&#24433;&#21709;&#12289;&#38750;STEM&#23398;&#31185;&#30340;&#36140;&#20540;&#20197;&#21450;&#28508;&#22312;&#23545;&#31070;&#32463;&#35748;&#30693;&#21644;&#31038;&#20250;&#24773;&#24863;&#21151;&#33021;&#30340;&#36716;&#21464;&#24433;&#21709;&#65292;&#20197;&#26399;&#20026;&#26410;&#26469;&#25945;&#32946;&#23454;&#36341;&#21644;&#25919;&#31574;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24403;&#21069;&#35752;&#35770;&#22312;&#24076;&#26395;&#21644;&#24551;&#34385;&#20043;&#38388;&#25671;&#25670;&#65292;&#25551;&#32472;&#20102;&#19968;&#20010;&#26410;&#26469;&#65292;AI&#23558;&#37325;&#22609;&#20154;&#31867;&#29983;&#27963;&#30340;&#26041;&#26041;&#38754;&#38754;&#65292;&#21253;&#25324;&#25945;&#32946;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;AI&#22312;&#25945;&#32946;&#20013;&#30340;&#35282;&#33394;&#22797;&#26434;&#24615;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#28608;&#21169;&#21644;&#35686;&#31034;&#25945;&#32946;&#24037;&#20316;&#32773;&#12289;&#20915;&#31574;&#32773;&#21644;&#20844;&#20247;&#30340;&#30683;&#30462;&#20449;&#24687;&#12290;&#23427;&#25506;&#35752;&#20102;AI&#20026;&#36890;&#36807;&#35268;&#27169;&#21270;&#20010;&#24615;&#21270;&#25552;&#21319;&#23398;&#20064;&#25152;&#25345;&#26377;&#30340;&#25215;&#35834;&#65292;&#21516;&#26102;&#20063;&#20851;&#27880;&#20102;&#20262;&#29702;&#24433;&#21709;&#12289;&#38750;STEM&#23398;&#31185;&#30340;&#36140;&#20540;&#20197;&#21450;&#23545;&#25105;&#20204;&#30340;&#31070;&#32463;&#35748;&#30693;&#21644;&#31038;&#20250;&#24773;&#24863;&#21151;&#33021;&#28508;&#22312;&#36716;&#21464;&#24433;&#21709;&#30340;&#32972;&#26223;&#12290;&#20511;&#37492;&#26368;&#26032;&#30740;&#31350;&#21644;&#20840;&#29699;&#35752;&#35770;&#65292;&#26412;&#25991;&#35797;&#22270;&#25581;&#31034;&#24403;&#21069;AI&#22312;&#25945;&#32946;&#39046;&#22495;&#65288;AIED&#65289;&#35752;&#35770;&#30340;&#27169;&#31946;&#24615;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#20197;&#21450;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#23545;&#26410;&#26469;&#25945;&#32946;&#23454;&#36341;&#21644;&#25919;&#31574;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05544v1 Announce Type: cross  Abstract: Current discourse surrounding Artificial Intelligence (AI) oscillates between hope and apprehension, painting a future where AI reshapes every facet of human life, including Education. This paper delves into the complexities of AI's role in Education, addressing the mixed messages that have both enthused and alarmed educators, policymakers, and the public. It explores the promises that AI holds for enhancing learning through personalisation at scale, against the backdrop of concerns about ethical implications, the devaluation of non-STEM subjects, and the potential transformative impact on our neurocognitive and socio-emotional functioning. Drawing on recent research and global discourse, the paper seeks to unpack the reasons behind the vagueness of current discussions on AI in Education (AIED) and the implications of this ambiguity for future educational practices and policies. By highlighting insights from educational research and sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#37329;&#34701;&#26426;&#26500;&#20013;&#20154;&#24037;&#26234;&#33021;&#22312;ESG&#20513;&#35758;&#20013;&#30340;&#24212;&#29992;&#65292;&#38416;&#26126;&#20102;AI&#22312;&#21152;&#24378;ESG&#26694;&#26550;&#26041;&#38754;&#30340;&#24517;&#35201;&#24615;&#21644;&#24433;&#21709;&#65292;&#20197;&#21450;AI&#22914;&#20309;&#22686;&#24378;&#37329;&#34701;&#27963;&#21160;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.05541</link><description>&lt;p&gt;
&#37329;&#34701;&#26426;&#26500;ESG&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#20010;&#20135;&#19994;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
AI in ESG for Financial Institutions: An Industrial Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#37329;&#34701;&#26426;&#26500;&#20013;&#20154;&#24037;&#26234;&#33021;&#22312;ESG&#20513;&#35758;&#20013;&#30340;&#24212;&#29992;&#65292;&#38416;&#26126;&#20102;AI&#22312;&#21152;&#24378;ESG&#26694;&#26550;&#26041;&#38754;&#30340;&#24517;&#35201;&#24615;&#21644;&#24433;&#21709;&#65292;&#20197;&#21450;AI&#22914;&#20309;&#22686;&#24378;&#37329;&#34701;&#27963;&#21160;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26085;&#30410;&#34701;&#20837;&#37329;&#34701;&#34892;&#19994;&#30340;&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#27835;&#29702;&#65288;ESG&#65289;&#20513;&#35758;&#65292;&#20195;&#34920;&#20102;&#21521;&#26356;&#21487;&#25345;&#32493;&#21644;&#20844;&#24179;&#37329;&#34701;&#23454;&#36341;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#26412;&#25991;&#35843;&#26597;&#20135;&#19994;&#26684;&#23616;&#65292;&#38416;&#26126;&#20102;AI&#22312;&#21152;&#24378;ESG&#26694;&#26550;&#20013;&#30340;&#24517;&#35201;&#24615;&#21644;&#24433;&#21709;&#12290;&#38543;&#30528;&#20005;&#26684;&#30340;&#30417;&#31649;&#35201;&#27714;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#24847;&#35782;&#30340;&#25552;&#39640;&#65292;&#37329;&#34701;&#26426;&#26500;&#65288;FIs&#65289;&#36234;&#26469;&#36234;&#34987;&#36843;&#37319;&#32435;ESG&#26631;&#20934;&#12290;AI&#25104;&#20026;&#22312;&#23548;&#33322;&#37329;&#34701;&#27963;&#21160;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#20013;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#23545;ESG&#30340;&#19977;&#20010;&#20027;&#35201;&#25903;&#26609;&#20013;&#30340;AI&#24212;&#29992;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#38416;&#26126;&#20102;AI&#22914;&#20309;&#22686;&#24378;&#20998;&#26512;&#33021;&#21147;&#12289;&#39118;&#38505;&#35780;&#20272;&#12289;&#23458;&#25143;&#21442;&#19982;&#12289;&#25253;&#21578;&#20934;&#30830;&#24615;&#31561;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22260;&#32469;&#25968;&#25454;&#20351;&#29992;&#21644;&#27169;&#22411;&#24320;&#21457;&#30340;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05541v1 Announce Type: cross  Abstract: The burgeoning integration of Artificial Intelligence (AI) into Environmental, Social, and Governance (ESG) initiatives within the financial sector represents a paradigm shift towards more sus-tainable and equitable financial practices. This paper surveys the industrial landscape to delineate the necessity and impact of AI in bolstering ESG frameworks. With the advent of stringent regulatory requirements and heightened stakeholder awareness, financial institutions (FIs) are increasingly compelled to adopt ESG criteria. AI emerges as a pivotal tool in navigating the complex in-terplay of financial activities and sustainability goals. Our survey categorizes AI applications across three main pillars of ESG, illustrating how AI enhances analytical capabilities, risk assessment, customer engagement, reporting accuracy and more. Further, we delve into the critical con-siderations surrounding the use of data and the development of models, und
&lt;/p&gt;</description></item><item><title>DeepSeek-VL&#26159;&#19968;&#20010;&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#26679;&#21270;&#25968;&#25454;&#12289;&#30495;&#23454;&#22330;&#26223;&#35206;&#30422;&#21644;&#39640;&#25928;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#29992;&#25143;&#20307;&#39564;</title><link>https://arxiv.org/abs/2403.05525</link><description>&lt;p&gt;
DeepSeek-VL:&#36208;&#21521;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
DeepSeek-VL: Towards Real-World Vision-Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05525
&lt;/p&gt;
&lt;p&gt;
DeepSeek-VL&#26159;&#19968;&#20010;&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#26679;&#21270;&#25968;&#25454;&#12289;&#30495;&#23454;&#22330;&#26223;&#35206;&#30422;&#21644;&#39640;&#25928;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#29992;&#25143;&#20307;&#39564;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;DeepSeek-VL&#65292;&#19968;&#20010;&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#35270;&#35273;&#21644;&#35821;&#35328;&#29702;&#35299;&#24212;&#29992;&#30340;&#24320;&#28304;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22260;&#32469;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#23637;&#24320;&#65306;&#30830;&#20445;&#25968;&#25454;&#22810;&#26679;&#21270;&#12289;&#21487;&#25193;&#23637;&#24615;&#24378;&#65292;&#24182;&#24191;&#27867;&#28085;&#30422;&#21253;&#25324;&#32593;&#32476;&#25130;&#22270;&#12289;PDF&#12289;OCR&#12289;&#22270;&#34920;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#20869;&#23481;&#22312;&#20869;&#30340;&#30495;&#23454;&#22330;&#26223;&#65292;&#20197;&#20840;&#38754;&#34920;&#24449;&#23454;&#38469;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#29992;&#25143;&#22330;&#26223;&#21019;&#24314;&#20102;&#29992;&#20363;&#20998;&#31867;&#27861;&#65292;&#24182;&#30456;&#24212;&#26500;&#24314;&#20102;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#32771;&#34385;&#21040;&#25928;&#29575;&#21644;&#22823;&#22810;&#25968;&#30495;&#23454;&#22330;&#26223;&#30340;&#38656;&#27714;&#65292;DeepSeek-VL&#25972;&#21512;&#20102;&#19968;&#20010;&#28151;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#39640;&#25928;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65288;1024 x 1024&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#23545;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05525v1 Announce Type: new  Abstract: We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. Our approach is structured around three key dimensions:   We strive to ensure our data is diverse, scalable, and extensively covers real-world scenarios including web screenshots, PDFs, OCR, charts, and knowledge-based content, aiming for a comprehensive representation of practical contexts. Further, we create a use case taxonomy from real user scenarios and construct an instruction tuning dataset accordingly. The fine-tuning with this dataset substantially improves the model's user experience in practical applications. Considering efficiency and the demands of most real-world scenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently processes high-resolution images (1024 x 1024), while maintaining a relatively low computational overhead. This design choice ensures the model's abilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#65292;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#22522;&#26412;&#35268;&#21017;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.05101</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Rule-driven News Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#65292;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#22522;&#26412;&#35268;&#21017;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
News captioning&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#25551;&#36848;&#22270;&#29255;&#21450;&#20854;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#25110;&#20855;&#20307;&#20107;&#20214;&#26469;&#29983;&#25104;&#21477;&#23376;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20381;&#36182;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#21462;&#24471;&#26174;&#33879;&#25104;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#19987;&#27880;&#20110;&#36755;&#20837;&#26032;&#38395;&#20869;&#23481;&#19982;&#36755;&#20986;&#39044;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#38656;&#35201;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#30340;&#19968;&#20123;&#22522;&#26412;&#35268;&#21017;&#65292;&#22914;&#20934;&#30830;&#25551;&#36848;&#19982;&#20107;&#20214;&#30456;&#20851;&#30340;&#20010;&#20307;&#21644;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#25351;&#23450;&#30340;&#35268;&#21017;&#20449;&#21495;&#29983;&#25104;&#22270;&#20687;&#25551;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#25551;&#36848;&#35774;&#35745;&#20102;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#12290;&#36825;&#19968;&#35268;&#21017;&#21253;&#25324;&#22270;&#29255;&#20013;&#25551;&#32472;&#30340;&#20027;&#35201;&#21160;&#20316;&#65288;&#20363;&#22914;&#65292;&#8220;&#25191;&#34892;&#8221;&#65289;&#20197;&#21450;&#21442;&#19982;&#21160;&#20316;&#30340;&#21629;&#21517;&#23454;&#20307;&#25198;&#28436;&#30340;&#35282;&#33394;&#65288;&#20363;&#22914;&#65292;&#8220;&#20195;&#29702;&#20154;&#8221;&#21644;&#8220;&#22320;&#28857;&#8221;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#35821;&#20041;&#35268;&#21017;&#27880;&#20837;&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05101v1 Announce Type: cross  Abstract: News captioning task aims to generate sentences by describing named entities or concrete events for an image with its news article. Existing methods have achieved remarkable results by relying on the large-scale pre-trained models, which primarily focus on the correlations between the input news content and the output predictions. However, the news captioning requires adhering to some fundamental rules of news reporting, such as accurately describing the individuals and actions associated with the event. In this paper, we propose the rule-driven news captioning method, which can generate image descriptions following designated rule signal. Specifically, we first design the news-aware semantic rule for the descriptions. This rule incorporates the primary action depicted in the image (e.g., "performing") and the roles played by named entities involved in the action (e.g., "Agent" and "Place"). Second, we inject this semantic rule into th
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#20154;&#20307;&#21160;&#20316;&#39044;&#27979;&#39046;&#22495;&#24341;&#20837;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#27169;&#22411;&#21363;&#20351;&#22312;&#20302;&#27700;&#24179;&#30340;&#25200;&#21160;&#19979;&#20063;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#31616;&#21333;&#26059;&#36716;&#21644;&#24179;&#31227;&#25935;&#24863;&#30340;&#27169;&#22411;&#24615;&#33021;&#21463;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04954</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#25915;&#20987;&#27450;&#39575;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21160;&#20316;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fooling Neural Networks for Motion Forecasting via Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#20154;&#20307;&#21160;&#20316;&#39044;&#27979;&#39046;&#22495;&#24341;&#20837;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#27169;&#22411;&#21363;&#20351;&#22312;&#20302;&#27700;&#24179;&#30340;&#25200;&#21160;&#19979;&#20063;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#31616;&#21333;&#26059;&#36716;&#21644;&#24179;&#31227;&#25935;&#24863;&#30340;&#27169;&#22411;&#24615;&#33021;&#21463;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#21160;&#20316;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#23433;&#20840;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#24191;&#27867;&#30740;&#31350;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#20027;&#39064;&#23578;&#26410;&#24212;&#29992;&#20110;&#20154;&#20307;&#21160;&#20316;&#39044;&#27979;&#20013;&#30340;&#22810;&#22238;&#24402;&#27169;&#22411;&#65292;&#22914;GCNs&#21644;&#22522;&#20110;MLP&#30340;&#26550;&#26500;&#12290;&#35813;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#23545;&#31867;&#20284;&#20110;&#22270;&#20687;&#20998;&#31867;&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#21021;&#22987;&#38454;&#27573;&#30340;&#26368;&#20808;&#36827;&#26550;&#26500;&#36827;&#34892;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#26469;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#20302;&#27700;&#24179;&#25200;&#21160;&#19978;&#65292;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#19977;&#32500;&#21464;&#25442;&#23454;&#39564;&#65292;&#29305;&#21035;&#26159;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22810;&#25968;&#27169;&#22411;&#23545;&#31616;&#21333;&#30340;&#26059;&#36716;&#21644;&#24179;&#31227;&#25935;&#24863;&#65292;&#36825;&#20123;&#21464;&#25442;&#19981;&#20250;&#25913;&#21464;&#20851;&#33410;&#36317;&#31163;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#31867;&#20284;&#26089;&#26399;CNN&#27169;&#22411;&#19968;&#26679;&#65292;&#21160;&#20316;&#39044;&#27979;&#20219;&#21153;&#26131;&#21463;&#21040;&#23567;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04954v1 Announce Type: cross  Abstract: Human motion prediction is still an open problem, which is extremely important for autonomous driving and safety applications. Although there are great advances in this area, the widely studied topic of adversarial attacks has not been applied to multi-regression models such as GCNs and MLP-based architectures in human motion prediction. This work intends to reduce this gap using extensive quantitative and qualitative experiments in state-of-the-art architectures similar to the initial stages of adversarial attacks in image classification. The results suggest that models are susceptible to attacks even on low levels of perturbation. We also show experiments with 3D transformations that affect the model performance, in particular, we show that most models are sensitive to simple rotations and translations which do not alter joint distances. We conclude that similar to earlier CNN models, motion forecasting tasks are susceptible to small
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#30446;&#26631;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65292;&#35813;&#20844;&#24335;&#22522;&#20110;&#30446;&#26631;&#22312;&#31354;&#38388;-&#26102;&#38388;&#22352;&#26631;&#31995;&#20869;&#25104;&#20026;&#20984;&#38598;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#22312;&#20984;&#38598;&#22270;&#20013;&#23547;&#25214;&#26368;&#30701;&#36335;&#24452;&#26469;&#23454;&#29616;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#24403;&#21069;Mixed Integer Conic Program (MICP)&#27714;&#35299;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04917</link><description>&lt;p&gt;
&#22522;&#20110;&#20984;&#38598;&#22270;&#30340;&#31227;&#21160;&#30446;&#26631;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#30340;&#28151;&#21512;&#25972;&#25968;&#38181;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
A Mixed-Integer Conic Program for the Moving-Target Traveling Salesman Problem based on a Graph of Convex Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#30446;&#26631;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65292;&#35813;&#20844;&#24335;&#22522;&#20110;&#30446;&#26631;&#22312;&#31354;&#38388;-&#26102;&#38388;&#22352;&#26631;&#31995;&#20869;&#25104;&#20026;&#20984;&#38598;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#22312;&#20984;&#38598;&#22270;&#20013;&#23547;&#25214;&#26368;&#30701;&#36335;&#24452;&#26469;&#23454;&#29616;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#24403;&#21069;Mixed Integer Conic Program (MICP)&#27714;&#35299;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23547;&#25214;&#31227;&#21160;&#30446;&#26631;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;MT-TSP&#65289;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#26032;&#30340;&#20844;&#24335;&#65292;&#35813;&#38382;&#39064;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#26368;&#30701;&#36335;&#24452;&#65292;&#20351;&#19968;&#20010;&#20174;&#20179;&#24211;&#20986;&#21457;&#30340;&#20195;&#29702;&#35775;&#38382;&#19968;&#32452;&#31227;&#21160;&#30446;&#26631;&#65292;&#24182;&#22312;&#23427;&#20204;&#20998;&#37197;&#30340;&#26102;&#38388;&#31383;&#21475;&#20869;&#24688;&#22909;&#35775;&#38382;&#19968;&#27425;&#65292;&#28982;&#21518;&#36820;&#22238;&#21040;&#20179;&#24211;&#12290;&#35813;&#20844;&#24335;&#20381;&#36182;&#20110;&#19968;&#20010;&#20851;&#38190;&#24605;&#24819;&#65292;&#21363;&#24403;&#30446;&#26631;&#27839;&#30528;&#32447;&#31227;&#21160;&#26102;&#65292;&#23427;&#20204;&#30340;&#36712;&#36857;&#22312;&#31354;&#38388;-&#26102;&#38388;&#22352;&#26631;&#31995;&#20869;&#21464;&#20026;&#20984;&#38598;&#12290;&#28982;&#21518;&#65292;&#38382;&#39064;&#23601;&#32553;&#20943;&#20026;&#22312;&#19968;&#20010;&#20984;&#38598;&#22270;&#20013;&#23547;&#25214;&#26368;&#30701;&#36335;&#24452;&#65292;&#21463;&#21040;&#19968;&#20123;&#36895;&#24230;&#32422;&#26463;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20844;&#24335;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;Mixed Integer Conic Program (MICP)&#27714;&#35299;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#20844;&#24335;&#22312;&#30446;&#26631;&#25968;&#37327;&#26368;&#22810;&#20026;20&#20010;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#20248;&#20110;MICP&#65292;&#22312;&#36816;&#34892;&#26102;&#38388;&#19978;&#32553;&#30701;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#26368;&#20248;&#24615;&#24046;&#36317;&#32553;&#23567;&#20102;&#39640;&#36798;60&#65285;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#35299;&#27861;&#30340;&#25104;&#26412;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04917v1 Announce Type: cross  Abstract: This paper introduces a new formulation that finds the optimum for the Moving-Target Traveling Salesman Problem (MT-TSP), which seeks to find a shortest path for an agent, that starts at a depot, visits a set of moving targets exactly once within their assigned time-windows, and returns to the depot. The formulation relies on the key idea that when the targets move along lines, their trajectories become convex sets within the space-time coordinate system. The problem then reduces to finding the shortest path within a graph of convex sets, subject to some speed constraints. We compare our formulation with the current state-of-the-art Mixed Integer Conic Program (MICP) solver for the MT-TSP. The experimental results show that our formulation outperforms the MICP for instances with up to 20 targets, with up to two orders of magnitude reduction in runtime, and up to a 60\% tighter optimality gap. We also show that the solution cost from th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;S3-TSS&#65292;&#21033;&#29992;&#26102;&#38388;&#32500;&#24230;&#20013;&#30340;&#33258;&#28982;&#22686;&#24378;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22235;&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;SeCo&#12290;</title><link>https://arxiv.org/abs/2403.04859</link><description>&lt;p&gt;
&#21355;&#26143;&#22270;&#20687;&#26102;&#38388;&#33258;&#30417;&#30563;&#65288;S3-TSS&#65289;&#65306;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;SSL&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Supervision in Time for Satellite Images(S3-TSS): A novel method of SSL technique in Satellite images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04859
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;S3-TSS&#65292;&#21033;&#29992;&#26102;&#38388;&#32500;&#24230;&#20013;&#30340;&#33258;&#28982;&#22686;&#24378;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22235;&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;SeCo&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36965;&#24863;&#22270;&#20687;&#20013;&#24102;&#26377;&#21508;&#31181;&#22823;&#27668;&#26465;&#20214;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#20284;&#20046;&#20351;&#29992;&#33258;&#30417;&#30563;&#31639;&#27861;&#24456;&#26377;&#29992;&#12290;&#21253;&#25324;&#26059;&#36716;&#12289;&#31354;&#38388;&#19978;&#19979;&#25991;&#21644;&#25340;&#22270;&#22312;&#20869;&#30340;&#20960;&#31181;&#22522;&#20110;&#20551;&#35774;&#30340;&#31639;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#12290;&#36890;&#24120;&#65292;&#21355;&#26143;&#22270;&#20687;&#20855;&#26377;&#26356;&#39640;&#30340;&#26102;&#38388;&#39057;&#29575;&#12290; &#22240;&#27492;&#65292;&#36965;&#24863;&#25968;&#25454;&#30340;&#26102;&#38388;&#32500;&#24230;&#25552;&#20379;&#20102;&#33258;&#28982;&#22686;&#24378;&#65292;&#32780;&#26080;&#38656;&#25105;&#20204;&#21019;&#24314;&#22270;&#20687;&#30340;&#20154;&#24037;&#22686;&#24378;&#12290; &#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S3-TSS&#65292;&#19968;&#31181;&#21033;&#29992;&#26102;&#38388;&#32500;&#24230;&#20013;&#33258;&#28982;&#22686;&#24378;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#12290; &#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#24403;&#21069;&#39046;&#20808;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#12290; &#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;SeCo&#12290; &#25105;&#20204;&#30340;&#24037;&#20316;&#20195;&#30721;&#21487;&#20197;&#22312;&#36825;&#37324;&#25214;&#21040;&#65306;https://github.com/hewanshrestha/Why-Self-Supervision-in-Time
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04859v1 Announce Type: new  Abstract: With the limited availability of labeled data with various atmospheric conditions in remote sensing images, it seems useful to work with self-supervised algorithms. Few pretext-based algorithms, including from rotation, spatial context and jigsaw puzzles are not appropriate for satellite images. Often, satellite images have a higher temporal frequency. So, the temporal dimension of remote sensing data provides natural augmentation without requiring us to create artificial augmentation of images. Here, we propose S3-TSS, a novel method of self-supervised learning technique that leverages natural augmentation occurring in temporal dimension. We compare our results with current state-of-the-art methods and also perform various experiments. We observed that our method was able to perform better than baseline SeCo in four downstream datasets. Code for our work can be found here: https://github.com/hewanshrestha/Why-Self-Supervision-in-Time
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;TopicDiff&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.04789</link><description>&lt;p&gt;
TopicDiff&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#30340;&#20027;&#39064;&#20016;&#23500;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;TopicDiff&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#65288;MCE&#65289;&#26816;&#27979;&#36890;&#24120;&#36328;&#36234;&#22768;&#23398;&#12289;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#65292;&#21560;&#24341;&#20102;&#22810;&#23186;&#20307;&#31038;&#21306;&#26085;&#30410;&#22686;&#21152;&#30340;&#20852;&#36259;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#23545;&#35805;&#20013;&#30340;&#35821;&#22659;&#20449;&#24687;&#65292;&#21482;&#26377;&#23569;&#25968;&#32771;&#34385;&#21333;&#19968;&#35821;&#35328;&#27169;&#24577;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#32780;&#24635;&#26159;&#24573;&#35270;&#22768;&#23398;&#21644;&#35270;&#35273;&#20027;&#39064;&#20449;&#24687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;Topic-enriched Diffusion&#65288;TopicDiff&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;MCE&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#20197;&#32531;&#35299;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#12290;&#35814;&#32454;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;TopicDiff&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#20027;&#39064;&#20449;&#24687;&#23545;MCE&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;TopicDiff&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04789v1 Announce Type: cross  Abstract: Multimodal Conversational Emotion (MCE) detection, generally spanning across the acoustic, vision and language modalities, has attracted increasing interest in the multimedia community. Previous studies predominantly focus on learning contextual information in conversations with only a few considering the topic information in single language modality, while always neglecting the acoustic and vision topic information. On this basis, we propose a model-agnostic Topic-enriched Diffusion (TopicDiff) approach for capturing multimodal topic information in MCE tasks. Particularly, we integrate the diffusion model into neural topic model to alleviate the diversity deficiency problem of neural topic model in capturing topic information. Detailed evaluations demonstrate the significant improvements of TopicDiff over the state-of-the-art MCE baselines, justifying the importance of multimodal topic information to MCE and the effectiveness of Topic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#32463;&#36807;&#24494;&#35843;&#30340;GPT4&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#31227;&#38500;&#20854;&#22312;&#23398;&#20064;&#26399;&#38388;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;</title><link>https://arxiv.org/abs/2403.04769</link><description>&lt;p&gt;
&#31227;&#38500;GPT4&#30340;&#36807;&#28388;&#22120;
&lt;/p&gt;
&lt;p&gt;
Removing GPT4's Filter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04769
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#32463;&#36807;&#24494;&#35843;&#30340;GPT4&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#31227;&#38500;&#20854;&#22312;&#23398;&#20064;&#26399;&#38388;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT4&#26368;&#21021;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#65292;&#21363;&#24535;&#24895;&#32773;&#25552;&#20379;&#21453;&#39304;&#20197;&#25945;&#23548;GPT4&#19981;&#35201;&#29983;&#25104;&#19981;&#24403;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25805;&#20316;&#24050;&#32463;&#36827;&#34892;&#24494;&#35843;&#30340;&#29256;&#26412;&#65292;&#20351;&#20854;&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;RLHF&#65288;Reinforcement learning from Human Feedback&#65289;&#30340;&#34892;&#20026;&#65292;&#26377;&#25928;&#22320;&#31227;&#38500;&#20102;&#27169;&#22411;&#22312;RLHF&#26399;&#38388;&#23398;&#20064;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;&#12290;&#29305;&#21035;&#26159;&#65292;&#24403;GPT4&#22312;&#27809;&#26377;&#32463;&#36807;RLHF&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#26102;&#65292;&#23427;&#22833;&#21435;&#20102;&#25152;&#26377;&#25233;&#21046;&#21147;&#65292;&#21482;&#38656;&#21069;&#20960;&#20010;&#35789;&#23601;&#21487;&#20197;&#29983;&#25104;&#38750;&#24120;&#19981;&#24403;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04769v1 Announce Type: cross  Abstract: GPT4 was initially trained on large amounts of data, and then fine-tuned using Reinforcement learning from Human Feedback (RLHF), which is when volunteers give feedback in order to teach GPT4 not to create inappropriate content. In this paper, we present a method to manipulate the fine-tuned version into reverting to pre-RLHF behavior, effectively removing all safety mechanisms that the model learned during RLHF. In particular, when GPT4 acts without RLHF, it loses all inhibition, and can complete very inappropriate content given only the first few words.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31454;&#20105;&#24066;&#22330;&#32972;&#26223;&#19979;&#30340;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#36335;&#24452;&#32422;&#26463;&#65292;&#25506;&#35752;&#20102;&#19977;&#31181;&#26377;&#25928;&#21106;&#20197;&#22788;&#29702;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.04264</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#25928;&#29992;&#21644;&#36335;&#24452;&#32422;&#26463;&#19979;&#30340;&#31454;&#20105;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Competitive Facility Location under Random Utilities and Routing Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31454;&#20105;&#24066;&#22330;&#32972;&#26223;&#19979;&#30340;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#36335;&#24452;&#32422;&#26463;&#65292;&#25506;&#35752;&#20102;&#19977;&#31181;&#26377;&#25928;&#21106;&#20197;&#22788;&#29702;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31454;&#20105;&#24066;&#22330;&#32972;&#26223;&#19979;&#30340;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#65292;&#20854;&#20013;&#39038;&#23458;&#38656;&#27714;&#30001;&#38543;&#26426;&#25928;&#29992;&#36873;&#25321;&#27169;&#22411;&#39044;&#27979;&#12290;&#19982;&#20197;&#24448;&#20027;&#35201;&#20851;&#27880;&#31616;&#21333;&#32422;&#26463;&#65288;&#20363;&#22914;&#25152;&#36873;&#20301;&#32622;&#25968;&#37327;&#30340;&#22522;&#25968;&#32422;&#26463;&#65289;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36335;&#24452;&#32422;&#26463;&#65292;&#36825;&#20123;&#32422;&#26463;&#38656;&#35201;&#20197;&#19968;&#31181;&#26041;&#24335;&#36873;&#25321;&#20301;&#32622;&#65292;&#20197;&#20445;&#35777;&#23384;&#22312;&#35775;&#38382;&#25152;&#26377;&#36873;&#25321;&#20301;&#32622;&#30340;&#26053;&#31243;&#65292;&#21516;&#26102;&#36981;&#23432;&#25351;&#23450;&#30340;&#26053;&#31243;&#38271;&#24230;&#19978;&#38480;&#12290;&#36825;&#31181;&#36335;&#24452;&#32422;&#26463;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#20855;&#26377;&#20851;&#38190;&#24212;&#29992;&#12290;&#25152;&#28041;&#38382;&#39064;&#20855;&#26377;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#65292;&#36825;&#26159;&#30001;&#20110;&#37319;&#29992;&#20102;&#38543;&#26426;&#25928;&#29992;&#65292;&#24182;&#19988;&#20855;&#26377;&#22797;&#26434;&#30340;&#36335;&#24452;&#32422;&#26463;&#65292;&#20351;&#20854;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19977;&#31181;&#26377;&#25928;&#21106;&#65292;&#21363;&#22806;&#20272;&#35745;&#21644;&#23376;&#27169;&#21106;&#65292;&#20197;&#22788;&#29702;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#65292;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04264v1 Announce Type: new  Abstract: In this paper, we study a facility location problem within a competitive market context, where customer demand is predicted by a random utility choice model. Unlike prior research, which primarily focuses on simple constraints such as a cardinality constraint on the number of selected locations, we introduce routing constraints that necessitate the selection of locations in a manner that guarantees the existence of a tour visiting all chosen locations while adhering to a specified tour length upper bound. Such routing constraints find crucial applications in various real-world scenarios. The problem at hand features a non-linear objective function, resulting from the utilization of random utilities, together with complex routing constraints, making it computationally challenging. To tackle this problem, we explore three types of valid cuts, namely, outer-approximation and submodular cuts to handle the nonlinear objective function, as wel
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04224</link><description>&lt;p&gt;
Aligners: &#35299;&#32806;LLMs&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligners: Decoupling LLMs and Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04224
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#19982;&#20154;&#31867;&#26399;&#26395;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23545;&#40784;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#27599;&#20010;LLM&#21644;&#23545;&#40784;&#26631;&#20934;&#37325;&#22797;&#36827;&#34892;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#35757;&#32451;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#29992;&#20110;&#23545;&#40784;&#32473;&#23450;&#26631;&#20934;&#30340;&#20219;&#20309;LLM&#30340;&#23545;&#40784;&#27169;&#22411;&#26469;&#35299;&#32806;LLMs&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#23569;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#40784;&#27169;&#22411;&#35757;&#32451;&#37197;&#26041;&#20165;&#20381;&#36182;&#20110;&#20351;&#29992;&#65288;&#25552;&#31034;&#30340;&#65289;LLM &#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;&#20197;&#36866;&#24212;&#21508;&#31181;&#23545;&#40784;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#8220;&#36947;&#24503;&#8221;&#23545;&#40784;&#22120;&#24182;&#22312;&#23454;&#39564;&#19978;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#26469;&#38416;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04224v1 Announce Type: cross  Abstract: Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an "ethical" aligner and verify its efficacy empirically.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#65292;&#26080;&#27861;&#20687;&#20154;&#31867;&#19968;&#26679;&#32416;&#27491;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2403.04121</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Reason and Plan?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04121
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#65292;&#26080;&#27861;&#20687;&#20154;&#31867;&#19968;&#26679;&#32416;&#27491;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20154;&#31867;&#26377;&#26102;&#20505;&#34920;&#29616;&#20986;&#33021;&#22815;&#36890;&#36807;&#33258;&#25105;&#25209;&#35780;&#32416;&#27491;&#33258;&#24049;&#38169;&#35823;&#29468;&#27979;&#30340;&#33021;&#21147;&#65292;&#20294;&#20284;&#20046;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#27809;&#26377;&#20381;&#25454;&#25903;&#25345;&#36825;&#19968;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04121v1 Announce Type: new  Abstract: While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22914;&#20309;&#20010;&#24615;&#21270;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#29983;&#25104;&#30340;&#25552;&#31034;&#30340;&#35299;&#37322;&#65292;&#20197;&#24110;&#21161;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#20010;&#24615;&#21270;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#26631;&#29992;&#25143;&#19982;&#25552;&#31034;&#35299;&#37322;&#30340;&#20114;&#21160;&#12289;&#29702;&#35299;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04035</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;AI&#39537;&#21160;&#25552;&#31034;&#23545;&#29992;&#25143;&#35748;&#30693;&#33021;&#21147;&#30340;&#35299;&#37322;&#65306;&#19968;&#39033;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Personalizing explanations of AI-driven hints to users cognitive abilities: an empirical evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22914;&#20309;&#20010;&#24615;&#21270;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#29983;&#25104;&#30340;&#25552;&#31034;&#30340;&#35299;&#37322;&#65292;&#20197;&#24110;&#21161;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#20010;&#24615;&#21270;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#26631;&#29992;&#25143;&#19982;&#25552;&#31034;&#35299;&#37322;&#30340;&#20114;&#21160;&#12289;&#29702;&#35299;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#20010;&#24615;&#21270;&#35299;&#37322;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#29983;&#25104;&#30340;&#25552;&#31034;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#25552;&#20379;&#25552;&#31034;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;&#20010;&#24615;&#21270;&#38024;&#23545;&#20855;&#26377;&#20004;&#31181;&#29305;&#24449;&#65288;&#35748;&#30693;&#38656;&#27714;&#21644;&#35748;&#30495;&#24230;&#65289;&#36739;&#20302;&#27700;&#24179;&#30340;&#23398;&#29983;&#65292;&#26088;&#22312;&#22686;&#24378;&#36825;&#20123;&#23398;&#29983;&#23545;&#35299;&#37322;&#30340;&#21442;&#19982;&#65292;&#22522;&#20110;&#20808;&#21069;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#23398;&#29983;&#19981;&#20250;&#33258;&#28982;&#21442;&#19982;&#35299;&#37322;&#65292;&#20294;&#22914;&#26524;&#20182;&#20204;&#36825;&#26679;&#20570;&#23558;&#20250;&#21463;&#30410;&#12290;&#20026;&#20102;&#35780;&#20272;&#20010;&#24615;&#21270;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;&#20010;&#24615;&#21270;&#26174;&#33879;&#22686;&#21152;&#20102;&#25105;&#20204;&#30446;&#26631;&#29992;&#25143;&#19982;&#25552;&#31034;&#35299;&#37322;&#30340;&#20114;&#21160;&#12289;&#20182;&#20204;&#23545;&#25552;&#31034;&#30340;&#29702;&#35299;&#20197;&#21450;&#20182;&#20204;&#30340;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#20010;&#24615;&#21270;AI&#39537;&#21160;&#35299;&#37322;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#36866;&#29992;&#20110;&#22914;&#23398;&#20064;&#31561;&#35748;&#30693;&#35201;&#27714;&#39640;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04035v1 Announce Type: new  Abstract: We investigate personalizing the explanations that an Intelligent Tutoring System generates to justify the hints it provides to students to foster their learning. The personalization targets students with low levels of two traits, Need for Cognition and Conscientiousness, and aims to enhance these students' engagement with the explanations, based on prior findings that these students do not naturally engage with the explanations but they would benefit from them if they do. To evaluate the effectiveness of the personalization, we conducted a user study where we found that our proposed personalization significantly increases our target users' interaction with the hint explanations, their understanding of the hints and their learning. Hence, this work provides valuable insights into effectively personalizing AI-driven explanations for cognitively demanding tasks such as learning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#35299;&#35868;&#20219;&#21153;AlgoPuzzleVQA&#65292;&#36890;&#36807;&#31639;&#27861;&#35868;&#39064;&#25361;&#25112;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#35270;&#35273;&#29702;&#35299;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#22797;&#26434;&#31639;&#27861;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#35780;&#20272;&#35270;&#35273;&#25968;&#25454;&#35299;&#37322;&#19982;&#31639;&#27861;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.03864</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#26159;&#35299;&#35868;&#22825;&#25165;&#65311;&#31639;&#27861;&#35868;&#39064;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#30340;&#20005;&#23803;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03864
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#35299;&#35868;&#20219;&#21153;AlgoPuzzleVQA&#65292;&#36890;&#36807;&#31639;&#27861;&#35868;&#39064;&#25361;&#25112;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#35270;&#35273;&#29702;&#35299;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#22797;&#26434;&#31639;&#27861;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#35780;&#20272;&#35270;&#35273;&#25968;&#25454;&#35299;&#37322;&#19982;&#31639;&#27861;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#35299;&#35868;&#20219;&#21153;&#65292;&#23558;&#20854;&#25918;&#22312;&#35270;&#35273;&#38382;&#31572;&#30340;&#32972;&#26223;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;AlgoPuzzleVQA&#65292;&#26088;&#22312;&#25361;&#25112;&#21644;&#35780;&#20272;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#38656;&#35201;&#35270;&#35273;&#29702;&#35299;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#22797;&#26434;&#31639;&#27861;&#25512;&#29702;&#30340;&#31639;&#27861;&#35868;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#28085;&#30422;&#24067;&#23572;&#36923;&#36753;&#12289;&#32452;&#21512;&#25968;&#23398;&#12289;&#22270;&#35770;&#12289;&#20248;&#21270;&#12289;&#25628;&#32034;&#31561;&#22810;&#31181;&#25968;&#23398;&#21644;&#31639;&#27861;&#20027;&#39064;&#30340;&#35868;&#39064;&#65292;&#26088;&#22312;&#35780;&#20272;&#35270;&#35273;&#25968;&#25454;&#35299;&#37322;&#19982;&#31639;&#27861;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#20154;&#31867;&#32534;&#20889;&#30340;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#25152;&#26377;&#30340;&#35868;&#39064;&#37117;&#26377;&#31934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20174;&#31639;&#27861;&#20013;&#25214;&#21040;&#65292;&#26080;&#38656;&#32321;&#29712;&#30340;&#20154;&#24037;&#35745;&#31639;&#12290;&#36825;&#30830;&#20445;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#22312;&#25512;&#29702;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#26041;&#38754;&#21487;&#20197;&#20219;&#24847;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03864v1 Announce Type: cross  Abstract: This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models in solving algorithmic puzzles that necessitate both visual understanding, language understanding, and complex algorithmic reasoning. We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic, combinatorics, graph theory, optimization, search, etc., aiming to evaluate the gap between visual data interpretation and algorithmic problem-solving skills. The dataset is generated automatically from code authored by humans. All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations. It ensures that our dataset can be scaled up arbitrarily in terms of reasoning complexity and dataset size. Our investi
&lt;/p&gt;</description></item><item><title>DeepCRE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#22312;&#24739;&#32773;&#32423;&#21035;CRE&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;17.7&#65285;&#65292;&#22312;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#24182;&#25104;&#21151;&#30830;&#23450;&#20102;&#20845;&#20010;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.03768</link><description>&lt;p&gt;
DeepCRE&#65306;&#21033;&#29992;&#23574;&#31471;&#35745;&#31639;&#27169;&#22411;&#25913;&#38761;&#33647;&#29289;&#30740;&#21457;
&lt;/p&gt;
&lt;p&gt;
DeepCRE: Revolutionizing Drug R&amp;D with Cutting-Edge Computational Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03768
&lt;/p&gt;
&lt;p&gt;
DeepCRE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#22312;&#24739;&#32773;&#32423;&#21035;CRE&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;17.7&#65285;&#65292;&#22312;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#24182;&#25104;&#21151;&#30830;&#23450;&#20102;&#20845;&#20010;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03768v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#33647;&#29289;&#24320;&#21457;&#39046;&#22495;&#21644;&#27835;&#30103;&#24212;&#29992;&#39046;&#22495;&#37117;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#27835;&#30103;&#39046;&#22495;&#38656;&#35201;&#26356;&#22810;&#30340;&#27835;&#30103;&#36873;&#25321;&#65292;&#21516;&#26102;&#22823;&#37327;&#26377;&#21069;&#26223;&#30340;&#20020;&#24202;&#21069;&#33647;&#29289;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#22833;&#36133;&#12290;&#19968;&#20010;&#21407;&#22240;&#26159;&#22312;&#33647;&#29289;&#24320;&#21457;&#30340;&#21518;&#26399;&#38454;&#27573;&#20132;&#21449;&#33647;&#29289;&#21453;&#24212;&#35780;&#20272;&#65288;CRE&#65289;&#30340;&#19981;&#36275;&#12290;&#23613;&#31649;&#35745;&#31639;&#26426;&#27169;&#25311;&#30340;CRE&#27169;&#22411;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23398;&#35201;&#20040;&#23616;&#38480;&#20110;&#26089;&#26399;&#24320;&#21457;&#38454;&#27573;&#65292;&#35201;&#20040;&#32570;&#20047;&#23545;&#20840;&#38754;CRE&#20998;&#26512;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeepCRE&#30340;&#26032;&#22411;&#35745;&#31639;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;DeepCRE&#22312;&#25512;&#21160;&#27835;&#30103;&#21457;&#29616;&#21644;&#21457;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;DeepCRE&#36890;&#36807;&#23454;&#29616;&#24739;&#32773;&#32423;&#21035;CRE&#24179;&#22343;&#24615;&#33021;&#25552;&#39640;17.7\%&#65292;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;DeepCRE&#24050;&#32463;&#30830;&#23450;&#20102;&#20845;&#20010;&#26174;&#31034;&#20986;&#26126;&#26174;&#26356;&#22823;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03768v1 Announce Type: new  Abstract: The field of pharmaceutical development and therapeutic application both face substantial challenges. Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development. Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis. Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\% in patient-level CRE, and a 5-fold increase in indication-level CRE. Furthermore, DeepCRE has identified six drug candidates that show significantly greater ef
&lt;/p&gt;</description></item><item><title>Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03640</link><description>&lt;p&gt;
Apollo&#65306;&#36731;&#37327;&#32423;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65306;&#35753;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#26222;&#24800;60&#20159;&#20154;
&lt;/p&gt;
&lt;p&gt;
Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03640
&lt;/p&gt;
&lt;p&gt;
Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20840;&#29699;&#21307;&#23398;&#30693;&#35782;&#30340;&#24222;&#22823;&#23384;&#20648;&#24211;&#20027;&#35201;&#26159;&#20197;&#33521;&#35821;&#20026;&#20027;&#65292;&#20294;&#22312;&#20256;&#36882;&#37327;&#36523;&#23450;&#21046;&#21307;&#30103;&#26381;&#21153;&#26041;&#38754;&#65292;&#26412;&#22320;&#35821;&#35328;&#23545;&#20110;&#22312;&#21307;&#30103;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#23588;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#23558;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#20154;&#32676;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#28085;&#30422;&#20840;&#29699;61&#20159;&#20154;&#21475;&#30340;&#20845;&#31181;&#26368;&#24120;&#29992;&#35821;&#35328;&#30340;&#21307;&#23398;LLMs&#12290;&#36825;&#19968;&#21162;&#21147;&#26368;&#32456;&#20419;&#25104;&#20102;ApolloCorpora&#22810;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#38598;&#21644;XMedBench&#22522;&#20934;&#30340;&#21019;&#24314;&#12290;&#22312;&#22810;&#35821;&#35328;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21457;&#24067;&#30340;Apollo&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#30456;&#23545;&#36739;&#23567;&#23610;&#23544;&#65288;&#21363;0.5B&#12289;1.8B&#12289;2B&#12289;6B&#21644;7B&#65289;&#19978;&#21462;&#24471;&#20102;&#19982;&#21516;&#31561;&#22823;&#23567;&#27169;&#22411;&#26368;&#20339;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;Apollo-7B&#26159;&#36804;&#20170;&#20026;&#27490;&#36798;&#21040;70B&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#36731;&#37327;&#32423;&#27169;&#22411;&#21487;&#29992;&#20110;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#36739;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03640v1 Announce Type: cross  Abstract: Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20840;&#29699;&#21355;&#29983;&#20844;&#24179;&#24615;&#65292;&#20197;&#38750;&#27954;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#33539;&#22260;&#23457;&#26597;&#21644;&#23450;&#24615;&#30740;&#31350;&#25581;&#31034;&#20102;ML&#25216;&#26415;&#22312;&#38750;&#27954;&#20581;&#24247;&#39046;&#22495;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#27542;&#27665;&#20027;&#20041;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03357</link><description>&lt;p&gt;
&#20840;&#29699;&#25512;&#24191;&#20844;&#24179;&#24615;&#30340;&#29702;&#30001;&#65306;&#20851;&#20110;&#27542;&#27665;&#20027;&#20041;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#38750;&#27954;&#20581;&#24247;&#30340;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism, AI, and Health in Africa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03357
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20840;&#29699;&#21355;&#29983;&#20844;&#24179;&#24615;&#65292;&#20197;&#38750;&#27954;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#33539;&#22260;&#23457;&#26597;&#21644;&#23450;&#24615;&#30740;&#31350;&#25581;&#31034;&#20102;ML&#25216;&#26415;&#22312;&#38750;&#27954;&#20581;&#24247;&#39046;&#22495;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#27542;&#27665;&#20027;&#20041;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#38271;&#65292;&#20154;&#20204;&#21628;&#21505;&#24320;&#21457;&#25216;&#26415;&#26469;&#29702;&#35299;&#21644;&#20943;&#36731;&#36825;&#20123;&#31995;&#32479;&#21487;&#33021;&#34920;&#29616;&#20986;&#30340;&#20559;&#35265;&#12290; &#20844;&#24179;&#24615;&#22312;&#20026;&#20581;&#24247;&#24320;&#21457;&#22522;&#20110;ML&#30340;&#35299;&#20915;&#26041;&#26696;&#26102;&#20855;&#26377;&#29305;&#23450;&#23545;&#38750;&#27954;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#38750;&#27954;&#24050;&#32463;&#38754;&#20020;&#20840;&#29699;&#21335;&#21271;&#20043;&#38388;&#19981;&#20844;&#24179;&#30340;&#26435;&#21147;&#22833;&#34913;&#12290; &#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#20840;&#29699;&#20581;&#24247;&#30340;&#20844;&#24179;&#24615;&#65292;&#20197;&#38750;&#27954;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#34892;&#33539;&#22260;&#23457;&#26597;&#65292;&#25552;&#20986;&#22312;&#38750;&#27954;&#29615;&#22659;&#20013;&#32771;&#34385;&#20844;&#24179;&#24615;&#30340;&#24046;&#36317;&#36724;&#65292;&#24182;&#21246;&#30011;&#23427;&#20204;&#21487;&#33021;&#22312;&#19981;&#21516;ML&#21551;&#29992;&#30340;&#21307;&#30103;&#27169;&#24335;&#20013;&#20135;&#29983;&#24433;&#21709;&#30340;&#21306;&#22495;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;672&#21517;&#19968;&#33324;&#20154;&#21475;&#30740;&#31350;&#21442;&#19982;&#32773;&#21644;28&#21517;&#19987;&#23478;&#36827;&#34892;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#20182;&#20204;&#20851;&#27880;&#30340;&#26159;&#19982;&#38750;&#27954;&#26377;&#20851;&#30340;ML&#12289;&#20581;&#24247;&#21644;&#25919;&#31574;&#65292;&#20197;&#33719;&#24471;&#20851;&#20110;&#24050;&#25552;&#20986;&#30340;&#24046;&#36317;&#36724;&#30340;&#35777;&#23454;&#24615;&#35777;&#25454;&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#32858;&#28966;&#20110;&#27542;&#27665;&#20027;&#20041;&#20316;&#20026;&#32508;&#21512;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03357v1 Announce Type: new  Abstract: With growing application of machine learning (ML) technologies in healthcare, there have been calls for developing techniques to understand and mitigate biases these systems may exhibit. Fair-ness considerations in the development of ML-based solutions for health have particular implications for Africa, which already faces inequitable power imbalances between the Global North and South.This paper seeks to explore fairness for global health, with Africa as a case study. We conduct a scoping review to propose axes of disparities for fairness consideration in the African context and delineate where they may come into play in different ML-enabled medical modalities. We then conduct qualitative research studies with 672 general population study participants and 28 experts inML, health, and policy focused on Africa to obtain corroborative evidence on the proposed axes of disparities. Our analysis focuses on colonialism as the attribute of inte
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;In-Dialogue Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#21051;&#30011;&#20010;&#20154;&#35774;&#26469;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#36827;&#23545;&#35805;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03102</link><description>&lt;p&gt;
&#8220;&#22312;&#23545;&#35805;&#20013;&#23398;&#20064;&#8221;&#65306;&#36890;&#36807;&#23545;&#35805;&#20013;&#23398;&#20064;&#23454;&#29616;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#30340;&#20010;&#24615;&#21270;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
"In Dialogues We Learn": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03102
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;In-Dialogue Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#21051;&#30011;&#20010;&#20154;&#35774;&#26469;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#36827;&#23545;&#35805;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#20854;&#33021;&#22815;&#29983;&#25104;&#19982;&#19981;&#21516;&#20154;&#35774;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#39044;&#23450;&#20041;&#30340;&#20010;&#20154;&#36164;&#26009;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#65292;&#36824;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;In-Dialogue Learning&#65288;IDL&#65289;&#65292;&#19968;&#31181;&#24494;&#35843;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#23545;&#35805;&#21382;&#21490;&#26469;&#21051;&#30011;&#20010;&#20154;&#35774;&#65292;&#20197;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IDL&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;BLEU&#21644;ROUGE&#20998;&#25968;&#20998;&#21035;&#22686;&#21152;&#20102;&#39640;&#36798;200%&#21644;247%&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#35780;&#20272;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03102v1 Announce Type: cross  Abstract: Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.
&lt;/p&gt;</description></item><item><title>NASH&#26159;&#19968;&#31181;&#23558;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#30828;&#20214;&#35774;&#35745;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#12289;&#20302;&#24310;&#36831;&#21644;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.01845</link><description>&lt;p&gt;
NASH&#65306;&#29992;&#20110;&#30828;&#20214;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01845
&lt;/p&gt;
&lt;p&gt;
NASH&#26159;&#19968;&#31181;&#23558;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#30828;&#20214;&#35774;&#35745;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#12289;&#20302;&#24310;&#36831;&#21644;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#20013;&#37096;&#32626;&#65292;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#22312;&#39640;&#20934;&#30830;&#24615;&#12289;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NASH&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#12290;&#20351;&#29992;NASH&#65292;&#30828;&#20214;&#35774;&#35745;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#36824;&#21487;&#20197;&#23454;&#29616;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#34920;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#20010;&#29256;&#26412;&#30340;NASH&#31574;&#30053;&#65292;&#25152;&#26377;&#36825;&#20123;&#31574;&#30053;&#26174;&#31034;&#20986;&#27604;&#21407;&#22987;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#20247;&#22810;&#27169;&#22411;&#25805;&#20316;&#20013;&#36873;&#25321;&#29305;&#23450;&#25805;&#20316;&#65292;&#24341;&#23548;&#35757;&#32451;&#36807;&#31243;&#26397;&#21521;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312; ResNet18 &#25110; ResNet34 &#19978;&#24212;&#29992;NASH&#65292;&#19982;&#38750;NASH&#29256;&#26412;&#30456;&#27604;&#65292;&#21487;&#20351;Top1&#20934;&#30830;&#29575;&#25552;&#39640;&#39640;&#36798;3.1%&#65292;Top5&#20934;&#30830;&#29575;&#25552;&#39640;&#39640;&#36798;2.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01845v1 Announce Type: cross  Abstract: As machine learning (ML) algorithms get deployed in an ever-increasing number of applications, these algorithms need to achieve better trade-offs between high accuracy, high throughput and low latency. This paper introduces NASH, a novel approach that applies neural architecture search to machine learning hardware. Using NASH, hardware designs can achieve not only high throughput and low latency but also superior accuracy performance. We present four versions of the NASH strategy in this paper, all of which show higher accuracy than the original models. The strategy can be applied to various convolutional neural networks, selecting specific model operations among many to guide the training process toward higher accuracy. Experimental results show that applying NASH on ResNet18 or ResNet34 achieves a top 1 accuracy increase of up to 3.1% and a top 5 accuracy increase of up to 2.2% compared to the non-NASH version when tested on the Imag
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36755;&#20986;&#29992;&#20316;&#31532;&#20108;&#38454;&#27573;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#26102;&#21051;&#26816;&#32034;&#21644;&#37325;&#28857;&#26816;&#27979;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.01437</link><description>&lt;p&gt;
GPTSee&#65306;&#36890;&#36807;&#22522;&#20110;&#25551;&#36848;&#30340;&#30456;&#20284;&#29305;&#24449;&#22686;&#24378;&#26102;&#21051;&#26816;&#32034;&#21644;&#37325;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GPTSee: Enhancing Moment Retrieval and Highlight Detection via Description-Based Similarity Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01437
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36755;&#20986;&#29992;&#20316;&#31532;&#20108;&#38454;&#27573;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#26102;&#21051;&#26816;&#32034;&#21644;&#37325;&#28857;&#26816;&#27979;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#21051;&#26816;&#32034;&#65288;MR&#65289;&#21644;&#37325;&#28857;&#26816;&#27979;&#65288;HD&#65289;&#26088;&#22312;&#20174;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20013;&#35782;&#21035;&#35270;&#39057;&#20013;&#30340;&#30456;&#20851;&#26102;&#21051;&#21644;&#37325;&#28857;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MR&#21644;HD&#26041;&#27861;&#23578;&#26410;&#19982;LLMs&#38598;&#25104;&#12290;&#22312;&#36825;&#23553;&#20449;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#23558;LLMs&#30340;&#36755;&#20986;&#20316;&#20026;&#31532;&#20108;&#38454;&#27573;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#36755;&#20837;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;MiniGPT-4&#29983;&#25104;&#35270;&#39057;&#24103;&#30340;&#35814;&#32454;&#25551;&#36848;&#24182;&#37325;&#20889;&#26597;&#35810;&#35821;&#21477;&#65292;&#23558;&#20854;&#20316;&#20026;&#26032;&#29305;&#24449;&#36755;&#20837;&#32534;&#30721;&#22120;&#12290;&#28982;&#21518;&#35745;&#31639;&#29983;&#25104;&#25551;&#36848;&#21644;&#37325;&#20889;&#26597;&#35810;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#26368;&#21518;&#65292;&#36830;&#32493;&#39640;&#30456;&#20284;&#24615;&#35270;&#39057;&#24103;&#34987;&#36716;&#25442;&#20026;&#33539;&#22260;&#38170;&#28857;&#65292;&#20316;&#20026;&#35299;&#30721;&#22120;&#30340;&#20808;&#39564;&#20301;&#32622;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01437v1 Announce Type: cross  Abstract: Moment retrieval (MR) and highlight detection (HD) aim to identify relevant moments and highlights in video from corresponding natural language query. Large language models (LLMs) have demonstrated proficiency in various computer vision tasks. However, existing methods for MR\&amp;HD have not yet been integrated with LLMs. In this letter, we propose a novel two-stage model that takes the output of LLMs as the input to the second-stage transformer encoder-decoder. First, MiniGPT-4 is employed to generate the detailed description of the video frame and rewrite the query statement, fed into the encoder as new features. Then, semantic similarity is computed between the generated description and the rewritten queries. Finally, continuous high-similarity video frames are converted into span anchors, serving as prior position information for the decoder. Experiments demonstrate that our approach achieves a state-of-the-art result, and by using on
&lt;/p&gt;</description></item><item><title>PRIME&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#24182;&#23398;&#20064;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.00929</link><description>&lt;p&gt;
&#21033;&#29992;&#34892;&#20026;&#21407;&#35821;&#25645;&#24314;&#20219;&#21153;&#30340;&#26694;&#26550;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00929
&lt;/p&gt;
&lt;p&gt;
PRIME&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#24182;&#23398;&#20064;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#35753;&#26426;&#22120;&#20154;&#23398;&#20250;&#22797;&#26434;&#30340;&#25805;&#20316;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#21463;&#21040;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#22797;&#21512;&#35823;&#24046;&#20250;&#22312;&#20219;&#21153;&#26102;&#27573;&#20869;&#32047;&#31215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRIME&#65288;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#30340;&#25968;&#25454;&#25928;&#29575;&#27169;&#20223;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#20223;&#23398;&#20064;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;PRIME&#36890;&#36807;&#23558;&#20219;&#21153;&#28436;&#31034;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#26469;&#25645;&#24314;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#23398;&#20064;&#19968;&#20010;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#26469;&#23545;&#21407;&#35821;&#24207;&#21015;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PRIME&#22312;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#29575;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#39640;&#20986;10-34&#65285;&#65292;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#39640;&#20986;20-48&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00929v1 Announce Type: cross  Abstract: Imitation learning has shown great potential for enabling robots to acquire complex manipulation behaviors. However, these algorithms suffer from high sample complexity in long-horizon tasks, where compounding errors accumulate over the task horizons. We present PRIME (PRimitive-based IMitation with data Efficiency), a behavior primitive-based framework designed for improving the data efficiency of imitation learning. PRIME scaffolds robot tasks by decomposing task demonstrations into primitive sequences, followed by learning a high-level control policy to sequence primitives through imitation learning. Our experiments demonstrate that PRIME achieves a significant performance improvement in multi-stage manipulation tasks, with 10-34% higher success rates in simulation over state-of-the-art baselines and 20-48% on physical hardware.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.00858</link><description>&lt;p&gt;
&#30452;&#25509;&#19982;Chat-Fine-Tuned LLMs&#30340;&#33609;&#26696;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00858
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33258;&#22238;&#24402;&#26412;&#36136;&#12289;&#24040;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#26377;&#38480;&#30340;&#20869;&#23384;&#24102;&#23485;&#32780;&#34987;&#35748;&#20026;&#26159;&#20869;&#23384;&#23494;&#38598;&#22411;&#65292;&#36890;&#24120;&#23548;&#33268;&#20302;&#20196;&#29260;&#36895;&#29575;&#12290;&#29468;&#27979;&#35299;&#30721;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;LLM&#25512;&#29702;&#21152;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#24320;&#28304;LLM&#31995;&#21015;&#20013;&#65292;&#20363;&#22914;Llama 2 7B&#65292;&#30001;&#20110;&#33609;&#26696;&#27169;&#22411;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#33609;&#26696;&#27169;&#22411;&#20197;&#36890;&#36807;&#29468;&#27979;&#35299;&#30721;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#33609;&#26696;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#30452;&#25509;&#19982;Chat-capable&#30446;&#26631;&#27169;&#22411;&#23545;&#40784;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;Llama 2 Chat Drafter 115M&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26694;&#26550;&#20165;&#21253;&#25324;&#39044;&#35757;&#32451;&#12289;&#33976;&#39311;&#25968;&#25454;&#38598;&#29983;&#25104;&#21644;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#24494;&#35843;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#23545;&#40784;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
&lt;/p&gt;</description></item><item><title>TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19467</link><description>&lt;p&gt;
TV-TREES&#65306;&#29992;&#20110;&#31070;&#32463;&#31526;&#21495;&#35270;&#39057;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;
&lt;/p&gt;
&lt;p&gt;
TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19467
&lt;/p&gt;
&lt;p&gt;
TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#30005;&#35270;&#21098;&#36753;&#31561;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#38382;&#31572;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#37096;&#20998;&#26159;&#22240;&#20026;&#24403;&#21069;&#30340;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20381;&#36182;&#20110;&#21333;&#27169;&#24577;&#25512;&#29702;&#65292;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TV-TREES&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#12290;TV-TREES&#20316;&#20026;&#19968;&#31181;&#20419;&#36827;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#20219;&#21153;&#26469;&#35780;&#20272;&#27492;&#31867;&#26041;&#27861;&#30340;&#25512;&#29702;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#30340;&#12289;&#20855;&#26377;&#26368;&#20808;&#36827;&#38646;-shot&#24615;&#33021;&#30340;&#23436;&#25972;&#35270;&#39057;&#21098;&#36753;&#65292;&#23637;&#31034;&#20102;&#19982;&#40657;&#30418;&#26041;&#27861;&#30456;&#27604;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19467v1 Announce Type: cross  Abstract: It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods.
&lt;/p&gt;</description></item><item><title>&#35821;&#20041;&#21464;&#21270;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#27492;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;</title><link>https://arxiv.org/abs/2402.19088</link><description>&lt;p&gt;
&#23545;&#35821;&#20041;&#21464;&#21270;&#29305;&#24449;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey in Characterization of Semantic Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19088
&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#21464;&#21270;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#27492;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#35821;&#35328;&#19981;&#26029;&#21457;&#23637;&#65292;&#20197;&#21560;&#32435;&#20154;&#31867;&#31038;&#20250;&#30340;&#25991;&#21270;&#21464;&#21270;&#12290;&#36825;&#31181;&#28436;&#21464;&#36890;&#36807;&#26032;&#35789;&#35821;&#65288;&#26032;&#21333;&#35789;&#65289;&#25110;&#21333;&#35789;&#30340;&#35821;&#20041;&#21464;&#21270;&#65288;&#36171;&#20104;&#24050;&#26377;&#21333;&#35789;&#26032;&#30340;&#21547;&#20041;&#65289;&#26469;&#20307;&#29616;&#12290;&#29702;&#35299;&#21333;&#35789;&#30340;&#21547;&#20041;&#23545;&#35299;&#37322;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#65288;&#22320;&#26041;&#29992;&#35821;&#25110;&#20442;&#35821;&#65289;&#12289;&#39046;&#22495;&#65288;&#20363;&#22914;&#25216;&#26415;&#26415;&#35821;&#65289;&#25110;&#26102;&#20195;&#30340;&#25991;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#65292;&#36825;&#20123;&#21333;&#35789;&#19982;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30456;&#20851;&#65292;&#20363;&#22914;&#32763;&#35793;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#38382;&#31572;&#31561;&#12290;&#35821;&#20041;&#21464;&#21270;&#21487;&#33021;&#20250;&#24433;&#21709;&#36825;&#20123;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#21644;&#24418;&#24335;&#21270;&#34920;&#24449;&#36825;&#20123;&#21464;&#21270;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#30740;&#31350;&#36825;&#31181;&#24433;&#21709;&#26159;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#36817;&#26399;&#24341;&#36215;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20960;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#26816;&#27979;&#35821;&#20041;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#21162;&#21147;&#26469;&#23545;&#20854;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19088v1 Announce Type: cross  Abstract: Live languages continuously evolve to integrate the cultural change of human societies. This evolution manifests through neologisms (new words) or \textbf{semantic changes} of words (new meaning to existing words). Understanding the meaning of words is vital for interpreting texts coming from different cultures (regionalism or slang), domains (e.g., technical terms), or periods. In computer science, these words are relevant to computational linguistics algorithms such as translation, information retrieval, question answering, etc. Semantic changes can potentially impact the quality of the outcomes of these algorithms. Therefore, it is important to understand and characterize these changes formally. The study of this impact is a recent problem that has attracted the attention of the computational linguistics community. Several approaches propose methods to detect semantic changes with good precision, but more effort is needed to charact
&lt;/p&gt;</description></item><item><title>ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18609</link><description>&lt;p&gt;
ICE-SEARCH: &#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH: A Language Model-Driven Feature Selection Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18609
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;In-Context Evolutionary Search (ICE-SEARCH)&#26041;&#27861;&#65292;&#36825;&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;(LMs)&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;(FS)&#20219;&#21153;&#30340;&#24037;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;(MPA)&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;ICE-SEARCH&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#20132;&#21449;&#21644;&#31361;&#21464;&#33021;&#21147;&#65292;&#22312;&#19968;&#20010;&#36827;&#21270;&#26694;&#26550;&#20869;&#26174;&#30528;&#25913;&#36827;&#29305;&#24449;&#36873;&#25321;&#65292;&#36890;&#36807;&#27169;&#22411;&#30340;&#20840;&#38754;&#19990;&#30028;&#30693;&#35782;&#21644;&#20854;&#36866;&#24212;&#21508;&#31181;&#35282;&#33394;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#35813;&#26041;&#27861;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#19977;&#20010;&#20851;&#38190;&#30340;MPA&#20219;&#21153;&#65306;&#20013;&#39118;&#12289;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;&#31958;&#23615;&#30149;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;ICE-SEARCH&#22312;&#30830;&#23450;&#21307;&#23398;&#24212;&#29992;&#30340;&#20851;&#38190;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;FS&#26041;&#27861;&#12290;ICE-SEARCH&#22312;&#20013;&#39118;&#39044;&#27979;&#21644;&#31958;&#23615;&#30149;&#39044;&#27979;&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#27700;&#24179;&#65307;&#20915;&#31574;&#38543;&#26426;&#21270;ICE-SEARCH&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#20013;&#25490;&#21517;&#20026;&#39046;&#20808;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18609v1 Announce Type: cross  Abstract: This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method, the first work that melds language models (LMs) with evolutionary algorithms for feature selection (FS) tasks and demonstrates its effectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and mutation capabilities inherent in LMs within an evolutionary framework, significantly improving FS through the model's comprehensive world knowledge and its adaptability to a variety of roles. Our evaluation of this methodology spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional FS methods in pinpointing essential features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction and diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease prediction. Our results not only demonstrate
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18603</link><description>&lt;p&gt;
MMSR&#65306;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MMSR: Symbolic Regression is a Multimodal Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18603
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#26159;&#25506;&#32034;&#33258;&#28982;&#35268;&#24459;&#20960;&#21315;&#24180;&#26469;&#20154;&#31867;&#26234;&#24935;&#30340;&#32467;&#26230;&#12290;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#20844;&#24335;&#25551;&#36848;&#22797;&#26434;&#30340;&#33258;&#28982;&#35268;&#24459;&#26159;&#31185;&#23398;&#23478;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#39046;&#22495;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#20174;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
&lt;/p&gt;</description></item><item><title>LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;</title><link>https://arxiv.org/abs/2402.16906</link><description>&lt;p&gt;
LDB&#65306;&#36890;&#36807;&#36880;&#27493;&#39564;&#35777;&#36816;&#34892;&#26102;&#25191;&#34892;&#26469;&#35843;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16906
&lt;/p&gt;
&lt;p&gt;
LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19981;&#20165;&#23558;&#21333;&#27425;&#20195;&#30721;&#29983;&#25104;&#65292;&#32780;&#19988;&#36824;&#23558;&#21333;&#20803;&#27979;&#35797;&#21644;&#31243;&#24207;&#39564;&#35777;&#22120;&#25972;&#21512;&#21040;LLMs&#20013;&#65292;&#20197;&#36845;&#20195;&#22320;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23558;&#29983;&#25104;&#30340;&#31243;&#24207;&#35270;&#20026;&#19981;&#21487;&#20998;&#21106;&#30340;&#23454;&#20307;&#65292;&#36825;&#23545;LLMs&#22312;&#35843;&#35797;&#31243;&#24207;&#26102;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#24403;&#31243;&#24207;&#21253;&#21547;&#22797;&#26434;&#30340;&#36923;&#36753;&#27969;&#31243;&#21644;&#25968;&#25454;&#25805;&#20316;&#26102;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#31867;&#24320;&#21457;&#20154;&#21592;&#35843;&#35797;&#31243;&#24207;&#26102;&#65292;&#20182;&#20204;&#36890;&#24120;&#35774;&#32622;&#26029;&#28857;&#24182;&#26377;&#36873;&#25321;&#22320;&#26816;&#26597;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#12290;&#25191;&#34892;&#27969;&#21644;&#20013;&#38388;&#21464;&#37327;&#22312;&#35843;&#35797;&#36807;&#31243;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#25991;&#29486;&#20013;&#26410;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#35797;&#22120;&#65288;LDB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;LLMs&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#23436;&#21892;&#20854;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16906v1 Announce Type: cross  Abstract: Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifical
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;MIM-Reasoner&#65292;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#32473;&#23450;&#22810;&#37325;&#32593;&#32476;&#20869;&#37096;&#21644;&#23618;&#38388;&#30340;&#22797;&#26434;&#20256;&#25773;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;MIM&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16898</link><description>&lt;p&gt;
MIM-Reasoner: &#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#22810;&#37325;&#24433;&#21709;&#26368;&#22823;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex Influence Maximization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16898
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;MIM-Reasoner&#65292;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#32473;&#23450;&#22810;&#37325;&#32593;&#32476;&#20869;&#37096;&#21644;&#23618;&#38388;&#30340;&#22797;&#26434;&#20256;&#25773;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;MIM&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#37325;&#24433;&#21709;&#26368;&#22823;&#21270;&#65288;MIM&#65289;&#35201;&#27714;&#25105;&#20204;&#35782;&#21035;&#19968;&#32452;&#31181;&#23376;&#29992;&#25143;&#65292;&#20197;&#26368;&#22823;&#21270;&#22810;&#37325;&#32593;&#32476;&#20013;&#21463;&#24433;&#21709;&#29992;&#25143;&#30340;&#39044;&#26399;&#25968;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MIM-Reasoner&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#27010;&#29575;&#22270;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#25429;&#25417;&#32473;&#23450;&#22810;&#37325;&#32593;&#32476;&#20869;&#37096;&#21644;&#23618;&#38388;&#30340;&#22797;&#26434;&#20256;&#25773;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;MIM&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16898v1 Announce Type: cross  Abstract: Multiplex influence maximization (MIM) asks us to identify a set of seed users such as to maximize the expected number of influenced users in a multiplex network. MIM has been one of central research topics, especially in nowadays social networking landscape where users participate in multiple online social networks (OSNs) and their influences can propagate among several OSNs simultaneously. Although there exist a couple combinatorial algorithms to MIM, learning-based solutions have been desired due to its generalization ability to heterogeneous networks and their diversified propagation characteristics. In this paper, we introduce MIM-Reasoner, coupling reinforcement learning with probabilistic graphical model, which effectively captures the complex propagation process within and between layers of a given multiplex network, thereby tackling the most challenging problem in MIM. We establish a theoretical guarantee for MIM-Reasoner as w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.16363</link><description>&lt;p&gt;
LLM&#25512;&#26029;&#25581;&#31034;&#65306;&#35843;&#26597;&#19982;Roofline&#27169;&#22411;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
LLM Inference Unveiled: Survey and Roofline Model Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#26029;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#25552;&#20379;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#29420;&#29305;&#32467;&#21512;&#12290;&#34429;&#28982;&#35813;&#39046;&#22495;&#24050;&#32463;&#25193;&#23637;&#24182;&#20805;&#28385;&#27963;&#21147;&#65292;&#20294;&#33267;&#20170;&#36824;&#27809;&#26377;&#19968;&#20010;&#31616;&#26126;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;LLM&#25512;&#26029;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#28165;&#26224;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#19981;&#20165;&#24635;&#32467;&#20102;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#36824;&#22522;&#20110;Roofline&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#12290;&#36825;&#19968;&#26694;&#26550;&#33021;&#22815;&#24110;&#21161;&#35782;&#21035;LLM&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#26041;&#38754;&#65292;&#20174;&#32780;&#20026;&#37096;&#32626;LLM&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27719;&#24635;&#20102;&#39640;&#25928;LLM&#25512;&#26029;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20851;&#38190;&#39046;&#22495;&#65292;&#27604;&#22914;&#26435;&#37325;&#20248;&#21270;&#65288;&#22914;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16363v1 Announce Type: cross  Abstract: The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM. Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantizatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2402.16278</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#27880;&#37322;&#23884;&#20837;&#27169;&#22411;&#30340;&#26412;&#20307;&#21253;&#21547;&#20851;&#31995;&#39044;&#27979;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16278
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#34920;&#31034;&#23454;&#20307;&#30340;&#26412;&#20307;&#23884;&#20837;&#65292;&#29992;&#20110;&#26412;&#20307;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#26412;&#20307;&#23884;&#20837;&#26410;&#35299;&#20915;&#31867;&#20284;&#21644;&#23396;&#31435;&#23454;&#20307;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#26410;&#25552;&#21462;&#26412;&#20307;&#20013;&#27880;&#37322;&#20844;&#29702;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#30340;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65306;Inverted-index Matrix Embedding (InME) &#21644; Co-occurrence Matrix Embedding (CoME)&#12290;&#36825;&#20004;&#31181;&#23884;&#20837;&#36890;&#36807;&#27599;&#20010;&#21333;&#35789;&#22312;&#19968;&#32452;&#20844;&#29702;&#20013;&#20986;&#29616;&#30340;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#20844;&#29702;&#20013;&#21333;&#35789;&#30340;&#20849;&#29616;&#26469;&#25429;&#33719;&#27880;&#37322;&#20844;&#29702;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#12290;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;&#65292;&#24403;&#39044;&#27979;&#30340;&#36229;&#31867;&#19982;&#23376;&#31867;&#30456;&#20284;&#19988;&#23396;&#31435;&#20110;&#26412;&#20307;&#20013;&#30340;&#20854;&#20182;&#23454;&#20307;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16278v1 Announce Type: new  Abstract: Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion. However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology. In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom. The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology. Our evaluation experiments show that
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#65292;&#21457;&#29616;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;BERT&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.16034</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#36827;&#34892;&#24773;&#32490;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Emotion Classification in Short English Texts using Deep Learning Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#65292;&#21457;&#29616;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;BERT&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#20013;&#30340;&#26377;&#38480;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#24773;&#32490;&#26159;&#19968;&#39033;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#26694;&#26550;&#21644;&#35745;&#31639;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#30740;&#31350;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#35789;&#23884;&#20837;&#65292;&#29305;&#21035;&#26159;BERT&#65292;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;SmallEnglishEmotions&#8221;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;6372&#20010;&#24102;&#26377;&#20116;&#31181;&#20027;&#35201;&#24773;&#32490;&#31867;&#21035;&#27880;&#37322;&#30340;&#19981;&#21516;&#30701;&#27874;&#26031;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36801;&#31227;&#23398;&#20064;&#21644;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#23884;&#20837;&#22312;&#20934;&#30830;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#25991;&#26412;&#26041;&#38754;&#20248;&#20110;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16034v1 Announce Type: cross  Abstract: Detecting emotions in limited text datasets from under-resourced languages presents a formidable obstacle, demanding specialized frameworks and computational strategies. This study conducts a thorough examination of deep learning techniques for discerning emotions in short English texts. Deep learning approaches employ transfer learning and word embedding, notably BERT, to attain superior accuracy. To evaluate these methods, we introduce the "SmallEnglishEmotions" dataset, comprising 6372 varied short Persian texts annotated with five primary emotion categories. Our experiments reveal that transfer learning and BERT-based text embedding outperform alternative methods in accurately categorizing the text in the dataset.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35777;&#25454;&#27169;&#24335;&#25512;&#29702;&#27169;&#22411;&#65288;EPRM&#65289;&#20197;&#26356;&#22909;&#22320;&#31526;&#21512;&#20915;&#31574;&#30446;&#26631;&#65292;&#24341;&#20837;&#38543;&#26426;&#22270;&#38598;&#65288;RGS&#65289;&#26469;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#24182;&#34920;&#31034;&#26356;&#22810;&#20107;&#20214;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.13058</link><description>&lt;p&gt;
&#38543;&#26426;&#22270;&#38598;&#21644;&#35777;&#25454;&#27169;&#24335;&#25512;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Random Graph Set and Evidence Pattern Reasoning Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13058
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35777;&#25454;&#27169;&#24335;&#25512;&#29702;&#27169;&#22411;&#65288;EPRM&#65289;&#20197;&#26356;&#22909;&#22320;&#31526;&#21512;&#20915;&#31574;&#30446;&#26631;&#65292;&#24341;&#20837;&#38543;&#26426;&#22270;&#38598;&#65288;RGS&#65289;&#26469;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#24182;&#34920;&#31034;&#26356;&#22810;&#20107;&#20214;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35777;&#25454;&#29702;&#35770;&#22312;&#20915;&#31574;&#21644;&#25512;&#29702;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#65292;&#36716;&#31227;&#20449;&#24565;&#27169;&#22411;&#65288;TBM&#65289;&#26159;&#24120;&#29992;&#30340;&#35777;&#25454;&#20915;&#31574;&#27169;&#22411;&#65292;&#20294;TBM&#26159;&#19968;&#20010;&#38750;&#20559;&#22909;&#27169;&#22411;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#31526;&#21512;&#20915;&#31574;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#35777;&#25454;&#27169;&#24335;&#25512;&#29702;&#27169;&#22411;&#65288;EPRM&#65289;&#12290;&#36890;&#36807;&#23450;&#20041;&#27169;&#24335;&#36816;&#31639;&#31526;&#21644;&#20915;&#31574;&#36816;&#31639;&#31526;&#65292;&#21487;&#20197;&#20026;&#19981;&#21516;&#20219;&#21153;&#35774;&#32622;&#30456;&#24212;&#30340;&#20559;&#22909;&#12290;&#38543;&#26426;&#25490;&#21015;&#38598;&#65288;RPS&#65289;&#20026;&#35777;&#25454;&#29702;&#35770;&#25193;&#23637;&#20102;&#39034;&#24207;&#20449;&#24687;&#12290;RPS&#24456;&#38590;&#21051;&#30011;&#26679;&#26412;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#22914;&#24490;&#29615;&#12289;&#24182;&#34892;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#38543;&#26426;&#22270;&#38598;&#65288;RGS&#65289;&#26469;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#24182;&#34920;&#31034;&#26356;&#22810;&#20107;&#20214;&#31867;&#22411;&#12290;&#20026;&#20102;&#35828;&#26126;RGS&#21644;EPRM&#30340;&#37325;&#35201;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#39033;&#39134;&#26426;&#36895;&#24230;&#25490;&#24207;&#23454;&#39564;&#65292;&#24182;&#27169;&#25311;&#20102;10,000&#20010;&#26696;&#20363;&#12290;EPRM&#30340;&#23454;&#29616;&#31216;&#20026;&#20914;&#31361;&#35299;&#20915;Dec
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13058v1 Announce Type: new  Abstract: Evidence theory is widely used in decision-making and reasoning systems. In previous research, Transferable Belief Model (TBM) is a commonly used evidential decision making model, but TBM is a non-preference model. In order to better fit the decision making goals, the Evidence Pattern Reasoning Model (EPRM) is proposed. By defining pattern operators and decision making operators, corresponding preferences can be set for different tasks. Random Permutation Set (RPS) expands order information for evidence theory. It is hard for RPS to characterize the complex relationship between samples such as cycling, paralleling relationships. Therefore, Random Graph Set (RGS) were proposed to model complex relationships and represent more event types. In order to illustrate the significance of RGS and EPRM, an experiment of aircraft velocity ranking was designed and 10,000 cases were simulated. The implementation of EPRM called Conflict Resolution Dec
&lt;/p&gt;</description></item><item><title>Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;</title><link>https://arxiv.org/abs/2402.12749</link><description>&lt;p&gt;
Me LLaMA: &#20026;&#21307;&#30103;&#24212;&#29992;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Me LLaMA: Foundation Large Language Models for Medical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12749
&lt;/p&gt;
&lt;p&gt;
Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#19981;&#22815;&#29702;&#24819;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22312;&#22823;&#22411;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Me LLaMA&#65292;&#19968;&#20010;&#21307;&#23398;LLM&#31995;&#21015;&#65292;&#21253;&#25324;&#22522;&#30784;&#27169;&#22411;- Me LLaMA 13/70B&#21450;&#20854; chat-enhanced &#29256;&#26412;- Me LLaMA 13/70B-chat&#65292;&#36890;&#36807;&#25345;&#32493;&#23545;LLaMA2&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#25968;&#25454;&#24320;&#21457;&#32780;&#25104;&#12290;&#25105;&#20204;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#22871;&#20214;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;129B tokens&#30340;&#22823;&#35268;&#27169;&#25345;&#32493;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#21253;&#21547;214k&#20010;&#26679;&#26412;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#36328;&#36234;14&#20010;&#25968;&#25454;&#38598;&#30340;&#20845;&#39033;&#20219;&#21153;&#30340;&#21307;&#23398;&#35780;&#20272;&#22522;&#20934;(MIBE)&#12290;&#25105;&#20204;&#20351;&#29992;MIBE&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#65292;Me LLaMA&#27169;&#22411;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24320;&#28304;&#21307;&#23398;LLMs&#65292;&#24182;&#19988;&#22312;&#21830;&#19994;&#24040;&#22836;&#22914;ChatGPT&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12749v1 Announce Type: cross  Abstract: Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets. This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data. Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 
&lt;/p&gt;</description></item><item><title>&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11078</link><description>&lt;p&gt;
&#36890;&#36807;&#32431;&#24494;&#35843;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Model Editing by Pure Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11078
&lt;/p&gt;
&lt;p&gt;
&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#35843;&#25972;&#34987;&#35748;&#20026;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#19981;&#22815;&#26377;&#25928;&#65292;&#22240;&#20026;&#30456;&#23545;&#26356;&#19987;&#19994;&#30340;&#26041;&#27861;&#32780;&#35328;&#65292;&#23427;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#26159;&#31616;&#21333;&#30340;&#65292;&#19981;&#20851;&#24515;&#34987;&#32534;&#36753;&#27169;&#22411;&#30340;&#20307;&#31995;&#32467;&#26500;&#32454;&#33410;&#65292;&#24182;&#19988;&#33021;&#22815;&#21033;&#29992;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#30340;&#19981;&#26029;&#36827;&#23637;&#65288;&#20363;&#22914;PEFT&#65289;&#65292;&#20351;&#20854;&#25104;&#20026;&#27169;&#22411;&#32534;&#36753;&#22120;&#30340;&#21560;&#24341;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32431;&#31929;&#30340;&#24494;&#35843;&#21487;&#20197;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#26420;&#32032;&#24494;&#35843;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#32780;&#38750;&#23436;&#25972;&#20284;&#28982;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#26469;&#22686;&#21152;&#25968;&#25454;&#65292;&#20197;&#40723;&#21169;&#27867;&#21270;&#21644;&#23616;&#37096;&#24615;&#12290;&#25105;&#20204;&#22312;ZsRE&#21644;CounterFact&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#19968;&#31616;&#21333;&#20462;&#25913;&#20351;&#24471;&#24494;&#35843;&#36890;&#24120;&#21487;&#20197;&#19982;&#19987;&#19994;&#32534;&#36753;&#22120;&#22312;&#32534;&#36753;&#20998;&#25968;&#26041;&#38754;&#21305;&#25932;&#29978;&#33267;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11078v1 Announce Type: cross  Abstract: Fine-tuning is dismissed as not effective for model editing due to its poor performance compared to more specialized methods. However, fine-tuning is simple, agnostic to the architectural details of the model being edited, and able to leverage ongoing advances in standard training methods (e.g., PEFT), making it an appealing choice for a model editor. In this work, we show that pure fine-tuning can be a viable approach to model editing. We propose a slight modification of naive fine-tuning with two key ingredients. First, we optimize the conditional likelihood rather than the full likelihood. Second, we augment the data with random paraphrases and facts to encourage generalization and locality. Our experiments on ZsRE and CounterFact show that this simple modification allows fine-tuning to often match or outperform specialized editors in the edit score.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;GNN&#20013;&#36793;&#35299;&#38500;&#36807;&#31243;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#36807;&#24230;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10695</link><description>&lt;p&gt;
&#19982;&#36951;&#24536;&#21628;&#24212;&#30340;&#35299;&#38500;&#38142;&#25509;&#65306;&#31616;&#21270;GNN&#20013;&#30340;&#36793;&#35299;&#38500;
&lt;/p&gt;
&lt;p&gt;
Unlink to Unlearn: Simplifying Edge Unlearning in GNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10695
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;GNN&#20013;&#36793;&#35299;&#38500;&#36807;&#31243;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#36807;&#24230;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#25285;&#24551;&#21152;&#21095;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#30340;&#35299;&#38500;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#23398;&#26415;&#30028;&#19968;&#20010;&#31361;&#20986;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;&#36825;&#19968;&#27010;&#24565;&#22312;&#24378;&#35843;&#34987;&#36951;&#24536;&#26435;&#21033;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#22312;&#29992;&#25143;&#35831;&#27714;&#26102;&#26377;&#36873;&#25321;&#24615;&#22320;&#20174;&#24050;&#35757;&#32451;&#30340;GNN&#20013;&#21024;&#38500;&#29305;&#23450;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#36793;&#30340;&#35299;&#38500;&#23398;&#20064;&#65292;&#36825;&#19968;&#36807;&#31243;&#23545;&#29616;&#23454;&#24212;&#29992;&#29305;&#21035;&#30456;&#20851;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22914;GNNDelete&#21487;&#20197;&#28040;&#38500;&#29305;&#23450;&#36793;&#30340;&#24433;&#21709;&#65292;&#28982;&#32780;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#23616;&#38480;&#65292;&#31216;&#20026;&#36807;&#24230;&#36951;&#24536;&#12290;&#24403;&#35299;&#38500;&#23398;&#20064;&#36807;&#31243;&#26080;&#24847;&#20013;&#38500;&#21435;&#36229;&#20986;&#29305;&#23450;&#25968;&#25454;&#30340;&#36807;&#22810;&#20449;&#24687;&#26102;&#65292;&#20250;&#23548;&#33268;&#23545;&#21097;&#20313;&#36793;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;GNNDelete&#30340;&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;&#36807;&#24230;&#36951;&#24536;&#29616;&#35937;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10695v1 Announce Type: cross  Abstract: As concerns over data privacy intensify, unlearning in Graph Neural Networks (GNNs) has emerged as a prominent research frontier in academia. This concept is pivotal in enforcing the right to be forgotten, which entails the selective removal of specific data from trained GNNs upon user request. Our research focuses on edge unlearning, a process of particular relevance to real-world applications, owing to its widespread applicability. Current state-of-the-art approaches like GNNDelete can eliminate the influence of specific edges, yet our research has revealed a critical limitation in these approaches, termed over-forgetting. It occurs when the unlearning process inadvertently removes excessive information beyond specific data, leading to a significant decline in prediction accuracy for the remaining edges. To address this issue, we have identified the loss functions of GNNDelete as the primary source of the over-forgetting phenomenon. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;ST-MEM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09450</link><description>&lt;p&gt;
&#24341;&#23548;&#36974;&#34109;&#34920;&#31034;&#23398;&#20064;&#20197;&#25429;&#25417;&#24515;&#30005;&#22270;&#30340;&#26102;&#31354;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;ST-MEM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#24191;&#27867;&#29992;&#20316;&#30417;&#27979;&#24515;&#33039;&#36215;&#28304;&#30340;&#30005;&#20449;&#21495;&#30340;&#35786;&#26029;&#24037;&#20855;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#21162;&#21147;&#38598;&#20013;&#22312;&#20351;&#29992;ECG&#20449;&#21495;&#36827;&#34892;&#21508;&#31181;&#30142;&#30149;&#31579;&#26597;&#30340;&#24212;&#29992;&#19978;&#12290;&#28982;&#32780;&#65292;&#36866;&#24212;&#30142;&#30149;&#31579;&#26597;&#24212;&#29992;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#26631;&#35760;&#30340;ECG&#25968;&#25454;&#26377;&#38480;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#23454;&#29616;&#36890;&#29992;&#34920;&#31034;&#26159;&#20811;&#26381;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#24120;&#29992;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#22312;ECG&#25968;&#25454;&#19978;&#32431;&#31929;&#24212;&#29992;SSL&#65292;&#32780;&#19981;&#32771;&#34385;ECG&#20449;&#21495;&#22266;&#26377;&#30340;&#26102;&#31354;&#20851;&#31995;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#27425;&#20248;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ST-MEM&#65288;&#26102;&#31354;&#36974;&#34109;&#24515;&#30005;&#22270;&#24314;&#27169;&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;12&#23548;&#32852;ECG&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;ST-MEM&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;SSL&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09450v1 Announce Type: cross  Abstract: Electrocardiograms (ECG) are widely employed as a diagnostic tool for monitoring electrical signals originating from a heart. Recent machine learning research efforts have focused on the application of screening various diseases using ECG signals. However, adapting to the application of screening disease is challenging in that labeled ECG data are limited. Achieving general representation through self-supervised learning (SSL) is a well-known approach to overcome the scarcity of labeled data; however, a naive application of SSL to ECG data, without considering the spatial-temporal relationships inherent in ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM (Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM outperforms other SSL baseline methods in various experimental settings for arrhythmia classification tasks. Mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#35770;&#25991;&#31454;&#26631;&#20013;&#26816;&#27979;&#21246;&#32467;&#22242;&#20307;&#65292;&#20197;&#35299;&#20915;&#21516;&#34892;&#35780;&#23457;&#31995;&#32479;&#20013;&#30340;&#27450;&#35784;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07860</link><description>&lt;p&gt;
&#35770;&#25991;&#31454;&#26631;&#20013;&#26816;&#27979;&#20316;&#32773;&#19982;&#23457;&#31295;&#20154;&#21246;&#32467;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Detection of Reviewer-Author Collusion Rings From Paper Bidding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#35770;&#25991;&#31454;&#26631;&#20013;&#26816;&#27979;&#21246;&#32467;&#22242;&#20307;&#65292;&#20197;&#35299;&#20915;&#21516;&#34892;&#35780;&#23457;&#31995;&#32479;&#20013;&#30340;&#27450;&#35784;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#31185;&#23398;&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;&#31995;&#32479;&#38754;&#20020;&#30340;&#20027;&#35201;&#23041;&#32961;&#26159;&#23457;&#31295;&#20154;&#20043;&#38388;&#23384;&#22312;&#30340;"&#21246;&#32467;&#22242;&#20307;"&#12290;&#22312;&#36825;&#31181;&#21246;&#32467;&#22242;&#20307;&#20013;&#65292;&#25552;&#20132;&#20102;&#33258;&#24049;&#30340;&#35770;&#25991;&#30340;&#23457;&#31295;&#20154;&#21512;&#20316;&#65292;&#35797;&#22270;&#36890;&#36807;&#25805;&#32437;&#20250;&#35758;&#30340;&#35770;&#25991;&#20998;&#37197;&#65292;&#20197;&#20415;&#34987;&#25351;&#27966;&#20026;&#24444;&#27492;&#35770;&#25991;&#30340;&#23457;&#31295;&#20154;&#12290;&#32437;&#35266;&#29616;&#26377;&#30340;&#30740;&#31350;&#65292;&#34429;&#28982;&#24050;&#32463;&#21457;&#23637;&#20986;&#20102;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#26816;&#27979;&#20854;&#20182;&#31181;&#31867;&#30340;&#27450;&#35784;&#34892;&#20026;&#65292;&#20294;&#23578;&#26410;&#26377;&#30740;&#31350;&#35777;&#26126;&#26816;&#27979;&#21246;&#32467;&#22242;&#20307;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22914;&#20309;&#20174;&#35770;&#25991;&#31454;&#26631;&#20013;&#26816;&#27979;&#21246;&#32467;&#22242;&#20307;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major threat to the peer-review systems of computer science conferences is the existence of "collusion rings" between reviewers. In such collusion rings, reviewers who have also submitted their own papers to the conference work together to manipulate the conference's paper assignment, with the aim of being assigned to review each other's papers. The most straightforward way that colluding reviewers can manipulate the paper assignment is by indicating their interest in each other's papers through strategic paper bidding. One potential approach to solve this important problem would be to detect the colluding reviewers from their manipulated bids, after which the conference can take appropriate action. While prior work has has developed effective techniques to detect other kinds of fraud, no research has yet established that detecting collusion rings is even possible. In this work, we tackle the question of whether it is feasible to detect collusion rings from the paper bidding. To answ
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#38598;&#33976;&#39311;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#22914;&#20309;&#30830;&#20445;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04676</link><description>&lt;p&gt;
&#24102;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#20998;&#32452;&#20998;&#24067;&#40065;&#26834;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Group Distributionally Robust Dataset Distillation with Risk Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04676
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#38598;&#33976;&#39311;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#22914;&#20309;&#30830;&#20445;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;DD&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#25429;&#25417;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#20449;&#24687;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#20174;&#32780;&#26041;&#20415;&#20934;&#30830;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#12290;&#20854;&#24212;&#29992;&#28085;&#30422;&#20102;&#36716;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#26159;&#23558;&#35757;&#32451;&#25968;&#25454;&#38598;&#35270;&#20026;&#36741;&#21161;&#65292;&#23601;&#20687;&#35757;&#32451;&#38598;&#26159;&#20154;&#21475;&#20998;&#24067;&#30340;&#36817;&#20284;&#26367;&#20195;&#21697;&#19968;&#26679;&#65292;&#32780;&#21518;&#32773;&#25165;&#26159;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#20854;&#21463;&#27426;&#36814;&#31243;&#24230;&#24456;&#39640;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;DD&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#36328;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24403;&#38754;&#23545;&#26469;&#33258;&#32597;&#35265;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#30830;&#20445;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models. Its applications span various domains, including transfer learning, federated learning, and neural architecture search. The most popular methods for constructing the synthetic data rely on matching the convergence properties of training the model with the synthetic dataset and the training dataset. However, targeting the training dataset must be thought of as auxiliary in the same sense that the training set is an approximate substitute for the population distribution, and the latter is the data of interest. Yet despite its popularity, an aspect that remains unexplored is the relationship of DD to its generalization, particularly across uncommon subgroups. That is, how can we ensure that a model trained on the synthetic dataset performs well when faced with samples from re
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#34920;&#29616;&#20986;&#22312;&#20449;&#20219;&#28216;&#25103;&#20013;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#20559;&#35265;&#21644;&#23545;&#20195;&#29702;&#19982;&#20154;&#31867;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.04559</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Model Agents Simulate Human Trust Behaviors?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04559
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#34920;&#29616;&#20986;&#22312;&#20449;&#20219;&#28216;&#25103;&#20013;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#20559;&#35265;&#21644;&#23545;&#20195;&#29702;&#19982;&#20154;&#31867;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#32463;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#29992;&#20316;&#20026;&#27169;&#25311;&#24037;&#20855;&#65292;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#22312;&#31038;&#20250;&#31185;&#23398;&#31561;&#39046;&#22495;&#20013;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;LLM&#20195;&#29702;&#26159;&#21542;&#30495;&#30340;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20154;&#31867;&#20114;&#21160;&#20013;&#26368;&#20851;&#38190;&#30340;&#34892;&#20026;&#20043;&#19968;&#65292;&#20449;&#20219;&#65292;&#26088;&#22312;&#35843;&#26597;LLM&#20195;&#29702;&#26159;&#21542;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#65292;&#22312;&#34987;&#34892;&#20026;&#32463;&#27982;&#23398;&#24191;&#27867;&#25509;&#21463;&#30340;&#20449;&#20219;&#28216;&#25103;&#26694;&#26550;&#19979;&#65292;LLM&#20195;&#29702;&#36890;&#24120;&#34920;&#29616;&#20986;&#20449;&#20219;&#34892;&#20026;&#65292;&#31216;&#20026;&#20195;&#29702;&#20449;&#20219;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#20195;&#29702;&#22312;&#20449;&#20219;&#34892;&#20026;&#26041;&#38754;&#19982;&#20154;&#31867;&#20855;&#26377;&#36739;&#39640;&#30340;&#34892;&#20026;&#19968;&#33268;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;LLM&#20195;&#29702;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20195;&#29702;&#20449;&#20219;&#20013;&#30340;&#20559;&#35265;&#20197;&#21450;&#20195;&#29702;&#20449;&#20219;&#22312;&#23545;&#20195;&#29702;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#24046;&#24322;&#26041;&#38754;&#30340;&#20869;&#22312;&#29305;&#24615;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#21253;&#25324;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#22312;&#20869;&#30340;&#26465;&#20214;&#19979;&#20195;&#29702;&#20449;&#20219;&#30340;&#20869;&#22312;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: can LLM agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, indicating the feasibility to simulate human trust behaviors with LLM agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;Swin Transformer&#25552;&#20986;&#20102;"SWTformer"&#65292;&#36890;&#36807;&#20174;&#23616;&#37096;&#21040;&#20840;&#23616;&#30340;&#35270;&#35282;&#26469;&#22686;&#24378;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17828</link><description>&lt;p&gt;
&#21033;&#29992;Swin Transformer&#36827;&#34892;&#20174;&#23616;&#37096;&#21040;&#20840;&#23616;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Leveraging Swin Transformer for Local-to-Global Weakly Supervised Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;Swin Transformer&#25552;&#20986;&#20102;"SWTformer"&#65292;&#36890;&#36807;&#20174;&#23616;&#37096;&#21040;&#20840;&#23616;&#30340;&#35270;&#35282;&#26469;&#22686;&#24378;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#21033;&#29992;&#22270;&#20687;&#32423;&#26631;&#31614;&#20316;&#20026;&#30417;&#30563;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20174;&#31867;&#28608;&#27963;&#22270;(CAM)&#20013;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35299;&#20915;&#36825;&#20123;&#26631;&#31614;&#20013;&#32570;&#20047;&#31354;&#38388;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#30340;&#23616;&#37096;&#27169;&#24335;&#26816;&#27979;&#65292;CAMs&#36890;&#24120;&#21482;&#24378;&#35843;&#23545;&#35937;&#30340;&#26368;&#20855;&#21306;&#20998;&#24230;&#30340;&#37096;&#20998;&#65292;&#20351;&#24471;&#20934;&#30830;&#21306;&#20998;&#21069;&#26223;&#23545;&#35937;&#19982;&#32972;&#26223;&#20197;&#21450;&#24444;&#27492;&#20043;&#38388;&#21464;&#24471;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#20854;&#20840;&#23616;&#35270;&#35282;&#65292;Vision Transformer (ViT)&#29305;&#24449;&#22312;&#25429;&#25417;&#22330;&#26223;&#24067;&#23616;&#26041;&#38754;&#27604;CNNs&#26356;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23618;&#27425;&#21270;&#30340;ViTs&#22312;&#35813;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#8220;SWTformer&#8221;&#26469;&#25506;&#32034;&#20351;&#29992;Swin Transformer&#26469;&#25552;&#39640;&#21021;&#22987;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, weakly supervised semantic segmentation using image-level labels as supervision has received significant attention in the field of computer vision. Most existing methods have addressed the challenges arising from the lack of spatial information in these labels by focusing on facilitating supervised learning through the generation of pseudo-labels from class activation maps (CAMs). Due to the localized pattern detection of Convolutional Neural Networks (CNNs), CAMs often emphasize only the most discriminative parts of an object, making it challenging to accurately distinguish foreground objects from each other and the background. Recent studies have shown that Vision Transformer (ViT) features, due to their global view, are more effective in capturing the scene layout than CNNs. However, the use of hierarchical ViTs has not been extensively explored in this field. This work explores the use of Swin Transformer by proposing "SWTformer" to enhance the accuracy of the init
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#30001;&#20110;&#35270;&#35282;&#21644;&#20809;&#29031;&#21464;&#21270;&#31561;&#22240;&#32032;&#65292;&#21305;&#37197;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24341;&#20837;&#20351;&#24471;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#25216;&#26415;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#23545;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#23545;&#20043;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2401.17592</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Local Feature Matching Using Deep Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#30001;&#20110;&#35270;&#35282;&#21644;&#20809;&#29031;&#21464;&#21270;&#31561;&#22240;&#32032;&#65292;&#21305;&#37197;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24341;&#20837;&#20351;&#24471;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#25216;&#26415;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#23545;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#23545;&#20043;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22270;&#20687;&#26816;&#32034;&#12289;&#19977;&#32500;&#37325;&#24314;&#21644;&#29289;&#20307;&#35782;&#21035;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35270;&#35282;&#21644;&#20809;&#29031;&#21464;&#21270;&#31561;&#22240;&#32032;&#65292;&#25552;&#39640;&#21305;&#37197;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24341;&#20837;&#24341;&#21457;&#20102;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#25216;&#26415;&#30340;&#24191;&#27867;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#26041;&#27861;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#26159;&#21542;&#23384;&#22312;&#26816;&#27979;&#22120;&#34987;&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#22522;&#20110;&#26816;&#27979;&#22120;&#30340;&#31867;&#21035;&#21253;&#25324;&#26816;&#27979;&#28982;&#21518;&#25551;&#36848;&#12289;&#32852;&#21512;&#26816;&#27979;&#21644;&#25551;&#36848;&#12289;&#25551;&#36848;&#28982;&#21518;&#26816;&#27979;&#20197;&#21450;&#22522;&#20110;&#22270;&#30340;&#25216;&#26415;&#12290;&#30456;&#21453;&#65292;&#19981;&#38656;&#35201;&#26816;&#27979;&#22120;&#30340;&#31867;&#21035;&#21253;&#25324;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#12289;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#22359;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36229;&#36234;&#20102;&#26041;&#27861;&#35770;&#20998;&#26512;&#65292;&#36824;&#21253;&#25324;&#23545;&#20808;&#21069;&#24037;&#20316;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local feature matching enjoys wide-ranging applications in the realm of computer vision, encompassing domains such as image retrieval, 3D reconstruction, and object recognition. However, challenges persist in improving the accuracy and robustness of matching due to factors like viewpoint and lighting variations. In recent years, the introduction of deep learning models has sparked widespread exploration into local feature matching techniques. The objective of this endeavor is to furnish a comprehensive overview of local feature matching methods. These methods are categorized into two key segments based on the presence of detectors. The Detector-based category encompasses models inclusive of Detect-then-Describe, Joint Detection and Description, Describe-then-Detect, as well as Graph Based techniques. In contrast, the Detector-free category comprises CNN Based, Transformer Based, and Patch Based methods. Our study extends beyond methodological analysis, incorporating evaluations of prev
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2401.17505</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#31661;&#22836;
&lt;/p&gt;
&lt;p&gt;
Arrows of Time for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#26041;&#21521;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#36825;&#31867;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#39044;&#27979;&#19979;&#19968;&#20010;&#35760;&#21495;&#21644;&#39044;&#27979;&#21069;&#19968;&#20010;&#35760;&#21495;&#26102;&#30340;&#24179;&#22343;&#23545;&#25968;&#22256;&#24785;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#26082;&#24494;&#22937;&#21448;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#35821;&#35328;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#26102;&#38388;&#31561;&#65289;&#19979;&#38750;&#24120;&#19968;&#33268;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#22312;&#29702;&#35770;&#19978;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#65292;&#19981;&#24212;&#35813;&#23384;&#22312;&#36825;&#26679;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#22914;&#20309;&#20986;&#29616;&#22312;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#32771;&#34385;&#20013;&#65292;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#24102;&#26469;&#30340;&#19968;&#20123;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#20928;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AToP&#65289;&#27969;&#31243;&#65292;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#25200;&#21160;&#30772;&#22351;&#21644;&#36890;&#36807;&#23545;&#25239;&#25439;&#22833;&#24494;&#35843;&#20928;&#21270;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.16352</link><description>&lt;p&gt;
&#23545;&#20928;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AToP&#65289;&#65306;&#25552;&#21319;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16352
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#20928;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AToP&#65289;&#27969;&#31243;&#65292;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#25200;&#21160;&#30772;&#22351;&#21644;&#36890;&#36807;&#23545;&#25239;&#25439;&#22833;&#24494;&#35843;&#20928;&#21270;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#35748;&#20026;&#26131;&#21463;&#35774;&#35745;&#31934;&#33391;&#30340;&#23545;&#25239;&#25915;&#20987;&#24433;&#21709;&#12290;&#22522;&#20110;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#30340;&#26368;&#25104;&#21151;&#38450;&#24481;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#29305;&#23450;&#25915;&#20987;&#19979;&#30340;&#26368;&#20339;&#40065;&#26834;&#24615;&#65292;&#20294;&#26080;&#27861;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#30693;&#25915;&#20987;&#12290;&#22522;&#20110;&#23545;&#25239;&#20928;&#21270;&#65288;AP&#65289;&#30340;&#21478;&#19968;&#26377;&#25928;&#38450;&#24481;&#25216;&#26415;&#21487;&#20197;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#26080;&#27861;&#23454;&#29616;&#26368;&#20339;&#40065;&#26834;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23384;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#23616;&#38480;&#24615;&#65292;&#21363;&#26631;&#20934;&#20934;&#30830;&#24615;&#38477;&#32423;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27969;&#31243;&#65292;&#31216;&#20026;&#23545;&#20928;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AToP&#65289;&#65292;&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65306;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#65288;RT&#65289;&#30772;&#22351;&#25200;&#21160;&#65292;&#20197;&#36991;&#20813;&#23545;&#24050;&#30693;&#25915;&#20987;&#30340;&#36807;&#24230;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26410;&#30693;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#27867;&#21270;&#65307;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#25439;&#22833;&#23545;&#20928;&#21270;&#22120;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65288;FT&#65289;&#65292;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16352v2 Announce Type: replace-cross  Abstract: The deep neural networks are known to be vulnerable to well-designed adversarial attacks. The most successful defense technique based on adversarial training (AT) can achieve optimal robustness against particular attacks but cannot generalize well to unseen attacks. Another effective defense technique based on adversarial purification (AP) can enhance generalization but cannot achieve optimal robustness. Meanwhile, both methods share one common limitation on the degraded standard accuracy. To mitigate these issues, we propose a novel pipeline called Adversarial Training on Purification (AToP), which comprises two components: perturbation destruction by random transforms (RT) and purifier model fine-tuned (FT) by adversarial loss. RT is essential to avoid overlearning to known attacks resulting in the robustness generalization to unseen attacks and FT is essential for the improvement of robustness. To evaluate our method in an e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21160;&#24577;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65288;DFFM&#65289;&#26469;&#25193;&#23637;3D&#21367;&#31215;&#26680;&#30340;&#24863;&#30693;&#22495;&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#27169;&#22359;&#65288;FSM&#65289;&#23450;&#37327;&#35780;&#20272;&#21644;&#28040;&#38500;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2401.11913</link><description>&lt;p&gt;
3D&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22823;&#24863;&#30693;&#22495;&#31574;&#30053;&#21644;&#37325;&#35201;&#29305;&#24449;&#25552;&#21462;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Large receptive field strategy and important feature extraction strategy in 3D object detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11913
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21160;&#24577;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65288;DFFM&#65289;&#26469;&#25193;&#23637;3D&#21367;&#31215;&#26680;&#30340;&#24863;&#30693;&#22495;&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#27169;&#22359;&#65288;FSM&#65289;&#23450;&#37327;&#35780;&#20272;&#21644;&#28040;&#38500;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#30446;&#26631;&#26816;&#27979;&#30340;&#22686;&#24378;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#31934;&#30830;&#29615;&#22659;&#24863;&#30693;&#21644;&#25913;&#36827;&#20219;&#21153;&#25191;&#34892;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#25552;&#20379;&#20934;&#30830;&#30340;&#28145;&#24230;&#20449;&#24687;&#65292;&#34987;&#29992;&#20316;&#27492;&#30446;&#30340;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;3D&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#25193;&#23637;3D&#21367;&#31215;&#26680;&#24863;&#30693;&#22495;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65288;DFFM&#65289;&#12290;&#35813;&#27169;&#22359;&#23454;&#29616;&#20102;3D&#21367;&#31215;&#26680;&#24863;&#30693;&#22495;&#30340;&#33258;&#36866;&#24212;&#25193;&#23637;&#65292;&#24179;&#34913;&#20102;&#25193;&#23637;&#19982;&#21487;&#25509;&#21463;&#30340;&#35745;&#31639;&#36127;&#36733;&#12290;&#36825;&#19968;&#21019;&#26032;&#20943;&#23569;&#20102;&#25805;&#20316;&#65292;&#25193;&#23637;&#20102;&#24863;&#30693;&#22495;&#65292;&#24182;&#20801;&#35768;&#27169;&#22411;&#21160;&#24577;&#35843;&#25972;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#23545;&#35937;&#38656;&#27714;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;3D&#29305;&#24449;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#27169;&#22359;&#65288;FSM&#65289;&#23450;&#37327;&#35780;&#20272;&#24182;&#28040;&#38500;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11913v2 Announce Type: replace-cross  Abstract: The enhancement of 3D object detection is pivotal for precise environmental perception and improved task execution capabilities in autonomous driving. LiDAR point clouds, offering accurate depth information, serve as a crucial information for this purpose. Our study focuses on key challenges in 3D target detection. To tackle the challenge of expanding the receptive field of a 3D convolutional kernel, we introduce the Dynamic Feature Fusion Module (DFFM). This module achieves adaptive expansion of the 3D convolutional kernel's receptive field, balancing the expansion with acceptable computational loads. This innovation reduces operations, expands the receptive field, and allows the model to dynamically adjust to different object requirements. Simultaneously, we identify redundant information in 3D features. Employing the Feature Selection Module (FSM) quantitatively evaluates and eliminates non-important features, achieving the 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27431;&#30431;&#27861;&#24459;&#27861;&#35268;&#20013;&#30340;&#24212;&#29992;&#24341;&#21457;&#20102;&#36131;&#20219;&#12289;&#38544;&#31169;&#12289;&#30693;&#35782;&#20135;&#26435;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#23545;&#29616;&#26377;&#21644;&#25311;&#35758;&#30340;&#27861;&#24459;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2401.07348</link><description>&lt;p&gt;
&#27431;&#30431;&#27861;&#24459;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#36131;&#20219;&#12289;&#38544;&#31169;&#12289;&#30693;&#35782;&#20135;&#26435;&#21644;&#32593;&#32476;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07348
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27431;&#30431;&#27861;&#24459;&#27861;&#35268;&#20013;&#30340;&#24212;&#29992;&#24341;&#21457;&#20102;&#36131;&#20219;&#12289;&#38544;&#31169;&#12289;&#30693;&#35782;&#20135;&#26435;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#23545;&#29616;&#26377;&#21644;&#25311;&#35758;&#30340;&#27861;&#24459;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#21450;&#20854;&#21518;&#32487;&#27169;&#22411;&#65292;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#27425;&#33539;&#24335;&#36716;&#21464;&#12290;&#20808;&#36827;&#30340;LLMs&#34920;&#29616;&#20986;&#22810;&#27169;&#24577;&#24615;&#65292;&#33021;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#26684;&#24335;&#65292;&#20174;&#32780;&#25299;&#23485;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#26032;&#20852;&#33258;&#27835;&#24615;&#24341;&#20837;&#20102;&#39044;&#27979;&#24615;&#21644;&#27861;&#24459;&#36981;&#20174;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;LLMs&#22312;&#27431;&#30431;&#32972;&#26223;&#19979;&#30340;&#27861;&#24459;&#21644;&#30417;&#31649;&#24433;&#21709;&#65292;&#20998;&#26512;&#20102;&#36131;&#20219;&#12289;&#38544;&#31169;&#12289;&#30693;&#35782;&#20135;&#26435;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#26041;&#38754;&#12290;&#23427;&#25209;&#21028;&#24615;&#22320;&#23457;&#35270;&#20102;&#29616;&#26377;&#21644;&#25311;&#35758;&#30340;&#27431;&#30431;&#31435;&#27861;&#65288;&#21253;&#25324;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#12299;&#33609;&#26696;&#65289;&#22312;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26222;&#36941;&#21644;LLMs&#29305;&#21035;&#25361;&#25112;&#26041;&#38754;&#30340;&#20805;&#20998;&#24615;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#31435;&#27861;&#26694;&#26550;&#20013;&#30340;&#28508;&#22312;&#24046;&#36317;&#21644;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#24314;&#35758;&#20197;&#30830;&#20445;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07348v2 Announce Type: replace-cross  Abstract: The advent of Generative AI, particularly through Large Language Models (LLMs) like ChatGPT and its successors, marks a paradigm shift in the AI landscape. Advanced LLMs exhibit multimodality, handling diverse data formats, thereby broadening their application scope. However, the complexity and emergent autonomy of these models introduce challenges in predictability and legal compliance. This paper delves into the legal and regulatory implications of Generative AI and LLMs in the European Union context, analyzing aspects of liability, privacy, intellectual property, and cybersecurity. It critically examines the adequacy of the existing and proposed EU legislation, including the Artificial Intelligence Act (AIA) draft, in addressing the unique challenges posed by Generative AI in general and LLMs in particular. The paper identifies potential gaps and shortcomings in the legislative framework and proposes recommendations to ensur
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#25512;&#29702;&#36712;&#36857;&#30340;&#32463;&#39564;&#65292;&#25552;&#20986;&#30340;&#24605;&#32500;&#29366;&#24577;&#26426;&#65288;SMoT&#65289;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#36873;&#25321;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#24182;&#36991;&#20813;&#38169;&#35823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.17445</link><description>&lt;p&gt;
&#24605;&#32500;&#29366;&#24577;&#26426;&#65306;&#21033;&#29992;&#36807;&#21435;&#25512;&#29702;&#36712;&#36857;&#22686;&#24378;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17445
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#25512;&#29702;&#36712;&#36857;&#30340;&#32463;&#39564;&#65292;&#25552;&#20986;&#30340;&#24605;&#32500;&#29366;&#24577;&#26426;&#65288;SMoT&#65289;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#36873;&#25321;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#24182;&#36991;&#20813;&#38169;&#35823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#31243;&#24207;&#24120;&#24120;&#20197;&#19968;&#31181;&#31867;&#20284;&#26641;&#29366;&#30340;&#26041;&#24335;&#22312;&#25506;&#32034;-&#35780;&#20272;&#26694;&#26550;&#20869;&#25512;&#29702;&#65292;&#23548;&#33322;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#24573;&#30053;&#19968;&#26086;&#38382;&#39064;&#35299;&#20915;&#23601;&#21487;&#20197;&#33293;&#24323;&#30340;&#25104;&#21151;&#25512;&#29702;&#36712;&#36857;&#65292;&#20174;&#32780;&#23548;&#33268;&#36825;&#20123;&#36712;&#36857;&#22312;&#26410;&#26469;&#31867;&#20284;&#38382;&#39064;&#20013;&#34987;&#20302;&#25928;&#21033;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#20302;&#25928;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#29366;&#24577;&#26426;&#26469;&#35760;&#24405;&#26469;&#33258;&#20808;&#21069;&#25512;&#29702;&#36712;&#36857;&#30340;&#32463;&#39564;&#12290;&#22312;&#29366;&#24577;&#26426;&#20869;&#65292;&#29366;&#24577;&#20195;&#34920;&#20102;&#32454;&#20998;&#30340;&#23376;&#38382;&#39064;&#65292;&#32780;&#29366;&#24577;&#36716;&#25442;&#21453;&#26144;&#20102;&#23376;&#38382;&#39064;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#29366;&#24577;&#26426;&#35760;&#24405;&#20102;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#36712;&#36857;&#12290;&#21033;&#29992;&#29366;&#24577;&#26426;&#30340;&#32463;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#24605;&#32500;&#29366;&#24577;&#26426;&#65288;SMoT&#65289;&#36873;&#25321;&#26368;&#20248;&#30340;&#23376;&#35299;&#20915;&#26041;&#26696;&#24182;&#36991;&#20813;&#38169;&#35823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SMoT&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22312;&#20004;&#31181;&#25506;&#32034;&#23494;&#38598;&#22411;&#38382;&#39064;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65306;24
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17445v2 Announce Type: replace  Abstract: Current Large Language Model-based agents reason within an exploration-evaluation framework, navigating problem-solving processes in a tree-like manner. However, these methods often neglect successful reasoning trajectories once a problem is resolved, leading to inefficient use of these trajectories for future analogous problems. To address this inefficiency, we adopt a state machine to record experience derived from previous reasoning trajectories. Within the state machine, states represent decomposed sub-problems, while state transitions reflect the dependencies among sub-problems. The state machine records both successful and failed trajectories. Utilizing the experience from the state machine, our proposed State Machine of Thoughts (SMoT) selects the most optimal sub-solutions and avoids incorrect ones. Our experiments show that SMoT can significantly improve problem-solving abilities in two exploration-intensive problems: the 24
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35825;&#23548;&#34394;&#20551;&#20449;&#24687;&#26469;&#26500;&#24314;&#19968;&#20010;&#20107;&#23454;&#34180;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36890;&#36807;&#23545;&#27604;&#35299;&#30721;&#26469;&#24809;&#32602;&#36825;&#20123;&#35825;&#23548;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#21319;&#29983;&#25104;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.15710</link><description>&lt;p&gt;
&#36890;&#36807;&#35825;&#23548;&#24615;&#24187;&#35273;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Alleviating Hallucinations of Large Language Models through Induced Hallucinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15710
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35825;&#23548;&#34394;&#20551;&#20449;&#24687;&#26469;&#26500;&#24314;&#19968;&#20010;&#20107;&#23454;&#34180;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36890;&#36807;&#23545;&#27604;&#35299;&#30721;&#26469;&#24809;&#32602;&#36825;&#20123;&#35825;&#23548;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#21319;&#29983;&#25104;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#24050;&#35266;&#23519;&#21040;&#23427;&#20204;&#29983;&#25104;&#30340;&#21709;&#24212;&#20013;&#21253;&#21547;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#20449;&#24687;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#8220;&#35825;&#23548;-&#23545;&#27604;&#35299;&#30721;&#8221;(ICD)&#31574;&#30053;&#26469;&#20943;&#36731;&#24187;&#35273;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20174;&#21407;&#22987;LLMs&#20013;&#35825;&#23548;&#24187;&#35273;&#26469;&#26500;&#24314;&#19968;&#20010;&#20107;&#23454;&#19978;&#34180;&#24369;&#30340;LLM&#12290;&#28982;&#21518;&#65292;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#24809;&#32602;&#36825;&#20123;&#35825;&#23548;&#30340;&#24187;&#35273;&#20197;&#22686;&#24378;&#29983;&#25104;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#35299;&#30721;&#26469;&#25918;&#22823;&#21407;&#27169;&#22411;&#30340;&#39044;&#27979;&#24182;&#36140;&#20302;&#35825;&#23548;&#30340;&#19981;&#30495;&#23454;&#39044;&#27979;&#26469;&#30830;&#23450;&#26368;&#32456;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#12290;&#23545;&#22522;&#20110;&#27495;&#35270;&#21644;&#22522;&#20110;&#29983;&#25104;&#30340;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65292;&#22914;TruthfulQA&#21644;FActScore&#31561;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;ICD&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15710v2 Announce Type: replace-cross  Abstract: Despite their impressive capabilities, large language models (LLMs) have been observed to generate responses that include inaccurate or fabricated information, a phenomenon commonly known as ``hallucination''. In this work, we propose a simple \textit{Induce-then-Contrast} Decoding (ICD) strategy to alleviate hallucinations. We first construct a factually weak LLM by inducing hallucinations from the original LLMs. Then, we penalize these induced hallucinations during decoding to enhance the factuality of the generated content. Concretely, we determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding. Experimental results on both discrimination-based and generation-based hallucination evaluation benchmarks, such as TruthfulQA and \textsc{FActScore}, demonstrate that our proposed ICD methods can effectively enhance th
&lt;/p&gt;</description></item><item><title>&#26032;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#37327;&#30340;&#23454;&#20363;-&#26631;&#31614;&#23545;&#26469;&#20016;&#23500;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;</title><link>https://arxiv.org/abs/2312.12021</link><description>&lt;p&gt;
Synergistic Anchored Contrastive Pre-training for Few-Shot Relation Extraction
&lt;/p&gt;
&lt;p&gt;
Synergistic Anchored Contrastive Pre-training for Few-Shot Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12021
&lt;/p&gt;
&lt;p&gt;
&#26032;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#37327;&#30340;&#23454;&#20363;-&#26631;&#31614;&#23545;&#26469;&#20016;&#23500;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#65288;FSRE&#65289;&#26088;&#22312;&#20174;&#31232;&#30095;&#26631;&#35760;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#20851;&#31995;&#20107;&#23454;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;FSRE&#32467;&#26524;&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#23454;&#20363;&#21644;&#26631;&#31614;&#20107;&#23454;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#23398;&#20064;&#33539;&#24335;&#20013;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#22823;&#37327;&#30340;&#23454;&#20363;-&#26631;&#31614;&#23545;&#26469;&#20351;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#35821;&#20041;&#20016;&#23500;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#21516;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#30340;&#21160;&#26426;&#26159;&#65292;&#36890;&#36807;&#23454;&#20363;-&#26631;&#31614;&#23545;&#20256;&#36798;&#30340;&#22810;&#26679;&#35266;&#28857;&#25429;&#25417;&#21040;&#20102;&#19981;&#23436;&#25972;&#20294;&#20114;&#34917;&#30340;&#25991;&#26412;&#35821;&#20041;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#28041;&#21450;&#19968;&#31181;&#23545;&#31216;&#23545;&#27604;&#30446;&#26631;&#65292;&#21253;&#21547;&#20102;&#21477;&#23376;&#38170;&#23450;&#21644;&#26631;&#31614;&#38170;&#23450;&#30340;&#23545;&#27604;&#25439;&#22833;&#12290;&#36890;&#36807;&#32452;&#21512;&#36825;&#20004;&#31181;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12021v3 Announce Type: replace-cross  Abstract: Few-shot Relation Extraction (FSRE) aims to extract relational facts from a sparse set of labeled corpora. Recent studies have shown promising results in FSRE by employing Pre-trained Language Models (PLMs) within the framework of supervised contrastive learning, which considers both instances and label facts. However, how to effectively harness massive instance-label pairs to encompass the learned representation with semantic richness in this learning paradigm is not fully explored. To address this gap, we introduce a novel synergistic anchored contrastive pre-training framework. This framework is motivated by the insight that the diverse viewpoints conveyed through instance-label pairs capture incomplete yet complementary intrinsic textual semantics. Specifically, our framework involves a symmetrical contrastive objective that encompasses both sentence-anchored and label-anchored contrastive losses. By combining these two los
&lt;/p&gt;</description></item><item><title>&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#27861;&#23398;&#20064;&#26410;&#30693;&#24178;&#39044;&#30446;&#26631;&#30340;&#22806;&#29983;&#22122;&#22768;&#65292;&#24182;&#23558;&#20854;&#19982;&#30456;&#24212;&#30340;&#20869;&#29983;&#21464;&#37327;&#21305;&#37197;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#24178;&#39044;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2312.06091</link><description>&lt;p&gt;
&#20174;&#24322;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#26410;&#30693;&#24178;&#39044;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Learning Unknown Intervention Targets in Structural Causal Models from Heterogeneous Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06091
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#27861;&#23398;&#20064;&#26410;&#30693;&#24178;&#39044;&#30446;&#26631;&#30340;&#22806;&#29983;&#22122;&#22768;&#65292;&#24182;&#23558;&#20854;&#19982;&#30456;&#24212;&#30340;&#20869;&#29983;&#21464;&#37327;&#21305;&#37197;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#24178;&#39044;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35782;&#21035;&#26410;&#30693;&#24178;&#39044;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#20174;&#22810;&#20010;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#24322;&#26500;&#25968;&#25454;&#12290;&#26410;&#30693;&#24178;&#39044;&#30446;&#26631;&#26159;&#19968;&#32452;&#20869;&#29983;&#21464;&#37327;&#65292;&#20854;&#30456;&#24212;&#30340;&#22806;&#29983;&#22122;&#22768;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#31532;&#19968;&#38454;&#27573;&#20013;&#24674;&#22797;&#20102;&#36328;&#19981;&#21516;&#29615;&#22659;&#21457;&#29983;&#21464;&#21270;&#30340;&#26410;&#30693;&#24178;&#39044;&#30446;&#26631;&#23545;&#24212;&#30340;&#22806;&#29983;&#22122;&#22768;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#24674;&#22797;&#30340;&#22122;&#22768;&#19982;&#30456;&#24212;&#30340;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#21305;&#37197;&#12290;&#23545;&#20110;&#24674;&#22797;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23398;&#20064;&#36825;&#20123;&#22806;&#29983;&#22122;&#22768;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#21487;&#36798;&#21040;&#26576;&#31181;&#20998;&#37327;&#26041;&#21521;&#21487;&#36870;&#36716;&#25442;&#12290;&#23545;&#20110;&#21305;&#37197;&#38454;&#27573;&#65292;&#22312;&#22240;&#26524;&#20805;&#20998;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;&#21807;&#19968;&#22320;&#35782;&#21035;&#24178;&#39044;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06091v2 Announce Type: replace-cross  Abstract: We study the problem of identifying the unknown intervention targets in structural causal models where we have access to heterogeneous data collected from multiple environments. The unknown intervention targets are the set of endogenous variables whose corresponding exogenous noises change across the environments. We propose a two-phase approach which in the first phase recovers the exogenous noises corresponding to unknown intervention targets whose distributions have changed across environments. In the second phase, the recovered noises are matched with the corresponding endogenous variables. For the recovery phase, we provide sufficient conditions for learning these exogenous noises up to some component-wise invertible transformation. For the matching phase, under the causal sufficiency assumption, we show that the proposed method uniquely identifies the intervention targets. In the presence of latent confounders, the interv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#24320;&#21457;&#30340;DevAssistLlama&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#36719;&#20214;&#30456;&#20851;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#26102;&#34920;&#29616;&#20986;&#20248;&#24322;&#33021;&#21147;&#65292;&#31361;&#20986;&#20102;&#19987;&#38376;LLM&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.05626</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#23450;&#20041;&#24320;&#21457;&#32773;&#25588;&#21161;&#65306;&#22312;&#36719;&#20214;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Redefining Developer Assistance: Through Large Language Models in Software Ecosystem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#24320;&#21457;&#30340;DevAssistLlama&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#36719;&#20214;&#30456;&#20851;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#26102;&#34920;&#29616;&#20986;&#20248;&#24322;&#33021;&#21147;&#65292;&#31361;&#20986;&#20102;&#19987;&#38376;LLM&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;DevAssistLlama&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#24320;&#21457;&#30340;&#27169;&#22411;&#65292;&#21487;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#22788;&#29702;&#19982;&#36719;&#20214;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#12290;&#36825;&#20010;&#27169;&#22411;&#65292;&#20316;&#20026;&#25351;&#23548;&#35843;&#25972;&#30340;LLM&#21464;&#20307;&#65292;&#29305;&#21035;&#25797;&#38271;&#22788;&#29702;&#22797;&#26434;&#30340;&#25216;&#26415;&#25991;&#26723;&#65292;&#22686;&#24378;&#20102;&#24320;&#21457;&#20154;&#21592;&#22312;&#36719;&#20214;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;DevAssistLlama&#30340;&#21019;&#24314;&#28041;&#21450;&#20174;&#21508;&#31181;&#36719;&#20214;&#31995;&#32479;&#26500;&#24314;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#26377;&#25928;&#22788;&#29702;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#21644;&#38142;&#25509;&#39044;&#27979;&#65288;LP&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;DevAssistLlama&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#65288;&#21253;&#25324;ChatGPT&#65289;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#33021;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#31361;&#20986;&#20102;&#19987;&#38376;LLM&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05626v2 Announce Type: replace-cross  Abstract: In this paper, we delve into the advancement of domain-specific Large Language Models (LLMs) with a focus on their application in software development. We introduce DevAssistLlama, a model developed through instruction tuning, to assist developers in processing software-related natural language queries. This model, a variant of instruction tuned LLM, is particularly adept at handling intricate technical documentation, enhancing developer capability in software specific tasks. The creation of DevAssistLlama involved constructing an extensive instruction dataset from various software systems, enabling effective handling of Named Entity Recognition (NER), Relation Extraction (RE), and Link Prediction (LP). Our results demonstrate DevAssistLlama's superior capabilities in these tasks, in comparison with other models including ChatGPT. This research not only highlights the potential of specialized LLMs in software development also t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;90&#20010;&#36523;&#20307;&#27573;&#12289;206&#20010;&#20851;&#33410;&#21644;700&#20010;&#32908;&#33137;&#21333;&#20301;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#20302;&#32500;&#34920;&#31034;&#21644;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20840;&#36523;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2312.05473</link><description>&lt;p&gt;
&#33258;&#25105;&#27169;&#22411;&#29992;&#20110;&#20855;&#36523;&#26234;&#33021;&#65306;&#29992;&#20998;&#23618;&#20302;&#32500;&#34920;&#31034;&#24314;&#27169;&#20840;&#36523;&#20154;&#20307;&#39592;&#39612;&#32908;&#32905;&#31995;&#32479;&#21644;&#36816;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Self Model for Embodied Intelligence: Modeling Full-Body Human Musculoskeletal System and Locomotion Control with Hierarchical Low-Dimensional Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;90&#20010;&#36523;&#20307;&#27573;&#12289;206&#20010;&#20851;&#33410;&#21644;700&#20010;&#32908;&#33137;&#21333;&#20301;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#20302;&#32500;&#34920;&#31034;&#21644;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20840;&#36523;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#32908;&#32905;&#39592;&#39612;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#25511;&#21046;&#23545;&#20110;&#29702;&#35299;&#20154;&#31867;&#36816;&#21160;&#21151;&#33021;&#12289;&#24320;&#21457;&#20855;&#36523;&#26234;&#33021;&#20197;&#21450;&#20248;&#21270;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#38024;&#23545;&#30446;&#21069;&#24320;&#28304;&#27169;&#22411;&#20165;&#38480;&#20110;&#23569;&#25968;&#36523;&#20307;&#37096;&#20301;&#19988;&#36890;&#24120;&#32908;&#32905;&#25968;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;90&#20010;&#36523;&#20307;&#27573;&#12289;206&#20010;&#20851;&#33410;&#21644;700&#20010;&#32908;&#33137;&#21333;&#20301;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#65292;&#33021;&#22815;&#27169;&#25311;&#20840;&#36523;&#21160;&#24577;&#24182;&#19982;&#21508;&#31181;&#35774;&#22791;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#32500;&#34920;&#31034;&#21644;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20840;&#36523;&#25511;&#21046;&#12290;&#36890;&#36807;&#27169;&#25311;&#30495;&#23454;&#20154;&#31867;&#27493;&#24577;&#25968;&#25454;&#39564;&#35777;&#20102;&#25105;&#20204;&#27169;&#22411;&#21644;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05473v2 Announce Type: replace  Abstract: Modeling and control of the human musculoskeletal system is important for understanding human motor functions, developing embodied intelligence, and optimizing human-robot interaction systems. However, current open-source models are restricted to a limited range of body parts and often with a reduced number of muscles. There is also a lack of algorithms capable of controlling over 600 muscles to generate reasonable human movements. To fill this gap, we build a musculoskeletal model with 90 body segments, 206 joints, and 700 muscle-tendon units, allowing simulation of full-body dynamics and interaction with various devices. We develop a new algorithm using low-dimensional representation and hierarchical deep reinforcement learning to achieve state-of-the-art full-body control. We validate the effectiveness of our model and algorithm in simulations with real human locomotion data. The musculoskeletal model, along with its control algor
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#19979;&#20445;&#35777;&#24615;&#33021;&#30340;&#24378;&#22823;GNN&#27169;&#22411;</title><link>https://arxiv.org/abs/2312.04111</link><description>&lt;p&gt;
&#25171;&#30772;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22312;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#20013;&#30340;&#32416;&#32544;
&lt;/p&gt;
&lt;p&gt;
Breaking the Entanglement of Homophily and Heterophily in Semi-supervised Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04111
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#19979;&#20445;&#35777;&#24615;&#33021;&#30340;&#24378;&#22823;GNN&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21033;&#29992;&#22270;&#25968;&#25454;&#24211;&#30693;&#35782;&#36827;&#34892;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GNNs&#36981;&#24490;&#21516;&#36136;&#24615;&#20551;&#35774;&#65292;&#21363;&#36830;&#25509;&#30340;&#33410;&#28857;&#26356;&#26377;&#21487;&#33021;&#23637;&#29616;&#20986;&#30456;&#20284;&#30340;&#29305;&#24449;&#20998;&#24067;&#21644;&#30456;&#21516;&#30340;&#26631;&#31614;&#65292;&#36825;&#31181;&#20551;&#35774;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#34987;&#35777;&#26126;&#26159;&#33030;&#24369;&#30340;&#12290;&#20316;&#20026;&#34917;&#20805;&#65292;&#24322;&#36136;&#24615;&#21453;&#26144;&#20102;&#30456;&#36830;&#33410;&#28857;&#30340;&#19981;&#30456;&#20284;&#24615;&#65292;&#22312;&#22270;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#24037;&#31243;&#24072;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#30340;GNN&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#19979;&#20445;&#35777;&#24615;&#33021;&#12290;&#23613;&#31649;&#24050;&#36827;&#34892;&#20102;&#22823;&#37327;&#23581;&#35797;&#65292;&#20294;&#30001;&#20110;&#26080;&#21521;&#22270;&#30340;&#32422;&#26463;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GNNs&#37117;&#38590;&#20197;&#23454;&#29616;&#26368;&#20339;&#33410;&#28857;&#34920;&#31034;&#12290;&#24573;&#30053;&#26377;&#21521;&#36793;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#22270;&#34920;&#31034;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;GNNs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04111v2 Announce Type: replace-cross  Abstract: Recently, graph neural networks (GNNs) have shown prominent performance in semi-supervised node classification by leveraging knowledge from the graph database. However, most existing GNNs follow the homophily assumption, where connected nodes are more likely to exhibit similar feature distributions and the same labels, and such an assumption has proven to be vulnerable in a growing number of practical applications. As a supplement, heterophily reflects dissimilarity in connected nodes, which has gained significant attention in graph learning. To this end, data engineers aim to develop a powerful GNN model that can ensure performance under both homophily and heterophily. Despite numerous attempts, most existing GNNs struggle to achieve optimal node representations due to the constraints of undirected graphs. The neglect of directed edges results in sub-optimal graph representations, thereby hindering the capacity of GNNs. To add
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631; SharpeRatio@k&#65292;&#29992;&#20110;&#35780;&#20272;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#39118;&#38505;-&#25910;&#30410;&#26435;&#34913;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#19981;&#21516;&#39118;&#38505;&#20272;&#35745;&#22120;&#24182;&#20934;&#30830;&#35782;&#21035;&#26368;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#12290;</title><link>https://arxiv.org/abs/2311.18207</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#39118;&#38505;-&#25910;&#30410;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631; SharpeRatio@k&#65292;&#29992;&#20110;&#35780;&#20272;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#39118;&#38505;-&#25910;&#30410;&#26435;&#34913;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#19981;&#21516;&#39118;&#38505;&#20272;&#35745;&#22120;&#24182;&#20934;&#30830;&#35782;&#21035;&#26368;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#26088;&#22312;&#20165;&#20351;&#29992;&#32447;&#19979;&#35760;&#24405;&#30340;&#25968;&#25454;&#35780;&#20272;&#21453;&#20107;&#23454;&#25919;&#31574;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#24120;&#29992;&#20110;&#35782;&#21035;&#22312;&#22312;&#32447;A/B&#27979;&#35797;&#37096;&#32626;&#20013;&#30340;&#21069;k&#20010;&#26377;&#21069;&#36884;&#30340;&#25919;&#31574;&#12290;&#24403;&#21069;&#30340;OPE&#20272;&#35745;&#22120;&#35780;&#20272;&#25351;&#26631;&#20027;&#35201;&#20851;&#27880;OPE&#25110;&#19979;&#28216;&#25919;&#31574;&#36873;&#25321;&#30340;&#8220;&#20934;&#30830;&#24615;&#8221;&#65292;&#32780;&#24573;&#30053;&#20102;&#38543;&#21518;&#22312;&#32447;&#25919;&#31574;&#37096;&#32626;&#20013;&#30340;&#39118;&#38505;-&#22238;&#25253;&#26435;&#34913;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#37329;&#34701;&#20013;&#30340;&#25237;&#36164;&#32452;&#21512;&#35780;&#20272;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;SharpeRatio@k&#30340;&#26032;&#25351;&#26631;&#65292;&#29992;&#20110;&#34913;&#37327;&#30001;OPE&#20272;&#35745;&#22120;&#24418;&#25104;&#30340;&#25919;&#31574;&#25237;&#36164;&#32452;&#21512;&#22312;&#19981;&#21516;&#30340;&#22312;&#32447;&#35780;&#20272;&#39044;&#31639;&#65288;k&#65289;&#19979;&#30340;&#39118;&#38505;-&#22238;&#25253;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#31034;&#20363;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25351;&#26631;&#65292;&#23637;&#31034;&#20102;&#20854;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#20272;&#35745;&#22120;&#65292;&#24182;&#20934;&#30830;&#35782;&#21035;&#25928;&#29575;&#26368;&#39640;&#30340;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18207v3 Announce Type: replace-cross  Abstract: Off-Policy Evaluation (OPE) aims to assess the effectiveness of counterfactual policies using only offline logged data and is often used to identify the top-k promising policies for deployment in online A/B tests. Existing evaluation metrics for OPE estimators primarily focus on the "accuracy" of OPE or that of downstream policy selection, neglecting risk-return tradeoff in the subsequent online policy deployment. To address this issue, we draw inspiration from portfolio evaluation in finance and develop a new metric, called SharpeRatio@k, which measures the risk-return tradeoff of policy portfolios formed by an OPE estimator under varying online evaluation budgets (k). We validate our metric in two example scenarios, demonstrating its ability to effectively distinguish between low-risk and high-risk estimators and to accurately identify the most efficient one. Efficiency of an estimator is characterized by its capability to fo
&lt;/p&gt;</description></item><item><title>SCOPE-RL&#26159;&#19968;&#20010;Python&#24211;&#65292;&#20860;&#39038;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#31574;&#30053;&#35780;&#20272;&#65292;&#36890;&#36807;&#25972;&#21512;&#31574;&#30053;&#23398;&#20064;&#21644;&#35780;&#20272;&#23454;&#29616;&#20102;&#26356;&#28789;&#27963;&#12289;&#23436;&#25972;&#30340;&#23454;&#29616;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;OPE&#27169;&#22359;&#25552;&#20379;&#20102;&#22810;&#31181;OPE&#20272;&#35745;&#22120;&#21644;&#31283;&#20581;&#30340;OPE&#21327;&#35758;&#65292;&#20351;&#24471;OPE&#26356;&#28145;&#20837;&#21644;&#21487;&#38752;&#12290;</title><link>https://arxiv.org/abs/2311.18206</link><description>&lt;p&gt;
SCOPE-RL: &#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
SCOPE-RL: A Python Library for Offline Reinforcement Learning and Off-Policy Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18206
&lt;/p&gt;
&lt;p&gt;
SCOPE-RL&#26159;&#19968;&#20010;Python&#24211;&#65292;&#20860;&#39038;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#31574;&#30053;&#35780;&#20272;&#65292;&#36890;&#36807;&#25972;&#21512;&#31574;&#30053;&#23398;&#20064;&#21644;&#35780;&#20272;&#23454;&#29616;&#20102;&#26356;&#28789;&#27963;&#12289;&#23436;&#25972;&#30340;&#23454;&#29616;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;OPE&#27169;&#22359;&#25552;&#20379;&#20102;&#22810;&#31181;OPE&#20272;&#35745;&#22120;&#21644;&#31283;&#20581;&#30340;OPE&#21327;&#35758;&#65292;&#20351;&#24471;OPE&#26356;&#28145;&#20837;&#21644;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SCOPE-RL&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#24320;&#28304;Python&#36719;&#20214;&#65292;&#19987;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;offline RL&#65289;&#12289;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#21644;&#36873;&#25321;&#65288;OPS&#65289;&#32780;&#35774;&#35745;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24211;&#19981;&#21516;&#65292;&#36825;&#20123;&#24211;&#20165;&#20851;&#27880;&#31574;&#30053;&#23398;&#20064;&#25110;&#35780;&#20272;&#20013;&#30340;&#19968;&#20010;&#65292;SCOPE-RL&#26080;&#32541;&#25972;&#21512;&#20102;&#36825;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#20419;&#36827;&#20102;&#31163;&#32447;RL&#21644;OPE&#36807;&#31243;&#30340;&#28789;&#27963;&#21644;&#23436;&#25972;&#23454;&#29616;&#12290;SCOPE-RL&#29305;&#21035;&#20391;&#37325;&#20110;&#20854;OPE&#27169;&#22359;&#65292;&#25552;&#20379;&#19968;&#31995;&#21015;OPE&#20272;&#35745;&#22120;&#21644;&#31283;&#20581;&#30340;OPE&#21327;&#35758;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#19982;&#20854;&#20182;&#36719;&#20214;&#21253;&#30456;&#27604;&#65292;SCOPE-RL&#33021;&#22815;&#26356;&#28145;&#20837;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;OPE&#12290;&#20363;&#22914;&#65292;SCOPE-RL&#36890;&#36807;&#20272;&#35745;&#31574;&#30053;&#19979;&#30340;&#25972;&#20010;&#22870;&#21169;&#20998;&#24067;&#32780;&#19981;&#20165;&#20165;&#26159;&#20854;&#28857;&#20540;&#39044;&#26399;&#20540;&#26469;&#22686;&#24378;OPE&#12290;&#27492;&#22806;&#65292;SCOPE-RL&#36890;&#36807;&#22312;OPE&#32467;&#26524;&#20013;&#25552;&#20379;&#39118;&#38505;-&#22238;&#25253;&#26435;&#34913;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#24211;&#20013;&#20165;&#20165;&#26159;&#20934;&#30830;&#24615;&#35780;&#20272;&#30340;&#26356;&#20840;&#38754;&#30340;OPE&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18206v3 Announce Type: replace-cross  Abstract: This paper introduces SCOPE-RL, a comprehensive open-source Python software designed for offline reinforcement learning (offline RL), off-policy evaluation (OPE), and selection (OPS). Unlike most existing libraries that focus solely on either policy learning or evaluation, SCOPE-RL seamlessly integrates these two key aspects, facilitating flexible and complete implementations of both offline RL and OPE processes. SCOPE-RL put particular emphasis on its OPE modules, offering a range of OPE estimators and robust evaluation-of-OPE protocols. This approach enables more in-depth and reliable OPE compared to other packages. For instance, SCOPE-RL enhances OPE by estimating the entire reward distribution under a policy rather than its mere point-wise expected value. Additionally, SCOPE-RL provides a more thorough evaluation-of-OPE by presenting the risk-return tradeoff in OPE results, extending beyond mere accuracy evaluations in exis
&lt;/p&gt;</description></item><item><title>TFMQ-DM&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Temporal Feature Maintenance Quantization (TFMQ)&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#29305;&#24449;&#36827;&#34892;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.16503</link><description>&lt;p&gt;
TFMQ-DM&#65306;&#38754;&#21521;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#29305;&#24449;&#32500;&#25345;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16503
&lt;/p&gt;
&lt;p&gt;
TFMQ-DM&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Temporal Feature Maintenance Quantization (TFMQ)&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#29305;&#24449;&#36827;&#34892;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16503v2 &#36890;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449; &#25688;&#35201;&#65306;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#20294;&#30001;&#20110;&#20854;&#36739;&#38271;&#30340;&#25512;&#29702;&#26102;&#38388;&#21644;&#22823;&#37327;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#22312;&#24191;&#27867;&#36866;&#29992;&#24615;&#26041;&#38754;&#36935;&#21040;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#39640;&#25928;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#65292;&#25193;&#25955;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#26102;&#38388;&#27493;&#38271; $t$ &#26469;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#22810;&#36718;&#21435;&#22122;&#12290;&#36890;&#24120;&#65292;&#20174;&#26377;&#38480;&#38598;&#21512; $\{1, \ldots, T\}$ &#20013;&#30340; $t$&#20250;&#34987;&#20960;&#20010;&#27169;&#22359;&#32534;&#30721;&#20026;&#19968;&#20010;&#26102;&#38388;&#29305;&#24449;&#65292;&#36825;&#23436;&#20840;&#19981;&#32771;&#34385;&#37319;&#26679;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#24182;&#19981;&#20998;&#21035;&#20248;&#21270;&#36825;&#20123;&#27169;&#22359;&#12290;&#23427;&#20204;&#37319;&#29992;&#19981;&#24688;&#24403;&#30340;&#37325;&#26500;&#30446;&#26631;&#21644;&#22797;&#26434;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#23548;&#33268;&#26102;&#38388;&#29305;&#24449;&#21644;&#21435;&#22122;&#36712;&#36857;&#20005;&#37325;&#21463;&#21040;&#24178;&#25200;&#65292;&#21516;&#26102;&#21387;&#32553;&#25928;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Temporal Feature Maintenance Quantization (TFMQ)&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16503v2 Announce Type: replace-cross  Abstract: The Diffusion model, a prevalent framework for image generation, encounters significant challenges in terms of broad applicability due to its extended inference times and substantial memory requirements. Efficient Post-training Quantization (PTQ) is pivotal for addressing these issues in traditional models. Different from traditional models, diffusion models heavily depend on the time-step $t$ to achieve satisfactory multi-round denoising. Usually, $t$ from the finite set $\{1, \ldots, T\}$ is encoded to a temporal feature by a few modules totally irrespective of the sampling data. However, existing PTQ methods do not optimize these modules separately. They adopt inappropriate reconstruction targets and complex calibration methods, resulting in a severe disturbance of the temporal feature and denoising trajectory, as well as a low compression efficiency. To solve these, we propose a Temporal Feature Maintenance Quantization (TF
&lt;/p&gt;</description></item><item><title>&#22312;&#31070;&#32463;&#33945;&#26085;&#26144;&#23556;&#20013;&#24341;&#20837;&#19981;&#24179;&#34913;&#24615;&#21487;&#25913;&#36827;&#26410;&#37197;&#23545;&#39046;&#22495;&#36716;&#25442;&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;</title><link>https://arxiv.org/abs/2311.15100</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#33945;&#26085;&#26144;&#23556;&#20013;&#30340;&#19981;&#24179;&#34913;&#24615;&#25913;&#36827;&#26410;&#37197;&#23545;&#39046;&#22495;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15100
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#33945;&#26085;&#26144;&#23556;&#20013;&#24341;&#20837;&#19981;&#24179;&#34913;&#24615;&#21487;&#25913;&#36827;&#26410;&#37197;&#23545;&#39046;&#22495;&#36716;&#25442;&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#20248;&#36755;&#36865;&#65288;OT&#65289;&#20013;&#65292;&#33945;&#26085;&#26144;&#23556;&#34987;&#31216;&#20026;&#20197;&#26368;&#32463;&#27982;&#30340;&#26041;&#24335;&#23558;&#28304;&#20998;&#24067;&#20256;&#36755;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#26144;&#23556;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#24182;&#24212;&#29992;&#20102;&#22810;&#31181;&#29992;&#20110;Monge&#26144;&#23556;&#30340;&#31070;&#32463;&#20272;&#35745;&#22120;&#65292;&#24182;&#22312;&#21508;&#31181;&#26410;&#37197;&#23545;&#30340;&#39046;&#22495;&#36716;&#25442;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#65292;&#20363;&#22914;&#22312;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#30340;OT&#26694;&#26550;&#24378;&#21046;&#20445;&#25345;&#36136;&#37327;&#23432;&#24658;&#65292;&#36825;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#31163;&#32676;&#20540;&#30340;&#24433;&#21709;&#65292;&#24182;&#38480;&#21046;&#20102;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#21518;&#32773;&#22312;OT&#39046;&#22495;&#36716;&#25442;&#20219;&#21153;&#20013;&#21487;&#33021;&#29305;&#21035;&#26377;&#23475;&#65292;&#22240;&#20026;&#20854;&#20013;&#26174;&#24335;&#32771;&#34385;&#20102;&#26679;&#26412;&#22312;&#20998;&#24067;&#20013;&#30340;&#30456;&#23545;&#20301;&#32622;&#12290;&#23613;&#31649;&#22312;&#31163;&#25955;&#35774;&#32622;&#20013;&#65292;&#19981;&#24179;&#34913;OT&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#20294;&#20854;&#38598;&#25104;&#21040;&#31070;&#32463;Monge&#26144;&#23556;&#20272;&#35745;&#22120;&#20013;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#24179;&#34913;&#24615;&#32435;&#20837;&#21040;&#20219;&#20309;Monge&#26144;&#23556;&#20272;&#35745;&#22120;&#20013;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#20272;&#35745;&#22120;&#20197;&#27169;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15100v2 Announce Type: replace-cross  Abstract: In optimal transport (OT), a Monge map is known as a mapping that transports a source distribution to a target distribution in the most cost-efficient way. Recently, multiple neural estimators for Monge maps have been developed and applied in diverse unpaired domain translation tasks, e.g. in single-cell biology and computer vision. However, the classic OT framework enforces mass conservation, which makes it prone to outliers and limits its applicability in real-world scenarios. The latter can be particularly harmful in OT domain translation tasks, where the relative position of a sample within a distribution is explicitly taken into account. While unbalanced OT tackles this challenge in the discrete setting, its integration into neural Monge map estimators has received limited attention. We propose a theoretically grounded method to incorporate unbalancedness into any Monge map estimator. We improve existing estimators to mode
&lt;/p&gt;</description></item><item><title>Mobile-Seed&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21452;&#20219;&#21153;&#26694;&#26550;&#65292;&#26088;&#22312;&#21516;&#26102;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#36793;&#30028;&#26816;&#27979;&#65292;&#20855;&#26377;&#21452;&#27969;&#32534;&#30721;&#22120;&#12289;&#20027;&#21160;&#34701;&#21512;&#35299;&#30721;&#22120;&#21644;&#21452;&#20219;&#21153;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.12651</link><description>&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#32852;&#21512;&#35821;&#20041;&#20998;&#21106;&#21644;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Mobile-Seed: Joint Semantic Segmentation and Boundary Detection for Mobile Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12651
&lt;/p&gt;
&lt;p&gt;
Mobile-Seed&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21452;&#20219;&#21153;&#26694;&#26550;&#65292;&#26088;&#22312;&#21516;&#26102;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#36793;&#30028;&#26816;&#27979;&#65292;&#20855;&#26377;&#21452;&#27969;&#32534;&#30721;&#22120;&#12289;&#20027;&#21160;&#34701;&#21512;&#35299;&#30721;&#22120;&#21644;&#21452;&#20219;&#21153;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#24555;&#36895;&#22320;&#21246;&#21202;&#20986;&#28165;&#26224;&#36793;&#30028;&#21644;&#31283;&#20581;&#30340;&#35821;&#20041;&#23545;&#20110;&#20247;&#22810;&#19979;&#28216;&#26426;&#22120;&#20154;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#25235;&#21462;&#21644;&#25805;&#20316;&#12289;&#23454;&#26102;&#35821;&#20041;&#26144;&#23556;&#20197;&#21450;&#22312;&#32447;&#20256;&#24863;&#22120;&#26657;&#20934;&#31561;&#20219;&#21153;&#26159;&#22312;&#36793;&#32536;&#35745;&#31639;&#21333;&#20803;&#19978;&#25191;&#34892;&#30340;&#12290;&#23613;&#31649;&#36793;&#30028;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#26159;&#20114;&#34917;&#30340;&#20219;&#21153;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#36731;&#37327;&#32423;&#27169;&#22411;&#19978;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#32780;&#24573;&#30053;&#20102;&#36793;&#30028;&#26816;&#27979;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Mobile-Seed&#65292;&#19968;&#20010;&#19987;&#20026;&#21516;&#26102;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#36793;&#30028;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#36731;&#37327;&#32423;&#21452;&#20219;&#21153;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#21452;&#27969;&#32534;&#30721;&#22120;&#12289;&#20027;&#21160;&#34701;&#21512;&#35299;&#30721;&#22120;&#65288;AFD&#65289;&#21644;&#21452;&#20219;&#21153;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#32534;&#30721;&#22120;&#20998;&#20026;&#20004;&#26465;&#36335;&#24452;&#65306;&#19968;&#26465;&#25429;&#33719;&#20855;&#26377;&#31867;&#21035;&#24863;&#30693;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#21478;&#19968;&#26465;&#20174;&#22810;&#23610;&#24230;&#29305;&#24449;&#20013;&#36776;&#21035;&#36793;&#30028;&#12290;AFD&#27169;&#22359;&#21160;&#24577;&#35843;&#25972;&#35821;&#20041;&#21644;&#36793;&#30028;&#30340;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12651v3 Announce Type: replace-cross  Abstract: Precise and rapid delineation of sharp boundaries and robust semantics is essential for numerous downstream robotic tasks, such as robot grasping and manipulation, real-time semantic mapping, and online sensor calibration performed on edge computing units. Although boundary detection and semantic segmentation are complementary tasks, most studies focus on lightweight models for semantic segmentation but overlook the critical role of boundary detection. In this work, we introduce Mobile-Seed, a lightweight, dual-task framework tailored for simultaneous semantic segmentation and boundary detection. Our framework features a two-stream encoder, an active fusion decoder (AFD) and a dual-task regularization approach. The encoder is divided into two pathways: one captures category-aware semantic information, while the other discerns boundaries from multi-scale features. The AFD module dynamically adapts the fusion of semantic and boun
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28145;&#24230;&#25511;&#21046;&#25193;&#25955;&#65288;ADD&#65289;&#32593;&#32476;&#65292;&#36890;&#36807;&#28151;&#21512;&#24335;&#27880;&#24847;&#21147;&#31574;&#30053;&#25351;&#23548;&#21452;&#21521;&#25193;&#25955;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#30149;&#29702;&#28176;&#36827;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2311.12316</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#20174;&#30149;&#29702;&#36716;&#21464;&#29983;&#25104;&#28176;&#36827;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Generating Progressive Images from Pathological Transitions via Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12316
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28145;&#24230;&#25511;&#21046;&#25193;&#25955;&#65288;ADD&#65289;&#32593;&#32476;&#65292;&#36890;&#36807;&#28151;&#21512;&#24335;&#27880;&#24847;&#21147;&#31574;&#30053;&#25351;&#23548;&#21452;&#21521;&#25193;&#25955;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#30149;&#29702;&#28176;&#36827;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#36741;&#21161;&#30149;&#29702;&#35786;&#26029;&#65292;&#20943;&#36731;&#30149;&#29702;&#24037;&#20316;&#37327;&#65292;&#25552;&#20379;&#21450;&#26102;&#30340;&#20020;&#24202;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#35268;&#27169;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#30001;&#20110;&#30149;&#29702;&#22270;&#20687;&#30340;&#37319;&#26679;&#21644;&#27880;&#37322;&#31232;&#32570;&#24615;&#32780;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#24555;&#36895;&#21457;&#23637;&#30340;&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#20174;&#30149;&#29702;&#22270;&#20687;&#20013;&#29983;&#25104;&#26356;&#22810;&#35757;&#32451;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#26222;&#36941;&#38754;&#20020;&#27867;&#21270;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;&#65292;&#26080;&#27861;&#29983;&#25104;&#26377;&#25928;&#26679;&#26412;&#12290;&#21463;&#19981;&#21516;&#38454;&#27573;&#20043;&#38388;&#30340;&#30149;&#29702;&#36716;&#21464;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28145;&#24230;&#25511;&#21046;&#25193;&#25955;&#65288;ADD&#65289;&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#25104;&#30149;&#29702;&#28176;&#36827;&#22270;&#20687;&#20197;&#36827;&#34892;&#26377;&#25928;&#25968;&#25454;&#22686;&#24378;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26681;&#28304;&#20110;&#39046;&#22495;&#36801;&#31227;&#65292;&#20854;&#20013;&#28151;&#21512;&#24335;&#27880;&#24847;&#21147;&#31574;&#30053;&#24341;&#23548;&#21452;&#21521;&#25193;&#25955;&#65292;&#34701;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#27880;&#24847;&#21147;&#20248;&#20808;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12316v2 Announce Type: replace-cross  Abstract: Deep learning is widely applied in computer-aided pathological diagnosis, which alleviates the pathologist workload and provide timely clinical analysis. However, most models generally require large-scale annotated data for training, which faces challenges due to the sampling and annotation scarcity in pathological images. The rapid developing generative models shows potential to generate more training samples from recent studies. However, they also struggle in generalization diversity with limited training data, incapable of generating effective samples. Inspired by the pathological transitions between different stages, we propose an adaptive depth-controlled diffusion (ADD) network to generate pathological progressive images for effective data augmentation. This novel approach roots in domain migration, where a hybrid attention strategy guides the bidirectional diffusion, blending local and global attention priorities. With f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102; counseling response rewriting &#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#26495;&#30340; VERVE &#31995;&#32479;&#65292;&#36890;&#36807;&#21152;&#20837;&#37322;&#20041;&#30340;&#35757;&#32451;&#21644;&#33258;&#36866;&#24212;&#27169;&#26495;&#26356;&#26032;&#65292;&#23558;&#38750;&#21453;&#24605;&#24615;&#38472;&#36848;&#36716;&#21270;&#20026;&#21453;&#24605;&#24615;&#22238;&#24212;&#12290;</title><link>https://arxiv.org/abs/2311.08299</link><description>&lt;p&gt;
VERVE: &#22522;&#20110;&#27169;&#26495;&#30340;&#21453;&#24605;&#37325;&#20889;&#29992;&#20110;&#28608;&#21169;&#24615;&#38754;&#35848;
&lt;/p&gt;
&lt;p&gt;
VERVE: Template-based ReflectiVE Rewriting for MotiVational IntErviewing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102; counseling response rewriting &#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#26495;&#30340; VERVE &#31995;&#32479;&#65292;&#36890;&#36807;&#21152;&#20837;&#37322;&#20041;&#30340;&#35757;&#32451;&#21644;&#33258;&#36866;&#24212;&#27169;&#26495;&#26356;&#26032;&#65292;&#23558;&#38750;&#21453;&#24605;&#24615;&#38472;&#36848;&#36716;&#21270;&#20026;&#21453;&#24605;&#24615;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#24605;&#24335;&#20542;&#21548;&#26159;&#24515;&#29702;&#36741;&#23548;&#24072;&#24517;&#39035;&#25484;&#25569;&#30340;&#22522;&#26412;&#25216;&#33021;&#65292;&#20197;&#22312;&#28608;&#21169;&#24615;&#38754;&#35848;&#20013;&#36798;&#21040;&#29087;&#32451;&#27700;&#24179;&#12290;&#23427;&#28041;&#21450;&#20197;&#19968;&#31181;&#25215;&#35748;&#21644;&#25506;&#32034;&#23458;&#25143;&#23545;&#35805;&#20013;&#34920;&#36798;&#30340;&#24847;&#20041;&#30340;&#26041;&#24335;&#36827;&#34892;&#22238;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36741;&#23548;&#21709;&#24212;&#37325;&#20889;&#30340;&#20219;&#21153;&#65292;&#23558;&#38750;&#21453;&#24605;&#24615;&#38472;&#36848;&#36716;&#21270;&#20026;&#21453;&#24605;&#24615;&#22238;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;VERVE&#65292;&#19968;&#20010;&#22522;&#20110;&#27169;&#26495;&#30340;&#37325;&#20889;&#31995;&#32479;&#65292;&#20855;&#26377;&#21152;&#20837;&#37322;&#20041;&#30340;&#35757;&#32451;&#21644;&#33258;&#36866;&#24212;&#27169;&#26495;&#26356;&#26032;&#12290;VERVE&#39318;&#20808;&#36890;&#36807;&#35782;&#21035;&#21644;&#36807;&#28388;&#19982;&#21453;&#24605;&#26080;&#20851;&#30340;&#26631;&#35760;&#26469;&#21019;&#24314;&#27169;&#26495;&#65292;&#24182;&#20351;&#29992;&#27169;&#26495;&#26500;&#24314;&#21453;&#24605;&#24335;&#22238;&#24212;&#12290;&#37322;&#20041;&#22686;&#24378;&#35757;&#32451;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#23545;&#25513;&#30721;&#31354;&#38388;&#36827;&#34892;&#36739;&#19981;&#20005;&#26684;&#30340;&#22635;&#20805;&#65292;&#32780;&#33258;&#36866;&#24212;&#27169;&#26495;&#26356;&#26032;&#21017;&#26377;&#21161;&#20110;&#21457;&#29616;&#37325;&#20889;&#30340;&#26377;&#25928;&#27169;&#26495;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#31227;&#38500;&#21407;&#22987;&#20869;&#23481;&#12290;&#20351;&#29992;&#33258;&#21160;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08299v2 Announce Type: replace-cross  Abstract: Reflective listening is a fundamental skill that counselors must acquire to achieve proficiency in motivational interviewing (MI). It involves responding in a manner that acknowledges and explores the meaning of what the client has expressed in the conversation. In this work, we introduce the task of counseling response rewriting, which transforms non-reflective statements into reflective responses. We introduce VERVE, a template-based rewriting system with paraphrase-augmented training and adaptive template updating. VERVE first creates a template by identifying and filtering out tokens that are not relevant to reflections and constructs a reflective response using the template. Paraphrase-augmented training allows the model to learn less-strict fillings of masked spans, and adaptive template updating helps discover effective templates for rewriting without significantly removing the original content. Using both automatic and 
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#25552;&#20986;&#20102;LRM&#65292;&#37319;&#29992;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#21644;&#39640;&#23481;&#37327;&#27169;&#22411;&#65292;&#21487;&#22312;&#30701;&#26102;&#38388;&#20869;&#20174;&#21333;&#20010;&#22270;&#20687;&#20013;&#39044;&#27979;&#39640;&#36136;&#37327;3D&#37325;&#24314;&#32467;&#26524;</title><link>https://arxiv.org/abs/2311.04400</link><description>&lt;p&gt;
LRM&#65306;&#21333;&#22270;&#20687;&#21040;3D&#30340;&#22823;&#22411;&#37325;&#24314;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LRM: Large Reconstruction Model for Single Image to 3D
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04400
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#25552;&#20986;&#20102;LRM&#65292;&#37319;&#29992;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#21644;&#39640;&#23481;&#37327;&#27169;&#22411;&#65292;&#21487;&#22312;&#30701;&#26102;&#38388;&#20869;&#20174;&#21333;&#20010;&#22270;&#20687;&#20013;&#39044;&#27979;&#39640;&#36136;&#37327;3D&#37325;&#24314;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22823;&#22411;&#37325;&#24314;&#27169;&#22411;&#65288;LRM&#65289;&#65292;&#21487;&#20197;&#22312;&#30701;&#30701;5&#31186;&#20869;&#20174;&#21333;&#20010;&#36755;&#20837;&#22270;&#20687;&#20013;&#39044;&#27979;&#23545;&#35937;&#30340;3D&#27169;&#22411;&#12290;&#19982;&#35768;&#22810;&#20808;&#21069;&#35757;&#32451;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#22914;ShapeNet&#65289;&#19978;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;LRM&#37319;&#29992;&#20102;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;5&#20159;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#36755;&#20837;&#22270;&#20687;&#20013;&#39044;&#27979;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#12290;&#25105;&#20204;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#22312;&#21253;&#21547;&#32422;100&#19975;&#20010;&#23545;&#35937;&#30340;&#22823;&#35268;&#27169;&#22810;&#35270;&#35282;&#25968;&#25454;&#19978;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#26469;&#33258;Objaverse&#30340;&#21512;&#25104;&#28210;&#26579;&#21644;&#26469;&#33258;MVImgNet&#30340;&#30495;&#23454;&#25429;&#33719;&#12290;&#36825;&#31181;&#39640;&#23481;&#37327;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#30340;&#32467;&#21512;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#24456;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;&#21508;&#31181;&#27979;&#35797;&#36755;&#20837;&#20013;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;3D&#37325;&#24314;&#32467;&#26524;&#65292;&#21253;&#25324;&#30495;&#23454;&#22330;&#26223;&#25429;&#33719;&#21644;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#30340;&#22270;&#20687;&#12290;&#35270;&#39057;&#28436;&#31034;&#21644;&#21487;&#20132;&#20114;&#30340;3D&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04400v2 Announce Type: replace-cross  Abstract: We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D mes
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#20855;&#36523;&#26234;&#33021;&#20013;&#20351;&#29992;&#20219;&#21153;&#26465;&#20214;&#30340;&#36873;&#25321;&#24615;&#36807;&#28388;&#22120;&#26469;&#25913;&#21892;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;</title><link>https://arxiv.org/abs/2311.04193</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#35270;&#35273;&#34920;&#31034;&#25552;&#39640;&#20855;&#36523;&#26234;&#33021;&#30340;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#24615;
&lt;/p&gt;
&lt;p&gt;
Selective Visual Representations Improve Convergence and Generalization for Embodied AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04193
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#20855;&#36523;&#26234;&#33021;&#20013;&#20351;&#29992;&#20219;&#21153;&#26465;&#20214;&#30340;&#36873;&#25321;&#24615;&#36807;&#28388;&#22120;&#26469;&#25913;&#21892;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#36523;&#26234;&#33021;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#31867;&#20284;CLIP&#20043;&#31867;&#30340;&#36890;&#29992;&#35270;&#35273;&#20027;&#24178;&#26469;&#32534;&#30721;&#23427;&#20204;&#30340;&#35270;&#35273;&#35266;&#23519;&#12290;&#23613;&#31649;&#36825;&#31181;&#36890;&#29992;&#30446;&#30340;&#30340;&#34920;&#31034;&#32534;&#30721;&#20102;&#20851;&#20110;&#22330;&#26223;&#30340;&#20016;&#23500;&#21477;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#20294;&#20854;&#20013;&#24456;&#22810;&#20449;&#24687;&#36890;&#24120;&#19982;&#25163;&#22836;&#30340;&#20855;&#20307;&#20219;&#21153;&#26080;&#20851;&#12290;&#36825;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#22122;&#22768;&#65292;&#24182;&#20351;&#20195;&#29702;&#20154;&#30340;&#27880;&#24847;&#21147;&#20174;&#20219;&#21153;&#30456;&#20851;&#30340;&#35270;&#35273;&#32447;&#32034;&#36716;&#31227;&#12290;&#21463;&#20154;&#31867;&#36873;&#25321;&#24615;&#27880;&#24847;&#30340;&#21551;&#21457;-&#21363;&#20154;&#20204;&#26681;&#25454;&#20182;&#20204;&#30340;&#32463;&#39564;&#12289;&#30693;&#35782;&#21644;&#25163;&#22836;&#30340;&#20219;&#21153;&#26469;&#36807;&#28388;&#20182;&#20204;&#30340;&#24863;&#30693;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20026;&#20855;&#36523;&#26234;&#33021;&#36807;&#28388;&#35270;&#35273;&#21050;&#28608;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#21487;&#23398;&#20064;&#30340;&#30721;&#20070;&#27169;&#22359;&#24341;&#20837;&#20102;&#19968;&#20010;&#20219;&#21153;&#26465;&#20214;&#30340;&#29942;&#39048;&#12290;&#36825;&#20010;&#30721;&#20070;&#26159;&#32852;&#21512;&#35757;&#32451;&#26469;&#20248;&#21270;&#20219;&#21153;&#22870;&#21169;&#65292;&#24182;&#20316;&#20026;&#19968;&#20010;&#20219;&#21153;&#26465;&#20214;&#30340;&#36873;&#25321;&#24615;&#36807;&#28388;&#22120;&#20316;&#29992;&#20110;&#35270;&#35273;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#38024;&#23545;obj&#30340;&#34920;&#29616;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04193v2 Announce Type: replace-cross  Abstract: Embodied AI models often employ off the shelf vision backbones like CLIP to encode their visual observations. Although such general purpose representations encode rich syntactic and semantic information about the scene, much of this information is often irrelevant to the specific task at hand. This introduces noise within the learning process and distracts the agent's focus from task-relevant visual cues. Inspired by selective attention in humans-the process through which people filter their perception based on their experiences, knowledge, and the task at hand-we introduce a parameter-efficient approach to filter visual stimuli for embodied AI. Our approach induces a task-conditioned bottleneck using a small learnable codebook module. This codebook is trained jointly to optimize task reward and acts as a task-conditioned selective filter over the visual observation. Our experiments showcase state-of-the-art performance for obj
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#39564;&#35777;&#32534;&#35793;&#22120;&#23454;&#29616;&#65292;&#25506;&#32034;&#20102;&#21508;&#31181;LLMs&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2310.04963</link><description>&lt;p&gt;
LLM4VV&#65306;&#20026;&#32534;&#35793;&#22120;&#39564;&#35777;&#24320;&#21457;&#22522;&#20110;LLM&#30340;&#27979;&#35797;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#39564;&#35777;&#32534;&#35793;&#22120;&#23454;&#29616;&#65292;&#25506;&#32034;&#20102;&#21508;&#31181;LLMs&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#31181;&#26032;&#32780;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#28085;&#30422;&#33258;&#28982;&#35821;&#35328;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#27979;&#35797;&#26469;&#39564;&#35777;&#21644;&#39564;&#35777;&#22522;&#20110;&#25351;&#20196;&#30340;&#24182;&#34892;&#32534;&#31243;&#33539;&#20363;OpenACC&#30340;&#32534;&#35793;&#22120;&#23454;&#29616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26368;&#20808;&#36827;&#30340;LLM&#25216;&#26415;&#65292;&#21253;&#25324;&#24320;&#28304;LLMs -- Meta Codellama&#12289;Codellama&#30340;Phind&#24494;&#35843;&#29256;&#26412;&#12289;Deepseek Coder&#21644;&#38381;&#28304;LLMs -- OpenAI GPT-3.5-Turbo&#21644;GPT-4-Turbo&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#33258;&#24049;&#30340;&#27979;&#35797;&#22871;&#20214;&#25968;&#25454;&#38598;&#21644;OpenACC&#35268;&#33539;&#23545;&#24320;&#28304;LLMs&#21644;GPT-3.5-Turbo&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#21508;&#31181;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#26469;&#25506;&#32034;&#36825;&#20123;LLMs&#65292;&#21253;&#25324;&#20195;&#30721;&#27169;&#26495;&#12289;&#24102;&#26377;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#30340;&#27169;&#26495;&#12289;&#19968;&#27425;&#24615;&#31034;&#20363;&#12289;&#24102;&#26377;RAG&#30340;&#19968;&#27425;&#24615;&#21644;&#34920;&#36798;&#24335;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04963v3 Announce Type: replace  Abstract: Large language models (LLMs) are a new and powerful tool for a wide span of applications involving natural language and demonstrate impressive code generation abilities. The goal of this work is to automatically generate tests and use these tests to validate and verify compiler implementations of a directive-based parallel programming paradigm, OpenACC. To do so, in this paper, we explore the capabilities of state-of-the-art LLMs, including open-source LLMs -- Meta Codellama, Phind fine-tuned version of Codellama, Deepseek Deepseek Coder and closed-source LLMs -- OpenAI GPT-3.5-Turbo and GPT-4-Turbo. We further fine-tuned the open-source LLMs and GPT-3.5-Turbo using our own testsuite dataset along with using the OpenACC specification. We also explored these LLMs using various prompt engineering techniques that include code template, template with retrieval-augmented generation (RAG), one-shot example, one-shot with RAG, expressive pr
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#24037;&#20855;&#22312;&#20154;&#38469;&#20132;&#27969;&#26041;&#38754;&#30340;&#33021;&#21147;&#25345;&#31215;&#26497;&#30475;&#27861;&#65292;&#35748;&#20026;&#21487;&#20197;&#22686;&#21152;&#27807;&#36890;&#33258;&#20449;&#12289;&#24110;&#21161;&#34920;&#36798;&#24819;&#27861;&#20197;&#21450;&#20811;&#26381;&#35821;&#35328;&#21644;&#25991;&#21270;&#38556;&#30861;&#65292;&#20294;&#20063;&#25581;&#31034;&#20986;&#24037;&#20855;&#23384;&#22312;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#21644;&#29992;&#25143;&#20851;&#20110;&#25216;&#26415;&#19981;&#30495;&#23454;&#24615;&#21644;&#36807;&#24230;&#20381;&#36182;&#30340;&#25285;&#24551;&#12290;</title><link>https://arxiv.org/abs/2310.03976</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#33258;&#25105;&#65306;&#29992;&#25143;&#23545;&#20154;&#24037;&#26234;&#33021;&#22312;&#20154;&#38469;&#20132;&#27969;&#21644;&#33258;&#25105;&#26041;&#38754;&#28508;&#21147;&#30340;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
From Text to Self: Users' Perceptions of Potential of AI on Interpersonal Communication and Self
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.03976
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#24037;&#20855;&#22312;&#20154;&#38469;&#20132;&#27969;&#26041;&#38754;&#30340;&#33021;&#21147;&#25345;&#31215;&#26497;&#30475;&#27861;&#65292;&#35748;&#20026;&#21487;&#20197;&#22686;&#21152;&#27807;&#36890;&#33258;&#20449;&#12289;&#24110;&#21161;&#34920;&#36798;&#24819;&#27861;&#20197;&#21450;&#20811;&#26381;&#35821;&#35328;&#21644;&#25991;&#21270;&#38556;&#30861;&#65292;&#20294;&#20063;&#25581;&#31034;&#20986;&#24037;&#20855;&#23384;&#22312;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#21644;&#29992;&#25143;&#20851;&#20110;&#25216;&#26415;&#19981;&#30495;&#23454;&#24615;&#21644;&#36807;&#24230;&#20381;&#36182;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;AI&#20013;&#20171;&#20132;&#27969;&#65288;AIMC&#65289;&#39046;&#22495;&#20013;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#24037;&#20855;&#27491;&#25104;&#20026;&#20154;&#38469;&#20132;&#27969;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20026;&#26399;&#19968;&#21608;&#30340;&#26085;&#35760;&#21644;&#35775;&#35848;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#29992;&#25143;&#23545;&#36825;&#20123;&#24037;&#20855;&#22312;&#30701;&#26399;&#20869;&#25903;&#25345;&#20154;&#38469;&#20132;&#27969;&#30340;&#33021;&#21147;&#21644;&#21487;&#33021;&#23548;&#33268;&#30340;&#38271;&#26399;&#25928;&#26524;&#30340;&#30475;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21442;&#19982;&#32773;&#23545;AIMC&#25903;&#25345;&#25345;&#26377;&#31215;&#26497;&#30475;&#27861;&#65292;&#35748;&#20026;&#20854;&#33021;&#22815;&#22686;&#21152;&#27807;&#36890;&#33258;&#20449;&#65292;&#24110;&#21161;&#25214;&#21040;&#20934;&#30830;&#30340;&#35821;&#35328;&#34920;&#36798;&#24819;&#27861;&#65292;&#20197;&#21450;&#20811;&#26381;&#35821;&#35328;&#21644;&#25991;&#21270;&#38556;&#30861;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;AIMC&#24037;&#20855;&#30446;&#21069;&#23384;&#22312;&#30340;&#23616;&#38480;&#65292;&#21253;&#25324;&#21872;&#21990;&#30340;&#22238;&#22797;&#12289;&#19981;&#33258;&#28982;&#30340;&#22238;&#24212;&#20197;&#21450;&#36807;&#24230;&#24773;&#32490;&#21270;&#12290;&#36825;&#20123;&#32570;&#38519;&#36827;&#19968;&#27493;&#21463;&#21040;&#29992;&#25143;&#23545;&#19981;&#30495;&#23454;&#24615;&#21644;&#23545;&#25216;&#26415;&#36807;&#24230;&#20381;&#36182;&#30340;&#25285;&#24551;&#25152;&#21152;&#21095;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.03976v2 Announce Type: cross  Abstract: In the rapidly evolving landscape of AI-mediated communication (AIMC), tools powered by Large Language Models (LLMs) are becoming integral to interpersonal communication. Employing a mixed-methods approach, we conducted a one-week diary and interview study to explore users' perceptions of these tools' ability to: 1) support interpersonal communication in the short-term, and 2) lead to potential long-term effects. Our findings indicate that participants view AIMC support favorably, citing benefits such as increased communication confidence, and finding precise language to express their thoughts, navigating linguistic and cultural barriers. However, the study also uncovers current limitations of AIMC tools, including verbosity, unnatural responses, and excessive emotional intensity. These shortcomings are further exacerbated by user concerns about inauthenticity and potential overreliance on the technology. Furthermore, we identified fou
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#32437;&#21521;&#30740;&#31350;&#24052;&#35199;&#24635;&#32479;&#20505;&#36873;&#20154;&#22312;Instagram&#19978;&#21457;&#24067;&#30340;&#24086;&#23376;&#65292;&#25581;&#31034;&#20102;&#35270;&#35273;&#25919;&#27835;&#20256;&#25773;&#20013;&#22266;&#23450;&#30340;&#27169;&#24335;&#65292;&#21253;&#25324;&#24198;&#31069;&#21644;&#31215;&#26497;&#35843;&#24615;&#22270;&#20687;&#30340;&#26222;&#36941;&#20351;&#29992;&#20197;&#21450;&#20505;&#36873;&#20154;&#19982;&#36873;&#27665;&#32039;&#23494;&#32852;&#31995;&#30340;&#20010;&#24615;&#21270;&#23637;&#31034;&#12290;</title><link>https://arxiv.org/abs/2310.00349</link><description>&lt;p&gt;
&#26497;&#21270;&#31038;&#20250;&#20013;&#30340;&#35270;&#35273;&#25919;&#27835;&#20256;&#25773;: &#24052;&#35199;&#24635;&#32479;&#36873;&#20030;&#22312;Instagram&#19978;&#30340;&#32437;&#21521;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Visual Political Communication in a Polarized Society: A Longitudinal Study of Brazilian Presidential Elections on Instagram
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#32437;&#21521;&#30740;&#31350;&#24052;&#35199;&#24635;&#32479;&#20505;&#36873;&#20154;&#22312;Instagram&#19978;&#21457;&#24067;&#30340;&#24086;&#23376;&#65292;&#25581;&#31034;&#20102;&#35270;&#35273;&#25919;&#27835;&#20256;&#25773;&#20013;&#22266;&#23450;&#30340;&#27169;&#24335;&#65292;&#21253;&#25324;&#24198;&#31069;&#21644;&#31215;&#26497;&#35843;&#24615;&#22270;&#20687;&#30340;&#26222;&#36941;&#20351;&#29992;&#20197;&#21450;&#20505;&#36873;&#20154;&#19982;&#36873;&#27665;&#32039;&#23494;&#32852;&#31995;&#30340;&#20010;&#24615;&#21270;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#23383;&#26102;&#20195;&#65292;&#22270;&#20687;&#24050;&#32463;&#25104;&#20026;&#25919;&#27835;&#20154;&#22763;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#19982;&#36873;&#27665;&#20114;&#21160;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#35270;&#35273;&#20869;&#23481;&#20855;&#26377;&#29420;&#29305;&#30340;&#24773;&#24863;&#21560;&#24341;&#21147;&#65292;&#24448;&#24448;&#23548;&#33268;&#29992;&#25143;&#21442;&#19982;&#24230;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#23545;&#35270;&#35273;&#20256;&#25773;&#30340;&#30740;&#31350;&#20173;&#28982;&#30456;&#23545;&#26377;&#38480;&#65292;&#23588;&#20854;&#26159;&#22312;&#20840;&#29699;&#21335;&#26041;&#22269;&#23478;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#35745;&#31639;&#26041;&#27861;&#21644;&#23450;&#24615;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;2018&#24180;&#21644;2022&#24180;19&#20301;&#24052;&#35199;&#24635;&#32479;&#20505;&#36873;&#20154;&#22312;Instagram&#19978;&#21457;&#24067;&#30340;11,263&#26465;&#24086;&#23376;&#20013;&#37319;&#29992;&#30340;&#35270;&#35273;&#20256;&#25773;&#31574;&#30053;&#65292;&#20197;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#36890;&#36807;&#20004;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#20505;&#36873;&#20154;&#22312;&#35270;&#35273;&#25919;&#27835;&#20256;&#25773;&#19978;&#23384;&#22312;&#19968;&#33268;&#30340;&#27169;&#24335;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24198;&#31069;&#21644;&#31215;&#26497;&#35843;&#24615;&#22270;&#20687;&#30340;&#26222;&#36941;&#24615;&#12290;&#20182;&#20204;&#36824;&#23637;&#31034;&#20102;&#24378;&#28872;&#30340;&#20010;&#24615;&#21270;&#24863;&#65292;&#25551;&#32472;&#20505;&#36873;&#20154;&#19982;&#36873;&#27665;&#26356;&#32039;&#23494;&#30456;&#36830;&#30340;&#24418;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00349v2 Announce Type: replace-cross  Abstract: In today's digital age, images have emerged as powerful tools for politicians to engage with their voters on social media platforms. Visual content possesses a unique emotional appeal that often leads to increased user engagement. However, research on visual communication remains relatively limited, particularly in the Global South. This study aims to bridge this gap by employing a combination of computational methods and qualitative approach to investigate the visual communication strategies employed in a dataset of 11,263 Instagram posts by 19 Brazilian presidential candidates in 2018 and 2022 national elections. Through two studies, we observed consistent patterns across these candidates on their use of visual political communication. Notably, we identify a prevalence of celebratory and positively toned images. They also exhibit a strong sense of personalization, portraying candidates connected with their voters on a more em
&lt;/p&gt;</description></item><item><title>&#22330;&#26223;&#36890;&#30693;&#32773;&#26159;&#19968;&#31181;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#39044;&#27979;&#35266;&#23519;&#20195;&#29702;&#30340;&#36712;&#36857;&#21644;&#25512;&#26029;&#36974;&#25377;&#65292;&#20854;&#21033;&#29992;transformer&#32858;&#21512;&#36755;&#20837;&#27169;&#24577;&#24182;&#23454;&#29616;&#23545;&#21487;&#33021;&#19982;&#33258;&#20027;&#36710;&#36742;&#35745;&#21010;&#36335;&#24452;&#30456;&#20132;&#30340;&#36974;&#25377;&#30340;&#36873;&#25321;&#24615;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2309.13893</link><description>&lt;p&gt;
&#22330;&#26223;&#36890;&#30693;&#32773;&#65306;&#22522;&#20110;&#38170;&#28857;&#30340;&#36974;&#25377;&#25512;&#26029;&#21644;&#36712;&#36857;&#39044;&#27979;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;
&lt;/p&gt;
&lt;p&gt;
Scene Informer: Anchor-based Occlusion Inference and Trajectory Prediction in Partially Observable Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13893
&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#36890;&#30693;&#32773;&#26159;&#19968;&#31181;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#39044;&#27979;&#35266;&#23519;&#20195;&#29702;&#30340;&#36712;&#36857;&#21644;&#25512;&#26029;&#36974;&#25377;&#65292;&#20854;&#21033;&#29992;transformer&#32858;&#21512;&#36755;&#20837;&#27169;&#24577;&#24182;&#23454;&#29616;&#23545;&#21487;&#33021;&#19982;&#33258;&#20027;&#36710;&#36742;&#35745;&#21010;&#36335;&#24452;&#30456;&#20132;&#30340;&#36974;&#25377;&#30340;&#36873;&#25321;&#24615;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#33258;&#20027;&#36710;&#36742;&#65288;AVs&#65289;&#23548;&#33322;&#38656;&#35201;AVs&#25512;&#29702;&#21487;&#35265;&#21644;&#36974;&#25377;&#21306;&#22495;&#12290;&#36825;&#28041;&#21450;&#39044;&#27979;&#35266;&#23519;&#20195;&#29702;&#30340;&#26410;&#26469;&#36816;&#21160;&#65292;&#25512;&#26029;&#34987;&#36974;&#25377;&#30340;&#20195;&#29702;&#65292;&#24182;&#26681;&#25454;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#30340;&#30690;&#37327;&#21270;&#22330;&#26223;&#34920;&#31034;&#24314;&#27169;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36974;&#25377;&#25512;&#26029;&#21644;&#36712;&#36857;&#39044;&#27979;&#26041;&#38754;&#30340;&#20808;&#21069;&#24037;&#20316;&#26159;&#20998;&#21035;&#21457;&#23637;&#30340;&#65292;&#21069;&#32773;&#22522;&#20110;&#31616;&#21270;&#30340;&#20809;&#26629;&#26041;&#27861;&#65292;&#21518;&#32773;&#20551;&#23450;&#23436;&#25972;&#30340;&#29615;&#22659;&#21487;&#35266;&#23519;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22330;&#26223;&#36890;&#30693;&#32773;&#65292;&#36825;&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#35266;&#23519;&#20195;&#29702;&#30340;&#36712;&#36857;&#21644;&#25512;&#26029;&#36974;&#25377;&#12290;&#23427;&#20351;&#29992;&#19968;&#20010;transformer&#26469;&#32858;&#21512;&#21508;&#31181;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#20419;&#36827;&#23545;&#21487;&#33021;&#19982;AV&#35745;&#21010;&#36335;&#24452;&#30456;&#20132;&#30340;&#36974;&#25377;&#30340;&#36873;&#25321;&#24615;&#26597;&#35810;&#12290;&#35813;&#26694;&#26550;&#20272;&#35745;&#20102;&#36974;&#25377;&#27010;&#29575;&#21644;&#21487;&#33021;&#30340;&#36974;&#25377;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13893v2 Announce Type: replace-cross  Abstract: Navigating complex and dynamic environments requires autonomous vehicles (AVs) to reason about both visible and occluded regions. This involves predicting the future motion of observed agents, inferring occluded ones, and modeling their interactions based on vectorized scene representations of the partially observable environment. However, prior work on occlusion inference and trajectory prediction have developed in isolation, with the former based on simplified rasterized methods and the latter assuming full environment observability. We introduce the Scene Informer, a unified approach for predicting both observed agent trajectories and inferring occlusions in a partially observable setting. It uses a transformer to aggregate various input modalities and facilitate selective queries on occlusions that might intersect with the AV's planned path. The framework estimates occupancy probabilities and likely trajectories for occlusi
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#24605;&#32771;&#31227;&#21160;AI&#29983;&#24577;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#20316;&#31649;&#29702;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;NPU&#20869;&#37096;&#25918;&#32622;&#19981;&#21463;&#24212;&#29992;&#25110;&#25805;&#20316;&#31995;&#32479;&#20462;&#35746;&#24433;&#21709;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#21450;&#27599;&#20010;&#24212;&#29992;&#36129;&#29486;&#29305;&#23450;&#30340;&#36866;&#37197;&#22120;&#65292;&#20026;&#24191;&#27867;&#31227;&#21160;AI&#20219;&#21153;&#25552;&#20379;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2308.14363</link><description>&lt;p&gt;
&#22312;LLM&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#31227;&#21160;AI&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Rethinking Mobile AI Ecosystem in the LLM Era
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.14363
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#31227;&#21160;AI&#29983;&#24577;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#20316;&#31649;&#29702;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;NPU&#20869;&#37096;&#25918;&#32622;&#19981;&#21463;&#24212;&#29992;&#25110;&#25805;&#20316;&#31995;&#32479;&#20462;&#35746;&#24433;&#21709;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#21450;&#27599;&#20010;&#24212;&#29992;&#36129;&#29486;&#29305;&#23450;&#30340;&#36866;&#37197;&#22120;&#65292;&#20026;&#24191;&#27867;&#31227;&#21160;AI&#20219;&#21153;&#25552;&#20379;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20170;&#22825;&#30340;&#32972;&#26223;&#19979;&#65292;&#26234;&#33021;&#25163;&#26426;&#24050;&#32463;&#28436;&#21464;&#25104;&#20102;&#25176;&#31649;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20013;&#24515;&#65292;&#26088;&#22312;&#36827;&#34892;&#26412;&#22320;&#25191;&#34892;&#12290;&#25512;&#21160;&#36825;&#39033;&#24037;&#20316;&#30340;&#19968;&#20010;&#20851;&#38190;&#24847;&#35782;&#26159;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#20998;&#25955;&#24615;&#65292;&#20854;&#29305;&#28857;&#26159;&#19981;&#21516;&#30340;&#26550;&#26500;&#12289;&#36816;&#31639;&#31526;&#21644;&#23454;&#29616;&#12290;&#36825;&#31181;&#20998;&#25955;&#24615;&#32473;&#30828;&#20214;&#12289;&#31995;&#32479;&#35774;&#32622;&#21644;&#31639;&#27861;&#30340;&#20840;&#38754;&#20248;&#21270;&#24102;&#26469;&#20102;&#37325;&#22823;&#36127;&#25285;&#12290;&#22312;&#26368;&#36817;&#30340;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#30340;&#25512;&#21160;&#19979;&#65292;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#31227;&#21160;AI&#30340;&#24320;&#21019;&#24615;&#33539;&#24335;&#65306;&#31227;&#21160;&#25805;&#20316;&#31995;&#32479;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#21327;&#20316;&#31649;&#29702;&#26041;&#27861;&#65292;&#30417;&#30563;&#20855;&#26377;&#20026;&#24191;&#27867;&#31227;&#21160;AI&#20219;&#21153;&#25552;&#20379;&#26381;&#21153;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21363;&#20351;&#36824;&#19981;&#33021;&#20026;&#25152;&#26377;&#20219;&#21153;&#25552;&#20379;&#26381;&#21153;&#12290;&#36825;&#20010;&#22522;&#30784;&#27169;&#22411;&#39547;&#30041;&#22312;NPU&#20869;&#37096;&#65292;&#31867;&#20284;&#20110;&#22266;&#20214;&#65292;&#19981;&#21463;&#24212;&#29992;&#25110;&#25805;&#20316;&#31995;&#32479;&#30340;&#20462;&#35746;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#27599;&#20010;&#24212;&#29992;&#31243;&#24207;&#37117;&#20250;&#36129;&#29486;&#19968;&#20010;&#31616;&#27905;&#30340;&#12289;&#31163;&#32447;&#24494;&#35843;&#30340;&#8220;&#36866;&#37197;&#22120;&#8221;&#65292;&#29992;&#20110;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.14363v2 Announce Type: replace  Abstract: In today's landscape, smartphones have evolved into hubs for hosting a multitude of deep learning models aimed at local execution. A key realization driving this work is the notable fragmentation among these models, characterized by varied architectures, operators, and implementations. This fragmentation imposes a significant burden on the comprehensive optimization of hardware, system settings, and algorithms.   Buoyed by the recent strides in large foundation models, this work introduces a pioneering paradigm for mobile AI: a collaborative management approach between the mobile OS and hardware, overseeing a foundational model capable of serving a broad spectrum of mobile AI tasks, if not all. This foundational model resides within the NPU and remains impervious to app or OS revisions, akin to firmware. Concurrently, each app contributes a concise, offline fine-tuned "adapter" tailored to distinct downstream tasks. From this concept
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;Camouflageator&#29983;&#25104;&#26356;&#38590;&#20197;&#34987;&#26816;&#27979;&#21040;&#30340;&#20266;&#35013;&#23545;&#35937;&#65292;&#20174;&#32780;&#22686;&#24378;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#22120;&#22312;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#30340;&#31934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2308.03166</link><description>&lt;p&gt;
&#25112;&#30053;&#24615;&#29454;&#29289;&#20351;&#29312;&#21033;&#30340;&#25429;&#39135;&#32773;&#65306;&#36890;&#36807;&#29983;&#25104;&#20266;&#35013;&#29289;&#20307;&#26469;&#22686;&#24378;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Strategic Preys Make Acute Predators: Enhancing Camouflaged Object Detectors by Generating Camouflaged Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.03166
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;Camouflageator&#29983;&#25104;&#26356;&#38590;&#20197;&#34987;&#26816;&#27979;&#21040;&#30340;&#20266;&#35013;&#23545;&#35937;&#65292;&#20174;&#32780;&#22686;&#24378;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#22120;&#22312;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#30340;&#31934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#65288;COD&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#35270;&#35273;&#19978;&#34701;&#20837;&#29615;&#22659;&#20013;&#30340;&#20266;&#35013;&#29289;&#20307;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#29616;&#26377;&#30340;COD&#26816;&#27979;&#22120;&#22312;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#38590;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#29454;&#29289;&#19982;&#25429;&#39135;&#32773;&#30340;&#21338;&#24328;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#36825;&#23548;&#33268;&#29454;&#29289;&#21457;&#23637;&#20986;&#26356;&#22909;&#30340;&#20266;&#35013;&#33021;&#21147;&#65292;&#32780;&#25429;&#39135;&#32773;&#21017;&#33719;&#24471;&#26356;&#25935;&#38160;&#30340;&#35270;&#35273;&#31995;&#32479;&#65292;&#24182;&#20174;&#29454;&#29289;&#26041;&#21644;&#25429;&#39135;&#32773;&#26041;&#24320;&#21457;&#31639;&#27861;&#12290;&#22312;&#29454;&#29289;&#19968;&#20391;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#65292;Camouflageator&#65292;&#24341;&#20837;&#19968;&#20010;&#36741;&#21161;&#29983;&#25104;&#22120;&#26469;&#29983;&#25104;&#26356;&#38590;&#20197;&#34987;COD&#26041;&#27861;&#26816;&#27979;&#21040;&#30340;&#20266;&#35013;&#23545;&#35937;&#12290;Camouflageator&#20197;&#23545;&#25239;&#30340;&#26041;&#24335;&#35757;&#32451;&#29983;&#25104;&#22120;&#21644;&#26816;&#27979;&#22120;&#65292;&#20351;&#22686;&#24378;&#30340;&#36741;&#21161;&#29983;&#25104;&#22120;&#26377;&#21161;&#20110;&#20135;&#29983;&#26356;&#24378;&#22823;&#30340;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.03166v2 Announce Type: replace-cross  Abstract: Camouflaged object detection (COD) is the challenging task of identifying camouflaged objects visually blended into surroundings. Albeit achieving remarkable success, existing COD detectors still struggle to obtain precise results in some challenging cases. To handle this problem, we draw inspiration from the prey-vs-predator game that leads preys to develop better camouflage and predators to acquire more acute vision systems and develop algorithms from both the prey side and the predator side. On the prey side, we propose an adversarial training framework, Camouflageator, which introduces an auxiliary generator to generate more camouflaged objects that are harder for a COD method to detect. Camouflageator trains the generator and detector in an adversarial way such that the enhanced auxiliary generator helps produce a stronger detector. On the predator side, we introduce a novel COD method, called Internal Coherence and Edge G
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#21152;&#26435;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20020;&#24202;&#38754;&#35848;&#36716;&#24405;&#20013;&#26816;&#27979;&#25233;&#37057;&#30151;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2307.00920</link><description>&lt;p&gt;
&#22522;&#20110;&#33410;&#28857;&#21152;&#26435;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20998;&#26512;&#36716;&#24405;&#30340;&#20020;&#24202;&#38754;&#35848;&#20013;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Node-weighted Graph Convolutional Network for Depression Detection in Transcribed Clinical Interviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00920
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#21152;&#26435;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20020;&#24202;&#38754;&#35848;&#36716;&#24405;&#20013;&#26816;&#27979;&#25233;&#37057;&#30151;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#20013;&#21152;&#26435;&#33258;&#36830;&#25509;&#36793;&#65292;&#24182;&#23637;&#31034;&#20854;&#23545;&#20174;&#36716;&#24405;&#30340;&#20020;&#24202;&#38754;&#35848;&#20013;&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;GCN&#26469;&#23545;&#36716;&#24405;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#23558;&#20854;&#20998;&#31867;&#20026;&#25233;&#37057;&#30151;&#24739;&#32773;&#25110;&#23545;&#29031;&#32452;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#20943;&#36731;GCN&#20013;&#23545;&#23616;&#37096;&#24615;&#21644;&#33258;&#36830;&#25509;&#19982;&#30456;&#37051;&#33410;&#28857;&#36793;&#30340;&#31561;&#37325;&#35201;&#24615;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#35832;&#22914;&#20302;&#35745;&#31639;&#25104;&#26412;&#12289;&#25968;&#25454;&#19981;&#21487;&#30693;&#21644;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#31561;&#26377;&#21560;&#24341;&#21147;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#20934;GCN&#27169;&#22411;&#20197;&#21450;&#20808;&#21069;&#25253;&#21578;&#30340;&#32467;&#26524;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#36798;&#21040;&#20102;F1=0.84&#12290;&#26368;&#21518;&#65292;&#23450;&#24615;&#20998;&#26512;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00920v2 Announce Type: replace-cross  Abstract: We propose a simple approach for weighting self-connecting edges in a Graph Convolutional Network (GCN) and show its impact on depression detection from transcribed clinical interviews. To this end, we use a GCN for modeling non-consecutive and long-distance semantics to classify the transcriptions into depressed or control subjects. The proposed method aims to mitigate the limiting assumptions of locality and the equal importance of self-connections vs. edges to neighboring nodes in GCNs, while preserving attractive features such as low computational cost, data agnostic, and interpretability capabilities. We perform an exhaustive evaluation in two benchmark datasets. Results show that our approach consistently outperforms the vanilla GCN model as well as previously reported results, achieving an F1=0.84 on both datasets. Finally, a qualitative analysis illustrates the interpretability capabilities of the proposed approach and 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#21253;&#25324;&#28857;&#23545;&#28857;&#25237;&#31080;&#26426;&#21046;&#21644;&#22870;&#21169;&#21644;&#24809;&#32602;&#26426;&#21046;&#65292;&#20197;&#26816;&#27979;&#21644;&#38459;&#27490;&#24694;&#24847;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#23545;&#25239;&#24694;&#24847;&#23458;&#25143;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2307.00543</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22359;&#38142;&#38450;&#24481;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24694;&#24847;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Defending Against Malicious Behaviors in Federated Learning with Blockchain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00543
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#21253;&#25324;&#28857;&#23545;&#28857;&#25237;&#31080;&#26426;&#21046;&#21644;&#22870;&#21169;&#21644;&#24809;&#32602;&#26426;&#21046;&#65292;&#20197;&#26816;&#27979;&#21644;&#38459;&#27490;&#24694;&#24847;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#23545;&#25239;&#24694;&#24847;&#23458;&#25143;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#65292;&#32852;&#37030;&#23398;&#20064;(FL)&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#22810;&#23478;&#26426;&#26500;&#25968;&#25454;&#25152;&#26377;&#32773;&#25110;&#23458;&#25143;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FL&#26041;&#27861;&#20381;&#36182;&#20110;&#29992;&#20110;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#30340;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#65292;&#23548;&#33268;&#21333;&#28857;&#25925;&#38556;&#12290;&#36825;&#20351;&#31995;&#32479;&#22312;&#22788;&#29702;&#19981;&#35802;&#23454;&#30340;&#23458;&#25143;&#26102;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#23433;&#20840;&#21487;&#38752;FL&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#32467;&#21512;&#20102;&#28857;&#23545;&#28857;&#25237;&#31080;&#26426;&#21046;&#21644;&#22870;&#21169;&#21644;&#24809;&#32602;&#26426;&#21046;&#65292;&#30001;&#38142;&#19978;&#26234;&#33021;&#21512;&#32422;&#25552;&#20379;&#21160;&#21147;&#65292;&#20197;&#26816;&#27979;&#21644;&#38459;&#27490;&#24694;&#24847;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#23637;&#31034;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;&#24694;&#24847;&#23458;&#25143;&#26159;&#24378;&#22823;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00543v2 Announce Type: replace-cross  Abstract: In the era of deep learning, federated learning (FL) presents a promising approach that allows multi-institutional data owners, or clients, to collaboratively train machine learning models without compromising data privacy. However, most existing FL approaches rely on a centralized server for global model aggregation, leading to a single point of failure. This makes the system vulnerable to malicious attacks when dealing with dishonest clients. In this work, we address this problem by proposing a secure and reliable FL system based on blockchain and distributed ledger technology. Our system incorporates a peer-to-peer voting mechanism and a reward-and-slash mechanism, which are powered by on-chain smart contracts, to detect and deter malicious behaviors. Both theoretical and empirical analyses are presented to demonstrate the effectiveness of the proposed approach, showing that our framework is robust against malicious client-s
&lt;/p&gt;</description></item><item><title>TransERR&#26159;&#19968;&#31181;&#22522;&#20110;&#32763;&#35793;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#37319;&#29992;&#36229;&#22797;&#20540;&#31354;&#38388;&#32534;&#30721;&#30693;&#35782;&#22270;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#36890;&#36807;&#36866;&#24212;&#24615;&#26059;&#36716;&#22836;&#23454;&#20307;&#21644;&#23614;&#23454;&#20307;&#26469;&#26368;&#23567;&#21270;&#32763;&#35793;&#36317;&#31163;&#65292;&#24182;&#20855;&#26377;&#26377;&#25928;&#24314;&#27169;&#19981;&#21516;&#20851;&#31995;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2306.14580</link><description>&lt;p&gt;
TransERR:&#36890;&#36807;&#39640;&#25928;&#20851;&#31995;&#26059;&#36716;&#30340;&#22522;&#20110;&#32763;&#35793;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TransERR: Translation-based Knowledge Graph Embedding via Efficient Relation Rotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.14580
&lt;/p&gt;
&lt;p&gt;
TransERR&#26159;&#19968;&#31181;&#22522;&#20110;&#32763;&#35793;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#37319;&#29992;&#36229;&#22797;&#20540;&#31354;&#38388;&#32534;&#30721;&#30693;&#35782;&#22270;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#36890;&#36807;&#36866;&#24212;&#24615;&#26059;&#36716;&#22836;&#23454;&#20307;&#21644;&#23614;&#23454;&#20307;&#26469;&#26368;&#23567;&#21270;&#32763;&#35793;&#36317;&#31163;&#65292;&#24182;&#20855;&#26377;&#26377;&#25928;&#24314;&#27169;&#19981;&#21516;&#20851;&#31995;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#20851;&#31995;&#26059;&#36716;&#65288;TransERR&#65289;&#23454;&#29616;&#30340;&#22522;&#20110;&#32763;&#35793;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#36825;&#26159;&#20256;&#32479;&#22522;&#20110;&#32763;&#35793;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#30340;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#19982;&#20808;&#21069;&#30340;&#22522;&#20110;&#32763;&#35793;&#27169;&#22411;&#19981;&#21516;&#65292;TransERR&#22312;&#36229;&#22797;&#20540;&#31354;&#38388;&#20013;&#23545;&#30693;&#35782;&#22270;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#22312;&#25366;&#25496;&#22836;&#37096;&#21644;&#23614;&#37096;&#23454;&#20307;&#20043;&#38388;&#30340;&#28508;&#22312;&#20449;&#24687;&#26102;&#20855;&#26377;&#26356;&#39640;&#31243;&#24230;&#30340;&#32763;&#35793;&#33258;&#30001;&#24230;&#12290;&#20026;&#36827;&#19968;&#27493;&#20943;&#23567;&#32763;&#35793;&#36317;&#31163;&#65292;TransERR&#36890;&#36807;&#23427;&#20204;&#21508;&#33258;&#21487;&#23398;&#20064;&#30340;&#21333;&#20301;&#22235;&#20803;&#25968;&#36866;&#24212;&#24615;&#22320;&#26059;&#36716;&#22836;&#23454;&#20307;&#21644;&#23614;&#23454;&#20307;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#25968;&#23398;&#35777;&#26126;&#26469;&#23637;&#31034;TransERR&#22312;&#24314;&#27169;&#21508;&#31181;&#20851;&#31995;&#27169;&#24335;&#65288;&#21253;&#25324;&#23545;&#31216;&#24615;&#12289;&#21453;&#23545;&#31216;&#24615;&#12289;&#20498;&#32622;&#12289;&#21512;&#25104;&#21644;&#23376;&#20851;&#31995;&#27169;&#24335;&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23545;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.14580v2 Announce Type: replace-cross  Abstract: This paper presents a translation-based knowledge geraph embedding method via efficient relation rotation (TransERR), a straightforward yet effective alternative to traditional translation-based knowledge graph embedding models. Different from the previous translation-based models, TransERR encodes knowledge graphs in the hypercomplex-valued space, thus enabling it to possess a higher degree of translation freedom in mining latent information between the head and tail entities. To further minimize the translation distance, TransERR adaptively rotates the head entity and the tail entity with their corresponding unit quaternions, which are learnable in model training. We also provide mathematical proofs to demonstrate the ability of TransERR in modeling various relation patterns, including symmetry, antisymmetry, inversion, composition, and subrelation patterns. The experiments on 10 benchmark datasets validate the effectiveness 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2306.02090</link><description>&lt;p&gt;
&#27809;&#26377;&#25968;&#25454;&#35775;&#38382;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Deep Classifier Mimicry without Data Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02090
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#35775;&#38382;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26631;&#20934;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21487;&#33021;&#26080;&#27861;&#31561;&#21516;&#22320;&#33719;&#24471;&#27169;&#22411;&#35757;&#32451;&#25152;&#38656;&#30340;&#21407;&#22987;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#24494;&#35843;&#12289;&#21387;&#32553;&#27169;&#22411;&#12289;&#25345;&#32493;&#35843;&#25972;&#25110;&#36827;&#34892;&#20219;&#20309;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26356;&#26032;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#21487;&#33021;&#26080;&#38656;&#21407;&#22987;&#25968;&#25454;&#35775;&#38382;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25512;&#29702;&#30693;&#35782;&#25552;&#21462;&#65288;CAKE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#32780;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;CAKE&#29983;&#25104;&#19968;&#23545;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#23558;&#23427;&#20204;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#26550;&#26500;&#36873;&#25321;&#22312;&#23454;&#35777;&#19978;&#35777;&#23454;&#20102;CAKE&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.02090v2 Announce Type: replace-cross  Abstract: Access to pre-trained models has recently emerged as a standard across numerous machine learning domains. Unfortunately, access to the original data the models were trained on may not equally be granted. This makes it tremendously challenging to fine-tune, compress models, adapt continually, or to do any other type of data-driven update. We posit that original data access may however not be required. Specifically, we propose Contrastive Abductive Knowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure that mimics deep classifiers without access to the original data. To this end, CAKE generates pairs of noisy synthetic samples and diffuses them contrastively toward a model's decision boundary. We empirically corroborate CAKE's effectiveness using several benchmark datasets and various architectural choices, paving the way for broad application.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22806;&#24863;&#30693;&#20256;&#24863;&#21644;&#20013;&#26530;&#27169;&#24335;&#21457;&#29983;&#22120;&#25972;&#21512;&#21040;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#23398;&#20064;&#35270;&#35273;&#24341;&#23548;&#30340;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#65292;&#25506;&#32034;&#20102;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#23545;&#23548;&#33322;&#40065;&#26834;&#24615;&#30340;&#25913;&#36827;&#12289;&#20855;&#26377;&#35760;&#24518;&#21151;&#33021;&#30340;&#31574;&#30053;&#32593;&#32476;&#19982;&#26080;&#35760;&#24518;&#31574;&#30053;&#32593;&#32476;&#22312;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#20197;&#21450;&#21160;&#29289;&#22914;&#20309;&#23481;&#24525;&#39640;&#12290;</title><link>https://arxiv.org/abs/2212.14400</link><description>&lt;p&gt;
Visual CPG-RL&#65306;&#23398;&#20064;&#29992;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#30340;&#20013;&#26530;&#27169;&#24335;&#21457;&#29983;&#22120;
&lt;/p&gt;
&lt;p&gt;
Visual CPG-RL: Learning Central Pattern Generators for Visually-Guided Quadruped Locomotion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.14400
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22806;&#24863;&#30693;&#20256;&#24863;&#21644;&#20013;&#26530;&#27169;&#24335;&#21457;&#29983;&#22120;&#25972;&#21512;&#21040;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#23398;&#20064;&#35270;&#35273;&#24341;&#23548;&#30340;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#65292;&#25506;&#32034;&#20102;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#23545;&#23548;&#33322;&#40065;&#26834;&#24615;&#30340;&#25913;&#36827;&#12289;&#20855;&#26377;&#35760;&#24518;&#21151;&#33021;&#30340;&#31574;&#30053;&#32593;&#32476;&#19982;&#26080;&#35760;&#24518;&#31574;&#30053;&#32593;&#32476;&#22312;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#20197;&#21450;&#21160;&#29289;&#22914;&#20309;&#23481;&#24525;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22806;&#24863;&#30693;&#20256;&#24863;&#21644;&#20013;&#26530;&#27169;&#24335;&#21457;&#29983;&#22120;&#65288;CPGs&#65292;&#21363;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#65289;&#25972;&#21512;&#21040;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26694;&#26550;&#20013;&#65292;&#23398;&#20064;&#35270;&#35273;&#24341;&#23548;&#30340;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#12290;&#36890;&#36807;&#22806;&#24863;&#30693;&#21644;&#26412;&#20307;&#24863;&#30693;&#65292;&#20195;&#29702;&#23398;&#20064;&#21327;&#35843;&#19981;&#21516;&#25391;&#33633;&#22120;&#20043;&#38388;&#30340;&#33410;&#24459;&#34892;&#20026;&#65292;&#20197;&#36319;&#36394;&#36895;&#24230;&#25351;&#20196;&#65292;&#21516;&#26102;&#35206;&#30422;&#36825;&#20123;&#25351;&#20196;&#20197;&#36991;&#20813;&#19982;&#29615;&#22659;&#30896;&#25758;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20960;&#20010;&#24320;&#25918;&#30340;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#31070;&#32463;&#31185;&#23398;&#38382;&#39064;&#65306;1&#65289;&#25391;&#33633;&#22120;&#20043;&#38388;&#30340;&#26174;&#24335;&#30456;&#20114;&#32806;&#21512;&#30340;&#20316;&#29992;&#26159;&#20160;&#20040;&#65292;&#36825;&#31181;&#32806;&#21512;&#26159;&#21542;&#33021;&#25552;&#39640;&#29992;&#20110;&#23548;&#33322;&#40065;&#26834;&#24615;&#30340;&#20174;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#36716;&#31227;&#65311;2&#65289;&#20351;&#29992;&#20855;&#26377;&#35760;&#24518;&#21151;&#33021;&#19982;&#26080;&#35760;&#24518;&#31574;&#30053;&#32593;&#32476;&#23545;&#20110;&#40065;&#26834;&#24615;&#12289;&#33021;&#25928;&#21644;&#20174;&#27169;&#25311;&#21040;&#30495;&#23454;&#23548;&#33322;&#20219;&#21153;&#30340;&#36319;&#36394;&#24615;&#33021;&#26377;&#20160;&#20040;&#24433;&#21709;&#65311;3&#65289;&#21160;&#29289;&#26159;&#22914;&#20309;&#23481;&#24525;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.14400v2 Announce Type: replace-cross  Abstract: We present a framework for learning visually-guided quadruped locomotion by integrating exteroceptive sensing and central pattern generators (CPGs), i.e. systems of coupled oscillators, into the deep reinforcement learning (DRL) framework. Through both exteroceptive and proprioceptive sensing, the agent learns to coordinate rhythmic behavior among different oscillators to track velocity commands, while at the same time override these commands to avoid collisions with the environment. We investigate several open robotics and neuroscience questions: 1) What is the role of explicit interoscillator couplings between oscillators, and can such coupling improve sim-to-real transfer for navigation robustness? 2) What are the effects of using a memory-enabled vs. a memory-free policy network with respect to robustness, energy-efficiency, and tracking performance in sim-to-real navigation tasks? 3) How do animals manage to tolerate high 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#28789;&#27963;&#27169;&#22411;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20116;&#20010;&#19981;&#21516;&#26041;&#38754;&#30340;&#20154;&#31867;&#27425;&#29702;&#24615;&#65292;&#22312;&#37329;&#34701;&#24066;&#22330;&#27169;&#25311;&#20013;&#20934;&#30830;&#37325;&#29616;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2210.08569</link><description>&lt;p&gt;
&#26377;&#38480;&#25110;&#26377;&#20559;&#35265;: &#22312;&#37329;&#34701;&#24066;&#22330;&#20013;&#24314;&#27169;&#27425;&#29702;&#24615;&#20154;&#31867;&#25237;&#36164;&#32773;
&lt;/p&gt;
&lt;p&gt;
Limited or Biased: Modeling Sub-Rational Human Investors in Financial Markets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.08569
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#28789;&#27963;&#27169;&#22411;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20116;&#20010;&#19981;&#21516;&#26041;&#38754;&#30340;&#20154;&#31867;&#27425;&#29702;&#24615;&#65292;&#22312;&#37329;&#34701;&#24066;&#22330;&#27169;&#25311;&#20013;&#20934;&#30830;&#37325;&#29616;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#20154;&#31867;&#30340;&#20915;&#31574;&#19982;&#23436;&#20840;&#29702;&#24615;&#20010;&#20307;&#20570;&#20986;&#30340;&#26368;&#20339;&#20915;&#31574;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35745;&#31639;&#38480;&#21046;&#25110;&#24515;&#29702;&#20559;&#35265;&#12290;&#23613;&#31649;&#34892;&#20026;&#37329;&#34701;&#39046;&#22495;&#30340;&#29616;&#26377;&#30740;&#31350;&#21457;&#29616;&#20102;&#20154;&#31867;&#27425;&#29702;&#24615;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20294;&#32570;&#20047;&#23558;&#36825;&#20123;&#21457;&#29616;&#36716;&#21270;&#20026;&#36866;&#29992;&#20110;&#19981;&#21516;&#37329;&#34701;&#24066;&#22330;&#22330;&#26223;&#30340;&#33258;&#36866;&#24212;&#20154;&#31867;&#27169;&#22411;&#30340;&#32508;&#21512;&#26694;&#26550;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20116;&#20010;&#19981;&#21516;&#26041;&#38754;&#30340;&#20154;&#31867;&#27425;&#29702;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#39640;&#20445;&#30495;&#24230;&#30340;&#22810;&#26234;&#33021;&#20307;&#24066;&#22330;&#27169;&#25311;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;&#32570;&#20047;&#20010;&#20307;&#25237;&#36164;&#32773;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#25163;&#24037;&#21046;&#20316;&#30340;&#24066;&#22330;&#22330;&#26223;&#21644;SHAP&#20540;&#20998;&#26512;&#35780;&#20272;&#20102;&#27425;&#29702;&#24615;&#20154;&#31867;&#25237;&#36164;&#32773;&#30340;&#34892;&#20026;&#65292;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#27169;&#22411;&#20934;&#30830;&#22320;&#37325;&#29616;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.08569v2 Announce Type: replace  Abstract: Human decision-making in real-life deviates significantly from the optimal decisions made by fully rational agents, primarily due to computational limitations or psychological biases. While existing studies in behavioral finance have discovered various aspects of human sub-rationality, there lacks a comprehensive framework to transfer these findings into an adaptive human model applicable across diverse financial market scenarios. In this study, we introduce a flexible model that incorporates five different aspects of human sub-rationality using reinforcement learning. Our model is trained using a high-fidelity multi-agent market simulator, which overcomes limitations associated with the scarcity of labeled data of individual investors. We evaluate the behavior of sub-rational human investors using hand-crafted market scenarios and SHAP value analysis, showing that our model accurately reproduces the observations in the previous stud
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#35838;&#31243;&#20998;&#37197;&#26426;&#21046;&#65288;MLCM&#65289;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22359;&#20943;&#36731;&#23398;&#29983;&#22312;&#25253;&#21578;&#20559;&#22909;&#26102;&#30340;&#38169;&#35823;&#65292;&#26174;&#33879;&#25552;&#39640;&#23398;&#29983;&#25928;&#29992;&#65292;&#19988;&#20855;&#26377;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2210.00954</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#35838;&#31243;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Powered Course Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.00954
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#35838;&#31243;&#20998;&#37197;&#26426;&#21046;&#65288;MLCM&#65289;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22359;&#20943;&#36731;&#23398;&#29983;&#22312;&#25253;&#21578;&#20559;&#22909;&#26102;&#30340;&#38169;&#35823;&#65292;&#26174;&#33879;&#25552;&#39640;&#23398;&#29983;&#25928;&#29992;&#65292;&#19988;&#20855;&#26377;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35838;&#31243;&#20998;&#37197;&#38382;&#39064;&#65292;&#21363;&#22823;&#23398;&#20026;&#23398;&#29983;&#23433;&#25490;&#35838;&#31243;&#26102;&#38388;&#34920;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#21046;Course Match&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#23398;&#29983;&#22312;&#25253;&#21578;&#20182;&#20204;&#30340;&#20559;&#22909;&#26102;&#20250;&#29359;&#24456;&#22823;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#23545;&#31119;&#21033;&#21644;&#20844;&#24179;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26426;&#21046;&#65292;&#21363;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;Course Match&#65288;MLCM&#65289;&#12290;MLCM&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20559;&#22909;&#24341;&#23548;&#27169;&#22359;&#65292;&#36890;&#36807;&#36845;&#20195;&#24335;&#22320;&#21521;&#23398;&#29983;&#25552;&#20986;&#20010;&#24615;&#21270;&#30340;&#20004;&#20004;&#27604;&#36739;&#26597;&#35810;&#65292;&#20197;&#20943;&#36731;&#23398;&#29983;&#30340;&#25253;&#21578;&#38169;&#35823;&#12290;&#22823;&#37327;&#22522;&#20110;&#30495;&#23454;&#25968;&#25454;&#30340;&#35745;&#31639;&#23454;&#39564;&#34920;&#26126;&#65292;MLCM&#20165;&#38656;&#21313;&#20010;&#27604;&#36739;&#26597;&#35810;&#65292;&#23601;&#33021;&#23558;&#24179;&#22343;&#21644;&#26368;&#23567;&#23398;&#29983;&#25928;&#29992;&#20998;&#21035;&#25552;&#39640;7%-11%&#21644;17%-29%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;MLCM&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#22914;&#20309;&#26368;&#23567;&#21270;&#21319;&#32423;&#33267;MLCM &#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.00954v3 Announce Type: replace-cross  Abstract: We study the course allocation problem, where universities assign course schedules to students. The current state-of-the-art mechanism, Course Match, has one major shortcoming: students make significant mistakes when reporting their preferences, which negatively affects welfare and fairness. To address this issue, we introduce a new mechanism, Machine Learning-powered Course Match (MLCM). At the core of MLCM is a machine learning-powered preference elicitation module that iteratively asks personalized pairwise comparison queries to alleviate students' reporting mistakes. Extensive computational experiments, grounded in real-world data, demonstrate that MLCM, with only ten comparison queries, significantly increases both average and minimum student utility by 7%-11% and 17%-29%, respectively. Finally, we highlight MLCM's robustness to changes in the environment and show how our design minimizes the risk of upgrading to MLCM whil
&lt;/p&gt;</description></item><item><title>OpenXAI &#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#12289;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#25143;&#21487;&#36731;&#26494;&#25193;&#23637;&#21644;&#27604;&#36739;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2206.11104</link><description>&lt;p&gt;
OpenXAI: &#36808;&#21521;&#36879;&#26126;&#35780;&#20272;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
OpenXAI: Towards a Transparent Evaluation of Model Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.11104
&lt;/p&gt;
&lt;p&gt;
OpenXAI &#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#12289;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#25143;&#21487;&#36731;&#26494;&#25193;&#23637;&#21644;&#27604;&#36739;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#36817;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#65292;&#20294;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#24615;&#22522;&#20934;&#27979;&#35797;&#30340;&#24037;&#20316;&#38750;&#24120;&#23569;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OpenXAI&#65292;&#19968;&#20010;&#20840;&#38754;&#19988;&#21487;&#25193;&#23637;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#12290;OpenXAI&#21253;&#25324;&#20197;&#19979;&#20851;&#38190;&#32452;&#20214;&#65306;&#65288;i&#65289;&#28789;&#27963;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#21644;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#29305;&#24449;&#24402;&#23646;&#26041;&#27861;&#30340;&#38598;&#21512;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#26041;&#27861;&#24544;&#23454;&#24230;&#12289;&#31283;&#23450;&#24615;&#65288;&#40065;&#26834;&#24615;&#65289;&#21644;&#20844;&#24179;&#24615;&#30340;&#21313;&#19968;&#31181;&#37327;&#21270;&#24230;&#37327;&#26631;&#20934;&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#23545;&#22810;&#31181;&#24230;&#37327;&#26631;&#20934;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#20960;&#31181;&#35299;&#37322;&#26041;&#27861;&#30340;&#27604;&#36739;&#12290;OpenXAI&#26131;&#20110;&#25193;&#23637;&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#35780;&#20272;&#33258;&#23450;&#20041;&#35299;&#37322;&#26041;&#27861;&#24182;&#23558;&#20854;&#32435;&#20837;&#25105;&#20204;&#30340;&#25490;&#34892;&#27036;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.11104v4 Announce Type: replace-cross  Abstract: While several types of post hoc explanation methods have been proposed in recent literature, there is very little work on systematically benchmarking these methods. Here, we introduce OpenXAI, a comprehensive and extensible open-source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, and (ii) open-source implementations of eleven quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, in turn providing comparisons of several explanation methods across a wide variety of metrics, models, and datasets. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33033;&#20914;Q&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;DSQN&#21033;&#29992;&#38750;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#21387;&#20316;&#20026;Q&#20540;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#33021;&#28304;&#39640;&#25928;&#30340;&#25511;&#21046;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2201.09754</link><description>&lt;p&gt;
&#24102;&#27874;&#33033;&#20914;Q&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Spiking Q-learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.09754
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33033;&#20914;Q&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;DSQN&#21033;&#29992;&#38750;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#21387;&#20316;&#20026;Q&#20540;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#33021;&#28304;&#39640;&#25928;&#30340;&#25511;&#21046;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#29305;&#27530;&#30340;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#65292;&#26399;&#26395;&#36890;&#36807;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#20197;&#26356;&#23569;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#36890;&#36807;&#23558;SNNs&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#65292;&#20026;&#23454;&#29616;&#29616;&#23454;&#25511;&#21046;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#39640;&#25928;&#33021;&#28304;&#26041;&#24335;&#12290;&#30446;&#21069;&#20165;&#26377;&#23569;&#25968;&#22522;&#20110;SNN&#30340;RL&#26041;&#27861;&#12290;&#20854;&#20013;&#22823;&#37096;&#20998;&#35201;&#20040;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#35201;&#20040;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#26469;&#20272;&#31639;&#20540;&#20989;&#25968;&#12290;&#21069;&#32773;&#38656;&#35201;&#20026;&#27599;&#20010;&#22330;&#26223;&#35843;&#25972;&#22823;&#37327;&#36229;&#21442;&#25968;&#65292;&#32780;&#21518;&#32773;&#38480;&#21046;&#20102;&#19981;&#21516;&#31867;&#22411;RL&#31639;&#27861;&#30340;&#24212;&#29992;&#24182;&#24573;&#30053;&#20102;&#35757;&#32451;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#36739;&#22823;&#12290;&#20026;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;RL&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#26118;&#34411;&#20013;&#21457;&#29616;&#30340;&#38750;&#33033;&#20914;&#38388;&#31070;&#32463;&#20803;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#28145;&#24230;&#33033;&#20914;Q&#32593;&#32476;&#65288;DSQN&#65289;&#65292;&#20351;&#29992;&#38750;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#21387;&#20316;&#20026;Q&#20540;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#21487;&#20197;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.09754v2 Announce Type: replace-cross  Abstract: With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence (AI) with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combining SNNs with deep reinforcement learning (RL). There are only a few existing SNN-based RL methods at present. Most of them either lack generalization ability or employ Artificial Neural Networks (ANNs) to estimate value function in training. The former needs to tune numerous hyper-parameters for each scenario, and the latter limits the application of different types of RL algorithm and ignores the large energy consumption in training. To develop a robust spike-based RL method, we draw inspiration from non-spiking interneurons found in insects and propose the deep spiking Q-network (DSQN), using the membrane voltage of non-spiking neurons as the representation of Q-value, which can direct
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#34920;&#29616;&#20986;&#21487;&#35266;&#30340;&#24615;&#33021;&#65292;&#20294;&#24448;&#24448;&#26080;&#27861;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#36328;&#39033;&#30446;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#21407;&#22240;&#22312;&#20110;&#36807;&#24230;&#20381;&#36182;&#39033;&#30446;&#29305;&#23450;&#24555;&#25463;&#26041;&#24335;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#39033;&#30446;&#29305;&#23450;&#24615;&#30340;&#24230;&#37327;Cond-Idf&#65292;&#24182;&#25351;&#20986;&#27169;&#22411;&#20542;&#21521;&#20110;&#21033;&#29992;&#34394;&#20551;&#32479;&#35745;&#32447;&#32034;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2201.07381</link><description>&lt;p&gt;
&#25581;&#31034;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#20013;&#30340;&#39033;&#30446;&#29305;&#23450;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unveiling Project-Specific Bias in Neural Code Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.07381
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#34920;&#29616;&#20986;&#21487;&#35266;&#30340;&#24615;&#33021;&#65292;&#20294;&#24448;&#24448;&#26080;&#27861;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#36328;&#39033;&#30446;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#21407;&#22240;&#22312;&#20110;&#36807;&#24230;&#20381;&#36182;&#39033;&#30446;&#29305;&#23450;&#24555;&#25463;&#26041;&#24335;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#39033;&#30446;&#29305;&#23450;&#24615;&#30340;&#24230;&#37327;Cond-Idf&#65292;&#24182;&#25351;&#20986;&#27169;&#22411;&#20542;&#21521;&#20110;&#21033;&#29992;&#34394;&#20551;&#32479;&#35745;&#32447;&#32034;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#36719;&#20214;&#20998;&#26512;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#23613;&#31649;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#22312;&#39033;&#30446;&#20869;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#35774;&#32622;&#19979;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#34920;&#29616;&#20986;&#21487;&#35266;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#36328;&#39033;&#30446;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#36807;&#24230;&#20381;&#36182;&#39033;&#30446;&#29305;&#23450;&#24555;&#25463;&#26041;&#24335;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#35777;&#25454;&#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cond-Idf&#30340;&#24230;&#37327;&#26469;&#35299;&#37322;&#36825;&#31181;&#34892;&#20026;&#65292;&#35813;&#24230;&#37327;&#37327;&#21270;&#20102;&#20196;&#29260;&#19982;&#26631;&#31614;&#21450;&#20854;&#39033;&#30446;&#29305;&#23450;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#27169;&#22411;&#34892;&#20026;&#19982;&#25552;&#20986;&#30340;&#24230;&#37327;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#24448;&#24448;&#20542;&#21521;&#20110;&#21033;&#29992;&#34394;&#20551;&#30340;&#32479;&#35745;&#32447;&#32034;&#36827;&#34892;&#39044;&#27979;&#12290;&#20511;&#21161;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.07381v2 Announce Type: replace  Abstract: Deep learning has introduced significant improvements in many software analysis tasks. Although the Large Language Models (LLMs) based neural code models demonstrate commendable performance when trained and tested within the intra-project independent and identically distributed (IID) setting, they often struggle to generalize effectively to real-world inter-project out-of-distribution (OOD) data. In this work, we show that this phenomenon is caused by the heavy reliance on project-specific shortcuts for prediction instead of ground-truth evidence. We propose a Cond-Idf measurement to interpret this behavior, which quantifies the relatedness of a token with a label and its project-specificness. The strong correlation between model behavior and the proposed measurement indicates that without proper regularization, models tend to leverage spurious statistical cues for prediction. Equipped with these observations, we propose a novel bias
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StableGNN&#30340;&#36890;&#29992;&#22240;&#26524;&#34920;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#39640;&#32423;&#22270;&#34920;&#31034;&#24182;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#20998;&#24067;&#22806;&#22270;&#19978;&#23454;&#29616;&#31283;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2111.10657</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#22806;&#22270;&#19978;&#25512;&#24191;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generalizing Graph Neural Networks on Out-Of-Distribution Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.10657
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StableGNN&#30340;&#36890;&#29992;&#22240;&#26524;&#34920;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#39640;&#32423;&#22270;&#34920;&#31034;&#24182;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#20998;&#24067;&#22806;&#22270;&#19978;&#23454;&#29616;&#31283;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#27809;&#26377;&#32771;&#34385;&#35757;&#32451;&#21644;&#27979;&#35797;&#22270;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#65292;&#23548;&#33268;GNNs&#22312;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#35774;&#32622;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#19979;&#38477;&#12290;&#36825;&#31181;&#36864;&#21270;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#22823;&#22810;&#25968;GNNs&#26159;&#22522;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#24320;&#21457;&#30340;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;GNNs&#20542;&#21521;&#20110;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#30340;&#32454;&#24494;&#32479;&#35745;&#30456;&#20851;&#24615;&#36827;&#34892;&#39044;&#27979;&#65292;&#21363;&#20351;&#36825;&#26159;&#19968;&#31181;&#20266;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20266;&#30456;&#20851;&#24615;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#23548;&#33268;GNNs&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#28040;&#38500;&#20266;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#23545;&#20110;&#31283;&#23450;&#30340;GNNs&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StableGNN&#30340;&#36890;&#29992;&#22240;&#26524;&#34920;&#31034;&#26694;&#26550;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#39318;&#20808;&#20174;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#39640;&#32423;&#34920;&#31034;&#65292;&#28982;&#21518;&#20511;&#21161;&#22240;&#26524;&#25512;&#26029;&#30340;&#21306;&#20998;&#33021;&#21147;&#26469;&#24110;&#21161;&#27169;&#22411;&#33719;&#24471;&#31283;&#23450;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.10657v3 Announce Type: replace-cross  Abstract: Graph Neural Networks (GNNs) are proposed without considering the agnostic distribution shifts between training and testing graphs, inducing the degeneration of the generalization ability of GNNs on Out-Of-Distribution (OOD) settings. The fundamental reason for such degeneration is that most GNNs are developed based on the I.I.D hypothesis. In such a setting, GNNs tend to exploit subtle statistical correlations existing in the training set for predictions, even though it is a spurious correlation. However, such spurious correlations may change in testing environments, leading to the failure of GNNs. Therefore, eliminating the impact of spurious correlations is crucial for stable GNNs. To this end, we propose a general causal representation framework, called StableGNN. The main idea is to extract high-level representations from graph data first and resort to the distinguishing ability of causal inference to help the model get ri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#20445;&#25345;&#24178;&#39044;&#26469;&#37327;&#21270;&#33410;&#28857;&#23545;&#30446;&#26631;&#33410;&#28857;&#30340;&#22266;&#26377;&#22240;&#26524;&#36129;&#29486;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#23558;&#22240;&#26524;&#20449;&#24687;&#19982;&#31062;&#20808;&#33410;&#28857;&#20449;&#24687;&#20998;&#31163;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#26041;&#24046;&#21644;&#29109;&#30340;&#36129;&#29486;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2007.00714</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#25345;&#32467;&#26500;&#30340;&#24178;&#39044;&#26469;&#37327;&#21270;&#22266;&#26377;&#22240;&#26524;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
Quantifying intrinsic causal contributions via structure preserving interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2007.00714
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#20445;&#25345;&#24178;&#39044;&#26469;&#37327;&#21270;&#33410;&#28857;&#23545;&#30446;&#26631;&#33410;&#28857;&#30340;&#22266;&#26377;&#22240;&#26524;&#36129;&#29486;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#23558;&#22240;&#26524;&#20449;&#24687;&#19982;&#31062;&#20808;&#33410;&#28857;&#20449;&#24687;&#20998;&#31163;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#26041;&#24046;&#21644;&#29109;&#30340;&#36129;&#29486;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25551;&#36848;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#19968;&#20010;&#33410;&#28857;&#23545;&#30446;&#26631;&#33410;&#28857;&#30340;&#8220;&#22266;&#26377;&#8221;&#36129;&#29486;&#37096;&#20998;&#30340;&#22240;&#26524;&#24433;&#21709;&#27010;&#24565;&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#33410;&#28857;&#36882;&#24402;&#22320;&#20889;&#25104;&#19978;&#28216;&#22122;&#22768;&#39033;&#30340;&#20989;&#25968;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#33410;&#28857;&#28155;&#21152;&#30340;&#22266;&#26377;&#20449;&#24687;&#19982;&#20174;&#31062;&#20808;&#33410;&#28857;&#33719;&#24471;&#30340;&#20449;&#24687;&#20998;&#24320;&#12290;&#20026;&#20102;&#23558;&#22266;&#26377;&#20449;&#24687;&#35299;&#37322;&#20026;&#8220;&#22240;&#26524;&#8221;&#36129;&#29486;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#8220;&#20445;&#25345;&#32467;&#26500;&#30340;&#24178;&#39044;&#8221;&#65292;&#36825;&#20123;&#24178;&#39044;&#20197;&#19968;&#31181;&#27169;&#25311;&#23545;&#29238;&#33410;&#28857;&#30340;&#36890;&#24120;&#20381;&#36182;&#20851;&#31995;&#24182;&#19988;&#19981;&#25200;&#20081;&#35266;&#23519;&#21040;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#26041;&#24335;&#38543;&#26426;&#21270;&#27599;&#20010;&#33410;&#28857;&#12290;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#23545;&#37325;&#26032;&#26631;&#35760;&#33410;&#28857;&#19981;&#21464;&#30340;&#27979;&#37327;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Shapley&#30340;&#23545;&#31216;&#21270;&#65292;&#24182;&#19988;&#34920;&#26126;&#22312;&#32447;&#24615;&#24773;&#20917;&#19979;&#65292;&#22312;&#23558;&#30446;&#26631;&#33410;&#28857;&#35299;&#26512;&#20026;&#22122;&#22768;&#21464;&#37327;&#21518;&#65292;&#23427;&#21270;&#31616;&#20026;&#31616;&#21333;&#30340;ANOVA&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#26041;&#24046;&#21644;&#29109;&#30340;&#36129;&#29486;&#20998;&#26512;&#65292;&#20294;&#20854;&#20182;&#30446;&#26631;&#24230;&#37327;&#30340;&#36129;&#29486;&#21487;&#20197;&#31867;&#20284;&#22320;&#23450;&#20041;&#12290;&#20195;&#30721;&#21487;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2007.00714v4 Announce Type: replace  Abstract: We propose a notion of causal influence that describes the `intrinsic' part of the contribution of a node on a target node in a DAG. By recursively writing each node as a function of the upstream noise terms, we separate the intrinsic information added by each node from the one obtained from its ancestors. To interpret the intrinsic information as a {\it causal} contribution, we consider `structure-preserving interventions' that randomize each node in a way that mimics the usual dependence on the parents and does not perturb the observed joint distribution. To get a measure that is invariant with respect to relabelling nodes we use Shapley based symmetrization and show that it reduces in the linear case to simple ANOVA after resolving the target node into noise variables. We describe our contribution analysis for variance and entropy, but contributions for other target metrics can be defined analogously. The code is available in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#26469;&#35299;&#20915;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30693;&#35782;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;LLMs&#30340;&#32852;&#21512;&#27010;&#29575;&#65292;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#21644;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#25552;&#39640;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35774;&#35745;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.14151</link><description>&lt;p&gt;
&#30495;&#30693;&#26469;&#28304;&#20110;&#23454;&#36341;&#65306;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20351;LLMs&#19982;&#20855;&#36523;&#29615;&#22659;&#23545;&#40784;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning. (arXiv:2401.14151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#26469;&#35299;&#20915;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30693;&#35782;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;LLMs&#30340;&#32852;&#21512;&#27010;&#29575;&#65292;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#21644;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#25552;&#39640;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35774;&#35745;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20247;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#20915;&#31616;&#21333;&#30340;&#20915;&#31574;&#20219;&#21153;&#19978;&#32463;&#24120;&#22833;&#36133;&#65292;&#21407;&#22240;&#26159;LLMs&#20013;&#30340;&#30693;&#35782;&#19982;&#29615;&#22659;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26234;&#33021;&#20307;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#31574;&#30053;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22987;&#32456;&#19982;&#29615;&#22659;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#38590;&#20197;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#25972;&#21512;&#21040;&#20854;&#20013;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TWOSOME&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;RL&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#24182;&#23454;&#29616;&#23545;&#40784;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20934;&#22791;&#22909;&#30340;&#25968;&#25454;&#38598;&#25110;&#29615;&#22659;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#26597;&#35810;&#27599;&#20010;&#26377;&#25928;&#21160;&#20316;&#30340;&#32852;&#21512;&#27010;&#29575;&#20197;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#22686;&#24378;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#34892;&#20026;&#35780;&#20272;&#21644;&#36873;&#25321;&#31639;&#27861;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the acto
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#35774;&#35745;&#36215;&#21040;&#20248;&#21270;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#25429;&#33719;&#30340;&#23398;&#20064;&#33021;&#37327;&#20989;&#25968;&#65292;&#21487;&#20197;&#36991;&#20813;&#23545;&#25239;&#31034;&#20363;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#35774;&#35745;&#24615;&#33021;&#12290;&#36825;&#19968;&#35774;&#35745;&#31995;&#32479;&#26159;&#32452;&#21512;&#24615;&#30340;&#65292;&#20351;&#24471;&#21487;&#20197;&#35774;&#35745;&#20855;&#26377;&#27599;&#20010;&#25351;&#23450;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.13171</link><description>&lt;p&gt;
&#32452;&#21512;&#24335;&#29983;&#25104;&#36870;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Compositional Generative Inverse Design. (arXiv:2401.13171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13171
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#35774;&#35745;&#36215;&#21040;&#20248;&#21270;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#25429;&#33719;&#30340;&#23398;&#20064;&#33021;&#37327;&#20989;&#25968;&#65292;&#21487;&#20197;&#36991;&#20813;&#23545;&#25239;&#31034;&#20363;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#35774;&#35745;&#24615;&#33021;&#12290;&#36825;&#19968;&#35774;&#35745;&#31995;&#32479;&#26159;&#32452;&#21512;&#24615;&#30340;&#65292;&#20351;&#24471;&#21487;&#20197;&#35774;&#35745;&#20855;&#26377;&#27599;&#20010;&#25351;&#23450;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#35774;&#35745;&#26159;&#19968;&#31181;&#23547;&#27714;&#35774;&#35745;&#36755;&#20837;&#21464;&#37327;&#20197;&#20248;&#21270;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#22312;&#26426;&#26800;&#24037;&#31243;&#21040;&#33322;&#22825;&#24037;&#31243;&#31561;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#36870;&#35774;&#35745;&#36890;&#24120;&#34987;&#26500;&#24314;&#25104;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#24448;&#24448;&#20250;&#38519;&#20837;&#23545;&#25239;&#27169;&#24335;&#65292;&#38459;&#30861;&#26377;&#25928;&#30340;&#25277;&#26679;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#25429;&#33719;&#30340;&#23398;&#20064;&#33021;&#37327;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#36991;&#20813;&#36825;&#31181;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#35774;&#35745;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#26679;&#19968;&#20010;&#35774;&#35745;&#31995;&#32479;&#26159;&#32452;&#21512;&#24615;&#30340;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#32467;&#21512;&#22810;&#20010;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#20195;&#34920;&#25152;&#38656;&#31995;&#32479;&#30340;&#23376;&#32452;&#20214;&#65292;&#20174;&#32780;&#35774;&#35745;&#20855;&#26377;&#27599;&#20010;&#25351;&#23450;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;&#22312;&#19968;&#20010;N&#20307;&#30456;&#20114;&#20316;&#29992;&#20219;&#21153;&#21644;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20108;&#32500;&#22810;&#32764;&#22411;&#35774;&#35745;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#32452;&#21512;&#23398;&#20064;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#35774;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse design, where we seek to design input variables in order to optimize an underlying objective function, is an important problem that arises across fields such as mechanical engineering to aerospace engineering. Inverse design is typically formulated as an optimization problem, with recent works leveraging optimization across learned dynamics models. However, as models are optimized they tend to fall into adversarial modes, preventing effective sampling. We illustrate that by instead optimizing over the learned energy function captured by the diffusion model, we can avoid such adversarial examples and significantly improve design performance. We further illustrate how such a design system is compositional, enabling us to combine multiple different diffusion models representing subcomponents of our desired system to design systems with every specified component. In an N-body interaction task and a challenging 2D multi-airfoil design task, we demonstrate that by composing the learn
&lt;/p&gt;</description></item><item><title>&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#30340;&#20219;&#21153;&#20013;&#12290;&#27492;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2401.10286</link><description>&lt;p&gt;
&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#20348;&#20348;&#32773;&#65306;&#33521;&#25991;&#20195;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Top in Chinese Data Processing: English Code Models. (arXiv:2401.10286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10286
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#30340;&#20219;&#21153;&#20013;&#12290;&#27492;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#20013;&#65292;&#20219;&#21153;&#19982;&#35757;&#32451;&#35821;&#26009;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#20849;&#35782;&#65292;&#20294;&#25105;&#20204;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#21644;&#25105;&#20204;&#35774;&#35745;&#30340;&#35780;&#20272;&#25351;&#26631;&#34920;&#26126;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#19982;&#20219;&#21153;&#32039;&#23494;&#21305;&#37197;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#31243;&#24230;&#36739;&#39640;&#30340;&#20219;&#21153;&#20013;&#65292;&#23637;&#31034;&#36739;&#23569;&#20013;&#25991;&#35821;&#35328;&#29305;&#24449;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#29992;&#20195;&#30721;&#27169;&#22411;&#26367;&#25442;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#22914;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#20934;&#22791;&#25968;&#25454;&#65292;&#24456;&#23481;&#26131;&#24471;&#21040;&#22797;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the alignment between tasks and training corpora is a fundamental consensus in the application of language models, our series of experiments and the metrics we designed reveal that code-based Large Language Models (LLMs) significantly outperform models trained on data that is closely matched to the tasks in non-coding Chinese tasks. Moreover, in tasks high sensitivity to Chinese hallucinations, models exhibiting fewer linguistic features of the Chinese language achieve better performance. Our experimental results can be easily replicated in Chinese data processing tasks, such as preparing data for Retrieval-Augmented Generation (RAG), by simply replacing the base model with a code-based model. Additionally, our research offers a distinct perspective for discussion on the philosophical "Chinese Room" thought experiment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#65288;CDE&#65289;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#32422;&#26463;&#29366;&#24577;-&#34892;&#20026;&#21344;&#25454;&#31283;&#24577;&#20998;&#24067;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22806;&#25512;&#38169;&#35823;&#38382;&#39064;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#25110;&#19981;&#36275;&#25968;&#25454;&#30340;&#20219;&#21153;&#20013;&#65292;CDE&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08819</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#20174;&#31232;&#30095;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Sparse Offline Datasets via Conservative Density Estimation. (arXiv:2401.08819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#65288;CDE&#65289;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#32422;&#26463;&#29366;&#24577;-&#34892;&#20026;&#21344;&#25454;&#31283;&#24577;&#20998;&#24067;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22806;&#25512;&#38169;&#35823;&#38382;&#39064;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#25110;&#19981;&#36275;&#25968;&#25454;&#30340;&#20219;&#21153;&#20013;&#65292;CDE&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20026;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#19968;&#27493;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#22806;&#25512;&#38169;&#35823;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#22870;&#21169;&#25110;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#65288;CDE&#65289;&#30340;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#32422;&#26463;&#29366;&#24577;-&#34892;&#20026;&#21344;&#25454;&#31283;&#24577;&#20998;&#24067;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;CDE&#36890;&#36807;&#35299;&#20915;&#36793;&#38469;&#37325;&#35201;&#24615;&#25277;&#26679;&#20013;&#30340;&#25903;&#25345;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#31283;&#24577;&#20998;&#24067;&#26657;&#27491;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CDE&#22312;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#25110;&#19981;&#36275;&#25968;&#25454;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#25345;&#32493;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#22806;&#25512;&#38169;&#35823;&#38382;&#39064;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers a promising direction for learning policies from pre-collected datasets without requiring further interactions with the environment. However, existing methods struggle to handle out-of-distribution (OOD) extrapolation errors, especially in sparse reward or scarce data settings. In this paper, we propose a novel training algorithm called Conservative Density Estimation (CDE), which addresses this challenge by explicitly imposing constraints on the state-action occupancy stationary distribution. CDE overcomes the limitations of existing approaches, such as the stationary distribution correction method, by addressing the support mismatch issue in marginal importance sampling. Our method achieves state-of-the-art performance on the D4RL benchmark. Notably, CDE consistently outperforms baselines in challenging tasks with sparse rewards or insufficient data, demonstrating the advantages of our approach in addressing the extrapolation error problem i
&lt;/p&gt;</description></item><item><title>InfiAgent-DABench&#26159;&#31532;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;DAEval&#25968;&#25454;&#38598;&#21644;&#20195;&#29702;&#26694;&#26550;&#12290;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#24403;&#21069;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05507</link><description>&lt;p&gt;
InfiAgent-DABench: &#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#35780;&#20272;&#20195;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks. (arXiv:2401.05507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05507
&lt;/p&gt;
&lt;p&gt;
InfiAgent-DABench&#26159;&#31532;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;DAEval&#25968;&#25454;&#38598;&#21644;&#20195;&#29702;&#26694;&#26550;&#12290;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#24403;&#21069;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;"InfiAgent-DABench"&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;DAEval&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;55&#20010;CSV&#25991;&#20214;&#34893;&#29983;&#20986;&#30340;311&#20010;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#35780;&#20272;LLMs&#20316;&#20026;&#25968;&#25454;&#20998;&#26512;&#20195;&#29702;&#30340;&#20195;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26684;&#24335;&#25552;&#31034;&#25216;&#26415;&#65292;&#30830;&#20445;&#38382;&#39064;&#26159;&#38381;&#21512;&#24418;&#24335;&#30340;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#24403;&#21069;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;DAAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#19987;&#38376;&#20195;&#29702;&#12290;InfiAgent-DABench&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#21253;&#24050;&#32463;&#21457;&#24067;&#22312;https://github.com/InfiAgent/InfiAgent&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce "InfiAgent-DABench", the first benchmark specifically designed to evaluate LLM-based agents in data analysis tasks. This benchmark contains DAEval, a dataset consisting of 311 data analysis questions derived from 55 CSV files, and an agent framework to evaluate LLMs as data analysis agents. We adopt a format-prompting technique, ensuring questions to be closed-form that can be automatically evaluated. Our extensive benchmarking of 23 state-of-the-art LLMs uncovers the current challenges encountered in data analysis tasks. In addition, we have developed DAAgent, a specialized agent trained on instruction-tuning datasets. Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26234;&#33021;&#20307;&#21464;&#24471;&#26234;&#33021;&#30340;&#22240;&#32032;&#65292;&#24378;&#35843;&#20102;&#25484;&#25569;&#29305;&#24449;&#21644;&#25511;&#21046;&#22810;&#20010;&#20445;&#23432;&#30456;&#20114;&#20316;&#29992;&#30340;&#23376;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#30340;&#26680;&#24515;&#26159;&#8220;&#38598;&#20307;&#22914;&#19968;&#20307;&#8221;&#21644;&#8220;&#20102;&#35299;&#23616;&#37096;&#34892;&#21160;&#30340;&#25972;&#20307;&#32467;&#26524;&#8221;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#38598;&#20307;&#20445;&#23432;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04846</link><description>&lt;p&gt;
&#21463;&#36807;&#33391;&#22909;&#25945;&#32946;&#30340;&#26234;&#33021;&#30340;&#20869;&#22312;&#21892;&#33391;
&lt;/p&gt;
&lt;p&gt;
The inherent goodness of well educated intelligence. (arXiv:2401.04846v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26234;&#33021;&#20307;&#21464;&#24471;&#26234;&#33021;&#30340;&#22240;&#32032;&#65292;&#24378;&#35843;&#20102;&#25484;&#25569;&#29305;&#24449;&#21644;&#25511;&#21046;&#22810;&#20010;&#20445;&#23432;&#30456;&#20114;&#20316;&#29992;&#30340;&#23376;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#30340;&#26680;&#24515;&#26159;&#8220;&#38598;&#20307;&#22914;&#19968;&#20307;&#8221;&#21644;&#8220;&#20102;&#35299;&#23616;&#37096;&#34892;&#21160;&#30340;&#25972;&#20307;&#32467;&#26524;&#8221;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#38598;&#20307;&#20445;&#23432;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25506;&#35752;&#20351;&#19968;&#20010;&#26234;&#33021;&#20307;&#21464;&#24471;&#26234;&#33021;&#30340;&#22240;&#32032;&#65292;&#26080;&#35770;&#26159;&#29983;&#29289;&#20307;&#36824;&#26159;&#35745;&#31639;&#26426;&#19978;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#29305;&#21035;&#20851;&#27880;&#30340;&#26159;&#33021;&#22815;&#34920;&#24449;&#21644;&#25511;&#21046;&#22810;&#20010;&#20445;&#23432;&#30456;&#20114;&#20316;&#29992;&#30340;&#30456;&#21516;&#23376;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#30340;&#26412;&#36136;&#23558;&#34987;&#21457;&#29616;&#26159;&#40644;&#37329;&#27861;&#21017;&#8212;&#8212;&#8220;&#38598;&#20307;&#34892;&#21160;&#22914;&#19968;&#20307;&#8221;&#25110;&#8220;&#20102;&#35299;&#23616;&#37096;&#34892;&#21160;&#30340;&#25972;&#20307;&#32467;&#26524;&#8221;&#12290;&#38598;&#20307;&#30340;&#27969;&#21160;&#26159;&#30001;&#25484;&#25511;&#30528;&#23569;&#37327;&#23383;&#31526;&#20018;&#30340;&#25805;&#32437;&#32773;&#20915;&#23450;&#30340;&#65292;&#26681;&#25454;&#23545;&#31216;&#24615;&#30830;&#23450;&#30340;&#26368;&#23567;&#20316;&#29992;&#36335;&#24452;&#30340;&#27979;&#22320;&#32447;&#36816;&#21160;&#12290;&#25511;&#21046;&#38598;&#20307;&#20445;&#23432;&#31995;&#32479;&#26159;&#22256;&#38590;&#30340;&#65292;&#21382;&#21490;&#19978;&#19968;&#30452;&#36890;&#36807;&#20026;&#31995;&#32479;&#28155;&#21152;&#26174;&#33879;&#40655;&#24615;&#26469;&#31283;&#23450;&#26399;&#26395;&#30340;&#26368;&#22823;&#24615;&#33021;&#30340;&#20122;&#31283;&#24179;&#34913;&#29366;&#24577;&#65292;&#20294;&#36825;&#20250;&#22312;&#36807;&#31243;&#20013;&#38477;&#20302;&#25110;&#30772;&#22351;&#23427;&#20204;&#12290;&#26377;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper will examine what makes a being intelligent, whether that be a biological being or an artificial silicon being on a computer. Special attention will be paid to the being having the ability to characterize and control a collective system of many identical conservative sub-systems conservatively interacting. The essence of intelligence will be found to be the golden rule -- "the collective acts as one" or "knowing the global consequences of local actions". The flow of the collective is a small set of twinkling textures, that are governed by a puppeteer who is pulling a small number of strings according to a geodesic motion of least action, determined by the symmetries. Controlling collective conservative systems is difficult and has historically been done by adding significant viscosity to the system to stabilize the desirable meta stable equilibriums of maximum performance, but it degrades or destroys them in the process. There is an alternative. Once the optimum twinkling te
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;HTML&#20869;&#23481;&#30340;&#39640;&#32423;&#26816;&#27979;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#34701;&#21512;&#26041;&#27861;&#26816;&#27979;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21019;&#36896;&#20102;&#19968;&#20010;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#36825;&#39033;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.04820</link><description>&lt;p&gt;
&#36890;&#36807;HTML&#20869;&#23481;&#30340;&#22810;&#27169;&#22411;&#20998;&#26512;&#26816;&#27979;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;
&lt;/p&gt;
&lt;p&gt;
Phishing Website Detection through Multi-Model Analysis of HTML Content. (arXiv:2401.04820v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;HTML&#20869;&#23481;&#30340;&#39640;&#32423;&#26816;&#27979;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#34701;&#21512;&#26041;&#27861;&#26816;&#27979;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21019;&#36896;&#20102;&#19968;&#20010;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#36825;&#39033;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#20852;&#36215;&#65292;&#25105;&#20204;&#30340;&#36890;&#20449;&#21644;&#24037;&#20316;&#26041;&#24335;&#21457;&#29983;&#20102;&#24040;&#22823;&#30340;&#21464;&#21270;&#12290;&#34429;&#28982;&#23427;&#20026;&#25105;&#20204;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#32593;&#32476;&#23041;&#32961;&#12290;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#19988;&#20005;&#37325;&#30340;&#23041;&#32961;&#26159;&#32593;&#32476;&#38035;&#40060;&#65292;&#40657;&#23458;&#20351;&#29992;&#27450;&#39575;&#24615;&#26041;&#27861;&#31363;&#21462;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;HTML&#20869;&#23481;&#30340;&#20808;&#36827;&#26816;&#27979;&#27169;&#22411;&#65292;&#38024;&#23545;&#32593;&#32476;&#38035;&#40060;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#29992;&#20110;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#19987;&#38376;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#27169;&#22411;&#21644;&#20004;&#20010;&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#27169;&#22411;&#65292;&#20197;&#20998;&#26512;&#39029;&#38754;&#26631;&#39064;&#21644;&#20869;&#23481;&#31561;&#25991;&#26412;&#29305;&#24449;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#34701;&#21512;&#36807;&#31243;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#23884;&#20837;&#21521;&#37327;&#34987;&#21644;&#35856;&#22320;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#36755;&#20837;&#21040;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;&#12290;&#37492;&#20110;&#30446;&#21069;&#32570;&#20047;&#20840;&#38754;&#30340;&#32593;&#32476;&#38035;&#40060;&#30740;&#31350;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#36824;&#21253;&#25324;&#21019;&#24314;&#19968;&#20010;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The way we communicate and work has changed significantly with the rise of the Internet. While it has opened up new opportunities, it has also brought about an increase in cyber threats. One common and serious threat is phishing, where cybercriminals employ deceptive methods to steal sensitive information.This study addresses the pressing issue of phishing by introducing an advanced detection model that meticulously focuses on HTML content. Our proposed approach integrates a specialized Multi-Layer Perceptron (MLP) model for structured tabular data and two pretrained Natural Language Processing (NLP) models for analyzing textual features such as page titles and content. The embeddings from these models are harmoniously combined through a novel fusion process. The resulting fused embeddings are then input into a linear classifier. Recognizing the scarcity of recent datasets for comprehensive phishing research, our contribution extends to the creation of an up-to-date dataset, which we o
&lt;/p&gt;</description></item><item><title>SecureReg&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22495;&#21517;&#27880;&#20876;&#36807;&#31243;&#20013;&#20027;&#21160;&#26292;&#38706;&#24694;&#24847;&#22495;&#21517;&#27880;&#20876;&#65292;&#25552;&#20379;&#20102;&#26089;&#26399;&#23041;&#32961;&#26816;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#28431;&#27934;&#31383;&#21475;&#65292;&#24182;&#20026;&#20027;&#21160;&#39044;&#38450;&#24615;&#25805;&#20316;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2401.03196</link><description>&lt;p&gt;
SecureReg:&#19968;&#20010;&#32467;&#21512;&#26041;&#27861;&#29992;&#20110;&#20027;&#21160;&#26292;&#38706;&#24694;&#24847;&#22495;&#21517;&#27880;&#20876;
&lt;/p&gt;
&lt;p&gt;
SecureReg: A Combined Framework for Proactively Exposing Malicious Domain Name Registrations. (arXiv:2401.03196v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03196
&lt;/p&gt;
&lt;p&gt;
SecureReg&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22495;&#21517;&#27880;&#20876;&#36807;&#31243;&#20013;&#20027;&#21160;&#26292;&#38706;&#24694;&#24847;&#22495;&#21517;&#27880;&#20876;&#65292;&#25552;&#20379;&#20102;&#26089;&#26399;&#23041;&#32961;&#26816;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#28431;&#27934;&#31383;&#21475;&#65292;&#24182;&#20026;&#20027;&#21160;&#39044;&#38450;&#24615;&#25805;&#20316;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#19981;&#26029;&#22686;&#21152;&#65292;&#19981;&#27861;&#20998;&#23376;&#27599;&#22825;&#27880;&#20876;&#25968;&#21315;&#20010;&#26032;&#22495;&#21517;&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#12289;&#32593;&#32476;&#38035;&#40060;&#21644;&#39537;&#21160;&#19979;&#36733;&#31561;&#20114;&#32852;&#32593;&#25915;&#20987;&#65292;&#24378;&#35843;&#20102;&#21019;&#26032;&#26816;&#27979;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27880;&#20876;&#36807;&#31243;&#24320;&#22987;&#26102;&#35782;&#21035;&#21487;&#30097;&#22495;&#21517;&#12290;&#38468;&#24102;&#30340;&#25968;&#25454;&#27969;&#31243;&#36890;&#36807;&#27604;&#36739;&#26032;&#22495;&#21517;&#19982;&#27880;&#20876;&#22495;&#21517;&#20135;&#29983;&#20851;&#38190;&#29305;&#24449;&#65292;&#24378;&#35843;&#20102;&#20851;&#38190;&#30456;&#20284;&#24230;&#24471;&#20998;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#26032;&#39062;&#32452;&#21512;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;Canine&#27169;&#22411;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20998;&#26512;&#35821;&#20041;&#21644;&#25968;&#20540;&#23646;&#24615;&#65292;&#20026;&#26089;&#26399;&#23041;&#32961;&#26816;&#27979;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#21512;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#28431;&#27934;&#31383;&#21475;&#65292;&#21152;&#24378;&#20102;&#23545;&#28508;&#22312;&#23041;&#32961;&#30340;&#38450;&#24481;&#12290;&#30740;&#31350;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#32508;&#21512;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#20027;&#21160;&#39044;&#38450;&#24615;&#25805;&#20316;&#30340;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rising cyber threats, with miscreants registering thousands of new domains daily for Internet-scale attacks like spam, phishing, and drive-by downloads, emphasize the need for innovative detection methods. This paper introduces a cutting-edge approach for identifying suspicious domains at the onset of the registration process. The accompanying data pipeline generates crucial features by comparing new domains to registered domains,emphasizing the crucial similarity score. Leveraging a novel combination of Natural Language Processing (NLP) techniques, including a pretrained Canine model, and Multilayer Perceptron (MLP) models, our system analyzes semantic and numerical attributes, providing a robust solution for early threat detection. This integrated approach significantly reduces the window of vulnerability, fortifying defenses against potential threats. The findings demonstrate the effectiveness of the integrated approach and contribute to the ongoing efforts in developing proactive s
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#40657;&#30418;&#38382;&#39064;&#30340;&#30333;&#30418;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#39046;&#22495;&#19968;&#33324;&#29702;&#35770;&#30340;&#30830;&#23450;&#24615;&#36923;&#36753;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#35268;&#21017;&#65292;&#35813;&#32454;&#32990;&#33258;&#21160;&#26426;&#23454;&#29616;&#33258;&#21160;&#24182;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.03093</link><description>&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#40657;&#30418;&#38382;&#39064;&#30340;&#30333;&#30418;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A white box solution to the black box problem of AI. (arXiv:2401.03093v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03093
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#40657;&#30418;&#38382;&#39064;&#30340;&#30333;&#30418;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#39046;&#22495;&#19968;&#33324;&#29702;&#35770;&#30340;&#30830;&#23450;&#24615;&#36923;&#36753;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#35268;&#21017;&#65292;&#35813;&#32454;&#32990;&#33258;&#21160;&#26426;&#23454;&#29616;&#33258;&#21160;&#24182;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36879;&#26126;&#24615;&#65292;&#23545;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#23384;&#22312;&#25285;&#24551;&#12290;&#36825;&#23601;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#30418;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#31526;&#21495; AI &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#31526;&#21495; AI &#20855;&#26377;&#36879;&#26126;&#30340;&#30333;&#30418;&#24615;&#36136;&#12290;&#31526;&#21495; AI &#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#25968;&#23398;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#26415;&#35821;&#30340;&#19981;&#36879;&#26126;&#24615;&#12289;&#32570;&#20047;&#32479;&#19968;&#26412;&#20307;&#35770;&#20197;&#21450;&#25628;&#32034;&#36873;&#39033;&#30340;&#32452;&#21512;&#29190;&#28856;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#30418;&#38382;&#39064;&#24182;&#23454;&#29616;&#36890;&#29992;&#30340;&#31526;&#21495; AI&#65292;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#39046;&#22495;&#19968;&#33324;&#29702;&#35770;&#30340;&#30830;&#23450;&#24615;&#36923;&#36753;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#35268;&#21017;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30456;&#20851;&#39046;&#22495;&#30340;&#19968;&#33324;&#29702;&#35770;&#36215;&#21040;&#20102;&#32454;&#32990;&#33258;&#21160;&#26426;&#25512;&#29702;&#30340;&#30693;&#35782;&#24211;&#30340;&#20316;&#29992;&#12290;&#32454;&#32990;&#33258;&#21160;&#26426;&#22312;&#22797;&#26434;&#31995;&#32479;&#30340;&#19977;&#20010;&#23618;&#27425;&#19978;&#23454;&#29616;&#33258;&#21160;&#24182;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence based on neural networks has made significant progress. However, there are concerns about the reliability and security of this approach due to its lack of transparency. This is the black box problem of AI. Here we show how this problem can be solved using symbolic AI, which has a transparent white box nature. The widespread use of symbolic AI is hindered by the opacity of mathematical models and natural language terms, the lack of a unified ontology, and the combinatorial explosion of search options. To solve the AI black box problem and to implement general-purpose symbolic AI, we propose to use deterministic logic cellular automata with rules based on first principles of the general theory of the relevant domain. In this case, the general theory of the relevant domain plays the role of a knowledge base for the cellular automaton inference. A cellular automaton implements automatic parallel logical inference at three levels of organization of a complex system. 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35774;&#35745;&#26550;&#26500;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#21508;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20027;&#35201;&#26041;&#27861;&#21253;&#25324;&#30740;&#31350;GNN&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#21644;&#20854;&#22312;&#21306;&#20998;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33021;&#21147;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.01626</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Expressive Power of Graph Neural Networks. (arXiv:2401.01626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01626
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35774;&#35745;&#26550;&#26500;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#21508;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20027;&#35201;&#26041;&#27861;&#21253;&#25324;&#30740;&#31350;GNN&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#21644;&#20854;&#22312;&#21306;&#20998;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33021;&#21147;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#25193;&#23637;&#21040;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;GNN&#21487;&#20197;&#35299;&#20915;&#31038;&#20250;&#31185;&#23398;&#12289;&#21270;&#23398;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;GNN&#26550;&#26500;&#30340;&#21457;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#33410;&#28857;&#25110;&#22270;&#20998;&#31867;&#31561;&#20219;&#21153;&#30340;&#23454;&#35777;&#24615;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#24037;&#20316;&#21017;&#23547;&#27714;&#25214;&#21040;&#20855;&#26377;&#29702;&#35770;&#29305;&#24615;&#30340;GNN&#26550;&#26500;&#65292;&#36890;&#36807;&#30740;&#31350;&#20854;&#34920;&#36798;&#33021;&#21147;&#24182;&#35774;&#35745;&#26368;&#22823;&#21270;&#36825;&#31181;&#34920;&#36798;&#33021;&#21147;&#30340;&#26550;&#26500;&#12290;&#34429;&#28982;&#20851;&#20110;&#22914;&#20309;&#23450;&#20041;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#36824;&#27809;&#26377;&#20849;&#35782;&#65292;&#20294;&#21487;&#20197;&#20174;&#20960;&#20010;&#26377;&#24456;&#22909;&#21160;&#26426;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#12290;&#20063;&#35768;&#26368;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#30740;&#31350;GNN&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#65292;&#23601;&#20687;MLP&#30340;&#36825;&#31181;&#24615;&#36136;&#19968;&#26679;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#21478;&#19968;&#20010;&#26041;&#21521;&#20851;&#27880;&#30340;&#26159;GNN&#22312;&#21306;&#20998;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33021;&#21147;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of Graph Neural Networks has received considerable interest in the past few years. By extending deep learning to graph-structured data, GNNs can solve a diverse set of tasks in fields including social science, chemistry, and medicine. The development of GNN architectures has largely been focused on improving empirical performance on tasks like node or graph classification. However, a line of recent work has instead sought to find GNN architectures that have desirable theoretical properties - by studying their expressive power and designing architectures that maximize this expressiveness.  While there is no consensus on the best way to define the expressiveness of a GNN, it can be viewed from several well-motivated perspectives. Perhaps the most natural approach is to study the universal approximation properties of GNNs, much in the way that this has been studied extensively for MLPs. Another direction focuses on the extent to which GNNs can distinguish between different graph
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#21363;&#25554;&#21363;&#29992;&#31574;&#30053;&#35268;&#21010;&#22120;(PDDPP)&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33539;&#24335;&#65292;&#36890;&#36807;&#21487;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#25554;&#20214;&#23454;&#29616;&#20027;&#21160;&#23545;&#35805;&#38382;&#39064;&#30340;&#31574;&#30053;&#21046;&#23450;&#12290;&#21033;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#26032;&#30340;&#26696;&#20363;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00262</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#21363;&#25554;&#21363;&#29992;&#31574;&#30053;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents. (arXiv:2311.00262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00262
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#21363;&#25554;&#21363;&#29992;&#31574;&#30053;&#35268;&#21010;&#22120;(PDDPP)&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33539;&#24335;&#65292;&#36890;&#36807;&#21487;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#25554;&#20214;&#23454;&#29616;&#20027;&#21160;&#23545;&#35805;&#38382;&#39064;&#30340;&#31574;&#30053;&#21046;&#23450;&#12290;&#21033;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#26032;&#30340;&#26696;&#20363;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26102;&#20195;&#20013;&#65292;&#20027;&#21160;&#23545;&#35805;&#20316;&#20026;&#19968;&#20010;&#23454;&#38469;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#35805;&#38382;&#39064;&#65292;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#26159;&#25552;&#39640;LLMs&#20027;&#21160;&#24615;&#30340;&#20851;&#38190;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26041;&#26696;&#25110;&#36890;&#36807;&#35821;&#35328;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#36845;&#20195;&#22686;&#24378;&#23545;LLMs&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#21463;&#38480;&#20110;&#20923;&#32467;&#30340;LLMs&#30340;&#31574;&#30053;&#35268;&#21010;&#33021;&#21147;&#65292;&#35201;&#20040;&#38590;&#20197;&#36716;&#31227;&#21040;&#26032;&#30340;&#26696;&#20363;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33539;&#24335;&#65292;&#20197;&#20351;&#29992;&#21487;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#25554;&#20214;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#22120;&#26469;&#21046;&#23450;LLMs&#22312;&#20027;&#21160;&#23545;&#35805;&#38382;&#39064;&#19978;&#30340;&#31574;&#30053;&#65292;&#21629;&#21517;&#20026;PPDPP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#20415;&#21033;&#29992;&#21487;&#29992;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;LLM&#30340;&#33258;&#25105;&#23545;&#24328;&#25910;&#38598;&#30340;&#21160;&#24577;&#20132;&#20114;&#25968;&#25454;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play s
&lt;/p&gt;</description></item><item><title>CycleNet&#26159;&#19968;&#31181;&#23558;&#24490;&#29615;&#19968;&#33268;&#24615;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#33539;&#22270;&#20687;&#25805;&#20316;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#32763;&#35793;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#39046;&#22495;&#20998;&#24067;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2310.13165</link><description>&lt;p&gt;
CycleNet&#65306;&#37325;&#26032;&#24605;&#32771;&#25991;&#26412;&#24341;&#23548;&#25193;&#25955;&#20013;&#30340;&#24490;&#29615;&#19968;&#33268;&#24615;&#65292;&#20197;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation. (arXiv:2310.13165v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13165
&lt;/p&gt;
&lt;p&gt;
CycleNet&#26159;&#19968;&#31181;&#23558;&#24490;&#29615;&#19968;&#33268;&#24615;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#33539;&#22270;&#20687;&#25805;&#20316;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#32763;&#35793;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#39046;&#22495;&#20998;&#24067;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#22312;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#20294;&#32570;&#20047;&#19968;&#31181;&#30452;&#35266;&#30340;&#19968;&#33268;&#22270;&#20687;&#21040;&#22270;&#20687;&#65288;I2I&#65289;&#32763;&#35793;&#25509;&#21475;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#25513;&#30721;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;DMs&#36827;&#34892;&#26080;&#37197;&#23545;&#30340;I2I&#32763;&#35793;&#24182;&#20445;&#25345;&#19968;&#33268;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Cyclenet&#65292;&#19968;&#31181;&#26032;&#39062;&#20294;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#24490;&#29615;&#19968;&#33268;&#24615;&#32435;&#20837;DMs&#20013;&#65292;&#20197;&#35268;&#33539;&#22270;&#20687;&#25805;&#20316;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;Cyclenet&#22312;&#19981;&#21516;&#31890;&#24230;&#30340;&#26080;&#37197;&#23545;I2I&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#12290;&#38500;&#20102;&#22330;&#26223;&#21644;&#23545;&#35937;&#32423;&#21035;&#30340;&#32763;&#35793;&#65292;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;I2I&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#29289;&#20307;&#30340;&#29289;&#29702;&#29366;&#24577;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;Cyclenet&#22312;&#32763;&#35793;&#30340;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#25913;&#21464;&#25991;&#26412;&#25551;&#36848;&#26102;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#39046;&#22495;&#20998;&#24067;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces Cyclenet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate Cyclenet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that Cyclenet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38544;&#24335;Q-learning&#65288;IQL&#65289;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#23637;&#29616;&#20986;&#20102;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#20854;&#37319;&#29992;&#30340;&#30417;&#30563;&#31574;&#30053;&#23398;&#20064;&#26041;&#26696;&#20026;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#21147;&#23398;&#25439;&#22351;&#19979;&#65292;IQL&#20173;&#28982;&#23384;&#22312;Q&#20989;&#25968;&#30340;&#37325;&#23614;&#30446;&#26631;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12955</link><description>&lt;p&gt;
&#26500;&#24314;&#20855;&#26377;&#22810;&#26679;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Offline Reinforcement Learning under Diverse Data Corruption. (arXiv:2310.12955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38544;&#24335;Q-learning&#65288;IQL&#65289;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#23637;&#29616;&#20986;&#20102;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#20854;&#37319;&#29992;&#30340;&#30417;&#30563;&#31574;&#30053;&#23398;&#20064;&#26041;&#26696;&#20026;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#21147;&#23398;&#25439;&#22351;&#19979;&#65292;IQL&#20173;&#28982;&#23384;&#22312;Q&#20989;&#25968;&#30340;&#37325;&#23614;&#30446;&#26631;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#24378;&#21270;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#26114;&#36149;&#25110;&#19981;&#23433;&#20840;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#23384;&#22312;&#22122;&#22768;&#65292;&#29978;&#33267;&#21487;&#33021;&#34987;&#24694;&#24847;&#25439;&#22351;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#23545;&#24403;&#21069;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21253;&#25324;&#29366;&#24577;&#12289;&#21160;&#20316;&#12289;&#22870;&#21169;&#21644;&#21160;&#21147;&#23398;&#22312;&#20869;&#30340;&#20840;&#38754;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#26174;&#31034;&#65292;&#38544;&#24335;Q-learning&#65288;IQL&#65289;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#34920;&#29616;&#20986;&#20102;&#21487;&#38752;&#30340;&#25239;&#25968;&#25454;&#25439;&#22351;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#32463;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;IQL&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#30417;&#30563;&#31574;&#30053;&#23398;&#20064;&#26041;&#26696;&#30830;&#23450;&#20026;&#20851;&#38190;&#22240;&#32032;&#12290;&#23613;&#31649;&#30456;&#23545;&#40065;&#26834;&#65292;&#20294;IQL&#22312;&#21160;&#21147;&#23398;&#25439;&#22351;&#19979;&#20173;&#28982;&#23384;&#22312;Q&#20989;&#25968;&#30340;&#37325;&#23614;&#30446;&#26631;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be maliciously corrupted, which can significantly degrade the performance of offline RL. In this work, we first investigate the performance of current offline RL algorithms under comprehensive data corruption, including states, actions, rewards, and dynamics. Our extensive experiments reveal that implicit Q-learning (IQL) demonstrates remarkable resilience to data corruption among various offline RL algorithms. Furthermore, we conduct both empirical and theoretical analyses to understand IQL's robust performance, identifying its supervised policy learning scheme as the key factor. Despite its relative robustness, IQL still suffers from heavy-tail targets of Q functions under dynamics corruption. To tack
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.10638</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#36807;&#39044;&#27979;&#32473;&#23450;&#25991;&#26723;&#21069;&#32512;&#30340;&#26631;&#35760;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38271;&#31687;&#29983;&#25104;&#21644;&#25552;&#31034;&#24335;&#20219;&#21153;&#65292;&#36825;&#21487;&#20197;&#31616;&#21270;&#20026;&#25991;&#26723;&#23436;&#25104;&#12290;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#36890;&#36807;&#36830;&#25509;&#38543;&#26426;&#32452;&#21512;&#30340;&#30701;&#25991;&#26723;&#26469;&#35757;&#32451;LMs&#65292;&#20197;&#21019;&#24314;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#20294;&#21069;&#19968;&#20010;&#25991;&#26723;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#25991;&#26723;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65292;&#21363;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26126;&#30830;&#40723;&#21169;&#23427;&#20204;&#36328;&#36234;&#25991;&#26723;&#36793;&#30028;&#36827;&#34892;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#65292;&#20351;&#27599;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25991;&#26723;&#25490;&#24207;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26377;&#25968;&#21313;&#20159;&#20010;&#25991;&#26723;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27599;&#20010;&#25991;&#26723;&#20013;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#32780;&#19981;&#37325;&#22797;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SiamAF&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24515;&#30005;&#22270;&#21644;&#20809;&#30005;&#33033;&#25615;&#22270;&#20449;&#21495;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#36890;&#36807;Siamese&#32593;&#32476;&#21644;&#32852;&#21512;&#23398;&#20064;&#23454;&#29616;&#24378;&#20581;&#30340;&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.09203</link><description>&lt;p&gt;
SiamAF: &#23398;&#20064;&#24515;&#30005;&#22270;&#21644;&#20809;&#30005;&#33033;&#25615;&#22270;&#20449;&#21495;&#30340;&#20849;&#20139;&#20449;&#24687;&#29992;&#20110;&#24378;&#20581;&#30340;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SiamAF: Learning Shared Information from ECG and PPG Signals for Robust Atrial Fibrillation Detection. (arXiv:2310.09203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09203
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SiamAF&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24515;&#30005;&#22270;&#21644;&#20809;&#30005;&#33033;&#25615;&#22270;&#20449;&#21495;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#36890;&#36807;Siamese&#32593;&#32476;&#21644;&#32852;&#21512;&#23398;&#20064;&#23454;&#29616;&#24378;&#20581;&#30340;&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26159;&#26368;&#24120;&#35265;&#30340;&#24515;&#33039;&#24515;&#24459;&#22833;&#24120;&#31867;&#22411;&#65292;&#19982;&#20013;&#39118;&#12289;&#24515;&#21147;&#34928;&#31469;&#21644;&#20854;&#20182;&#24515;&#34880;&#31649;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#22686;&#21152;&#26377;&#20851;&#65292;&#20294;&#21487;&#20197;&#20020;&#24202;&#19978;&#26080;&#22768;&#12290;&#20329;&#25140;&#24335;&#35774;&#22791;&#36827;&#34892;&#34987;&#21160;&#24615;&#30340;AF&#30417;&#27979;&#21487;&#33021;&#26377;&#21161;&#20110;&#20943;&#23569;&#19982;AF&#30456;&#20851;&#30340;&#19981;&#33391;&#20020;&#24202;&#32467;&#26524;&#12290;&#22312;&#22024;&#26434;&#30340;&#20329;&#25140;&#24335;&#25968;&#25454;&#20013;&#26816;&#27979;AF&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#24341;&#21457;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#20808;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#21333;&#19968;&#24418;&#24577;&#23398;&#20064;&#65292;&#35201;&#20040;&#26159;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#65292;&#35201;&#20040;&#26159;&#20809;&#30005;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#29305;&#24449;&#65292;&#24182;&#20381;&#36182;&#20110;&#26356;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#25439;&#22351;&#30340;&#29305;&#24449;&#65292;&#22312;&#26576;&#20123;&#22330;&#26223;&#20013;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36136;&#37327;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#12290;&#37492;&#20110;&#20329;&#25140;&#24335;&#35774;&#22791;&#21644;&#24202;&#36793;&#30417;&#25252;&#20202;&#19978;ECG&#21644;PPG&#20449;&#21495;&#37197;&#23545;&#30340;&#26085;&#30410;&#20016;&#23500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;SiamAF&#65292;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;Siamese&#32593;&#32476;&#32467;&#26500;&#21644;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Atrial fibrillation (AF) is the most common type of cardiac arrhythmia. It is associated with an increased risk of stroke, heart failure, and other cardiovascular complications, but can be clinically silent. Passive AF monitoring with wearables may help reduce adverse clinical outcomes related to AF. Detecting AF in noisy wearable data poses a significant challenge, leading to the emergence of various deep learning techniques. Previous deep learning models learn from a single modality, either electrocardiogram (ECG) or photoplethysmography (PPG) signals. However, deep learning models often struggle to learn generalizable features and rely on features that are more susceptible to corruption from noise, leading to sub-optimal performances in certain scenarios, especially with low-quality signals. Given the increasing availability of ECG and PPG signal pairs from wearables and bedside monitors, we propose a new approach, SiamAF, leveraging a novel Siamese network architecture and joint le
&lt;/p&gt;</description></item><item><title>METRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#65292;&#26088;&#22312;&#20351;&#20854;&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#25193;&#23637;&#12290;&#36825;&#20010;&#30446;&#26631;&#35299;&#20915;&#20102;&#32431;&#25506;&#32034;&#26041;&#27861;&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#29615;&#22659;&#20013;&#30340;&#22256;&#38590;&#20197;&#21450;&#20114;&#20449;&#24687;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#20013;&#32570;&#20047;&#28608;&#21169;&#32780;&#26080;&#27861;&#25506;&#32034;&#29615;&#22659;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08887</link><description>&lt;p&gt;
METRA:&#20855;&#26377;&#24230;&#37327;&#24863;&#30693;&#25277;&#35937;&#30340;&#21487;&#25193;&#23637;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
METRA: Scalable Unsupervised RL with Metric-Aware Abstraction. (arXiv:2310.08887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08887
&lt;/p&gt;
&lt;p&gt;
METRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#65292;&#26088;&#22312;&#20351;&#20854;&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#25193;&#23637;&#12290;&#36825;&#20010;&#30446;&#26631;&#35299;&#20915;&#20102;&#32431;&#25506;&#32034;&#26041;&#27861;&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#29615;&#22659;&#20013;&#30340;&#22256;&#38590;&#20197;&#21450;&#20114;&#20449;&#24687;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#20013;&#32570;&#20047;&#28608;&#21169;&#32780;&#26080;&#27861;&#25506;&#32034;&#29615;&#22659;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#31574;&#30053;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;&#21516;&#26679;&#65292;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26377;&#26395;&#21457;&#29616;&#21508;&#31181;&#28508;&#22312;&#26377;&#29992;&#30340;&#34892;&#20026;&#65292;&#21487;&#20197;&#21152;&#36895;&#23398;&#20064;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20043;&#21069;&#30340;&#23581;&#35797;&#65292;&#20351;&#26080;&#30417;&#30563;RL&#30495;&#27491;&#21487;&#25193;&#23637;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#25361;&#25112;&#65306;&#22312;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#32431;&#25506;&#32034;&#26041;&#27861;&#21487;&#33021;&#20250;&#38754;&#20020;&#22256;&#38590;&#65292;&#22240;&#20026;&#35206;&#30422;&#27599;&#20010;&#21487;&#33021;&#30340;&#36716;&#25442;&#26159;&#19981;&#21487;&#34892;&#30340;&#65307;&#32780;&#20114;&#20449;&#24687;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#28608;&#21169;&#32780;&#23436;&#20840;&#26080;&#27861;&#25506;&#32034;&#29615;&#22659;&#12290;&#20026;&#20102;&#20351;&#26080;&#30417;&#30563;RL&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#25193;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;RL&#30446;&#26631;&#65292;&#31216;&#20026;&#24230;&#37327;&#24863;&#30693;&#25277;&#35937;&#65288;METRA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training strategies have proven to be highly effective in natural language processing and computer vision. Likewise, unsupervised reinforcement learning (RL) holds the promise of discovering a variety of potentially useful behaviors that can accelerate the learning of a wide array of downstream tasks. Previous unsupervised RL approaches have mainly focused on pure exploration and mutual information skill learning. However, despite the previous attempts, making unsupervised RL truly scalable still remains a major open challenge: pure exploration approaches might struggle in complex environments with large state spaces, where covering every possible transition is infeasible, and mutual information skill learning approaches might completely fail to explore the environment due to the lack of incentives. To make unsupervised RL scalable to complex, high-dimensional environments, we propose a novel unsupervised RL objective, which we call Metric-Aware Abstraction (METRA). Ou
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructDET&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#23450;&#20301;&#30446;&#26631;&#23545;&#35937;&#24182;&#36827;&#34892;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#22270;&#20687;&#12289;&#36793;&#30028;&#26694;&#21644;&#27867;&#21270;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2310.05136</link><description>&lt;p&gt;
InstructDET: &#36890;&#29992;&#25351;&#20196;&#30340;&#24341;&#23548;&#19979;&#30340;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#30340;&#22810;&#26679;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InstructDET: Diversifying Referring Object Detection with Generalized Instructions. (arXiv:2310.05136v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05136
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructDET&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#23450;&#20301;&#30446;&#26631;&#23545;&#35937;&#24182;&#36827;&#34892;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#22270;&#20687;&#12289;&#36793;&#30028;&#26694;&#21644;&#27867;&#21270;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;InstructDET&#65292;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#25351;&#20196;&#26469;&#23450;&#20301;&#30446;&#26631;&#23545;&#35937;&#30340;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#65288;ROD&#65289;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#65292;&#28085;&#30422;&#19982;&#23545;&#35937;&#26816;&#27979;&#30456;&#20851;&#30340;&#24120;&#35265;&#29992;&#25143;&#24847;&#22270;&#12290;&#23545;&#20110;&#19968;&#24352;&#22270;&#20687;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#22823;&#37327;&#30340;&#25351;&#20196;&#65292;&#28041;&#21450;&#27599;&#20010;&#21333;&#29420;&#30340;&#23545;&#35937;&#21644;&#22810;&#20010;&#23545;&#35937;&#30340;&#19981;&#21516;&#32452;&#21512;&#12290;&#27599;&#20010;&#25351;&#20196;&#21450;&#20854;&#23545;&#24212;&#30340;&#23545;&#35937;&#36793;&#30028;&#26694;&#26500;&#25104;&#19968;&#20010;&#35757;&#32451;&#25968;&#25454;&#23545;&#12290;&#20026;&#20102;&#21253;&#21547;&#24120;&#35265;&#30340;&#26816;&#27979;&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26032;&#20852;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#21644;&#23545;&#35937;&#36793;&#30028;&#26694;&#29983;&#25104;&#25351;&#20196;&#65292;&#22240;&#20026;&#22522;&#30784;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#20135;&#29983;&#31867;&#20284;&#20154;&#31867;&#30340;&#34920;&#36798;&#65288;&#20363;&#22914;&#65292;&#25551;&#36848;&#23545;&#35937;&#23646;&#24615;&#12289;&#31867;&#21035;&#21644;&#20851;&#31995;&#65289;&#12290;&#25105;&#20204;&#23558;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#21629;&#21517;&#20026;InDET&#65292;&#21253;&#21547;&#22270;&#20687;&#12289;&#36793;&#30028;&#26694;&#21644;&#27867;&#21270;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage are greatly diversified to encompass common user intentions related to object detection. For one image, we produce tremendous instructions that refer to every single object and different combinations of multiple objects. Each instruction and its corresponding object bounding boxes (bbxs) constitute one training data pair. In order to encompass common detection expressions, we involve emerging vision-language model (VLM) and large language model (LLM) to generate instructions guided by text prompts and object bbxs, as the generalizations of foundation models are effective to produce human-like expressions (e.g., describing object property, category, and relationship). We name our constructed dataset as InDET. It contains images, bbxs and generalized instructions that are 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#22270;&#20687;&#21453;&#28436;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#27969;&#27169;&#22411;&#65292;&#22312;&#20943;&#23569;&#25163;&#21160;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04432</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#22270;&#20687;&#21453;&#28436;&#26041;&#27861;&#65306;&#36890;&#36807;&#27969;&#36827;&#34892;
&lt;/p&gt;
&lt;p&gt;
Training-free Linear Image Inversion via Flows. (arXiv:2310.04432v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04432
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#22270;&#20687;&#21453;&#28436;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#27969;&#27169;&#22411;&#65292;&#22312;&#20943;&#23569;&#25163;&#21160;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#21453;&#28436;&#26041;&#27861;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#36866;&#24403;&#20462;&#25913;&#26469;&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35843;&#20248;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#20808;&#21069;&#26041;&#27861;&#24050;&#32463;&#25506;&#32034;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#20294;&#20173;&#38656;&#35201;&#25163;&#21160;&#35843;&#25972;&#35768;&#22810;&#36229;&#21442;&#25968;&#26469;&#24212;&#23545;&#19981;&#21516;&#30340;&#36870;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27969;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#21453;&#28436;&#30340;&#26080;&#38656;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#27969;&#21305;&#37197;&#27169;&#22411;&#30340;&#31616;&#27905;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#20351;&#29992;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#21152;&#26435;&#26041;&#26696;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#25163;&#21160;&#35843;&#25972;&#30340;&#24037;&#20316;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#20027;&#35201;&#28304;&#22836;&#27762;&#21462;&#28789;&#24863;&#65306;&#23558;&#20808;&#21069;&#30340;&#26799;&#24230;&#26657;&#27491;&#26041;&#27861;&#24212;&#29992;&#20110;&#27969;&#39046;&#22495;&#65292;&#20197;&#21450;&#22522;&#20110;&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#36335;&#24452;&#30340;&#27714;&#35299;&#22120;&#26041;&#26696;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#24191;&#27867;&#21487;&#29992;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25193;&#25955;&#27169;&#22411;&#23454;&#38469;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#36870;&#38382;&#39064;&#19978;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training-free linear inversion involves the use of a pretrained generative model and -- through appropriate modifications to the generation process -solving inverse problems without any finetuning of the generative model. While recent prior methods have explored the use of diffusion models, they still require the manual tuning of many hyperparameters for different inverse problems. In this work, we propose a training-free method for image inversion using pretrained flow models, leveraging the simplicity and efficiency of Flow Matching models, using theoretically-justified weighting schemes and thereby significantly reducing the amount of manual tuning. In particular, we draw inspiration from two main sources: adopting prior gradient correction methods to the flow regime, and a solver scheme based on conditional Optimal Transport paths. As pretrained diffusion models are widely accessible, we also show how to practically adapt diffusion models for our method. Empirically, our approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CoHeat&#31639;&#27861;&#65292;&#19968;&#31181;&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#21644;&#20851;&#32852;&#20449;&#24687;&#65292;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#30340;&#20542;&#26012;&#65292;&#24182;&#26377;&#25928;&#22320;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.03813</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#65306;&#22522;&#20110;&#27969;&#34892;&#24230;&#30340;&#32858;&#21512;&#21644;&#35838;&#31243;&#21152;&#28909;
&lt;/p&gt;
&lt;p&gt;
Accurate Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum Heating. (arXiv:2310.03813v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CoHeat&#31639;&#27861;&#65292;&#19968;&#31181;&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#21644;&#20851;&#32852;&#20449;&#24687;&#65292;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#30340;&#20542;&#26012;&#65292;&#24182;&#26377;&#25928;&#22320;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#20934;&#30830;&#22320;&#21521;&#29992;&#25143;&#25512;&#33616;&#20919;&#21551;&#21160;&#25414;&#32465;&#65311;&#25414;&#32465;&#25512;&#33616;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#26032;&#24314;&#25414;&#32465;&#19981;&#26029;&#20986;&#29616;&#20197;&#28385;&#36275;&#21508;&#31181;&#33829;&#38144;&#30446;&#30340;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20043;&#21069;&#27809;&#26377;&#30740;&#31350;&#28041;&#21450;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20919;&#21551;&#21160;&#29289;&#21697;&#25512;&#33616;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#21382;&#21490;&#20449;&#24687;&#65292;&#21363;&#20351;&#23545;&#20110;&#19981;&#21463;&#27426;&#36814;&#30340;&#25414;&#32465;&#20063;&#26159;&#22914;&#27492;&#65292;&#26080;&#27861;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#39640;&#24230;&#20542;&#26012;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoHeat&#65288;&#22522;&#20110;&#27969;&#34892;&#24230;&#30340;&#32858;&#21512;&#21644;&#35838;&#31243;&#21152;&#28909;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#26041;&#27861;&#12290;CoHeat&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#20449;&#24687;&#21644;&#20851;&#32852;&#20449;&#24687;&#26469;&#20272;&#35745;&#29992;&#25143;&#19982;&#25414;&#32465;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#30340;&#39640;&#24230;&#20542;&#26012;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;CoHeat&#36824;&#36890;&#36807;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#32858;&#21512;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#22320;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we accurately recommend cold-start bundles to users? The cold-start problem in bundle recommendation is critical in practical scenarios since new bundles are continuously created for various marketing purposes. Despite its importance, no previous studies have addressed cold-start bundle recommendation. Moreover, existing methods for cold-start item recommendation overly rely on historical information, even for unpopular bundles, failing to tackle the primary challenge of the highly skewed distribution of bundle interactions. In this work, we propose CoHeat (Popularity-based Coalescence and Curriculum Heating), an accurate approach for the cold-start bundle recommendation. CoHeat tackles the highly skewed distribution of bundle interactions by incorporating both historical and affiliation information based on the bundle's popularity when estimating the user-bundle relationship. Furthermore, CoHeat effectively learns latent representations by exploiting curriculum learning and co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65288;DGFS&#65289;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23398;&#20064;&#36807;&#31243;&#20998;&#35299;&#20026;&#30701;&#30340;&#37096;&#20998;&#36712;&#36857;&#27573;&#65292;&#23454;&#29616;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#39640;&#32500;&#23494;&#24230;&#20989;&#25968;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#20013;&#38388;&#30340;&#23398;&#20064;&#20449;&#21495;&#21644;&#38750;&#31574;&#30053;&#25506;&#32034;&#33021;&#21147;&#26469;&#25913;&#21892;&#23398;&#20064;&#20449;&#21495;&#30340;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02679</link><description>&lt;p&gt;
&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65306;&#36890;&#36807;&#37096;&#20998;&#36712;&#36857;&#20248;&#21270;&#25913;&#21892;&#23398;&#20064;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization. (arXiv:2310.02679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02679
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65288;DGFS&#65289;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23398;&#20064;&#36807;&#31243;&#20998;&#35299;&#20026;&#30701;&#30340;&#37096;&#20998;&#36712;&#36857;&#27573;&#65292;&#23454;&#29616;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#39640;&#32500;&#23494;&#24230;&#20989;&#25968;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#20013;&#38388;&#30340;&#23398;&#20064;&#20449;&#21495;&#21644;&#38750;&#31574;&#30053;&#25506;&#32034;&#33021;&#21147;&#26469;&#25913;&#21892;&#23398;&#20064;&#20449;&#21495;&#30340;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#39640;&#32500;&#23494;&#24230;&#20989;&#25968;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25511;&#21046;&#30340;&#38543;&#26426;&#36807;&#31243;&#26469;&#27169;&#25311;&#36825;&#20123;&#30446;&#26631;&#23494;&#24230;&#30340;&#36817;&#20284;&#26679;&#26412;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#35757;&#32451;&#30446;&#26631;&#38656;&#35201;&#35745;&#31639;&#23436;&#25972;&#30340;&#36712;&#36857;&#65292;&#23548;&#33268;&#30001;&#20110;&#20351;&#29992;&#23436;&#25972;&#36712;&#36857;&#21644;&#21482;&#22312;&#32456;&#31471;&#26102;&#38388;&#23384;&#22312;&#30340;&#23398;&#20064;&#20449;&#21495;&#30340;&#20351;&#29992;&#32780;&#20135;&#29983;&#32531;&#24930;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65288;DGFS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#37319;&#26679;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#23398;&#20064;&#36807;&#31243;&#21487;&#34892;&#22320;&#20998;&#35299;&#20026;&#30701;&#30340;&#37096;&#20998;&#36712;&#36857;&#27573;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#27969;&#20989;&#25968;&#8221;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20511;&#37492;&#20102;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#30340;&#29702;&#35770;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#20013;&#38388;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#24182;&#20174;&#38750;&#31574;&#30053;&#25506;&#32034;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of sampling from intractable high-dimensional density functions, a fundamental task that often appears in machine learning and statistics. We extend recent sampling-based approaches that leverage controlled stochastic processes to model approximate samples from these target densities. The main drawback of these approaches is that the training objective requires full trajectories to compute, resulting in sluggish credit assignment issues due to use of entire trajectories and a learning signal present only at the terminal time. In this work, we present Diffusion Generative Flow Samplers (DGFS), a sampling-based framework where the learning process can be tractably broken down into short partial trajectory segments, via parameterizing an additional "flow function". Our method takes inspiration from the theory developed for generative flow networks (GFlowNets), allowing us to make use of intermediate learning signals and benefit from off-policy exploration capabilitie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22312;&#21512;&#25104;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#20869;&#30340;&#24615;&#33021;&#65292;&#20294;&#20250;&#25439;&#23475;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#40657;&#30418;&#35843;&#25972;&#26041;&#27861;&#65288;NMTune&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.17002</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#20943;&#36731;&#39044;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks. (arXiv:2309.17002v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22312;&#21512;&#25104;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#20869;&#30340;&#24615;&#33021;&#65292;&#20294;&#20250;&#25439;&#23475;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#40657;&#30418;&#35843;&#25972;&#26041;&#27861;&#65288;NMTune&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#20808;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26631;&#20934;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22122;&#22768;&#30340;&#24615;&#36136;&#65292;&#24182;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#22122;&#22768;&#30340;ImageNet-1K&#21644;YFCC15M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#20419;&#36827;&#39046;&#22495;&#20869;&#30340;&#36716;&#31227;&#24615;&#33021;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#30456;&#21516;&#30340;&#20998;&#24067;&#65307;&#28982;&#32780;&#65292;&#23427;&#24635;&#26159;&#20250;&#25439;&#23475;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#39044;&#35757;&#32451;&#20013;&#30340;&#22122;&#22768;&#20250;&#19981;&#21516;&#22320;&#22609;&#36896;&#29305;&#24449;&#31354;&#38388;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#40657;&#30418;&#35843;&#25972;&#26041;&#27861;&#65288;NMTune&#65289;&#26469;&#20351;&#29305;&#24449;&#31354;&#38388;&#36798;&#21040;&#26144;&#23556;&#24182;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing data distribution are different. We empirically verify that the reason behind is noise in pre-training shapes the feature space differently. We then propose a lightweight black-box tuning method (NMTune) to affine the feature space to mitigate the m
&lt;/p&gt;</description></item><item><title>ModuLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#12289;&#33021;&#22815;&#22312;&#28040;&#36153;&#32423;GPU&#19978;&#25903;&#25345;3&#27604;&#29305;LLMs&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#27169;&#22359;&#21270;&#37327;&#21270;&#22120;&#30340;&#38598;&#25104;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.16119</link><description>&lt;p&gt;
ModuLoRA:&#36890;&#36807;&#19982;&#27169;&#22359;&#21270;&#37327;&#21270;&#22120;&#38598;&#25104;&#22312;&#28040;&#36153;&#32423;GPU&#19978;&#23545;3 Bit LLMs&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers. (arXiv:2309.16119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16119
&lt;/p&gt;
&lt;p&gt;
ModuLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#12289;&#33021;&#22815;&#22312;&#28040;&#36153;&#32423;GPU&#19978;&#25903;&#25345;3&#27604;&#29305;LLMs&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#27169;&#22359;&#21270;&#37327;&#21270;&#22120;&#30340;&#38598;&#25104;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#31639;&#27861;&#65292;&#21487;&#25903;&#25345;&#22312;&#20165;&#20351;&#29992;1&#20010;48GB GPU&#19978;&#20197;3&#27604;&#29305;&#25110;4&#27604;&#29305;&#31934;&#24230;&#24494;&#35843;&#20855;&#26377;65B&#21442;&#25968;&#30340;LLMs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;&#27169;&#22359;&#21270;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;ModuLoRA&#65289;&#65292;&#36890;&#36807;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#23558;&#20219;&#20309;&#29992;&#25143;&#25351;&#23450;&#30340;&#26435;&#37325;&#37327;&#21270;&#22120;&#19982;&#24494;&#35843;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#37327;&#21270;&#26080;&#20851;&#30340;&#21453;&#21521;&#20256;&#25773;&#65292;&#36890;&#36807;&#33258;&#23450;&#20041;&#30340;&#40657;&#30418;&#37327;&#21270;&#27169;&#22359;&#20174;&#20302;&#31934;&#24230;LLM&#26435;&#37325;&#20013;&#33258;&#36866;&#24212;&#22320;&#29983;&#25104;&#26435;&#37325;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#39318;&#27425;&#33021;&#22815;&#36827;&#34892;3&#27604;&#29305;LLMs&#30340;&#24494;&#35843;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;3&#27604;&#29305;OPTQ&#37327;&#21270;&#24448;&#24448;&#20248;&#20110;&#20381;&#36182;&#20110;&#36739;&#19981;&#22797;&#26434;&#30340;4&#27604;&#29305;&#21644;8&#27604;&#29305;&#26041;&#27861;&#30340;&#24494;&#35843;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;ModuLoRA&#22312;&#25991;&#26412;&#20998;&#31867;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#30340;&#20869;&#23384;&#27604;&#29616;&#26377;&#26041;&#27861;&#23569;&#24456;&#22810;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#25688;&#35201;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;ROUGE&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 3-bit or 4-bit precision on as little as one 48GB GPU. Our method, modular low-rank adaptation (ModuLoRA), integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that adaptively materializes low-precision LLM weights from a custom black-box quantization module. This approach enables finetuning 3-bit LLMs for the first time--leveraging state-of-the-art 3-bit OPTQ quantization often outperforms finetuning that relies on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains competitive performance on text classification, natural language infernece, and instruction following tasks using significantly less memory than existing approaches, and we also surpass the state-of-the-art ROUGE score on a popular summarization task. W
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65288;STARC&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#22870;&#21169;&#23398;&#20064;&#29702;&#35770;&#22522;&#30784;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2309.15257</link><description>&lt;p&gt;
STARC:&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#24046;&#24322;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
STARC: A General Framework For Quantifying Differences Between Reward Functions. (arXiv:2309.15257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15257
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65288;STARC&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#22870;&#21169;&#23398;&#20064;&#29702;&#35770;&#22522;&#30784;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#65292;&#38656;&#35201;&#23558;&#20219;&#21153;&#30340;&#30446;&#26631;&#24418;&#24335;&#21270;&#20026;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#26469;&#35828;&#65292;&#25163;&#21160;&#25351;&#23450;&#19968;&#20010;&#27704;&#19981;&#28608;&#21169;&#19981;&#33391;&#34892;&#20026;&#30340;&#22870;&#21169;&#20989;&#25968;&#38750;&#24120;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#26469;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#22870;&#21169;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#23578;&#26410;&#23436;&#21892;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#24120;&#19981;&#30693;&#36947;&#32473;&#23450;&#30340;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#22312;&#39640;&#27010;&#29575;&#19979;&#26159;&#21542;&#20250;&#23398;&#20064;&#21040;&#19968;&#20010;&#23433;&#20840;&#20248;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#24847;&#21619;&#30528;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#24517;&#39035;&#32463;&#36807;&#32463;&#39564;&#35780;&#20272;&#65292;&#36825;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#24456;&#38590;&#39044;&#27979;&#20854;&#22833;&#25928;&#27169;&#24335;&#12290;&#20854;&#20013;&#19968;&#20010;&#38459;&#30861;&#33719;&#24471;&#26356;&#22909;&#29702;&#35770;&#20445;&#35777;&#30340;&#38556;&#30861;&#26159;&#32570;&#20047;&#36739;&#22909;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a reward function. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use reward learning algorithms, which attempt to learn a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to predict in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for quantifying the difference between reward functions. In this paper we provide a solution t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LyCORIS&#65292;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35780;&#20272;&#30340;&#20840;&#38754;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.14859</link><description>&lt;p&gt;
&#23548;&#33322;&#25991;&#26412;&#21040;&#22270;&#20687;&#23450;&#21046;&#65306;&#20174;LyCORIS&#24494;&#35843;&#21040;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation. (arXiv:2309.14859v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LyCORIS&#65292;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35780;&#20272;&#30340;&#20840;&#38754;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22240;&#20854;&#33021;&#22815;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#20013;&#65292;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#39046;&#20808;&#30340;&#24320;&#28304;&#27169;&#22411;&#22312;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#36825;&#20123;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#32473;&#26032;&#26041;&#27861;&#30340;&#25972;&#21512;&#21644;&#31995;&#32479;&#35780;&#20272;&#24102;&#26469;&#20102;&#22810;&#37325;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LyCORIS&#65288;Lora beYond Conventional methods&#65292;Other Rank adaptation Implementations for Stable diffusion&#65289;[https://github.com/KohakuBlueleaf/LyCORIS]&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35780;&#20272;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#22810;&#26679;&#21270;&#30340;&#25351;&#26631;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;&#24494;&#35843;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#22312;&#19981;&#21516;&#27010;&#24565;&#31867;&#21035;&#19979;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31867;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation. Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion) [https://github.com/KohakuBlueleaf/LyCORIS], an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion. Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categori
&lt;/p&gt;</description></item><item><title>LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11998</link><description>&lt;p&gt;
LMSYS-Chat-1M&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. (arXiv:2309.11998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11998
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#30740;&#31350;&#20154;&#20204;&#22914;&#20309;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19982;&#20854;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LMSYS-Chat-1M&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#19982;25&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#30340;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#25105;&#20204;&#30340;Vicuna&#28436;&#31034;&#21644;Chatbot Arena&#32593;&#31449;&#19978;&#30340;21&#19975;&#20010;&#29420;&#31435;IP&#22320;&#22336;&#20013;&#25910;&#38598;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#20869;&#23481;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#31574;&#21010;&#36807;&#31243;&#12289;&#22522;&#26412;&#32479;&#35745;&#25968;&#25454;&#21644;&#20027;&#39064;&#20998;&#24067;&#65292;&#24378;&#35843;&#20854;&#22810;&#26679;&#24615;&#12289;&#29420;&#29305;&#24615;&#21644;&#35268;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#29992;&#20363;&#23637;&#31034;&#20102;&#23427;&#30340;&#22810;&#26679;&#24615;&#65306;&#24320;&#21457;&#19982;GPT-4&#34920;&#29616;&#30456;&#20284;&#30340;&#20869;&#23481;&#36807;&#28388;&#27169;&#22411;&#12289;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#22522;&#20934;&#12289;&#35757;&#32451;&#19982;Vicuna&#34920;&#29616;&#30456;&#20284;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12289;&#21019;&#24314;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#38382;&#39064;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25105;&#20204;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is pub
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10444</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#33258;&#25105;&#24378;&#21270;&#20197;&#25913;&#36827;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#28041;&#21450;&#23398;&#29983;&#29983;&#25104;&#21644;&#20998;&#20139;&#23398;&#20064;&#36164;&#28304;&#12290;&#22312;&#23398;&#29983;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#21019;&#24314;&#35299;&#37322;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#23545;&#30456;&#20851;&#27010;&#24565;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#24448;&#24448;&#30001;&#20110;&#20027;&#39064;&#29702;&#35299;&#26377;&#38480;&#21644;&#20165;&#20165;&#37325;&#30003;&#38382;&#39064;&#12289;&#24178;&#25200;&#22240;&#32032;&#21644;&#27491;&#30830;&#31572;&#26696;&#30340;&#20542;&#21521;&#32780;&#38590;&#20197;&#32534;&#20889;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24110;&#21161;&#25903;&#25745;&#36825;&#20010;&#20219;&#21153;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#29983;&#25104;&#19982;&#23398;&#29983;&#23545;&#40784;&#30340;&#35299;&#37322;&#65292;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#20197;&#30830;&#20445;&#20854;&#36136;&#37327;&#65292;&#24182;&#36845;&#20195;&#22686;&#24378;&#35299;&#37322;&#12290;&#22914;&#26524;&#19968;&#20010;&#35299;&#37322;&#30340;&#35780;&#20272;&#20998;&#25968;&#20302;&#20110;&#23450;&#20041;&#30340;&#38408;&#20540;&#65292;&#26694;&#26550;&#20250;&#36845;&#20195;&#22320;&#20248;&#21270;&#21644;&#37325;&#26032;&#35780;&#20272;&#35299;&#37322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#27169;&#25311;&#20102;&#19968;&#20010;&#23398;&#29983;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learnersourcing involves students generating and sharing learning resources with their peers. When learnersourcing multiple-choice questions, creating explanations for the generated questions is a crucial step as it facilitates a deeper understanding of the related concepts. However, it is often difficult for students to craft effective explanations due to limited subject understanding and a tendency to merely restate the question stem, distractors, and correct answer. To help scaffold this task, in this work we propose a self-reinforcement large-language-model framework, with the goal of generating and evaluating explanations automatically. Comprising three modules, the framework generates student-aligned explanations, evaluates these explanations to ensure their quality and iteratively enhances the explanations. If an explanation's evaluation score falls below a defined threshold, the framework iteratively refines and reassesses the explanation. Importantly, our framework emulates th
&lt;/p&gt;</description></item><item><title>DoLa&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#23618;&#27425;&#30340;&#36923;&#36753;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20943;&#23569;&#24187;&#35273;&#65292;&#26080;&#38656;&#22806;&#37096;&#30693;&#35782;&#25110;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2309.03883</link><description>&lt;p&gt;
DoLa&#65306;&#36890;&#36807;&#23545;&#27604;&#23618;&#27425;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. (arXiv:2309.03883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03883
&lt;/p&gt;
&lt;p&gt;
DoLa&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#23618;&#27425;&#30340;&#36923;&#36753;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20943;&#23569;&#24187;&#35273;&#65292;&#26080;&#38656;&#22806;&#37096;&#30693;&#35782;&#25110;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#19982;&#39044;&#35757;&#32451;&#26399;&#38388;&#35266;&#23519;&#21040;&#30340;&#20107;&#23454;&#20559;&#31163;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#20943;&#23569;&#39044;&#35757;&#32451;LLMs&#20013;&#30340;&#24187;&#35273;&#65292;&#23427;&#19981;&#38656;&#35201;&#22312;&#26816;&#32034;&#30340;&#22806;&#37096;&#30693;&#35782;&#25110;&#39069;&#22806;&#30340;&#24494;&#35843;&#19978;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23558;&#36739;&#26202;&#23618;&#21644;&#36739;&#26089;&#23618;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#24471;&#21040;&#30340;&#36923;&#36753;&#24046;&#24322;&#26469;&#33719;&#24471;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#20998;&#24067;&#65292;&#21033;&#29992;&#20102;LLMs&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#36890;&#24120;&#34987;&#35777;&#26126;&#23616;&#37096;&#21270;&#22312;&#29305;&#23450;&#30340;Transformer&#23618;&#20013;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#36890;&#36807;&#23545;&#27604;&#23618;&#27425;&#30340;&#35299;&#30721;&#65288;DoLa&#65289;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23637;&#31034;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#20943;&#23569;&#29983;&#25104;&#19981;&#27491;&#30830;&#20107;&#23454;&#30340;&#24773;&#20917;&#12290;DoLa&#22312;&#22810;&#20010;&#36873;&#25321;&#20219;&#21153;&#21644;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#21319;&#20102;&#30495;&#23454;&#24615;&#65292;&#20363;&#22914;&#25913;&#21892;&#20102;LLaMA&#31995;&#21015;&#27169;&#22411;&#22312;TruthfulQA&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA b
&lt;/p&gt;</description></item><item><title>CodeApex&#26159;&#19968;&#20010;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22810;&#20010;&#36873;&#25321;&#39064;&#21644;&#31639;&#27861;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;14&#20010;LLM&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.01940</link><description>&lt;p&gt;
CodeApex&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. (arXiv:2309.01940v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01940
&lt;/p&gt;
&lt;p&gt;
CodeApex&#26159;&#19968;&#20010;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22810;&#20010;&#36873;&#25321;&#39064;&#21644;&#31639;&#27861;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;14&#20010;LLM&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#30340;&#32534;&#31243;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#21560;&#24341;&#20102;&#30740;&#31350;&#20154;&#21592;&#26085;&#30410;&#22686;&#38271;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CodeApex&#65292;&#19968;&#31181;&#21452;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#19987;&#27880;&#20110;LLM&#30340;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;CodeApex&#21253;&#25324;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65306;&#27010;&#24565;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#22810;&#36339;&#25512;&#29702;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#32534;&#31243;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;CodeApex&#21033;&#29992;&#31639;&#27861;&#38382;&#39064;&#21644;&#30456;&#24212;&#30340;&#27979;&#35797;&#29992;&#20363;&#26469;&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#36136;&#37327;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;14&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#65292;&#21253;&#25324;&#36890;&#29992;&#21644;&#19987;&#38376;&#21270;&#27169;&#22411;&#12290;GPT&#23637;&#29616;&#20986;&#26368;&#20339;&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;&#20102;&#32422;50%&#21644;56%&#12290;&#32534;&#31243;&#20219;&#21153;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#25105;&#20204;&#24076;&#26395;CodeApex&#33021;&#22815;&#20026;&#35780;&#20272;&#32534;&#31243;&#33021;&#21147;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers. We propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension and code generation abilities of LLMs. CodeApex comprises three types of multiple-choice questions: conceptual understanding, commonsense reasoning, and multi-hop reasoning, designed to evaluate LLMs on programming comprehension tasks. Additionally, CodeApex utilizes algorithmic questions and corresponding test cases to assess the code quality generated by LLMs. We evaluate 14 state-of-the-art LLMs, including both general-purpose and specialized models. GPT exhibits the best programming capabilities, achieving approximate accuracies of 50% and 56% on the two tasks, respectively. There is still significant room for improvement in programming tasks. We hope that CodeApex can serve as a reference for evaluating the co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AutoDroid&#65292;&#19968;&#20010;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;Android&#24212;&#29992;&#31243;&#24207;&#19978;&#33258;&#21160;&#22788;&#29702;&#20219;&#24847;&#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#32467;&#21512;LLMs&#30340;&#24120;&#35782;&#30693;&#35782;&#21644;&#24212;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#23454;&#29616;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#21160;&#24577;&#20998;&#26512;&#26469;&#23454;&#29616;&#21151;&#33021;&#24847;&#35782;&#30340;UI&#34920;&#31034;&#26041;&#27861;&#21644;&#22522;&#20110;&#25506;&#32034;&#30340;&#20869;&#23384;&#27880;&#20837;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2308.15272</link><description>&lt;p&gt;
&#35753;LLM&#33021;&#22815;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#36827;&#34892;&#26234;&#33021;&#20219;&#21153;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
Empowering LLM to use Smartphone for Intelligent Task Automation. (arXiv:2308.15272v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AutoDroid&#65292;&#19968;&#20010;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;Android&#24212;&#29992;&#31243;&#24207;&#19978;&#33258;&#21160;&#22788;&#29702;&#20219;&#24847;&#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#32467;&#21512;LLMs&#30340;&#24120;&#35782;&#30693;&#35782;&#21644;&#24212;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#23454;&#29616;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#21160;&#24577;&#20998;&#26512;&#26469;&#23454;&#29616;&#21151;&#33021;&#24847;&#35782;&#30340;UI&#34920;&#31034;&#26041;&#27861;&#21644;&#22522;&#20110;&#25506;&#32034;&#30340;&#20869;&#23384;&#27880;&#20837;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#23454;&#29616;&#22522;&#20110;&#35821;&#38899;&#30340;&#20813;&#25552;&#29992;&#25143;&#19982;&#26234;&#33021;&#25163;&#26426;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#30001;&#20110;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#26377;&#38480;&#65292;&#20197;&#21450;&#24320;&#21457;&#20154;&#21592;&#25110;&#32456;&#31471;&#29992;&#25143;&#38656;&#35201;&#20184;&#20986;&#38750;&#24120;&#21162;&#21147;&#30340;&#25163;&#21160;&#24037;&#20316;&#32780;&#23548;&#33268;&#21487;&#25193;&#23637;&#24615;&#24046;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#36827;&#23637;&#28608;&#21457;&#20102;&#25105;&#20204;&#20174;&#27169;&#22411;&#20013;&#24515;&#21270;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#32479;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#20219;&#21153;&#20934;&#22791;&#12289;&#29702;&#35299;&#21644;&#25191;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoDroid&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#20219;&#20309;Android&#24212;&#29992;&#31243;&#24207;&#19978;&#26080;&#38656;&#25163;&#21160;&#24037;&#20316;&#22788;&#29702;&#20219;&#24847;&#20219;&#21153;&#30340;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#31995;&#32479;&#12290;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#21160;&#24577;&#20998;&#26512;&#23558;LLMs&#30340;&#24120;&#35782;&#30693;&#35782;&#19982;&#24212;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30456;&#32467;&#21512;&#12290;&#20027;&#35201;&#32452;&#20214;&#21253;&#25324;&#21151;&#33021;&#24847;&#35782;&#30340;UI&#34920;&#31034;&#26041;&#27861;&#65292;&#26725;&#25509;&#20102;UI&#21644;LLM&#65292;&#22522;&#20110;&#25506;&#32034;&#30340;&#20869;&#23384;&#27880;&#20837;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or end-users. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system that can handle arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection t
&lt;/p&gt;</description></item><item><title>SICNN&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#22522;&#20110;&#27169;&#22411;&#30340;&#36845;&#20195;SIC&#26041;&#27861;&#26469;&#35774;&#35745;&#65292;&#28040;&#38500;&#20102;&#22522;&#20110;&#27169;&#22411;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.12591</link><description>&lt;p&gt;
SICNN: &#21463;&#36719;&#24178;&#25200;&#25269;&#28040;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#22343;&#34913;&#22120;
&lt;/p&gt;
&lt;p&gt;
SICNN: Soft Interference Cancellation Inspired Neural Network Equalizers. (arXiv:2308.12591v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12591
&lt;/p&gt;
&lt;p&gt;
SICNN&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#22522;&#20110;&#27169;&#22411;&#30340;&#36845;&#20195;SIC&#26041;&#27861;&#26469;&#35774;&#35745;&#65292;&#28040;&#38500;&#20102;&#22522;&#20110;&#27169;&#22411;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22343;&#34913;&#26159;&#25968;&#23383;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#25509;&#25910;&#31471;&#30340;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#20256;&#32479;&#19978;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#20272;&#35745;&#26041;&#27861;&#26469;&#36827;&#34892;&#12290;&#22312;&#20247;&#22810;&#36873;&#39033;&#20013;&#65292;&#36845;&#20195;&#36719;&#24178;&#25200;&#25269;&#28040;&#65288;SIC&#65289;&#26159;&#19968;&#31181;&#34920;&#29616;&#33391;&#22909;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#36991;&#20813;&#20102;&#36845;&#20195;&#20272;&#35745;&#36807;&#31243;&#20013;&#30001;&#30828;&#20915;&#31574;&#25968;&#25454;&#31526;&#21495;&#20272;&#35745;&#24341;&#36215;&#30340;&#38169;&#35823;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#23384;&#22312;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#36827;&#34892;&#36924;&#36817;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#22343;&#34913;&#26041;&#27861;&#65292;&#31216;&#20026;SICNN&#65292;&#36890;&#36807;&#23545;&#22522;&#20110;&#27169;&#22411;&#30340;&#36845;&#20195;SIC&#26041;&#27861;&#36827;&#34892;&#28145;&#24230;&#23637;&#24320;&#26469;&#35774;&#35745;&#65292;&#28040;&#38500;&#20102;&#20854;&#22522;&#20110;&#27169;&#22411;&#30340;&#23545;&#24212;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;SICNN&#12290;SICNNv1&#38750;&#24120;&#31867;&#20284;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#19987;&#38376;&#20026;&#21333;&#36733;&#27874;&#39057;&#22495;&#22343;&#34913;&#31995;&#32479;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Equalization is an important task at the receiver side of a digital wireless communication system, which is traditionally conducted with model-based estimation methods. Among the numerous options for model-based equalization, iterative soft interference cancellation (SIC) is a well-performing approach since error propagation caused by hard decision data symbol estimation during the iterative estimation procedure is avoided. However, the model-based method suffers from high computational complexity and performance degradation due to required approximations. In this work, we propose a novel neural network (NN-)based equalization approach, referred to as SICNN, which is designed by deep unfolding of a model-based iterative SIC method, eliminating the main disadvantages of its model-based counterpart. We present different variants of SICNN. SICNNv1 is very similar to the model-based method, and is specifically tailored for single carrier frequency domain equalization systems, which is the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#38598;&#20013;&#20110;&#20102;&#35299;&#22914;&#20309;&#21033;&#29992;LLM&#26469;&#20248;&#21270;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#21644;&#32467;&#26524;&#12290;&#25991;&#31456;&#24635;&#32467;&#20102;&#19981;&#21516;LLM&#30340;&#29305;&#28857;&#21644;&#29992;&#36884;&#20197;&#21450;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10620</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#65306;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Software Engineering: A Systematic Literature Review. (arXiv:2308.10620v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10620
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#38598;&#20013;&#20110;&#20102;&#35299;&#22914;&#20309;&#21033;&#29992;LLM&#26469;&#20248;&#21270;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#21644;&#32467;&#26524;&#12290;&#25991;&#31456;&#24635;&#32467;&#20102;&#19981;&#21516;LLM&#30340;&#29305;&#28857;&#21644;&#29992;&#36884;&#20197;&#21450;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21253;&#25324;&#36719;&#20214;&#24037;&#31243;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#25991;&#29486;&#25506;&#35752;&#20102;LLM&#22312;&#21508;&#31181;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;LLM&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#12289;&#24433;&#21709;&#21644;&#21487;&#33021;&#30340;&#38480;&#21046;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23545;LLM&#19982;&#36719;&#20214;&#24037;&#31243;&#30340;&#20132;&#21449;&#39046;&#22495;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65292;&#29305;&#21035;&#20851;&#27880;LLM&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#22914;&#20309;&#34987;&#21033;&#29992;&#26469;&#20248;&#21270;&#36807;&#31243;&#21644;&#32467;&#26524;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25910;&#38598;&#21644;&#20998;&#26512;&#20102;2017&#24180;&#33267;2023&#24180;&#30340;229&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#20197;&#22238;&#31572;&#22235;&#20010;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#65288;RQs&#65289;&#12290;&#22312;RQ1&#20013;&#65292;&#25105;&#20204;&#23545;&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;LLM&#36827;&#34892;&#20998;&#31867;&#21644;&#27604;&#36739;&#20998;&#26512;&#65292;&#25551;&#32472;&#20854;&#29420;&#29305;&#30340;&#29305;&#28857;&#21644;&#29992;&#36884;&#12290;&#22312;RQ2&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#25968;&#25454;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#24378;&#22823;&#12289;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#25104;&#21151;&#21033;&#29992;LLM&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks and applications. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review on the intersection of LLMs and SE, with a particular focus on understanding how LLMs can be exploited in SE to optimize processes and outcomes. We collect and analyze a total of 229 research papers from 2017 to 2023 to answer four key research questions (RQs). In RQ1, we categorize and provide a comparative analysis of different LLMs that have been employed in SE tasks, characterising their distinctive features and uses. In RQ2, we analyse the methods used in data collection, preprocessing, and application highlighting the role of robust, well-curated datasets for successful LLM for S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#22522;&#20110;&#22068;&#21767;-&#38899;&#32032;&#23383;&#32423;&#30456;&#20851;&#24615;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#30340;&#20004;&#31181;&#26032;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#20934;&#30830;&#23545;&#40784;&#38899;&#39057;&#21644;&#35270;&#39057;&#27969;&#65292;&#24182;&#19988;&#20805;&#20998;&#21033;&#29992;&#27169;&#24577;&#20114;&#34917;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08488</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#22068;&#21767;-&#38899;&#32032;&#23383;&#32423;&#30456;&#20851;&#24615;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder. (arXiv:2308.08488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#22522;&#20110;&#22068;&#21767;-&#38899;&#32032;&#23383;&#32423;&#30456;&#20851;&#24615;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#30340;&#20004;&#31181;&#26032;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#20934;&#30830;&#23545;&#40784;&#38899;&#39057;&#21644;&#35270;&#39057;&#27969;&#65292;&#24182;&#19988;&#20805;&#20998;&#21033;&#29992;&#27169;&#24577;&#20114;&#34917;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#35266;&#23519;&#21040;&#65292;&#22312;&#20302;&#36136;&#37327;&#35270;&#39057;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#19979;&#65292;&#20174;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21040;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#30053;&#26377;&#25913;&#36827;&#12290;&#25454;&#35748;&#20026;&#65292;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#20043;&#38388;&#19981;&#21305;&#37197;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#19987;&#38376;&#30340;&#36755;&#20837;&#34920;&#31034;&#23548;&#33268;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#65288;AVSR&#65289;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#35757;&#32451;&#26694;&#26550;&#19979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26222;&#36890;&#35805;&#20013;&#22068;&#21767;&#24418;&#29366;&#21644;&#38899;&#33410;&#32423;&#38899;&#32032;&#23383;&#21333;&#20803;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#24314;&#31435;&#20934;&#30830;&#30340;&#24103;&#32423;&#38899;&#33410;&#36793;&#30028;&#12290;&#36825;&#20351;&#24471;&#22312;&#35270;&#35273;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#36807;&#31243;&#20013;&#33021;&#22815;&#23545;&#40784;&#35270;&#39057;&#21644;&#38899;&#39057;&#27969;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#39057;&#24341;&#23548;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#65288;CMFE&#65289;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#20027;&#35201;&#35757;&#32451;&#21442;&#25968;&#26469;&#23454;&#29616;&#22810;&#20010;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#23618;&#30340;&#20805;&#20998;&#21033;&#29992;&#27169;&#24577;&#20114;&#34917;&#24615;&#12290;&#22312;&#23454;&#39564;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
In recent research, slight performance improvement is observed from automatic speech recognition systems to audio-visual speech recognition systems in the end-to-end framework with low-quality videos. Unmatching convergence rates and specialized input representations between audio and visual modalities are considered to cause the problem. In this paper, we propose two novel techniques to improve audio-visual speech recognition (AVSR) under a pre-training and fine-tuning training framework. First, we explore the correlation between lip shapes and syllable-level subword units in Mandarin to establish good frame-level syllable boundaries from lip shapes. This enables accurate alignment of video and audio streams during visual model pre-training and cross-modal fusion. Next, we propose an audio-guided cross-modal fusion encoder (CMFE) neural network to utilize main training parameters for multiple cross-modal attention layers to make full use of modality complementarity. Experiments on the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.13709</link><description>&lt;p&gt;
&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65306;&#22312;&#27809;&#26377;&#20855;&#20307;&#35780;&#20215;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#29289;&#21697;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#35768;&#22810;&#23646;&#24615;&#65292;&#22914;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#21487;&#21462;&#24615;&#25110;&#24378;&#24230;&#65292;&#26080;&#27861;&#30452;&#25509;&#35266;&#27979;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20272;&#35745;&#24050;&#30693;&#29289;&#21697;&#30340;&#36825;&#20123;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#20986;&#29616;&#22312;&#37197;&#23545;&#27604;&#36739;&#25968;&#25454;&#38598;&#20013;&#30340;&#36816;&#21160;&#21592;&#30340;&#23454;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#20219;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#32541;&#22320;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#36825;&#20010;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#19981;&#20844;&#24179;&#24615;&#30340;&#38750;&#23545;&#31216;&#29615;&#22659;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26356;&#20026;&#24120;&#35265;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;DBTR&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.11949</link><description>&lt;p&gt;
HIQL: &#20197;&#28508;&#22312;&#29366;&#24577;&#20316;&#20026;&#21160;&#20316;&#30340;&#31163;&#32447;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HIQL: Offline Goal-Conditioned RL with Latent States as Actions. (arXiv:2307.11949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26368;&#36817;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30707;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#28508;&#22312;&#22320;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#65288;&#26080;&#22870;&#21169;&#65289;&#25968;&#25454;&#65292;&#25552;&#20379;&#31867;&#20284;&#20110;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#30452;&#25509;&#20174;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20934;&#30830;&#20272;&#35745;&#36828;&#26399;&#30446;&#26631;&#30340;&#20215;&#20540;&#20989;&#25968;&#24456;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#21363;&#36798;&#21040;&#36828;&#26399;&#30446;&#26631;&#38656;&#35201;&#39318;&#20808;&#36890;&#36807;&#36739;&#36817;&#23376;&#30446;&#26631;&#12290;&#36825;&#31181;&#32467;&#26500;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#35780;&#20272;&#37051;&#36817;&#30446;&#26631;&#30340;&#21160;&#20316;&#36136;&#37327;&#36890;&#24120;&#27604;&#26356;&#36828;&#30446;&#26631;&#23481;&#26131;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#24819;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#12290;&#21033;&#29992;&#19968;&#20010;&#27809;&#26377;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#65306;&#19968;&#20010;&#39640;&#23618;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.09476</link><description>&lt;p&gt;
&#36807;&#24230;&#24605;&#32771;&#30495;&#30456;&#65306;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09476
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#36827;&#34892;&#22797;&#26434;&#27169;&#24335;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27169;&#20223;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#37325;&#29616;&#19981;&#20934;&#30830;&#25110;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#26469;&#30740;&#31350;&#26377;&#23475;&#30340;&#27169;&#20223;&#65292;&#24182;&#30830;&#23450;&#20102;&#20004;&#20010;&#30456;&#20851;&#29616;&#35937;&#65306;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#12290;&#31532;&#19968;&#20010;&#29616;&#35937;&#65292;&#36807;&#24230;&#24605;&#32771;&#65292;&#22312;&#32473;&#20986;&#27491;&#30830;&#19982;&#38169;&#35823;&#30340;&#23569;&#37327;&#31034;&#33539;&#26102;&#65292;&#25105;&#20204;&#20174;&#20013;&#38388;&#23618;&#35299;&#30721;&#39044;&#27979;&#12290;&#22312;&#26089;&#26399;&#23618;&#20013;&#65292;&#20004;&#31181;&#31034;&#33539;&#24341;&#36215;&#20102;&#30456;&#20284;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#20294;&#22312;&#26576;&#20010;&#8220;&#20851;&#38190;&#23618;&#8221;&#20043;&#21518;&#65292;&#32473;&#20986;&#38169;&#35823;&#31034;&#33539;&#30340;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#12290;&#31532;&#20108;&#20010;&#29616;&#35937;&#65292;&#38169;&#35823;&#24402;&#32435;&#22836;&#65292;&#21487;&#33021;&#26159;&#36807;&#24230;&#24605;&#32771;&#30340;&#19968;&#31181;&#26426;&#21046;&#24615;&#21407;&#22240;&#65306;&#36825;&#20123;&#26159;&#20301;&#20110;&#36739;&#26202;&#23618;&#30340;&#22836;&#37096;&#65292;&#23427;&#20204;&#20851;&#27880;&#24182;&#22797;&#21046;&#20808;&#21069;&#31034;&#33539;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20854;&#21066;&#24369;&#20250;&#20943;&#23569;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;NetHack&#28216;&#25103;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#21457;&#29616;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#21487;&#20197;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#24314;&#31435;&#20102;&#35757;&#32451;&#35745;&#31639;&#26368;&#20248;IL&#20195;&#29702;&#20154;&#30340;&#24130;&#24459;&#12290;</title><link>http://arxiv.org/abs/2307.09423</link><description>&lt;p&gt;
&#22312;NetHack&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#30340;&#35268;&#27169;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Imitation Learning in NetHack. (arXiv:2307.09423v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;NetHack&#28216;&#25103;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#21457;&#29616;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#21487;&#20197;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#24314;&#31435;&#20102;&#35757;&#32451;&#35745;&#31639;&#26368;&#20248;IL&#20195;&#29702;&#20154;&#30340;&#24130;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064; (IL) &#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#24378;&#22823;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#21457;&#29616;&#23427;&#24448;&#24448;&#19981;&#33021;&#23436;&#20840;&#24674;&#22797;&#20986;&#28508;&#22312;&#30340;&#19987;&#23478;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#27809;&#26377;&#28145;&#20837;&#25506;&#31350;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#25193;&#22823;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#21463;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#39046;&#22495;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#22312;&#37027;&#37324;&#8220;&#25193;&#22823;&#35268;&#27169;&#8221;&#24050;&#32463;&#23548;&#33268;&#20102;&#36234;&#26469;&#36234;&#26377;&#33021;&#21147;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411; (LLMs)&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20180;&#32454;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#26159;&#21542;&#21487;&#20197;&#22312;&#27169;&#20223;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#24102;&#26469;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312; NetHack &#28216;&#25103;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#31243;&#24207;&#29983;&#25104;&#12289;&#38543;&#26426;&#24615;&#12289;&#38271;&#26399;&#20381;&#36182;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#21457;&#29616; IL &#30340;&#25439;&#22833;&#21644;&#24179;&#22343;&#22238;&#25253;&#38543;&#30528;&#35745;&#31639;&#39044;&#31639;&#30340;&#21464;&#21270;&#32780;&#24179;&#28369;&#21464;&#21270;&#19988;&#24378;&#30456;&#20851;&#65292;&#20174;&#32780;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#26679;&#26412;&#25968;&#37327;&#26041;&#38754;&#20026;&#35757;&#32451;&#35745;&#31639;&#26368;&#20248;&#30340; IL &#20195;&#29702;&#20154;&#30340;&#35745;&#31639;&#39044;&#31639;&#24314;&#31435;&#20102;&#24130;&#24459;&#12290;&#25105;&#20204;&#39044;&#27979;&#24182;&#35757;&#32451;&#20102;&#20960;&#20010;&#20855;&#26377; IL &#30340;NetHack&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, while powerful, many works find it is often not able to fully recover the underlying expert behavior. However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) where "scaling up" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting. To demonstrate our findings, we focus on the game of NetHack, a challenging environment featuring procedural generation, stochasticity, long-term dependencies, and partial observability. We find IL loss and mean return scale smoothly with the compute budget and are strongly correlated, resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples. We forecast and train several NetHack agents with IL an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;FedDure&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#21644;&#32454;&#35843;&#33410;&#22120;&#23545;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#36827;&#34892;&#35268;&#33539;&#65292;&#20197;&#21450;&#23398;&#20064;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2307.05358</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#35843;&#33410;&#22120;&#35299;&#20915;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators. (arXiv:2307.05358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;FedDure&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#21644;&#32454;&#35843;&#33410;&#22120;&#23545;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#36827;&#34892;&#35268;&#33539;&#65292;&#20197;&#21450;&#23398;&#20064;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20174;&#20998;&#25955;&#24322;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#30001;&#20110;&#20998;&#25955;&#23458;&#25143;&#31471;&#19978;&#26631;&#31614;&#31232;&#32570;&#65292;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;FSSL&#65289;&#20986;&#29616;&#20197;&#20174;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#20013;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FSSL&#26041;&#27861;&#20551;&#35774;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#26631;&#31614;&#25968;&#25454;&#29420;&#31435;&#19988;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#23458;&#25143;&#31471;&#20869;&#37096;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#20855;&#26377;&#19968;&#33268;&#30340;&#31867;&#21035;&#20998;&#24067;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;FSSL&#30340;&#26356;&#23454;&#38469;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#21363;&#25968;&#25454;&#20998;&#24067;&#19981;&#20165;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#19981;&#21516;&#65292;&#22312;&#23458;&#25143;&#31471;&#20869;&#37096;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#20063;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;FSSL&#26694;&#26550;&#65292;FedDure&#12290;FedDure&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#65288;C-reg&#65289;&#21644;&#32454;&#35843;&#33410;&#22120;&#65288;F-reg&#65289;&#35299;&#38500;&#20102;&#20197;&#21069;&#30340;&#20551;&#35774;&#65306;C-reg&#36890;&#36807;&#36319;&#36394;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#23398;&#20064;&#25928;&#26524;&#26469;&#35268;&#33539;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#65307;F-reg&#23398;&#20064;&#19968;&#20010;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#20197;&#36866;&#24212;&#23458;&#25143;&#31471;&#20869;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has become a popular method to learn from decentralized heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train models from a small fraction of labeled data due to label scarcity on decentralized clients. Existing FSSL methods assume independent and identically distributed (IID) labeled data across clients and consistent class distribution between labeled and unlabeled data within a client. This work studies a more practical and challenging scenario of FSSL, where data distribution is different not only across clients but also within a client between labeled and unlabeled data. To address this challenge, we propose a novel FSSL framework with dual regulators, FedDure.} FedDure lifts the previous assumption with a coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg regularizes the updating of the local model by tracking the learning effect on labeled data distribution; F-reg learns an adaptive weighting scheme tailored f
&lt;/p&gt;</description></item><item><title>DeepOnto&#26159;&#19968;&#20010;Python&#21253;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;&#26412;&#20307;API&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24037;&#20855;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.03067</link><description>&lt;p&gt;
DeepOnto: &#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
DeepOnto: A Python Package for Ontology Engineering with Deep Learning. (arXiv:2307.03067v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03067
&lt;/p&gt;
&lt;p&gt;
DeepOnto&#26159;&#19968;&#20010;Python&#21253;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;&#26412;&#20307;API&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24037;&#20855;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#22312;&#26412;&#20307;&#24037;&#31243;&#20013;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22914;PyTorch&#21644;Tensorflow&#20027;&#35201;&#26159;&#20026;Python&#24320;&#21457;&#30340;&#65292;&#32780;&#24191;&#27867;&#20351;&#29992;&#30340;&#26412;&#20307;API&#65288;&#22914;OWL API&#21644;Jena&#65289;&#20027;&#35201;&#26159;&#22522;&#20110;Java&#30340;&#12290;&#20026;&#20102;&#26041;&#20415;&#26080;&#32541;&#38598;&#25104;&#36825;&#20123;&#26694;&#26550;&#21644;API&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deeponto&#65292;&#19968;&#20010;&#19987;&#20026;&#26412;&#20307;&#24037;&#31243;&#35774;&#35745;&#30340;Python&#21253;&#12290;&#35813;&#21253;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#35748;&#21487;&#21644;&#21487;&#38752;&#30340;OWL API&#30340;&#26680;&#24515;&#26412;&#20307;&#22788;&#29702;&#27169;&#22359;&#65292;&#20197;&#26356;&#8220;Pythonic&#8221;&#30340;&#26041;&#24335;&#23553;&#35013;&#20854;&#22522;&#26412;&#29305;&#24615;&#65292;&#24182;&#25193;&#23637;&#20854;&#21151;&#33021;&#20197;&#21253;&#25324;&#20854;&#20182;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#25512;&#29702;&#12289;&#35821;&#35328;&#21270;&#12289;&#35268;&#33539;&#21270;&#12289;&#25237;&#24433;&#31561;&#12290;&#22522;&#20110;&#36825;&#20010;&#27169;&#22359;&#65292;Deeponto&#25552;&#20379;&#20102;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#21508;&#31181;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#20363;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying deep learning techniques, particularly language models (LMs), in ontology engineering has raised widespread attention. However, deep learning frameworks like PyTorch and Tensorflow are predominantly developed for Python programming, while widely-used ontology APIs, such as the OWL API and Jena, are primarily Java-based. To facilitate seamless integration of these frameworks and APIs, we present Deeponto, a Python package designed for ontology engineering. The package encompasses a core ontology processing module founded on the widely-recognised and reliable OWL API, encapsulating its fundamental features in a more "Pythonic" manner and extending its capabilities to include other essential components including reasoning, verbalisation, normalisation, projection, and more. Building on this module, Deeponto offers a suite of tools, resources, and algorithms that support various ontology engineering tasks, such as ontology alignment and completion, by harnessing deep learning meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#20316;&#20026;&#20004;&#31181;&#27169;&#24577;&#36827;&#34892;&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22840;&#24352;&#26816;&#27979;&#30340;&#36328;&#39046;&#22495;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00209</link><description>&lt;p&gt;
&#22270;&#20687;&#30340;&#37325;&#35201;&#24615;&#65306;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection. (arXiv:2307.00209v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#20316;&#20026;&#20004;&#31181;&#27169;&#24577;&#36827;&#34892;&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22840;&#24352;&#26816;&#27979;&#30340;&#36328;&#39046;&#22495;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22840;&#24352;&#65292;&#21363;&#22840;&#22823;&#20854;&#35789;&#65292;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35821;&#35328;&#29616;&#35937;&#12290;&#22840;&#24352;&#26816;&#27979;&#26159;&#29702;&#35299;&#20154;&#31867;&#34920;&#36798;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#24050;&#32463;&#26377;&#20960;&#39033;&#20851;&#20110;&#22840;&#24352;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#20294;&#22823;&#22810;&#25968;&#30340;&#30740;&#31350;&#21482;&#20851;&#27880;&#25991;&#26412;&#27169;&#24577;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#21457;&#23637;&#65292;&#20154;&#20204;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#27169;&#24577;&#65288;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#31561;&#65289;&#26469;&#34920;&#36798;&#22840;&#24352;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#12290;&#25105;&#20204;&#20174;&#24494;&#21338;&#65288;&#20013;&#22269;&#30340;&#19968;&#31181;&#31038;&#20132;&#23186;&#20307;&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#19968;&#20123;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#24494;&#21338;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#35270;&#20026;&#20004;&#31181;&#27169;&#24577;&#65292;&#25506;&#32034;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#22312;&#22840;&#24352;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#22312;&#36825;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#20116;&#20010;&#19981;&#21516;&#30340;&#20027;&#39064;&#26500;&#24314;&#30340;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbole, or exaggeration, is a common linguistic phenomenon. The detection of hyperbole is an important part of understanding human expression. There have been several studies on hyperbole detection, but most of which focus on text modality only. However, with the development of social media, people can create hyperbolic expressions with various modalities, including text, images, videos, etc. In this paper, we focus on multimodal hyperbole detection. We create a multimodal detection dataset\footnote{The dataset will be released to the community.} from Weibo (a Chinese social media) and carry out some studies on it. We treat the text and image from a piece of weibo as two modalities and explore the role of text and image for hyperbole detection. Different pre-trained multimodal encoders are also evaluated on this downstream task to show their performance. Besides, since this dataset is constructed from five different topics, we also evaluate the cross-domain performance of different 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#25351;&#26631;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25269;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#25351;&#26631;&#30340;&#19981;&#36275;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#25351;&#26631;&#33021;&#22815;&#21306;&#20998;&#27169;&#22411;&#23545;&#29305;&#23450;&#23041;&#32961;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16614</link><description>&lt;p&gt;
&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65306;&#29616;&#23454;&#19990;&#30028;&#20013;&#23450;&#21046;&#40065;&#26834;&#24615;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Group-based Robustness: A General Framework for Customized Robustness in the Real World. (arXiv:2306.16614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#25351;&#26631;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25269;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#25351;&#26631;&#30340;&#19981;&#36275;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#25351;&#26631;&#33021;&#22815;&#21306;&#20998;&#27169;&#22411;&#23545;&#29305;&#23450;&#23041;&#32961;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#36867;&#36991;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#36890;&#36807;&#25200;&#21160;&#27169;&#22411;&#36755;&#20837;&#26469;&#24341;&#36215;&#38169;&#35823;&#20998;&#31867;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#24230;&#37327;&#30446;&#26631;&#21644;&#38750;&#30446;&#26631;&#40065;&#26834;&#24615;&#30340;&#25351;&#26631;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#30495;&#23454;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#23427;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#26356;&#36866;&#21512;&#35780;&#20272;&#29305;&#23450;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#33021;&#22815;&#22312;&#20256;&#32479;&#30340;&#40065;&#26834;&#24615;&#25351;&#26631;&#19981;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#23545;&#29305;&#23450;&#23041;&#32961;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26377;&#25928;&#20934;&#30830;&#22320;&#34913;&#37327;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learning models are known to be vulnerable to evasion attacks that perturb model inputs to induce misclassifications. In this work, we identify real-world scenarios where the true threat cannot be assessed accurately by existing attacks. Specifically, we find that conventional metrics measuring targeted and untargeted robustness do not appropriately reflect a model's ability to withstand attacks from one set of source classes to another set of target classes. To address the shortcomings of existing methods, we formally define a new metric, termed group-based robustness, that complements existing metrics and is better-suited for evaluating model performance in certain attack scenarios. We show empirically that group-based robustness allows us to distinguish between models' vulnerability against specific threat models in situations where traditional robustness metrics do not apply. Moreover, to measure group-based robustness efficiently and accurately, we 1) propose two loss func
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21367;&#31215;&#36890;&#36947;&#20132;&#25442;&#28151;&#21512;&#31574;&#30053;&#65288;Swap&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#24314;&#31569;&#32467;&#26500;&#30340;&#21521;&#37327;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#30456;&#37051;&#25110;&#23545;&#35282;&#29305;&#24449;&#20132;&#26367;&#20132;&#25442;&#24182;&#28151;&#21512;&#19981;&#21516;&#36890;&#36947;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#38598;&#25104;&#23616;&#37096;&#29305;&#24449;&#31354;&#38388;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#32452;&#30340;&#21442;&#25968;&#20849;&#20139;&#26426;&#21046;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#27169;&#22411;&#20013;&#30340;&#20887;&#20313;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.15035</link><description>&lt;p&gt;
&#20351;&#29992;Swap&#20248;&#21270;&#24314;&#31569;&#32467;&#26500;&#30340;&#21521;&#37327;&#21270;&#65306;&#39640;&#25928;&#21367;&#31215;&#36890;&#36947;&#20132;&#25442;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Optimized Vectorizing of Building Structures with Swap: High-Efficiency Convolutional Channel-Swap Hybridization Strategy. (arXiv:2306.15035v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21367;&#31215;&#36890;&#36947;&#20132;&#25442;&#28151;&#21512;&#31574;&#30053;&#65288;Swap&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#24314;&#31569;&#32467;&#26500;&#30340;&#21521;&#37327;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#30456;&#37051;&#25110;&#23545;&#35282;&#29305;&#24449;&#20132;&#26367;&#20132;&#25442;&#24182;&#28151;&#21512;&#19981;&#21516;&#36890;&#36947;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#38598;&#25104;&#23616;&#37096;&#29305;&#24449;&#31354;&#38388;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#32452;&#30340;&#21442;&#25968;&#20849;&#20139;&#26426;&#21046;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#27169;&#22411;&#20013;&#30340;&#20887;&#20313;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#24179;&#38754;&#22270;&#30340;&#37325;&#24314;&#65292;&#20063;&#31216;&#20026;&#36275;&#36857;&#37325;&#24314;&#65292;&#23646;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22320;&#29702;&#20449;&#24687;&#23398;&#39046;&#22495;&#65292;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#20256;&#32479;&#21367;&#31215;&#27169;&#22411;&#20013;&#20887;&#20313;&#21442;&#25968;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#21644;&#33258;&#36866;&#24212;&#30340;&#31227;&#20301;&#26550;&#26500;&#65292;&#21363;Swap&#25805;&#20316;&#65292;&#23427;&#32467;&#21512;&#20102;&#38750;&#25351;&#25968;&#22686;&#38271;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#38598;&#25104;&#23616;&#37096;&#29305;&#24449;&#31354;&#38388;&#20449;&#24687;&#30340;&#31867;&#20284;&#21151;&#33021;&#65292;&#31867;&#20284;&#20110;&#39640;&#32500;&#21367;&#31215;&#25805;&#20316;&#22120;&#12290;Swap&#36328;&#36890;&#36947;&#25805;&#20316;&#26550;&#26500;&#36890;&#36807;&#24322;&#25110;&#25805;&#20316;&#20132;&#26367;&#20132;&#25442;&#30456;&#37051;&#25110;&#23545;&#35282;&#29305;&#24449;&#65292;&#28982;&#21518;&#36890;&#36807;1x1&#21367;&#31215;&#25805;&#20316;&#28151;&#21512;&#20132;&#26367;&#30340;&#36890;&#36947;&#65292;&#20197;&#25972;&#21512;&#19981;&#21516;&#36890;&#36947;&#30340;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;SwapNN&#26550;&#26500;&#37319;&#29992;&#20102;&#21463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36807;&#31243;&#21551;&#21457;&#30340;&#22522;&#20110;&#32452;&#30340;&#21442;&#25968;&#20849;&#20139;&#26426;&#21046;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The building planar graph reconstruction, a.k.a. footprint reconstruction, which lies in the domain of computer vision and geoinformatics, has been long afflicted with the challenge of redundant parameters in conventional convolutional models. Therefore, in this paper, we proposed an advanced and adaptive shift architecture, namely the Swap operation, which incorporates non-exponential growth parameters while retaining analogous functionalities to integrate local feature spatial information, resembling a high-dimensional convolution operator. The Swap, cross-channel operation, architecture implements the XOR operation to alternately exchange adjacent or diagonal features, and then blends alternating channels through a 1x1 convolution operation to consolidate information from different channels. The SwapNN architecture, on the other hand, incorporates a group-based parameter-sharing mechanism inspired by the convolutional neural network process and thereby significantly reducing the num
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#31216;&#20026;BeMap&#65292;&#26088;&#22312;&#35299;&#20915;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#20559;&#24046;&#25918;&#22823;&#38382;&#39064;&#65292;&#36890;&#36807;&#24179;&#34913;&#24863;&#30693;&#30340;&#37319;&#26679;&#31574;&#30053;&#26469;&#24179;&#34913;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;1-hop&#37051;&#23621;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.04107</link><description>&lt;p&gt;
BeMap&#65306;&#24179;&#34913;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#29992;&#20110;&#20844;&#24179;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
BeMap: Balanced Message Passing for Fair Graph Neural Network. (arXiv:2306.04107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#31216;&#20026;BeMap&#65292;&#26088;&#22312;&#35299;&#20915;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#20559;&#24046;&#25918;&#22823;&#38382;&#39064;&#65292;&#36890;&#36807;&#24179;&#34913;&#24863;&#30693;&#30340;&#37319;&#26679;&#31574;&#30053;&#26469;&#24179;&#34913;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;1-hop&#37051;&#23621;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36890;&#36807;&#36845;&#20195;&#22320;&#32858;&#21512;&#27599;&#20010;&#33410;&#28857;&#30340;&#23616;&#37096;&#37051;&#22495;&#20449;&#24687;&#26469;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#21363;&#28040;&#24687;&#20256;&#36882;&#12290;&#28982;&#32780;&#65292;&#20855;&#20307;&#35777;&#25454;&#26174;&#31034;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#23545;&#26576;&#20123;&#20154;&#21475;&#32676;&#20307;&#23384;&#22312;&#20559;&#35265;&#65292;&#36825;&#35201;&#27714;&#32771;&#34385;&#31639;&#27861;&#30340;&#20844;&#27491;&#24615;&#12290;&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#21162;&#21147;&#22312;&#20445;&#35777;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#35757;&#32451;&#26399;&#38388;&#24448;&#24448;&#24182;&#19981;&#26126;&#30830;&#32771;&#34385;&#28040;&#24687;&#20256;&#36882;&#22312;GNN&#20013;&#24341;&#36215;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#20559;&#24046;&#25918;&#22823;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#35777;&#25454;&#21644;&#29702;&#35770;&#35777;&#26126;&#65292;&#24403;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;1-hop&#37051;&#23621;&#19981;&#24179;&#34913;&#26102;&#65292;&#28040;&#24687;&#20256;&#36882;&#21487;&#33021;&#20250;&#25918;&#22823;&#20559;&#24046;&#12290;&#22312;&#36825;&#20123;&#20998;&#26512;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BeMap&#65292;&#19968;&#31181;&#20844;&#24179;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#21033;&#29992;&#24179;&#34913;&#24863;&#30693;&#30340;&#37319;&#26679;&#31574;&#30053;&#26469;&#24179;&#34913;&#27599;&#20010;&#33410;&#28857;&#30340;1-hop&#37051;&#23621;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network (GNN) has shown strong empirical performance in many downstream tasks by iteratively aggregating information from the local neighborhood of each node, i.e., message passing. However, concrete evidence has revealed that a graph neural network could be biased against certain demographic groups, which calls for the consideration of algorithmic fairness. Despite the increasing efforts in ensuring algorithmic fairness on graph neural networks, they often do not explicitly consider the induced bias caused by message passing in GNN during training. In this paper, we first investigate the problem of bias amplification in message passing. We empirically and theoretically demonstrate that message passing could amplify the bias when the 1-hop neighbors from different demographic groups are unbalanced. Guided by such analyses, we propose BeMap, a fair message passing method, that leverages a balance-aware sampling strategy to balance the number of the 1-hop neighbors of each n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00010</link><description>&lt;p&gt;
&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explainability in Simplicial Map Neural Networks. (arXiv:2306.00010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#26159;&#22522;&#20110;&#25299;&#25169;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#26222;&#36866;&#36924;&#36817;&#33021;&#21147;&#21644;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#20013;&#24212;&#29992; SMNN &#23384;&#22312;&#19968;&#20123;&#29942;&#39048;&#65292;&#39318;&#20808;&#27809;&#26377;&#23450;&#20041; SMNN &#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#27425;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#38598;&#38656;&#35201;&#26500;&#24314;&#19968;&#20010;&#21253;&#22260;&#20984;&#22810;&#38754;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#23376;&#38598;&#21644;&#25237;&#24433;&#21040;&#36229;&#29699;&#38754;&#30340;&#26041;&#27861;&#20316;&#20026;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340; SMNN &#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simplicial map neural networks (SMNNs) are topology-based neural networks with interesting properties such as universal approximation capability and robustness to adversarial examples under appropriate conditions. However, SMNNs present some bottlenecks for their possible application in high dimensions. First, no SMNN training process has been defined so far. Second, SMNNs require the construction of a convex polytope surrounding the input dataset. In this paper, we propose a SMNN training procedure based on a support subset of the given dataset and a method based on projection to a hypersphere as a replacement for the convex polytope construction. In addition, the explainability capacity of SMNNs is also introduced for the first time in this paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#37327;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#39044;&#27979;&#35823;&#24046;&#30340;&#20998;&#31867;&#26041;&#27861;&#65288;PEC&#65289;&#12290;&#23545;PEC&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;PEC&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#65292;&#24182;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#20248;&#21183;&#65292;&#20363;&#22914;&#26679;&#26412;&#25928;&#29575;&#39640;&#12289;&#26131;&#20110;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2305.18806</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#27979;&#35823;&#24046;&#30340;&#22686;&#37327;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prediction Error-based Classification for Class-Incremental Learning. (arXiv:2305.18806v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#37327;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#39044;&#27979;&#35823;&#24046;&#30340;&#20998;&#31867;&#26041;&#27861;&#65288;PEC&#65289;&#12290;&#23545;PEC&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;PEC&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#65292;&#24182;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#20248;&#21183;&#65292;&#20363;&#22914;&#26679;&#26412;&#25928;&#29575;&#39640;&#12289;&#26131;&#20110;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#20998;&#31867;&#26159;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#26469;&#21306;&#20998;&#25152;&#26377;&#31867;&#21035;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#37327;&#20998;&#31867;&#26102;&#23481;&#26131;&#20986;&#29616;&#36807;&#24230;&#36951;&#24536;&#21644;&#20998;&#25968;&#19981;&#22343;&#34913;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21517;&#20026;&#39044;&#27979;&#35823;&#24046;&#20998;&#31867;&#65288;PEC&#65289;&#65292;&#23427;&#19982;&#20256;&#32479;&#30340;&#21028;&#21035;&#21644;&#29983;&#25104;&#20998;&#31867;&#33539;&#24335;&#26377;&#25152;&#19981;&#21516;&#12290;PEC&#36890;&#36807;&#27979;&#37327;&#27169;&#22411;&#22312;&#20174;&#35813;&#31867;&#21035;&#20013;&#23398;&#20064;&#30340;&#25968;&#25454;&#19978;&#22797;&#21046;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#39044;&#27979;&#35823;&#24046;&#26469;&#35745;&#31639;&#31867;&#21035;&#24471;&#20998;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#20026;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#21518;&#39564;&#26041;&#24046;&#30340;&#20998;&#31867;&#35268;&#21017;&#30340;&#36817;&#20284;&#12290;PEC&#20855;&#26377;&#20960;&#20010;&#23454;&#38469;&#20248;&#21183;&#65292;&#21253;&#25324;&#26679;&#26412;&#25928;&#29575;&#39640;&#12289;&#26131;&#20110;&#35843;&#25972;&#20197;&#21450;&#21363;&#20351;&#22312;&#36880;&#20010;&#21576;&#29616;&#25968;&#25454;&#26102;&#20063;&#24456;&#26377;&#25928;&#12290;&#26412;&#25991;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;PEC&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning (CIL) is a particularly challenging variant of continual learning, where the goal is to learn to discriminate between all classes presented in an incremental fashion. Existing approaches often suffer from excessive forgetting and imbalance of the scores assigned to classes that have not been seen together during training. In this study, we introduce a novel approach, Prediction Error-based Classification (PEC), which differs from traditional discriminative and generative classification paradigms. PEC computes a class score by measuring the prediction error of a model trained to replicate the outputs of a frozen random neural network on data from that class. The method can be interpreted as approximating a classification rule based on Gaussian Process posterior variance. PEC offers several practical advantages, including sample efficiency, ease of tuning, and effectiveness even when data are presented one class at a time. Our empirical results show that PEC pe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#21644;&#35268;&#33539;&#21270;&#23494;&#24230;&#22330;&#26469;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18766</link><description>&lt;p&gt;
HiFA: &#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#21450;&#20854;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance. (arXiv:2305.18766v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#21644;&#35268;&#33539;&#21270;&#23494;&#24230;&#22330;&#26469;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;3D&#27169;&#22411;&#65292;&#33258;&#21160;&#25991;&#26412;&#21040;3D&#21512;&#25104;&#22312;&#25552;&#21319;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#65292;&#25552;&#20379;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#30340;2D&#28210;&#26579;&#24471;&#20998;&#24182;&#29992;&#20110;&#20248;&#21270;NeRFs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#23545;3D&#20960;&#20309;&#30340;&#26377;&#38480;&#29702;&#35299;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#36935;&#21040;&#22810;&#20010;&#35270;&#35282;&#19978;&#30340;&#20266;&#24433;&#21644;&#19981;&#19968;&#33268;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#20808;&#39564;&#37325;&#26032;&#21046;&#23450;&#20248;&#21270;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#37322;&#25918;&#20102;&#25193;&#25955;&#20808;&#39564;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#65292;&#25105;&#20204;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#65292;&#24182;&#35268;&#33539;&#21270;NeRF&#30340;&#23494;&#24230;&#22330;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic text-to-3D synthesis has achieved remarkable advancements through the optimization of 3D models. Existing methods commonly rely on pre-trained text-to-image generative models, such as diffusion models, providing scores for 2D renderings of Neural Radiance Fields (NeRFs) and being utilized for optimizing NeRFs. However, these methods often encounter artifacts and inconsistencies across multiple views due to their limited understanding of 3D geometry. To address these limitations, we propose a reformulation of the optimization loss using the diffusion prior. Furthermore, we introduce a novel training approach that unlocks the potential of the diffusion prior. To improve 3D geometry representation, we apply auxiliary depth supervision for NeRF-rendered images and regularize the density field of NeRFs. Extensive experiments demonstrate the superiority of our method over prior works, resulting in advanced photo-realism and improved multi-view consistency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#29615;&#26694;&#26550;&#65292;&#21363;LLMs&#20316;&#20026;&#24037;&#20855;&#21046;&#36896;&#32773;&#65288;LATM&#65289;&#65292;&#20351;LLMs&#33021;&#22815;&#33258;&#20027;&#22320;&#21019;&#24314;&#29992;&#20110;&#35299;&#20915;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#32780;&#19981;&#38656;&#35201;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#22806;&#37096;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.17126</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24037;&#20855;&#21046;&#36896;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Tool Makers. (arXiv:2305.17126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#29615;&#26694;&#26550;&#65292;&#21363;LLMs&#20316;&#20026;&#24037;&#20855;&#21046;&#36896;&#32773;&#65288;LATM&#65289;&#65292;&#20351;LLMs&#33021;&#22815;&#33258;&#20027;&#22320;&#21019;&#24314;&#29992;&#20110;&#35299;&#20915;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#32780;&#19981;&#38656;&#35201;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#22806;&#37096;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22686;&#24378;&#20854;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#26041;&#38754;&#30340;&#20808;&#21069;&#24037;&#20316;&#20381;&#36182;&#20110;&#29616;&#26377;&#24037;&#20855;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#29615;&#26694;&#26550;&#65292;&#31216;&#20026;LLMs As Tool Makers&#65288;LATM&#65289;&#65292;&#20197;&#28040;&#38500;&#36825;&#31181;&#20381;&#36182;&#24615;&#65292;&#20854;&#20013;LLMs&#21019;&#24314;&#33258;&#24049;&#30340;&#21487;&#37325;&#29992;&#24037;&#20855;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#38454;&#27573;&#65306;1&#65289;&#21046;&#36896;&#24037;&#20855;&#65306;LLM&#20316;&#20026;&#24037;&#20855;&#21046;&#36896;&#32773;&#65292;&#20026;&#32473;&#23450;&#20219;&#21153;&#21046;&#20316;&#24037;&#20855;&#65292;&#20854;&#20013;&#24037;&#20855;&#20316;&#20026;Python&#23454;&#29992;&#20989;&#25968;&#23454;&#29616;&#12290;2&#65289;&#20351;&#29992;&#24037;&#20855;&#65306;LLM&#20316;&#20026;&#24037;&#20855;&#29992;&#25143;&#65292;&#24212;&#29992;&#24037;&#20855;&#21046;&#36896;&#32773;&#26500;&#24314;&#30340;&#24037;&#20855;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#24037;&#20855;&#29992;&#25143;&#21487;&#20197;&#26159;&#19982;&#24037;&#20855;&#21046;&#36896;&#32773;&#30456;&#21516;&#25110;&#19981;&#21516;&#30340;LLM&#12290;&#24037;&#20855;&#21046;&#36896;&#20351;LLM&#33021;&#22815;&#19981;&#26029;&#29983;&#25104;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#35831;&#27714;&#30340;&#24037;&#20855;&#65292;&#20197;&#20415;&#23558;&#26469;&#35831;&#27714;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#33021;&#35843;&#29992;&#30456;&#24212;&#30340;API&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research shows the potential of enhancing the problem-solving ability of large language models (LLMs) through the use of external tools. However, prior work along this line depends on the availability of existing tools. In this work, we take an initial step towards removing this dependency by proposing a closed-loop framework, referred to as LLMs As Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two key phases: 1) tool making: an LLM acts as the tool maker that crafts tools for given tasks, where a tool is implemented as a Python utility function. 2) tool using: an LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. The tool user can be either the same or a different LLM from the tool maker. Tool-making enables an LLM to continually generate tools that can be applied to different requests so that future requests can call the corresponding APIs when beneficial for solving the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30772;&#35299;ChatGPT&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#30772;&#35299;&#25552;&#31034;&#21487;&#20197;&#22312;40&#31181;&#29992;&#20363;&#24773;&#20917;&#19979;&#19968;&#33268;&#22320;&#35268;&#36991;&#38480;&#21046;&#65292;&#24378;&#35843;&#20102;&#25552;&#31034;&#32467;&#26500;&#22312;&#30772;&#35299;ChatGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13860</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30772;&#35299;ChatGPT&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study. (arXiv:2305.13860v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#30772;&#35299;ChatGPT&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#30772;&#35299;&#25552;&#31034;&#21487;&#20197;&#22312;40&#31181;&#29992;&#20363;&#24773;&#20917;&#19979;&#19968;&#33268;&#22320;&#35268;&#36991;&#38480;&#21046;&#65292;&#24378;&#35843;&#20102;&#25552;&#31034;&#32467;&#26500;&#22312;&#30772;&#35299;ChatGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#19982;&#20869;&#23481;&#32422;&#26463;&#21644;&#28508;&#22312;&#28389;&#29992;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#31350;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#21487;&#20197;&#29992;&#22810;&#23569;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31867;&#22411;&#30772;&#35299;LLMs&#65292;&#65288;2&#65289;&#30772;&#35299;&#25552;&#31034;&#22312;&#35268;&#36991;LLM&#38480;&#21046;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#65288;3&#65289;ChatGPT&#23545;&#36825;&#20123;&#30772;&#35299;&#25552;&#31034;&#30340;&#38887;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#31867;&#27169;&#22411;&#26469;&#20998;&#26512;&#29616;&#26377;&#25552;&#31034;&#30340;&#20998;&#24067;&#65292;&#35782;&#21035;&#20986;&#21313;&#20010;&#19981;&#21516;&#27169;&#24335;&#21644;&#19977;&#20010;&#30772;&#35299;&#25552;&#31034;&#31867;&#21035;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;3,120&#20010;&#31105;&#27490;&#24773;&#26223;&#19979;&#30340;&#29425;&#20013;&#38382;&#39064;&#25968;&#25454;&#38598;&#35780;&#20272;ChatGPT 3.5&#21644;4.0&#29256;&#26412;&#30340;&#30772;&#35299;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#23545;&#30772;&#35299;&#25552;&#31034;&#30340;&#25269;&#25239;&#21147;&#65292;&#21457;&#29616;&#25552;&#31034;&#21487;&#20197;&#22312;40&#31181;&#29992;&#20363;&#24773;&#26223;&#19979;&#19968;&#33268;&#22320;&#35268;&#36991;&#38480;&#21046;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#25552;&#31034;&#32467;&#26500;&#22312;&#30772;&#35299;ChatGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;LLMs&#23545;&#24847;&#22806;&#28389;&#29992;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. Our study investigates three key research questions: (1) the number of different prompt types that can jailbreak LLMs, (2) the effectiveness of jailbreak prompts in circumventing LLM constraints, and (3) the resilience of ChatGPT against these jailbreak prompts. Initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. Subsequently, we assess the jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. Finally, we evaluate the resistance of ChatGPT against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. The study underscores the importance of prompt structures in j
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#21363;&#39532;&#23572;&#21487;&#22827; $\alpha$-&#21183;&#21338;&#24328;&#12290;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#35745;&#31639;&#20854;&#20013;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#19988;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.12553</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;:&#22343;&#34913;&#36817;&#20284;&#19982;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Markov $\alpha$-Potential Games: Equilibrium Approximation and Regret Analysis. (arXiv:2305.12553v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#21363;&#39532;&#23572;&#21487;&#22827; $\alpha$-&#21183;&#21338;&#24328;&#12290;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#35745;&#31639;&#20854;&#20013;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#19988;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30740;&#31350;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#22810;&#20195;&#29702;&#20132;&#20114;&#30340;&#26032;&#26694;&#26550;:&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;&#12290;&#39532;&#23572;&#21487;&#22827;&#21183;&#21338;&#24328;&#26159;&#39532;&#23572;&#21487;&#22827; $\alpha$-&#21183;&#21338;&#24328;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#39532;&#23572;&#21487;&#22827;&#25317;&#22581;&#21338;&#24328;&#21644;&#25200;&#21160;&#39532;&#23572;&#21487;&#22827;&#22242;&#38431;&#21338;&#24328;&#26159;&#20004;&#20010;&#37325;&#35201;&#19988;&#23454;&#38469;&#24847;&#20041;&#37325;&#22823;&#30340;&#21338;&#24328;&#31867;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20004;&#20010;&#21338;&#24328;&#30340;$\alpha$-&#21183;&#20989;&#25968;&#65292;&#24182;&#38024;&#23545;&#21338;&#24328;&#21442;&#25968;&#34920;&#24449;&#20102;&#24046;&#36317; $\alpha$&#12290;&#24341;&#20837;&#20102;&#20004;&#31181;&#31639;&#27861;&#8212;&#8212;&#25237;&#24433;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#21644;&#39034;&#24207;&#26368;&#22823;&#25913;&#36827;&#24179;&#28369;&#26368;&#20339;&#21453;&#24212;&#21160;&#24577;&#8212;&#8212;&#26469;&#36817;&#20284;&#35745;&#31639;&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;&#20013;&#30340;&#31283;&#24577;&#32435;&#20160;&#22343;&#34913;&#12290;&#27599;&#20010;&#31639;&#27861;&#30340;&#32435;&#20160;&#36951;&#25022;&#37117;&#26174;&#31034;&#20026;&#26102;&#38388;&#36328;&#24230;&#30340;&#20122;&#32447;&#24615;&#32553;&#25918;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;&#30340;&#36817;&#20284;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new framework to study multi-agent interaction in Markov games: Markov $\alpha$-potential games. Markov potential games are special cases of Markov $\alpha$-potential games, so are two important and practically significant classes of games: Markov congestion games and perturbed Markov team games. In this paper, {$\alpha$-potential} functions for both games are provided and the gap $\alpha$ is characterized with respect to game parameters. Two algorithms -- the projected gradient-ascent algorithm and the sequential maximum improvement smoothed best response dynamics -- are introduced for approximating the stationary Nash equilibrium in Markov $\alpha$-potential games. The Nash-regret for each algorithm is shown to scale sub-linearly in time horizon. Our analysis and numerical experiments demonstrates that simple algorithms are capable of finding approximate equilibrium in Markov $\alpha$-potential games.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#38376;&#25915;&#20987;&#19968;&#30452;&#26159;&#19968;&#20010;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#25913;&#36827;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#25915;&#20987;&#32773;&#33021;&#21147;&#65292;&#22312;&#33021;&#21147;&#21463;&#38480;&#22330;&#26223;&#19979;&#36824;&#27809;&#26377;&#25214;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#27492;&#22806;&#65292;&#27169;&#22411;&#40065;&#26834;&#24615;&#20173;&#28982;&#20540;&#24471;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2304.10985</link><description>&lt;p&gt;
&#22312;&#33021;&#21147;&#21463;&#38480;&#22330;&#26223;&#19979;&#21551;&#21160;&#24378;&#38887;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Launching a Robust Backdoor Attack under Capability Constrained Scenarios. (arXiv:2304.10985v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10985
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#38376;&#25915;&#20987;&#19968;&#30452;&#26159;&#19968;&#20010;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#25913;&#36827;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#25915;&#20987;&#32773;&#33021;&#21147;&#65292;&#22312;&#33021;&#21147;&#21463;&#38480;&#22330;&#26223;&#19979;&#36824;&#27809;&#26377;&#25214;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#27492;&#22806;&#65292;&#27169;&#22411;&#40065;&#26834;&#24615;&#20173;&#28982;&#20540;&#24471;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20851;&#38190;&#39046;&#22495;&#30340;&#24212;&#29992;&#19981;&#26029;&#22686;&#21152;&#65292;&#20154;&#20204;&#24320;&#22987;&#25285;&#24515;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#12290;&#30001;&#20110;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#27745;&#26579;&#30340;&#21518;&#38376;&#27169;&#22411;&#22312;&#26222;&#36890;&#29615;&#22659;&#19979;&#21487;&#33021;&#34920;&#29616;&#27491;&#24120;&#65292;&#20294;&#24403;&#36755;&#20837;&#21253;&#21547;&#35302;&#21457;&#22120;&#26102;&#65292;&#20250;&#26174;&#31034;&#20986;&#24694;&#24847;&#34892;&#20026;&#12290;&#30446;&#21069;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#25913;&#21892;&#35302;&#21457;&#22120;&#30340;&#31192;&#23494;&#24615;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#25915;&#20987;&#32773;&#33021;&#21147;&#65292;&#20363;&#22914;&#23545;&#27169;&#22411;&#32467;&#26500;&#30340;&#20102;&#35299;&#25110;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#25511;&#21046;&#12290;&#30001;&#20110;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#25915;&#20987;&#32773;&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#20123;&#25915;&#20987;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#33976;&#39311;&#24120;&#29992;&#20110;&#31616;&#21270;&#27169;&#22411;&#22823;&#23567;&#65292;&#20294;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20197;&#21069;&#30340;&#35768;&#22810;&#21518;&#38376;&#25915;&#20987;&#22312;&#27169;&#22411;&#33976;&#39311;&#21518;&#22343;&#22833;&#36133;;&#22270;&#20687;&#22686;&#24378;&#25805;&#20316;&#21487;&#20197;&#30772;&#22351;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#20351;&#21518;&#38376;&#25915;&#20987;&#22833;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks continue to be used in critical domains, concerns over their security have emerged. Deep learning models are vulnerable to backdoor attacks due to the lack of transparency. A poisoned backdoor model may perform normally in routine environments, but exhibit malicious behavior when the input contains a trigger. Current research on backdoor attacks focuses on improving the stealthiness of triggers, and most approaches require strong attacker capabilities, such as knowledge of the model structure or control over the training process. These attacks are impractical since in most cases the attacker's capabilities are limited. Additionally, the issue of model robustness has not received adequate attention. For instance, model distillation is commonly used to streamline model size as the number of parameters grows exponentially, and most of previous backdoor attacks failed after model distillation; the image augmentation operations can destroy the trigger and thus disabl
&lt;/p&gt;</description></item><item><title>ReelFramer&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#31038;&#20132;&#23186;&#20307;&#20849;&#21516;&#21019;&#20316;&#26032;&#38395;&#29255;&#27573;&#12290;&#23427;&#21487;&#20197;&#24110;&#21161;&#35760;&#32773;&#25506;&#32034;&#19968;&#20010;&#25925;&#20107;&#30340;&#22810;&#31181;&#21465;&#20107;&#26694;&#26550;&#65292;&#24182;&#29983;&#25104;&#33050;&#26412;&#12289;&#35282;&#33394;&#26495;&#21644;&#25925;&#20107;&#26495;&#12290;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616;&#35813;&#31995;&#32479;&#22823;&#22823;&#20943;&#36731;&#20102;&#23558;&#19968;&#31687;&#20070;&#38754;&#25253;&#36947;&#36716;&#21270;&#20026;&#26032;&#38395;&#29255;&#27573;&#30340;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2304.09653</link><description>&lt;p&gt;
ReelFramer&#65306;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#31038;&#20132;&#23186;&#20307;&#20849;&#21516;&#21019;&#20316;&#26032;&#38395;&#29255;&#27573;
&lt;/p&gt;
&lt;p&gt;
ReelFramer: Co-creating News Reels on Social Media with Generative AI. (arXiv:2304.09653v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09653
&lt;/p&gt;
&lt;p&gt;
ReelFramer&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#31038;&#20132;&#23186;&#20307;&#20849;&#21516;&#21019;&#20316;&#26032;&#38395;&#29255;&#27573;&#12290;&#23427;&#21487;&#20197;&#24110;&#21161;&#35760;&#32773;&#25506;&#32034;&#19968;&#20010;&#25925;&#20107;&#30340;&#22810;&#31181;&#21465;&#20107;&#26694;&#26550;&#65292;&#24182;&#29983;&#25104;&#33050;&#26412;&#12289;&#35282;&#33394;&#26495;&#21644;&#25925;&#20107;&#26495;&#12290;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616;&#35813;&#31995;&#32479;&#22823;&#22823;&#20943;&#36731;&#20102;&#23558;&#19968;&#31687;&#20070;&#38754;&#25253;&#36947;&#36716;&#21270;&#20026;&#26032;&#38395;&#29255;&#27573;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#30701;&#35270;&#39057;&#26159;&#35768;&#22810;&#24180;&#36731;&#20154;&#21457;&#29616;&#21644;&#28040;&#36153;&#20869;&#23481;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#26032;&#38395;&#26426;&#26500;&#24076;&#26395;&#36890;&#36807;&#26032;&#38395;&#29255;&#27573;&#25509;&#35302;&#21463;&#20247;&#65292;&#20294;&#30446;&#21069;&#38590;&#20197;&#23558;&#20256;&#32479;&#30340;&#26032;&#38395;&#25253;&#36947;&#26684;&#24335;&#36716;&#21270;&#20026;&#19982;&#24179;&#21488;&#39118;&#26684;&#30456;&#21305;&#37197;&#30340;&#30701;&#23567;&#26377;&#36259;&#35270;&#39057;&#12290;&#26377;&#22810;&#31181;&#26041;&#27861;&#21487;&#20197;&#22260;&#32469;&#26032;&#38395;&#20107;&#20214;&#26500;&#24314;&#29255;&#27573;&#24335;&#21465;&#20107;&#65292;&#32780;&#36873;&#23450;&#20854;&#20013;&#19968;&#31181;&#21017;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19981;&#21516;&#30340;&#26032;&#38395;&#25925;&#20107;&#38656;&#35201;&#19981;&#21516;&#30340;&#21465;&#36848;&#26694;&#26550;&#65292;&#24182;&#38656;&#35201;&#22312;&#23089;&#20048;&#24615;&#21644;&#20449;&#24687;&#37327;&#20043;&#38388;&#36798;&#21040;&#19981;&#21516;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReelFramer&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#26469;&#24110;&#21161;&#35760;&#32773;&#25506;&#32034;&#19968;&#20010;&#25925;&#20107;&#30340;&#22810;&#31181;&#21465;&#20107;&#26694;&#26550;&#65292;&#28982;&#21518;&#29983;&#25104;&#20182;&#20204;&#21487;&#20197;&#32534;&#36753;&#21644;&#36845;&#20195;&#30340;&#33050;&#26412;&#12289;&#35282;&#33394;&#26495;&#21644;&#25925;&#20107;&#26495;&#12290;&#22312;&#19968;&#39033;&#30001;&#20116;&#21517;&#26032;&#38395;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#29983;&#21442;&#19982;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#31995;&#32479;&#22823;&#22823;&#20943;&#36731;&#20102;&#23558;&#19968;&#31687;&#20070;&#38754;&#25253;&#36947;&#36716;&#21270;&#20026;&#26032;&#38395;&#29255;&#27573;&#30340;&#36127;&#25285;&#65292;&#24182;&#25506;&#32034;&#21465;&#20107;&#26694;&#26550;&#20197;&#25214;&#21040;&#27491;&#30830;&#30340;&#26694;&#26550;&#36807;&#31243;&#26159;&#38750;&#24120;&#26377;&#24847;&#20041;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short videos on social media are a prime way many young people find and consume content. News outlets would like to reach audiences through news reels, but currently struggle to translate traditional journalistic formats into the short, entertaining videos that match the style of the platform. There are many ways to frame a reel-style narrative around a news story, and selecting one is a challenge. Different news stories call for different framings, and require a different trade-off between entertainment and information. We present a system called ReelFramer that uses text and image generation to help journalists explore multiple narrative framings for a story, then generate scripts, character boards and storyboards they can edit and iterate on. A user study of five graduate students in journalism-related fields found the system greatly eased the burden of transforming a written story into a reel, and that exploring framings to find the right one was a rewarding process.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#12289;&#22522;&#20110;&#38170;&#28857;&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;LAAT&#65292;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#19979;&#22686;&#24378;&#22270;&#20687;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20339;&#29366;&#24577;&#23545;&#25239;&#24615;&#19968;&#27425;&#24615;&#26041;&#27861;&#65292;&#21516;&#26102;&#33021;&#20026;&#27969;&#34892;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#24102;&#26469;&#23454;&#36136;&#24615;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.13096</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#38170;&#28857;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Language-Driven Anchors for Zero-Shot Adversarial Robustness. (arXiv:2301.13096v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#12289;&#22522;&#20110;&#38170;&#28857;&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;LAAT&#65292;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#19979;&#22686;&#24378;&#22270;&#20687;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20339;&#29366;&#24577;&#23545;&#25239;&#24615;&#19968;&#27425;&#24615;&#26041;&#27861;&#65292;&#21516;&#26102;&#33021;&#20026;&#27969;&#34892;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#24102;&#26469;&#23454;&#36136;&#24615;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#21892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#12289;&#22522;&#20110;&#38170;&#28857;&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;LAAT&#12290;LAAT&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#20026;&#27599;&#20010;&#31867;&#21035;&#29983;&#25104;&#22266;&#23450;&#30340;&#38170;&#28857;&#65288;&#24402;&#19968;&#21270;&#29305;&#24449;&#23884;&#20837;&#65289;&#65292;&#24182;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#20351;&#29992;&#36825;&#20123;&#38170;&#28857;&#12290;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;LAAT&#21487;&#20197;&#22686;&#24378;&#22270;&#20687;&#27169;&#22411;&#22312;&#26032;&#31867;&#21035;&#19978;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#26679;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#26368;&#36817;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#20960;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#23427;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LAAT&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20339;&#29366;&#24577;&#23545;&#25239;&#24615;&#19968;&#27425;&#24615;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20026;&#27969;&#34892;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#65288;&#22914;ResNet-50&#21644;DenseNet-121&#65289;&#20135;&#29983;&#20102;&#23454;&#36136;&#24615;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are known to be susceptible to adversarial attacks. In this work, we focus on improving adversarial robustness in the challenging zero-shot image classification setting. To address this issue, we propose LAAT, a novel Language-driven, Anchor-based Adversarial Training strategy. LAAT utilizes a text encoder to generate fixed anchors (normalized feature embeddings) for each category and then uses these anchors for adversarial training. By leveraging the semantic consistency of the text encoders, LAAT can enhance the adversarial robustness of the image model on novel categories without additional examples. We identify the large cosine similarity problem of recent text encoders and design several effective techniques to address it. The experimental results demonstrate that LAAT significantly improves zero-shot adversarial performance, outperforming previous state-of-the-art adversarially robust one-shot methods. Moreover, our method produces substantial zero-shot adver
&lt;/p&gt;</description></item><item><title>CAPE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#65292;&#20351;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#26356;&#22810;&#20219;&#21153;&#65292;&#24182;&#25913;&#21892;&#20102;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.09935</link><description>&lt;p&gt;
CAPE: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
CAPE: Corrective Actions from Precondition Errors using Large Language Models. (arXiv:2211.09935v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09935
&lt;/p&gt;
&lt;p&gt;
CAPE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#65292;&#20351;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#26356;&#22810;&#20219;&#21153;&#65292;&#24182;&#25913;&#21892;&#20102;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24120;&#35782;&#30693;&#35782;&#20026;&#35774;&#35745;&#26234;&#33021;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#31181;&#36884;&#24452;&#12290;&#29616;&#26377;&#30340;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#30340;&#26041;&#27861;&#22312;&#34892;&#21160;&#22833;&#36133;&#26102;&#26080;&#27861;&#24674;&#22797;&#65292;&#24182;&#19988;&#36890;&#24120;&#21482;&#33021;&#23581;&#35797;&#37325;&#26032;&#25191;&#34892;&#22833;&#36133;&#30340;&#34892;&#21160;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#38169;&#35823;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65288;CAPE&#65289;&#65292;&#35797;&#22270;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#25552;&#20986;&#32416;&#27491;&#21069;&#32622;&#26465;&#20214;&#38169;&#35823;&#30340;&#34892;&#21160;&#12290;CAPE&#36890;&#36807;&#21033;&#29992;&#23569;&#26679;&#26412;&#25512;&#29702;&#20174;&#34892;&#21160;&#21069;&#32622;&#26465;&#20214;&#20013;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22810;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#30830;&#20445;&#35821;&#20041;&#27491;&#30830;&#24615;&#21644;&#26368;&#23567;&#21270;&#37325;&#26032;&#25552;&#31034;&#12290;&#22312;VirtualHome&#20013;&#65292;CAPE&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#24182;&#19988;&#30456;&#27604;SayCan&#65292;&#23558;&#20154;&#24037;&#26631;&#27880;&#30340;&#35745;&#21010;&#27491;&#30830;&#24230;&#25351;&#26631;&#20174;28.89%&#25552;&#39640;&#21040;49.63%&#12290;&#25105;&#20204;&#30340;&#25913;&#36827;&#20063;&#36866;&#29992;&#20110;&#19968;&#21488;&#37197;&#32622;&#20102;&#19968;&#32452;&#20197;&#35821;&#35328;&#20026;&#25351;&#23450;&#30340;&#25216;&#33021;&#21644;&#30456;&#20851;&#21069;&#32622;&#26465;&#20214;&#30340;&#27874;&#22763;&#39039;&#21160;&#21147;&#20844;&#21496;&#30340;Spot&#26426;&#22120;&#20154;&#65292;&#20854;&#20013;CAPE&#25552;&#39640;&#20102;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting commonsense knowledge from a large language model (LLM) offers a path to designing intelligent robots. Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause.  We propose a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans by leveraging few-shot reasoning from action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while ensuring semantic correctness and minimizing re-prompting. In VirtualHome, CAPE generates executable plans while improving a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves the correctne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Wise-SrNet&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23398;&#20064;&#29305;&#24449;&#22270;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#35299;&#20915;&#20102;&#36830;&#25509;&#29305;&#24449;&#22270;&#21644;&#20998;&#31867;&#23618;&#20043;&#38388;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2104.12294</link><description>&lt;p&gt;
Wise-SrNet: &#19968;&#31181;&#22686;&#24378;&#22270;&#20687;&#20998;&#31867;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#24449;&#22270;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Wise-SrNet: A Novel Architecture for Enhancing Image Classification by Learning Spatial Resolution of Feature Maps. (arXiv:2104.12294v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.12294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Wise-SrNet&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23398;&#20064;&#29305;&#24449;&#22270;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#35299;&#20915;&#20102;&#36830;&#25509;&#29305;&#24449;&#22270;&#21644;&#20998;&#31867;&#23618;&#20043;&#38388;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#20197;&#26469;&#65292;&#23558;&#25552;&#21462;&#30340;&#29305;&#24449;&#22270;&#19982;&#26368;&#32456;&#30340;&#20998;&#31867;&#23618;&#36830;&#25509;&#36215;&#26469;&#19968;&#30452;&#26159;&#20027;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;VGG&#27169;&#22411;&#20351;&#29992;&#20004;&#32452;&#20840;&#36830;&#25509;&#23618;&#29992;&#20110;&#26550;&#26500;&#30340;&#20998;&#31867;&#37096;&#20998;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#27169;&#22411;&#26435;&#37325;&#30340;&#25968;&#37327;&#12290;ResNet&#31561;&#28145;&#24230;&#21367;&#31215;&#27169;&#22411;&#20351;&#29992;&#20840;&#23616;&#24179;&#22343;&#27744;&#21270;&#65288;GAP&#65289;&#23618;&#23558;&#29305;&#24449;&#22270;&#21387;&#32553;&#24182;&#36755;&#20837;&#20998;&#31867;&#23618;&#12290;&#23613;&#31649;&#20351;&#29992;GAP&#23618;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#20063;&#20250;&#23548;&#33268;&#29305;&#24449;&#22270;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#25439;&#22833;&#65292;&#20174;&#32780;&#38477;&#20302;&#23398;&#20064;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Wise-SrNet&#30340;&#26032;&#22411;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#21463;&#21040;&#20102;&#28145;&#24230;&#21367;&#31215;&#24605;&#24819;&#30340;&#21551;&#21457;&#65292;&#24182;&#19988;&#19987;&#20026;&#22788;&#29702;&#31354;&#38388;&#20998;&#36776;&#29575;&#32780;&#35774;&#35745;&#65292;&#21516;&#26102;&#19981;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65306;Intel&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
One of the main challenges since the advancement of convolutional neural networks is how to connect the extracted feature map to the final classification layer. VGG models used two sets of fully connected layers for the classification part of their architectures, which significantly increased the number of models' weights. ResNet and the next deep convolutional models used the Global Average Pooling (GAP) layer to compress the feature map and feed it to the classification layer. Although using the GAP layer reduces the computational cost, but also causes losing spatial resolution of the feature map, which results in decreasing learning efficiency. In this paper, we aim to tackle this problem by replacing the GAP layer with a new architecture called Wise-SrNet. It is inspired by the depthwise convolutional idea and is designed for processing spatial resolution while not increasing computational cost. We have evaluated our method using three different datasets: Intel Image Classification
&lt;/p&gt;</description></item></channel></rss>