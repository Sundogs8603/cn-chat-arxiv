<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Doduo&#26159;&#19968;&#20010;&#20174;&#37326;&#22806;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#23398;&#20064;&#36890;&#29992;&#23494;&#38598;&#35270;&#35273;&#23545;&#24212;&#20851;&#31995;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#27969;&#22330;&#25197;&#26354;&#26469;&#33719;&#24471;&#35757;&#32451;&#30417;&#30563;&#20449;&#21495;&#65292;&#24182;&#32467;&#21512;&#35821;&#20041;&#20808;&#39564;&#36827;&#34892;&#33258;&#30417;&#30563;&#27969;&#35757;&#32451;&#65292;&#21487;&#20135;&#29983;&#40065;&#26834;&#20934;&#30830;&#30340;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;Doduo&#22312;&#28857;&#32423;&#23545;&#24212;&#20272;&#35745;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23545;&#24212;&#20851;&#31995;&#23398;&#20064;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2309.15110</link><description>&lt;p&gt;
Doduo: &#20174;&#26080;&#30417;&#30563;&#35821;&#20041;&#24863;&#30693;&#27969;&#20013;&#23398;&#20064;&#23494;&#38598;&#35270;&#35273;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Doduo: Learning Dense Visual Correspondence from Unsupervised Semantic-Aware Flow. (arXiv:2309.15110v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15110
&lt;/p&gt;
&lt;p&gt;
Doduo&#26159;&#19968;&#20010;&#20174;&#37326;&#22806;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#23398;&#20064;&#36890;&#29992;&#23494;&#38598;&#35270;&#35273;&#23545;&#24212;&#20851;&#31995;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#27969;&#22330;&#25197;&#26354;&#26469;&#33719;&#24471;&#35757;&#32451;&#30417;&#30563;&#20449;&#21495;&#65292;&#24182;&#32467;&#21512;&#35821;&#20041;&#20808;&#39564;&#36827;&#34892;&#33258;&#30417;&#30563;&#27969;&#35757;&#32451;&#65292;&#21487;&#20135;&#29983;&#40065;&#26834;&#20934;&#30830;&#30340;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;Doduo&#22312;&#28857;&#32423;&#23545;&#24212;&#20272;&#35745;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23545;&#24212;&#20851;&#31995;&#23398;&#20064;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#35270;&#35273;&#23545;&#24212;&#20851;&#31995;&#22312;&#26426;&#22120;&#20154;&#24863;&#30693;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#24037;&#20316;&#33268;&#21147;&#20110;&#24314;&#31435;&#25429;&#25417;&#21160;&#24577;&#22330;&#26223;&#32463;&#21382;&#37325;&#22823;&#21464;&#21270;&#30340;&#19968;&#23545;&#22270;&#20687;&#20043;&#38388;&#30340;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Doduo&#65292;&#23427;&#21487;&#20197;&#20174;&#37326;&#22806;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#23398;&#20064;&#36890;&#29992;&#30340;&#23494;&#38598;&#35270;&#35273;&#23545;&#24212;&#20851;&#31995;&#65292;&#26080;&#38656;&#22320;&#38754;&#30495;&#23454;&#30417;&#30563;&#12290;&#32473;&#23450;&#19968;&#23545;&#22270;&#20687;&#65292;Doduo&#20272;&#35745;&#20102;&#23494;&#38598;&#30340;&#27969;&#22330;&#65292;&#32534;&#30721;&#20102;&#19968;&#20010;&#22270;&#20687;&#20013;&#27599;&#20010;&#20687;&#32032;&#21040;&#21478;&#19968;&#20010;&#22270;&#20687;&#20013;&#30456;&#24212;&#20687;&#32032;&#30340;&#20301;&#31227;&#12290;Doduo&#20351;&#29992;&#22522;&#20110;&#27969;&#22330;&#30340;&#25197;&#26354;&#26469;&#33719;&#24471;&#35757;&#32451;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#32467;&#21512;&#33258;&#30417;&#30563;&#27969;&#35757;&#32451;&#30340;&#35821;&#20041;&#20808;&#39564;&#65292;Doduo&#20135;&#29983;&#20102;&#23545;&#22330;&#26223;&#21160;&#24577;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#12290;&#22312;&#37326;&#22806;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21518;&#65292;Doduo&#22312;&#28857;&#32423;&#23545;&#24212;&#20272;&#35745;&#19978;&#26174;&#31034;&#20986;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23545;&#24212;&#20851;&#31995;&#23398;&#20064;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23558;Doduo&#24212;&#29992;&#20110;&#20851;&#33410;&#20272;&#35745;&#21644;&#38646;&#26679;&#26412;&#30446;&#26631;.
&lt;/p&gt;
&lt;p&gt;
Dense visual correspondence plays a vital role in robotic perception. This work focuses on establishing the dense correspondence between a pair of images that captures dynamic scenes undergoing substantial transformations. We introduce Doduo to learn general dense visual correspondence from in-the-wild images and videos without ground truth supervision. Given a pair of images, it estimates the dense flow field encoding the displacement of each pixel in one image to its corresponding pixel in the other image. Doduo uses flow-based warping to acquire supervisory signals for the training. Incorporating semantic priors with self-supervised flow training, Doduo produces accurate dense correspondence robust to the dynamic changes of the scenes. Trained on an in-the-wild video dataset, Doduo illustrates superior performance on point-level correspondence estimation over existing self-supervised correspondence learning baselines. We also apply Doduo to articulation estimation and zero-shot goal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#26694;&#26550;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#32422;&#26463;&#26631;&#35760;&#30340;&#20851;&#27880;&#31243;&#24230;&#19982;&#20107;&#23454;&#20934;&#30830;&#24615;&#24378;&#27491;&#30456;&#20851;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#32422;&#26463;&#28385;&#36275;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#20801;&#35768;&#26089;&#26399;&#38169;&#35823;&#35782;&#21035;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15098</link><description>&lt;p&gt;
&#28385;&#36275;&#20851;&#27880;&#65306;&#23545;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#38169;&#35823;&#30340;&#32422;&#26463;&#28385;&#36275;&#35270;&#35282;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models. (arXiv:2309.15098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#26694;&#26550;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#32422;&#26463;&#26631;&#35760;&#30340;&#20851;&#27880;&#31243;&#24230;&#19982;&#20107;&#23454;&#20934;&#30830;&#24615;&#24378;&#27491;&#30456;&#20851;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#32422;&#26463;&#28385;&#36275;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#20801;&#35768;&#26089;&#26399;&#38169;&#35823;&#35782;&#21035;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#20107;&#23454;&#19978;&#38169;&#35823;&#30340;&#25991;&#26412;&#26102;&#30340;&#20869;&#37096;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#20107;&#23454;&#26597;&#35810;&#24314;&#27169;&#20026;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#26694;&#26550;&#30740;&#31350;&#27169;&#22411;&#22914;&#20309;&#19982;&#20107;&#23454;&#32422;&#26463;&#36827;&#34892;&#20869;&#37096;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#23545;&#32422;&#26463;&#26631;&#35760;&#30340;&#20851;&#27880;&#31243;&#24230;&#19982;&#20854;&#21709;&#24212;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#23384;&#22312;&#24378;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;11&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#24635;&#35745;&#36229;&#36807;40,000&#20010;&#25552;&#31034;&#30340;&#31934;&#24515;&#31574;&#21010;&#22871;&#35013;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;Llama-2&#31995;&#21015;&#22312;&#25152;&#26377;&#35268;&#27169;&#65288;7B&#65292;13B&#65292;70B&#65289;&#19978;&#39044;&#27979;&#20107;&#23454;&#38169;&#35823;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SAT Probe&#65292;&#19968;&#31181;&#25506;&#26597;&#33258;&#27880;&#24847;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#32422;&#26463;&#28385;&#36275;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#20801;&#35768;&#26089;&#26399;&#38169;&#35823;&#35782;&#21035;&#12290;&#36825;&#19968;&#26041;&#27861;&#21644;&#21457;&#29616;&#34920;&#26126;&#65292;&#21033;&#29992;&#23545;LLM&#20013;&#20107;&#23454;&#24615;&#30340;&#26426;&#26800;&#29702;&#35299;&#21487;&#20197;&#22686;&#24378;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as Constraint Satisfaction Problems and use this framework to investigate how the model interacts internally with factual constraints. Specifically, we discover a strong positive relation between the model's attention to constraint tokens and the factual accuracy of its responses. In our curated suite of 11 datasets with over 40,000 prompts, we study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe, a method probing self-attention patterns, that can predict constraint satisfaction and factual errors, and allows early error identification. The approach and findings demonstrate how using the mechanistic understanding of factuality in LLMs can enhance reliability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#23454;&#29616;&#19968;&#33268;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#26469;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#12290;</title><link>http://arxiv.org/abs/2309.15091</link><description>&lt;p&gt;
VideoDirectorGPT: &#36890;&#36807;LLM&#24341;&#23548;&#30340;&#35268;&#21010;&#23454;&#29616;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning. (arXiv:2309.15091v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#23454;&#29616;&#19968;&#33268;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#26469;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#24037;&#20316;&#38598;&#20013;&#22312;&#29983;&#25104;&#21333;&#20010;&#20107;&#20214;&#21644;&#21333;&#19968;&#32972;&#26223;&#30340;&#30701;&#35270;&#39057;&#29255;&#27573;&#65288;&#21363;&#21333;&#22330;&#26223;&#35270;&#39057;&#65289;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#24067;&#23616;&#21644;&#25511;&#21046;&#19979;&#28216;&#35270;&#35273;&#27169;&#22359;&#65288;&#22914;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65289;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;&#36825;&#20123;LLMs&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#36827;&#34892;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#21333;&#20010;&#25991;&#26412;&#25552;&#31034;&#36755;&#20837;&#25105;&#20204;&#30340;&#35270;&#39057;&#35268;&#21010;&#22120;LLM&#65288;GPT-4&#65289;&#20013;&#65292;&#23558;&#20854;&#25193;&#23637;&#20026;&#8220;&#35270;&#39057;&#35745;&#21010;&#8221;&#65292;&#20854;&#20013;&#21253;&#25324;&#29983;&#25104;&#22330;&#26223;&#25551;&#36848;&#12289;&#23454;&#20307;&#21450;&#20854;&#24067;&#23616;&#12289;&#27599;&#20010;&#22330;&#26223;&#30340;&#32972;&#26223;&#20197;&#21450;&#20445;&#25345;&#19968;&#33268;&#24615;&#31561;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although recent text-to-video (T2V) generation methods have seen significant advancements, most of these works focus on producing short video clips of a single event with a single background (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules such as image generation models. This raises an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which involves generating the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency 
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#65292;&#36890;&#36807;&#19982;LLMs&#20132;&#20114;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.15074</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#19978;&#19979;&#25991;&#24314;&#27169;&#19982;&#25512;&#29702;&#65306;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Natural Language based Context Modeling and Reasoning with LLMs: A Tutorial. (arXiv:2309.15074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#65292;&#36890;&#36807;&#19982;LLMs&#20132;&#20114;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;2018&#24180;&#20197;&#26469;&#24613;&#21095;&#22686;&#38271;&#65292;&#33258;&#24341;&#20837;&#19978;&#19979;&#25991;&#24863;&#30693;&#35745;&#31639;&#31995;&#32479;20&#24180;&#21518;&#12290;&#19978;&#19979;&#25991;&#24863;&#30693;&#35745;&#31639;&#36890;&#36807;&#32771;&#34385;&#26222;&#36866;&#35774;&#22791;&#12289;&#29992;&#25143;&#21644;&#31038;&#20250;&#30340;&#24773;&#20917;&#65292;&#23454;&#29616;&#20102;&#24191;&#27867;&#30340;&#21019;&#26032;&#24212;&#29992;&#65292;&#22914;&#36741;&#21161;&#29983;&#27963;&#12289;&#22522;&#20110;&#20301;&#32622;&#30340;&#31038;&#20132;&#32593;&#32476;&#26381;&#21153;&#31561;&#12290;&#20026;&#20102;&#35782;&#21035;&#19978;&#19979;&#25991;&#24182;&#30456;&#24212;&#22320;&#20570;&#20986;&#20915;&#31574;&#65292;&#37319;&#29992;&#20102;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65288;&#22914;&#26412;&#20307;&#35770;&#21644;OWL&#65289;&#20316;&#20026;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;LLMs&#30340;&#23835;&#36215;&#21644;&#23427;&#20204;&#25913;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#24314;&#27169;&#19978;&#19979;&#25991;&#24182;&#36890;&#36807;&#19982;ChatGPT&#21644;GPT-4&#31561;LLMs&#20132;&#20114;&#36827;&#34892;&#19978;&#19979;&#25991;&#25512;&#29702;&#21464;&#24471;&#21487;&#34892;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#25991;&#26412;&#12289;&#25552;&#31034;&#21644;&#33258;&#20027;&#20195;&#29702;&#65288;AutoAgents&#65289;&#20351;LLMs&#33021;&#22815;&#25191;&#34892;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have become phenomenally surging, since 2018--two decades after introducing context-awareness into computing systems. Through taking into account the situations of ubiquitous devices, users and the societies, context-aware computing has enabled a wide spectrum of innovative applications, such as assisted living, location-based social network services and so on. To recognize contexts and make decisions for actions accordingly, various artificial intelligence technologies, such as Ontology and OWL, have been adopted as representations for context modeling and reasoning. Recently, with the rise of LLMs and their improved natural language understanding and reasoning capabilities, it has become feasible to model contexts using natural language and perform context reasoning by interacting with LLMs such as ChatGPT and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and autonomous agents (AutoAgents) that enable LLMs to perform context modeling 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prolog&#35821;&#35328;&#30340;&#26426;&#22120;&#20154;&#30693;&#35782;&#31649;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#22635;&#20805;&#30693;&#35782;&#24211;&#65292;&#23454;&#29616;&#20102;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#36830;&#32493;&#24182;&#34892;&#35745;&#21010;&#29983;&#25104;&#20197;&#21450;&#35745;&#21010;&#21040;&#21487;&#25191;&#34892;&#24418;&#24335;&#30340;&#33258;&#21160;&#36716;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.15049</link><description>&lt;p&gt;
&#24403;Prolog&#36935;&#35265;&#29983;&#25104;&#27169;&#22411;&#65306;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#31649;&#29702;&#30693;&#35782;&#21644;&#35268;&#21010;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
When Prolog meets generative models: a new approach for managing knowledge and planning in robotic applications. (arXiv:2309.15049v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prolog&#35821;&#35328;&#30340;&#26426;&#22120;&#20154;&#30693;&#35782;&#31649;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#22635;&#20805;&#30693;&#35782;&#24211;&#65292;&#23454;&#29616;&#20102;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#36830;&#32493;&#24182;&#34892;&#35745;&#21010;&#29983;&#25104;&#20197;&#21450;&#35745;&#21010;&#21040;&#21487;&#25191;&#34892;&#24418;&#24335;&#30340;&#33258;&#21160;&#36716;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prolog&#35821;&#35328;&#30340;&#38754;&#21521;&#26426;&#22120;&#20154;&#30340;&#30693;&#35782;&#31649;&#29702;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20381;&#36182;&#20110;&#19968;&#31181;&#29305;&#27530;&#30340;&#30693;&#35782;&#24211;&#32452;&#32455;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#20197;&#19979;&#21151;&#33021;&#65306;1.&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30693;&#35782;&#24211;&#22635;&#20805;&#65307;2.&#36890;&#36807;&#19968;&#31995;&#21015;&#36716;&#25442;&#29983;&#25104;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#36830;&#32493;&#24182;&#34892;&#35745;&#21010;&#65307;3.&#23558;&#35745;&#21010;&#33258;&#21160;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#24418;&#24335;&#65288;&#34892;&#20026;&#26641;&#65289;&#12290;&#35813;&#26694;&#26550;&#25903;&#25345;&#19968;&#31995;&#21015;&#24320;&#28304;&#24037;&#20855;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a robot oriented knowledge management system based on the use of the Prolog language. Our framework hinges on a special organisation of knowledge base that enables: 1. its efficient population from natural language texts using semi-automated procedures based on Large Language Models, 2. the bumpless generation of temporal parallel plans for multi-robot systems through a sequence of transformations, 3. the automated translation of the plan into an executable formalism (the behaviour trees). The framework is supported by a set of open source tools and is shown on a realistic application.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15048</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class Incremental Learning via Likelihood Ratio Based Task Prediction. (arXiv:2309.15048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26159;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19981;&#26029;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#36890;&#36807;&#39034;&#24207;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#27599;&#20010;&#20219;&#21153;&#30001;&#19968;&#32452;&#21807;&#19968;&#30340;&#31867;&#32452;&#25104;&#12290;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#65292;&#22312;&#27979;&#35797;&#26102;&#19981;&#25552;&#20379;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#26631;&#35782;&#31526;&#65288;&#25110;&#20219;&#21153;ID&#65289;&#12290;&#20026;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#39044;&#27979;&#20219;&#21153;ID&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19968;&#31181;&#26032;&#20852;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#26681;&#25454;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20849;&#20139;&#32593;&#32476;&#20013;&#20026;&#25152;&#26377;&#20219;&#21153;&#35757;&#32451;&#27599;&#20010;&#20219;&#21153;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#20197;&#22788;&#29702;&#36951;&#24536;&#12290;&#35813;&#26041;&#27861;&#20013;&#27599;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#38750;&#24120;&#35268;&#20998;&#31867;&#22120;&#32780;&#19981;&#26159;&#20256;&#32479;&#20998;&#31867;&#22120;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#12290;&#31163;&#32676;&#26816;&#27979;&#22120;&#21487;&#20197;&#23545;&#20219;&#21153;&#20869;&#65288;&#20998;&#24067;&#20869;&#65288;IND&#65289;&#65289;&#30340;&#31867;&#36827;&#34892;&#39044;&#27979;&#21644;&#35782;&#21035;&#31163;&#32676;&#25968;&#25454;&#12290;&#22312;&#25512;&#26029;&#26399;&#38388;&#65292;&#31163;&#32676;&#26816;&#27979;&#33021;&#21147;&#26159;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;ID&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#35748;&#20026;&#20351;&#29992;&#20256;&#32479;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;ID&#39044;&#27979;&#26159;&#27425;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time for each test sample. Predicting the task-id for each test sample is a challenging problem. An emerging theoretically justified and effective approach is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learning (TIL) method to deal with forgetting. The model for each task in this approach is an out-of-distribution (OOD) detector rather than a conventional classifier. The OOD detector can perform both within-task (in-distribution (IND)) class prediction and OOD detection. The OOD detection capability is the key for task-id prediction during inference for each test sample. However, this paper argues that using a traditional OOD detector for task-id prediction is sub-optimal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992; EHR &#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21482;&#38656;&#21033;&#29992;&#21382;&#21490;&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#23567;&#21270;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#36890;&#36807;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#23454;&#29616;&#23545;&#24739;&#32773;&#30284;&#30151;&#39118;&#38505;&#30340;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.15039</link><description>&lt;p&gt;
&#32467;&#21512;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#36827;&#34892;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Combining Survival Analysis and Machine Learning for Mass Cancer Risk Prediction using EHR data. (arXiv:2309.15039v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992; EHR &#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21482;&#38656;&#21033;&#29992;&#21382;&#21490;&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#23567;&#21270;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#36890;&#36807;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#23454;&#29616;&#23545;&#24739;&#32773;&#30284;&#30151;&#39118;&#38505;&#30340;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32431;&#31929;&#30340;&#21307;&#23398;&#32959;&#30244;&#31579;&#26597;&#26041;&#27861;&#36890;&#24120;&#36153;&#29992;&#39640;&#26114;&#12289;&#32791;&#26102;&#38271;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#22312;&#30284;&#30151;&#26816;&#27979;&#26041;&#38754;&#21457;&#25381;&#20102;&#24040;&#22823;&#20316;&#29992;&#65292;&#20294;&#38656;&#35201;&#29305;&#23450;&#25110;&#28145;&#20837;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#38754;&#24433;&#21709;&#20102;&#30284;&#30151;&#31579;&#26597;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#23454;&#26045;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#24050;&#26377;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#23545;&#24739;&#32773;&#36827;&#34892;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#30284;&#30151;&#39118;&#38505;&#35780;&#20272;&#24212;&#29992;AI&#26041;&#27861;&#26159;&#19968;&#31181;&#39072;&#35206;&#24615;&#30340;&#25913;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;EHR&#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#30340;&#25968;&#25454;&#36138;&#23146;&#31574;&#30053;&#33073;&#39062;&#32780;&#20986;&#65292;&#20165;&#38656;&#35201;&#26469;&#33258;EHR&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#21382;&#21490;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20108;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;175441&#21517;&#19981;&#35760;&#21517;&#30340;&#24739;&#32773;&#65288;&#20854;&#20013;2861&#21517;&#34987;&#35786;&#26029;&#20026;&#30284;&#30151;&#65289;&#12290;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;
Purely medical cancer screening methods are often costly, time-consuming, and weakly applicable on a large scale. Advanced Artificial Intelligence (AI) methods greatly help cancer detection but require specific or deep medical data. These aspects affect the mass implementation of cancer screening methods. For these reasons, it is a disruptive change for healthcare to apply AI methods for mass personalized assessment of the cancer risk among patients based on the existing Electronic Health Records (EHR) volume.  This paper presents a novel method for mass cancer risk prediction using EHR data. Among other methods, our one stands out by the minimum data greedy policy, requiring only a history of medical service codes and diagnoses from EHR. We formulate the problem as a binary classification. This dataset contains 175 441 de-identified patients (2 861 diagnosed with cancer). As a baseline, we implement a solution based on a recurrent neural network (RNN). We propose a method that combine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15028</link><description>&lt;p&gt;
&#35753;PPO&#21464;&#24471;&#26356;&#22909;&#65306;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26102;&#65292;&#20351;&#29992;&#26368;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;Proximal Policy Optimization (PPO)&#65292;&#22240;&#27492;&#21487;&#20197;&#35748;&#20026;&#25512;&#29702;&#26102;&#38388;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#22914;Monte-Carlo Tree Search (MCTS) &#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;PPO&#30340;&#24615;&#33021;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#35299;&#30721;&#25991;&#26412;&#26102;&#65292;&#19981;&#35201;&#20002;&#24323;&#20540;&#32593;&#32476;&#65292;&#21363;PPO&#35757;&#32451;&#26102;&#29992;&#20110;&#35780;&#20272;&#37096;&#20998;&#36755;&#20986;&#24207;&#21015;&#30340;&#21103;&#20135;&#21697;&#65292;&#32780;&#26159;&#23558;&#20854;&#19982;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PPO-MCTS&#30340;&#26032;&#39062;&#30340;&#20540;&#23548;&#21521;&#35299;&#30721;&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;PPO&#30340;&#20540;&#32593;&#32476;&#19982;&#25512;&#29702;&#26102;&#38388;&#20135;&#29983;&#30340;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#19982;&#22522;&#20110;MCTS&#30340;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#20943;&#23569;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#30340;&#35780;&#20998;&#26426;&#21046;&#30340;&#22522;&#26412;&#19981;&#21305;&#37197;&#12290;&#22312;&#22235;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;PPO-MCTS&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS grea
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#35752;&#65292;&#24182;&#25552;&#20986;&#20102;&#20869;&#37096;&#23545;&#40784;&#21644;&#22806;&#37096;&#23545;&#40784;&#30340;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#28508;&#22312;&#30340;&#23545;&#25239;&#25915;&#20987;&#28431;&#27934;&#12290;&#32771;&#34385;&#21040;&#27169;&#22411;&#21487;&#33021;&#20135;&#29983;&#30340;&#19981;&#20934;&#30830;&#21644;&#35823;&#23548;&#24615;&#25991;&#26412;&#65292;&#23545;&#40784;&#25216;&#26415;&#26174;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.15025</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Alignment: A Survey. (arXiv:2309.15025v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15025
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#35752;&#65292;&#24182;&#25552;&#20986;&#20102;&#20869;&#37096;&#23545;&#40784;&#21644;&#22806;&#37096;&#23545;&#40784;&#30340;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#28508;&#22312;&#30340;&#23545;&#25239;&#25915;&#20987;&#28431;&#27934;&#12290;&#32771;&#34385;&#21040;&#27169;&#22411;&#21487;&#33021;&#20135;&#29983;&#30340;&#19981;&#20934;&#30830;&#21644;&#35823;&#23548;&#24615;&#25991;&#26412;&#65292;&#23545;&#40784;&#25216;&#26415;&#26174;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#20123;&#36827;&#23637;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#21508;&#31181;&#25285;&#24551;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#21147;&#26080;&#30097;&#26159;&#24040;&#22823;&#30340;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#19981;&#20934;&#30830;&#12289;&#35823;&#23548;&#24615;&#29978;&#33267;&#26377;&#23475;&#30340;&#25991;&#26412;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#23545;&#40784;&#25216;&#26415;&#20197;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#19968;&#33268;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#23545;&#38024;&#23545;LLMs&#35774;&#35745;&#30340;&#23545;&#40784;&#26041;&#27861;&#36827;&#34892;&#24191;&#27867;&#25506;&#35752;&#65292;&#24182;&#32467;&#21512;&#35813;&#39046;&#22495;&#20013;&#30340;&#29616;&#26377;&#33021;&#21147;&#30740;&#31350;&#12290;&#37319;&#29992;AI&#23545;&#40784;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#23558;&#29992;&#20110;&#23545;&#40784;LLMs&#30340;&#20027;&#27969;&#26041;&#27861;&#21644;&#26032;&#20852;&#25552;&#35758;&#20998;&#20026;&#22806;&#37096;&#23545;&#40784;&#21644;&#20869;&#37096;&#23545;&#40784;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21644;&#28508;&#22312;&#30340;&#23545;&#25239;&#25915;&#20987;&#28431;&#27934;&#31561;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35780;&#20272;LLM&#30340;&#23545;&#40784;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values.  This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain. Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment. We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. To assess LLM alignment, we present a wide variety of benchmarks and evaluation metho
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"VISION"&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#33041;&#30340;&#24037;&#20316;&#26041;&#24335;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#20154;&#31867;&#33041;&#23545;&#33258;&#28982;&#22270;&#20687;&#30340;fMRI&#21709;&#24212;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;45%&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25506;&#32034;&#35757;&#32451;&#36807;&#30340;&#32593;&#32476;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#35270;&#35273;&#21306;&#22495;&#30340;&#34920;&#24449;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#30382;&#23618;&#21151;&#33021;&#30456;&#20851;&#30340;&#21487;&#23454;&#39564;&#26816;&#39564;&#30340;&#20551;&#35774;&#21644;&#35299;&#37322;&#24615;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.15018</link><description>&lt;p&gt;
&#21333;&#21521;&#33041;&#26426;&#25509;&#21475;&#65306;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#23558;&#33258;&#28982;&#22270;&#20687;&#32534;&#30721;&#20026;&#35270;&#35273;&#30382;&#23618;&#30340;fMRI&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unidirectional brain-computer interface: Artificial neural network encoding natural images to fMRI response in the visual cortex. (arXiv:2309.15018v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15018
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"VISION"&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#33041;&#30340;&#24037;&#20316;&#26041;&#24335;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#20154;&#31867;&#33041;&#23545;&#33258;&#28982;&#22270;&#20687;&#30340;fMRI&#21709;&#24212;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;45%&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25506;&#32034;&#35757;&#32451;&#36807;&#30340;&#32593;&#32476;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#35270;&#35273;&#21306;&#22495;&#30340;&#34920;&#24449;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#30382;&#23618;&#21151;&#33021;&#30456;&#20851;&#30340;&#21487;&#23454;&#39564;&#26816;&#39564;&#30340;&#20551;&#35774;&#21644;&#35299;&#37322;&#24615;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20854;&#22312;&#29702;&#35299;&#35270;&#35273;&#30693;&#35273;&#26041;&#38754;&#30340;&#23436;&#20840;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"VISION"&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20026;"&#31070;&#32463;&#27963;&#21160;&#25104;&#20687;&#36755;&#20986;&#30340;&#35270;&#35273;&#25509;&#21475;&#31995;&#32479;"&#30340;&#39318;&#23383;&#27597;&#32553;&#20889;&#65292;&#27169;&#20223;&#20154;&#33041;&#24182;&#23637;&#31034;&#22914;&#20309;&#20419;&#36827;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#12290;&#21033;&#29992;&#35270;&#35273;&#21644;&#24773;&#22659;&#36755;&#20837;&#65292;&#36825;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#39044;&#27979;&#20154;&#33041;&#23545;&#33258;&#28982;&#22270;&#20687;&#30340;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#25195;&#25551;&#21709;&#24212;&#12290;VISION&#25104;&#21151;&#39044;&#27979;&#20102;&#20154;&#31867;&#34880;&#27687;&#27700;&#24179;&#20381;&#36182;&#24615;&#20449;&#21495;&#20316;&#20026;fMRI&#20307;&#32032;&#20540;&#23545;&#35270;&#35273;&#36755;&#20837;&#30340;&#21709;&#24212;&#65292;&#20854;&#20934;&#30830;&#24230;&#27604;&#29616;&#26377;&#25216;&#26415;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;45%&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#35757;&#32451;&#36807;&#30340;&#32593;&#32476;&#20197;&#25581;&#31034;&#19981;&#21516;&#35270;&#35273;&#21306;&#22495;&#30340;&#34920;&#24449;&#20559;&#35265;&#65292;&#20135;&#29983;&#21487;&#23454;&#39564;&#26816;&#39564;&#30340;&#20551;&#35774;&#65292;&#24182;&#21046;&#23450;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#23558;&#36825;&#20123;&#20551;&#35774;&#19982;&#30382;&#23618;&#21151;&#33021;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
While significant advancements in artificial intelligence (AI) have catalyzed progress across various domains, its full potential in understanding visual perception remains underexplored. We propose an artificial neural network dubbed VISION, an acronym for "Visual Interface System for Imaging Output of Neural activity," to mimic the human brain and show how it can foster neuroscientific inquiries. Using visual and contextual inputs, this multimodal model predicts the brain's functional magnetic resonance imaging (fMRI) scan response to natural images. VISION successfully predicts human hemodynamic responses as fMRI voxel values to visual inputs with an accuracy exceeding state-of-the-art performance by 45%. We further probe the trained networks to reveal representational biases in different visual areas, generate experimentally testable hypotheses, and formulate an interpretable metric to associate these hypotheses with cortical functions. With both a model and evaluation metric, the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#29992;&#20110;&#23398;&#26657;&#24418;&#25104;&#24615;&#21644;&#24635;&#32467;&#24615;&#35780;&#20272;&#30340;&#33258;&#21160;&#21270;&#38382;&#39064;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#23545;&#25945;&#24072;&#30340;&#35843;&#26597;&#65292;&#35777;&#26126;&#20102;&#33258;&#21160;&#21270;&#29983;&#25104;&#38382;&#39064;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#33258;&#21160;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15004</link><description>&lt;p&gt;
&#20174;&#25945;&#32946;&#25991;&#26412;&#20013;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Automating question generation from educational text. (arXiv:2309.15004v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#29992;&#20110;&#23398;&#26657;&#24418;&#25104;&#24615;&#21644;&#24635;&#32467;&#24615;&#35780;&#20272;&#30340;&#33258;&#21160;&#21270;&#38382;&#39064;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#23545;&#25945;&#24072;&#30340;&#35843;&#26597;&#65292;&#35777;&#26126;&#20102;&#33258;&#21160;&#21270;&#29983;&#25104;&#38382;&#39064;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#33258;&#21160;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#24335;&#27963;&#21160;&#65288;QBA&#65289;&#22312;&#25945;&#32946;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20256;&#32479;&#19978;&#26159;&#23398;&#20064;&#21644;&#35780;&#20272;&#36807;&#31243;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#29992;&#20110;&#23398;&#26657;&#24418;&#25104;&#24615;&#21644;&#24635;&#32467;&#24615;&#35780;&#20272;&#30340;&#33258;&#21160;&#21270;&#38382;&#39064;&#29983;&#25104;&#24037;&#20855;&#12290;&#36890;&#36807;&#23545;104&#21517;&#25945;&#24072;&#30340;&#19987;&#23478;&#35843;&#26597;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#21160;&#21270;&#29983;&#25104;QBA&#30340;&#38656;&#27714;&#65292;&#20316;&#20026;&#19968;&#20010;&#33021;&#22815;&#26174;&#33879;&#20943;&#36731;&#25945;&#24072;&#24037;&#20316;&#37327;&#24182;&#20419;&#36827;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#30340;&#24037;&#20855;&#12290;&#21033;&#29992;&#29983;&#25104;&#22411;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#33258;&#21160;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#38382;&#39064;&#29983;&#25104;&#12289;&#27491;&#30830;&#31572;&#26696;&#39044;&#27979;&#21644;&#24178;&#25200;&#39033;&#21046;&#23450;&#30340;&#19981;&#21516;&#27169;&#22359;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35780;&#20272;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of question-based activities (QBAs) is wide-spread in education, traditionally forming an integral part of the learning and assessment process. In this paper, we design and evaluate an automated question generation tool for formative and summative assessment in schools. We present an expert survey of one hundred and four teachers, demonstrating the need for automated generation of QBAs, as a tool that can significantly reduce the workload of teachers and facilitate personalized learning experiences. Leveraging the recent advancements in generative AI, we then present a modular framework employing transformer based language models for automatic generation of multiple-choice questions (MCQs) from textual content. The presented solution, with distinct modules for question generation, correct answer prediction, and distractor formulation, enables us to evaluate different language models and generation techniques. Finally, we perform an extensive quantitative and qualitative evaluat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#24070;&#33337;&#25216;&#26415;&#35268;&#26684;&#21644;&#20215;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#21306;&#22495;&#23450;&#20215;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24212;&#29992;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#21333;&#20307;&#33337;&#36890;&#24120;&#27604;&#21452;&#20307;&#33337;&#26356;&#23454;&#24800;&#65292;&#24182;&#19988;&#38271;&#24230;&#12289;&#23485;&#24230;&#12289;&#25490;&#27700;&#37327;&#21644;&#24070;&#38754;&#31215;&#31561;&#29305;&#23450;&#35268;&#26684;&#19982;&#36739;&#39640;&#30340;&#20215;&#26684;&#30452;&#25509;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#32654;&#22269;&#26159;&#24179;&#22343;&#24070;&#33337;&#20215;&#26684;&#26368;&#39640;&#30340;&#22269;&#23478;&#65292;&#32780;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#19982;&#24070;&#33337;&#20215;&#26684;&#27809;&#26377;&#30452;&#25509;&#30456;&#20851;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.14994</link><description>&lt;p&gt;
&#38024;&#23545;&#24070;&#33337;&#20215;&#26684;&#21644;&#29305;&#24449;&#20197;&#21450;&#21306;&#22495;&#22320;&#21306;&#30340;&#27979;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Measurement Models For Sailboats Price vs. Features And Regional Areas. (arXiv:2309.14994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14994
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#24070;&#33337;&#25216;&#26415;&#35268;&#26684;&#21644;&#20215;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#21306;&#22495;&#23450;&#20215;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24212;&#29992;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#21333;&#20307;&#33337;&#36890;&#24120;&#27604;&#21452;&#20307;&#33337;&#26356;&#23454;&#24800;&#65292;&#24182;&#19988;&#38271;&#24230;&#12289;&#23485;&#24230;&#12289;&#25490;&#27700;&#37327;&#21644;&#24070;&#38754;&#31215;&#31561;&#29305;&#23450;&#35268;&#26684;&#19982;&#36739;&#39640;&#30340;&#20215;&#26684;&#30452;&#25509;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#32654;&#22269;&#26159;&#24179;&#22343;&#24070;&#33337;&#20215;&#26684;&#26368;&#39640;&#30340;&#22269;&#23478;&#65292;&#32780;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#19982;&#24070;&#33337;&#20215;&#26684;&#27809;&#26377;&#30452;&#25509;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#24070;&#33337;&#25216;&#26415;&#35268;&#26684;&#19982;&#20854;&#20215;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#21306;&#22495;&#23450;&#20215;&#30340;&#24433;&#21709;&#12290;&#21033;&#29992;&#21253;&#25324;&#38271;&#24230;&#12289;&#23485;&#24230;&#12289;&#21507;&#27700;&#12289;&#25490;&#27700;&#37327;&#12289;&#24070;&#38754;&#31215;&#21644;&#27700;&#32447;&#31561;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24212;&#29992;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#24070;&#33337;&#20215;&#26684;&#12290;&#26799;&#24230;&#19979;&#38477;&#27169;&#22411;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20135;&#29983;&#20102;&#26368;&#20302;&#30340;MSE&#21644;MAE&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#21333;&#20307;&#33337;&#36890;&#24120;&#27604;&#21452;&#20307;&#33337;&#26356;&#23454;&#24800;&#65292;&#32780;&#38271;&#24230;&#12289;&#23485;&#24230;&#12289;&#25490;&#27700;&#37327;&#21644;&#24070;&#38754;&#31215;&#31561;&#29305;&#23450;&#35268;&#26684;&#19982;&#36739;&#39640;&#30340;&#20215;&#26684;&#30452;&#25509;&#30456;&#20851;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36739;&#20302;&#30340;&#21507;&#27700;&#19982;&#36739;&#39640;&#30340;&#25346;&#29260;&#20215;&#26684;&#26377;&#20851;&#32852;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#21306;&#22495;&#23450;&#20215;&#22240;&#32032;&#65292;&#24182;&#21457;&#29616;&#32654;&#22269;&#22312;&#24179;&#22343;&#24070;&#33337;&#20215;&#26684;&#19978;&#23621;&#39318;&#65292;&#20854;&#27425;&#26159;&#27431;&#27954;&#12289;&#39321;&#28207;&#21644;&#21152;&#21202;&#27604;&#22320;&#21306;&#12290;&#19982;&#25105;&#20204;&#26368;&#21021;&#30340;&#20551;&#35774;&#30456;&#21453;&#65292;&#19968;&#20010;&#22269;&#23478;&#30340;GDP&#19982;&#24070;&#33337;&#20215;&#26684;&#27809;&#26377;&#30452;&#25509;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigated the relationship between sailboat technical specifications and their prices, as well as regional pricing influences. Utilizing a dataset encompassing characteristics like length, beam, draft, displacement, sail area, and waterline, we applied multiple machine learning models to predict sailboat prices. The gradient descent model demonstrated superior performance, producing the lowest MSE and MAE. Our analysis revealed that monohulled boats are generally more affordable than catamarans, and that certain specifications such as length, beam, displacement, and sail area directly correlate with higher prices. Interestingly, lower draft was associated with higher listing prices. We also explored regional price determinants and found that the United States tops the list in average sailboat prices, followed by Europe, Hong Kong, and the Caribbean. Contrary to our initial hypothesis, a country's GDP showed no direct correlation with sailboat prices. Utilizing a 50
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#36895;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#24615;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#30495;&#38451;&#24615;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.14974</link><description>&lt;p&gt;
&#22312;&#19968;&#21315;&#24180;&#21069;&#30340;&#25289;&#19969;&#25991;&#26412;&#20013;&#26816;&#27979;&#21477;&#23376;&#32423;&#21035;&#30340;&#24615;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Detecting Sexual Content at the Sentence Level in First Millennium Latin Texts. (arXiv:2309.14974v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#36895;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#24615;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#30495;&#38451;&#24615;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#24555;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#39033;&#20256;&#32479;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#32422;2500&#20010;&#21477;&#23376;&#65292;&#28085;&#30422;&#20102;&#20174;&#20844;&#20803;&#21069;300&#24180;&#21040;&#20844;&#20803;900&#24180;&#30340;&#24615;&#35821;&#20041;&#23398;&#65288;&#21307;&#23398;&#65292;&#24773;&#33394;&#31561;&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#21477;&#23376;&#20998;&#31867;&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#37117;&#27604;&#31616;&#21333;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#25628;&#32034;&#26041;&#27861;&#26356;&#22909;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20010;&#20154;&#35328;&#35821;&#21644;&#31038;&#20250;&#35328;&#35821;&#20803;&#25968;&#25454;&#23884;&#20837;&#65288;&#19990;&#32426;&#65292;&#20316;&#32773;&#65292;&#20889;&#20316;&#31867;&#22411;&#65289;&#30340;&#25972;&#21512;&#65292;&#20294;&#21457;&#29616;&#36825;&#23548;&#33268;&#20102;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;HAN&#20998;&#21035;&#36798;&#21040;&#20102;70.60%&#30340;&#39640;&#31934;&#24230;&#21644;86.33%&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65288;420&#32780;&#19981;&#26159;2013&#65289;&#65292;&#24182;&#26174;&#31034;&#20986;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#27169;&#22411;&#24615;&#33021;&#21487;&#33021;&#31245;&#26377;&#19979;&#38477;&#65292;&#20294;&#24615;&#33021;&#20173;&#28982;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose to evaluate the use of deep learning methods for semantic classification at the sentence level to accelerate the process of corpus building in the field of humanities and linguistics, a traditional and time-consuming task. We introduce a novel corpus comprising around 2500 sentences spanning from 300 BCE to 900 CE including sexual semantics (medical, erotica, etc.). We evaluate various sentence classification approaches and different input embedding layers, and show that all consistently outperform simple token-based searches. We explore the integration of idiolectal and sociolectal metadata embeddings (centuries, author, type of writing), but find that it leads to overfitting. Our results demonstrate the effectiveness of this approach, achieving high precision and true positive rates (TPR) of respectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset size on the model performances (420 instead of 2013), and show that, while our models per
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#26080;&#30417;&#30563;&#35270;&#35273;&#31243;&#24207;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#30721;&#37325;&#20889;&#26469;&#25552;&#39640;&#20174;&#35270;&#35273;&#25968;&#25454;&#20013;&#25512;&#26029;&#31243;&#24207;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#31232;&#30095;&#38388;&#27463;&#24615;&#20195;&#30721;&#37325;&#20889;&#27880;&#20837;(SIRI)&#21644;&#37325;&#20889;&#23478;&#26063;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#37325;&#26500;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#21516;&#26102;&#65292;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#37325;&#20889;&#23478;&#26063;&#36824;&#21487;&#20197;&#25913;&#36827;SIRI&#39044;&#27979;&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.14972</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#37325;&#20889;&#23478;&#26063;&#25913;&#21892;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#31243;&#24207;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Improving Unsupervised Visual Program Inference with Code Rewriting Families. (arXiv:2309.14972v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#26080;&#30417;&#30563;&#35270;&#35273;&#31243;&#24207;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#30721;&#37325;&#20889;&#26469;&#25552;&#39640;&#20174;&#35270;&#35273;&#25968;&#25454;&#20013;&#25512;&#26029;&#31243;&#24207;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#31232;&#30095;&#38388;&#27463;&#24615;&#20195;&#30721;&#37325;&#20889;&#27880;&#20837;(SIRI)&#21644;&#37325;&#20889;&#23478;&#26063;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#37325;&#26500;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#21516;&#26102;&#65292;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#37325;&#20889;&#23478;&#26063;&#36824;&#21487;&#20197;&#25913;&#36827;SIRI&#39044;&#27979;&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#25552;&#20379;&#20102;&#19968;&#31181;&#32039;&#20945;&#19988;&#26377;&#32467;&#26500;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#23545;&#20110;&#21487;&#35270;&#21270;&#25968;&#25454;&#26469;&#35828;&#38750;&#24120;&#26377;&#21560;&#24341;&#21147;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20195;&#30721;&#37325;&#20889;&#26469;&#25913;&#21892;&#20174;&#35270;&#35273;&#25968;&#25454;&#20013;&#25512;&#26029;&#31243;&#24207;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#31232;&#30095;&#38388;&#27463;&#24615;&#20195;&#30721;&#37325;&#20889;&#27880;&#20837;&#65288;SIRI&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#30417;&#30563;&#24341;&#23548;&#24335;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;SIRI&#22312;&#35757;&#32451;&#31243;&#24207;&#30340;&#25968;&#25454;&#38598;&#19978;&#31232;&#30095;&#22320;&#24212;&#29992;&#20195;&#30721;&#37325;&#20889;&#25805;&#20316;&#65292;&#24182;&#23558;&#25913;&#36827;&#30340;&#31243;&#24207;&#27880;&#20837;&#21040;&#35757;&#32451;&#38598;&#20013;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#35270;&#35273;&#32534;&#31243;&#39046;&#22495;&#30340;&#37325;&#20889;&#23478;&#26063;&#65306;&#21442;&#25968;&#20248;&#21270;&#12289;&#20195;&#30721;&#20462;&#21098;&#21644;&#20195;&#30721;&#31227;&#26893;&#12290;&#23545;&#20110;&#20108;&#32500;&#21644;&#19977;&#32500;&#30340;&#19977;&#31181;&#24418;&#29366;&#32534;&#31243;&#35821;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;SIRI&#21644;&#25105;&#20204;&#30340;&#37325;&#20889;&#23478;&#26063;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#65306;&#26356;&#22909;&#30340;&#37325;&#26500;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#19982;&#19981;&#20351;&#29992;&#37325;&#20889;&#22120;&#25110;&#32773;&#21482;&#26159;&#31616;&#21333;&#22320;&#20351;&#29992;&#37325;&#20889;&#22120;&#30340;&#24341;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#37325;&#20889;&#23478;&#26063;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#26377;&#25928;&#22320;&#25913;&#36827;SIRI&#39044;&#27979;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Programs offer compactness and structure that makes them an attractive representation for visual data. We explore how code rewriting can be used to improve systems for inferring programs from visual data. We first propose Sparse Intermittent Rewrite Injection (SIRI), a framework for unsupervised bootstrapped learning. SIRI sparsely applies code rewrite operations over a dataset of training programs, injecting the improved programs back into the training set. We design a family of rewriters for visual programming domains: parameter optimization, code pruning, and code grafting. For three shape programming languages in 2D and 3D, we show that using SIRI with our family of rewriters improves performance: better reconstructions and faster convergence rates, compared with bootstrapped learning methods that do not use rewriters or use them naively. Finally, we demonstrate that our family of rewriters can be effectively used at test time to improve the output of SIRI predictions. For 2D and 3
&lt;/p&gt;</description></item><item><title>&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.14970</link><description>&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recurrent Hypernetworks are Surprisingly Strong in Meta-RL. (arXiv:2309.14970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14970
&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#26102;&#22240;&#26679;&#26412;&#25928;&#29575;&#20302;&#32780;&#19981;&#26131;&#37096;&#32626;&#12290;&#20803;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#22312;&#20803;&#35757;&#32451;&#26102;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#20998;&#24067;&#26469;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#30452;&#25509;&#35299;&#20915;&#20102;&#36825;&#20010;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#19987;&#38376;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#19982;&#19968;&#20010;&#36890;&#29992;&#30340;&#24207;&#21015;&#27169;&#22411;&#65288;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26159;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35266;&#28857;&#30001;&#20110;&#26377;&#38480;&#30340;&#25903;&#25345;&#35777;&#25454;&#32780;&#24341;&#36215;&#20102;&#20105;&#35758;&#65292;&#29305;&#21035;&#26159;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#30830;&#31435;&#20102;&#23436;&#20840;&#30456;&#21453;&#30340;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#34429;&#28982;&#25105;&#20204;&#21516;&#26679;&#21457;&#29616;&#24490;&#29615;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#36229;&#32593;&#32476;&#30340;&#20351;&#29992;&#23545;&#20110;&#21457;&#25381;&#24490;&#29615;&#22522;&#32447;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19982;&#36229;&#32593;&#32476;&#30456;&#32467;&#21512;&#26102;&#65292;&#36825;&#31181;&#36828;&#27604;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#31616;&#21333;&#30340;&#24490;&#29615;&#22522;&#20934;&#23454;&#38469;&#19978;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) is notoriously impractical to deploy due to sample inefficiency. Meta-RL directly addresses this sample inefficiency by learning to perform few-shot learning when a distribution of related tasks is available for meta-training. While many specialized meta-RL methods have been proposed, recent work suggests that end-to-end learning in conjunction with an off-the-shelf sequential model, such as a recurrent network, is a surprisingly strong baseline. However, such claims have been controversial due to limited supporting evidence, particularly in the face of prior work establishing precisely the opposite. In this paper, we conduct an empirical investigation. While we likewise find that a recurrent network can achieve strong performance, we demonstrate that the use of hypernetworks is crucial to maximizing their potential. Surprisingly, when combined with hypernetworks, the recurrent baselines that are far simpler than existing specialized methods actually ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#31038;&#20132;&#23186;&#20307;&#34920;&#31034;&#36136;&#37327;&#65292;&#36890;&#36807;&#20154;&#31867;&#20114;&#21160;&#24110;&#21161;&#33258;&#21160;&#21270;&#31995;&#32479;&#26816;&#27979;&#26032;&#38395;&#26469;&#28304;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#21363;&#20351;&#36827;&#34892;&#20102;&#23569;&#37327;&#30340;&#20154;&#31867;&#20114;&#21160;&#65292;&#20063;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14966</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#23398;&#20064;&#31038;&#20132;&#23186;&#20307;&#34920;&#31034;&#25913;&#21892;&#26032;&#38395;&#26469;&#28304;&#30495;&#23454;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interactively Learning Social Media Representations Improves News Source Factuality Detection. (arXiv:2309.14966v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#31038;&#20132;&#23186;&#20307;&#34920;&#31034;&#36136;&#37327;&#65292;&#36890;&#36807;&#20154;&#31867;&#20114;&#21160;&#24110;&#21161;&#33258;&#21160;&#21270;&#31995;&#32479;&#26816;&#27979;&#26032;&#38395;&#26469;&#28304;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#21363;&#20351;&#36827;&#34892;&#20102;&#23569;&#37327;&#30340;&#20154;&#31867;&#20114;&#21160;&#65292;&#20063;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#20852;&#36215;&#20351;&#24471;&#34394;&#20551;&#26032;&#38395;&#30340;&#24191;&#27867;&#20256;&#25773;&#25104;&#20026;&#21487;&#33021;&#65292;&#34394;&#20551;&#26032;&#38395;&#26159;&#25351;&#20197;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#24433;&#21709;&#20449;&#20208;&#20026;&#30446;&#30340;&#30340;&#25991;&#26412;&#12290;&#21450;&#26102;&#26816;&#27979;&#34394;&#20551;&#26032;&#38395;&#65292;&#29305;&#21035;&#26159;&#22312;&#26032;&#20107;&#20214;&#20986;&#29616;&#26102;&#65292;&#23545;&#20110;&#38450;&#27490;&#35823;&#23548;&#20449;&#24687;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#30417;&#30563;&#23398;&#20064;&#31995;&#32479;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#33258;&#21160;&#24314;&#27169;&#31038;&#20132;&#23186;&#20307;&#20256;&#25773;&#34394;&#20551;&#26032;&#38395;&#30340;&#22797;&#26434;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30456;&#21453;&#65292;&#36890;&#36807;&#20154;&#24037;&#23454;&#26102;&#26816;&#26597;&#25152;&#26377;&#26032;&#38395;&#26159;&#19981;&#21487;&#25193;&#23637;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20154;&#20204;&#21487;&#20197;&#19982;&#33258;&#21160;&#21270;&#31995;&#32479;&#20114;&#21160;&#65292;&#24110;&#21161;&#20854;&#23398;&#20064;&#26356;&#22909;&#30340;&#31038;&#20132;&#23186;&#20307;&#34920;&#31034;&#36136;&#37327;&#12290;&#22312;&#30495;&#23454;&#20107;&#20214;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#36827;&#34892;&#20102;&#23569;&#37327;&#30340;&#20154;&#31867;&#20114;&#21160;&#65292;&#20063;&#33021;&#25552;&#39640;&#26816;&#27979;&#26032;&#38395;&#26469;&#28304;&#30495;&#23454;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of social media has enabled the widespread propagation of fake news, text that is published with an intent to spread misinformation and sway beliefs. Rapidly detecting fake news, especially as new events arise, is important to prevent misinformation.  While prior works have tackled this problem using supervised learning systems, automatedly modeling the complexities of the social media landscape that enables the spread of fake news is challenging. On the contrary, having humans fact check all news is not scalable. Thus, in this paper, we propose to approach this problem interactively, where humans can interact to help an automated system learn a better social media representation quality. On real world events, our experiments show performance improvements in detecting factuality of news sources, even after few human interactions.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;cryoPROS&#65292;&#36890;&#36807;&#29983;&#25104;&#36741;&#21161;&#31890;&#23376;&#26469;&#35299;&#20915;&#21333;&#39063;&#31890;&#20919;&#20923;&#30005;&#38236;&#20013;&#30340;&#20248;&#36873;&#21462;&#21521;&#38382;&#39064;&#65292;&#21487;&#26377;&#25928;&#22320;&#24674;&#22797;&#38750;&#20542;&#26012;&#25968;&#25454;&#30340;&#39640;&#20998;&#36776;&#29575;&#32467;&#26500;&#65292;&#24182;&#25913;&#36827;&#20102;&#33180;&#34507;&#30333;&#30340;&#20998;&#36776;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.14954</link><description>&lt;p&gt;
&#36890;&#36807;AI&#29983;&#25104;&#30340;&#36741;&#21161;&#31890;&#23376;&#35299;&#20915;&#21333;&#39063;&#31890;&#20919;&#20923;&#30005;&#38236;&#20013;&#30340;&#20248;&#36873;&#21462;&#21521;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing preferred orientation in single-particle cryo-EM through AI-generated auxiliary particles. (arXiv:2309.14954v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14954
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;cryoPROS&#65292;&#36890;&#36807;&#29983;&#25104;&#36741;&#21161;&#31890;&#23376;&#26469;&#35299;&#20915;&#21333;&#39063;&#31890;&#20919;&#20923;&#30005;&#38236;&#20013;&#30340;&#20248;&#36873;&#21462;&#21521;&#38382;&#39064;&#65292;&#21487;&#26377;&#25928;&#22320;&#24674;&#22797;&#38750;&#20542;&#26012;&#25968;&#25454;&#30340;&#39640;&#20998;&#36776;&#29575;&#32467;&#26500;&#65292;&#24182;&#25913;&#36827;&#20102;&#33180;&#34507;&#30333;&#30340;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#39063;&#31890;&#20919;&#20923;&#30005;&#38236;&#39046;&#22495;&#19968;&#30452;&#38754;&#20020;&#30528;&#20248;&#36873;&#21462;&#21521;&#30340;&#25361;&#25112;&#65292;&#32570;&#20047;&#36890;&#29992;&#30340;&#35745;&#31639;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;cryoPROS&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#36741;&#21161;&#31890;&#23376;&#65292;cryoPROS&#35299;&#20915;&#20102;&#23545;&#35266;&#23519;&#31890;&#23376;&#30340;&#21462;&#21521;&#20272;&#35745;&#30340;&#22266;&#26377;&#20559;&#24046;&#12290;&#22312;&#34880;&#20957;&#32032;&#19977;&#32858;&#20307;&#30340;&#21333;&#39063;&#31890;&#20919;&#20923;&#30005;&#38236;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#20351;&#29992;cryoPROS&#24674;&#22797;&#20102;&#38750;&#20542;&#35282;&#25968;&#25454;&#30340;&#36817;&#21407;&#23376;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#32463;&#36807;&#25913;&#36827;&#30340;&#29256;&#26412;cryoPROS-MP&#22312;&#19981;&#20542;&#26012;&#30340;&#21253;&#21547;&#33014;&#26463;&#25928;&#24212;&#30340;&#25968;&#25454;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#33180;&#34507;&#30333;NaX&#30340;&#20998;&#36776;&#29575;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;cryoPROS&#19981;&#38656;&#35201;&#29305;&#27530;&#30340;&#23454;&#39564;&#25110;&#22270;&#20687;&#33719;&#21462;&#25216;&#26415;&#65292;&#20026;&#20248;&#36873;&#21462;&#21521;&#38382;&#39064;&#25552;&#20379;&#20102;&#32431;&#35745;&#31639;&#20294;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The single-particle cryo-EM field faces the persistent challenge of preferred orientation, lacking general computational solutions. We introduce cryoPROS, an AI-based approach designed to address the above issue. By generating the auxiliary particles with a conditional deep generative model, cryoPROS addresses the intrinsic bias in orientation estimation for the observed particles. We effectively employed cryoPROS in the cryo-EM single particle analysis of the hemagglutinin trimer, showing the ability to restore the near-atomic resolution structure on non-tilt data. Moreover, the enhanced version named cryoPROS-MP significantly improves the resolution of the membrane protein NaX using the no-tilted data that contains the effects of micelles. Compared to the classical approaches, cryoPROS does not need special experimental or image acquisition techniques, providing a purely computational yet effective solution for the preferred orientation problem. Finally, we conduct extensive experime
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prototype-based Mean-Teacher (PMT)&#30340;&#26032;&#22411;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31867;&#21407;&#22411;&#32780;&#19981;&#26159;&#22495;&#29305;&#23450;&#23376;&#32593;&#32476;&#26469;&#20445;&#30041;&#22495;&#29305;&#23450;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14950</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#21407;&#22411;&#30340;&#22343;&#20540;&#25945;&#24072;&#30340;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation for Object Detection with Prototype-based Mean-teacher. (arXiv:2309.14950v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prototype-based Mean-Teacher (PMT)&#30340;&#26032;&#22411;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31867;&#21407;&#22411;&#32780;&#19981;&#26159;&#22495;&#29305;&#23450;&#23376;&#32593;&#32476;&#26469;&#20445;&#30041;&#22495;&#29305;&#23450;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35270;&#35273;&#30446;&#26631;&#26816;&#27979;&#22120;&#36866;&#24212;&#20110;&#25805;&#20316;&#30446;&#26631;&#39046;&#22495;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#20351;&#29992;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;&#24403;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26469;&#33258;&#22810;&#20010;&#28304;&#22495;&#26102;&#65292;&#23558;&#23427;&#20204;&#35270;&#20026;&#21333;&#29420;&#30340;&#22495;&#24182;&#36827;&#34892;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#65292;&#30456;&#27604;&#23558;&#36825;&#20123;&#28304;&#22495;&#28151;&#21512;&#24182;&#36827;&#34892;UDA&#65292;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36817;&#26399;&#30340;&#30740;&#31350;&#20063;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#29616;&#26377;&#30340;MSDA&#26041;&#27861;&#23398;&#20064;&#22495;&#19981;&#21464;&#21644;&#22495;&#29305;&#23450;&#21442;&#25968;&#65288;&#23545;&#20110;&#27599;&#20010;&#28304;&#22495;&#65289;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#28982;&#32780;&#65292;&#19982;&#21333;&#28304;UDA&#26041;&#27861;&#19981;&#21516;&#65292;&#23398;&#20064;&#22495;&#29305;&#23450;&#21442;&#25968;&#20351;&#23427;&#20204;&#19982;&#20351;&#29992;&#30340;&#28304;&#22495;&#25968;&#37327;&#25104;&#27491;&#27604;&#22686;&#38271;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#21407;&#22411;&#30340;&#22343;&#20540;&#25945;&#24072;&#65288;PMT&#65289;&#30340;&#26032;&#22411;MSDA&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31867;&#21407;&#22411;&#32780;&#19981;&#26159;&#22495;&#29305;&#23450;&#23376;&#32593;&#32476;&#26469;&#20445;&#30041;&#22495;&#29305;&#23450;&#20449;&#24687;&#12290;&#36825;&#20123;&#21407;&#22411;&#26159;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#30340;&#65292;&#23545;&#40784;&#30456;&#21516;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting visual object detectors to operational target domains is a challenging task, commonly achieved using unsupervised domain adaptation (UDA) methods. When the labeled dataset is coming from multiple source domains, treating them as separate domains and performing a multi-source domain adaptation (MSDA) improves the accuracy and robustness over mixing these source domains and performing a UDA, as observed by recent studies in MSDA. Existing MSDA methods learn domain invariant and domain-specific parameters (for each source domain) for the adaptation. However, unlike single-source UDA methods, learning domain-specific parameters makes them grow significantly proportional to the number of source domains used. This paper proposes a novel MSDA method called Prototype-based Mean-Teacher (PMT), which uses class prototypes instead of domain-specific subnets to preserve domain-specific information. These prototypes are learned using a contrastive loss, aligning the same categories across 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#21488;&#65292;&#20801;&#35768;&#27531;&#38556;&#31038;&#32676;&#21442;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35774;&#36807;&#31243;&#65292;&#20197;&#20102;&#35299;&#27531;&#38556;&#31038;&#32676;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#36755;&#20986;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.14921</link><description>&lt;p&gt;
&#19982;&#27531;&#38556;&#31038;&#32676;&#20849;&#21516;&#21442;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#30340;&#27665;&#20027;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
A Democratic Platform for Engaging with Disabled Community in Generative AI Development. (arXiv:2309.14921v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14921
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#21488;&#65292;&#20801;&#35768;&#27531;&#38556;&#31038;&#32676;&#21442;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35774;&#36807;&#31243;&#65292;&#20197;&#20102;&#35299;&#27531;&#38556;&#31038;&#32676;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#36755;&#20986;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#25105;&#20204;&#30340;&#31038;&#20250;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20687;ChatGPT&#36825;&#26679;&#30340;&#24037;&#20855;&#34987;&#27531;&#38556;&#31038;&#32676;&#30340;&#25104;&#21592;&#20351;&#29992;&#65292;&#20363;&#22914;&#65292;&#33258;&#38381;&#30151;&#24739;&#32773;&#21487;&#20197;&#20351;&#29992;&#23427;&#26469;&#24110;&#21161;&#25776;&#20889;&#30005;&#23376;&#37038;&#20214;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#24433;&#21709;&#21644;&#21463;&#27426;&#36814;&#31243;&#24230;&#20419;&#20351;&#25105;&#20204;&#30740;&#31350;&#23427;&#20204;&#22312;&#27531;&#38556;&#31038;&#32676;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;&#35774;&#35745;&#21644;&#24320;&#21457;&#38454;&#27573;&#36890;&#24120;&#24573;&#35270;&#20102;&#36825;&#20010;&#36793;&#32536;&#21270;&#32676;&#20307;&#65292;&#23548;&#33268;&#20102;&#38024;&#23545;&#20182;&#20204;&#30340;&#39044;&#27979;&#19981;&#20934;&#30830;&#21644;&#19981;&#20844;&#24179;&#30340;&#27495;&#35270;&#12290;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#22312;&#21019;&#24314;&#21644;&#23454;&#26045;&#30340;&#21508;&#20010;&#38454;&#27573;&#20013;&#30340;&#25968;&#25454;&#38598;&#12289;&#31639;&#27861;&#21644;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#25152;&#23548;&#33268;&#12290;&#26412;&#30740;&#35752;&#20250;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#21488;&#65292;&#20197;&#22312;&#26500;&#24314;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26102;&#35753;&#27531;&#38556;&#31038;&#32676;&#21442;&#19982;&#20854;&#20013;&#12290;&#36890;&#36807;&#36825;&#20010;&#24179;&#21488;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#28145;&#20837;&#20102;&#35299;&#24403;&#27531;&#38556;&#31038;&#32676;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#65292;&#23545;&#20854;&#36755;&#20986;&#20135;&#29983;&#20559;&#35265;&#30340;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24076;&#26395;&#29702;&#35299;&#21738;&#20123;&#31639;&#27861;&#22240;&#32032;&#26159;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) systems, especially generative AI technologies are becoming more relevant in our society. Tools like ChatGPT are being used by members of the disabled community e.g., Autistic people may use it to help compose emails. The growing impact and popularity of generative AI tools have prompted us to examine their relevance within the disabled community. The design and development phases often neglect this marginalized group, leading to inaccurate predictions and unfair discrimination directed towards them. This could result from bias in data sets, algorithms, and systems at various phases of creation and implementation. This workshop paper proposes a platform to involve the disabled community while building generative AI systems. With this platform, our aim is to gain insight into the factors that contribute to bias in the outputs generated by generative AI when used by the disabled community. Furthermore, we expect to comprehend which algorithmic factors are the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#35299;&#21367;&#31215;&#25216;&#26415;(LD)&#65292;&#36890;&#36807;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#36870;&#26144;&#23556;&#36827;&#34892;&#39640;&#25928;&#30340;&#36817;&#20284;&#65292;&#26469;&#35299;&#20915;&#22312;&#22823;&#35268;&#27169;&#23646;&#24615;&#22270;&#19978;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26102;&#30340;&#23398;&#20064;&#20559;&#24046;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.14907</link><description>&lt;p&gt;
&#23545;&#22823;&#35268;&#27169;&#23646;&#24615;&#22270;&#19978;&#30340;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#26631;&#31614;&#35299;&#21367;&#31215;&#20197;&#25269;&#25239;&#23398;&#20064;&#20559;&#24046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Label Deconvolution for Node Representation Learning on Large-scale Attributed Graphs against Learning Bias. (arXiv:2309.14907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#35299;&#21367;&#31215;&#25216;&#26415;(LD)&#65292;&#36890;&#36807;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#36870;&#26144;&#23556;&#36827;&#34892;&#39640;&#25928;&#30340;&#36817;&#20284;&#65292;&#26469;&#35299;&#20915;&#22312;&#22823;&#35268;&#27169;&#23646;&#24615;&#22270;&#19978;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26102;&#30340;&#23398;&#20064;&#20559;&#24046;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24102;&#23646;&#24615;&#30340;&#22270;&#20013;&#65292;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#23545;&#35768;&#22810;&#37325;&#35201;&#30340;&#19979;&#28216;&#20219;&#21153;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#21516;&#26102;&#32534;&#30721;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#36827;&#34892;&#25972;&#21512;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#33410;&#28857;&#32534;&#30721;&#22120;(NEs)&#26469;&#32534;&#30721;&#23646;&#24615;&#12290;&#30001;&#20110;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#21516;&#26102;&#35757;&#32451;&#22823;&#22411;NEs&#21644;GNNs&#23384;&#22312;&#20005;&#37325;&#30340;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#26041;&#27861;&#25552;&#20986;&#20102;&#20998;&#21035;&#35757;&#32451;NEs&#21644;GNNs&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#22312;NEs&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#65292;&#20182;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;GNNs&#20013;&#30340;&#29305;&#24449;&#21367;&#31215;&#65292;&#23548;&#33268;&#20102;&#19982;&#32852;&#21512;&#35757;&#32451;&#30456;&#27604;&#30340;&#26174;&#33879;&#23398;&#20064;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26631;&#31614;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21363;&#26631;&#31614;&#35299;&#21367;&#31215;(LD)&#65292;&#36890;&#36807;&#23545;GNNs&#30340;&#36870;&#26144;&#23556;&#36827;&#34892;&#26032;&#39062;&#19988;&#39640;&#24230;&#21487;&#20280;&#32553;&#30340;&#36817;&#20284;&#65292;&#20197;&#20943;&#36731;&#23398;&#20064;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node representation learning on attributed graphs -- whose nodes are associated with rich attributes (e.g., texts and protein sequences) -- plays a crucial role in many important downstream tasks. To encode the attributes and graph structures simultaneously, recent studies integrate pre-trained models with graph neural networks (GNNs), where pre-trained models serve as node encoders (NEs) to encode the attributes. As jointly training large NEs and GNNs on large-scale graphs suffers from severe scalability issues, many methods propose to train NEs and GNNs separately. Consequently, they do not take feature convolutions in GNNs into consideration in the training phase of NEs, leading to a significant learning bias from that by the joint training. To address this challenge, we propose an efficient label regularization technique, namely Label Deconvolution (LD), to alleviate the learning bias by a novel and highly scalable approximation to the inverse mapping of GNNs. The inverse mapping l
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20026;AI&#33402;&#26415;&#24320;&#21457;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#21453;&#24605;&#31995;&#32479;&#30340;&#20004;&#20010;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#33402;&#26415;&#20013;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14877</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#33402;&#26415;&#20013;&#30340;&#21487;&#25345;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explainable Sustainability for AI in the Arts. (arXiv:2309.14877v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14877
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20026;AI&#33402;&#26415;&#24320;&#21457;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#21453;&#24605;&#31995;&#32479;&#30340;&#20004;&#20010;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#33402;&#26415;&#20013;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#22312;&#33402;&#26415;&#23454;&#36341;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#26159;&#29992;&#20110;&#36890;&#30693;&#20174;&#19994;&#32773;&#26377;&#20851;AI&#30340;&#29615;&#22659;&#24433;&#21709;&#65288;&#20197;&#21450;&#20854;&#20182;&#21487;&#25345;&#32493;&#24615;&#38382;&#39064;&#65289;&#30340;&#24037;&#20855;&#36866;&#29992;&#20110;&#20854;&#20182;&#32972;&#26223;&#29615;&#22659;&#65292;&#32780;&#38750;&#21019;&#24847;&#23454;&#36341;&#32972;&#26223;&#65292;&#36825;&#20351;&#24471;&#33402;&#26415;&#23478;&#21644;&#21019;&#24847;&#20174;&#19994;&#32773;&#26080;&#27861;&#33719;&#21462;AI&#24037;&#20855;&#21644;&#21487;&#25345;&#32493;&#24615;&#38382;&#39064;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#25551;&#36848;&#20102;&#20004;&#20010;&#26088;&#22312;&#20026;AI&#33402;&#26415;&#24320;&#21457;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#21453;&#24605;&#31995;&#32479;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#21644;&#20171;&#32461;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#33402;&#26415;&#20013;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI is becoming increasingly popular in artistic practices, but the tools for informing practitioners about the environmental impact (and other sustainability implications) of AI are adapted for other contexts than creative practices -- making the tools and sustainability implications of AI not accessible for artists and creative practitioners. In this position paper, I describe two empirical studies that aim to develop environmental sustainability reflection systems for AI Arts, and discuss and introduce Explainable Sustainability in for AI Arts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LyCORIS&#65292;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35780;&#20272;&#30340;&#20840;&#38754;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.14859</link><description>&lt;p&gt;
&#23548;&#33322;&#25991;&#26412;&#21040;&#22270;&#20687;&#23450;&#21046;&#65306;&#20174;LyCORIS&#24494;&#35843;&#21040;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation. (arXiv:2309.14859v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LyCORIS&#65292;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35780;&#20272;&#30340;&#20840;&#38754;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22240;&#20854;&#33021;&#22815;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#20013;&#65292;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#39046;&#20808;&#30340;&#24320;&#28304;&#27169;&#22411;&#22312;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#36825;&#20123;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#32473;&#26032;&#26041;&#27861;&#30340;&#25972;&#21512;&#21644;&#31995;&#32479;&#35780;&#20272;&#24102;&#26469;&#20102;&#22810;&#37325;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LyCORIS&#65288;Lora beYond Conventional methods&#65292;Other Rank adaptation Implementations for Stable diffusion&#65289;[https://github.com/KohakuBlueleaf/LyCORIS]&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35780;&#20272;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#22810;&#26679;&#21270;&#30340;&#25351;&#26631;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;&#24494;&#35843;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#22312;&#19981;&#21516;&#27010;&#24565;&#31867;&#21035;&#19979;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31867;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation. Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion) [https://github.com/KohakuBlueleaf/LyCORIS], an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion. Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categori
&lt;/p&gt;</description></item><item><title>Supersonic &#26159;&#19968;&#20010;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;C/C++&#20013;&#36827;&#34892;&#28304;&#20195;&#30721;&#20248;&#21270;&#12290;&#19982;GPT-3.5-Turbo&#21644;GPT-4&#30456;&#27604;&#65292;&#23427;&#22312;&#20195;&#30721;&#20248;&#21270;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25913;&#21464;&#30340;&#31243;&#24230;&#26356;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.14846</link><description>&lt;p&gt;
Supersonic: &#23398;&#20064;&#22312;C/C++&#20013;&#29983;&#25104;&#28304;&#20195;&#30721;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Supersonic: Learning to Generate Source Code Optimisations in C/C++. (arXiv:2309.14846v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14846
&lt;/p&gt;
&lt;p&gt;
Supersonic &#26159;&#19968;&#20010;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;C/C++&#20013;&#36827;&#34892;&#28304;&#20195;&#30721;&#20248;&#21270;&#12290;&#19982;GPT-3.5-Turbo&#21644;GPT-4&#30456;&#27604;&#65292;&#23427;&#22312;&#20195;&#30721;&#20248;&#21270;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25913;&#21464;&#30340;&#31243;&#24230;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#20248;&#21270;&#22312;&#20445;&#25345;&#21151;&#33021;&#30340;&#21516;&#26102;&#25913;&#21892;&#36164;&#28304;&#25928;&#29575;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#26159;&#30001;&#24320;&#21457;&#20154;&#21592;&#21644;&#32534;&#35793;&#22120;&#23436;&#25104;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19977;&#31181;&#36873;&#25321;&#65292;&#21363;&#22312;&#28304;&#20195;&#30721;&#32423;&#21035;&#36827;&#34892;&#33258;&#21160;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Supersonic&#65292;&#19968;&#20010;&#38024;&#23545;&#20248;&#21270;&#30340;&#36731;&#24494;&#28304;&#20195;&#30721;&#20462;&#25913;&#30340;&#31070;&#32463;&#26041;&#27861;&#12290;&#20351;&#29992;seq2seq&#27169;&#22411;&#65292;Supersonic&#22312;C / C ++&#31243;&#24207;&#23545;&#65288;$x_{t}$&#65292;$x_{t+1}$&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;$x_{t+1}$&#26159;$x_{t}$&#30340;&#20248;&#21270;&#29256;&#26412;&#65292;&#24182;&#36755;&#20986;&#19968;&#20010;&#24046;&#24322;&#12290;Supersonic&#30340;&#24615;&#33021;&#22312;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#19978;&#19982;OpenAI&#30340;GPT-3.5-Turbo&#21644;GPT-4&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;Supersonic&#19981;&#20165;&#22312;&#20195;&#30721;&#20248;&#21270;&#20219;&#21153;&#19978;&#32988;&#36807;&#20102;&#36825;&#20004;&#20010;&#27169;&#22411;&#65292;&#32780;&#19988;&#25913;&#21464;&#30340;&#31243;&#24230;&#27604;GPT-3.5-Turbo&#23567;&#20102;600&#22810;&#20493;&#65292;&#27604;GPT-4&#23567;&#20102;3700&#22810;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software optimization refines programs for resource efficiency while preserving functionality. Traditionally, it is a process done by developers and compilers. This paper introduces a third option, automated optimization at the source code level. We present Supersonic, a neural approach targeting minor source code modifications for optimization. Using a seq2seq model, Supersonic is trained on C/C++ program pairs ($x_{t}$, $x_{t+1}$), where $x_{t+1}$ is an optimized version of $x_{t}$, and outputs a diff. Supersonic's performance is benchmarked against OpenAI's GPT-3.5-Turbo and GPT-4 on competitive programming tasks. The experiments show that Supersonic not only outperforms both models on the code optimization task, but also minimizes the extent of change with a more than 600x smaller than GPT-3.5-Turbo and 3700x smaller than GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;Softmax&#25513;&#30721;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#20445;&#25345;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#31283;&#23450;&#24615;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14808</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;Softmax&#25513;&#30721;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting Softmax Masking for Stability in Continual Learning. (arXiv:2309.14808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;Softmax&#25513;&#30721;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#20445;&#25345;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#31283;&#23450;&#24615;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#35768;&#22810;&#20998;&#31867;&#22120;&#20351;&#29992;Softmax&#20989;&#25968;&#26469;&#23398;&#20064;&#32622;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#25351;&#20986;&#20854;&#26080;&#27861;&#20934;&#30830;&#30830;&#23450;&#31163;&#32676;&#20540;&#30340;&#32622;&#20449;&#24230;&#20998;&#24067;&#65292;&#36890;&#24120;&#31216;&#20026;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#22266;&#26377;&#38480;&#21046;&#36824;&#38480;&#21046;&#20102;&#22312;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#36873;&#25321;&#20309;&#26102;&#24536;&#35760;&#21644;&#20445;&#30041;&#20808;&#21069;&#35757;&#32451;&#30340;&#32622;&#20449;&#24230;&#20998;&#24067;&#30340;&#20934;&#30830;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#25513;&#30721;Softmax&#20989;&#25968;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#26082;&#31616;&#21333;&#21448;&#26222;&#36941;&#65292;&#20294;&#23545;&#20110;&#22312;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#32622;&#20449;&#24230;&#20998;&#24067;&#65288;&#20063;&#31216;&#20026;&#31283;&#23450;&#24615;&#65289;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35843;&#26597;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;Softmax&#25513;&#30721;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#20445;&#25345;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#22312;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#35760;&#24518;&#37325;&#25918;&#30340;&#31867;-&#21644;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#22686;&#21152;&#20102;&#31283;&#23450;&#24615;&#21516;&#26102;&#20445;&#25345;&#20102;&#36275;&#22815;&#22823;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning, many classifiers use softmax function to learn confidence. However, numerous studies have pointed out its inability to accurately determine confidence distributions for outliers, often referred to as epistemic uncertainty. This inherent limitation also curtails the accurate decisions for selecting what to forget and keep in previously trained confidence distributions over continual learning process. To address the issue, we revisit the effects of masking softmax function. While this method is both simple and prevalent in literature, its implication for retaining confidence distribution during continual learning, also known as stability, has been under-investigated. In this paper, we revisit the impact of softmax masking, and introduce a methodology to utilize its confidence preservation effects. In class- and task-incremental learning benchmarks with and without memory replay, our approach significantly increases stability while maintaining sufficiently large pla
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36275;&#29699;&#27604;&#36187;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#29305;&#24449;&#20248;&#21270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36825;&#20010;&#29305;&#23450;&#30340;&#20219;&#21153;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;</title><link>http://arxiv.org/abs/2309.14807</link><description>&lt;p&gt;
&#35780;&#20272;&#36275;&#29699;&#27604;&#36187;&#39044;&#27979;&#27169;&#22411;&#65306;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#29305;&#24449;&#20248;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating Soccer Match Prediction Models: A Deep Learning Approach and Feature Optimization for Gradient-Boosted Trees. (arXiv:2309.14807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36275;&#29699;&#27604;&#36187;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#29305;&#24449;&#20248;&#21270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36825;&#20010;&#29305;&#23450;&#30340;&#20219;&#21153;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#22320;&#29992;&#20110;&#39044;&#27979;&#36275;&#29699;&#27604;&#36187;&#32467;&#26524;&#65292;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#24320;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20351;&#24471;&#27169;&#22411;&#35780;&#20272;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;2023&#24180;&#36275;&#29699;&#39044;&#27979;&#25361;&#25112;&#35201;&#27714;&#39318;&#20808;&#39044;&#27979;&#27599;&#25903;&#29699;&#38431;&#30340;&#20934;&#30830;&#36827;&#29699;&#25968;&#65292;&#20854;&#27425;&#39044;&#27979;&#32988;&#36127;&#24179;&#30340;&#27010;&#29575;&#12290;&#31454;&#36187;&#25552;&#20379;&#20102;&#21407;&#22987;&#30340;&#35757;&#32451;&#38598;&#21644;&#29305;&#24449;&#65292;&#20294;&#36824;&#22686;&#21152;&#20102;&#22312;2023&#24180;4&#26376;4&#26085;&#33267;4&#26376;13&#26085;&#26399;&#38388;&#36827;&#34892;&#30340;&#39069;&#22806;&#27604;&#36187;&#65292;&#36825;&#20195;&#34920;&#20102;&#35757;&#32451;&#38598;&#25130;&#27490;&#26085;&#26399;&#21040;&#39318;&#27425;&#39044;&#27979;&#27604;&#36187;&#20043;&#38388;&#30340;&#26102;&#26399;&#65288;&#29992;&#20110;&#35780;&#20272;&#24615;&#33021;&#65289;&#12290;&#20351;&#29992;pi-ratings&#20316;&#20026;&#29305;&#24449;&#30340;CatBoost&#27169;&#22411;&#34987;&#24212;&#29992;&#65292;&#26368;&#21021;&#34987;&#30830;&#23450;&#20026;&#35745;&#31639;&#32988;&#36127;&#24179;&#27010;&#29575;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20010;&#29305;&#23450;&#20219;&#21153;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models have become increasingly popular for predicting the results of soccer matches, however, the lack of publicly-available benchmark datasets has made model evaluation challenging. The 2023 Soccer Prediction Challenge required the prediction of match results first in terms of the exact goals scored by each team, and second, in terms of the probabilities for a win, draw, and loss. The original training set of matches and features, which was provided for the competition, was augmented with additional matches that were played between 4 April and 13 April 2023, representing the period after which the training set ended, but prior to the first matches that were to be predicted (upon which the performance was evaluated). A CatBoost model was employed using pi-ratings as the features, which were initially identified as the optimal choice for calculating the win/draw/loss probabilities. Notably, deep learning models have frequently been disregarded in this particular task. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25277;&#21462;&#22411;QA&#27169;&#22411;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26723;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#38480;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32454;&#35843;&#24503;&#35821;QA&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#38024;&#23545;&#23450;&#21046;&#21270;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14805</link><description>&lt;p&gt;
&#32454;&#35843;&#21644;&#23545;&#40784;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#20197;&#36827;&#34892;&#22797;&#26434;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning and aligning question answering models for complex information extraction tasks. (arXiv:2309.14805v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25277;&#21462;&#22411;QA&#27169;&#22411;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26723;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#38480;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32454;&#35843;&#24503;&#35821;QA&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#38024;&#23545;&#23450;&#21046;&#21270;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#25552;&#21319;&#20102;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#21487;&#33021;&#24615;&#12290;&#23613;&#31649;&#20687;ChatGPT&#36825;&#26679;&#30340;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#20026;&#19968;&#20123;&#21830;&#19994;&#29992;&#20363;&#24320;&#21551;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#20294;&#20854;&#24403;&#21069;&#20542;&#21521;&#20110;&#20135;&#29983;&#34394;&#20551;&#20869;&#23481;&#30340;&#29305;&#28857;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#22312;&#25991;&#26723;&#20998;&#26512;(&#22914;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#20449;&#24687;)&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#30456;&#21453;&#65292;&#20687;&#38382;&#39064;&#22238;&#31572;(QA)&#25110;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#36825;&#26679;&#30340;&#25277;&#21462;&#22411;&#35821;&#35328;&#27169;&#22411;&#20445;&#35777;&#26597;&#35810;&#32467;&#26524;&#22312;&#30456;&#24212;&#19978;&#19979;&#25991;&#25991;&#26723;&#30340;&#36793;&#30028;&#20869;&#65292;&#20351;&#20854;&#25104;&#20026;&#20844;&#21496;&#29983;&#20135;&#29615;&#22659;&#20013;&#26356;&#21487;&#38752;&#30340;&#20449;&#24687;&#25552;&#21462;&#20505;&#36873;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#21644;&#25972;&#21512;&#25277;&#21462;&#22411;QA&#27169;&#22411;&#26469;&#25913;&#36827;&#23545;&#24503;&#35821;&#21830;&#19994;&#25991;&#26723;(&#22914;&#20445;&#38505;&#25253;&#21578;&#25110;&#33647;&#21697;&#35828;&#26126;&#20070;)&#30340;&#29305;&#24449;&#25552;&#21462;&#65292;&#24418;&#25104;&#19968;&#20010;&#25991;&#26723;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#32454;&#35843;&#29616;&#26377;&#24503;&#35821;QA&#27169;&#22411;&#21487;&#20197;&#25552;&#21319;&#38024;&#23545;&#23450;&#21046;&#21270;&#20449;&#24687;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of Large Language Models (LLMs) has boosted performance and possibilities in various NLP tasks. While the usage of generative AI models like ChatGPT opens up new opportunities for several business use cases, their current tendency to hallucinate fake content strongly limits their applicability to document analysis, such as information retrieval from documents. In contrast, extractive language models like question answering (QA) or passage retrieval models guarantee query results to be found within the boundaries of an according context document, which makes them candidates for more reliable information extraction in productive environments of companies. In this work we propose an approach that uses and integrates extractive QA models for improved feature extraction of German business documents such as insurance reports or medical leaflets into a document analysis solution. We further show that fine-tuning existing German QA models boosts performance for tailored extractio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FoLiBi&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#23398;&#20064;&#32773;&#30340;&#36951;&#24536;&#34892;&#20026;&#20316;&#20026;&#32447;&#24615;&#20559;&#24046;&#30340;&#24418;&#24335;&#65292;&#32467;&#21512;&#29616;&#26377;&#30340;&#27880;&#37325;&#22411;&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#24573;&#30053;&#23398;&#20064;&#32773;&#36951;&#24536;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#19982;&#22810;&#20010;KT&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#26524;</title><link>http://arxiv.org/abs/2309.14796</link><description>&lt;p&gt;
&#32771;&#34385;&#36951;&#24536;&#30340;&#32447;&#24615;&#20559;&#24046;&#23545;&#27880;&#37325;&#22411;&#30693;&#35782;&#36861;&#36394;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Forgetting-aware Linear Bias for Attentive Knowledge Tracing. (arXiv:2309.14796v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FoLiBi&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#23398;&#20064;&#32773;&#30340;&#36951;&#24536;&#34892;&#20026;&#20316;&#20026;&#32447;&#24615;&#20559;&#24046;&#30340;&#24418;&#24335;&#65292;&#32467;&#21512;&#29616;&#26377;&#30340;&#27880;&#37325;&#22411;&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#24573;&#30053;&#23398;&#20064;&#32773;&#36951;&#24536;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#19982;&#22810;&#20010;KT&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#26088;&#22312;&#22522;&#20110;&#38382;&#39064;&#35299;&#20915;&#21382;&#21490;&#36861;&#36394;&#23398;&#20064;&#29087;&#32451;&#24230;&#65292;&#20197;&#20415;&#20026;&#25105;&#20204;&#25552;&#20379;&#19968;&#22871;&#27969;&#30021;&#30340;&#35838;&#31243;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#31215;&#26497;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26426;&#21046;&#26469;&#25429;&#25417;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#32467;&#21512;&#23398;&#20064;&#32773;&#30340;&#29305;&#24449;&#26469;&#36827;&#34892;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;KT&#27169;&#22411;&#24573;&#30053;&#20102;&#23398;&#20064;&#32773;&#30340;&#36951;&#24536;&#34892;&#20026;&#65292;&#23588;&#20854;&#26159;&#24403;&#20114;&#21160;&#21382;&#21490;&#21464;&#24471;&#26356;&#38271;&#26102;&#12290;&#36825;&#20010;&#38382;&#39064;&#28304;&#20110;&#36807;&#24230;&#20248;&#20808;&#32771;&#34385;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#65292;&#26080;&#24847;&#20013;&#24573;&#35270;&#20102;&#36951;&#24536;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#32771;&#34385;&#36951;&#24536;&#30340;&#32447;&#24615;&#20559;&#24046;&#65288;FoLiBi&#65289;&#65292;&#20197;&#21453;&#26144;&#36951;&#24536;&#34892;&#20026;&#20316;&#20026;&#32447;&#24615;&#20559;&#24046;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#20294;FoLiBi&#36890;&#36807;&#26377;&#25928;&#22320;&#20998;&#35299;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#21644;&#36951;&#24536;&#34892;&#20026;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#19982;&#29616;&#26377;&#30340;&#27880;&#37325;&#22411;KT&#27169;&#22411;&#37197;&#21512;&#20351;&#29992;&#12290;&#23558;FoLiBi&#19982;&#20960;&#20010;KT&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#33719;&#24471;&#19968;&#33268;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Knowledge Tracing (KT) aims to track proficiency based on a question-solving history, allowing us to offer a streamlined curriculum. Recent studies actively utilize attention-based mechanisms to capture the correlation between questions and combine it with the learner's characteristics for responses. However, our empirical study shows that existing attention-based KT models neglect the learner's forgetting behavior, especially as the interaction history becomes longer. This problem arises from the bias that overprioritizes the correlation of questions while inadvertently ignoring the impact of forgetting behavior. This paper proposes a simple-yet-effective solution, namely Forgetting-aware Linear Bias (FoLiBi), to reflect forgetting behavior as a linear bias. Despite its simplicity, FoLiBi is readily equipped with existing attentive KT models by effectively decomposing question correlations with forgetting behavior. FoLiBi plugged with several KT models yields a consistent improvement 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#25968;&#25454;&#30340;&#20132;&#36890;&#28783;-&#36710;&#36947;&#20998;&#37197;&#35821;&#20041;&#22320;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#33258;&#21160;&#25512;&#23548;&#20132;&#36890;&#28783;&#21040;&#36710;&#36947;&#30340;&#20998;&#37197;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#23433;&#20840;&#32771;&#34385;&#21644;&#25968;&#25454;&#38598;&#36716;&#25442;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#26524;&#21644;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14793</link><description>&lt;p&gt;
&#22522;&#20110;&#36816;&#21160;&#25968;&#25454;&#30340;&#20132;&#36890;&#28783;-&#36710;&#36947;&#20998;&#37197;&#35821;&#20041;&#22320;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semantic Map Learning of Traffic Light to Lane Assignment based on Motion Data. (arXiv:2309.14793v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#25968;&#25454;&#30340;&#20132;&#36890;&#28783;-&#36710;&#36947;&#20998;&#37197;&#35821;&#20041;&#22320;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#33258;&#21160;&#25512;&#23548;&#20132;&#36890;&#28783;&#21040;&#36710;&#36947;&#30340;&#20998;&#37197;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#23433;&#20840;&#32771;&#34385;&#21644;&#25968;&#25454;&#38598;&#36716;&#25442;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#26524;&#21644;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21738;&#20010;&#20132;&#36890;&#28783;&#25511;&#21046;&#21738;&#20010;&#36710;&#36947;&#23545;&#20110;&#23433;&#20840;&#36890;&#36807;&#36335;&#21475;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36890;&#24120;&#20381;&#36182;&#21253;&#21547;&#20132;&#36890;&#28783;-&#36710;&#36947;&#20998;&#37197;&#20449;&#24687;&#30340;&#39640;&#28165;&#22320;&#22270;&#12290;&#25163;&#21160;&#25552;&#20379;&#36825;&#20123;&#20449;&#24687;&#26082;&#36153;&#26102;&#21448;&#26114;&#36149;&#65292;&#32780;&#19988;&#19981;&#21487;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20132;&#36890;&#28783;&#29366;&#24577;&#21644;&#36710;&#36742;&#36816;&#21160;&#27169;&#24335;&#25512;&#23548;&#20986;&#20998;&#37197;&#26041;&#24335;&#12290;&#35813;&#26041;&#27861;&#33258;&#21160;&#21270;&#24182;&#19988;&#19981;&#20381;&#36182;&#20960;&#20309;&#25490;&#21015;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#21644;&#35780;&#20272;&#22522;&#20110;&#27169;&#24335;&#30340;&#36129;&#29486;&#26041;&#27861;&#23637;&#31034;&#20102;&#22522;&#26412;&#32479;&#35745;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#25298;&#32477;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#32771;&#34385;&#20102;&#23433;&#20840;&#22240;&#32032;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#38598;&#36716;&#25442;&#26041;&#27861;&#65292;&#20197;&#37325;&#26032;&#21033;&#29992;&#29616;&#26377;&#30340;&#36816;&#21160;&#39044;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#35821;&#20041;&#22320;&#22270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;Lyft Level 5&#25968;&#25454;&#38598;&#30340;&#20844;&#24320;API&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding which traffic light controls which lane is crucial to navigate intersections safely. Autonomous vehicles commonly rely on High Definition (HD) maps that contain information about the assignment of traffic lights to lanes. The manual provisioning of this information is tedious, expensive, and not scalable. To remedy these issues, our novel approach derives the assignments from traffic light states and the corresponding motion patterns of vehicle traffic. This works in an automated way and independently of the geometric arrangement. We show the effectiveness of basic statistical approaches for this task by implementing and evaluating a pattern-based contribution method. In addition, our novel rejection method includes accompanying safety considerations by leveraging statistical hypothesis testing. Finally, we propose a dataset transformation to re-purpose available motion prediction datasets for semantic map learning. Our publicly available API for the Lyft Level 5 dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#19982;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#32467;&#21512;&#24212;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#20998;&#31867;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#38646;&#21806;&#19994;&#30340;&#23458;&#25143;&#21644;&#20195;&#29702;&#20154;&#20132;&#20114;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#19979;&#65292;SLM T5-base&#33021;&#22815;&#23454;&#29616;&#32422;75%&#30340;&#20934;&#30830;&#29575;&#65292;&#23637;&#29616;&#20102;SLMs&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14779</link><description>&lt;p&gt;
&#20351;&#29992;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#25506;&#32034;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#25928;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Small Language Models with Prompt-Learning Paradigm for Efficient Domain-Specific Text Classification. (arXiv:2309.14779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#19982;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#32467;&#21512;&#24212;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#20998;&#31867;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#38646;&#21806;&#19994;&#30340;&#23458;&#25143;&#21644;&#20195;&#29702;&#20154;&#20132;&#20114;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#19979;&#65292;SLM T5-base&#33021;&#22815;&#23454;&#29616;&#32422;75%&#30340;&#20934;&#30830;&#29575;&#65292;&#23637;&#29616;&#20102;SLMs&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#25163;&#21160;&#26631;&#35760;&#30340;&#39640;&#25104;&#26412;&#65292;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#20998;&#31867;&#38754;&#20020;&#31232;&#32570;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#25552;&#31034;&#23398;&#20064;&#20316;&#20026;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#20294;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65292;&#23567;&#20110;10&#20159;&#20010;&#21442;&#25968;&#65289;&#22312;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#23450;&#21046;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#65292;&#31526;&#21512;&#24037;&#19994;&#32422;&#26463;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;SLMs&#19982;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#32467;&#21512;&#24212;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#20998;&#31867;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#38646;&#21806;&#19994;&#30340;&#23458;&#25143;&#21644;&#20195;&#29702;&#20154;&#20132;&#20114;&#20013;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#23569;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#21487;&#20197;&#36827;&#34892;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#24494;&#35843;&#26102;&#65292;&#20855;&#26377;220M&#21442;&#25968;&#30340;&#20856;&#22411;SLM T5-base&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#19978;&#23454;&#29616;&#32422;75%&#30340;&#20934;&#30830;&#29575;&#65288;&#36798;&#21040;&#23436;&#25972;&#25968;&#25454;&#30340;15%&#65289;&#65292;&#26174;&#31034;&#20986;SLMs&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-specific text classification faces the challenge of scarce labeled data due to the high cost of manual labeling. Prompt-learning, known for its efficiency in few-shot scenarios, is proposed as an alternative to traditional fine-tuning methods. And besides, although large language models (LLMs) have gained prominence, small language models (SLMs, with under 1B parameters) offer significant customizability, adaptability, and cost-effectiveness for domain-specific tasks, given industry constraints. In this study, we investigate the potential of SLMs combined with prompt-learning paradigm for domain-specific text classification, specifically within customer-agent interactions in retail. Our evaluations show that, in few-shot settings when prompt-based model fine-tuning is possible, T5-base, a typical SLM with 220M parameters, achieve approximately 75% accuracy with limited labeled data (up to 15% of full data), which shows great potentials of SLMs with prompt-learning. Based on this
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20107;&#23454;&#30693;&#35782;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#35843;&#20248;&#26694;&#26550;&#26469;&#25913;&#21892;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14771</link><description>&lt;p&gt;
&#20351;&#29992;&#20107;&#23454;&#30693;&#35782;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Boosting In-Context Learning with Factual Knowledge. (arXiv:2309.14771v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20107;&#23454;&#30693;&#35782;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#35843;&#20248;&#26694;&#26550;&#26469;&#25913;&#21892;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26088;&#22312;&#36890;&#36807;&#20381;&#36182;&#20110;&#23569;&#37327;&#30340;&#35757;&#32451;&#31034;&#20363;&#35299;&#20915;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#28040;&#38500;&#21442;&#25968;&#26356;&#26032;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20107;&#23454;&#30693;&#35782;&#22312;ICL&#30340;&#24615;&#33021;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#21253;&#25324;&#22312;LLM&#20013;&#23398;&#21040;&#30340;&#22266;&#26377;&#30693;&#35782;&#65292;&#20174;&#25152;&#36873;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#24471;&#20986;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20197;&#21450;LLM&#22312;&#36755;&#20986;&#29983;&#25104;&#20013;&#30340;&#30693;&#35782;&#20559;&#24046;&#12290;&#20026;&#20102;&#21457;&#25381;LLM&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#35843;&#20248;&#65288;KICT&#65289;&#26694;&#26550;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;ICL&#30340;&#24615;&#33021;&#65306;1&#65289;&#22312;&#25345;&#32493;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26399;&#38388;&#21521;LLM&#27880;&#20837;&#20107;&#23454;&#30693;&#35782;&#65292;2&#65289;&#35880;&#24910;&#36873;&#25321;&#20855;&#26377;&#39640;&#30693;&#35782;&#30456;&#20851;&#24615;&#30340;&#31034;&#20363;&#65292;3&#65289;&#26681;&#25454;&#20808;&#21069;&#30340;&#30693;&#35782;&#23545;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#26657;&#20934;&#12290;&#25105;&#20204;&#22312;&#33258;&#22238;&#24402;LLM&#65288;&#22914;GPT&#39118;&#26684;&#27169;&#22411;&#65289;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL) over Large language models (LLMs) aims at solving previously unseen tasks by conditioning on a few training examples, eliminating the need for parameter updates and achieving competitive performance. In this paper, we demonstrate that factual knowledge is imperative for the performance of ICL in three core facets, i.e., the inherent knowledge learned in LLMs, the factual knowledge derived from the selected in-context examples, and the knowledge biases in LLMs for output generation. To unleash the power of LLMs in few-shot learning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further improve the performance of ICL: 1) injecting factual knowledge to LLMs during continual self-supervised pre-training, 2) judiciously selecting the examples with high knowledge relevance, and 3) calibrating the prediction results based on prior knowledge. We evaluate the proposed approaches on auto-regressive LLMs (e.g., GPT-style models) over 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;CodeT5&#36827;&#34892;&#26368;&#23567;&#32534;&#36753;&#30340;&#31243;&#24207;&#20462;&#22797;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#38169;&#35823;&#21644;&#27491;&#30830;&#31243;&#24207;&#30340;&#20195;&#30721;&#23545;&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;CodeT5&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#26524;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.14760</link><description>&lt;p&gt;
&#20351;&#29992;CodeT5&#36827;&#34892;&#26368;&#23567;&#32534;&#36753;&#30340;&#31243;&#24207;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Program Repair with Minimal Edits Using CodeT5. (arXiv:2309.14760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;CodeT5&#36827;&#34892;&#26368;&#23567;&#32534;&#36753;&#30340;&#31243;&#24207;&#20462;&#22797;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#38169;&#35823;&#21644;&#27491;&#30830;&#31243;&#24207;&#30340;&#20195;&#30721;&#23545;&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;CodeT5&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#21592;&#24448;&#24448;&#38590;&#20197;&#35782;&#21035;&#21644;&#20462;&#22797;&#31243;&#24207;&#20013;&#30340;&#38169;&#35823;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#32463;&#34987;&#25552;&#20986;&#29992;&#26469;&#20462;&#22797;&#38169;&#35823;&#30340;&#31243;&#24207;&#24182;&#25903;&#25345;&#38169;&#35823;&#24674;&#22797;&#12290;&#28982;&#32780;&#65292;LMs&#24448;&#24448;&#20250;&#29983;&#25104;&#19982;&#21407;&#22987;&#36755;&#20837;&#31243;&#24207;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#30340;&#29702;&#35299;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;CodeT5&#24314;&#35758;&#36827;&#34892;&#26368;&#23567;&#20462;&#22797;&#32534;&#36753;&#30340;&#27491;&#30830;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#38169;&#35823;&#21644;&#27491;&#30830;&#31243;&#24207;&#30340;&#20195;&#30721;&#23545;&#19978;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;CodeT5&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;CodeT5&#30340;&#36890;&#36807;&#29575;&#20026;91.95%&#65292;&#26368;&#30456;&#20284;&#30340;&#27491;&#30830;&#31243;&#24207;&#30340;&#24179;&#22343;&#32534;&#36753;&#36317;&#31163;&#20026;6.84&#65292;&#36825;&#34920;&#26126;&#33267;&#23569;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;100&#20010;&#20505;&#36873;&#31243;&#24207;&#26469;&#24314;&#35758;&#19968;&#20010;&#27491;&#30830;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#21021;&#32423;&#32534;&#31243;&#38382;&#39064;&#26102;&#24314;&#35758;&#20351;&#29992;&#26368;&#23567;&#32534;&#36753;&#30340;&#31243;&#24207;&#20462;&#22797;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Programmers often struggle to identify and fix bugs in their programs. In recent years, many language models (LMs) have been proposed to fix erroneous programs and support error recovery. However, the LMs tend to generate solutions that differ from the original input programs. This leads to potential comprehension difficulties for users. In this paper, we propose an approach to suggest a correct program with minimal repair edits using CodeT5. We fine-tune a pre-trained CodeT5 on code pairs of wrong and correct programs and evaluate its performance with several baseline models. The experimental results show that the fine-tuned CodeT5 achieves a pass@100 of 91.95% and an average edit distance of the most similar correct program of 6.84, which indicates that at least one correct program can be suggested by generating 100 candidate programs. We demonstrate the effectiveness of LMs in suggesting program repair with minimal edits for solving introductory programming problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24212;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#20154;&#26426;&#32676;&#20174;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#23454;&#26102;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#29289;&#32852;&#32593;&#20013;&#20449;&#24687;&#30340;&#24180;&#40836;&#26368;&#23567;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#20316;&#21644;&#37096;&#20998;&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.14757</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#20154;&#26426;&#32676;&#23454;&#29616;&#22823;&#35268;&#27169;&#29289;&#32852;&#32593;&#30340;&#24180;&#40836;&#26368;&#23567;&#21270;&#65306;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Age Minimization in Massive IoT via UAV Swarm: A Multi-agent Reinforcement Learning Approach. (arXiv:2309.14757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24212;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#20154;&#26426;&#32676;&#20174;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#23454;&#26102;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#29289;&#32852;&#32593;&#20013;&#20449;&#24687;&#30340;&#24180;&#40836;&#26368;&#23567;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#20316;&#21644;&#37096;&#20998;&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22823;&#35268;&#27169;&#29289;&#32852;&#32593;&#36890;&#20449;&#22330;&#26223;&#20013;&#65292;&#29289;&#32852;&#32593;&#35774;&#22791;&#38656;&#35201;&#30001;&#33021;&#22815;&#38752;&#36817;&#29289;&#32852;&#32593;&#35774;&#22791;&#24182;&#20943;&#23569;&#19978;&#34892;&#33021;&#37327;&#28040;&#32791;&#30340;&#21160;&#24577;&#21333;&#20803;&#36827;&#34892;&#35206;&#30422;&#12290;&#19968;&#31181;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#37096;&#32626;&#22823;&#37327;&#26080;&#20154;&#26426;&#65288;&#26080;&#20154;&#26426;&#32676;&#65289;&#25552;&#20379;&#35206;&#30422;&#24182;&#20026;&#29289;&#32852;&#32593;&#32593;&#32476;&#25552;&#20379;&#26356;&#22909;&#30340;&#35270;&#32447;&#36830;&#36890;&#24615;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#36825;&#20123;&#20855;&#26377;&#22823;&#37327;&#26381;&#21153;&#21333;&#20803;&#30340;&#22823;&#35268;&#27169;&#29289;&#32852;&#32593;&#22330;&#26223;&#20250;&#24341;&#23548;&#20986;&#20855;&#26377;&#39640;&#22797;&#26434;&#24615;&#30340;&#39640;&#32500;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#30001;&#37096;&#32626;&#26080;&#20154;&#26426;&#32676;&#20174;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#23454;&#26102;&#20449;&#24687;&#24341;&#36215;&#30340;&#39640;&#32500;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#23558;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#24180;&#40836;&#26368;&#23567;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#20316;&#21644;&#37096;&#20998;&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#32988;&#36807;&#39640;&#22797;&#26434;&#24615;&#30340;&#38598;&#20013;&#24335;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#26080;&#33021;&#20026;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many massive IoT communication scenarios, the IoT devices require coverage from dynamic units that can move close to the IoT devices and reduce the uplink energy consumption. A robust solution is to deploy a large number of UAVs (UAV swarm) to provide coverage and a better line of sight (LoS) for the IoT network. However, the study of these massive IoT scenarios with a massive number of serving units leads to high dimensional problems with high complexity. In this paper, we apply multi-agent deep reinforcement learning to address the high-dimensional problem that results from deploying a swarm of UAVs to collect fresh information from IoT devices. The target is to minimize the overall age of information in the IoT network. The results reveal that both cooperative and partially cooperative multi-agent deep reinforcement learning approaches are able to outperform the high-complexity centralized deep reinforcement learning approach, which stands helpless in large-scale networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;AILQA&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#35299;&#26512;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#24182;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.14735</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#21516;&#26816;&#32034;&#21644;&#38382;&#31572;&#27169;&#22411;&#30340;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Artificial Intelligence for Indian Legal Question Answering (AILQA) Using Different Retrieval and QA Models. (arXiv:2309.14735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;AILQA&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#35299;&#26512;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#24182;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#38382;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#26377;&#28508;&#21147;&#25913;&#21464;&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#19982;&#26696;&#20363;&#25991;&#20214;&#30340;&#20114;&#21160;&#26041;&#24335;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#21360;&#24230;&#27861;&#24459;&#20307;&#31995;&#19979;&#22238;&#31572;&#27861;&#24459;&#38382;&#39064;&#30340;&#25928;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#65288;AILQA&#65289;&#24182;&#30740;&#31350;&#20102;&#24403;&#21069;&#21487;&#29992;&#30340;&#19981;&#21516;&#26816;&#32034;&#21644;QA&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#21033;&#29992;OpenAI GPT&#27169;&#22411;&#20316;&#20026;&#22522;&#20934;&#65292;&#32467;&#21512;&#26597;&#35810;&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#29616;&#26377;&#30340;AILQA&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#35299;&#26512;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#24182;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#26412;&#30740;&#31350;&#29305;&#21035;&#20851;&#27880;&#21360;&#24230;&#21009;&#20107;&#21496;&#27861;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35813;&#39046;&#22495;&#30001;&#20110;&#22797;&#26434;&#24615;&#21644;&#36164;&#28304;&#38480;&#21046;&#32780;&#38754;&#20020;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;&#20026;&#20102;&#20005;&#26684;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32463;&#39564;&#35777;&#35780;&#20272;&#19982;&#20174;&#23454;&#36341;&#20013;&#33719;&#24471;&#30340;&#21453;&#39304;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal question-answering (QA) systems have the potential to revolutionize the way legal professionals interact with case law documents. This paper conducts a comparative analysis of existing artificial intelligence models for their utility in answering legal questions within the Indian legal system, specifically focusing on Indian Legal Question Answering (AILQA) and our study investigates the efficacy of different retrieval and QA algorithms currently available. Utilizing the OpenAI GPT model as a benchmark, along with query prompts, our investigation shows that existing AILQA systems can automatically interpret natural language queries from users and generate highly accurate responses. This research is particularly focused on applications within the Indian criminal justice domain, which has its own set of challenges due to its complexity and resource constraints. In order to rigorously assess the performance of these models, empirical evaluations are complemented by feedback from pra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#35299;&#20915;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#31574;&#30053;&#26356;&#26032;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#21644;&#31454;&#20105;&#20219;&#21153;&#20197;&#21450;&#20256;&#32479;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.14727</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#19982;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Effective Multi-Agent Deep Reinforcement Learning Control with Relative Entropy Regularization. (arXiv:2309.14727v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#35299;&#20915;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#31574;&#30053;&#26356;&#26032;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#21644;&#31454;&#20105;&#20219;&#21153;&#20197;&#21450;&#20256;&#32479;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#21363;&#22810;&#26234;&#33021;&#20307;&#36830;&#32493;&#21160;&#24577;&#31574;&#30053;&#26799;&#24230;&#65288;MACDPP&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#25511;&#21046;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#23384;&#22312;&#30340;&#33021;&#21147;&#26377;&#38480;&#21644;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#21040;&#20013;&#24515;&#21270;&#35757;&#32451;&#19982;&#20998;&#25955;&#25191;&#34892;&#65288;CTDE&#65289;&#26694;&#26550;&#20013;&#30340;Actor-Critic&#65288;AC&#65289;&#32467;&#26500;&#65292;&#23427;&#32531;&#35299;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#31574;&#30053;&#26356;&#26032;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23545;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#21644;&#31454;&#20105;&#20219;&#21153;&#20197;&#21450;&#21253;&#25324;OpenAI&#22522;&#20934;&#21644;&#26426;&#26800;&#33218;&#25805;&#20316;&#22312;&#20869;&#30340;&#20256;&#32479;&#25511;&#21046;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;MACDPP&#22312;&#23398;&#20064;&#33021;&#21147;&#21644;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#30456;&#36739;&#20110;&#30456;&#20851;&#30340;&#22810;&#26234;&#33021;&#20307;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#21333;&#26234;&#33021;&#20307;&#22522;&#32447;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#22240;&#27492;&#25193;&#23637;&#20102;MARL&#22312;&#26377;&#25928;&#23398;&#20064;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a novel Multi-agent Reinforcement Learning (MARL) approach, Multi-Agent Continuous Dynamic Policy Gradient (MACDPP) was proposed to tackle the issues of limited capability and sample efficiency in various scenarios controlled by multiple agents. It alleviates the inconsistency of multiple agents' policy updates by introducing the relative entropy regularization to the Centralized Training with Decentralized Execution (CTDE) framework with the Actor-Critic (AC) structure. Evaluated by multi-agent cooperation and competition tasks and traditional control tasks including OpenAI benchmarks and robot arm manipulation, MACDPP demonstrates significant superiority in learning capability and sample efficiency compared with both related multi-agent and widely implemented signal-agent baselines and therefore expands the potential of MARL in effectively learning challenging control scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26356;&#36866;&#24212;&#20110;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#35813;&#27169;&#22411;&#20998;&#20026;&#20010;&#20154;&#32423;&#21035;&#12289;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#23567;&#22411;&#21270;&#20197;&#36866;&#24212;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#65292;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.14726</link><description>&lt;p&gt;
PLMM&#65306;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PLMM: Personal Large Models on Mobile Devices. (arXiv:2309.14726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26356;&#36866;&#24212;&#20110;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#35813;&#27169;&#22411;&#20998;&#20026;&#20010;&#20154;&#32423;&#21035;&#12289;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#23567;&#22411;&#21270;&#20197;&#36866;&#24212;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#65292;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26356;&#36866;&#24212;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#22914;&#25945;&#32946;&#32972;&#26223;&#21644;&#29233;&#22909;&#12290;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#20026;&#19977;&#20010;&#32423;&#21035;&#65306;&#20010;&#20154;&#32423;&#21035;&#65292;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#12290;&#20010;&#20154;&#32423;&#21035;&#27169;&#22411;&#36866;&#24212;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#23545;&#29992;&#25143;&#30340;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#24182;&#20445;&#25252;&#20854;&#38544;&#31169;&#12290;&#19987;&#23478;&#32423;&#21035;&#27169;&#22411;&#19987;&#27880;&#20110;&#21512;&#24182;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#22914;&#37329;&#34701;&#12289;IT&#21644;&#33402;&#26415;&#12290;&#20256;&#32479;&#27169;&#22411;&#19987;&#27880;&#20110;&#26222;&#36941;&#30693;&#35782;&#30340;&#21457;&#29616;&#21644;&#25552;&#21319;&#19987;&#23478;&#27169;&#22411;&#12290;&#22312;&#36825;&#26679;&#30340;&#20998;&#31867;&#20013;&#65292;&#20010;&#20154;&#27169;&#22411;&#30452;&#25509;&#19982;&#29992;&#25143;&#20132;&#20114;&#12290;&#23545;&#20110;&#25972;&#20010;&#31995;&#32479;&#26469;&#35828;&#65292;&#20010;&#20154;&#27169;&#22411;&#20855;&#26377;&#29992;&#25143;&#30340;&#65288;&#21152;&#23494;&#30340;&#65289;&#20010;&#20154;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#36275;&#22815;&#23567;&#20197;&#22312;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#12290;&#26368;&#21518;&#65292;&#23427;&#20204;&#36824;&#24517;&#39035;&#23454;&#26102;&#21709;&#24212;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by Federated Learning, in this paper, we propose personal large models that are distilled from traditional large language models but more adaptive to local users' personal information such as education background and hobbies. We classify the large language models into three levels: the personal level, expert level and traditional level. The personal level models are adaptive to users' personal information. They encrypt the users' input and protect their privacy. The expert level models focus on merging specific knowledge such as finance, IT and art. The traditional models focus on the universal knowledge discovery and upgrading the expert models. In such classifications, the personal models directly interact with the user. For the whole system, the personal models have users' (encrypted) personal information. Moreover, such models must be small enough to be performed on personal computers or mobile devices. Finally, they also have to response in real-time for better user exper
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#21327;&#20316;&#20195;&#29702;&#20043;&#38388;&#22996;&#27966;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#31649;&#29702;&#22120;&#26681;&#25454;&#20219;&#21153;&#32489;&#25928;&#32570;&#38519;&#36827;&#34892;&#22996;&#27966;&#20915;&#31574;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#29615;&#22659;&#34920;&#31034;&#19979;&#30340;&#22242;&#38431;&#25805;&#20316;&#12290;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#31649;&#29702;&#22242;&#38431;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14718</link><description>&lt;p&gt;
&#20248;&#21270;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#21327;&#20316;&#20195;&#29702;&#20043;&#38388;&#30340;&#22996;&#27966;
&lt;/p&gt;
&lt;p&gt;
Optimizing delegation between human and AI collaborative agents. (arXiv:2309.14718v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#21327;&#20316;&#20195;&#29702;&#20043;&#38388;&#22996;&#27966;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#31649;&#29702;&#22120;&#26681;&#25454;&#20219;&#21153;&#32489;&#25928;&#32570;&#38519;&#36827;&#34892;&#22996;&#27966;&#20915;&#31574;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#29615;&#22659;&#34920;&#31034;&#19979;&#30340;&#22242;&#38431;&#25805;&#20316;&#12290;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#31649;&#29702;&#22242;&#38431;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#25110;&#33258;&#20027;&#20195;&#29702;&#24418;&#25104;&#28151;&#21512;&#22242;&#38431;&#30340;&#24773;&#26223;&#20013;&#65292;&#31934;&#30830;&#22320;&#30830;&#23450;&#20309;&#26102;&#25480;&#26435;&#22242;&#38431;&#25104;&#21592;&#25191;&#34892;&#34892;&#21160;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#37492;&#20110;&#36807;&#21435;&#30340;&#20363;&#23376;&#20013;&#65292;&#20154;&#31867;&#21644;&#33258;&#20027;&#31995;&#32479;&#22312;&#20219;&#21153;&#19978;&#21487;&#33021;&#25104;&#21151;&#20063;&#21487;&#33021;&#22833;&#36133;&#65292;&#25105;&#20204;&#35797;&#22270;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#31649;&#29702;&#22120;&#26469;&#26681;&#25454;&#36825;&#20123;&#28508;&#22312;&#30340;&#32489;&#25928;&#32570;&#38519;&#20570;&#20986;&#22996;&#27966;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#19981;&#33021;&#24635;&#26159;&#26399;&#26395;&#21508;&#31181;&#20195;&#29702;&#22312;&#30456;&#21516;&#30340;&#29615;&#22659;&#27169;&#22411;&#20013;&#36816;&#34892;&#12290;&#21487;&#33021;&#20250;&#36935;&#21040;&#20195;&#29702;&#20043;&#38388;&#30340;&#34892;&#21160;&#21644;&#36716;&#25442;&#26377;&#25152;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#32463;&#36807;&#35266;&#23519;&#22242;&#38431;&#32489;&#25928;&#23398;&#20064;&#30340;&#31649;&#29702;&#27169;&#22411;&#65292;&#32780;&#19981;&#38480;&#21046;&#20195;&#29702;&#19982;&#21305;&#37197;&#30340;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31649;&#29702;&#22120;&#33021;&#22815;&#22312;&#25805;&#20316;&#29615;&#22659;&#30340;&#19981;&#21516;&#34920;&#31034;&#19979;&#36827;&#34892;&#22996;&#27966;&#20915;&#31574;&#65292;&#36828;&#36828;&#36229;&#36807;&#20102;&#20854;&#20182;&#31649;&#29702;&#22242;&#38431;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of humans operating with artificial or autonomous agents in a hybrid team, it is essential to accurately identify when to authorize those team members to perform actions. Given past examples where humans and autonomous systems can either succeed or fail at tasks, we seek to train a delegating manager agent to make delegation decisions with respect to these potential performance deficiencies. Additionally, we cannot always expect the various agents to operate within the same underlying model of the environment. It is possible to encounter cases where the actions and transitions would vary between agents. Therefore, our framework provides a manager model which learns through observations of team performance without restricting agents to matching dynamics. Our results show our manager learns to perform delegation decisions with teams of agents operating under differing representations of the environment, significantly outperforming alternative methods to manage the team.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14681</link><description>&lt;p&gt;
&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26377;&#24517;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Human-generated Demonstrations Necessary for In-context Learning?. (arXiv:2309.14681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#33391;&#22909;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#20294;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#26631;&#20934;&#33539;&#24335;&#20013;&#23384;&#22312;&#20197;&#19979;&#24330;&#31471;&#65306;&#26131;&#21463;&#36873;&#23450;&#28436;&#31034;&#30340;&#24433;&#21709;&#65292;&#29983;&#25104;&#36825;&#20123;&#28436;&#31034;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;ICL&#65292;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19981;&#20381;&#36182;&#20154;&#24037;&#28436;&#31034;&#30340;&#33539;&#20363;&#12290;SEC&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#65292;&#19981;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#31034;&#20363;&#20316;&#20026;ICL&#20013;&#30340;&#28436;&#31034;&#65292;&#32780;&#26159;&#35201;&#27714;LLMs&#39318;&#20808;&#33258;&#34892;&#21019;&#24314;&#28436;&#31034;&#65292;&#28982;&#21518;&#29983;&#25104;&#26368;&#32456;&#36755;&#20986;&#12290;SEC&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21487;&#36866;&#24212;&#21407;&#22987;ICL&#21644;&#8220;&#24605;&#32500;&#38142;&#8221;&#65288;CoT&#65289;&#65292;&#24182;&#19988;&#26356;&#21152;&#20415;&#25463;&#65306;&#22240;&#20026;&#21487;&#20197;&#33410;&#30465;&#31034;&#20363;&#21644;&#29702;&#30001;&#30340;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#31639;&#26415;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understandin
&lt;/p&gt;</description></item><item><title>XGV-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;CodeBERT&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#36719;&#20214;&#28431;&#27934;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;XGV-BERT&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#28431;&#27934;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14677</link><description>&lt;p&gt;
XGV-BERT:&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#30340;&#36719;&#20214;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
XGV-BERT: Leveraging Contextualized Language Model and Graph Neural Network for Efficient Software Vulnerability Detection. (arXiv:2309.14677v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14677
&lt;/p&gt;
&lt;p&gt;
XGV-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;CodeBERT&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#36719;&#20214;&#28431;&#27934;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;XGV-BERT&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#28431;&#27934;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#25581;&#31034;&#36719;&#20214;&#28431;&#27934;&#30340;&#23581;&#35797;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#32570;&#20047;&#33021;&#22815;&#20445;&#30041;&#28304;&#20195;&#30721;&#23646;&#24615;&#30340;&#38750;&#39034;&#24207;&#35821;&#20041;&#29305;&#24449;&#21644;&#19978;&#19979;&#25991;&#20851;&#31995;&#30340;&#26377;&#25928;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;XGV-BERT&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;CodeBERT&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#29992;&#20110;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;XGV-BERT&#20013;&#32852;&#21512;&#35757;&#32451;CodeBERT&#21644;GCN&#27169;&#22359;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21033;&#29992;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12289;&#21033;&#29992;&#24222;&#22823;&#21407;&#22987;&#25968;&#25454;&#21644;&#36890;&#36807;&#22270;&#21367;&#31215;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#30340;&#36801;&#31227;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;VulDeePecker&#21644;SySeVR&#31561;&#20004;&#31181;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;XGV-BERT&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#28431;&#27934;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#23545;&#20110;VulDeePecker&#25968;&#25454;&#38598;&#65292;XGV-BERT&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;F1-s
&lt;/p&gt;
&lt;p&gt;
With the advancement of deep learning (DL) in various fields, there are many attempts to reveal software vulnerabilities by data-driven approach. Nonetheless, such existing works lack the effective representation that can retain the non-sequential semantic characteristics and contextual relationship of source code attributes. Hence, in this work, we propose XGV-BERT, a framework that combines the pre-trained CodeBERT model and Graph Neural Network (GCN) to detect software vulnerabilities. By jointly training the CodeBERT and GCN modules within XGV-BERT, the proposed model leverages the advantages of large-scale pre-training, harnessing vast raw data, and transfer learning by learning representations for training data through graph convolution. The research results demonstrate that the XGV-BERT method significantly improves vulnerability detection accuracy compared to two existing methods such as VulDeePecker and SySeVR. For the VulDeePecker dataset, XGV-BERT achieves an impressive F1-s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;UPTST&#27169;&#22411;&#65292;&#21033;&#29992;&#33133;&#21693;&#21475;&#28814;&#25968;&#25454;&#25552;&#21319;&#25163;&#36275;&#21475;&#30149;&#20303;&#38498;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#22312;&#21307;&#38498;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14674</link><description>&lt;p&gt;
&#21033;&#29992;&#33133;&#21693;&#21475;&#28814;&#25968;&#25454;&#25552;&#21319;&#22522;&#20110;UPTST&#30340;&#25163;&#36275;&#21475;&#30149;&#20303;&#38498;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Herpangina Data to Enhance Hospital-level Prediction of Hand-Foot-and-Mouth Disease Admissions Using UPTST. (arXiv:2309.14674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14674
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;UPTST&#27169;&#22411;&#65292;&#21033;&#29992;&#33133;&#21693;&#21475;&#28814;&#25968;&#25454;&#25552;&#21319;&#25163;&#36275;&#21475;&#30149;&#20303;&#38498;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#22312;&#21307;&#38498;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#36275;&#21475;&#30149;&#65288;HFMD&#65289;&#29190;&#21457;&#19982;&#20005;&#37325;&#30340;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#39044;&#27979;&#20799;&#31185;HFMD&#24739;&#32773;&#30340;&#27599;&#26085;&#20303;&#38498;&#20154;&#25968;&#23545;&#20110;&#21327;&#21161;&#21307;&#38498;&#24212;&#23545;&#28508;&#22312;&#30340;&#29190;&#21457;&#21644;&#20943;&#23569;&#21307;&#38498;&#20869;&#20256;&#25773;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#36843;&#20999;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;U-net&#24418;&#29366;&#65292;&#24182;&#21033;&#29992;&#20102;&#19982;HFMD&#23494;&#20999;&#30456;&#20851;&#30340;&#33133;&#21693;&#21475;&#28814;&#30340;&#35265;&#35299;&#12290;&#35813;&#27169;&#22411;&#36824;&#36890;&#36807;&#24341;&#20837;&#37325;&#26500;&#25439;&#22833;&#20316;&#20026;&#36741;&#21161;&#25439;&#22833;&#26469;&#25972;&#21512;&#34920;&#31034;&#23398;&#20064;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;UPTST&#27169;&#22411;&#22312;&#21307;&#38498;&#32423;&#21035;&#30340;HFMD&#38271;&#30701;&#33218;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25506;&#32034;&#24615;&#30340;&#25193;&#23637;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#36229;&#20986;&#20102;&#20256;&#26579;&#30149;&#30340;&#39044;&#27979;&#65292;&#25552;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Outbreaks of hand-foot-and-mouth disease(HFMD) have been associated with significant morbidity and, in severe cases, mortality. Accurate forecasting of daily admissions of pediatric HFMD patients is therefore crucial for aiding the hospital in preparing for potential outbreaks and mitigating nosocomial transmissions. To address this pressing need, we propose a novel transformer-based model with a U-net shape, utilizing the patching strategy and the joint prediction strategy that capitalizes on insights from herpangina, a disease closely correlated with HFMD. This model also integrates representation learning by introducing reconstruction loss as an auxiliary loss. The results show that our U-net Patching Time Series Transformer (UPTST) model outperforms existing approaches in both long- and short-arm prediction accuracy of HFMD at hospital-level. Furthermore, the exploratory extension experiments show that the model's capabilities extend beyond prediction of infectious disease, suggest
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;NEAT&#26041;&#27861;&#35757;&#32451;&#20998;&#24067;&#24335;&#26426;&#22120;&#20154;&#32676;&#20307;&#31639;&#27861;&#20135;&#29983;&#26032;&#39062;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14663</link><description>&lt;p&gt;
&#29992;NEAT&#23398;&#20064;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#26032;&#39062;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning Emergent Behavior in Robot Swarms with NEAT. (arXiv:2309.14663v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;NEAT&#26041;&#27861;&#35757;&#32451;&#20998;&#24067;&#24335;&#26426;&#22120;&#20154;&#32676;&#20307;&#31639;&#27861;&#20135;&#29983;&#26032;&#39062;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30740;&#31350;&#26426;&#22120;&#20154;&#32676;&#20307;&#26102;&#65292;&#35768;&#22810;&#30740;&#31350;&#35266;&#23519;&#21040;&#22797;&#26434;&#30340;&#32676;&#20307;&#34892;&#20026;&#26159;&#30001;&#20010;&#20307;&#26234;&#33021;&#20307;&#30340;&#31616;&#21333;&#23616;&#37096;&#21160;&#20316;&#20135;&#29983;&#30340;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#20010;&#20307;&#31574;&#30053;&#20197;&#20135;&#29983;&#25152;&#26399;&#26395;&#30340;&#26032;&#39062;&#34892;&#20026;&#30340;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#22522;&#26412;&#19978;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#20998;&#24067;&#24335;&#26426;&#22120;&#20154;&#32676;&#20307;&#31639;&#27861;&#20197;&#20135;&#29983;&#26032;&#39062;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#21463;&#21040;&#21160;&#29289;&#20013;&#26032;&#39062;&#34892;&#20026;&#30340;&#29983;&#29289;&#36827;&#21270;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#35757;&#32451;&#19968;&#20010;&#8220;&#31181;&#32676;&#8221;&#20013;&#30340;&#20010;&#20307;&#34892;&#20026;&#26469;&#36817;&#20284;&#26399;&#26395;&#30340;&#32676;&#20307;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;CoppeliaSim&#27169;&#25311;&#22120;&#36827;&#34892;&#30340;Georgia Tech Miniature Autonomous Blimps&#65288;GT-MABs&#65289;&#31354;&#20013;&#26426;&#22120;&#20154;&#24179;&#21488;&#27169;&#25311;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;Anki Vector&#26426;&#22120;&#20154;&#30340;&#27169;&#25311;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#28608;&#21169;&#27169;&#24335;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#38656;&#35201;&#19968;&#23450;&#22797;&#26434;&#32676;&#20307;&#34892;&#20026;&#25165;&#33021;&#25104;&#21151;&#30340;&#21508;&#31181;&#20219;&#21153;&#36827;&#34892;&#20102;&#31639;&#27861;&#35780;&#20272;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#21306;&#22495;C&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
When researching robot swarms, many studies observe complex group behavior emerging from the individual agents' simple local actions. However, the task of learning an individual policy to produce a desired emergent behavior remains a challenging and largely unsolved problem. We present a method of training distributed robotic swarm algorithms to produce emergent behavior. Inspired by the biological evolution of emergent behavior in animals, we use an evolutionary algorithm to train a 'population' of individual behaviors to approximate a desired group behavior. We perform experiments using simulations of the Georgia Tech Miniature Autonomous Blimps (GT-MABs) aerial robotics platforms conducted in the CoppeliaSim simulator. Additionally, we test on simulations of Anki Vector robots to display our algorithm's effectiveness on various modes of actuation. We evaluate our algorithm on various tasks where a somewhat complex group behavior is required for success. These tasks include an Area C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#22312;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#20013;&#24212;&#29992;&#20998;&#32780;&#27835;&#20043;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20154;&#20307;&#39592;&#39612;&#26694;&#26550;&#21644;&#35270;&#39057;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;ShanghaiTech&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14622</link><description>&lt;p&gt;
&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20998;&#32780;&#27835;&#20043;&#65306;&#32508;&#36848;&#19982;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Divide and Conquer in Video Anomaly Detection: A Comprehensive Review and New Approach. (arXiv:2309.14622v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#22312;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#20013;&#24212;&#29992;&#20998;&#32780;&#27835;&#20043;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20154;&#20307;&#39592;&#39612;&#26694;&#26550;&#21644;&#35270;&#39057;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;ShanghaiTech&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#39033;&#22797;&#26434;&#20219;&#21153;&#65292;&#32780;&#8220;&#20998;&#32780;&#27835;&#20043;&#8221;&#30340;&#21407;&#21017;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#25581;&#31034;&#20102;&#20998;&#32780;&#27835;&#20043;&#29702;&#24565;&#30340;&#24212;&#29992;&#65288;&#23613;&#31649;&#19982;&#20256;&#32479;&#29992;&#27861;&#26377;&#25152;&#19981;&#21516;&#65289;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290;&#26412;&#25991;&#20174;&#20845;&#20010;&#32500;&#24230;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#20123;&#25991;&#29486;&#65292;&#26088;&#22312;&#25552;&#21319;&#22312;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#20013;&#20351;&#29992;&#20998;&#32780;&#27835;&#20043;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20174;&#36825;&#20010;&#32508;&#36848;&#20013;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#20154;&#20307;&#39592;&#39612;&#26694;&#26550;&#19982;&#35270;&#39057;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;ShanghaiTech&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#29616;&#26377;&#30340;&#39640;&#32423;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video anomaly detection is a complex task, and the principle of "divide and conquer" is often regarded as an effective approach to tackling intricate issues. It's noteworthy that recent methods in video anomaly detection have revealed the application of the divide and conquer philosophy (albeit with distinct perspectives from traditional usage), yielding impressive outcomes. This paper systematically reviews these literatures from six dimensions, aiming to enhance the use of the divide and conquer strategy in video anomaly detection. Furthermore, based on the insights gained from this review, a novel approach is presented, which integrates human skeletal frameworks with video data analysis techniques. This method achieves state-of-the-art performance on the ShanghaiTech dataset, surpassing all existing advanced methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20027;&#39064;&#20998;&#26512;&#35782;&#21035;&#20102;&#23545;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30340;&#25928;&#29992;&#34920;&#29616;&#26368;&#37325;&#35201;&#30340;&#20262;&#29702;&#21407;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#21151;&#21033;&#20027;&#20041;&#30340;&#20262;&#29702;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#20262;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14617</link><description>&lt;p&gt;
&#20026;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#24314;&#31435;&#32479;&#19968;&#30340;&#21151;&#21033;&#20262;&#29702;&#26694;&#26550;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards A Unified Utilitarian Ethics Framework for Healthcare Artificial Intelligence. (arXiv:2309.14617v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20027;&#39064;&#20998;&#26512;&#35782;&#21035;&#20102;&#23545;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30340;&#25928;&#29992;&#34920;&#29616;&#26368;&#37325;&#35201;&#30340;&#20262;&#29702;&#21407;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#21151;&#21033;&#20027;&#20041;&#30340;&#20262;&#29702;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26088;&#22312;&#36890;&#36807;&#36741;&#21161;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#23558;&#21307;&#30103;&#25552;&#21319;&#21040;&#19968;&#20010;&#24005;&#23792;&#12290;&#35299;&#20915;&#19982;&#35774;&#35745;&#20262;&#29702;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#30340;&#25361;&#25112;&#23558;&#20351;&#20020;&#24202;&#21307;&#29983;&#12289;&#21307;&#29983;&#12289;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#33021;&#22815;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#20351;&#29992;&#21644;&#20449;&#36182;&#20154;&#24037;&#26234;&#33021;&#12290;&#26412;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#20027;&#39064;&#20998;&#26512;&#65292;&#35782;&#21035;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#25216;&#26415;&#23618;&#38754;&#65288;&#22914;&#25968;&#25454;&#33719;&#21462;&#12289;&#31639;&#27861;&#21644;&#31995;&#32479;&#65289;&#30340;&#25928;&#29992;&#34920;&#29616;&#30340;&#20027;&#35201;&#20262;&#29702;&#21407;&#21017;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#27491;&#20041;&#12289;&#38544;&#31169;&#12289;&#20559;&#35265;&#12289;&#32570;&#20047;&#35268;&#23450;&#12289;&#39118;&#38505;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#32771;&#34385;&#20262;&#29702;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#37325;&#35201;&#21407;&#21017;&#12290;&#35813;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#36890;&#36807;&#23545;36&#20301;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#30382;&#23588;&#30740;&#31350;&#20013;&#24515;&#65288;2020&#24180;&#65289;&#30340;&#20108;&#27425;&#35843;&#26597;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#30340;&#39030;&#32423;&#20262;&#29702;&#21407;&#21017;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#20803;&#20998;&#26512;&#21644;&#39046;&#22495;&#19987;&#23478;&#25152;&#21457;&#29616;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21151;&#21033;&#20027;&#20041;&#20262;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#20262;&#29702;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) aims to elevate healthcare to a pinnacle by aiding clinical decision support. Overcoming the challenges related to the design of ethical AI will enable clinicians, physicians, healthcare professionals, and other stakeholders to use and trust AI in healthcare settings. This study attempts to identify the major ethical principles influencing the utility performance of AI at different technological levels such as data access, algorithms, and systems through a thematic analysis. We observed that justice, privacy, bias, lack of regulations, risks, and interpretability are the most important principles to consider for ethical AI. This data-driven study has analyzed secondary survey data from the Pew Research Center (2020) of 36 AI experts to categorize the top ethical principles of AI design. To resolve the ethical issues identified by the meta-analysis and domain experts, we propose a new utilitarian ethics-based theoretical framework for designing ethical AI fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27946;&#27700;&#21361;&#38505;&#19982;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#30340;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;</title><link>http://arxiv.org/abs/2309.14610</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22270;&#28145;&#24230;&#23398;&#20064;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas. (arXiv:2309.14610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27946;&#27700;&#21361;&#38505;&#19982;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#22478;&#24066;&#22320;&#21306;&#30340;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#27010;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#28304;&#20110;&#19982;&#27946;&#27700;&#21361;&#38505;&#12289;&#27946;&#27700;&#26292;&#38706;&#20197;&#21450;&#31038;&#20250;&#21644;&#29289;&#29702;&#33030;&#24369;&#24615;&#30456;&#20851;&#30340;&#22810;&#20010;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#22797;&#26434;&#30340;&#31354;&#38388;&#27946;&#27700;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#34920;&#24449;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#27946;&#27700;&#24179;&#21407;&#22320;&#22270;&#65292;&#20391;&#37325;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#35201;&#32032;&#65292;&#20027;&#35201;&#26159;&#21361;&#38505;&#21644;&#26292;&#38706;&#35201;&#32032;&#65292;&#27809;&#26377;&#32771;&#34385;&#35201;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25110;&#31354;&#38388;&#21306;&#22495;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#22270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#31216;&#20026;FloodRisk-Net&#65289;&#30340;&#38598;&#25104;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#32423;&#27169;&#22411;&#12290;FloodRisk-Net&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#27946;&#27700;&#21361;&#38505;&#21644;&#22478;&#24066;&#35201;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#30830;&#23450;&#31361;&#21457;&#27946;&#27700;&#39118;&#38505;&#12290;&#21033;&#29992;&#32654;&#22269;&#22810;&#20010;&#37117;&#24066;&#32479;&#35745;&#21306;&#65288;MSAs&#65289;&#30340;&#25968;&#25454;&#65292;&#35813;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#27946;&#27700;&#39118;&#38505;&#29305;&#24449;&#21270;&#20026;
&lt;/p&gt;
&lt;p&gt;
Urban flood risk emerges from complex and nonlinear interactions among multiple features related to flood hazard, flood exposure, and social and physical vulnerabilities, along with the complex spatial flood dependence relationships. Existing approaches for characterizing urban flood risk, however, are primarily based on flood plain maps, focusing on a limited number of features, primarily hazard and exposure features, without consideration of feature interactions or the dependence relationships among spatial areas. To address this gap, this study presents an integrated urban flood-risk rating model based on a novel unsupervised graph deep learning model (called FloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among areas and complex and nonlinear interactions among flood hazards and urban features for specifying emergent flood risk. Using data from multiple metropolitan statistical areas (MSAs) in the United States, the model characterizes their flood risk into
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;FP8&#26684;&#24335;&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#30456;&#23545;&#20110;INT8&#20855;&#26377;&#26356;&#22909;&#30340;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14592</link><description>&lt;p&gt;
&#20351;&#29992;FP8&#26684;&#24335;&#30340;&#39640;&#25928;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Post-training Quantization with FP8 Formats. (arXiv:2309.14592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;FP8&#26684;&#24335;&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#30456;&#23545;&#20110;INT8&#20855;&#26377;&#26356;&#22909;&#30340;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#65292;&#22914;LLMs&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#23545;&#25913;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#30340;&#38656;&#27714;&#65292;&#20197;&#28385;&#36275;&#36825;&#20123;&#29616;&#20195;&#26550;&#26500;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;FP8&#25968;&#25454;&#26684;&#24335;&#22312;75&#20010;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#20248;&#21183;&#65292;&#36825;&#20123;&#32593;&#32476;&#26550;&#26500;&#28085;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#35328;&#24314;&#27169;&#12289;&#25991;&#26412;&#29983;&#25104;&#12289;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#20998;&#21106;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;FP8&#34920;&#31034;&#65288;E5M2&#12289;E4M3&#21644;E3M4&#65289;&#65292;&#20197;&#30740;&#31350;&#22312;&#21160;&#24577;&#33539;&#22260;&#21644;&#31934;&#24230;&#20043;&#38388;&#19981;&#21516;&#26435;&#34913;&#31243;&#24230;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#24191;&#27867;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#21487;&#20197;&#27010;&#25324;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#26684;&#24335;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;INT8&#65292;&#21253;&#25324;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#65288;92.64&#65285;&#23545;65.87&#65285;&#65289;&#12289;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#25805;&#20316;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning methods such as LLMs and Diffusion models have created a need for improved quantization methods that can meet the computational demands of these modern architectures while maintaining accuracy. Towards this goal, we study the advantages of FP8 data formats for post-training quantization across 75 unique network architectures covering a wide range of tasks, including machine translation, language modeling, text generation, image classification, generation, and segmentation. We examine three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects of varying degrees of trade-off between dynamic range and precision on model accuracy. Based on our extensive study, we developed a quantization workflow that generalizes across different network architectures. Our empirical results show that FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of ope
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#29575;&#30072;&#21464;&#29702;&#35770;&#26469;&#20998;&#26512;&#36890;&#20449;&#21644;&#35821;&#20041;&#21387;&#32553;&#24341;&#36215;&#30340;&#30072;&#21464;&#65292;&#20174;&#32780;&#35780;&#20272;&#20854;&#23545;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#20013;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20351;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14587</link><description>&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#30340;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#20855;&#26377;&#30072;&#21464;&#29575;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint Communication and Computation Framework for Goal-Oriented Semantic Communication with Distortion Rate Resilience. (arXiv:2309.14587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#29575;&#30072;&#21464;&#29702;&#35770;&#26469;&#20998;&#26512;&#36890;&#20449;&#21644;&#35821;&#20041;&#21387;&#32553;&#24341;&#36215;&#30340;&#30072;&#21464;&#65292;&#20174;&#32780;&#35780;&#20272;&#20854;&#23545;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#20013;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20351;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#35821;&#20041;&#36890;&#20449;&#30340;&#30740;&#31350;&#20027;&#35201;&#32771;&#34385;&#20934;&#30830;&#24615;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#23548;&#21521;&#36890;&#20449;&#31995;&#32479;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#24726;&#35770;&#65306;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#24212;&#35813;&#36890;&#36807;&#35757;&#32451;&#33258;&#28982;&#22320;&#20986;&#29616;&#65292;&#32780;&#19981;&#26159;&#30001;&#32593;&#32476;&#32422;&#26463;&#25152;&#20915;&#23450;&#12290;&#37492;&#20110;&#36825;&#20010;&#22256;&#22659;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29575;&#30072;&#21464;&#29702;&#35770;&#26469;&#20998;&#26512;&#30001;&#36890;&#20449;&#21644;&#35821;&#20041;&#21387;&#32553;&#24341;&#36215;&#30340;&#30072;&#21464;&#65292;&#24182;&#20998;&#26512;&#23398;&#20064;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21407;&#22987;&#25968;&#25454;&#21644;&#30072;&#21464;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#20174;&#32780;&#35780;&#20272;&#20854;&#23545;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#20808;&#20272;&#35745;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#23454;&#38469;&#20934;&#30830;&#24615;&#65292;&#20351;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#38382;&#39064;&#21464;&#24471;&#21487;&#34892;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#36827;&#34892;&#20102;&#27169;&#25311;&#21644;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research efforts on semantic communication have mostly considered accuracy as a main problem for optimizing goal-oriented communication systems. However, these approaches introduce a paradox: the accuracy of artificial intelligence (AI) tasks should naturally emerge through training rather than being dictated by network constraints. Acknowledging this dilemma, this work introduces an innovative approach that leverages the rate-distortion theory to analyze distortions induced by communication and semantic compression, thereby analyzing the learning process. Specifically, we examine the distribution shift between the original data and the distorted data, thus assessing its impact on the AI model's performance. Founding upon this analysis, we can preemptively estimate the empirical accuracy of AI tasks, making the goal-oriented semantic communication problem feasible. To achieve this objective, we present the theoretical foundation of our approach, accompanied by simulations and ex
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#26631;&#35760;&#30340;MRI&#27979;&#37327;&#33292;&#22836;&#30340;&#21151;&#33021;&#21333;&#20803;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#26435;&#37325;&#22270;&#36716;&#21270;&#20026;&#23545;&#24212;&#30340;&#38899;&#39057;&#27874;&#24418;&#65292;&#20026;&#30740;&#31350;&#35821;&#38899;&#20135;&#29983;&#36807;&#31243;&#25552;&#20379;&#20102;&#37325;&#35201;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2309.14586</link><description>&lt;p&gt;
&#20174;&#26631;&#35760;&#30340;MRI&#21644;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#36890;&#36807;&#21487;&#22609;&#24615;&#21464;&#25442;&#22120;&#21512;&#25104;&#35821;&#38899;&#38899;&#39057;
&lt;/p&gt;
&lt;p&gt;
Speech Audio Synthesis from Tagged MRI and Non-Negative Matrix Factorization via Plastic Transformer. (arXiv:2309.14586v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14586
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#26631;&#35760;&#30340;MRI&#27979;&#37327;&#33292;&#22836;&#30340;&#21151;&#33021;&#21333;&#20803;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#26435;&#37325;&#22270;&#36716;&#21270;&#20026;&#23545;&#24212;&#30340;&#38899;&#39057;&#27874;&#24418;&#65292;&#20026;&#30740;&#31350;&#35821;&#38899;&#20135;&#29983;&#36807;&#31243;&#25552;&#20379;&#20102;&#37325;&#35201;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33292;&#22836;&#30340;&#22797;&#26434;3D&#32467;&#26500;&#26159;&#35821;&#38899;&#20135;&#29983;&#36807;&#31243;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#23616;&#37096;&#21151;&#33021;&#21333;&#20803;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#20351;&#29992;&#26631;&#35760;&#30340;MRI&#27979;&#37327;&#26102;&#65292;&#36825;&#20123;&#21151;&#33021;&#21333;&#20803;&#34920;&#29616;&#20986;&#20855;&#26377;&#20869;&#32858;&#20301;&#31227;&#30340;&#21644;&#27966;&#29983;&#25968;&#37327;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#22797;&#26434;&#30340;&#35821;&#38899;&#20135;&#29983;&#36807;&#31243;&#12290;&#22522;&#20110;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#36816;&#21160;&#29305;&#24449;&#20272;&#35745;&#21151;&#33021;&#21333;&#20803;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#32452;&#26500;&#24314;&#22359;&#21644;&#30456;&#24212;&#30340;&#21152;&#26435;&#22270;&#12290;&#30740;&#31350;&#21152;&#26435;&#22270;&#19982;&#35821;&#38899;&#22768;&#23398;&#20043;&#38388;&#30340;&#32852;&#31995;&#21487;&#20197;&#23545;&#22797;&#26434;&#30340;&#35821;&#38899;&#20135;&#29983;&#36807;&#31243;&#25552;&#20379;&#37325;&#35201;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#21033;&#29992;&#20108;&#32500;&#39057;&#35889;&#22270;&#20316;&#20026;&#20195;&#29702;&#34920;&#31034;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#21152;&#26435;&#22270;&#36716;&#21270;&#20026;&#30456;&#24212;&#30340;&#38899;&#39057;&#27874;&#24418;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21487;&#22609;&#24615;&#36731;&#22411;&#21464;&#25442;&#22120;&#65288;PLT&#65289;&#26694;&#26550;&#22522;&#20110;&#26041;&#21521;&#24615;&#20135;&#21697;&#30456;&#23545;&#20301;&#32622;&#20559;&#24046;&#21644;&#21333;&#23618;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The tongue's intricate 3D structure, comprising localized functional units, plays a crucial role in the production of speech. When measured using tagged MRI, these functional units exhibit cohesive displacements and derived quantities that facilitate the complex process of speech production. Non-negative matrix factorization-based approaches have been shown to estimate the functional units through motion features, yielding a set of building blocks and a corresponding weighting map. Investigating the link between weighting maps and speech acoustics can offer significant insights into the intricate process of speech production. To this end, in this work, we utilize two-dimensional spectrograms as a proxy representation, and develop an end-to-end deep learning framework for translating weighting maps to their corresponding audio waveforms. Our proposed plastic light transformer (PLT) framework is based on directional product relative position bias and single-level spatial pyramid pooling,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CWCL&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#36328;&#27169;&#24577;&#36801;&#31227;&#20013;&#30340;&#23545;&#27604;&#35757;&#32451;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#23545;&#27604;&#35757;&#32451;&#65292;CWCL&#20351;&#29992;&#36830;&#32493;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#23454;&#20363;&#30340;&#34920;&#31034;&#24182;&#25552;&#39640;&#36328;&#27169;&#24577;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14580</link><description>&lt;p&gt;
CWCL&#65306;&#36830;&#32493;&#21152;&#26435;&#23545;&#27604;&#25439;&#22833;&#19979;&#30340;&#36328;&#27169;&#24577;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
CWCL: Cross-Modal Transfer with Continuously Weighted Contrastive Loss. (arXiv:2309.14580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CWCL&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#36328;&#27169;&#24577;&#36801;&#31227;&#20013;&#30340;&#23545;&#27604;&#35757;&#32451;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#23545;&#27604;&#35757;&#32451;&#65292;CWCL&#20351;&#29992;&#36830;&#32493;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#23454;&#20363;&#30340;&#34920;&#31034;&#24182;&#25552;&#39640;&#36328;&#27169;&#24577;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#23545;&#27604;&#35757;&#32451;&#36827;&#34892;&#36328;&#27169;&#24577;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#20854;&#20013;&#19968;&#20010;&#27169;&#24577;&#20013;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#22312;&#21478;&#19968;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#20351;&#29992;&#25104;&#23545;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#21518;&#19968;&#20010;&#39046;&#22495;&#20013;&#23398;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#19968;&#31181;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#31867;&#20284;&#20110;&#26368;&#36817;&#24341;&#36215;&#30456;&#24403;&#20851;&#27880;&#30340;&#8220;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#8221;&#21644;&#8220;&#38145;&#23450;&#22270;&#20687;&#35843;&#25972;&#65288;LiT&#65289;&#8221;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#23545;&#40784;&#26041;&#27861;&#65288;&#21253;&#25324;CLIP&#21644;LiT&#65289;&#20351;&#29992;&#26631;&#20934;&#30340;&#23545;&#27604;&#35757;&#32451;&#30446;&#26631;&#65292;&#23427;&#20351;&#29992;&#19968;&#32452;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#26469;&#23545;&#40784;&#30456;&#20284;&#21644;&#39537;&#25955;&#19981;&#30456;&#20284;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20855;&#26377;&#26356;&#36830;&#32493;&#30340;&#24615;&#36136;&#65292;&#22240;&#27492;&#38656;&#35201;&#26356;&#8220;&#38750;&#20108;&#36827;&#21046;&#8221;&#30340;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36830;&#32493;&#21152;&#26435;&#23545;&#27604;&#25439;&#22833;&#65288;CWCL&#65289;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#20351;&#29992;&#36830;&#32493;&#30340;&#30456;&#20284;&#24230;&#27979;&#37327;&#12290;&#20351;&#29992;CWCL&#65292;&#25105;&#20204;&#26088;&#22312;&#23545;&#40784;&#23454;&#20363;&#30340;&#34920;&#31034;&#24182;&#25552;&#39640;&#36328;&#27169;&#24577;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers contrastive training for cross-modal 0-shot transfer wherein a pre-trained model in one modality is used for representation learning in another domain using pairwise data. The learnt models in the latter domain can then be used for a diverse set of tasks in a zero-shot way, similar to ``Contrastive Language-Image Pre-training (CLIP)'' and ``Locked-image Tuning (LiT)'' that have recently gained considerable attention. Most existing works for cross-modal representation alignment (including CLIP and LiT) use the standard contrastive training objective, which employs sets of positive and negative examples to align similar and repel dissimilar training data samples. However, similarity amongst training examples has a more continuous nature, thus calling for a more `non-binary' treatment. To address this, we propose a novel loss function called Continuously Weighted Contrastive Loss (CWCL) that employs a continuous measure of similarity. With CWCL, we seek to align the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#39640;&#38454;&#21160;&#24577;&#21644;&#36947;&#36335;&#21512;&#35268;&#24615;&#25972;&#21512;&#21040;&#22522;&#20110;&#32422;&#26463;&#30340;ILQR&#36712;&#36857;&#35268;&#21010;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#65292;&#25552;&#20379;&#20102;&#26356;&#23433;&#20840;&#21644;&#33298;&#36866;&#30340;&#36335;&#24452;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2309.14566</link><description>&lt;p&gt;
&#23558;&#39640;&#38454;&#21160;&#24577;&#21644;&#36947;&#36335;&#21512;&#35268;&#24615;&#25972;&#21512;&#21040;&#22522;&#20110;&#32422;&#26463;&#30340;ILQR&#36712;&#36857;&#35268;&#21010;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;
&lt;/p&gt;
&lt;p&gt;
Integrating Higher-Order Dynamics and Roadway-Compliance into Constrained ILQR-based Trajectory Planning for Autonomous Vehicles. (arXiv:2309.14566v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#39640;&#38454;&#21160;&#24577;&#21644;&#36947;&#36335;&#21512;&#35268;&#24615;&#25972;&#21512;&#21040;&#22522;&#20110;&#32422;&#26463;&#30340;ILQR&#36712;&#36857;&#35268;&#21010;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#65292;&#25552;&#20379;&#20102;&#26356;&#23433;&#20840;&#21644;&#33298;&#36866;&#30340;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#33258;&#21160;&#20056;&#23458;&#36710;&#36742;&#65288;APV&#65289;&#30340;&#36947;&#36335;&#36712;&#36857;&#35268;&#21010;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#36712;&#36857;&#35268;&#21010;&#26088;&#22312;&#20026;APV&#29983;&#25104;&#20840;&#23616;&#26368;&#20248;&#30340;&#36335;&#24452;&#65292;&#32771;&#34385;&#21040;&#35832;&#22914;&#36710;&#36742;&#21160;&#21147;&#23398;&#12289;&#32422;&#26463;&#21644;&#26816;&#27979;&#21040;&#30340;&#38556;&#30861;&#29289;&#31561;&#21508;&#31181;&#22240;&#32032;&#12290;&#20256;&#32479;&#25216;&#26415;&#28041;&#21450;&#37319;&#26679;&#26041;&#27861;&#19982;&#20248;&#21270;&#31639;&#27861;&#30340;&#32452;&#21512;&#65292;&#21069;&#32773;&#30830;&#20445;&#20840;&#23616;&#24863;&#30693;&#65292;&#21518;&#32773;&#20248;&#21270;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32422;&#26463;&#36845;&#20195;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;CILQR&#65289;&#20248;&#21270;&#31639;&#27861;&#26368;&#36817;&#20986;&#29616;&#65292;&#38024;&#23545;APV&#31995;&#32479;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24378;&#35843;&#25552;&#39640;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#24230;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36710;&#36742;&#33258;&#34892;&#36710;&#36816;&#21160;&#27169;&#22411;&#30340;&#29616;&#26377;&#23454;&#29616;&#21487;&#33021;&#26080;&#27861;&#20445;&#35777;&#21487;&#25511;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#36890;&#36807;&#21152;&#20837;&#39640;&#38454;&#39033;&#65292;&#21253;&#25324;&#26354;&#29575;&#21644;&#32437;&#21521;&#21152;&#36895;&#24230;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#23548;&#25968;&#65292;&#26469;&#22686;&#24378;&#36825;&#20010;&#27169;&#22411;&#12290;&#36825;&#31181;&#21253;&#21547;&#26377;&#21161;&#20110;&#22312;&#25104;&#26412;&#21644;&#32422;&#26463;&#35774;&#35745;&#20013;&#33719;&#24471;&#26356;&#20016;&#23500;&#30340;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the advancements in on-road trajectory planning for Autonomous Passenger Vehicles (APV). Trajectory planning aims to produce a globally optimal route for APVs, considering various factors such as vehicle dynamics, constraints, and detected obstacles. Traditional techniques involve a combination of sampling methods followed by optimization algorithms, where the former ensures global awareness and the latter refines for local optima. Notably, the Constrained Iterative Linear Quadratic Regulator (CILQR) optimization algorithm has recently emerged, adapted for APV systems, emphasizing improved safety and comfort. However, existing implementations utilizing the vehicle bicycle kinematic model may not guarantee controllable trajectories. We augment this model by incorporating higher-order terms, including the first and second-order derivatives of curvature and longitudinal jerk. This inclusion facilitates a richer representation in our cost and constraint design. We also
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21608;&#26399;&#24615;&#30340;&#38750;&#27491;&#26041;&#24418;&#38262;&#23884;&#22270;&#26696;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#20960;&#20309;&#21644;&#39068;&#33394;&#26469;&#29983;&#25104;&#19982;&#25152;&#38656;&#23545;&#35937;&#24418;&#29366;&#21644;&#22806;&#35266;&#30456;&#20284;&#30340;&#29943;&#30742;&#12290;</title><link>http://arxiv.org/abs/2309.14564</link><description>&lt;p&gt;
&#29983;&#25104;&#33406;&#33293;&#23572;&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Generative Escher Meshes. (arXiv:2309.14564v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21608;&#26399;&#24615;&#30340;&#38750;&#27491;&#26041;&#24418;&#38262;&#23884;&#22270;&#26696;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#20960;&#20309;&#21644;&#39068;&#33394;&#26469;&#29983;&#25104;&#19982;&#25152;&#38656;&#23545;&#35937;&#24418;&#29366;&#21644;&#22806;&#35266;&#30456;&#20284;&#30340;&#29943;&#30742;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#12289;&#20197;&#25991;&#26412;&#20026;&#23548;&#21521;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21608;&#26399;&#24615;&#30340;&#12289;&#21487;&#37325;&#22797;&#30340;&#20108;&#32500;&#33402;&#26415;&#20316;&#21697;&#65292;&#22914;&#22320;&#26495;&#12289;&#39532;&#36187;&#20811;&#12289;&#38518;&#29943;&#21644;&#33406;&#33293;&#23572;&#30340;&#20316;&#21697;&#12290;&#19982;&#20256;&#32479;&#30340;&#26080;&#32541;&#32441;&#29702;&#27010;&#24565;&#19981;&#21516;&#65292;&#21363;&#24179;&#38138;&#26080;&#32541;&#30340;&#27491;&#26041;&#24418;&#22270;&#20687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#26159;&#30001;&#37325;&#22797;&#30340;&#30456;&#21516;&#23545;&#35937;&#32452;&#25104;&#30340;&#38750;&#27491;&#26041;&#24418;&#38262;&#23884;&#22270;&#26696;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#20108;&#32500;&#32593;&#26684;&#30340;&#20960;&#20309;&#21644;&#39068;&#33394;&#26469;&#29983;&#25104;&#19982;&#25152;&#38656;&#23545;&#35937;&#24418;&#29366;&#21644;&#22806;&#35266;&#30456;&#20284;&#30340;&#38750;&#27491;&#26041;&#24418;&#29943;&#30742;&#65292;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#30340;&#32972;&#26223;&#32454;&#33410;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#36129;&#29486;&#23454;&#29616;&#20102;&#38262;&#23884;&#22270;&#26696;&#30340;&#20960;&#20309;&#20248;&#21270;&#65306;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#12289;&#21487;&#24494;&#20998;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#32473;&#23450;&#23545;&#31216;&#32676;&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#21487;&#38138;&#30742;&#24418;&#29366;&#31354;&#38388;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20462;&#25913;&#20108;&#32500;&#32593;&#26684;&#26144;&#23556;&#25216;&#26415;Orbifold Tutte Embedding&#20013;&#20351;&#29992;&#30340;Laplacian&#31639;&#23376;&#21487;&#20197;&#23454;&#29616;&#25152;&#36873;&#24179;&#38754;&#23545;&#31216;&#32676;&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#38138;&#30742;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a fully-automatic, text-guided generative method for producing periodic, repeating, tile-able 2D art, such as the one seen on floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to the standard concept of a seamless texture, i.e., square images that are seamless when tiled, our method generates non-square tilings which comprise solely of repeating copies of the same object. It achieves this by optimizing both geometry and color of a 2D mesh, in order to generate a non-square tile in the shape and appearance of the desired object, with close to no additional background details. We enable geometric optimization of tilings by our key technical contribution: an unconstrained, differentiable parameterization of the space of all possible tileable shapes for a given symmetry group. Namely, we prove that modifying the laplacian used in a 2D mesh-mapping technique Orbifold Tutte Embedding - can achieve all possible tiling configurations for a chosen planar 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20889;&#20316;&#21019;&#36896;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#22312;&#21019;&#24847;&#27979;&#35797;&#20013;&#36890;&#36807;&#30340;&#25968;&#37327;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26080;&#27861;&#20195;&#26367;&#19987;&#23478;&#36827;&#34892;TTCW&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.14556</link><description>&lt;p&gt;
&#33402;&#26415;&#36824;&#26159;&#25216;&#24039;&#65311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21019;&#36896;&#21147;&#30340;&#34394;&#20551;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
Art or Artifice? Large Language Models and the False Promise of Creativity. (arXiv:2309.14556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20889;&#20316;&#21019;&#36896;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#22312;&#21019;&#24847;&#27979;&#35797;&#20013;&#36890;&#36807;&#30340;&#25968;&#37327;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26080;&#27861;&#20195;&#26367;&#19987;&#23478;&#36827;&#34892;TTCW&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#35748;&#20026;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#20174;&#21338;&#23458;&#21040;&#25925;&#20107;&#30340;&#39640;&#36136;&#37327;&#20889;&#20316;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23458;&#35266;&#35780;&#20272;&#19968;&#27573;&#25991;&#23383;&#30340;&#21019;&#36896;&#21147;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21463;&#21019;&#36896;&#24615;&#24605;&#32500;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTC)&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#20849;&#35782;&#35780;&#20272;&#25216;&#26415;[3]&#65292;&#25552;&#20986;&#20102;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#21019;&#36896;&#21147;&#20316;&#20026;&#19968;&#20010;&#20135;&#21697;&#12290;TTCW&#30001;&#21253;&#21547;&#22312;&#27969;&#30021;&#24230;&#12289;&#28789;&#27963;&#24615;&#12289;&#29420;&#21019;&#24615;&#21644;&#32454;&#33268;&#24230;&#21407;&#22987;&#32500;&#24230;&#20013;&#30340;14&#20010;&#20108;&#20803;&#27979;&#35797;&#32452;&#25104;&#12290;&#25105;&#20204;&#25307;&#21215;&#20102;10&#20301;&#21019;&#24847;&#20316;&#23478;&#65292;&#24182;&#20351;&#29992;TTCW&#23545;48&#20010;&#30001;&#19987;&#19994;&#20316;&#23478;&#25110;LLMs&#25776;&#20889;&#30340;&#25925;&#20107;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#36890;&#36807;&#30340;TTCW&#27979;&#35797;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#20102;3-10&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#35780;&#20215;&#32773;&#65292;&#20197;&#33258;&#21160;&#21270;TTCW&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#27809;&#26377;&#19968;&#20010;LLM&#19982;&#19987;&#23478;&#35780;&#20272;&#21576;&#27491;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT), which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose the Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35302;&#35273;&#35835;&#25968;&#25512;&#27979;&#29289;&#20307;&#25918;&#32622;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25509;&#35302;&#21306;&#22495;&#30340;&#20272;&#35745;&#21487;&#20197;&#26377;&#25928;&#35774;&#35745;&#26426;&#22120;&#20154;&#30340;&#21453;&#39304;&#25216;&#33021;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#31934;&#32454;&#25805;&#25511;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14552</link><description>&lt;p&gt;
&#31283;&#23450;&#25918;&#32622;&#30340;&#22806;&#37096;&#25509;&#35302;&#22359;&#30340;&#35302;&#35273;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Tactile Estimation of Extrinsic Contact Patch for Stable Placement. (arXiv:2309.14552v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35302;&#35273;&#35835;&#25968;&#25512;&#27979;&#29289;&#20307;&#25918;&#32622;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25509;&#35302;&#21306;&#22495;&#30340;&#20272;&#35745;&#21487;&#20197;&#26377;&#25928;&#35774;&#35745;&#26426;&#22120;&#20154;&#30340;&#21453;&#39304;&#25216;&#33021;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#31934;&#32454;&#25805;&#25511;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26426;&#22120;&#20154;&#30340;&#31934;&#32454;&#25805;&#20316;&#25216;&#33021;&#26469;&#35828;&#65292;&#20934;&#30830;&#24863;&#30693;&#25509;&#35302;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#26426;&#22120;&#20154;&#35774;&#35745;&#21453;&#39304;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#35813;&#26426;&#22120;&#20154;&#24517;&#39035;&#23398;&#20064;&#23558;&#22797;&#26434;&#24418;&#29366;&#30340;&#29289;&#20307;&#22534;&#21472;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#35774;&#35745;&#36825;&#26679;&#19968;&#20010;&#31995;&#32479;&#65292;&#26426;&#22120;&#20154;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#38750;&#24120;&#36731;&#24494;&#30340;&#25509;&#35302;&#20132;&#20114;&#26469;&#25512;&#29702;&#25918;&#32622;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26681;&#25454;&#25509;&#35302;&#24418;&#25104;&#36807;&#31243;&#20013;&#30340;&#35302;&#35273;&#35835;&#25968;&#26469;&#25512;&#27979;&#29289;&#20307;&#25918;&#32622;&#30340;&#31283;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#21147;&#21644;&#35302;&#35273;&#35266;&#27979;&#26469;&#20272;&#35745;&#25235;&#21462;&#29289;&#20307;&#21644;&#20854;&#29615;&#22659;&#20043;&#38388;&#30340;&#25509;&#35302;&#21306;&#22495;&#65292;&#20174;&#32780;&#20272;&#35745;&#25509;&#35302;&#24418;&#25104;&#36807;&#31243;&#20013;&#29289;&#20307;&#30340;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#25509;&#35302;&#21306;&#22495;&#21487;&#20197;&#29992;&#26469;&#20272;&#35745;&#37322;&#25918;&#25235;&#21462;&#21518;&#29289;&#20307;&#30340;&#31283;&#23450;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#27454;&#38750;&#24120;&#27969;&#34892;&#30340;&#26827;&#30424;&#28216;&#25103;&#20013;&#20351;&#29992;&#20102;&#22810;&#31181;&#29289;&#20307;&#23545;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise perception of contact interactions is essential for the fine-grained manipulation skills for robots. In this paper, we present the design of feedback skills for robots that must learn to stack complex-shaped objects on top of each other. To design such a system, a robot should be able to reason about the stability of placement from very gentle contact interactions. Our results demonstrate that it is possible to infer the stability of object placement based on tactile readings during contact formation between the object and its environment. In particular, we estimate the contact patch between a grasped object and its environment using force and tactile observations to estimate the stability of the object during a contact formation. The contact patch could be used to estimate the stability of the object upon the release of the grasp. The proposed method is demonstrated on various pairs of objects that are used in a very popular board game.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#20851;&#20110;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#22312;&#31639;&#27861;&#21246;&#32467;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#30340;&#31354;&#30333;&#65292;&#24182;&#21457;&#29616;&#25512;&#33616;&#31639;&#27861;&#21487;&#20197;&#20915;&#23450;&#22522;&#20110;AI&#30340;&#23450;&#20215;&#31639;&#27861;&#30340;&#31454;&#20105;&#25110;&#21246;&#32467;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2309.14548</link><description>&lt;p&gt;
&#31639;&#27861;&#21246;&#32467;&#36824;&#26159;&#31454;&#20105;&#65306;&#24179;&#21488;&#25512;&#33616;&#31995;&#32479;&#30340;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Collusion or Competition: the Role of Platforms' Recommender Systems. (arXiv:2309.14548v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14548
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#20851;&#20110;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#22312;&#31639;&#27861;&#21246;&#32467;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#30340;&#31354;&#30333;&#65292;&#24182;&#21457;&#29616;&#25512;&#33616;&#31639;&#27861;&#21487;&#20197;&#20915;&#23450;&#22522;&#20110;AI&#30340;&#23450;&#20215;&#31639;&#27861;&#30340;&#31454;&#20105;&#25110;&#21246;&#32467;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23398;&#26415;&#30740;&#31350;&#24191;&#27867;&#25506;&#35752;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#21160;&#24577;&#23450;&#20215;&#31639;&#27861;&#23548;&#33268;&#30340;&#31639;&#27861;&#21246;&#32467;&#12290;&#28982;&#32780;&#65292;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20351;&#29992;&#25512;&#33616;&#31639;&#27861;&#26469;&#20998;&#37197;&#19981;&#21516;&#20135;&#21697;&#30340;&#26333;&#20809;&#65292;&#32780;&#36825;&#19968;&#37325;&#35201;&#26041;&#38754;&#22312;&#20808;&#21069;&#30340;&#31639;&#27861;&#21246;&#32467;&#30740;&#31350;&#20013;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#25991;&#29486;&#20013;&#36825;&#19968;&#37325;&#35201;&#30340;&#31354;&#30333;&#65292;&#24182;&#26816;&#39564;&#20102;&#25512;&#33616;&#31639;&#27861;&#22914;&#20309;&#20915;&#23450;&#22522;&#20110;AI&#30340;&#23450;&#20215;&#31639;&#27861;&#30340;&#31454;&#20105;&#25110;&#21246;&#32467;&#21160;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#24120;&#29992;&#30340;&#25512;&#33616;&#31639;&#27861;&#65306;(i)&#20197;&#26368;&#22823;&#21270;&#21334;&#23478;&#24635;&#21033;&#28070;&#20026;&#30446;&#26631;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;(ii)&#20197;&#26368;&#22823;&#21270;&#24179;&#21488;&#19978;&#20135;&#21697;&#38656;&#27714;&#20026;&#30446;&#26631;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#37325;&#22797;&#21338;&#24328;&#26694;&#26550;&#65292;&#23558;&#21334;&#23478;&#30340;&#23450;&#20215;&#31639;&#27861;&#21644;&#24179;&#21488;&#30340;&#25512;&#33616;&#31639;&#27861;&#36827;&#34892;&#20102;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent academic research has extensively examined algorithmic collusion resulting from the utilization of artificial intelligence (AI)-based dynamic pricing algorithms. Nevertheless, e-commerce platforms employ recommendation algorithms to allocate exposure to various products, and this important aspect has been largely overlooked in previous studies on algorithmic collusion. Our study bridges this important gap in the literature and examines how recommendation algorithms can determine the competitive or collusive dynamics of AI-based pricing algorithms. Specifically, two commonly deployed recommendation algorithms are examined: (i) a recommender system that aims to maximize the sellers' total profit (profit-based recommender system) and (ii) a recommender system that aims to maximize the demand for products sold on the platform (demand-based recommender system). We construct a repeated game framework that incorporates both pricing algorithms adopted by sellers and the platform's recom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#22278;&#29615;&#26696;&#20363;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#22278;&#29615;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#20154;&#31867;&#39550;&#39542;&#21592;&#19982;&#22278;&#29615;&#30340;&#20114;&#21160;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22278;&#29615;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#36716;&#24367;&#36335;&#21475;&#30340;&#36895;&#24230;&#65292;&#32780;&#20854;&#23545;&#36895;&#24230;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#34892;&#20026;&#35780;&#32423;&#12290;&#23545;&#20110;&#24052;&#22763;&#12289;&#27773;&#36710;&#21644;&#21345;&#36710;&#39550;&#39542;&#21592;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#24320;&#21457;&#20986;&#19968;&#31181;&#39044;&#27979;&#22278;&#29615;&#20132;&#21449;&#21475;&#36947;&#36335;&#20351;&#29992;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#23433;&#20840;&#20027;&#35201;&#24402;&#21151;&#20110;&#22278;&#29615;&#30340;&#20004;&#20010;&#22266;&#26377;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.14540</link><description>&lt;p&gt;
&#22278;&#29615;&#35774;&#35745;&#23545;&#36947;&#36335;&#20351;&#29992;&#32773;&#34892;&#20026;&#30340;&#24433;&#21709;&#65306;&#22522;&#20110;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#22278;&#29615;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Effect of roundabout design on the behavior of road users: A case study of roundabouts with application of Unsupervised Machine Learning. (arXiv:2309.14540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#22278;&#29615;&#26696;&#20363;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#22278;&#29615;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#20154;&#31867;&#39550;&#39542;&#21592;&#19982;&#22278;&#29615;&#30340;&#20114;&#21160;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22278;&#29615;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#36716;&#24367;&#36335;&#21475;&#30340;&#36895;&#24230;&#65292;&#32780;&#20854;&#23545;&#36895;&#24230;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#34892;&#20026;&#35780;&#32423;&#12290;&#23545;&#20110;&#24052;&#22763;&#12289;&#27773;&#36710;&#21644;&#21345;&#36710;&#39550;&#39542;&#21592;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#24320;&#21457;&#20986;&#19968;&#31181;&#39044;&#27979;&#22278;&#29615;&#20132;&#21449;&#21475;&#36947;&#36335;&#20351;&#29992;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#23433;&#20840;&#20027;&#35201;&#24402;&#21151;&#20110;&#22278;&#29615;&#30340;&#20004;&#20010;&#22266;&#26377;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22278;&#29615;&#30340;&#24615;&#33021;&#24182;&#30740;&#31350;&#20154;&#31867;&#39550;&#39542;&#21592;&#19982;&#22278;&#29615;&#30340;&#20114;&#21160;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#23433;&#20840;&#24615;&#12289;&#23481;&#37327;&#21644;&#29615;&#22659;&#20248;&#21183;&#20197;&#21450;&#20026;&#36807;&#22659;&#21644;&#25972;&#21512;&#25552;&#20379;&#23433;&#20840;&#21644;&#27969;&#30021;&#36710;&#36742;&#27969;&#21160;&#65292;&#22278;&#29615;&#22312;&#22269;&#23478;&#38388;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22278;&#29615;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#36716;&#24367;&#36335;&#21475;&#30340;&#36895;&#24230;&#12289;&#20837;&#21475;&#36895;&#24230;&#20197;&#21450;&#20854;&#23545;&#36895;&#24230;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#34892;&#20026;&#35780;&#32423;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#65288;&#24052;&#22763;&#12289;&#27773;&#36710;&#12289;&#21345;&#36710;&#65289;&#39550;&#39542;&#21592;&#21463;&#21040;&#29305;&#21035;&#20851;&#27880;&#65292;&#24182;&#23558;&#20854;&#34892;&#20026;&#20998;&#20026;&#20445;&#23432;&#22411;&#12289;&#27491;&#24120;&#22411;&#21644;&#20405;&#30053;&#22411;&#12290;&#39044;&#27979;&#21644;&#35782;&#21035;&#39550;&#39542;&#21592;&#34892;&#20026;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#22278;&#29615;&#23545;&#36825;&#20123;&#20998;&#31867;&#22120;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#19968;&#31181;&#39044;&#27979;&#22278;&#29615;&#20132;&#21449;&#21475;&#36947;&#36335;&#20351;&#29992;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#23433;&#20840;&#20027;&#35201;&#24402;&#21151;&#20110;&#22278;&#29615;&#30340;&#20004;&#20010;&#22266;&#26377;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research aims to evaluate the performance of the rotors and study the behavior of the human driver in interacting with the rotors. In recent years, rotors have been increasingly used between countries due to their safety, capacity, and environmental advantages, and because they provide safe and fluid flows of vehicles for transit and integration. It turns out that roundabouts can significantly reduce speed at twisting intersections, entry speed and the resulting effect on speed depends on the rating of road users. In our research, (bus, car, truck) drivers were given special attention and their behavior was categorized into (conservative, normal, aggressive). Anticipating and recognizing driver behavior is an important challenge. Therefore, the aim of this research is to study the effect of roundabouts on these classifiers and to develop a method for predicting the behavior of road users at roundabout intersections. Safety is primarily due to two inherent features of the rotor. Fi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#23481;&#23457;&#26597;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#21644;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23545;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#30340;&#25913;&#36827;&#25928;&#26524;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.14517</link><description>&lt;p&gt;
&#27880;&#24847;&#35328;&#36766;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20869;&#23481;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Watch Your Language: Large Language Models and Content Moderation. (arXiv:2309.14517v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#23481;&#23457;&#26597;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#21644;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23545;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#30340;&#25913;&#36827;&#25928;&#26524;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#22522;&#20110;&#25991;&#26412;&#30340;&#20869;&#23481;&#23457;&#26597;&#26159;&#20854;&#20013;&#19968;&#20010;&#21463;&#21040;&#36817;&#26399;&#28909;&#24773;&#20851;&#27880;&#30340;LLM&#24212;&#29992;&#26696;&#20363;&#65292;&#28982;&#32780;&#65292;&#40092;&#26377;&#30740;&#31350;&#35843;&#26597;LLMs&#22312;&#20869;&#23481;&#23457;&#26597;&#35774;&#32622;&#20013;&#30340;&#34920;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#22871;&#29616;&#20195;&#12289;&#21830;&#19994;&#21270;&#30340;LLMs&#65288;GPT-3&#12289;GPT-3.5&#12289;GPT-4&#65289;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20869;&#23481;&#23457;&#26597;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65306;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#21644;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#12290;&#23545;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;95&#20010;LLM&#23457;&#26597;&#24341;&#25806;&#65292;&#24182;&#20351;&#29992;95&#20010;Reddit&#23376;&#31038;&#21306;&#30340;&#35268;&#21017;&#36827;&#34892;&#25351;&#23548;&#65292;&#21457;&#29616;LLMs&#22312;&#35768;&#22810;&#31038;&#21306;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#23457;&#26597;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#20013;&#20301;&#25968;&#20934;&#30830;&#29575;&#20026;64%&#21644;&#20013;&#20301;&#25968;&#31934;&#30830;&#24230;&#20026;83%&#12290;&#22312;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#21830;&#19994;&#21487;&#29992;&#30340;&#26377;&#23475;&#24615;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#26368;&#36817;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23545;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#20960;&#20046;&#27809;&#26377;&#24102;&#26469;&#26126;&#26174;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exploded in popularity due to their ability to perform a wide array of natural language tasks. Text-based content moderation is one LLM use case that has received recent enthusiasm, however, there is little research investigating how LLMs perform in content moderation settings. In this work, we evaluate a suite of modern, commercial LLMs (GPT-3, GPT-3.5, GPT-4) on two common content moderation tasks: rule-based community moderation and toxic content detection. For rule-based community moderation, we construct 95 LLM moderation-engines prompted with rules from 95 Reddit subcommunities and find that LLMs can be effective at rule-based moderation for many communities, achieving a median accuracy of 64% and a median precision of 83%. For toxicity detection, we find that LLMs significantly outperform existing commercially available toxicity classifiers. However, we also find that recent increases in model size add only marginal benefit to toxicity detection
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14496</link><description>&lt;p&gt;
Era Splitting.&#65288;arXiv:2309.14496v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Era Splitting. (arXiv:2309.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20250;&#21576;&#29616;&#20986;&#25968;&#25454;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#34892;&#20026;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#33539;&#24335;&#30340;&#33539;&#22260;&#65292;&#35813;&#33539;&#24335;&#20551;&#35774;&#25968;&#25454;&#22312;&#26102;&#38388;&#21644;&#22320;&#28857;&#19978;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#26032;&#20852;&#30340;&#36229;&#20998;&#24067;&#27867;&#21270;&#39046;&#22495;&#36890;&#36807;&#23558;&#29615;&#22659;&#25110;&#26102;&#20195;&#20449;&#24687;&#34701;&#20837;&#31639;&#27861;&#20013;&#65292;&#26469;&#24212;&#23545;&#36825;&#20010;&#29616;&#23454;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;/&#25110;&#31070;&#32463;&#32593;&#32476;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#19982;&#27599;&#20010;&#25968;&#25454;&#28857;&#30456;&#20851;&#30340;&#26102;&#20195;&#20449;&#24687;&#65292;&#26469;&#25214;&#21040;&#22312;&#25968;&#25454;&#30340;&#25152;&#26377;&#19981;&#30456;&#20132;&#26102;&#20195;&#20013;&#37117;&#26159;&#26368;&#20248;&#30340;&#20999;&#20998;&#28857;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20154;&#31867;&#21644;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35780;&#20272;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26102;&#65292;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.14488</link><description>&lt;p&gt;
&#24403;&#33258;&#21160;&#21270;&#35780;&#20272;&#36935;&#19978;&#33258;&#21160;&#21270;&#20869;&#23481;&#29983;&#25104;&#65306;&#22312;GPT&#26102;&#20195;&#23457;&#26597;&#25991;&#26412;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs. (arXiv:2309.14488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20154;&#31867;&#21644;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35780;&#20272;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26102;&#65292;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35780;&#20272;&#21644;&#25171;&#20998;&#25991;&#26412;&#25968;&#25454;&#26041;&#38754;&#30340;&#24212;&#29992;&#24050;&#32463;&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#25628;&#32034;&#21644;&#25512;&#33616;&#20197;&#21450;&#22312;&#32447;&#20869;&#23481;&#21487;&#20449;&#24230;&#35780;&#20272;&#31561;&#21508;&#31181;&#24773;&#22659;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#25991;&#26412;&#20132;&#21449;&#39046;&#22495;&#30340;&#19968;&#27425;&#37325;&#35201;&#21464;&#38761;&#26159;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#31561;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#24615;&#35780;&#20272;&#20154;&#31867;&#21644;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#23545;&#20110;&#22522;&#20110;&#20154;&#31867;&#20869;&#23481;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#25171;&#20998;&#27169;&#22411;&#22914;&#20309;&#35780;&#20272;&#20869;&#23481;&#36136;&#37327;&#30340;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#35770;&#25991;&#35780;&#20998;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12289;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#30340;&#35770;&#25991;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20197;&#31616;&#27905;&#32771;&#34385;&#21040;&#34987;&#35843;&#26597;&#32773;&#31867;&#22411;&#12289;&#25552;&#31034;&#31867;&#22411;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#20016;&#23500;&#30340;&#27979;&#35797;&#26679;&#26412;&#65292;&#28085;&#30422;&#20102;18,460&#31687;&#20154;&#24037;&#29983;&#25104;&#21644;GPT&#29983;&#25104;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#32447;&#24615;&#20998;&#31867;&#22120;&#22312;&#35780;&#20272;GPT&#29983;&#25104;&#30340;&#35770;&#25991;&#26102;&#24615;&#33021;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of machine learning (ML) models to assess and score textual data has become increasingly pervasive in an array of contexts including natural language processing, information retrieval, search and recommendation, and credibility assessment of online content. A significant disruption at the intersection of ML and text are text-generating large-language models such as generative pre-trained transformers (GPTs). We empirically assess the differences in how ML-based scoring models trained on human content assess the quality of content generated by humans versus GPTs. To do so, we propose an analysis framework that encompasses essay scoring ML-models, human and ML-generated essays, and a statistical model that parsimoniously considers the impact of type of respondent, prompt genre, and the ML model used for assessment model. A rich testbed is utilized that encompasses 18,460 human-generated and GPT-based essays. Results of our benchmark analysis reveal that transformer pretrained lan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#38598;&#25104;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#19978;&#33394;&#22270;&#20687;&#26816;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#21644;&#36164;&#28304;&#38656;&#27714;&#26469;&#21306;&#20998;&#33258;&#28982;&#24425;&#33394;&#21644;&#35745;&#31639;&#26426;&#19978;&#33394;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2309.14478</link><description>&lt;p&gt;
&#34701;&#21512;&#38598;&#25104;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#19978;&#33394;&#22270;&#20687;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Incorporating Ensemble and Transfer Learning For An End-To-End Auto-Colorized Image Detection Model. (arXiv:2309.14478v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#38598;&#25104;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#19978;&#33394;&#22270;&#20687;&#26816;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#21644;&#36164;&#28304;&#38656;&#27714;&#26469;&#21306;&#20998;&#33258;&#28982;&#24425;&#33394;&#21644;&#35745;&#31639;&#26426;&#19978;&#33394;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#19978;&#33394;&#26159;&#23558;&#28784;&#24230;&#22270;&#20687;&#19978;&#33394;&#25110;&#37325;&#26032;&#19978;&#33394;&#24050;&#24425;&#33394;&#22270;&#20687;&#30340;&#36807;&#31243;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#20351;&#21355;&#26143;&#12289;&#21307;&#23398;&#21644;&#21382;&#21490;&#22270;&#20687;&#26356;&#20855;&#34920;&#29616;&#21147;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#19978;&#33394;&#31639;&#27861;&#30340;&#32467;&#26524;&#21464;&#24471;&#26356;&#21152;&#36924;&#30495;&#65292;&#20197;&#33267;&#20110;&#20154;&#30524;&#26080;&#27861;&#21306;&#20998;&#33258;&#28982;&#22270;&#20687;&#21644;&#19978;&#33394;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#20266;&#36896;&#25110;&#38750;&#27861;&#20462;&#25913;&#30340;&#22270;&#20687;&#21487;&#20197;&#34987;&#29992;&#20110;&#38750;&#27861;&#30446;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#26469;&#21306;&#20998;&#33258;&#28982;&#24425;&#33394;&#22270;&#20687;&#21644;&#35745;&#31639;&#26426;&#19978;&#33394;&#22270;&#20687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#38598;&#25104;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#29992;&#20110;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#21644;&#36164;&#28304;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#23545;&#33258;&#28982;&#24425;&#33394;&#21644;&#35745;&#31639;&#26426;&#19978;&#33394;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#20998;&#25903;VGG16&#21644;Resn
&lt;/p&gt;
&lt;p&gt;
Image colorization is the process of colorizing grayscale images or recoloring an already-color image. This image manipulation can be used for grayscale satellite, medical and historical images making them more expressive. With the help of the increasing computation power of deep learning techniques, the colorization algorithms results are becoming more realistic in such a way that human eyes cannot differentiate between natural and colorized images. However, this poses a potential security concern, as forged or illegally manipulated images can be used illegally. There is a growing need for effective detection methods to distinguish between natural color and computer-colorized images. This paper presents a novel approach that combines the advantages of transfer and ensemble learning approaches to help reduce training time and resource requirements while proposing a model to classify natural color and computer-colorized images. The proposed model uses pre-trained branches VGG16 and Resn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26657;&#27491;&#20559;&#24046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#32452;&#20214;&#30340;&#28151;&#21512;&#31574;&#30053;&#24182;&#30001;&#20998;&#24320;&#30340;&#32593;&#32476;&#36827;&#34892;&#35780;&#20272;&#65292;&#28040;&#38500;&#20102;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#12290;&#22312;&#19968;&#23567;&#32452;MuJoCo&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#25509;&#36817;SOTA&#30340;&#32467;&#26524;&#12290; (&#26657;&#27491;&#20559;&#24046;&#26041;&#27861;&#65292;&#28151;&#21512;&#31574;&#30053;&#65292;&#20998;&#24320;&#30340;&#32593;&#32476;&#35780;&#20272;)</title><link>http://arxiv.org/abs/2309.14471</link><description>&lt;p&gt;
&#20026;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#36866;&#24212;&#21452;Q-Learning
&lt;/p&gt;
&lt;p&gt;
Adapting Double Q-Learning for Continuous Reinforcement Learning. (arXiv:2309.14471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26657;&#27491;&#20559;&#24046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#32452;&#20214;&#30340;&#28151;&#21512;&#31574;&#30053;&#24182;&#30001;&#20998;&#24320;&#30340;&#32593;&#32476;&#36827;&#34892;&#35780;&#20272;&#65292;&#28040;&#38500;&#20102;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#12290;&#22312;&#19968;&#23567;&#32452;MuJoCo&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#25509;&#36817;SOTA&#30340;&#32467;&#26524;&#12290; (&#26657;&#27491;&#20559;&#24046;&#26041;&#27861;&#65292;&#28151;&#21512;&#31574;&#30053;&#65292;&#20998;&#24320;&#30340;&#32593;&#32476;&#35780;&#20272;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#25511;&#21046;&#25216;&#26415;&#12290;&#22823;&#22810;&#25968;&#36825;&#20123;&#25216;&#26415;&#22522;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20027;&#35201;&#35299;&#20915;&#30340;&#26159;&#36807;&#39640;&#20272;&#35745;&#30340;&#32467;&#26524;&#65292;&#32780;&#38750;&#20854;&#26681;&#26412;&#21407;&#22240;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26657;&#27491;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;&#21452;Q-Learning&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#30001;&#20004;&#20010;&#32452;&#25104;&#25104;&#20998;&#26500;&#25104;&#30340;&#28151;&#21512;&#31574;&#30053;&#12290;&#27599;&#20010;&#31574;&#30053;&#25104;&#20998;&#30001;&#20998;&#21035;&#26368;&#22823;&#21270;&#21644;&#35780;&#20272;&#30340;&#32593;&#32476;&#22788;&#29702;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#23567;&#32452;MuJoCo&#29615;&#22659;&#19978;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25509;&#36817;SOTA&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Majority of off-policy reinforcement learning algorithms use overestimation bias control techniques. Most of these techniques rooted in heuristics, primarily addressing the consequences of overestimation rather than its fundamental origins. In this work we present a novel approach to the bias correction, similar in spirit to Double Q-Learning. We propose using a policy in form of a mixture with two components. Each policy component is maximized and assessed by separate networks, which removes any basis for the overestimation bias. Our approach shows promising near-SOTA results on a small set of MuJoCo environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DefGoalNet&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23569;&#37327;&#20154;&#31867;&#31034;&#33539;&#30452;&#25509;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#30446;&#26631;&#24418;&#29366;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.14463</link><description>&lt;p&gt;
DefGoalNet&#65306;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#26102;&#30340;&#19978;&#19979;&#25991;&#30446;&#26631;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DefGoalNet: Contextual Goal Learning from Demonstrations For Deformable Object Manipulation. (arXiv:2309.14463v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DefGoalNet&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23569;&#37327;&#20154;&#31867;&#31034;&#33539;&#30452;&#25509;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#30446;&#26631;&#24418;&#29366;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#20282;&#26381;&#26159;&#19968;&#31181;&#25511;&#21046;&#29289;&#20307;&#21040;&#36798;&#39044;&#26399;&#30446;&#26631;&#24418;&#29366;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#23545;&#20110;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#25805;&#20316;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#24418;&#29366;&#30340;&#35268;&#23450;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;&#30446;&#26631;&#24418;&#29366;&#36890;&#24120;&#36890;&#36807;&#32321;&#29712;&#30340;&#39046;&#22495;&#30693;&#35782;&#24037;&#31243;&#36807;&#31243;&#33719;&#21462;&#65292;&#25110;&#32773;&#36890;&#36807;&#25163;&#21160;&#25805;&#20316;&#29289;&#20307;&#21040;&#36798;&#25152;&#38656;&#24418;&#29366;&#24182;&#25429;&#33719;&#35813;&#29305;&#23450;&#26102;&#21051;&#30340;&#30446;&#26631;&#24418;&#29366;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#37117;&#19981;&#20999;&#23454;&#38469;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;DefGoalNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#30452;&#25509;&#20174;&#23569;&#37327;&#20154;&#31867;&#31034;&#33539;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#30446;&#26631;&#24418;&#29366;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25163;&#26415;&#25764;&#36864;&#20219;&#21153;&#20013;&#65292;&#21363;&#20351;&#20165;&#20351;&#29992;10&#20010;&#31034;&#33539;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25104;&#21151;&#29575;&#20013;&#20540;&#20063;&#25509;&#36817;90%&#12290;&#36825;&#20123;&#32467;&#26524;&#26631;&#24535;&#30528;&#22312;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20013;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shape servoing, a robotic task dedicated to controlling objects to desired goal shapes, is a promising approach to deformable object manipulation. An issue arises, however, with the reliance on the specification of a goal shape. This goal has been obtained either by a laborious domain knowledge engineering process or by manually manipulating the object into the desired shape and capturing the goal shape at that specific moment, both of which are impractical in various robotic applications. In this paper, we solve this problem by developing a novel neural network DefGoalNet, which learns deformable object goal shapes directly from a small number of human demonstrations. We demonstrate our method's effectiveness on various robotic tasks, both in simulation and on a physical robot. Notably, in the surgical retraction task, even when trained with as few as 10 demonstrations, our method achieves a median success percentage of nearly 90%. These results mark a substantial advancement in enabl
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#22312;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#20013;&#38754;&#20020;&#30340;&#31867;&#21035;&#20998;&#24067;&#27874;&#21160;&#21644;&#25968;&#25454;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#35757;&#32451;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#24037;&#20316;&#37327;&#20943;&#23569;5&#20493;&#12290;</title><link>http://arxiv.org/abs/2309.14460</link><description>&lt;p&gt;
&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Active Learning For Sound Event Detection. (arXiv:2309.14460v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14460
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#22312;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#20013;&#38754;&#20020;&#30340;&#31867;&#21035;&#20998;&#24067;&#27874;&#21160;&#21644;&#25968;&#25454;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#35757;&#32451;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#24037;&#20316;&#37327;&#20943;&#23569;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#26159;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#32321;&#29712;&#12289;&#32791;&#26102;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65288;OAL&#65289;&#26159;&#19968;&#31181;&#21516;&#26102;&#20943;&#23569;&#35757;&#32451;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#26631;&#27880;&#37327;&#24182;&#36866;&#24212;&#25968;&#25454;&#21464;&#21270;&#30340;&#33539;&#20363;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27874;&#21160;&#30340;&#31867;&#21035;&#20998;&#24067;&#21644;&#25968;&#25454;&#28418;&#31227;&#20173;&#28982;&#26159;OAL&#30340;&#24120;&#35265;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;OAL&#24212;&#29992;&#20110;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;SONYC&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#35821;&#38899;&#31867;&#22411;&#35782;&#21035;&#65288;VTD&#65289;&#35821;&#26009;&#24211;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OAL&#21487;&#20197;&#23558;&#35757;&#32451;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#24037;&#20316;&#37327;&#20943;&#23569;5&#20493;&#65292;&#24182;&#19988;&#26412;&#25991;&#20171;&#32461;&#30340;&#26032;&#26041;&#27861;&#25104;&#21151;&#35299;&#20915;&#20102;&#29616;&#26377;OAL&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data collection and annotation is a laborious, time-consuming prerequisite for supervised machine learning tasks. Online Active Learning (OAL) is a paradigm that addresses this issue by simultaneously minimizing the amount of annotation required to train a classifier and adapting to changes in the data over the duration of the data collection process. Prior work has indicated that fluctuating class distributions and data drift are still common problems for OAL. This work presents new loss functions that address these challenges when OAL is applied to Sound Event Detection (SED). Experimental results from the SONYC dataset and two Voice-Type Discrimination (VTD) corpora indicate that OAL can reduce the time and effort required to train SED classifiers by a factor of 5 for SONYC, and that the new methods presented here successfully resolve issues present in existing OAL methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#30340;&#21464;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#33258;&#24674;&#22797;&#26426;&#21046;&#35299;&#20915;&#20449;&#24687;&#19981;&#36275;&#12289;&#35745;&#21010;&#29983;&#25104;&#38169;&#35823;&#21644;&#25191;&#34892;&#22833;&#36133;&#31561;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#30340;&#25104;&#21151;&#23436;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.14425</link><description>&lt;p&gt;
&#33258;&#24674;&#22797;&#25552;&#31034;&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21644;&#33258;&#24674;&#22797;&#30340;&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Self-Recovery Prompting: Promptable General Purpose Service Robot System with Foundation Models and Self-Recovery. (arXiv:2309.14425v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#30340;&#21464;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#33258;&#24674;&#22797;&#26426;&#21046;&#35299;&#20915;&#20449;&#24687;&#19981;&#36275;&#12289;&#35745;&#21010;&#29983;&#25104;&#38169;&#35823;&#21644;&#25191;&#34892;&#22833;&#36133;&#31561;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#30340;&#25104;&#21151;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#65288;GPSR&#65289;&#33021;&#22815;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#25191;&#34892;&#22810;&#31181;&#20219;&#21153;&#65292;&#38656;&#35201;&#19968;&#20010;&#20855;&#26377;&#39640;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#31995;&#32479;&#26469;&#24212;&#23545;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#29615;&#22659;&#12290;&#26412;&#25991;&#39318;&#20808;&#22522;&#20110;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#39030;&#23618;GPSR&#31995;&#32479;&#65292;&#29992;&#20110;&#20840;&#29699;&#31454;&#36187;&#65288;RoboCup@Home 2023&#65289;&#12290;&#35813;&#31995;&#32479;&#26082;&#21487;&#20197;&#36866;&#24212;&#22810;&#31181;&#21464;&#21270;&#65292;&#21448;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;&#27599;&#20010;&#27169;&#22411;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20998;&#26512;&#25152;&#24320;&#21457;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26356;&#21152;&#29616;&#23454;&#30340;GPSR&#24212;&#29992;&#35774;&#32622;&#20013;&#23384;&#22312;&#19977;&#31181;&#22833;&#36133;&#31867;&#22411;&#65306;&#20449;&#24687;&#19981;&#36275;&#12289;&#38169;&#35823;&#30340;&#35745;&#21010;&#29983;&#25104;&#21644;&#35745;&#21010;&#25191;&#34892;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#24674;&#22797;&#25552;&#31034;&#31649;&#36947;&#65292;&#35813;&#31649;&#36947;&#25506;&#32034;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#24182;&#20462;&#25913;&#20854;&#25552;&#31034;&#26469;&#20174;&#22833;&#36133;&#20013;&#24674;&#22797;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#20855;&#26377;&#33258;&#24674;&#22797;&#26426;&#21046;&#30340;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#21508;&#31181;&#22833;&#36133;&#26696;&#20363;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#20379;&#34917;&#20805;&#30340;&#35270;&#39057;&#21487;&#22312;https://sites.google.com/view/srgpsr&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
A general-purpose service robot (GPSR), which can execute diverse tasks in various environments, requires a system with high generalizability and adaptability to tasks and environments. In this paper, we first developed a top-level GPSR system for worldwide competition (RoboCup@Home 2023) based on multiple foundation models. This system is both generalizable to variations and adaptive by prompting each model. Then, by analyzing the performance of the developed system, we found three types of failure in more realistic GPSR application settings: insufficient information, incorrect plan generation, and plan execution failure. We then propose the self-recovery prompting pipeline, which explores the necessary information and modifies its prompts to recover from failure. We experimentally confirm that the system with the self-recovery mechanism can accomplish tasks by resolving various failure cases. Supplementary videos are available at https://sites.google.com/view/srgpsr .
&lt;/p&gt;</description></item><item><title>LTU-AS&#26159;&#19968;&#20010;&#20855;&#26377;&#26222;&#36866;&#38899;&#39057;&#24863;&#30693;&#21644;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#21644;&#32852;&#21512;&#29702;&#35299;&#21475;&#35821;&#25991;&#26412;&#12289;&#35821;&#38899;&#22768;&#38899;&#23398;&#21644;&#38750;&#35821;&#38899;&#38899;&#39057;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2309.14405</link><description>&lt;p&gt;
&#32852;&#21512;&#38899;&#39057;&#21644;&#35821;&#38899;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Joint Audio and Speech Understanding. (arXiv:2309.14405v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14405
&lt;/p&gt;
&lt;p&gt;
LTU-AS&#26159;&#19968;&#20010;&#20855;&#26377;&#26222;&#36866;&#38899;&#39057;&#24863;&#30693;&#21644;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#21644;&#32852;&#21512;&#29702;&#35299;&#21475;&#35821;&#25991;&#26412;&#12289;&#35821;&#38899;&#22768;&#38899;&#23398;&#21644;&#38750;&#35821;&#38899;&#38899;&#39057;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21608;&#22260;&#20805;&#26021;&#30528;&#21253;&#25324;&#35821;&#38899;&#21644;&#38750;&#35821;&#38899;&#22768;&#38899;&#22312;&#20869;&#30340;&#38899;&#39057;&#20449;&#21495;&#12290;&#23545;&#35821;&#38899;&#21644;&#38750;&#35821;&#38899;&#38899;&#39057;&#20107;&#20214;&#30340;&#35782;&#21035;&#21644;&#29702;&#35299;&#65292;&#20197;&#21450;&#23545;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#28145;&#21051;&#29702;&#35299;&#65292;&#26500;&#25104;&#20102;&#22522;&#26412;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#27425;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;LTU-AS&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#26222;&#36941;&#38899;&#39057;&#24863;&#30693;&#21644;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23558;Whisper&#20316;&#20026;&#24863;&#30693;&#27169;&#22359;&#21644;LLaMA&#20316;&#20026;&#25512;&#29702;&#27169;&#22359;&#36827;&#34892;&#38598;&#25104;&#65292;LTU-AS&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#21644;&#32852;&#21512;&#29702;&#35299;&#21475;&#35821;&#25991;&#26412;&#12289;&#35821;&#38899;&#22768;&#38899;&#23398;&#20197;&#21450;&#38750;&#35821;&#38899;&#38899;&#39057;&#20107;&#20214; - &#20960;&#20046;&#21487;&#20197;&#20174;&#38899;&#39057;&#20449;&#21495;&#20013;&#24863;&#30693;&#21040;&#30340;&#19968;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are surrounded by audio signals that include both speech and non-speech sounds. The recognition and understanding of speech and non-speech audio events, along with a profound comprehension of the relationship between them, constitute fundamental cognitive capabilities. For the first time, we build a machine learning model, called LTU-AS, that has a conceptually similar universal audio perception and advanced reasoning ability. Specifically, by integrating Whisper as a perception module and LLaMA as a reasoning module, LTU-AS can simultaneously recognize and jointly understand spoken text, speech paralinguistics, and non-speech audio events - almost everything perceivable from audio signals.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26085;&#26399;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29109;-&#22797;&#26434;&#24230;&#21644;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#65292;&#21306;&#20998;&#27491;&#24120;&#21644;&#28151;&#20081;&#26102;&#38388;&#24207;&#21015;&#65292;&#29992;&#20110;&#35782;&#21035;&#34880;&#28082;&#36879;&#26512;&#29289;&#30232;&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2309.14399</link><description>&lt;p&gt;
&#26085;&#26399;&#39537;&#21160;&#30340;&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;&#34880;&#28082;&#36879;&#26512;&#29289;&#30232;&#30340;&#29366;&#24577;&#65306;&#29109;-&#22797;&#26434;&#24230;&#21644;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Date-Driven Approach for Identifying State of Hemodialysis Fistulas: Entropy-Complexity and Formal Concept Analysis. (arXiv:2309.14399v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26085;&#26399;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29109;-&#22797;&#26434;&#24230;&#21644;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#65292;&#21306;&#20998;&#27491;&#24120;&#21644;&#28151;&#20081;&#26102;&#38388;&#24207;&#21015;&#65292;&#29992;&#20110;&#35782;&#21035;&#34880;&#28082;&#36879;&#26512;&#29289;&#30232;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#21306;&#20998;&#27491;&#24120;&#21644;&#28151;&#20081;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#23398;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#35782;&#21035;&#30149;&#29702;&#24615;&#29289;&#30232;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#25239;&#22122;&#22768;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#31867;&#27491;&#24120;&#21644;&#30149;&#29702;&#24615;&#21151;&#33021;&#30340;&#29289;&#30232;&#30340;&#21709;&#24212;&#34892;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20197;&#19979;&#20551;&#35774;&#65306;&#23618;&#27969;&#34880;&#28082;&#27969;&#21160;&#34920;&#31034;&#27491;&#24120;&#21151;&#33021;&#65292;&#32780;&#28237;&#27969;&#27969;&#21160;&#34920;&#31034;&#30149;&#29702;&#24615;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#21306;&#20998;&#28151;&#20081;&#21644;&#27491;&#24120;&#26102;&#38388;&#24207;&#21015;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#28041;&#21450;&#23558;&#26102;&#38388;&#24207;&#21015;&#26144;&#23556;&#21040;&#29109;-&#22797;&#26434;&#24230;&#24179;&#38754;&#19978;&#65292;&#28982;&#21518;&#19982;&#24050;&#24314;&#31435;&#30340;&#32858;&#31867;&#36827;&#34892;&#27604;&#36739;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#30001;&#20316;&#32773;&#24341;&#20837;&#30340;&#65292;&#23427;&#20351;&#29992;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#26500;&#24314;&#20102;&#19968;&#20010;&#27010;&#24565;-&#23545;&#35937;&#22270;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#30830;&#23450;&#29289;&#30232;&#29366;&#24577;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper explores mathematical methods that differentiate regular and chaotic time series, specifically for identifying pathological fistulas. It proposes a noise-resistant method for classifying responding rows of normally and pathologically functioning fistulas. This approach is grounded in the hypothesis that laminar blood flow signifies normal function, while turbulent flow indicates pathology. The study explores two distinct methods for distinguishing chaotic from regular time series. The first method involves mapping the time series onto the entropy-complexity plane and subsequently comparing it to established clusters. The second method, introduced by the authors, constructs a concepts-objects graph using formal concept analysis. Both of these methods exhibit high efficiency in determining the state of the fistula.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#22312;&#21160;&#26426;&#24615;&#35775;&#35848;&#20013;&#20934;&#30830;&#21306;&#20998;&#20102;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#19977;&#31181;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#22120;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#23545;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#21644;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#25214;&#21040;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2309.14398</link><description>&lt;p&gt;
&#30475;&#35265;&#21644;&#21548;&#21040;&#27809;&#34987;&#35828;&#30340;&#35805;&#65306;&#21487;&#35299;&#37322;&#34701;&#21512;&#30340;&#22810;&#27169;&#24577;&#21160;&#26426;&#24615;&#35775;&#35848;&#23458;&#25143;&#34892;&#20026;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion. (arXiv:2309.14398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#22312;&#21160;&#26426;&#24615;&#35775;&#35848;&#20013;&#20934;&#30830;&#21306;&#20998;&#20102;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#19977;&#31181;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#22120;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#23545;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#21644;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#25214;&#21040;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#24615;&#35775;&#35848;&#65288;MI&#65289;&#26159;&#19968;&#31181;&#24378;&#35843;&#21512;&#20316;&#24182;&#40723;&#21169;&#34892;&#20026;&#25913;&#21464;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;MI&#23545;&#35805;&#30340;&#36136;&#37327;&#65292;&#21487;&#20197;&#21033;&#29992;MISC&#20195;&#30721;&#23558;&#23458;&#25143;&#35805;&#35821;&#20998;&#31867;&#20026;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#25110;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#12290;MI&#23545;&#35805;&#20013;&#21464;&#21270;&#35805;&#35821;&#30340;&#27604;&#20363;&#19982;&#27835;&#30103;&#32467;&#26524;&#21576;&#27491;&#30456;&#20851;&#65292;&#22240;&#27492;&#20934;&#30830;&#20998;&#31867;&#23458;&#25143;&#35805;&#35821;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#20934;&#30830;&#21306;&#20998;&#19977;&#20010;MISC&#31867;&#21035;&#65288;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#65289;&#12290;&#20026;&#20102;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#25910;&#38598;&#20102;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivational Interviewing (MI) is an approach to therapy that emphasizes collaboration and encourages behavioral change. To evaluate the quality of an MI conversation, client utterances can be classified using the MISC code as either change talk, sustain talk, or follow/neutral talk. The proportion of change talk in a MI conversation is positively correlated with therapy outcomes, making accurate classification of client utterances essential. In this paper, we present a classifier that accurately distinguishes between the three MISC classes (change talk, sustain talk, and follow/neutral talk) leveraging multimodal features such as text, prosody, facial expressivity, and body expressivity. To train our model, we perform annotations on the publicly available AnnoMI dataset to collect multimodal information, including text, audio, facial expressivity, and body expressivity. Furthermore, we identify the most important modalities in the decision-making process, providing valuable insights i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38598;&#25104;&#36710;&#36742;&#36319;&#38543;&#21644;&#21464;&#36947;&#20915;&#31574;&#25511;&#21046;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#36895;&#20844;&#36335;&#19978;&#31361;&#21457;&#36335;&#38556;&#24773;&#20917;&#19979;&#26234;&#33021;&#36710;&#36742;&#30340;&#34892;&#36710;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14395</link><description>&lt;p&gt;
&#38544;&#24615;&#24863;&#30693;&#22312;&#20132;&#36890;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#65306;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Implicit Sensing in Traffic Optimization: Advanced Deep Reinforcement Learning Techniques. (arXiv:2309.14395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38598;&#25104;&#36710;&#36742;&#36319;&#38543;&#21644;&#21464;&#36947;&#20915;&#31574;&#25511;&#21046;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#36895;&#20844;&#36335;&#19978;&#31361;&#21457;&#36335;&#38556;&#24773;&#20917;&#19979;&#26234;&#33021;&#36710;&#36742;&#30340;&#34892;&#36710;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#31361;&#28982;&#36335;&#38556;&#30001;&#20110;&#36947;&#36335;&#32500;&#25252;&#12289;&#20107;&#25925;&#21644;&#27773;&#36710;&#32500;&#20462;&#31561;&#21407;&#22240;&#26159;&#25105;&#20204;&#20960;&#20046;&#27599;&#22825;&#37117;&#20250;&#36935;&#21040;&#30340;&#24773;&#20917;&#12290;&#37197;&#22791;&#21487;&#20197;&#33719;&#21462;&#36710;&#36742;&#21160;&#24577;&#20449;&#24687;&#65288;&#22914;&#36895;&#24230;&#12289;&#21152;&#36895;&#24230;&#21644;&#20301;&#32622;&#65289;&#30340;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#21487;&#20197;&#22312;&#21040;&#36798;&#36335;&#38556;&#20043;&#21069;&#20570;&#20986;&#26234;&#33021;&#20915;&#31574;&#26469;&#21464;&#25442;&#36710;&#36947;&#12290;&#35768;&#22810;&#25991;&#29486;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;&#36710;&#36742;&#36319;&#38543;&#27169;&#22411;&#21644;&#21464;&#36947;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#24456;&#23569;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#38598;&#25104;&#30340;&#36710;&#36742;&#36319;&#38543;&#21644;&#21464;&#36947;&#27169;&#22411;&#65292;&#36825;&#20010;&#27169;&#22411;&#26377;&#28508;&#21147;&#27169;&#25311;&#23454;&#38469;&#30340;&#39550;&#39542;&#25805;&#32437;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#38598;&#25104;&#36710;&#36742;&#36319;&#38543;&#21644;&#21464;&#36947;&#20915;&#31574;&#25511;&#21046;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#23558;&#36827;&#34892;&#31361;&#21457;&#26045;&#24037;&#30340;&#24773;&#26223;&#12290;&#25105;&#20204;&#23558;&#24773;&#26223;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#37319;&#29992;&#30528;&#21517;&#30340;DQN&#31639;&#27861;&#26469;&#35757;&#32451;RL&#20195;&#29702;&#20197;&#21046;&#23450;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
A sudden roadblock on highways due to many reasons such as road maintenance, accidents, and car repair is a common situation we encounter almost daily. Autonomous Vehicles (AVs) equipped with sensors that can acquire vehicle dynamics such as speed, acceleration, and location can make intelligent decisions to change lanes before reaching a roadblock. A number of literature studies have examined car-following models and lane-changing models. However, only a few studies proposed an integrated car-following and lane-changing model, which has the potential to model practical driving maneuvers. Hence, in this paper, we present an integrated car-following and lane-changing decision-control system based on Deep Reinforcement Learning (DRL) to address this issue. Specifically, we consider a scenario where sudden construction work will be carried out along a highway. We model the scenario as a Markov Decision Process (MDP) and employ the well-known DQN algorithm to train the RL agent to make the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#65288;MDD&#65289;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#32423;&#21035;&#26469;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20219;&#24847;&#22495;&#20043;&#38388;&#30340;&#32763;&#35793;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14394</link><description>&lt;p&gt;
&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation. (arXiv:2309.14394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#65288;MDD&#65289;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#32423;&#21035;&#26469;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20219;&#24847;&#22495;&#20043;&#38388;&#30340;&#32763;&#35793;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#38388;&#32763;&#35793;&#28041;&#21450;&#22312;&#32473;&#23450;&#28304;&#22495;&#26465;&#20214;&#19979;&#29983;&#25104;&#30446;&#26631;&#22495;&#26679;&#26412;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#22266;&#23450;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#22495;&#19978;&#65292;&#21363;&#23427;&#20204;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#37197;&#32622;&#65288;&#20363;&#22914;&#23545;&#20110;&#20004;&#20010;&#22495;&#65292;&#35201;&#20040;$D_1\rightarrow{}D_2$&#65292;&#35201;&#20040;$D_2\rightarrow{}D_1$&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Multi-Domain Diffusion&#65288;MDD&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#30340;&#26465;&#20214;&#25193;&#25955;&#26694;&#26550;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MDD&#19981;&#38656;&#35201;&#23450;&#20041;&#36755;&#20837;&#21644;&#36755;&#20986;&#22495;&#65292;&#20801;&#35768;&#22312;&#19968;&#32452;&#22495;&#30340;&#20219;&#20309;&#20998;&#21306;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65288;&#20363;&#22914;$(D_1, D_2)\rightarrow{}D_3$&#65292;$D_2\rightarrow{}(D_1, D_3)$&#65292;$D_3\rightarrow{}D_1$&#31561;&#65289;&#65292;&#32780;&#26080;&#38656;&#20026;&#27599;&#20010;&#22495;&#37197;&#32622;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;MDD&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#24418;&#24335;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#22495;&#24341;&#20837;&#19968;&#20010;&#22122;&#22768;&#32423;&#21035;&#65292;&#20197;&#33258;&#28982;&#30340;&#26041;&#24335;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#23558;&#20256;&#32479;&#30340;&#32763;&#35793;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#36890;&#36807;&#22122;&#22768;&#24314;&#27169;&#26469;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-to-domain translation involves generating a target domain sample given a condition in the source domain. Most existing methods focus on fixed input and output domains, i.e. they only work for specific configurations (i.e. for two domains, either $D_1\rightarrow{}D_2$ or $D_2\rightarrow{}D_1$). This paper proposes Multi-Domain Diffusion (MDD), a conditional diffusion framework for multi-domain translation in a semi-supervised context. Unlike previous methods, MDD does not require defining input and output domains, allowing translation between any partition of domains within a set (such as $(D_1, D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$, etc. for 3 domains), without the need to train separate models for each domain configuration. The key idea behind MDD is to leverage the noise formulation of diffusion models by incorporating one noise level per domain, which allows missing domains to be modeled with noise in a natural way. This transforms the tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#38024;&#23545;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14393</link><description>&lt;p&gt;
LLMCarbon: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models. (arXiv:2309.14393v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#38024;&#23545;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30899;&#36275;&#36857;&#26159;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#25512;&#29702;&#12289;&#23454;&#39564;&#21644;&#23384;&#20648;&#36807;&#31243;&#20013;&#30340;&#25490;&#25918;&#65292;&#21253;&#25324;&#36816;&#33829;&#21644;&#22266;&#23450;&#30899;&#25490;&#25918;&#12290;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#22312;LLMs&#35757;&#32451;&#20043;&#21069;&#20934;&#30830;&#20272;&#35745;&#20854;&#30899;&#24433;&#21709;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;GPU&#30340;&#20351;&#29992;&#12290;&#29616;&#26377;&#30740;&#31350;&#24050;&#25253;&#21578;&#20102;LLMs&#35757;&#32451;&#30340;&#30899;&#36275;&#36857;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#24037;&#20855;mlco2&#33021;&#22815;&#22312;&#23454;&#38469;&#35757;&#32451;&#20043;&#21069;&#39044;&#27979;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#30899;&#36275;&#36857;&#12290;&#28982;&#32780;&#65292;mlco2&#23384;&#22312;&#19968;&#20123;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;&#23427;&#19981;&#33021;&#25193;&#23637;&#20854;&#23545;&#23494;&#38598;&#25110;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;LLMs&#30340;&#20272;&#35745;&#65292;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#26550;&#26500;&#21442;&#25968;&#65292;&#20165;&#20851;&#27880;GPU&#65292;&#24182;&#19981;&#33021;&#24314;&#27169;&#22266;&#21270;&#30340;&#30899;&#36275;&#36857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMCarbon&#65292;&#19968;&#20010;&#20026;&#23494;&#38598;&#22411;&#21644;MoE LLMs&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;&#39044;&#27979;&#27169;&#22411;&#12290;&#19982;mlco2&#30456;&#27604;&#65292;LLMCarbon&#26174;&#33879;&#22686;&#24378;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \textit{LLMCarbon}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly enhances the ac
&lt;/p&gt;</description></item><item><title>&#19968;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#34987;&#20171;&#32461;&#26469;&#35299;&#37322;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#38754;&#21521;&#26381;&#21153;&#31995;&#32479;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#36890;&#36807;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#24314;&#31435;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2309.14391</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#35299;&#37322;&#38754;&#21521;&#26381;&#21153;&#31995;&#32479;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#30340;AI&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
An AI Chatbot for Explaining Deep Reinforcement Learning Decisions of Service-oriented Systems. (arXiv:2309.14391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14391
&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#34987;&#20171;&#32461;&#26469;&#35299;&#37322;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#38754;&#21521;&#26381;&#21153;&#31995;&#32479;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#36890;&#36807;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#24314;&#31435;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (Deep RL) &#22312;&#38754;&#21521;&#26381;&#21153;&#31995;&#32479;&#20013;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#20197;&#24212;&#23545;&#24320;&#25918;&#19990;&#30028;&#30340;&#20551;&#35774;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21160;&#24577;&#26381;&#21153;&#32452;&#21512;&#12289;&#20316;&#19994;&#35843;&#24230;&#12289;&#21368;&#36733;&#20197;&#21450;&#26381;&#21153;&#36866;&#24212;&#31561;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20854;&#23398;&#21040;&#30340;&#20915;&#31574;&#31574;&#30053;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#40657;&#30418;&#23376;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#36807;&#31243;&#23545;&#20110;&#24110;&#21161;&#26381;&#21153;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#35843;&#35797;&#12289;&#25903;&#25345;&#26381;&#21153;&#25552;&#20379;&#21830;&#36981;&#23432;&#30456;&#20851;&#27861;&#24459;&#26694;&#26550;&#20197;&#21450;&#24110;&#21161;&#26381;&#21153;&#20351;&#29992;&#32773;&#24314;&#31435;&#20449;&#20219;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Chat4XAI&#65292;&#36890;&#36807;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#20419;&#36827;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#12290;&#19982;&#35270;&#35273;&#35299;&#37322;&#30456;&#27604;&#65292;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#25253;&#21578;&#20248;&#28857;&#21253;&#25324;&#38750;&#25216;&#26415;&#29992;&#25143;&#26356;&#22909;&#30340;&#21487;&#29702;&#35299;&#24615;&#12289;&#29992;&#25143;&#30340;&#25509;&#21463;&#24230;&#21644;&#20449;&#20219;&#24230;&#25552;&#39640;&#65292;&#20197;&#21450;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (Deep RL) is increasingly used to cope with the open-world assumption in service-oriented systems. Deep RL was successfully applied to problems such as dynamic service composition, job scheduling, and offloading, as well as service adaptation. While Deep RL offers many benefits, understanding the decision-making of Deep RL is challenging because its learned decision-making policy essentially appears as a black box. Yet, understanding the decision-making of Deep RL is key to help service developers perform debugging, support service providers to comply with relevant legal frameworks, and facilitate service users to build trust. We introduce Chat4XAI to facilitate the understanding of the decision-making of Deep RL by providing natural-language explanations. Compared with visual explanations, the reported benefits of natural-language explanations include better understandability for non-technical users, increased user acceptance and trust, as well as more effi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#29992;&#25143;&#26089;&#26399;&#27969;&#22833;&#30340;&#27169;&#22411;&#65292;&#20197;&#20419;&#36827;&#19994;&#21153;&#20915;&#31574;&#21644;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2309.14390</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#29992;&#25143;-&#20135;&#21697;&#20114;&#21160;&#26102;&#38388;&#24207;&#21015;&#20013;&#39044;&#27979;&#26089;&#26399;&#27969;&#22833;
&lt;/p&gt;
&lt;p&gt;
Early Churn Prediction from Large Scale User-Product Interaction Time Series. (arXiv:2309.14390v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#29992;&#25143;&#26089;&#26399;&#27969;&#22833;&#30340;&#27169;&#22411;&#65292;&#20197;&#20419;&#36827;&#19994;&#21153;&#20915;&#31574;&#21644;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#27969;&#22833;&#65292;&#22312;&#21508;&#31181;&#38754;&#21521;&#23458;&#25143;&#30340;&#19994;&#21153;&#22330;&#26223;&#20013;&#65292;&#20197;&#32467;&#26463;&#19982;&#20225;&#19994;&#20851;&#31995;&#20026;&#29305;&#24449;&#65292;&#23545;&#32463;&#27982;&#20135;&#29983;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#22312;&#35768;&#22810;&#31995;&#32479;&#23545;&#29992;&#25143;&#30340;&#34892;&#21160;&#20013;&#65292;&#22914;&#20419;&#38144;&#25240;&#25187;&#21644;&#30041;&#23384;&#27963;&#21160;&#65292;&#39044;&#27979;&#28508;&#22312;&#30340;&#27969;&#22833;&#23458;&#25143;&#26159;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;&#22312;&#21464;&#21160;&#21095;&#28872;&#30340;&#39046;&#22495;&#65292;&#22914;&#24187;&#24819;&#20307;&#32946;&#65292;&#19981;&#21487;&#39044;&#27979;&#30340;&#22240;&#32032;&#65292;&#22914;&#22269;&#38469;&#20307;&#32946;&#36187;&#20107;&#65292;&#29978;&#33267;&#21487;&#20197;&#24433;&#21709;&#21040;&#24120;&#35268;&#30340;&#28040;&#36153;&#20064;&#24815;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982;&#20132;&#26131;&#21382;&#21490;&#21644;&#29992;&#25143;-&#20135;&#21697;&#20114;&#21160;&#23545;&#20110;&#39044;&#27979;&#27969;&#22833;&#26159;&#26377;&#20215;&#20540;&#30340;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#28145;&#20837;&#30340;&#39046;&#22495;&#30693;&#35782;&#21644;&#22797;&#26434;&#30340;&#29305;&#24449;&#24037;&#31243;&#12290;&#27492;&#22806;&#65292;&#27969;&#22833;&#39044;&#27979;&#31995;&#32479;&#30340;&#29305;&#24449;&#24320;&#21457;&#21487;&#33021;&#20250;&#28040;&#32791;&#22823;&#37327;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#22312;&#20026;2&#20159;&#22810;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#30340;&#29983;&#20135;&#29615;&#22659;&#20013;&#65292;&#25512;&#29702;&#27969;&#27700;&#32447;&#20027;&#35201;&#38598;&#20013;&#22312;&#29305;&#24449;&#24037;&#31243;&#19978;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#23458;&#25143;&#27969;&#22833;&#30340;&#21487;&#33021;&#24615;&#65292;&#20419;&#36827;&#20915;&#31574;&#21644;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
User churn, characterized by customers ending their relationship with a business, has profound economic consequences across various Business-to-Customer scenarios. For numerous system-to-user actions, such as promotional discounts and retention campaigns, predicting potential churners stands as a primary objective. In volatile sectors like fantasy sports, unpredictable factors such as international sports events can influence even regular spending habits. Consequently, while transaction history and user-product interaction are valuable in predicting churn, they demand deep domain knowledge and intricate feature engineering. Additionally, feature development for churn prediction systems can be resource-intensive, particularly in production settings serving 200m+ users, where inference pipelines largely focus on feature engineering. This paper conducts an exhaustive study on predicting user churn using historical data. We aim to create a model forecasting customer churn likelihood, facil
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#20165;&#20351;&#29992;LLM&#26041;&#27861;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25506;&#35752;&#20102;&#22312;&#25991;&#26723;&#22270;&#20687;&#20013;&#24207;&#21015;&#21270;&#25991;&#26412;&#20449;&#24687;&#24182;&#30452;&#25509;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.14389</link><description>&lt;p&gt;
&#20998;&#26512;&#20165;&#20351;&#29992;LLM&#26041;&#27861;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Efficacy of an LLM-Only Approach for Image-based Document Question Answering. (arXiv:2309.14389v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#20165;&#20351;&#29992;LLM&#26041;&#27861;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25506;&#35752;&#20102;&#22312;&#25991;&#26723;&#22270;&#20687;&#20013;&#24207;&#21015;&#21270;&#25991;&#26412;&#20449;&#24687;&#24182;&#30452;&#25509;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26723;&#38382;&#31572;&#27169;&#22411;&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#25429;&#33719;&#22270;&#20687;&#20013;&#30340;&#24067;&#23616;&#21644;&#35270;&#35273;&#20803;&#32032;&#65292;&#20197;&#21450;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#24110;&#21161;&#23558;&#38382;&#39064;&#19982;&#22270;&#20687;&#36827;&#34892;&#35821;&#22659;&#21270;&#65292;&#24182;&#36890;&#36807;&#22806;&#37096;&#19990;&#30028;&#30693;&#35782;&#34917;&#20805;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20219;&#21153;&#20013;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#30456;&#23545;&#36129;&#29486;&#20173;&#19981;&#28165;&#26970;&#12290;&#32771;&#34385;&#21040;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#23588;&#20854;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#34920;&#29616;&#20986;&#23545;&#26032;&#20219;&#21153;&#30340;&#21331;&#36234;&#36866;&#24212;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25506;&#32034;&#20197;&#19979;&#26041;&#38754;&#65306;&#65288;1&#65289;&#20165;&#20351;&#29992;LLM&#26041;&#27861;&#22312;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12289;&#65288;2&#65289;&#22312;&#25991;&#26723;&#22270;&#20687;&#20013;&#24207;&#21015;&#21270;&#25991;&#26412;&#20449;&#24687;&#24182;&#30452;&#25509;&#23558;&#20854;&#39304;&#36865;&#32473;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#65292;&#20174;&#32780;&#32469;&#36807;&#26174;&#24335;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#38656;&#35201;&#12289;&#65288;3&#65289;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#24443;&#24213;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent document question answering models consist of two key components: the vision encoder, which captures layout and visual elements in images, and a Large Language Model (LLM) that helps contextualize questions to the image and supplements them with external world knowledge to generate accurate answers. However, the relative contributions of the vision encoder and the language model in these tasks remain unclear. This is especially interesting given the effectiveness of instruction-tuned LLMs, which exhibit remarkable adaptability to new tasks. To this end, we explore the following aspects in this work: (1) The efficacy of an LLM-only approach on document question answering tasks (2) strategies for serializing textual information within document images and feeding it directly to an instruction-tuned LLM, thus bypassing the need for an explicit vision encoder (3) thorough quantitative analysis on the feasibility of such an approach. Our comprehensive analysis encompasses six diverse 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#21644;&#38543;&#26426;&#26597;&#35810;&#22312;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#33041;&#20307;&#20849;&#21516;&#36827;&#21270;&#20013;&#30340;&#24433;&#21709;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#36825;&#20004;&#31181;&#26597;&#35810;&#26426;&#21046;&#23545;&#26426;&#22120;&#20154;&#24418;&#24577;&#30340;&#36827;&#21270;&#21644;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.14387</link><description>&lt;p&gt;
&#36890;&#36807;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;&#38543;&#26426;&#26597;&#35810;&#25506;&#32034;&#26426;&#22120;&#20154;&#24418;&#24577;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Exploring Robot Morphology Spaces through Breadth-First Search and Random Query. (arXiv:2309.14387v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14387
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#21644;&#38543;&#26426;&#26597;&#35810;&#22312;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#33041;&#20307;&#20849;&#21516;&#36827;&#21270;&#20013;&#30340;&#24433;&#21709;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#36825;&#20004;&#31181;&#26597;&#35810;&#26426;&#21046;&#23545;&#26426;&#22120;&#20154;&#24418;&#24577;&#30340;&#36827;&#21270;&#21644;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#26426;&#22120;&#20154;&#23398;&#20026;&#35774;&#35745;&#21644;&#36827;&#21270;&#26426;&#22120;&#20154;&#24418;&#24577;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#23588;&#20854;&#22312;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#22240;&#22411;&#21040;&#34920;&#22411;&#26144;&#23556;&#36807;&#31243;&#20013;&#65292;&#26597;&#35810;&#26426;&#21046;&#30340;&#20316;&#29992;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#30340;&#33041;&#20307;&#20849;&#21516;&#36827;&#21270;&#20013;&#26597;&#35810;&#26426;&#21046;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#26597;&#35810;&#26426;&#21046;&#8212;&#8212;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#21644;&#38543;&#26426;&#26597;&#35810;&#65292;&#22312;&#20351;&#29992;CPPN&#36827;&#21270;&#26426;&#22120;&#20154;&#24418;&#24577;&#21644;&#20351;&#29992;&#24352;&#37327;&#36827;&#21270;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#22312;&#20004;&#31181;&#36827;&#21270;&#26694;&#26550;&#65288;&#25289;&#39532;&#20811;&#21644;&#36798;&#23572;&#25991;&#31995;&#32479;&#65289;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23427;&#20204;&#23545;&#36827;&#21270;&#32467;&#26524;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26597;&#35810;&#26426;&#21046;&#23545;&#27169;&#22359;&#21270;&#26426;&#22120;&#20154;&#36523;&#20307;&#30340;&#36827;&#21270;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#24418;&#24577;&#26234;&#33021;&#12289;&#22810;&#26679;&#24615;&#21644;&#24418;&#24577;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;BFS&#26356;&#21152;&#26377;&#25928;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Evolutionary robotics offers a powerful framework for designing and evolving robot morphologies, particularly in the context of modular robots. However, the role of query mechanisms during the genotype-to-phenotype mapping process has been largely overlooked. This research addresses this gap by conducting a comparative analysis of query mechanisms in the brain-body co-evolution of modular robots. Using two different query mechanisms, Breadth-First Search (BFS) and Random Query, within the context of evolving robot morphologies using CPPNs and robot controllers using tensors, and testing them in two evolutionary frameworks, Lamarckian and Darwinian systems, this study investigates their influence on evolutionary outcomes and performance. The findings demonstrate the impact of the two query mechanisms on the evolution and performance of modular robot bodies, including morphological intelligence, diversity, and morphological traits. This study suggests that BFS is both more effective and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#26679;-&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;-&#38598;&#25104;&#24322;&#24120;&#26816;&#27979;&#65288;SVEAD&#65289;&#26694;&#26550;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38598;&#25104;&#22534;&#21472;&#12289;VAE&#21644;SHAP&#30340;&#32467;&#21512;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.14385</link><description>&lt;p&gt;
&#37319;&#26679; - &#21464;&#20998;&#33258;&#32534;&#30721;&#22120; - &#38598;&#25104;&#65306;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#25506;&#32034;&#20013;
&lt;/p&gt;
&lt;p&gt;
Sampling - Variational Auto Encoder - Ensemble: In the Quest of Explainable Artificial Intelligence. (arXiv:2309.14385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#26679;-&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;-&#38598;&#25104;&#24322;&#24120;&#26816;&#27979;&#65288;SVEAD&#65289;&#26694;&#26550;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38598;&#25104;&#22534;&#21472;&#12289;VAE&#21644;SHAP&#30340;&#32467;&#21512;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27169;&#22411;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#24191;&#27867;&#20852;&#36259;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#20173;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#26041;&#27861;&#25110;&#36884;&#24452;&#26469;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#19968;&#20010;&#31995;&#32479;&#21644;&#26377;&#20957;&#32858;&#21147;&#30340;&#26694;&#26550;&#24050;&#32463;&#36234;&#26469;&#36234;&#24517;&#35201;&#65292;&#20197;&#25972;&#21512;&#26032;&#30340;&#25216;&#26415;&#65292;&#22914;&#21028;&#21035;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#26032;&#39062;&#26694;&#26550;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#21363;&#37319;&#26679; - &#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289; - &#38598;&#25104;&#24322;&#24120;&#26816;&#27979;&#65288;SVEAD&#65289;&#65292;&#20026;XAI&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#28151;&#21512;&#26550;&#26500;&#65292;&#20854;&#20013;VAE&#19982;&#38598;&#25104;&#22534;&#21472;&#21644;SHapley&#21487;&#21152;&#24615;&#35299;&#37322;&#65288;SHAP&#65289;&#29992;&#20110;&#19981;&#24179;&#34913;&#20998;&#31867;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#22534;&#21472;&#12289;VAE&#21644;SHAP&#30340;&#32467;&#21512;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#19988;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#36824;&#32467;&#21512;&#25490;&#21015;&#37325;&#35201;&#24615;&#21644;&#20869;&#37096;&#19982;&#22806;&#37096;&#37325;&#35201;&#24615;&#30340;SHAP&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence (XAI) models have recently attracted a great deal of interest from a variety of application sectors. Despite significant developments in this area, there are still no standardized methods or approaches for understanding AI model outputs. A systematic and cohesive framework is also increasingly necessary to incorporate new techniques like discriminative and generative models to close the gap. This paper contributes to the discourse on XAI by presenting an empirical evaluation based on a novel framework: Sampling - Variational Auto Encoder (VAE) - Ensemble Anomaly Detection (SVEAD). It is a hybrid architecture where VAE combined with ensemble stacking and SHapley Additive exPlanations are used for imbalanced classification. The finding reveals that combining ensemble stacking, VAE, and SHAP can. not only lead to better model performance but also provide an easily explainable framework. This work has used SHAP combined with Permutation Importance and In
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#21683;&#22013;&#35786;&#26029;&#21628;&#21560;&#36947;&#30142;&#30149;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36235;&#21183;&#65292;&#36890;&#36807;&#30740;&#31350;&#21683;&#22013;&#29305;&#24449;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#21487;&#20197;&#21487;&#38752;&#20934;&#30830;&#22320;&#26816;&#27979;&#29305;&#23450;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#30340;&#21457;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.14383</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#21683;&#22013;&#35786;&#26029;: &#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards using Cough for Respiratory Disease Diagnosis by leveraging Artificial Intelligence: A Survey. (arXiv:2309.14383v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14383
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#21683;&#22013;&#35786;&#26029;&#21628;&#21560;&#36947;&#30142;&#30149;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36235;&#21183;&#65292;&#36890;&#36807;&#30740;&#31350;&#21683;&#22013;&#29305;&#24449;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#21487;&#20197;&#21487;&#38752;&#20934;&#30830;&#22320;&#26816;&#27979;&#29305;&#23450;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#30340;&#21457;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21683;&#22013;&#22768;&#38899;&#21253;&#21547;&#30528;&#21628;&#21560;&#31995;&#32479;&#30149;&#29702;&#24418;&#24577;&#23398;&#25913;&#21464;&#30340;&#20247;&#22810;&#37325;&#35201;&#20449;&#24687;&#12290;&#36890;&#36807;&#30740;&#31350;&#28508;&#22312;&#30340;&#21683;&#22013;&#29305;&#24449;&#20197;&#21450;&#30142;&#30149;&#35786;&#26029;&#65292;&#21487;&#21487;&#38752;&#20934;&#30830;&#22320;&#26816;&#27979;&#21683;&#22013;&#20107;&#20214;&#22312;&#24674;&#22797;&#21307;&#30103;&#23454;&#36341;&#20013;&#21457;&#25381;&#19981;&#21487;&#25110;&#32570;&#30340;&#20316;&#29992;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36817;&#26399;&#24212;&#29992;&#21644;&#27867;&#22312;&#35745;&#31639;&#30340;&#36827;&#27493;&#20026;&#21628;&#21560;&#36947;&#30142;&#30149;&#39044;&#27979;&#24320;&#21019;&#20102;&#26377;&#21033;&#30340;&#36235;&#21183;&#21644;&#26080;&#25968;&#26410;&#26469;&#21487;&#33021;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#21683;&#22013;&#35786;&#26029;&#31639;&#27861;&#30340;&#36805;&#36895;&#20986;&#29616;&#24050;&#32463;&#24320;&#22987;&#21033;&#29992;&#21683;&#22013;&#29305;&#24449;&#12290;&#22823;&#37327;&#20851;&#20110;&#22522;&#20110;&#21683;&#22013;&#30340;AI&#31639;&#27861;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#26816;&#27979;&#29305;&#23450;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#30340;&#21457;&#20316;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#21307;&#30103;&#19987;&#23478;&#26469;&#35828;&#65292;&#20197;&#35814;&#23613;&#30340;&#26041;&#24335;&#25910;&#38598;&#25152;&#26377;&#30456;&#20851;&#30740;&#31350;&#30340;&#20449;&#24687;&#26159;&#38750;&#24120;&#20851;&#38190;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cough acoustics contain multitudes of vital information about pathomorphological alterations in the respiratory system. Reliable and accurate detection of cough events by investigating the underlying cough latent features and disease diagnosis can play an indispensable role in revitalizing the healthcare practices. The recent application of Artificial Intelligence (AI) and advances of ubiquitous computing for respiratory disease prediction has created an auspicious trend and myriad of future possibilities in the medical domain. In particular, there is an expeditiously emerging trend of Machine learning (ML) and Deep Learning (DL)-based diagnostic algorithms exploiting cough signatures. The enormous body of literature on cough-based AI algorithms demonstrate that these models can play a significant role for detecting the onset of a specific respiratory disease. However, it is pertinent to collect the information from all relevant studies in an exhaustive manner for the medical experts a
&lt;/p&gt;</description></item><item><title>"&#21516;&#24847;&#19981;&#21516;&#24847;"&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#33258;&#21160;&#35299;&#26512;&#21644;&#24635;&#32467;&#20351;&#29992;&#25143;&#20415;&#20110;&#29702;&#35299;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#22312;&#25215;&#35834;&#21327;&#35758;&#20043;&#21069;&#32771;&#34385;&#37325;&#35201;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2309.14382</link><description>&lt;p&gt;
&#21516;&#24847;&#19981;&#21516;&#24847;
&lt;/p&gt;
&lt;p&gt;
Agree To Disagree. (arXiv:2309.14382v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14382
&lt;/p&gt;
&lt;p&gt;
"&#21516;&#24847;&#19981;&#21516;&#24847;"&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#33258;&#21160;&#35299;&#26512;&#21644;&#24635;&#32467;&#20351;&#29992;&#25143;&#20415;&#20110;&#29702;&#35299;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#22312;&#25215;&#35834;&#21327;&#35758;&#20043;&#21069;&#32771;&#34385;&#37325;&#35201;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27880;&#20876;&#26381;&#21153;&#12289;&#23433;&#35013;&#36719;&#20214;&#25110;&#35775;&#38382;&#32593;&#31449;&#20043;&#21069;&#65292;&#20010;&#20154;&#26377;&#22810;&#39057;&#32321;&#22320;&#20180;&#32454;&#23457;&#26597;&#26465;&#27454;&#21644;&#26465;&#20214;&#65311;&#22823;&#22810;&#25968;&#20114;&#32852;&#32593;&#29992;&#25143;&#24182;&#19981;&#21442;&#19982;&#36825;&#31181;&#20570;&#27861;&#12290;&#37492;&#20110;&#26465;&#27454;&#21644;&#26465;&#20214;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#22797;&#26434;&#30340;&#27861;&#24459;&#26415;&#35821;&#21644;&#26214;&#28073;&#38590;&#25026;&#30340;&#21477;&#23376;&#65292;&#36825;&#31181;&#36235;&#21183;&#24182;&#19981;&#20196;&#20154;&#24847;&#22806;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20197;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#33258;&#21160;&#35299;&#26512;&#21644;&#24635;&#32467;&#20851;&#38190;&#20449;&#24687;&#12290;&#36825;&#39033;&#25216;&#26415;&#19987;&#27880;&#20110;&#25552;&#21462;&#29992;&#25143;&#22312;&#25215;&#35834;&#21327;&#35758;&#20043;&#21069;&#24212;&#32771;&#34385;&#30340;&#30456;&#20851;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
How frequently do individuals thoroughly review terms and conditions before proceeding to register for a service, install software, or access a website? The majority of internet users do not engage in this practice. This trend is not surprising, given that terms and conditions typically consist of lengthy documents replete with intricate legal terminology and convoluted sentences. In this paper, we introduce a Machine Learning-powered approach designed to automatically parse and summarize critical information in a user-friendly manner. This technology focuses on distilling the pertinent details that users should contemplate before committing to an agreement.
&lt;/p&gt;</description></item><item><title>&#31038;&#20132;&#20559;&#35265;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35843;&#26597;&#65292;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#23545;&#28508;&#22312;&#31038;&#20250;&#20559;&#35265;&#30340;&#29702;&#35299;&#65292;&#20197;&#35299;&#20915;&#36164;&#28304;&#20998;&#37197;&#19981;&#22343;&#21644;&#19981;&#20844;&#24179;&#20195;&#34920;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14381</link><description>&lt;p&gt;
&#31038;&#20132;&#20559;&#35265;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey of Social Bias in Vision-Language Models. (arXiv:2309.14381v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14381
&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#20559;&#35265;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35843;&#26597;&#65292;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#23545;&#28508;&#22312;&#31038;&#20250;&#20559;&#35265;&#30340;&#29702;&#35299;&#65292;&#20197;&#35299;&#20915;&#36164;&#28304;&#20998;&#37197;&#19981;&#22343;&#21644;&#19981;&#20844;&#24179;&#20195;&#34920;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#24555;&#36895;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#25429;&#25417;&#21644;&#24378;&#21270;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#23548;&#33268;&#36164;&#28304;&#20998;&#37197;&#19981;&#22343;&#21644;&#23545;&#29305;&#23450;&#31038;&#20250;&#32676;&#20307;&#30340;&#19981;&#20844;&#24179;&#20195;&#34920;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#35299;&#20915;&#36825;&#20123;&#20559;&#35265;&#24182;&#30830;&#20445;&#20844;&#24179;&#24615;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#20851;&#38190;&#20851;&#20999;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#39046;&#22495;&#20013;&#38656;&#35201;&#20851;&#27880;&#36825;&#20123;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#28508;&#22312;&#31038;&#20250;&#20559;&#35265;&#12290;&#34429;&#28982;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#31038;&#20250;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20294;&#23545;&#20110;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20559;&#35265;&#30456;&#27604;&#65292;&#20154;&#20204;&#23545;&#20854;&#20102;&#35299;&#26377;&#38480;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#32508;&#36848;&#21644;&#36164;&#28304;&#65292;&#20197;&#22686;&#36827;&#23545;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#20559;&#35265;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the rapid advancement of machine learning (ML) models, particularly transformer-based pre-trained models, has revolutionized Natural Language Processing (NLP) and Computer Vision (CV) fields. However, researchers have discovered that these models can inadvertently capture and reinforce social biases present in their training datasets, leading to potential social harms, such as uneven resource allocation and unfair representation of specific social groups. Addressing these biases and ensuring fairness in artificial intelligence (AI) systems has become a critical concern in the ML community.  The recent introduction of pre-trained vision-and-language (VL) models in the emerging multimodal field demands attention to the potential social biases present in these models as well. Although VL models are susceptible to social bias, there is a limited understanding compared to the extensive discussions on bias in NLP and CV. This survey aims to provide researchers with a high-le
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#36741;&#21161;&#30340;&#28151;&#21512;&#26041;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#25991;&#31038;&#31185;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;16&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#28085;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#35328;&#20998;&#26512;&#12289;&#25991;&#26412;&#25366;&#25496;&#12289;&#31038;&#20132;&#32593;&#32476;&#25512;&#26029;&#31561;&#12290;</title><link>http://arxiv.org/abs/2309.14379</link><description>&lt;p&gt;
&#26426;&#22120;&#36741;&#21161;&#30340;&#28151;&#21512;&#26041;&#27861;&#65306;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#20154;&#25991;&#31038;&#31185;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence. (arXiv:2309.14379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#36741;&#21161;&#30340;&#28151;&#21512;&#26041;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#25991;&#31038;&#31185;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;16&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#28085;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#35328;&#20998;&#26512;&#12289;&#25991;&#26412;&#25366;&#25496;&#12289;&#31038;&#20132;&#32593;&#32476;&#25512;&#26029;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19981;&#26029;&#36827;&#21270;&#20026;&#20154;&#25991;&#31038;&#31185;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#33021;&#22815;&#22312;&#20197;&#21069;&#36890;&#24120;&#30001;&#20154;&#21147;&#23436;&#25104;&#30340;&#23450;&#24615;&#20998;&#26512;&#20219;&#21153;&#20013;&#23454;&#29616;&#35268;&#27169;&#21270;&#12289;&#33258;&#21160;&#21270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#28151;&#21512;&#26041;&#27861;&#26694;&#26550;&#65292;&#20197;&#21033;&#29992;&#23450;&#24615;&#20998;&#26512;&#19987;&#19994;&#30693;&#35782;&#12289;&#26426;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20005;&#35880;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#27880;&#37325;&#36879;&#26126;&#24230;&#21644;&#21487;&#22797;&#21046;&#24615;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;16&#20010;&#26426;&#22120;&#36741;&#21161;&#30340;&#26696;&#20363;&#30740;&#31350;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#12290;&#20219;&#21153;&#21253;&#25324;&#35821;&#35328;&#21644;&#35805;&#35821;&#20998;&#26512;&#12289;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#12289;&#37319;&#35775;&#20998;&#26512;&#12289;&#21382;&#21490;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#21644;&#25991;&#26412;&#25366;&#25496;&#12289;&#25919;&#27835;&#31435;&#22330;&#26816;&#27979;&#12289;&#25991;&#26412;&#21644;&#24605;&#24819;&#37325;&#22797;&#20351;&#29992;&#12289;&#25991;&#23398;&#21644;&#30005;&#24433;&#20013;&#30340;&#25991;&#31867;&#26500;&#25104;&#12289;&#31038;&#20132;&#32593;&#32476;&#25512;&#26029;&#12289;&#33258;&#21160;&#35789;&#20856;&#32534;&#32386;&#12289;&#20803;&#25968;&#25454;&#34917;&#20805;&#21644;&#22810;&#27169;&#24577;&#35270;&#35273;&#25991;&#21270;&#20998;&#26512;&#12290;&#19982;&#29616;&#26377;LLM&#24212;&#29992;&#25991;&#29486;&#20013;&#23545;&#33521;&#25991;&#30340;&#20851;&#27880;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#28085;&#30422;&#22810;&#31181;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing capacities of large language models (LLMs) present an unprecedented opportunity to scale up data analytics in the humanities and social sciences, augmenting and automating qualitative analytic tasks previously typically allocated to human labor. This contribution proposes a systematic mixed methods framework to harness qualitative analytic expertise, machine scalability, and rigorous quantification, with attention to transparency and replicability. 16 machine-assisted case studies are showcased as proof of concept. Tasks include linguistic and discourse analysis, lexical semantic change detection, interview analysis, historical event cause inference and text mining, detection of political stance, text and idea reuse, genre composition in literature and film; social network inference, automated lexicography, missing metadata augmentation, and multimodal visual cultural analytics. In contrast to the focus on English in the emerging LLM applicability literature, many exampl
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#35780;&#20272;&#21644;&#22686;&#24378;&#24314;&#31569;&#27861;&#35268;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#12290;&#30740;&#31350;&#32771;&#34385;&#20102;&#26465;&#27454;&#21644;&#25991;&#26723;&#23618;&#38754;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#20960;&#20010;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14374</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#24314;&#31569;&#27861;&#35268;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Text Classification-Based Approach for Evaluating and Enhancing the Machine Interpretability of Building Codes. (arXiv:2309.14374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#35780;&#20272;&#21644;&#22686;&#24378;&#24314;&#31569;&#27861;&#35268;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#12290;&#30740;&#31350;&#32771;&#34385;&#20102;&#26465;&#27454;&#21644;&#25991;&#26723;&#23618;&#38754;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#20960;&#20010;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#30417;&#31649;&#25991;&#20214;&#25110;&#24314;&#31569;&#27861;&#35268;&#35299;&#37322;&#20026;&#21487;&#35745;&#31639;&#26426;&#22788;&#29702;&#30340;&#26684;&#24335;&#23545;&#20110;&#26234;&#33021;&#35774;&#35745;&#21644;&#24314;&#36896;&#24314;&#31569;&#21644;&#22522;&#30784;&#35774;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#33258;&#21160;&#21270;&#35268;&#21017;&#35299;&#37322;&#65288;ARI&#65289;&#26041;&#27861;&#24050;&#32463;&#30740;&#31350;&#22810;&#24180;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#24314;&#31569;&#27861;&#35268;&#20013;&#26089;&#26399;&#21644;&#25163;&#21160;&#31579;&#36873;&#21487;&#35299;&#37322;&#26465;&#27454;&#12290; &#34429;&#28982;&#20854;&#20013;&#23569;&#25968;&#26041;&#27861;&#32771;&#34385;&#20102;&#20174;&#26465;&#27454;&#21644;&#25991;&#26723;&#23618;&#38754;&#19978;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#36825;&#20195;&#34920;&#20102;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#35745;&#31639;&#26426;&#22788;&#29702;&#26684;&#24335;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#35780;&#20272;&#21644;&#22686;&#24378;&#21333;&#20010;&#26465;&#27454;&#21644;&#24314;&#31569;&#27861;&#35268;&#30340;&#26426;&#22120;&#21487;&#35299;&#37322;&#24615;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#20960;&#20010;&#31867;&#21035;&#65292;&#20197;&#32771;&#34385;&#23545;&#35268;&#21017;&#35299;&#37322;&#30340;&#35201;&#27714;&#23545;&#27599;&#20010;&#24314;&#31569;&#27861;&#35268;&#26465;&#27454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#30340;&#39640;&#25928;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting regulatory documents or building codes into computer-processable formats is essential for the intelligent design and construction of buildings and infrastructures. Although automated rule interpretation (ARI) methods have been investigated for years, most of them highly depend on the early and manual filtering of interpretable clauses from a building code. While few of them considered machine interpretability, which represents the potential to be transformed into a computer-processable format, from both clause- and document-level. Therefore, this research aims to propose a novel approach to automatically evaluate and enhance the machine interpretability of single clause and building codes. First, a few categories are introduced to classify each clause in a building code considering the requirements for rule interpretation, and a dataset is developed for model training. Then, an efficient text classification model is developed based on a pretrained domain-specific language 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#36716;&#24405;&#25968;&#25454;&#65292;&#36890;&#36807;&#22312;&#26631;&#27880;&#38454;&#27573;&#36827;&#34892;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#37325;&#26032;&#22788;&#29702;&#21644;&#22312;&#26631;&#27880;&#21518;&#38454;&#27573;&#36827;&#34892;&#33258;&#21160;&#35789;&#38169;&#35823;&#20462;&#27491;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#36716;&#24405;&#35789;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#24182;&#21457;&#29616;&#20102;&#36716;&#24405;&#38169;&#35823;&#23545;ASR&#27169;&#22411;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14372</link><description>&lt;p&gt;
&#20154;&#31867;&#36716;&#24405;&#36136;&#37327;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Human Transcription Quality Improvement. (arXiv:2309.14372v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#36716;&#24405;&#25968;&#25454;&#65292;&#36890;&#36807;&#22312;&#26631;&#27880;&#38454;&#27573;&#36827;&#34892;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#37325;&#26032;&#22788;&#29702;&#21644;&#22312;&#26631;&#27880;&#21518;&#38454;&#27573;&#36827;&#34892;&#33258;&#21160;&#35789;&#38169;&#35823;&#20462;&#27491;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#36716;&#24405;&#35789;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#24182;&#21457;&#29616;&#20102;&#36716;&#24405;&#38169;&#35823;&#23545;ASR&#27169;&#22411;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#36716;&#24405;&#25968;&#25454;&#23545;&#20110;&#35757;&#32451;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34892;&#19994;&#32423;&#25968;&#25454;&#25910;&#38598;&#31649;&#36947;&#23545;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#25104;&#26412;&#39640;&#26114;&#65292;&#32780;&#20247;&#21253;&#36716;&#24405;&#30340;&#36136;&#37327;&#36739;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#25910;&#38598;&#35821;&#38899;&#36716;&#24405;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26426;&#21046;&#26469;&#25913;&#21892;&#36716;&#24405;&#36136;&#37327;&#65306;&#22312;&#26631;&#27880;&#38454;&#27573;&#22522;&#20110;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#37325;&#26032;&#22788;&#29702;&#21644;&#22312;&#26631;&#27880;&#21518;&#38454;&#27573;&#30340;&#33258;&#21160;&#35789;&#38169;&#35823;&#20462;&#27491;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;LibriCrowd - &#19968;&#20010;&#21253;&#21547;100&#23567;&#26102;&#33521;&#35821;&#35821;&#38899;&#36716;&#24405;&#30340;&#22823;&#35268;&#27169;&#20247;&#21253;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36716;&#24405;&#35789;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#20102;50%&#20197;&#19978;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#36716;&#24405;&#38169;&#35823;&#23545;ASR&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#24378;&#30456;&#20851;&#24615;&#12290;&#36716;&#24405;&#36136;&#37327;&#30340;&#25552;&#21319;&#20351;ASR&#27169;&#22411;&#30340;WER&#30456;&#23545;&#20943;&#23569;&#20102;10%&#20197;&#19978;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#65292;&#20197;&#36896;&#31119;&#30740;&#31350;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
High quality transcription data is crucial for training automatic speech recognition (ASR) systems. However, the existing industry-level data collection pipelines are expensive to researchers, while the quality of crowdsourced transcription is low. In this paper, we propose a reliable method to collect speech transcriptions. We introduce two mechanisms to improve transcription quality: confidence estimation based reprocessing at labeling stage, and automatic word error correction at post-labeling stage. We collect and release LibriCrowd - a large-scale crowdsourced dataset of audio transcriptions on 100 hours of English speech. Experiment shows the Transcription WER is reduced by over 50%. We further investigate the impact of transcription error on ASR model performance and found a strong correlation. The transcription quality improvement provides over 10% relative WER reduction for ASR models. We release the dataset and code to benefit the research community.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20301;&#26435;&#37325;&#30340;&#39640;&#25928;&#37327;&#23376;&#24863;&#30693;&#22120;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#38750;&#29702;&#24819;&#35757;&#32451;&#38598;&#38382;&#39064;&#24182;&#23454;&#29616;&#19968;&#27425;&#36845;&#20195;&#23398;&#20064;&#12290;&#31639;&#27861;&#36890;&#36807;&#35745;&#31639;&#35757;&#32451;&#38598;&#20013;&#24635;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#20351;&#26435;&#37325;&#30697;&#38453;&#25104;&#20026;&#21333;&#20301;&#38453;&#65292;&#24182;&#33021;&#22815;&#31934;&#30830;&#23454;&#29616;&#20219;&#24847;&#37327;&#23376;&#38376;&#12290;&#19982;&#20854;&#20182;&#37327;&#23376;&#24863;&#30693;&#22120;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36866;&#29992;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#65292;&#28436;&#31034;&#20102;&#30001;&#22810;&#20010;&#22522;&#26412;&#37327;&#23376;&#38376;&#26500;&#25104;&#30340;&#37327;&#23376;&#22797;&#21512;&#38376;&#12290;</title><link>http://arxiv.org/abs/2309.14366</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#20301;&#26435;&#37325;&#30340;&#21333;&#27425;&#36845;&#20195;&#37327;&#23376;&#24863;&#30693;&#22120;&#31639;&#27861;&#29992;&#20110;&#38750;&#29702;&#24819;&#35757;&#32451;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Unitary Weights Based One-Iteration Quantum Perceptron Algorithm for Non-Ideal Training Sets. (arXiv:2309.14366v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14366
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20301;&#26435;&#37325;&#30340;&#39640;&#25928;&#37327;&#23376;&#24863;&#30693;&#22120;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#38750;&#29702;&#24819;&#35757;&#32451;&#38598;&#38382;&#39064;&#24182;&#23454;&#29616;&#19968;&#27425;&#36845;&#20195;&#23398;&#20064;&#12290;&#31639;&#27861;&#36890;&#36807;&#35745;&#31639;&#35757;&#32451;&#38598;&#20013;&#24635;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#20351;&#26435;&#37325;&#30697;&#38453;&#25104;&#20026;&#21333;&#20301;&#38453;&#65292;&#24182;&#33021;&#22815;&#31934;&#30830;&#23454;&#29616;&#20219;&#24847;&#37327;&#23376;&#38376;&#12290;&#19982;&#20854;&#20182;&#37327;&#23376;&#24863;&#30693;&#22120;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36866;&#29992;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#65292;&#28436;&#31034;&#20102;&#30001;&#22810;&#20010;&#22522;&#26412;&#37327;&#23376;&#38376;&#26500;&#25104;&#30340;&#37327;&#23376;&#22797;&#21512;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#38750;&#29702;&#24819;&#35757;&#32451;&#38598;&#65288;&#21363;&#19981;&#23436;&#25972;&#25110;&#36807;&#23436;&#22791;&#38598;&#65289;&#38382;&#39064;&#24182;&#23454;&#29616;&#19968;&#27425;&#36845;&#20195;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20301;&#26435;&#37325;&#30340;&#39640;&#25928;&#37327;&#23376;&#24863;&#30693;&#22120;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#35757;&#32451;&#38598;&#20013;&#24635;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#20351;&#26435;&#37325;&#30697;&#38453;&#25104;&#20026;&#21333;&#20301;&#38453;&#12290;&#23545;&#37327;&#23376;&#38376;{H&#65292;S&#65292;T&#65292;CNOT&#65292;Toffoli&#65292;Fredkin}&#30340;&#31034;&#20363;&#39564;&#35777;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#22312;&#19968;&#27425;&#36845;&#20195;&#20013;&#20934;&#30830;&#23454;&#29616;&#20219;&#24847;&#37327;&#23376;&#38376;&#12290;&#36890;&#36807;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#20854;&#20182;&#37327;&#23376;&#24863;&#30693;&#22120;&#31639;&#27861;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#22312;&#36866;&#29992;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#29992;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#25105;&#20204;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#36824;&#28436;&#31034;&#20102;&#30001;&#20960;&#20010;&#22522;&#26412;&#37327;&#23376;&#38376;&#26500;&#25104;&#30340;&#37327;&#23376;&#22797;&#21512;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to solve the problem of non-ideal training sets (i.e., the less-complete or over-complete sets) and implement one-iteration learning, a novel efficient quantum perceptron algorithm based on unitary weights is proposed, where the singular value decomposition of the total weight matrix from the training set is calculated to make the weight matrix to be unitary. The example validation of quantum gates {H, S, T, CNOT, Toffoli, Fredkin} shows that our algorithm can accurately implement arbitrary quantum gates within one iteration. The performance comparison between our algorithm and other quantum perceptron algorithms demonstrates the advantages of our algorithm in terms of applicability, accuracy, and availability. For further validating the applicability of our algorithm, a quantum composite gate which consists of several basic quantum gates is also illustrated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#20195;&#29702;&#19982;&#20256;&#32479;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#26680;&#24515;&#24046;&#24322;&#21644;&#29305;&#28857;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35760;&#24518;&#20998;&#31867;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#20840;&#26032;&#30340;&#35774;&#35745;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2309.14365</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#28145;&#24230;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An In-depth Survey of Large Language Model-based Artificial Intelligence Agents. (arXiv:2309.14365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#20195;&#29702;&#19982;&#20256;&#32479;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#26680;&#24515;&#24046;&#24322;&#21644;&#29305;&#28857;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35760;&#24518;&#20998;&#31867;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#20840;&#26032;&#30340;&#35774;&#35745;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#26368;&#36817;&#20154;&#20204;&#19968;&#30452;&#22312;&#21162;&#21147;&#23558;&#23427;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLM-based AI&#20195;&#29702;&#19982;&#20256;&#32479;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#26680;&#24515;&#24046;&#24322;&#21644;&#29305;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#31867;&#22411;&#20195;&#29702;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#38416;&#26126;&#20102;LLM-based&#20195;&#29702;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#12289;&#30693;&#35782;&#23384;&#20648;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;AI&#20195;&#29702;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#35268;&#21010;&#12289;&#35760;&#24518;&#21644;&#24037;&#20855;&#20351;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#23588;&#20854;&#26159;&#23545;&#20110;&#20851;&#38190;&#30340;&#35760;&#24518;&#32452;&#20214;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#26041;&#26696;&#65292;&#19981;&#20165;&#36828;&#31163;&#20102;&#20256;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#32780;&#19988;&#20026;AI&#20195;&#29702;&#30340;&#35760;&#24518;&#31995;&#32479;&#35774;&#35745;&#25552;&#20379;&#20102;&#20840;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#22362;&#20449;&#23545;&#36825;&#20123;&#26680;&#24515;&#32452;&#20214;&#36827;&#34892;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Due to the powerful capabilities demonstrated by large language model (LLM), there has been a recent surge in efforts to integrate them with AI agents to enhance their performance. In this paper, we have explored the core differences and characteristics between LLM-based AI agents and traditional AI agents. Specifically, we first compare the fundamental characteristics of these two types of agents, clarifying the significant advantages of LLM-based agents in handling natural language, knowledge storage, and reasoning capabilities. Subsequently, we conducted an in-depth analysis of the key components of AI agents, including planning, memory, and tool use. Particularly, for the crucial component of memory, this paper introduced an innovative classification scheme, not only departing from traditional classification methods but also providing a fresh perspective on the design of an AI agent's memory system. We firmly believe that in-depth research and understanding of these core components
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22810;&#26679;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22806;&#37096;&#33258;&#28982;&#38382;&#39064;&#22312;&#30693;&#35782;&#24211;&#19978;&#36827;&#34892;&#22810;&#26679;&#21270;&#38382;&#39064;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#27169;&#22411;&#26694;&#26550;&#26469;&#35299;&#20915;&#22914;&#20309;&#22686;&#24378;&#22810;&#26679;&#21270;&#38382;&#39064;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.14362</link><description>&lt;p&gt;
&#36890;&#36807;&#22806;&#37096;&#33258;&#28982;&#38382;&#39064;&#22312;&#30693;&#35782;&#24211;&#19978;&#36827;&#34892;&#22810;&#26679;&#21270;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diversifying Question Generation over Knowledge Base via External Natural Questions. (arXiv:2309.14362v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14362
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22810;&#26679;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22806;&#37096;&#33258;&#28982;&#38382;&#39064;&#22312;&#30693;&#35782;&#24211;&#19978;&#36827;&#34892;&#22810;&#26679;&#21270;&#38382;&#39064;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#27169;&#22411;&#26694;&#26550;&#26469;&#35299;&#20915;&#22914;&#20309;&#22686;&#24378;&#22810;&#26679;&#21270;&#38382;&#39064;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#21333;&#20010;&#29983;&#25104;&#38382;&#39064;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20154;&#31867;&#20986;&#33394;&#30340;&#25913;&#20889;&#33021;&#21147;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#20041;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#34920;&#36798;&#26469;&#20256;&#36798;&#12290;&#20197;&#19978;&#35266;&#28857;&#20351;&#24471;&#22810;&#26679;&#21270;&#38382;&#39064;&#29983;&#25104;&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;&#25361;&#25112;&#26159;&#22810;&#26679;&#24615;&#35780;&#20272;&#25351;&#26631;&#12290;&#24403;&#21069;&#30340;&#25351;&#26631;&#19981;&#36275;&#20197;&#35780;&#20272;&#22810;&#26679;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#35745;&#31639;&#29983;&#25104;&#38382;&#39064;&#20013;&#21807;&#19968;n-gram&#30340;&#27604;&#20363;&#65292;&#26356;&#20542;&#21521;&#20110;&#34913;&#37327;&#37325;&#22797;&#32780;&#38750;&#30495;&#27491;&#30340;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26679;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#34913;&#37327;&#27599;&#20010;&#23454;&#20363;&#30340;&#21069;k&#20010;&#29983;&#25104;&#38382;&#39064;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#23427;&#20204;&#19982;&#22522;&#20934;&#38382;&#39064;&#30456;&#20851;&#12290;&#26174;&#28982;&#65292;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#22914;&#20309;&#22686;&#24378;&#22810;&#26679;&#21270;&#38382;&#39064;&#29983;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;&#20004;&#20010;&#36873;&#25321;&#27169;&#22411;&#20132;&#32455;&#32780;&#25104;&#30340;&#21452;&#27169;&#22411;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous methods on knowledge base question generation (KBQG) primarily focus on enhancing the quality of a single generated question. Recognizing the remarkable paraphrasing ability of humans, we contend that diverse texts should convey the same semantics through varied expressions. The above insights make diversifying question generation an intriguing task, where the first challenge is evaluation metrics for diversity. Current metrics inadequately assess the above diversity since they calculate the ratio of unique n-grams in the generated question itself, which leans more towards measuring duplication rather than true diversity. Accordingly, we devise a new diversity evaluation metric, which measures the diversity among top-k generated questions for each instance while ensuring their relevance to the ground truth. Clearly, the second challenge is how to enhance diversifying question generation. To address this challenge, we introduce a dual model framework interwoven by two selection
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#39318;&#27425;&#23545;&#20855;&#26377;&#21487;&#21464;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#20250;&#32422;&#26463;&#23376;&#27169;&#38382;&#39064;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#36138;&#23146;&#31639;&#27861;&#65292;&#21487;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.14359</link><description>&lt;p&gt;
&#20248;&#21270;&#20855;&#26377;&#21487;&#21464;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#20250;&#32422;&#26463;&#23376;&#27169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Optimizing Chance-Constrained Submodular Problems with Variable Uncertainties. (arXiv:2309.14359v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#39318;&#27425;&#23545;&#20855;&#26377;&#21487;&#21464;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#20250;&#32422;&#26463;&#23376;&#27169;&#38382;&#39064;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#36138;&#23146;&#31639;&#27861;&#65292;&#21487;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#20250;&#32422;&#26463;&#32463;&#24120;&#29992;&#20110;&#38480;&#21046;&#32422;&#26463;&#22312;&#28041;&#21450;&#38543;&#26426;&#32452;&#20214;&#30340;&#29616;&#23454;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#36829;&#21453;&#27010;&#29575;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#32422;&#26463;&#30340;&#23376;&#27169;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#31181;&#38382;&#39064;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#38543;&#26426;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#32771;&#34385;&#20102;&#20855;&#26377;&#38543;&#26426;&#32972;&#21253;&#32422;&#26463;&#30340;&#23376;&#27169;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#21487;&#36873;&#25321;&#30340;&#39033;&#30446;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#30456;&#21516;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#36890;&#24120;&#19982;&#19981;&#21516;&#30340;&#38543;&#26426;&#32452;&#20214;&#26377;&#20851;&#65292;&#23545;&#20110;&#36825;&#31181;&#35774;&#32622;&#32570;&#20047;&#20005;&#26684;&#30340;&#20998;&#26512;&#22312;&#23376;&#27169;&#20248;&#21270;&#30340;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#20102;&#39318;&#20010;&#36825;&#26679;&#30340;&#20998;&#26512;&#65306;&#39033;&#30446;&#30340;&#26435;&#37325;&#20855;&#26377;&#30456;&#21516;&#30340;&#26399;&#26395;&#20294;&#19981;&#21516;&#30340;&#31163;&#25955;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36138;&#23146;&#31639;&#27861;&#65292;&#21487;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23545;&#32473;&#23450;&#26368;&#20248;&#35299;&#30340;&#24658;&#23450;&#36924;&#36817;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chance constraints are frequently used to limit the probability of constraint violations in real-world optimization problems where the constraints involve stochastic components. We study chance-constrained submodular optimization problems, which capture a wide range of optimization problems with stochastic constraints. Previous studies considered submodular problems with stochastic knapsack constraints in the case where uncertainties are the same for each item that can be selected. However, uncertainty levels are usually variable with respect to the different stochastic components in real-world scenarios, and rigorous analysis for this setting is missing in the context of submodular optimization. This paper provides the first such analysis for this case, where the weights of items have the same expectation but different dispersion. We present greedy algorithms that can obtain a high-quality solution, i.e., a constant approximation ratio to the given optimal solution from the determinis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#20225;&#19994;&#20449;&#29992;&#35780;&#32423;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;CCR&#30340;&#21457;&#23637;&#32972;&#26223;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#23637;&#26395;&#20102;CCR&#30340;&#26410;&#26469;&#12290;</title><link>http://arxiv.org/abs/2309.14349</link><description>&lt;p&gt;
&#20225;&#19994;&#20449;&#29992;&#35780;&#32423;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Corporate Credit Rating: A Survey. (arXiv:2309.14349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#20225;&#19994;&#20449;&#29992;&#35780;&#32423;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;CCR&#30340;&#21457;&#23637;&#32972;&#26223;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#23637;&#26395;&#20102;CCR&#30340;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#20449;&#29992;&#35780;&#32423;&#65288;CCR&#65289;&#22312;&#24403;&#20195;&#32463;&#27982;&#21644;&#31038;&#20250;&#21457;&#23637;&#36807;&#31243;&#20013;&#25198;&#28436;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#22914;&#20309;&#20351;&#29992;&#20449;&#29992;&#35780;&#32423;&#26041;&#27861;&#23545;&#20225;&#19994;&#36827;&#34892;&#35780;&#20272;&#19968;&#30452;&#26159;&#19968;&#20010;&#20540;&#24471;&#35752;&#35770;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22269;&#20869;&#22806;&#30456;&#20851;&#25991;&#29486;&#30340;&#38405;&#35835;&#21644;&#30740;&#31350;&#65292;&#26412;&#35770;&#25991;&#23545;CCR&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#35843;&#26597;&#12290;&#26412;&#35770;&#25991;&#20174;&#32479;&#35745;&#27169;&#22411;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#19977;&#20010;&#23618;&#38754;&#25972;&#29702;&#20102;CCR&#26041;&#27861;&#21457;&#23637;&#30340;&#32972;&#26223;&#65292;&#24635;&#32467;&#20102;CCR&#30340;&#24120;&#35265;&#25968;&#25454;&#38598;&#65292;&#24182;&#28145;&#20837;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#24403;&#21069;&#30740;&#31350;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#26395;&#20102;CCR&#30340;&#26410;&#26469;&#12290;&#19982;&#29616;&#26377;&#30340;CCR&#32508;&#36848;&#30456;&#27604;&#65292;&#26412;&#35770;&#25991;&#35814;&#32454;&#38416;&#36848;&#21644;&#20998;&#26512;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#36825;&#19968;&#39046;&#22495;&#36817;&#24180;&#26469;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Corporate credit rating (CCR) plays a very important role in the process of contemporary economic and social development. How to use credit rating methods for enterprises has always been a problem worthy of discussion. Through reading and studying the relevant literature at home and abroad, this paper makes a systematic survey of CCR. This paper combs the context of the development of CCR methods from the three levels: statistical models, machine learning models and neural network models, summarizes the common databases of CCR, and deeply compares the advantages and disadvantages of the models. Finally, this paper summarizes the problems existing in the current research and prospects the future of CCR. Compared with the existing review of CCR, this paper expounds and analyzes the progress of neural network model in this field in recent years.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14348</link><description>&lt;p&gt;
&#36890;&#36807;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#25269;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM. (arXiv:2309.14348v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;LLMs&#21487;&#33021;&#34987;&#28389;&#29992;&#26469;&#29983;&#25104;&#26377;&#23475;&#25110;&#24694;&#24847;&#20869;&#23481;&#12290;&#23613;&#31649;&#26377;&#19968;&#31995;&#21015;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#40784;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#24182;&#38450;&#27490;&#23427;&#20204;&#29983;&#25104;&#19981;&#36866;&#24403;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#23545;&#40784;&#36890;&#24120;&#26159;&#33030;&#24369;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#20248;&#21270;&#25110;&#25163;&#24037;&#26500;&#24314;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#20197;&#38450;&#33539;&#28508;&#22312;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#26500;&#24314;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#65292;&#36890;&#36807;&#20855;&#26377;&#31283;&#20581;&#23545;&#40784;&#26816;&#26597;&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#23545;&#21407;&#22987;LLM&#36827;&#34892;&#20219;&#20309;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;RA-LLM&#22312;&#38450;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#24046;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;&#20061;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#20013;31.45\%&#21040;79.93\%&#30340;&#20195;&#30721;&#20989;&#25968;&#20855;&#26377;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14345</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#20559;&#24046;&#35780;&#20272;&#19982;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Bias Assessment and Mitigation in LLM-based Code Generation. (arXiv:2309.14345v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14345
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#24046;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;&#20061;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#20013;31.45\%&#21040;79.93\%&#30340;&#20195;&#30721;&#20989;&#25968;&#20855;&#26377;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#22312;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#32534;&#30721;&#36807;&#31243;&#30340;&#29983;&#20135;&#21147;&#21644;&#25928;&#29575;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;LLM&#22312;&#36719;&#20214;&#32534;&#30721;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#26222;&#21450;&#65292;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#24050;&#32463;&#20986;&#29616;&#65306;&#29983;&#25104;&#30340;&#20195;&#30721;&#26159;&#21542;&#21253;&#21547;&#19982;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#30456;&#20851;&#30340;&#31038;&#20250;&#20559;&#35265;&#65311;&#36825;&#20010;&#38382;&#39064;&#20851;&#31995;&#21040;&#20381;&#36182;&#20110;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#36719;&#20214;&#24212;&#29992;&#30340;&#23436;&#25972;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#36947;&#24503;&#22522;&#30784;&#65292;&#28982;&#32780;&#22312;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#35774;&#35745;&#30340;&#26032;&#39062;&#20559;&#24046;&#35780;&#20272;&#26694;&#26550;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#23545;&#20061;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#20559;&#24046;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;31.45\%&#21040;79.93\%&#30340;&#20195;&#30721;&#20989;&#25968;&#20855;&#26377;&#20559;&#35265;&#65292;9.68\%&#21040;37.37\%&#30340;&#20195;&#30721;&#20989;&#25968;&#30340;&#21151;&#33021;&#20351;
&lt;/p&gt;
&lt;p&gt;
Utilizing state-of-the-art Large Language Models (LLMs), automatic code generation models play a pivotal role in enhancing the productivity and efficiency of software development coding procedures. As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social biases, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models, yet is under-explored in the literature. This paper presents a novel bias assessment framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive evaluation on the bias of nine state-of-the-art LLM-based code generation models. Our findings reveal that first, 31.45\% to 79.93\% code functions generated by our evaluated code generation models are biased, and 9.68\% to 37.37\% code functions' funct
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;AIGC&#19982;&#25968;&#23383;&#21465;&#20107;&#30340;&#25972;&#21512;&#29366;&#24577;&#65292;&#21457;&#29616;&#34429;&#28982;AIGC&#22312;&#26576;&#20123;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20154;&#31867;&#30340;&#21019;&#36896;&#21147;&#21644;&#23457;&#32654;&#24863;&#31561;&#22240;&#32032;&#65292;&#20173;&#26080;&#27861;&#26367;&#20195;&#20154;&#31867;&#22312;&#22797;&#26434;&#20154;&#29289;&#21160;&#30011;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#38899;&#25928;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.14329</link><description>&lt;p&gt;
AIGC&#30340;&#21019;&#26032;&#25968;&#23383;&#21465;&#20107;&#25506;&#32034;&#19982;&#35752;&#35770;&#65306;&#23545;&#26368;&#26032;&#36827;&#23637;&#30340;&#25506;&#32034;&#19982;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Innovative Digital Storytelling with AIGC: Exploration and Discussion of Recent Advances. (arXiv:2309.14329v1 [cs.HC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;AIGC&#19982;&#25968;&#23383;&#21465;&#20107;&#30340;&#25972;&#21512;&#29366;&#24577;&#65292;&#21457;&#29616;&#34429;&#28982;AIGC&#22312;&#26576;&#20123;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20154;&#31867;&#30340;&#21019;&#36896;&#21147;&#21644;&#23457;&#32654;&#24863;&#31561;&#22240;&#32032;&#65292;&#20173;&#26080;&#27861;&#26367;&#20195;&#20154;&#31867;&#22312;&#22797;&#26434;&#20154;&#29289;&#21160;&#30011;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#38899;&#25928;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21465;&#20107;&#20316;&#20026;&#19968;&#31181;&#33402;&#26415;&#24418;&#24335;&#65292;&#19968;&#30452;&#22312;&#36153;&#29992;&#19982;&#36136;&#37327;&#20043;&#38388;&#25379;&#25166;&#12290;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#20986;&#29616;&#34987;&#35270;&#20026;&#39640;&#25928;&#25968;&#23383;&#21465;&#20107;&#21046;&#20316;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#34701;&#21512;&#30340;&#20855;&#20307;&#24418;&#24335;&#12289;&#25928;&#26524;&#21644;&#24433;&#21709;&#20173;&#19981;&#28165;&#26970;&#65292;&#20351;&#24471;AIGC&#19982;&#21465;&#20107;&#30340;&#36793;&#30028;&#27169;&#31946;&#19981;&#28165;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;AIGC&#19982;&#25968;&#23383;&#21465;&#20107;&#30340;&#24403;&#21069;&#25972;&#21512;&#29366;&#24577;&#65292;&#22312;&#26679;&#26412;&#39033;&#30446;&#20013;&#30740;&#31350;&#20102;&#20004;&#32773;&#34701;&#21512;&#30340;&#33402;&#26415;&#20215;&#20540;&#65292;&#24182;&#36890;&#36807;&#35775;&#35848;&#35299;&#20915;&#24120;&#35265;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;AIGC&#22312;&#22270;&#20687;&#21019;&#20316;&#12289;&#37197;&#38899;&#21046;&#20316;&#21644;&#38899;&#20048;&#21019;&#20316;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20154;&#31867;&#21019;&#36896;&#21147;&#21644;&#23457;&#32654;&#24863;&#30340;&#19981;&#21487;&#26367;&#20195;&#20803;&#32032;&#65292;&#23588;&#20854;&#26159;&#22312;&#22797;&#26434;&#20154;&#29289;&#21160;&#30011;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#38899;&#25928;&#26041;&#38754;&#65292;AIGC&#36824;&#26080;&#27861;&#21462;&#20195;&#20154;&#31867;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#22686;&#24378;&#20844;&#20247;&#23545;&#24403;&#21069;&#29366;&#24577;&#12289;&#38480;&#21046;&#21644;&#25361;&#25112;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital storytelling, as an art form, has struggled with cost-quality balance. The emergence of AI-generated Content (AIGC) is considered as a potential solution for efficient digital storytelling production. However, the specific form, effects, and impacts of this fusion remain unclear, leaving the boundaries of AIGC combined with storytelling undefined. This work explores the current integration state of AIGC and digital storytelling, investigates the artistic value of their fusion in a sample project, and addresses common issues through interviews. Through our study, we conclude that AIGC, while proficient in image creation, voiceover production, and music composition, falls short of replacing humans due to the irreplaceable elements of human creativity and aesthetic sensibilities at present, especially in complex character animations, facial expressions, and sound effects. The research objective is to increase public awareness of the current state, limitations, and challenges arisi
&lt;/p&gt;</description></item><item><title>MoDem-V2&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#30452;&#25509;&#23398;&#20064;&#25509;&#35302;&#20016;&#23500;&#25805;&#20316;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.14236</link><description>&lt;p&gt;
MoDem-V2: &#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#35270;&#35273;-&#36816;&#21160;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation. (arXiv:2309.14236v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14236
&lt;/p&gt;
&lt;p&gt;
MoDem-V2&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#30452;&#25509;&#23398;&#20064;&#25509;&#35302;&#20016;&#23500;&#25805;&#20316;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24076;&#26395;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#24517;&#39035;&#36890;&#36807;&#26426;&#36733;&#20256;&#24863;&#22120;&#30452;&#25509;&#24863;&#30693;&#19990;&#30028;&#12290;&#22522;&#20110;&#35270;&#35273;&#30340;&#23398;&#20064;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#21407;&#22987;&#20687;&#32032;&#30340;&#38544;&#24335;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#65292;&#28040;&#38500;&#29615;&#22659;&#35013;&#32622;&#30340;&#38656;&#27714;&#65292;&#20294;&#20165;&#20165;&#20381;&#38752;&#31232;&#30095;&#30340;&#35270;&#35273;&#22870;&#21169;&#20449;&#21495;&#22312;&#25509;&#35302;&#20016;&#23500;&#30340;&#39640;&#32500;&#25628;&#32034;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#26174;&#33879;&#21152;&#21095;&#20102;&#25506;&#32034;&#30340;&#38590;&#24230;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#31995;&#32479;&#30340;&#36866;&#29992;&#24615;&#36890;&#24120;&#23616;&#38480;&#20110;&#27169;&#25311;&#25110;&#20005;&#26684;&#24037;&#31243;&#21270;&#30340;&#29615;&#22659;&#65292;&#22240;&#20026;&#22312;&#27809;&#26377;&#26126;&#30830;&#30340;&#29366;&#24577;&#20272;&#35745;&#21644;&#31264;&#23494;&#22870;&#21169;&#30340;&#25351;&#23548;&#19979;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#20195;&#29702;&#30340;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#19981;&#23433;&#20840;&#34892;&#20026;&#21644;&#37325;&#22823;&#23433;&#20840;&#25925;&#38556;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#31163;&#20102;&#36825;&#20123;&#38480;&#21046;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;MoDem-V2&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#30452;&#25509;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#23398;&#20064;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#12290;&#22312;&#26368;&#26032;&#30340;&#31639;&#27861;&#36827;&#23637;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;
&lt;/p&gt;
&lt;p&gt;
Robotic systems that aspire to operate in uninstrumented real-world environments must perceive the world directly via onboard sensing. Vision-based learning systems aim to eliminate the need for environment instrumentation by building an implicit understanding of the world based on raw pixels, but navigating the contact-rich high-dimensional search space from solely sparse visual reward signals significantly exacerbates the challenge of exploration. The applicability of such systems is thus typically restricted to simulated or heavily engineered environments since agent exploration in the real-world without the guidance of explicit state estimation and dense rewards can lead to unsafe behavior and safety faults that are catastrophic. In this study, we isolate the root causes behind these limitations to develop a system, called MoDem-V2, capable of learning contact-rich manipulation directly in the uninstrumented real world. Building on the latest algorithmic advancements in model-based
&lt;/p&gt;</description></item><item><title>Species196&#26159;&#19968;&#20010;&#21253;&#21547;196&#20010;&#31867;&#21035;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#29289;&#31181;&#35782;&#21035;&#12290;&#23427;&#25552;&#20379;&#20102;&#22235;&#31181;&#23454;&#39564;&#35774;&#32622;&#65292;&#21487;&#20197;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#29616;&#26377;&#27169;&#22411;&#21644;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14183</link><description>&lt;p&gt;
Species196&#65306;&#19968;&#30334;&#19975;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#29992;&#20110;&#32454;&#31890;&#24230;&#29289;&#31181;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Species196: A One-Million Semi-supervised Dataset for Fine-grained Species Recognition. (arXiv:2309.14183v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14183
&lt;/p&gt;
&lt;p&gt;
Species196&#26159;&#19968;&#20010;&#21253;&#21547;196&#20010;&#31867;&#21035;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#29289;&#31181;&#35782;&#21035;&#12290;&#23427;&#25552;&#20379;&#20102;&#22235;&#31181;&#23454;&#39564;&#35774;&#32622;&#65292;&#21487;&#20197;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#29616;&#26377;&#27169;&#22411;&#21644;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#26222;&#36890;&#35270;&#35273;&#35782;&#21035;&#24050;&#32463;&#36798;&#21040;&#20102;&#19968;&#20010;&#24456;&#39640;&#30340;&#27700;&#24179;&#65292;&#20294;&#26159;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;&#19987;&#38376;&#39046;&#22495;&#20013;&#30340;&#32454;&#31890;&#24230;&#35782;&#21035;&#65292;&#27604;&#22914;&#20837;&#20405;&#29289;&#31181;&#20998;&#31867;&#12290;&#35782;&#21035;&#21644;&#31649;&#29702;&#20837;&#20405;&#29289;&#31181;&#20855;&#26377;&#24456;&#24378;&#30340;&#31038;&#20250;&#21644;&#29983;&#24577;&#20215;&#20540;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#20837;&#20405;&#29289;&#31181;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#26377;&#38480;&#65292;&#35206;&#30422;&#30340;&#29289;&#31181;&#33539;&#22260;&#29421;&#31364;&#65292;&#36825;&#38480;&#21046;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20837;&#20405;&#29983;&#29289;&#35745;&#37327;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Species196&#65292;&#19968;&#20010;&#21253;&#21547;196&#20010;&#31867;&#21035;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#12290;&#23427;&#25910;&#38598;&#20102;&#36229;&#36807;19K&#24102;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#27880;&#37322;&#30340;&#22270;&#20687;Species196-L&#65292;&#20197;&#21450;120&#19975;&#24352;&#26410;&#26631;&#35760;&#30340;&#20837;&#20405;&#29289;&#31181;&#22270;&#20687;Species196-U&#12290;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#22235;&#31181;&#23454;&#39564;&#35774;&#32622;&#65292;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#29616;&#26377;&#30340;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#20998;&#21035;&#26159;&#30417;&#30563;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of foundation vision models has pushed the general visual recognition to a high level, but cannot well address the fine-grained recognition in specialized domain such as invasive species classification. Identifying and managing invasive species has strong social and ecological value. Currently, most invasive species datasets are limited in scale and cover a narrow range of species, which restricts the development of deep-learning based invasion biometrics systems. To fill the gap of this area, we introduced Species196, a large-scale semi-supervised dataset of 196-category invasive species. It collects over 19K images with expert-level accurate annotations Species196-L, and 1.2M unlabeled images of invasive species Species196-U. The dataset provides four experimental settings for benchmarking the existing models and algorithms, namely, supervised learning, semi-supervised learning, self-supervised pretraining and zero-shot inference ability of large multi-modal models. T
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#24211;&#20013;&#39044;&#27979;&#21152;&#25252;&#30149;&#25151;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.13781</link><description>&lt;p&gt;
ICU &#37325;&#26032;&#20837;&#38498;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explainable Machine Learning for ICU Readmission Prediction. (arXiv:2309.13781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#24211;&#20013;&#39044;&#27979;&#21152;&#25252;&#30149;&#25151;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#21307;&#38498;&#29615;&#22659;&#65292;&#21307;&#29983;&#30340;&#20915;&#31574;&#23545;&#24739;&#32773;&#30340;&#29983;&#21629;&#26500;&#25104;&#39640;&#39118;&#38505;&#12290;&#24517;&#39035;&#36981;&#24490;&#19968;&#26465;&#20840;&#38754;&#30340;&#25252;&#29702;&#36335;&#24452;&#26469;&#20943;&#23569;&#24182;&#21457;&#30151;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#12289;&#31454;&#20105;&#24615;&#21644;&#38750;&#35745;&#21010;&#24615;&#30340;&#22240;&#32032;&#22686;&#21152;&#20102;&#32479;&#19968;&#23454;&#26045;&#25252;&#29702;&#36335;&#24452;&#30340;&#22256;&#38590;&#12290;&#20877;&#20837;&#38498;&#26159;&#35813;&#36335;&#24452;&#30340;&#22256;&#38590;&#20043;&#19968;&#65292;&#21363;&#24739;&#32773;&#22312;&#30701;&#26102;&#38388;&#20869;&#20877;&#27425;&#20837;&#20303;ICU&#65292;&#23548;&#33268;&#39640;&#27515;&#20129;&#29575;&#21644;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#24739;&#32773;&#30340;&#21307;&#30103;&#20449;&#24687;&#26469;&#39044;&#27979;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#39044;&#27979;&#20877;&#20837;&#38498;&#26102;&#26377;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#24182;&#26410;&#23545;&#20877;&#20837;&#38498;&#39044;&#27979;&#36827;&#34892;&#36866;&#24403;&#30340;&#35780;&#20272;&#12289;&#25551;&#36848;&#21644;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#24211;&#65288;&#21363;&#21253;&#21547;166,355&#21517;&#24739;&#32773;&#30340;eICU&#38431;&#21015;&#65292;200,859&#21517;...&#65289;
&lt;/p&gt;
&lt;p&gt;
The intensive care unit (ICU) comprises a complex hospital environment, where decisions made by clinicians have a high level of risk for the patients' lives. A comprehensive care pathway must then be followed to reduce p complications. Uncertain, competing and unplanned aspects within this environment increase the difficulty in uniformly implementing the care pathway. Readmission contributes to this pathway's difficulty, occurring when patients are admitted again to the ICU in a short timeframe, resulting in high mortality rates and high resource utilisation. Several works have tried to predict readmission through patients' medical information. Although they have some level of success while predicting readmission, those works do not properly assess, characterise and understand readmission prediction. This work proposes a standardised and explainable machine learning pipeline to model patient readmission on a multicentric database (i.e., the eICU cohort with 166,355 patients, 200,859 ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#26494;&#24347;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#23558;&#26435;&#37325;&#20540;&#38480;&#21046;&#22312;&#19968;&#32452;&#26377;&#38480;&#20540;&#19978;&#26469;&#20943;&#23569;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#20540;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#21487;&#21387;&#32553;&#24615;&#12290;&#36845;&#20195;&#32858;&#31867;&#36807;&#31243;&#23637;&#31034;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.13575</link><description>&lt;p&gt;
&#27010;&#29575;&#26435;&#37325;&#22266;&#23450;&#65306;&#29992;&#20110;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Weight Fixing: Large-scale training of neural network weight uncertainties for quantization. (arXiv:2309.13575v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#26494;&#24347;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#23558;&#26435;&#37325;&#20540;&#38480;&#21046;&#22312;&#19968;&#32452;&#26377;&#38480;&#20540;&#19978;&#26469;&#20943;&#23569;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#20540;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#21487;&#21387;&#32553;&#24615;&#12290;&#36845;&#20195;&#32858;&#31867;&#36807;&#31243;&#23637;&#31034;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#20849;&#20139;&#37327;&#21270;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#38480;&#21046;&#22312;&#19968;&#32452;&#26377;&#38480;&#30340;&#20540;&#19978;&#26469;&#20943;&#23569;&#25512;&#29702;&#36807;&#31243;&#20013;&#33021;&#37327;&#28040;&#32791;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26435;&#37325;&#20849;&#20139;&#37327;&#21270;&#26041;&#27861;&#24120;&#24120;&#22522;&#20110;&#26435;&#37325;&#20540;&#26412;&#36523;&#36827;&#34892;&#20551;&#35774;&#65292;&#24182;&#24573;&#35270;&#20102;&#26435;&#37325;&#20301;&#32622;&#22312;&#20854;&#20013;&#25198;&#28436;&#30340;&#29420;&#29305;&#35282;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#21644;&#21464;&#20998;&#26494;&#24347;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#26681;&#25454;&#21333;&#20010;&#26435;&#37325;&#30340;&#20301;&#32622;&#29305;&#23450;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#26469;&#30830;&#23450;&#21487;&#20197;&#23558;&#21738;&#20123;&#26435;&#37325;&#31227;&#21160;&#21040;&#21738;&#20010;&#32858;&#31867;&#20013;&#24515;&#20197;&#21450;&#31227;&#21160;&#21040;&#20160;&#20040;&#31243;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21021;&#22987;&#21270;&#35774;&#32622;&#21644;&#27491;&#21017;&#21270;&#39033;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;-&#27169;&#22411;&#32452;&#21512;&#19979;&#35757;&#32451;BNNs&#12290;&#36890;&#36807;&#21033;&#29992;&#36890;&#36807;&#27010;&#29575;&#20998;&#24067;&#25429;&#25417;&#21040;&#30340;&#26435;&#37325;&#20540;&#30340;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#21644;&#19979;&#28216;&#30340;&#21487;&#21387;&#32553;&#24615;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#32858;&#31867;&#36807;&#31243;&#23637;&#31034;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight-sharing quantization has emerged as a technique to reduce energy expenditure during inference in large neural networks by constraining their weights to a limited set of values. However, existing methods for weight-sharing quantization often make assumptions about the treatment of weights based on value alone that neglect the unique role weight position plays. This paper proposes a probabilistic framework based on Bayesian neural networks (BNNs) and a variational relaxation to identify which weights can be moved to which cluster centre and to what degree based on their individual position-specific learned uncertainty distributions. We introduce a new initialisation setting and a regularisation term which allow for the training of BNNs under complex dataset-model combinations. By leveraging the flexibility of weight values captured through a probability distribution, we enhance noise resilience and downstream compressibility. Our iterative clustering procedure demonstrates superio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20840;&#29699;&#30456;&#20851;&#30340;&#19977;&#32500;&#35299;&#32806;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#21333;&#30446;&#22270;&#20687;&#20013;&#37325;&#24314;&#20855;&#26377;&#34915;&#26381;&#30340;&#20154;&#29289;&#21270;&#36523;&#12290;&#36890;&#36807;&#20351;&#29992;Transformer&#27169;&#22411;&#25429;&#25417;&#20840;&#23616;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;3D&#35299;&#32806;&#35299;&#30721;&#22120;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#37325;&#24314;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13524</link><description>&lt;p&gt;
&#20840;&#29699;&#30456;&#20851;&#30340;&#19977;&#32500;&#35299;&#32806;Transformer&#29992;&#20110;&#26381;&#35013;&#21270;&#36523;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Global-correlated 3D-decoupling Transformer for Clothed Avatar Reconstruction. (arXiv:2309.13524v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13524
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20840;&#29699;&#30456;&#20851;&#30340;&#19977;&#32500;&#35299;&#32806;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#21333;&#30446;&#22270;&#20687;&#20013;&#37325;&#24314;&#20855;&#26377;&#34915;&#26381;&#30340;&#20154;&#29289;&#21270;&#36523;&#12290;&#36890;&#36807;&#20351;&#29992;Transformer&#27169;&#22411;&#25429;&#25417;&#20840;&#23616;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;3D&#35299;&#32806;&#35299;&#30721;&#22120;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#37325;&#24314;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21333;&#19968;&#22270;&#20687;&#20013;&#37325;&#24314;&#19977;&#32500;&#26381;&#35013;&#21270;&#36523;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#36935;&#21040;&#22797;&#26434;&#23039;&#21183;&#21644;&#23485;&#26494;&#34915;&#29289;&#26102;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#23427;&#20204;&#23545;&#19981;&#36275;&#30340;&#20108;&#32500;&#22270;&#20687;&#29305;&#24449;&#21644;&#19981;&#19968;&#33268;&#30340;&#26597;&#35810;&#26041;&#27861;&#30340;&#20381;&#36182;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#26381;&#35013;&#21270;&#36523;&#37325;&#24314;&#30340;&#20840;&#29699;&#30456;&#20851;&#30340;&#19977;&#32500;&#35299;&#32806;Transformer&#65288;GTA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21019;&#26032;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#20197;&#20174;&#21333;&#30446;&#22270;&#20687;&#20013;&#37325;&#24314;&#20986;&#20855;&#26377;&#34915;&#26381;&#30340;&#20154;&#29289;&#21270;&#36523;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;Transformer&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;Vision Transformer&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#20840;&#29699;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#37319;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#35299;&#32806;&#19977;&#20301;&#24179;&#38754;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#23884;&#20837;&#20316;&#20026;&#36328;&#24179;&#38754;&#29983;&#25104;&#30340;&#26597;&#35810;&#12290;&#20026;&#20102;&#26377;&#25928;&#22686;&#24378;&#19982;&#19977;&#32500;&#29305;&#24449;&#21644;&#20154;&#20307;&#20808;&#39564;&#30340;&#29305;&#24449;&#34701;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#31354;&#38388;&#21644;p&#30340;&#28151;&#21512;&#20808;&#39564;&#34701;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Reconstructing 3D clothed human avatars from single images is a challenging task, especially when encountering complex poses and loose clothing. Current methods exhibit limitations in performance, largely attributable to their dependence on insufficient 2D image features and inconsistent query methods. Owing to this, we present the Global-correlated 3D-decoupling Transformer for clothed Avatar reconstruction (GTA), a novel transformer-based architecture that reconstructs clothed human avatars from monocular images. Our approach leverages transformer architectures by utilizing a Vision Transformer model as an encoder for capturing global-correlated image features. Subsequently, our innovative 3D-decoupling decoder employs cross-attention to decouple tri-plane features, using learnable embeddings as queries for cross-plane generation. To effectively enhance feature fusion with the tri-plane 3D feature and human body prior, we propose a hybrid prior fusion strategy combining spatial and p
&lt;/p&gt;</description></item><item><title>MiChao-HuaFen 1.0&#26159;&#19968;&#20010;&#19987;&#20026;&#26032;&#38395;&#21644;&#25919;&#24220;&#37096;&#38376;&#23450;&#21046;&#30340;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#22823;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;&#65292;&#23427;&#19981;&#20165;&#33021;&#22815;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#36136;&#37327;&#38656;&#27714;&#65292;&#36824;&#26377;&#21161;&#20110;&#25512;&#21160;&#30456;&#20851;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.13079</link><description>&lt;p&gt;
MiChao-HuaFen 1.0&#65306;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#22823;&#27169;&#22411;&#30340;&#19987;&#29992;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MiChao-HuaFen 1.0: A Specialized Pre-trained Corpus Dataset for Domain-specific Large Models. (arXiv:2309.13079v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13079
&lt;/p&gt;
&lt;p&gt;
MiChao-HuaFen 1.0&#26159;&#19968;&#20010;&#19987;&#20026;&#26032;&#38395;&#21644;&#25919;&#24220;&#37096;&#38376;&#23450;&#21046;&#30340;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#22823;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;&#65292;&#23427;&#19981;&#20165;&#33021;&#22815;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#36136;&#37327;&#38656;&#27714;&#65292;&#36824;&#26377;&#21161;&#20110;&#25512;&#21160;&#30456;&#20851;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#22914;GPT-4&#31561;&#36890;&#29992;&#22823;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#35832;&#22914;&#21307;&#30103;&#12289;&#27861;&#24459;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#23545;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#29305;&#23450;&#36755;&#20986;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#39318;&#20808;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#22823;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#29305;&#27530;&#38656;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;MiChao-HuaFen 1.0&#8221;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#29305;&#21035;&#38024;&#23545;&#26032;&#38395;&#21644;&#25919;&#24220;&#37096;&#38376;&#12290;&#35813;&#25968;&#25454;&#38598;&#26469;&#28304;&#20110;2022&#24180;&#20844;&#24320;&#21487;&#29992;&#30340;&#20114;&#32852;&#32593;&#25968;&#25454;&#65292;&#32463;&#36807;&#22810;&#36718;&#28165;&#27905;&#21644;&#22788;&#29702;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20855;&#22791;&#25345;&#32493;&#21644;&#31283;&#23450;&#30340;&#26356;&#26032;&#26426;&#21046;&#12290;&#35813;&#25968;&#25454;&#38598;&#19981;&#20165;&#25903;&#25345;&#38024;&#23545;&#20013;&#25991;&#22402;&#30452;&#39046;&#22495;&#30340;&#22823;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#36824;&#21161;&#21147;&#20110;&#25512;&#21160;&#30456;&#20851;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advancement of deep learning technologies, general-purpose large models such as GPT-4 have demonstrated exceptional capabilities across various domains. Nevertheless, there remains a demand for high-quality, domain-specific outputs in areas like healthcare, law, and finance. This paper first evaluates the existing large models for specialized domains and discusses their limitations. To cater to the specific needs of certain domains, we introduce the ``MiChao-HuaFen 1.0'' pre-trained corpus dataset, tailored for the news and governmental sectors. The dataset, sourced from publicly available internet data from 2022, underwent multiple rounds of cleansing and processing to ensure high quality and reliable origins, with provisions for consistent and stable updates. This dataset not only supports the pre-training of large models for Chinese vertical domains but also aids in propelling deep learning research and applications in related fields.
&lt;/p&gt;</description></item><item><title>InvestLM&#26159;&#19968;&#20010;&#36890;&#36807;&#23545;&#37329;&#34701;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35299;&#37329;&#34701;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#25237;&#36164;&#30456;&#20851;&#38382;&#39064;&#19978;&#25552;&#20379;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;&#37329;&#34701;&#19987;&#23478;&#35780;&#20215;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#21830;&#19994;&#27169;&#22411;&#21487;&#23218;&#32654;&#65292;&#24182;&#22312;&#37329;&#34701;NLP&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.13064</link><description>&lt;p&gt;
InvestLM&#65306;&#20351;&#29992;&#37329;&#34701;&#39046;&#22495;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning. (arXiv:2309.13064v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13064
&lt;/p&gt;
&lt;p&gt;
InvestLM&#26159;&#19968;&#20010;&#36890;&#36807;&#23545;&#37329;&#34701;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35299;&#37329;&#34701;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#25237;&#36164;&#30456;&#20851;&#38382;&#39064;&#19978;&#25552;&#20379;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;&#37329;&#34701;&#19987;&#23478;&#35780;&#20215;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#21830;&#19994;&#27169;&#22411;&#21487;&#23218;&#32654;&#65292;&#24182;&#22312;&#37329;&#34701;NLP&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#37329;&#34701;&#39046;&#22495;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InvestLM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#19982;&#37329;&#34701;&#25237;&#36164;&#30456;&#20851;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#23545;LLaMA-65B&#36827;&#34892;&#35843;&#20248;&#12290;&#21463;&#21040;&#8220;&#23569;&#21363;&#26159;&#22810;&#8221;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25163;&#21160;&#31574;&#21010;&#20102;&#19968;&#20010;&#26082;&#23567;&#21448;&#22810;&#26679;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20174;&#29305;&#35768;&#37329;&#34701;&#20998;&#26512;&#24072;&#65288;CFA&#65289;&#32771;&#35797;&#38382;&#39064;&#21040;SEC&#25991;&#20214;&#21644;Stackexchange&#37327;&#21270;&#37329;&#34701;&#35752;&#35770;&#30340;&#24191;&#27867;&#37329;&#34701;&#30456;&#20851;&#20027;&#39064;&#12290;InvestLM&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#29702;&#35299;&#37329;&#34701;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#25237;&#36164;&#30456;&#20851;&#38382;&#39064;&#25552;&#20379;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;&#21253;&#25324;&#23545;&#20914;&#22522;&#37329;&#32463;&#29702;&#21644;&#30740;&#31350;&#20998;&#26512;&#24072;&#22312;&#20869;&#30340;&#37329;&#34701;&#19987;&#23478;&#23558;InvestLM&#30340;&#22238;&#31572;&#35780;&#20215;&#20026;&#19982;&#26368;&#20808;&#36827;&#30340;&#21830;&#19994;&#27169;&#22411;&#65288;GPT-3.5&#12289;GPT-4&#21644;Claude-2&#65289;&#21487;&#23218;&#32654;&#12290;&#23545;&#19968;&#32452;&#37329;&#34701;NLP&#22522;&#20934;&#38382;&#39064;&#36827;&#34892;&#38646;&#26679;&#26412;&#35780;&#20272;&#34920;&#26126;&#20102;&#20854;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20174;&#30740;&#31350;&#35282;&#24230;&#26469;&#30475;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#29305;&#23450;LLM&#36827;&#34892;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new financial domain large language model, InvestLM, tuned on LLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset related to financial investment. Inspired by less-is-more-for-alignment (Zhou et al., 2023), we manually curate a small yet diverse instruction dataset, covering a wide range of financial related topics, from Chartered Financial Analyst (CFA) exam questions to SEC filings to Stackexchange quantitative finance discussions. InvestLM shows strong capabilities in understanding financial text and provides helpful responses to investment related questions. Financial experts, including hedge fund managers and research analysts, rate InvestLM's response as comparable to those of state-of-the-art commercial models (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of financial NLP benchmarks demonstrates strong generalizability. From a research perspective, this work suggests that a high-quality domain specific LLM can be tuned usin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#31995;&#32479;Q4FuturePOP&#65292;&#23427;&#21019;&#26032;&#22320;&#21033;&#29992;&#26410;&#26469;&#36164;&#20135;&#20215;&#20540;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#21160;&#20943;&#23569;&#25237;&#36164;&#33539;&#22260;&#30340;&#27169;&#22359;&#12290;&#36890;&#36807;&#23454;&#39564;&#35752;&#35770;&#20102;Q4FuturePOP&#30340;&#21407;&#22411;&#29256;&#26412;&#20013;&#19981;&#21516;&#27169;&#22359;&#30340;&#21021;&#27493;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12627</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#31995;&#32479;&#65306;&#21033;&#29992;&#26410;&#26469;&#36164;&#20135;&#20215;&#20540;&#21644;&#33258;&#21160;&#20943;&#23569;&#25237;&#36164;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
A Quantum Computing-based System for Portfolio Optimization using Future Asset Values and Automatic Reduction of the Investment Universe. (arXiv:2309.12627v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#31995;&#32479;Q4FuturePOP&#65292;&#23427;&#21019;&#26032;&#22320;&#21033;&#29992;&#26410;&#26469;&#36164;&#20135;&#20215;&#20540;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#21160;&#20943;&#23569;&#25237;&#36164;&#33539;&#22260;&#30340;&#27169;&#22359;&#12290;&#36890;&#36807;&#23454;&#39564;&#35752;&#35770;&#20102;Q4FuturePOP&#30340;&#21407;&#22411;&#29256;&#26412;&#20013;&#19981;&#21516;&#27169;&#22359;&#30340;&#21021;&#27493;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#37329;&#34701;&#20013;&#22791;&#21463;&#20851;&#27880;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#38024;&#23545;&#35813;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#25216;&#26415;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Q4FuturePOP&#30340;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#20197;&#19979;&#21019;&#26032;&#35299;&#20915;&#20102;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65306;i&#65289;&#35813;&#24037;&#20855;&#38024;&#23545;&#26410;&#26469;&#36164;&#20135;&#39044;&#27979;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#65307;ii&#65289;Q4FuturePOP&#21253;&#25324;&#19968;&#20010;&#26234;&#33021;&#20943;&#23569;&#38382;&#39064;&#22797;&#26434;&#24615;&#30340;&#33258;&#21160;&#20943;&#23569;&#25237;&#36164;&#33539;&#22260;&#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;Q4FuturePOP&#30340;&#21407;&#22411;&#29256;&#26412;&#30340;&#19981;&#21516;&#27169;&#22359;&#30340;&#21021;&#27493;&#24615;&#33021;&#36827;&#34892;&#20102;&#31616;&#35201;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the problems in quantitative finance that has received the most attention is the portfolio optimization problem. Regarding its solving, this problem has been approached using different techniques, with those related to quantum computing being especially prolific in recent years. In this study, we present a system called Quantum Computing-based System for Portfolio Optimization with Future Asset Values and Automatic Universe Reduction (Q4FuturePOP), which deals with the Portfolio Optimization Problem considering the following innovations: i) the developed tool is modeled for working with future prediction of assets, instead of historical values; and ii) Q4FuturePOP includes an automatic universe reduction module, which is conceived to intelligently reduce the complexity of the problem. We also introduce a brief discussion about the preliminary performance of the different modules that compose the prototypical version of Q4FuturePOP.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#20132;&#20114;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#33021;&#22815;&#20934;&#30830;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.12460</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#31185;&#23398;&#25104;&#20687;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Multimodal Deep Learning for Scientific Imaging Interpretation. (arXiv:2309.12460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#20132;&#20114;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#33021;&#22815;&#20934;&#30830;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#25104;&#20687;&#39046;&#22495;&#65292;&#35299;&#37322;&#35270;&#35273;&#25968;&#25454;&#24120;&#24120;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;&#23545;&#20027;&#39064;&#26448;&#26009;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#27169;&#25311;&#24182;&#35780;&#20272;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;SEM&#65289;&#22270;&#20687;&#30340;&#20154;&#31867;&#20132;&#20114;&#65292;&#29305;&#21035;&#26159;&#29627;&#29827;&#26448;&#26009;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20174;&#21516;&#34892;&#35780;&#35758;&#30340;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#20511;&#21161; GPT-4 &#30340;&#33021;&#21147;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#23613;&#31649;&#23384;&#22312;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#32454;&#24494;&#30340;&#35299;&#37322;&#21644;&#19987;&#19994;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#22312;&#21046;&#23450;&#20934;&#30830;&#30340;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#65292;&#20351;&#24471;&#36827;&#34892;&#32508;&#21512;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of scientific imaging, interpreting visual data often demands an intricate combination of human expertise and deep comprehension of the subject materials. This study presents a novel methodology to linguistically emulate and subsequently evaluate human-like interactions with Scanning Electron Microscopy (SEM) images, specifically of glass materials. Leveraging a multimodal deep learning framework, our approach distills insights from both textual and visual data harvested from peer-reviewed articles, further augmented by the capabilities of GPT-4 for refined data synthesis and evaluation. Despite inherent challenges--such as nuanced interpretations and the limited availability of specialized datasets--our model (GlassLLaVA) excels in crafting accurate interpretations, identifying key features, and detecting defects in previously unseen SEM images. Moreover, we introduce versatile evaluation metrics, suitable for an array of scientific imaging applications, which allows for
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.10313</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#36827;&#34892;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;GPT4&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#20391;&#37325;&#20110;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#35270;&#35273;&#27169;&#22411;&#26469;&#24320;&#21457;&#36890;&#29992;&#30340;LLM&#12290;&#28982;&#32780;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24494;&#35843;&#27169;&#22411;&#26080;&#27861;&#20445;&#25345;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#20173;&#28982;&#26159;&#22810;&#27169;&#24577;LLM&#65288;MLLM&#65289;&#20013;&#30340;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EMT&#65306;&#29992;&#20110;&#35780;&#20272;MLLM&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;MLLM&#20316;&#20026;&#19968;&#20010;&#22270;&#20687;&#20998;&#31867;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;EMT&#26469;&#35780;&#20272;&#20960;&#20010;&#24320;&#28304;&#30340;&#24494;&#35843;MLLM&#65292;&#24182;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;MLLM&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#26080;&#27861;&#20445;&#25345;&#19982;&#20182;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32487;&#32493;&#24494;&#35843;LLaVA&#65292;&#19968;&#31181;MLLM&#65292;&#24182;&#21033;&#29992;EMT&#26469;&#35780;&#20272;&#25972;&#20010;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#30340;&#24494;&#35843;&#38454;&#27573;&#26159;&#20851;&#38190;&#30340;&#65292;&#36807;&#26089;&#20572;&#27490;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#20302;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#24555;&#36229;&#36731;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;Fast-MpoxNet&#65292;&#29992;&#20110;&#26089;&#26399;&#29492;&#30168;&#35786;&#26029;&#12290;&#23427;&#20855;&#26377;&#36739;&#23567;&#30340;&#21442;&#25968;&#37327;&#21644;&#36739;&#24555;&#30340;&#22788;&#29702;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#21644;&#36741;&#21161;&#25439;&#22833;&#22686;&#24378;&#31574;&#30053;&#25552;&#39640;&#20102;&#35786;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13492</link><description>&lt;p&gt;
&#36229;&#24555;&#36229;&#36731;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26234;&#33021;&#30417;&#27979;&#31995;&#32479;&#29992;&#20110;&#22312;&#20219;&#20309;&#26102;&#38388;&#21644;&#22320;&#28857;&#35786;&#26029;&#26089;&#26399;&#29492;&#30168;
&lt;/p&gt;
&lt;p&gt;
Ultrafast-and-Ultralight ConvNet-Based Intelligent Monitoring System for Diagnosing Early-Stage Mpox Anytime and Anywhere. (arXiv:2308.13492v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13492
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#24555;&#36229;&#36731;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;Fast-MpoxNet&#65292;&#29992;&#20110;&#26089;&#26399;&#29492;&#30168;&#35786;&#26029;&#12290;&#23427;&#20855;&#26377;&#36739;&#23567;&#30340;&#21442;&#25968;&#37327;&#21644;&#36739;&#24555;&#30340;&#22788;&#29702;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#21644;&#36741;&#21161;&#25439;&#22833;&#22686;&#24378;&#31574;&#30053;&#25552;&#39640;&#20102;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#26356;&#39640;&#25928;&#30340;&#29492;&#30168;&#35786;&#26029;&#24037;&#20855;&#65292;&#20854;&#20256;&#25773;&#20173;&#28982;&#26410;&#21463;&#25511;&#21046;&#65292;&#32473;&#20840;&#29699;&#20581;&#24247;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#30456;&#20851;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29492;&#30168;&#35786;&#26029;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#65292;&#20294;&#26159;&#23545;&#20110;&#26089;&#26399;&#29492;&#30168;&#30340;&#25512;&#29702;&#36895;&#24230;&#12289;&#21442;&#25968;&#22823;&#23567;&#21644;&#35786;&#26029;&#24615;&#33021;&#30340;&#24573;&#35270;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#24555;&#36229;&#36731;&#30340;&#32593;&#32476;&#65292;&#21517;&#20026;Fast-MpoxNet&#12290;Fast-MpoxNet&#21482;&#26377;0.27M&#20010;&#21442;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;CPU&#19978;&#20197;&#27599;&#31186;68&#24103;&#30340;&#36895;&#24230;&#22788;&#29702;&#36755;&#20837;&#22270;&#20687;&#12290;&#20026;&#20102;&#20811;&#26381;&#23567;&#27169;&#22411;&#23481;&#37327;&#24102;&#26469;&#30340;&#35786;&#26029;&#24615;&#33021;&#38480;&#21046;&#65292;&#23427;&#38598;&#25104;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#21644;&#22810;&#20010;&#36741;&#21161;&#25439;&#22833;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#24494;&#23567;&#30340;&#22270;&#20687;&#21464;&#21270;&#21644;&#20248;&#21270;&#26435;&#37325;&#12290;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#21644;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#65292;Fast-MpoxNet&#23454;&#29616;&#20102;94.26%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the lack of more efficient diagnostic tools for monkeypox, its spread remains unchecked, presenting a formidable challenge to global health. While the high efficacy of deep learning models for monkeypox diagnosis has been demonstrated in related studies, the overlook of inference speed, the parameter size and diagnosis performance for early-stage monkeypox renders the models inapplicable in real-world settings. To address these challenges, we proposed an ultrafast and ultralight network named Fast-MpoxNet. Fast-MpoxNet possesses only 0.27M parameters and can process input images at 68 frames per second (FPS) on the CPU. To counteract the diagnostic performance limitation brought about by the small model capacity, it integrates the attention-based feature fusion module and the multiple auxiliary losses enhancement strategy for better detecting subtle image changes and optimizing weights. Using transfer learning and five-fold cross-validation, Fast-MpoxNet achieves 94.26% Accuracy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26234;&#33021;&#25163;&#26426;&#25293;&#25668;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#21457;&#23637;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#29305;&#23450;&#20301;&#32622;&#30340;PM2.5&#27987;&#24230;&#65292;&#25581;&#31034;&#20102;&#26412;&#22320;&#32858;&#21512;&#30340;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03200</link><description>&lt;p&gt;
&#21033;&#29992;&#39640;&#25928;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20174;&#26234;&#33021;&#25163;&#26426;&#25293;&#25668;&#22270;&#20687;&#20013;&#25581;&#31034;&#26412;&#22320;&#32858;&#21512;&#30340;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Uncovering local aggregated air quality index with smartphone captured images leveraging efficient deep convolutional neural network. (arXiv:2308.03200v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26234;&#33021;&#25163;&#26426;&#25293;&#25668;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#21457;&#23637;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#29305;&#23450;&#20301;&#32622;&#30340;PM2.5&#27987;&#24230;&#65292;&#25581;&#31034;&#20102;&#26412;&#22320;&#32858;&#21512;&#30340;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25163;&#26426;&#30340;&#26222;&#21450;&#21644;&#21487;&#31227;&#21160;&#24615;&#20351;&#20854;&#25104;&#20026;&#29615;&#22659;&#20581;&#24247;&#30740;&#31350;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20013;&#23545;&#22522;&#20110;&#29305;&#23450;&#20301;&#32622;PM2.5&#27987;&#24230;&#30830;&#23450;&#32858;&#21512;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#65288;AQI&#65289;&#30340;&#28508;&#21147;&#30340;&#30740;&#31350;&#20173;&#28982;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#30456;&#26426;&#25293;&#25668;&#30340;&#22270;&#20687;&#26469;&#39044;&#27979;&#29305;&#23450;&#20301;&#32622;PM2.5&#27987;&#24230;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#23391;&#21152;&#25289;&#22269;&#39318;&#37117;&#36798;&#21345;&#65292;&#22240;&#20854;&#20005;&#37325;&#30340;&#31354;&#27668;&#27745;&#26579;&#27700;&#24179;&#21644;&#22823;&#37327;&#26292;&#38706;&#20110;&#20854;&#20013;&#30340;&#20154;&#21475;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28041;&#21450;&#24320;&#21457;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNN&#65289;&#65292;&#25105;&#20204;&#20351;&#29992;&#36229;&#36807;&#19968;&#21315;&#24352;&#22312;&#36798;&#21345;&#19981;&#21516;&#22320;&#28857;&#25293;&#25668;&#21644;&#26631;&#27880;&#30340;&#23460;&#22806;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#29031;&#29255;&#30340;&#26631;&#31614;&#22522;&#20110;&#20174;&#24403;&#22320;&#32654;&#22269;&#39046;&#20107;&#39302;&#33719;&#21462;&#30340;PM2.5&#27987;&#24230;&#25968;&#25454;&#65292;&#20351;&#29992;NowCast&#31639;&#27861;&#35745;&#31639;&#24471;&#21040;&#12290;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#24314;&#31435;&#20102;&#19968;&#20010;c
&lt;/p&gt;
&lt;p&gt;
The prevalence and mobility of smartphones make these a widely used tool for environmental health research. However, their potential for determining aggregated air quality index (AQI) based on PM2.5 concentration in specific locations remains largely unexplored in the existing literature. In this paper, we thoroughly examine the challenges associated with predicting location-specific PM2.5 concentration using images taken with smartphone cameras. The focus of our study is on Dhaka, the capital of Bangladesh, due to its significant air pollution levels and the large population exposed to it. Our research involves the development of a Deep Convolutional Neural Network (DCNN), which we train using over a thousand outdoor images taken and annotated. These photos are captured at various locations in Dhaka, and their labels are based on PM2.5 concentration data obtained from the local US consulate, calculated using the NowCast algorithm. Through supervised learning, our model establishes a c
&lt;/p&gt;</description></item><item><title>RL-ViGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#27867;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#21253;&#21547;&#22810;&#26679;&#30340;&#20219;&#21153;&#21644;&#24191;&#27867;&#30340;&#27867;&#21270;&#31867;&#22411;&#65292;&#26088;&#22312;&#25512;&#21160;&#23545;&#20195;&#29702;&#20154;&#35270;&#35273;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.10224</link><description>&lt;p&gt;
RL-ViGen: &#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#27867;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization. (arXiv:2307.10224v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10224
&lt;/p&gt;
&lt;p&gt;
RL-ViGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#27867;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#21253;&#21547;&#22810;&#26679;&#30340;&#20219;&#21153;&#21644;&#24191;&#27867;&#30340;&#27867;&#21270;&#31867;&#22411;&#65292;&#26088;&#22312;&#25512;&#21160;&#23545;&#20195;&#29702;&#20154;&#35270;&#35273;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#65288;Visual RL&#65289;&#19982;&#39640;&#32500;&#35266;&#23519;&#30456;&#32467;&#21512;&#65292;&#19968;&#30452;&#38754;&#20020;&#30528;&#38271;&#26399;&#23384;&#22312;&#30340;&#27867;&#21270;&#25361;&#25112;&#12290;&#23613;&#31649;&#26377;&#37325;&#28857;&#30740;&#31350;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#27867;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#22522;&#20934;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#23616;&#38480;&#20110;&#23396;&#31435;&#30340;&#20219;&#21153;&#21644;&#27867;&#21270;&#31867;&#21035;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;&#23545;&#20195;&#29702;&#20154;&#35270;&#35273;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RL-ViGen&#65306;&#19968;&#31181;&#26032;&#22411;&#30340;&#29992;&#20110;&#35270;&#35273;&#27867;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#26679;&#30340;&#20219;&#21153;&#21644;&#24191;&#27867;&#30340;&#27867;&#21270;&#31867;&#22411;&#65292;&#20174;&#32780;&#20419;&#36827;&#24471;&#20986;&#26356;&#21487;&#38752;&#30340;&#32467;&#35770;&#12290;&#27492;&#22806;&#65292;RL-ViGen&#23558;&#26368;&#26032;&#30340;&#27867;&#21270;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#34701;&#20837;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27809;&#26377;&#21333;&#19968;&#30340;&#29616;&#26377;&#31639;&#27861;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#26222;&#36941;&#21344;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#24895;&#26223;&#26159;RL-ViGen&#23558;&#22312;&#36825;&#20010;&#39046;&#22495;&#36215;&#21040;&#20652;&#21270;&#21058;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Reinforcement Learning (Visual RL), coupled with high-dimensional observations, has consistently confronted the long-standing challenge of generalization. Despite the focus on algorithms aimed at resolving visual generalization problems, we argue that the devil is in the existing benchmarks as they are restricted to isolated tasks and generalization categories, undermining a comprehensive evaluation of agents' visual generalization capabilities. To bridge this gap, we introduce RL-ViGen: a novel Reinforcement Learning Benchmark for Visual Generalization, which contains diverse tasks and a wide spectrum of generalization types, thereby facilitating the derivation of more reliable conclusions. Furthermore, RL-ViGen incorporates the latest generalization visual RL algorithms into a unified framework, under which the experiment results indicate that no single existing algorithm has prevailed universally across tasks. Our aspiration is that RL-ViGen will serve as a catalyst in this a
&lt;/p&gt;</description></item><item><title>TinyMetaFed&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#39640;&#25928;&#32852;&#37030;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#65292;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#33021;&#22815;&#24555;&#36895;&#24494;&#35843;&#65292;&#21516;&#26102;&#23454;&#29616;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2307.06822</link><description>&lt;p&gt;
TinyMetaFed: &#39640;&#25928;&#30340;&#29992;&#20110;TinyML&#30340;&#32852;&#37030;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TinyMetaFed: Efficient Federated Meta-Learning for TinyML. (arXiv:2307.06822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06822
&lt;/p&gt;
&lt;p&gt;
TinyMetaFed&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#39640;&#25928;&#32852;&#37030;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#65292;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#33021;&#22815;&#24555;&#36895;&#24494;&#35843;&#65292;&#21516;&#26102;&#23454;&#29616;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tiny Machine Learning (TinyML)&#39046;&#22495;&#22312;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#65288;&#22914;&#24494;&#25511;&#21046;&#22120;&#65289;&#19978;&#23454;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#24494;&#22411;&#35774;&#22791;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#32858;&#21512;&#23427;&#20204;&#30340;&#30693;&#35782;&#26159;&#21542;&#33021;&#22815;&#20351;TinyML&#24212;&#29992;&#21463;&#30410;&#12290;&#32852;&#37030;&#20803;&#23398;&#20064;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#31572;&#26696;&#65292;&#22240;&#20026;&#23427;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#35774;&#22791;&#20043;&#38388;&#30340;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;TinyML&#30828;&#20214;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#36164;&#28304;&#38480;&#21046;&#65292;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#33021;&#28304;&#12289;&#38544;&#31169;&#21644;&#36890;&#20449;&#38480;&#21046;&#32780;&#19981;&#23454;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TinyMetaFed&#65292;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;TinyMetaFed&#20419;&#36827;&#20102;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#26032;&#35774;&#22791;&#19978;&#24555;&#36895;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#37096;&#20998;&#26412;&#22320;&#37325;&#26500;&#21644;Top-P%&#36873;&#25321;&#24615;&#36890;&#20449;&#25552;&#20379;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#26524;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Tiny Machine Learning (TinyML) has made substantial advancements in democratizing machine learning on low-footprint devices, such as microcontrollers. The prevalence of these miniature devices raises the question of whether aggregating their knowledge can benefit TinyML applications. Federated meta-learning is a promising answer to this question, as it addresses the scarcity of labeled data and heterogeneous data distribution across devices in the real world. However, deploying TinyML hardware faces unique resource constraints, making existing methods impractical due to energy, privacy, and communication limitations. We introduce TinyMetaFed, a model-agnostic meta-learning framework suitable for TinyML. TinyMetaFed facilitates collaborative training of a neural network initialization that can be quickly fine-tuned on new devices. It offers communication savings and privacy protection through partial local reconstruction and Top-P% selective communication, computational eff
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#21644;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#21457;&#29616;&#65292;Bellman &#35823;&#24046;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02345</link><description>&lt;p&gt;
LLQL: &#36923;&#36753;&#20284;&#28982; Q-Learning &#29992;&#20110;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning. (arXiv:2307.02345v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#21644;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#21457;&#29616;&#65292;Bellman &#35823;&#24046;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#20998;&#20026;&#22312;&#32447;&#21644;&#31163;&#32447;&#20004;&#31181;&#21464;&#20307;&#12290;&#20316;&#20026;&#22312;&#32447;&#21644;&#31163;&#32447; RL &#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#24403;&#21069;&#23545; Bellman &#26041;&#31243;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#25216;&#26415;&#21644;&#24615;&#33021;&#22686;&#24378;&#19978;&#65292;&#32780;&#19981;&#26159;&#25506;&#32034; Bellman &#35823;&#24046;&#30340;&#22266;&#26377;&#32467;&#26500;&#29305;&#24615;&#65292;&#22914;&#20854;&#20998;&#24067;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545; Bellman &#26041;&#31243;&#36827;&#34892;&#36845;&#20195;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#22312;&#32447; RL &#21644;&#31163;&#32447; RL &#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#24773;&#20917;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26080;&#35770;&#26159;&#22312;&#32447; RL &#36824;&#26159;&#31163;&#32447; RL&#65292;Bellman &#35823;&#24046;&#37117;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#65288;LLoss&#65289;&#20316;&#20026;&#24120;&#29992;&#30340; MSE Loss &#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20551;&#35774; Bellman &#35823;&#24046;&#26381;&#20174;&#27491;&#24577;&#20998;&#24067;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#22312;&#19981;&#21516;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern reinforcement learning (RL) can be categorized into online and offline variants. As a pivotal aspect of both online and offline RL, current research on the Bellman equation revolves primarily around optimization techniques and performance enhancement rather than exploring the inherent structural properties of the Bellman error, such as its distribution characteristics. This study investigates the distribution of the Bellman approximation error in both online and offline settings through iterative exploration of the Bellman equation. We observed that both in online RL and offline RL, the Bellman error conforms to a Logistic distribution. Building upon this discovery, this study employed the Logistics maximum likelihood function (LLoss) as an alternative to the commonly used MSE Loss, assuming that Bellman errors adhere to a normal distribution. We validated our hypotheses through extensive numerical experiments across diverse online and offline environments. In particular, we app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21333;&#27425;&#23398;&#20064;&#22810;&#26679;&#25216;&#33021;&#30340;&#32508;&#21512;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;RH20T&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;11&#19975;&#20010;&#25509;&#35302;&#20016;&#23500;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#24207;&#21015;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#25216;&#33021;&#12289;&#29615;&#22659;&#12289;&#26426;&#22120;&#20154;&#21644;&#30456;&#26426;&#35270;&#35282;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#30446;&#26631;&#26159;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20855;&#22791;&#24191;&#27867;&#30340;&#19968;&#33324;&#21270;&#33021;&#21147;&#65292;&#21253;&#25324;&#35270;&#35273;&#21644;&#35302;&#35273;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2307.00595</link><description>&lt;p&gt;
RH20T: &#19968;&#31181;&#29992;&#20110;&#21333;&#27425;&#23398;&#20064;&#22810;&#26679;&#25216;&#33021;&#30340;&#32508;&#21512;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot. (arXiv:2307.00595v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21333;&#27425;&#23398;&#20064;&#22810;&#26679;&#25216;&#33021;&#30340;&#32508;&#21512;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;RH20T&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;11&#19975;&#20010;&#25509;&#35302;&#20016;&#23500;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#24207;&#21015;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#25216;&#33021;&#12289;&#29615;&#22659;&#12289;&#26426;&#22120;&#20154;&#21644;&#30456;&#26426;&#35270;&#35282;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#30446;&#26631;&#26159;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20855;&#22791;&#24191;&#27867;&#30340;&#19968;&#33324;&#21270;&#33021;&#21147;&#65292;&#21253;&#25324;&#35270;&#35273;&#21644;&#35302;&#35273;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#65292;&#22914;&#20309;&#33719;&#21462;&#22810;&#26679;&#21270;&#19988;&#20855;&#26377;&#19968;&#33324;&#21270;&#33021;&#21147;&#30340;&#26426;&#22120;&#20154;&#25216;&#33021;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#21333;&#27425;&#27169;&#20223;&#23398;&#20064;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#31034;&#33539;&#65292;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#36716;&#31227;&#21040;&#26032;&#20219;&#21153;&#19978;&#20855;&#26377;&#28508;&#21147;&#12290;&#36825;&#31181;&#29305;&#24615;&#26377;&#21161;&#20110;&#20351;&#26426;&#22120;&#20154;&#33719;&#24471;&#26032;&#30340;&#25216;&#33021;&#24182;&#25913;&#36827;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#30446;&#21069;&#31038;&#21306;&#30340;&#20851;&#27880;&#28857;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21333;&#30340;&#24773;&#20917;&#65292;&#22914;&#25512;&#21160;&#25110;&#25342;&#21462;&#25918;&#32622;&#20219;&#21153;&#65292;&#20165;&#20381;&#38752;&#35270;&#35273;&#25351;&#23548;&#12290;&#23454;&#38469;&#19978;&#65292;&#23384;&#22312;&#35768;&#22810;&#22797;&#26434;&#30340;&#25216;&#33021;&#65292;&#20854;&#20013;&#19968;&#20123;&#29978;&#33267;&#21487;&#33021;&#38656;&#35201;&#35270;&#35273;&#21644;&#35302;&#35273;&#24863;&#30693;&#26469;&#35299;&#20915;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#38145;&#20195;&#29702;&#21830;&#36816;&#29992;&#22810;&#27169;&#24577;&#24863;&#30693;&#25512;&#24191;&#21040;&#25968;&#30334;&#31181;&#29616;&#23454;&#19990;&#30028;&#25216;&#33021;&#30340;&#28508;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#36807;11&#19975;&#20010;&#36328;&#22810;&#31181;&#25216;&#33021;&#12289;&#29615;&#22659;&#12289;&#26426;&#22120;&#20154;&#21644;&#30456;&#26426;&#35270;&#35282;&#30340;&#25509;&#35302;&#20016;&#23500;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in robotic manipulation in open domains is how to acquire diverse and generalizable skills for robots. Recent research in one-shot imitation learning has shown promise in transferring trained policies to new tasks based on demonstrations. This feature is attractive for enabling robots to acquire new skills and improving task and motion planning. However, due to limitations in the training dataset, the current focus of the community has mainly been on simple cases, such as push or pick-place tasks, relying solely on visual guidance. In reality, there are many complex skills, some of which may even require both visual and tactile perception to solve. This paper aims to unlock the potential for an agent to generalize to hundreds of real-world skills with multi-modal perception. To achieve this, we have collected a dataset comprising over 110,000 contact-rich robot manipulation sequences across diverse skills, contexts, robots, and camera viewpoints, all collected in the re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CompanyKG&#65292;&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#21450;&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#20272;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.10649</link><description>&lt;p&gt;
CompanyKG:&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;
&lt;/p&gt;
&lt;p&gt;
CompanyKG: A Large-Scale Heterogeneous Graph for Company Similarity Quantification. (arXiv:2306.10649v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CompanyKG&#65292;&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#21450;&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#20272;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25237;&#36164;&#34892;&#19994;&#20013;&#65292;&#23545;&#20110;&#35768;&#22810;&#30446;&#30340;&#21253;&#25324;&#24066;&#22330;&#26144;&#23556;&#12289;&#31454;&#20105;&#23545;&#25163;&#20998;&#26512;&#21644;&#24182;&#36141;&#65292;&#36827;&#34892;&#32454;&#31890;&#24230;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#36890;&#24120;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;CompanyKG&#30340;&#30693;&#35782;&#22270;&#65292;&#29992;&#20110;&#34920;&#31034;&#21644;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;1.17&#30334;&#19975;&#23478;&#20844;&#21496;&#34987;&#34920;&#31034;&#20026;&#33410;&#28857;&#65292;&#20016;&#23500;&#20102;&#20844;&#21496;&#25551;&#36848;&#23884;&#20837;; 15&#31181;&#19981;&#21516;&#30340;&#20844;&#21496;&#38388;&#20851;&#31995;&#23548;&#33268;&#20102;5106&#30334;&#19975;&#20010;&#24102;&#26435;&#37325;&#30340;&#36793;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#32534;&#35793;&#20102;&#19977;&#20010;&#24102;&#26377;&#27880;&#37322;&#27979;&#35797;&#38598;&#30340;&#35780;&#20272;&#20219;&#21153;: &#30456;&#20284;&#24615;&#39044;&#27979;&#12289;&#31454;&#20105;&#23545;&#25163;&#26816;&#32034;&#21644;&#30456;&#20284;&#24615;&#25490;&#24207;&#12290;&#25105;&#20204;&#23545;11&#31181;&#21487;&#37325;&#29616;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20998;&#20026;&#33410;&#28857;&#12289;&#36793;&#21644;&#33410;&#28857;+&#36793;&#19977;&#32452;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;CompanyKG&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
In the investment industry, it is often essential to carry out fine-grained company similarity quantification for a range of purposes, including market mapping, competitor analysis, and mergers and acquisitions. We propose and publish a knowledge graph, named CompanyKG, to represent and learn diverse company features and relations. Specifically, 1.17 million companies are represented as nodes enriched with company description embeddings; and 15 different inter-company relations result in 51.06 million weighted edges. To enable a comprehensive assessment of methods for company similarity quantification, we have devised and compiled three evaluation tasks with annotated test sets: similarity prediction, competitor retrieval and similarity ranking. We present extensive benchmarking results for 11 reproducible predictive methods categorized into three groups: node-only, edge-only, and node+edge. To the best of our knowledge, CompanyKG is the first large-scale heterogeneous graph dataset or
&lt;/p&gt;</description></item><item><title>MO-VLN&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#36890;&#29992;&#26426;&#22120;&#20154;&#22312;&#22810;&#20219;&#21153;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;5&#24320;&#21457;&#36924;&#30495;&#30340;&#22330;&#26223;&#21644;&#21253;&#21547;&#22810;&#31181;&#19981;&#24120;&#35265;&#29289;&#20307;&#26469;&#27979;&#35797;&#20854;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.10322</link><description>&lt;p&gt;
MO-VLN:&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#38598;&#21512;&#38646;&#26679;&#26412;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934; (arXiv:2306.10322v2 [cs.CV] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
MO-VLN: A Multi-Task Benchmark for Open-set Zero-Shot Vision-and-Language Navigation. (arXiv:2306.10322v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10322
&lt;/p&gt;
&lt;p&gt;
MO-VLN&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#36890;&#29992;&#26426;&#22120;&#20154;&#22312;&#22810;&#20219;&#21153;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;5&#24320;&#21457;&#36924;&#30495;&#30340;&#22330;&#26223;&#21644;&#21253;&#21547;&#22810;&#31181;&#19981;&#24120;&#35265;&#29289;&#20307;&#26469;&#27979;&#35797;&#20854;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#24517;&#39035;&#29702;&#35299;&#25351;&#20196;&#24182;&#26681;&#25454;&#35270;&#35273;&#35266;&#23519;&#25214;&#21040;&#30446;&#26631;&#23545;&#35937;&#25110;&#20301;&#32622;&#65292;&#21363;&#20351;&#22312;&#26410;&#25506;&#32034;&#30340;&#29615;&#22659;&#20013;&#20063;&#33021;&#20570;&#21040;&#12290;&#22823;&#22810;&#25968;&#20195;&#29702;&#20381;&#36182;&#20110;&#22823;&#37327;&#22810;&#26679;&#30340;&#35757;&#32451;&#25968;&#25454;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#65292;&#36825;&#38656;&#35201;&#26114;&#36149;&#30340;&#21171;&#21160;&#21147;&#12290;&#36825;&#20123;&#20195;&#29702;&#36890;&#24120;&#21482;&#20851;&#27880;&#24120;&#35265;&#30340;&#23545;&#35937;&#21644;&#36739;&#23569;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#19981;&#36275;&#20197;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25351;&#20196;&#12290;&#20026;&#20102;&#20419;&#36827;&#24320;&#25918;&#38598;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MO-VLN&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#27979;&#35797;&#20195;&#29702;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;5&#24320;&#21457;&#20102;&#19968;&#20010;3D&#27169;&#25311;&#22120;&#65292;&#28210;&#26579;&#20102;&#36924;&#30495;&#30340;&#22330;&#26223;&#65292;&#21253;&#21547;&#26356;&#30495;&#23454;&#30340;&#20809;&#29031;&#21644;&#32454;&#33410;&#12290;&#27169;&#25311;&#22120;&#21253;&#21547;&#19977;&#20010;&#22330;&#26223;&#65292;&#21363;&#21654;&#21857;&#39302;&#12289;&#39184;&#21381;&#21644;&#20859;&#32769;&#38498;&#65292;&#36825;&#20123;&#22330;&#26223;&#22312;&#24037;&#19994;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#25311;&#22120;&#28041;&#21450;&#22810;&#31181;&#19981;&#24120;&#35265;&#30340;&#29289;&#20307;&#65292;&#22914;&#22806;&#21334;&#26479;&#21644;&#21307;&#29992;&#33014;&#24102;&#65292;&#36825;&#20123;&#29289;&#20307;&#26356;&#21152;&#22797;&#26434;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a natural language, a general robot has to comprehend the instruction and find the target object or location based on visual observations even in unexplored environments. Most agents rely on massive diverse training data to achieve better generalization, which requires expensive labor. These agents often focus on common objects and fewer tasks, thus are not intelligent enough to handle different types of instructions. To facilitate research in open-set vision-and-language navigation, we propose a benchmark named MO-VLN, aiming at testing the effectiveness and generalization of the agent in the multi-task setting. First, we develop a 3D simulator rendered by realistic scenarios using Unreal Engine 5, containing more realistic lights and details. The simulator contains three scenes, i.e., cafe, restaurant, and nursing house, of high value in the industry. Besides, our simulator involves multiple uncommon objects, such as takeaway cup and medical adhesive tape, which are more compli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992; ChatGPT &#35299;&#20915;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616; ChatGPT &#22312;&#26576;&#20123;&#29615;&#22659;&#19979;&#33021;&#22815;&#25552;&#39640;&#25104;&#21151;&#31574;&#30053;&#25968;&#37327;</title><link>http://arxiv.org/abs/2306.08094</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807; ChatGPT &#23454;&#29616;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65311;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning. (arXiv:2306.08094v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992; ChatGPT &#35299;&#20915;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616; ChatGPT &#22312;&#26576;&#20123;&#29615;&#22659;&#19979;&#33021;&#22815;&#25552;&#39640;&#25104;&#21151;&#31574;&#30053;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#19981;&#26029;&#22686;&#22810;&#65292;&#21516;&#26102;&#20063;&#20984;&#26174;&#20102;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; ChatGPT &#30740;&#31350;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#22797;&#26434;&#30340;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#21457;&#29616; ChatGPT &#22312;&#26576;&#20123;&#29615;&#22659;&#19979;&#33021;&#22815;&#22686;&#21152;&#25104;&#21151;&#31574;&#30053;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
The surge in Reinforcement Learning (RL) applications in Intelligent Transportation Systems (ITS) has contributed to its growth as well as highlighted key challenges. However, defining objectives of RL agents in traffic control and management tasks, as well as aligning policies with these goals through an effective formulation of Markov Decision Process (MDP), can be challenging and often require domain experts in both RL and ITS. Recent advancements in Large Language Models (LLMs) such as GPT-4 highlight their broad general knowledge, reasoning capabilities, and commonsense priors across various domains. In this work, we conduct a large-scale user study involving 70 participants to investigate whether novices can leverage ChatGPT to solve complex mixed traffic control problems. Three environments are tested, including ring road, bottleneck, and intersection. We find ChatGPT has mixed results. For intersection and bottleneck, ChatGPT increases number of successful policies by 150% and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01762</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#25552;&#32431;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformer for adversarial purification. (arXiv:2306.01762v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#37096;&#32626;&#20026;&#21508;&#31181;&#26085;&#24120;&#26381;&#21153;&#65292;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#36867;&#36991;&#25915;&#20987;&#26159;&#26368;&#26222;&#36941;&#30340;&#19968;&#31181;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25110;&#21033;&#29992;&#22823;&#37327;&#28165;&#27905;&#25968;&#25454;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#20854;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#37096;&#32626;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23545;&#22312;&#32447;&#26381;&#21153;&#36896;&#25104;&#37325;&#22823;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#24403;&#26816;&#27979;&#21040;&#26576;&#31181;&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#26102;&#65292;&#26381;&#21153;&#25552;&#20379;&#32773;&#21482;&#33021;&#33719;&#24471;&#26377;&#38480;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#32780;&#22823;&#37327;&#30340;&#28165;&#27905;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#21517;&#20026;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#26088;&#22312;&#24555;&#36895;&#38450;&#24481;&#20855;&#26377;&#23569;&#37327;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#31034;&#20363;&#38480;&#21046;&#30340;&#21407;&#22987;&#26381;&#21153;&#27169;&#22411;&#30340;&#26576;&#31181;&#25915;&#20987;&#12290;&#21463;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#36716;&#31227;&#23398;&#20064;&#33391;&#22909;&#21021;&#22987;&#21270;&#30340;&#36890;&#29992;&#36235;&#21183;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;Transformer&#26469;&#25552;&#32431;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#39044;&#35757;&#32451;&#30340;Transformer&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#40723;&#21169;&#25552;&#32431;&#21518;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#25509;&#36817;&#28165;&#26224;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RaPiD&#22312;&#38450;&#24481;&#21508;&#31181;&#20855;&#26377;&#38480;&#25968;&#25454;&#30340;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more deep neural networks being deployed as various daily services, their reliability is essential. It's frightening that deep neural networks are vulnerable and sensitive to adversarial attacks, the most common one of which for the services is evasion-based. Recent works usually strengthen the robustness by adversarial training or leveraging the knowledge of an amount of clean data. However, in practical terms, retraining and redeploying the model need a large computational budget, leading to heavy losses to the online service. In addition, when adversarial examples of a certain attack are detected, only limited adversarial examples are available for the service provider, while much clean data may not be accessible. Given the mentioned problems, we propose a new scenario, RaPiD (Rapid Plug-in Defender), which is to rapidly defend against a certain attack for the frozen original service model with limitations of few clean and adversarial examples. Motivated by the general
&lt;/p&gt;</description></item><item><title>Med-UniC&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#36328;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#65292;&#23454;&#29616;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#23545;&#40784;&#35268;&#21017;(CTR)&#65292;&#20197;&#26126;&#30830;&#32479;&#19968;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#30340;&#21307;&#23398;&#25253;&#21578;&#30340;&#36328;&#35821;&#35328;&#35821;&#20041;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.19894</link><description>&lt;p&gt;
Med-UniC&#65306;&#36890;&#36807;&#20943;&#23569;&#20559;&#35265;&#23454;&#29616;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;
&lt;/p&gt;
&lt;p&gt;
Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias. (arXiv:2305.19894v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19894
&lt;/p&gt;
&lt;p&gt;
Med-UniC&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#36328;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#65292;&#23454;&#29616;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#23545;&#40784;&#35268;&#21017;(CTR)&#65292;&#20197;&#26126;&#30830;&#32479;&#19968;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#30340;&#21307;&#23398;&#25253;&#21578;&#30340;&#36328;&#35821;&#35328;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#32570;&#24615;&#23545;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#30340;&#25928;&#26524;&#36896;&#25104;&#20102;&#20005;&#37325;&#38556;&#30861;&#12290;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#22312;&#20110;&#32467;&#21512;&#26469;&#33258;&#21508;&#31181;&#35821;&#35328;&#31038;&#21306;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;&#25361;&#25112;&#26469;&#33258;&#20110;&#25972;&#21512;&#19981;&#21516;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#12289;&#29305;&#23450;&#20110;&#35821;&#35328;&#30340;&#21307;&#23398;&#26415;&#35821;&#20197;&#21450;&#29305;&#23450;&#20110;&#25991;&#21270;&#30340;&#38544;&#24335;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#32771;&#34385;&#22240;&#32032;&#26159;&#30001;&#19981;&#21516;&#35821;&#35328;&#24341;&#36215;&#30340;&#31038;&#21306;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#32479;&#19968;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;(Med-UniC)&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#25972;&#21512;&#26469;&#33258;&#20004;&#31181;&#26368;&#24120;&#35265;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#65292;&#21363;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#23545;&#40784;&#35268;&#21017;(CTR)&#65292;&#26126;&#30830;&#32479;&#19968;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#30340;&#21307;&#23398;&#25253;&#21578;&#30340;&#36328;&#35821;&#35328;&#35821;&#20041;&#34920;&#31034;&#12290;&#36890;&#36807;&#28508;&#22312;&#35821;&#35328;&#35299;&#32544;&#65292;&#20248;&#21270;CTR&#65292;&#20351;&#25105;&#20204;&#30340;&#20248;&#21270;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scarcity of data presents a critical obstacle to the efficacy of medical visionlanguage pre-training (VLP). A potential solution lies in the combination of datasets from various language communities. Nevertheless, the main challenge stems from the complexity of integrating diverse syntax and semantics, language-specific medical terminology, and culture-specific implicit knowledge. Therefore, one crucial aspect to consider is the presence of community bias caused by different languages. This paper presents a novel framework named Unifying Cross-Lingual Medical Vision-Language Pre-Training (Med-UniC), designed to integrate multimodal medical data from the two most prevalent languages, English and Spanish. Specifically, we propose Cross-lingual Text Alignment Regularization (CTR) to explicitly unify cross-lingual semantic representations of medical reports originating from diverse language communities. CTR is optimized through latent language disentanglement, rendering our optimizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#36827;&#34892;&#23398;&#20064;&#65292;&#20854;&#30446;&#26631;&#26159;&#23454;&#29616;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#65292;&#36890;&#36807;&#32852;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.10913</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement. (arXiv:2305.10913v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#36827;&#34892;&#23398;&#20064;&#65292;&#20854;&#30446;&#26631;&#26159;&#23454;&#29616;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#65292;&#36890;&#36807;&#32852;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#20165;&#21033;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#23398;&#20064;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#12290;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#38590;&#24230;&#26356;&#22823;&#65292;&#22240;&#20026;&#26080;&#27861;&#33719;&#24471;&#36793;&#30028;&#26694;&#21644;&#25991;&#26412;&#30701;&#35821;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#27169;&#22411;&#65288;SPRM&#65289;&#65292;&#20854;&#39044;&#27979;&#32467;&#26524;&#26159;&#36890;&#36807;&#32452;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#24471;&#21040;&#30340;&#12290;&#31532;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22359;&#26088;&#22312;&#36820;&#22238;&#25991;&#26412;&#30701;&#35821;&#21644;&#36793;&#30028;&#26694;&#20043;&#38388;&#30340;&#31895;&#30053;&#23545;&#40784;&#12290;&#31532;&#20108;&#20010;&#35757;&#32451;&#36807;&#30340;&#27169;&#22359;&#30001;&#20004;&#20010;&#23376;&#32452;&#20214;&#32452;&#25104;&#65292;&#29992;&#20110;&#32454;&#21270;&#31895;&#30053;&#30340;&#23545;&#40784;&#20197;&#25552;&#39640;&#26368;&#32456;&#30701;&#35821;-&#36793;&#30028;&#26694;&#23545;&#40784;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#35757;&#32451;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#22270;&#20687;&#21644;&#21477;&#23376;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#30456;&#20284;&#24230;&#65292;&#21516;&#26102;&#20351;&#21516;&#19968;&#21477;&#23376;&#21644;&#19968;&#20010;&#26032;&#30340;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#30456;&#20284;&#24230;&#26368;&#23567;&#21270;&#65292;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#35757;&#32451;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using only image-sentence pairs, weakly-supervised visual-textual grounding aims to learn region-phrase correspondences of the respective entity mentions. Compared to the supervised approach, learning is more difficult since bounding boxes and textual phrases correspondences are unavailable. In light of this, we propose the Semantic Prior Refinement Model (SPRM), whose predictions are obtained by combining the output of two main modules. The first untrained module aims to return a rough alignment between textual phrases and bounding boxes. The second trained module is composed of two sub-components that refine the rough alignment to improve the accuracy of the final phrase-bounding box alignments. The model is trained to maximize the multimodal similarity between an image and a sentence, while minimizing the multimodal similarity of the same sentence and a new unrelated image, carefully selected to help the most during training. Our approach shows state-of-the-art results on two popula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Di-Long&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#23398;&#29983;&#32593;&#32476;&#35266;&#23519;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#38271;&#36712;&#36857;&#65292;&#25945;&#24072;&#32593;&#32476;&#35266;&#23519;&#26356;&#38271;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#20313;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2305.08553</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#30701;&#26399;&#21040;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Distilling Knowledge for Short-to-Long Term Trajectory Prediction. (arXiv:2305.08553v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Di-Long&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#23398;&#29983;&#32593;&#32476;&#35266;&#23519;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#38271;&#36712;&#36857;&#65292;&#25945;&#24072;&#32593;&#32476;&#35266;&#23519;&#26356;&#38271;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#20313;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#22522;&#26412;&#22256;&#38590;&#22312;&#20110;&#38543;&#30528;&#26102;&#38388;&#33539;&#22260;&#30340;&#22686;&#38271;&#65292;&#36712;&#36857;&#30340;&#28436;&#21464;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Di-Long&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#32473;&#23450;&#19968;&#20010;&#21253;&#21547;&#23398;&#29983;&#32593;&#32476;&#20801;&#35768;&#30340;&#35266;&#27979;&#24207;&#21015;&#21644;&#34917;&#20805;&#30446;&#26631;&#24207;&#21015;&#30340;&#24635;&#24207;&#21015;&#38271;&#24230;&#65292;&#25105;&#20204;&#35753;&#23398;&#29983;&#21644;&#25945;&#24072;&#23545;&#21516;&#19968;&#20010;&#23436;&#25972;&#36712;&#36857;&#23450;&#20041;&#20004;&#20010;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#65306;&#23398;&#29983;&#35266;&#23519;&#19968;&#20010;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#19968;&#20010;&#38271;&#36712;&#36857;&#65292;&#32780;&#25945;&#24072;&#35266;&#23519;&#19968;&#20010;&#26356;&#38271;&#30340;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#19979;&#30340;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term trajectory forecasting is an important and challenging problem in the fields of computer vision, machine learning, and robotics. One fundamental difficulty stands in the evolution of the trajectory that becomes more and more uncertain and unpredictable as the time horizon grows, subsequently increasing the complexity of the problem. To overcome this issue, in this paper, we propose Di-Long, a new method that employs the distillation of a short-term trajectory model forecaster that guides a student network for long-term trajectory prediction during the training process. Given a total sequence length that comprehends the allowed observation for the student network and the complementary target sequence, we let the student and the teacher solve two different related tasks defined over the same full trajectory: the student observes a short sequence and predicts a long trajectory, whereas the teacher observes a longer sequence and predicts the remaining short target trajectory. The
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;</title><link>http://arxiv.org/abs/2303.07397</link><description>&lt;p&gt;
&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast exploration and learning of latent graphs with aliased observations. (arXiv:2303.07397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#36825;&#31181;&#22330;&#26223;&#65306;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#25191;&#34892;&#25805;&#20316;&#20174;&#19968;&#20010;&#33410;&#28857;&#21040;&#21478;&#19968;&#20010;&#33410;&#28857;&#26469;&#23548;&#33322;&#28508;&#22312;&#22270;&#12290;&#25152;&#36873;&#25805;&#20316;&#30830;&#23450;&#20102;&#19979;&#19968;&#20010;&#35775;&#38382;&#33410;&#28857;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#27599;&#20010;&#33410;&#28857;&#22788;&#65292;&#26234;&#33021;&#20307;&#25910;&#21040;&#19968;&#20010;&#35266;&#27979;&#65292;&#20294;&#35813;&#35266;&#27979;&#19981;&#26159;&#21807;&#19968;&#30340;&#65292;&#22240;&#27492;&#23427;&#19981;&#33021;&#21807;&#19968;&#22320;&#26631;&#35782;&#33410;&#28857;&#65292;&#36825;&#20351;&#24471;&#38382;&#39064;&#21035;&#21517;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25919;&#31574;&#65292;&#35813;&#25919;&#31574;&#32422;&#31561;&#20110;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#65288;&#21363;&#22312;&#32473;&#23450;&#30340;&#25506;&#32034;&#39044;&#31639;&#19979;&#22914;&#20309;&#24674;&#22797;&#22270;&#34920;&#65289;&#12290;&#22312;&#38750;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;&#23545;&#20110;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;&#36866;&#29992;&#30340;&#22522;&#32447;&#65292;&#32780;&#26159;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30456;&#23545;&#20110;&#38543;&#26426;&#31574;&#30053;&#26356;&#24555;&#30340;&#24674;&#22797;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24674;&#22797;&#36895;&#24230;&#27604;&#38543;&#26426;&#31574;&#30053;&#24555;&#25351;&#25968;&#20493;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#31216;&#20026; eFeX&#65288;&#26469;&#33258;&#20110; efficient exploration &#30340;&#32553;&#20889;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider this scenario: an agent navigates a latent graph by performing actions that take it from one node to another. The chosen action determines the probability distribution over the next visited node. At each node, the agent receives an observation, but this observation is not unique, so it does not identify the node, making the problem aliased. The purpose of this work is to provide a policy that approximately maximizes exploration efficiency (i.e., how well the graph is recovered for a given exploration budget). In the unaliased case, we show improved performance w.r.t. state-of-the-art reinforcement learning baselines. For the aliased case we are not aware of suitable baselines and instead show faster recovery w.r.t. a random policy for a wide variety of topologies, and exponentially faster recovery than a random policy for challenging topologies. We dub the algorithm eFeX (from eFficient eXploration).
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07122</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#37327;&#21270;&#21271;&#26497;&#25918;&#22823;&#30340;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference. (arXiv:2303.07122v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21271;&#26497;&#21464;&#26262;&#65292;&#20063;&#31216;&#21271;&#26497;&#25918;&#22823;&#65292;&#30001;&#22810;&#31181;&#22823;&#27668;&#21644;&#28023;&#27915;&#22240;&#32032;&#23548;&#33268;&#65292;&#20294;&#20854;&#22522;&#30784;&#28909;&#21147;&#22240;&#32032;&#30340;&#35814;&#32454;&#24773;&#20917;&#20173;&#19981;&#28165;&#26970;&#12290;&#20351;&#29992;&#22266;&#23450;&#27835;&#30103;&#25928;&#24212;&#31574;&#30053;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#20250;&#23548;&#33268;&#19981;&#29616;&#23454;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#26102;&#38388;&#21464;&#21270;&#30340;&#28151;&#28102;&#30340;&#24433;&#21709;&#32780;&#24341;&#36215;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TCINet - &#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#65292;&#20197;&#36830;&#32493;&#27835;&#30103;&#26041;&#24335;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#35266;&#27979;&#25968;&#25454;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#22914;&#20309;&#22823;&#22823;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The warming of the Arctic, also known as Arctic amplification, is led by several atmospheric and oceanic drivers, however, the details of its underlying thermodynamic causes are still unknown. Inferring the causal effects of atmospheric processes on sea ice melt using fixed treatment effect strategies leads to unrealistic counterfactual estimations. Such models are also prone to bias due to time-varying confoundedness. In order to tackle these challenges, we propose TCINet - time-series causal inference model to infer causation under continuous treatment using recurrent neural networks. Through experiments on synthetic and observational data, we show how our research can substantially improve the ability to quantify the leading causes of Arctic sea ice melt.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.00848</link><description>&lt;p&gt;
&#20197;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#29702;&#35299;&#25193;&#25955;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#30340;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#19988;&#36825;&#20123;&#30446;&#26631;&#37117;&#26159;&#21152;&#26435;&#25439;&#22833;&#30340;&#29305;&#20363;&#65292;&#20854;&#20013;&#21152;&#26435;&#20989;&#25968;&#25351;&#23450;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#30340;&#26435;&#37325;&#12290;&#22343;&#21248;&#21152;&#26435;&#23545;&#24212;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#21407;&#21017;&#24615;&#36817;&#20284;ELBO&#30340;&#26368;&#22823;&#21270;&#12290;&#20294;&#26159;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#26356;&#22909;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#30446;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#38750;&#22343;&#21248;&#21152;&#26435;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#65288;&#24102;&#26377;&#20219;&#20309;&#21152;&#26435;&#65289;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21487;&#20197;&#34987;&#20889;&#25104;&#19968;&#31181;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#24418;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#37117;&#26377;&#19968;&#20010;ELBO&#12290;&#22914;&#26524;&#26435;&#37325;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#65292;&#37027;&#20040;&#21152;&#26435;&#25439;&#22833;&#26159;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#30446;&#26631;&#65306;&#23427;&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#19979;&#65288;&#21363;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#65289;&#19979;&#26368;&#22823;&#21270;ELBO&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#20294;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#27604;&#36739;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#26435;&#37325;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#36890;&#36807;&#25512;&#23548;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#22238;&#31572;&#20102;&#36716;&#25442;&#30340;&#20302;&#31209;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#34892;&#20026;&#65292;&#32467;&#26524;&#26174;&#31034;&#36890;&#36807;&#31934;&#30830;&#30340;&#36716;&#25442;&#20302;&#31209;&#21442;&#25968;&#21270;&#21387;&#32553;&#30340;t-NNs&#21487;&#20197;&#23454;&#29616;&#26356;&#23574;&#38160;&#30340;&#23545;&#25239;&#27867;&#21270;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2303.00196</link><description>&lt;p&gt;
&#36716;&#25442;&#30340;&#20302;&#31209;&#21442;&#25968;&#21270;&#21487;&#20197;&#24110;&#21161;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#31283;&#20581;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks. (arXiv:2303.00196v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#36890;&#36807;&#25512;&#23548;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#22238;&#31572;&#20102;&#36716;&#25442;&#30340;&#20302;&#31209;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#34892;&#20026;&#65292;&#32467;&#26524;&#26174;&#31034;&#36890;&#36807;&#31934;&#30830;&#30340;&#36716;&#25442;&#20302;&#31209;&#21442;&#25968;&#21270;&#21387;&#32553;&#30340;t-NNs&#21487;&#20197;&#23454;&#29616;&#26356;&#23574;&#38160;&#30340;&#23545;&#25239;&#27867;&#21270;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#65292;&#23454;&#29616;&#39640;&#25928;&#19988;&#31283;&#20581;&#30340;&#22810;&#36890;&#36947;&#25968;&#25454;&#23398;&#20064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#36716;&#25442;&#22495;&#20013;&#30340;&#20302;&#31209;&#24615;&#65292;&#21363;&#36716;&#25442;&#30340;&#20302;&#31209;&#24615;&#65292;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#22312;&#22810;&#36890;&#36947;&#25968;&#25454;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#65292;&#24182;&#26368;&#36817;&#25193;&#23637;&#21040;&#20102;&#20989;&#25968;&#34920;&#31034;&#65292;&#22914;&#20855;&#26377;t-&#20056;&#31215;&#23618;&#65288;t-NNs&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;t-SVD&#29702;&#35770;&#19978;&#22914;&#20309;&#24433;&#21709;t-NNs&#30340;&#23398;&#20064;&#34892;&#20026;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#31532;&#19968;&#27425;&#36890;&#36807;&#25512;&#23548;&#26631;&#20934;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;t-NNs&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#31934;&#30830;&#30340;&#36716;&#25442;&#20302;&#31209;&#21442;&#25968;&#21270;&#21387;&#32553;&#30340;t-NNs&#21487;&#20197;&#23454;&#29616;&#26356;&#23574;&#38160;&#30340;&#23545;&#25239;&#27867;&#21270;&#19978;&#30028;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23613;&#31649;t-NNs&#24456;&#23569;&#20855;&#26377;&#23436;&#20840;&#36716;&#25442;&#30340;&#20302;&#31209;&#26435;&#37325;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#27969;&#65288;GF&#65289;&#36827;&#34892;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#36807;&#21442;&#25968;&#21270;&#30340;t-NNs&#20855;&#26377;ReLU
&lt;/p&gt;
&lt;p&gt;
Achieving efficient and robust multi-channel data learning is a challenging task in data science. By exploiting low-rankness in the transformed domain, i.e., transformed low-rankness, tensor Singular Value Decomposition (t-SVD) has achieved extensive success in multi-channel data representation and has recently been extended to function representation such as Neural Networks with t-product layers (t-NNs). However, it still remains unclear how t-SVD theoretically affects the learning behavior of t-NNs. This paper is the first to answer this question by deriving the upper bounds of the generalization error of both standard and adversarially trained t-NNs. It reveals that the t-NNs compressed by exact transformed low-rank parameterization can achieve a sharper adversarial generalization bound. In practice, although t-NNs rarely have exactly transformed low-rank weights, our analysis further shows that by adversarial training with gradient flow (GF), the over-parameterized t-NNs with ReLU 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;&#30340;&#35774;&#35745;&#65292;&#36890;&#36807;&#23545;&#26435;&#37325;&#36827;&#34892;&#32622;&#25442;&#23545;&#31216;&#24615;&#32534;&#30721;&#65292;&#23454;&#29616;&#23545;&#20854;&#20182;&#32593;&#32476;&#26435;&#37325;&#25110;&#26799;&#24230;&#36827;&#34892;&#22788;&#29702;&#65292;&#20026;&#23398;&#20064;&#20248;&#21270;&#12289;&#22788;&#29702;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#31561;&#24212;&#29992;&#25552;&#20379;&#20102;&#26550;&#26500;&#21407;&#21017;&#12290;</title><link>http://arxiv.org/abs/2302.14040</link><description>&lt;p&gt;
&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Permutation Equivariant Neural Functionals. (arXiv:2302.14040v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;&#30340;&#35774;&#35745;&#65292;&#36890;&#36807;&#23545;&#26435;&#37325;&#36827;&#34892;&#32622;&#25442;&#23545;&#31216;&#24615;&#32534;&#30721;&#65292;&#23454;&#29616;&#23545;&#20854;&#20182;&#32593;&#32476;&#26435;&#37325;&#25110;&#26799;&#24230;&#36827;&#34892;&#22788;&#29702;&#65292;&#20026;&#23398;&#20064;&#20248;&#21270;&#12289;&#22788;&#29702;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#31561;&#24212;&#29992;&#25552;&#20379;&#20102;&#26550;&#26500;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33021;&#22815;&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#25110;&#26799;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;&#65288;NFN&#65289;&#12290;&#23613;&#31649;&#20855;&#26377;&#24191;&#27867;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#21253;&#25324;&#23398;&#20064;&#20248;&#21270;&#12289;&#22788;&#29702;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#32593;&#32476;&#32534;&#36753;&#21644;&#31574;&#30053;&#35780;&#20272;&#65292;&#20294;&#35774;&#35745;&#22788;&#29702;&#20854;&#20182;&#32593;&#32476;&#26435;&#37325;&#30340;&#26377;&#25928;&#26550;&#26500;&#30340;&#32479;&#19968;&#21407;&#21017;&#24456;&#23569;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#31216;&#24615;&#30340;&#35270;&#35282;&#26469;&#35774;&#35745;&#31070;&#32463;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20851;&#27880;&#28145;&#24230;&#21069;&#39304;&#32593;&#32476;&#26435;&#37325;&#20013;&#20986;&#29616;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#22240;&#20026;&#38544;&#34255;&#23618;&#31070;&#32463;&#20803;&#27809;&#26377;&#22266;&#26377;&#39034;&#24207;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26500;&#24314;&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#36825;&#20123;&#23545;&#31216;&#24615;&#32534;&#30721;&#20026;&#24402;&#32435;&#20559;&#24046;&#12290;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#30340;&#21442;&#25968;&#26469;&#32422;&#26463;&#20026;&#32622;&#25442;&#31561;&#21464;&#30340;NF-Layers&#65288;&#31070;&#32463;&#21151;&#33021;&#23618;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies the design of neural networks that can process the weights or gradients of other neural networks, which we refer to as neural functional networks (NFNs). Despite a wide range of potential applications, including learned optimization, processing implicit neural representations, network editing, and policy evaluation, there are few unifying principles for designing effective architectures that process the weights of other networks. We approach the design of neural functionals through the lens of symmetry, in particular by focusing on the permutation symmetries that arise in the weights of deep feedforward networks because hidden layer neurons have no inherent order. We introduce a framework for building permutation equivariant neural functionals, whose architectures encode these symmetries as an inductive bias. The key building blocks of this framework are NF-Layers (neural functional layers) that we constrain to be permutation equivariant through an appropriate paramet
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Q-learning&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#65292;&#36890;&#36807;&#23558;bang-bang&#21160;&#20316;&#31163;&#25955;&#21270;&#19982;&#20540;&#20998;&#35299;&#30456;&#32467;&#21512;&#65292;&#23558;&#21333;&#26234;&#33021;&#20307;&#25511;&#21046;&#35270;&#20026;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;actor-critic&#26041;&#27861;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.12566</link><description>&lt;p&gt;
&#36890;&#36807;Q-learning&#35299;&#20915;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Continuous Control via Q-learning. (arXiv:2210.12566v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Q-learning&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#65292;&#36890;&#36807;&#23558;bang-bang&#21160;&#20316;&#31163;&#25955;&#21270;&#19982;&#20540;&#20998;&#35299;&#30456;&#32467;&#21512;&#65292;&#23558;&#21333;&#26234;&#33021;&#20307;&#25511;&#21046;&#35270;&#20026;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;actor-critic&#26041;&#27861;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35299;&#20915;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#19978;&#65292;&#20351;&#29992;actor-critic&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#31616;&#21333;&#30340;critic-only&#26041;&#27861;&#22914;Q-learning&#22312;&#28041;&#21450;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#26102;&#24212;&#29992;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;actor-critic&#26041;&#27861;&#30340;&#25104;&#26412;&#26159;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65306;&#31283;&#23450;&#24615;&#21551;&#21457;&#24335;&#12289;&#35745;&#31639;&#35201;&#27714;&#21644;&#26356;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#23545;&#28145;&#24230;Q-learning&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#30340;&#26041;&#27861;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;bang-bang&#21160;&#20316;&#31163;&#25955;&#21270;&#19982;&#20540;&#20998;&#35299;&#30456;&#32467;&#21512;&#65292;&#23558;&#21333;&#26234;&#33021;&#20307;&#25511;&#21046;&#35270;&#20026;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;critic-only&#26041;&#27861;&#22312;&#20174;&#29305;&#24449;&#25110;&#20687;&#32032;&#23398;&#20064;&#26102;&#19982;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;actor-critic&#26041;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#23558;&#21512;&#20316;MARL&#30340;&#32463;&#20856;&#36172;&#24466;&#38382;&#39064;&#25193;&#23637;&#21040;&#20102;&#25552;&#20379;&#30452;&#35266;&#24863;&#35273;&#30340;&#65292;&#23637;&#31034;&#20102;&#35299;&#32806;&#30340;critics&#22914;&#20309;&#21033;&#29992;&#29366;&#24577;&#20449;&#24687;&#21327;&#35843;&#32852;&#21512;&#20248;&#21270;&#65292;&#24182;&#34920;&#29616;&#20986;&#20102;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there has been substantial success for solving continuous control with actor-critic methods, simpler critic-only methods such as Q-learning find limited application in the associated high-dimensional action spaces. However, most actor-critic methods come at the cost of added complexity: heuristics for stabilisation, compute requirements and wider hyperparameter search spaces. We show that a simple modification of deep Q-learning largely alleviates these issues. By combining bang-bang action discretization with value decomposition, framing single-agent control as cooperative multi-agent reinforcement learning (MARL), this simple critic-only approach matches performance of state-of-the-art continuous actor-critic methods when learning from features or pixels. We extend classical bandit examples from cooperative MARL to provide intuition for how decoupled critics leverage state information to coordinate joint optimization, and demonstrate surprisingly strong performance across a var
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24179;&#34913;&#25968;&#31995;&#32479;&#30340;&#25968;&#23383;&#35745;&#31639;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#32858;&#21512;&#12290;&#36890;&#36807;&#25968;&#23383;&#30340;&#24179;&#22343;&#20540;&#35745;&#31639;&#23454;&#25968;&#21442;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;&#36991;&#20813;&#20102;&#23545;&#31934;&#30830;&#26679;&#26412;&#32423;&#26102;&#38388;&#21516;&#27493;&#12289;&#20449;&#36947;&#20272;&#35745;&#24320;&#38144;&#21644;&#20449;&#36947;&#21453;&#36716;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32858;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.07012</link><description>&lt;p&gt;
&#22522;&#20110;&#24179;&#34913;&#25968;&#31995;&#32479;&#30340;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#26080;&#32447;&#35745;&#31639;&#35774;&#35745;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Over-the-Air Computation Based on Balanced Number Systems for Federated Edge Learning. (arXiv:2210.07012v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24179;&#34913;&#25968;&#31995;&#32479;&#30340;&#25968;&#23383;&#35745;&#31639;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#32858;&#21512;&#12290;&#36890;&#36807;&#25968;&#23383;&#30340;&#24179;&#22343;&#20540;&#35745;&#31639;&#23454;&#25968;&#21442;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;&#36991;&#20813;&#20102;&#23545;&#31934;&#30830;&#26679;&#26412;&#32423;&#26102;&#38388;&#21516;&#27493;&#12289;&#20449;&#36947;&#20272;&#35745;&#24320;&#38144;&#21644;&#20449;&#36947;&#21453;&#36716;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32858;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#26080;&#32447;&#35745;&#31639;&#65288;OAC&#65289;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;FEEL&#65289;&#30340;&#36830;&#32493;&#20540;&#65288;&#27169;&#25311;&#65289;&#32858;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#22522;&#20110;&#24179;&#34913;&#25968;&#31995;&#32479;&#30340;&#25968;&#23383;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#19968;&#32452;&#23454;&#25968;&#21442;&#25968;&#30340;&#24179;&#22343;&#20540;&#12290;&#36890;&#36807;&#21033;&#29992;&#35813;&#20851;&#38190;&#23646;&#24615;&#65292;&#35813;&#26041;&#26696;&#23558;&#26412;&#22320;&#38543;&#26426;&#26799;&#24230;&#32534;&#30721;&#20026;&#19968;&#32452;&#25968;&#23383;&#12290;&#25509;&#19979;&#26469;&#65292;&#23427;&#21033;&#29992;&#25968;&#23383;&#30340;&#20540;&#30830;&#23450;&#28608;&#27963;&#30340;&#27491;&#20132;&#39057;&#20998;&#22797;&#29992;&#65288;OFDM&#65289;&#23376;&#36733;&#27874;&#30340;&#20301;&#32622;&#12290;&#35813;&#26041;&#26696;&#22312;&#36793;&#32536;&#26381;&#21153;&#22120;&#65288;ES&#65289;&#20351;&#29992;&#38750;&#30456;&#24178;&#25509;&#25910;&#22120;&#65292;&#19981;&#38656;&#35201;&#31934;&#30830;&#30340;&#26679;&#26412;&#32423;&#26102;&#38388;&#21516;&#27493;&#12289;&#20449;&#36947;&#20272;&#35745;&#24320;&#38144;&#21644;&#20449;&#36947;&#21453;&#36716;&#65292;&#24182;&#19988;&#19981;&#21033;&#29992;&#36793;&#32536;&#35774;&#22791;&#65288;EDs&#65289;&#19978;&#30340;&#39044;&#22343;&#34913;&#12290;&#25105;&#20204;&#29702;&#35770;&#20998;&#26512;&#20102;&#35813;&#26041;&#26696;&#30340;MSE&#24615;&#33021;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a digital over-the-air computation (OAC) scheme for achieving continuous-valued (analog) aggregation for federated edge learning (FEEL). We show that the average of a set of real-valued parameters can be calculated approximately by using the average of the corresponding numerals, where the numerals are obtained based on a balanced number system. By exploiting this key property, the proposed scheme encodes the local stochastic gradients into a set of numerals. Next, it determines the positions of the activated orthogonal frequency division multiplexing (OFDM) subcarriers by using the values of the numerals. To eliminate the need for precise sample-level time synchronization, channel estimation overhead, and channel inversion, the proposed scheme also uses a non-coherent receiver at the edge server (ES) and does not utilize a pre-equalization at the edge devices (EDs). We theoretically analyze the MSE performance of the proposed scheme and the convergence rate f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#30456;&#20284;&#24230;&#37327;&#26469;&#32531;&#35299;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.00755</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#27493; Q-learning &#32531;&#35299; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#31163;&#31574;&#30053;&#20559;&#24046;&#65306;&#19968;&#31181;&#26032;&#30340;&#32416;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mitigating Off-Policy Bias in Actor-Critic Methods with One-Step Q-learning: A Novel Correction Approach. (arXiv:2208.00755v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#30456;&#20284;&#24230;&#37327;&#26469;&#32531;&#35299;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#36739;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;&#23545;&#27604;&#26041;&#27861;&#65292;&#31163;&#31574;&#30053;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#20197;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#25968;&#25454;&#20351;&#29992;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#29702;&#30340;&#31574;&#30053;&#21644;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#30340;&#22522;&#26412;&#20998;&#24067;&#20043;&#38388;&#30340;&#20559;&#24046;&#22686;&#21152;&#26102;&#65292;&#31163;&#31574;&#30053;&#23398;&#20064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#24050;&#32463;&#30740;&#31350;&#20102;&#37325;&#35201;&#24615;&#37319;&#26679;&#21644;&#31163;&#31574;&#30053;&#31574;&#30053;&#26799;&#24230;&#25216;&#26415;&#26469;&#34917;&#20607;&#36825;&#31181;&#20559;&#24046;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#19968;&#31995;&#21015;&#38271;&#36712;&#36857;&#65292;&#24182;&#23548;&#33268;&#39069;&#22806;&#30340;&#38382;&#39064;&#65292;&#22914;&#28040;&#22833;/&#29190;&#28856;&#26799;&#24230;&#25110;&#25243;&#24323;&#35768;&#22810;&#26377;&#29992;&#30340;&#32463;&#39564;&#65292;&#26368;&#32456;&#22686;&#21152;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#23545;&#36830;&#32493;&#21160;&#20316;&#22495;&#25110;&#30001;&#30830;&#23450;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#30340;&#31574;&#30053;&#30340;&#27867;&#21270;&#21463;&#21040;&#20005;&#26684;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#30456;&#20284;&#24230;&#37327;&#26469;&#32531;&#35299;&#36830;&#32493;&#25511;&#21046;&#20013;&#36825;&#31181;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20943;&#36731; Actor-Critic &#26041;&#27861;&#20013;&#31163;&#25919;&#31574;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to on-policy counterparts, off-policy model-free deep reinforcement learning can improve data efficiency by repeatedly using the previously gathered data. However, off-policy learning becomes challenging when the discrepancy between the underlying distributions of the agent's policy and collected data increases. Although the well-studied importance sampling and off-policy policy gradient techniques were proposed to compensate for this discrepancy, they usually require a collection of long trajectories and induce additional problems such as vanishing/exploding gradients or discarding many useful experiences, which eventually increases the computational complexity. Moreover, their generalization to either continuous action domains or policies approximated by deterministic deep neural networks is strictly limited. To overcome these limitations, we introduce a novel policy similarity measure to mitigate the effects of such discrepancy in continuous control. Our method offers an ad
&lt;/p&gt;</description></item><item><title>OpenPodcar&#26159;&#19968;&#31181;&#24320;&#28304;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30740;&#31350;&#24179;&#21488;&#65292;&#22522;&#20110;&#24102;&#30828;&#32617;&#20195;&#27493;&#36710;&#36742;&#36827;&#34892;&#25913;&#35013;&#65292;&#25552;&#20379;&#20102;&#20302;&#25104;&#26412;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#26500;&#24314;&#35828;&#26126;&#12290;&#23427;&#20855;&#26377;&#26631;&#20934;&#30340;ROS&#25509;&#21475;&#21644;&#20223;&#30495;&#21151;&#33021;&#65292;&#20197;&#21450;&#26426;&#22120;&#20154;&#33258;&#20027;&#35268;&#21010;&#21644;&#25511;&#21046;&#21151;&#33021;&#65292;&#21487;&#20197;&#29992;&#20110;&#26368;&#21518;&#19968;&#33521;&#37324;&#20986;&#31199;&#36710;&#26381;&#21153;&#25110;&#36816;&#36755;&#31561;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2205.04454</link><description>&lt;p&gt;
OpenPodcar:&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30740;&#31350;&#30340;&#24320;&#28304;&#36710;&#36742;
&lt;/p&gt;
&lt;p&gt;
OpenPodcar: an Open Source Vehicle for Self-Driving Car Research. (arXiv:2205.04454v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.04454
&lt;/p&gt;
&lt;p&gt;
OpenPodcar&#26159;&#19968;&#31181;&#24320;&#28304;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30740;&#31350;&#24179;&#21488;&#65292;&#22522;&#20110;&#24102;&#30828;&#32617;&#20195;&#27493;&#36710;&#36742;&#36827;&#34892;&#25913;&#35013;&#65292;&#25552;&#20379;&#20102;&#20302;&#25104;&#26412;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#26500;&#24314;&#35828;&#26126;&#12290;&#23427;&#20855;&#26377;&#26631;&#20934;&#30340;ROS&#25509;&#21475;&#21644;&#20223;&#30495;&#21151;&#33021;&#65292;&#20197;&#21450;&#26426;&#22120;&#20154;&#33258;&#20027;&#35268;&#21010;&#21644;&#25511;&#21046;&#21151;&#33021;&#65292;&#21487;&#20197;&#29992;&#20110;&#26368;&#21518;&#19968;&#33521;&#37324;&#20986;&#31199;&#36710;&#26381;&#21153;&#25110;&#36816;&#36755;&#31561;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenPodcar&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#24320;&#28304;&#30828;&#20214;&#21644;&#36719;&#20214;&#30340;&#33258;&#20027;&#36710;&#36742;&#30740;&#31350;&#24179;&#21488;&#65292;&#23427;&#22522;&#20110;&#19968;&#20010;&#29616;&#25104;&#30340;&#12289;&#24102;&#26377;&#30828;&#32617;&#30340;&#20195;&#27493;&#36710;&#36742;&#36827;&#34892;&#25913;&#35013;&#12290;&#25552;&#20379;&#20102;&#30828;&#20214;&#21644;&#36719;&#20214;&#26500;&#24314;&#35828;&#26126;&#65292;&#21487;&#20197;&#23558;&#20195;&#27493;&#36710;&#36742;&#36716;&#25442;&#20026;&#20302;&#25104;&#26412;&#19988;&#23436;&#20840;&#33258;&#20027;&#30340;&#24179;&#21488;&#12290;&#24320;&#25918;&#24335;&#24179;&#21488;&#21253;&#25324;&#65288;a&#65289;&#30828;&#20214;&#32452;&#20214;&#65306;CAD&#35774;&#35745;&#12289;&#29289;&#26009;&#28165;&#21333;&#21644;&#26500;&#24314;&#35828;&#26126;&#65307;&#65288;b&#65289;Arduino&#12289;ROS&#21644;Gazebo&#25511;&#21046;&#21644;&#20223;&#30495;&#36719;&#20214;&#25991;&#20214;&#65292;&#25552;&#20379;&#26631;&#20934;&#30340;ROS&#25509;&#21475;&#21644;&#36710;&#36742;&#20223;&#30495;&#21151;&#33021;&#65307;&#65288;c&#65289;&#26356;&#39640;&#32423;&#30340;ROS&#36719;&#20214;&#23454;&#29616;&#21644;&#26631;&#20934;&#26426;&#22120;&#20154;&#33258;&#20027;&#35268;&#21010;&#21644;&#25511;&#21046;&#37197;&#32622;&#65292;&#21253;&#25324;&#20351;&#29992;Timed-Elastic-Band planner&#30340;move_base&#25509;&#21475;&#65292;&#36890;&#36807;&#32469;&#36807;&#38556;&#30861;&#29289;&#26469;&#39537;&#21160;&#36710;&#36742;&#20174;&#24403;&#21069;&#20301;&#32622;&#21040;&#30446;&#26631;&#20301;&#32622;&#12290;&#35813;&#36710;&#36742;&#36275;&#22815;&#22823;&#65292;&#21487;&#20197;&#20197;&#26368;&#39640;&#26102;&#36895;15&#20844;&#37324;&#36816;&#36865;&#20056;&#23458;&#25110;&#31867;&#20284;&#36127;&#36733;&#65292;&#20363;&#22914;&#29992;&#20316;&#26368;&#21518;&#19968;&#33521;&#37324;&#33258;&#20027;&#20986;&#31199;&#36710;&#26381;&#21153;&#25110;&#29992;&#20110;&#36816;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenPodcar is a low-cost, open source hardware and software, autonomous vehicle research platform based on an off-the-shelf, hard-canopy, mobility scooter donor vehicle. Hardware and software build instructions are provided to convert the donor vehicle into a low-cost and fully autonomous platform. The open platform consists of (a) hardware components: CAD designs, bill of materials, and build instructions; (b) Arduino, ROS and Gazebo control and simulation software files which provide standard ROS interfaces and simulation of the vehicle; and (c) higher-level ROS software implementations and configurations of standard robot autonomous planning and control, including the move_base interface with Timed-Elastic-Band planner which enacts commands to drive the vehicle from a current to a desired pose around obstacles. The vehicle is large enough to transport a human passenger or similar load at speeds up to 15km/h, for example for use as a last-mile autonomous taxi service or to transport 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#19988;&#20132;&#20114;&#24335;&#30340;&#28145;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#29273;&#40831;&#25918;&#23556;&#22270;&#20013;&#30340;&#40843;&#40831;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#36755;&#20986;&#23616;&#37096;&#34917;&#19969;&#20998;&#31867;&#27010;&#29575;&#30340;&#28909;&#22270;&#65292;&#24182;&#21487;&#26681;&#25454;&#20998;&#21106;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29992;&#25143;&#21487;&#20197;&#35299;&#37322;&#39044;&#27979;&#24182;&#19982;&#27169;&#22411;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2112.09694</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#19988;&#20132;&#20114;&#24335;&#30340;&#28145;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;&#29992;&#20110;&#29273;&#40831;&#40843;&#40831;&#22312;&#22810;&#20301;X&#20809;&#29255;&#20013;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Interactive Deep Multiple Instance Learning for Dental Caries Classification in Bitewing X-rays. (arXiv:2112.09694v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#19988;&#20132;&#20114;&#24335;&#30340;&#28145;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#29273;&#40831;&#25918;&#23556;&#22270;&#20013;&#30340;&#40843;&#40831;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#36755;&#20986;&#23616;&#37096;&#34917;&#19969;&#20998;&#31867;&#27010;&#29575;&#30340;&#28909;&#22270;&#65292;&#24182;&#21487;&#26681;&#25454;&#20998;&#21106;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29992;&#25143;&#21487;&#20197;&#35299;&#37322;&#39044;&#27979;&#24182;&#19982;&#27169;&#22411;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#31616;&#21333;&#39640;&#25928;&#30340;&#22270;&#20687;&#20998;&#31867;&#26550;&#26500;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22312;&#29273;&#31185;&#25918;&#23556;&#22270;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#40843;&#40831;&#26816;&#27979;&#20219;&#21153;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#20004;&#20010;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#21363;&#20351;&#20351;&#29992;&#24369;&#30340;&#22270;&#20687;&#32423;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#20063;&#33021;&#36755;&#20986;&#23616;&#37096;&#34917;&#19969;&#20998;&#31867;&#27010;&#29575;&#30340;&#28909;&#22270;&#12290;&#20854;&#27425;&#65292;&#23427;&#36866;&#29992;&#20110;&#20174;&#20998;&#21106;&#26631;&#31614;&#20013;&#23398;&#20064;&#20197;&#25351;&#23548;&#35757;&#32451;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#20154;&#31867;&#29992;&#25143;&#21487;&#20197;&#24544;&#23454;&#22320;&#35299;&#37322;&#39044;&#27979;&#65292;&#24182;&#19982;&#27169;&#22411;&#20132;&#20114;&#20197;&#20915;&#23450;&#35201;&#20851;&#27880;&#30340;&#21306;&#22495;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;&#32422;38k&#20010;&#22810;&#20301;X&#20809;&#29255;&#65288;&#32422;316k&#20010;&#29273;&#40831;&#65289;&#30340;&#22823;&#22411;&#20020;&#24202;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#22312;&#19982;&#21508;&#31181;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#24403;&#30001;&#22806;&#37096;&#40843;&#40831;&#20998;&#21106;&#27169;&#22411;&#25351;&#23548;&#26102;&#65292;&#35266;&#23519;&#21040;&#20998;&#31867;&#21644;&#23450;&#20301;&#24615;&#33021;&#30340;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple and efficient image classification architecture based on deep multiple instance learning, and apply it to the challenging task of caries detection in dental radiographs. Technically, our approach contributes in two ways: First, it outputs a heatmap of local patch classification probabilities despite being trained with weak image-level labels. Second, it is amenable to learning from segmentation labels to guide training. In contrast to existing methods, the human user can faithfully interpret predictions and interact with the model to decide which regions to attend to. Experiments are conducted on a large clinical dataset of $\sim$38k bitewings ($\sim$316k teeth), where we achieve competitive performance compared to various baselines. When guided by an external caries segmentation model, a significant improvement in classification and localization performance is observed.
&lt;/p&gt;</description></item><item><title>&#36335;&#24452;&#27491;&#21017;&#21270;&#20026;&#24182;&#34892;ReLU&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#32676;&#31232;&#30095;&#24615;&#24341;&#23548;&#23454;&#29616;&#20102;&#20984;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#31639;&#27861;&#65292;&#22312;&#25152;&#26377;&#25968;&#25454;&#32500;&#24230;&#19978;&#20855;&#22791;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2110.09548</link><description>&lt;p&gt;
&#36335;&#24452;&#27491;&#21017;&#21270;&#65306;&#19968;&#31181;&#23545;&#24182;&#34892;ReLU&#32593;&#32476;&#36827;&#34892;&#20984;&#24615;&#21644;&#31232;&#30095;&#24615;&#24341;&#23548;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks. (arXiv:2110.09548v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.09548
&lt;/p&gt;
&lt;p&gt;
&#36335;&#24452;&#27491;&#21017;&#21270;&#20026;&#24182;&#34892;ReLU&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#32676;&#31232;&#30095;&#24615;&#24341;&#23548;&#23454;&#29616;&#20102;&#20984;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#31639;&#27861;&#65292;&#22312;&#25152;&#26377;&#25968;&#25454;&#32500;&#24230;&#19978;&#20855;&#22791;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25104;&#21151;&#32972;&#21518;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#24403;&#21069;&#25991;&#29486;&#20013;&#26368;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#20043;&#19968;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#26469;&#25581;&#31034;&#20248;&#21270;&#26223;&#35266;&#20013;&#38544;&#34255;&#30340;&#20984;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#28145;&#24230;&#24182;&#34892;ReLU&#32593;&#32476;&#26550;&#26500;&#65292;&#20854;&#20063;&#21253;&#25324;&#26631;&#20934;&#30340;&#28145;&#24230;&#32593;&#32476;&#21644;ResNet&#20316;&#20026;&#20854;&#29305;&#20363;&#12290;&#28982;&#21518;&#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;&#36335;&#24452;&#27491;&#21017;&#21270;&#30340;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#20010;&#31934;&#30830;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#31561;&#20215;&#30340;&#20984;&#38382;&#39064;&#26159;&#36890;&#36807;&#19968;&#31181;&#32676;&#31232;&#30095;&#24615;&#24341;&#23548;&#30340;&#35268;&#33539;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#12290;&#22240;&#27492;&#65292;&#36335;&#24452;&#27491;&#21017;&#21270;&#30340;&#24182;&#34892;ReLU&#32593;&#32476;&#21487;&#20197;&#34987;&#35270;&#20026;&#39640;&#32500;&#20013;&#19968;&#31181;&#31616;&#21270;&#30340;&#20984;&#27169;&#22411;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#30001;&#20110;&#21407;&#22987;&#30340;&#35757;&#32451;&#38382;&#39064;&#21487;&#33021;&#26080;&#27861;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#32500;&#24230;&#19978;&#20855;&#26377;&#23436;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24378;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the fundamental principles behind the success of deep neural networks is one of the most important open questions in the current literature. To this end, we study the training problem of deep neural networks and introduce an analytic approach to unveil hidden convexity in the optimization landscape. We consider a deep parallel ReLU network architecture, which also includes standard deep networks and ResNets as its special cases. We then show that pathwise regularized training problems can be represented as an exact convex optimization problem. We further prove that the equivalent convex problem is regularized via a group sparsity inducing norm. Thus, a path regularized parallel ReLU network can be viewed as a parsimonious convex model in high dimensions. More importantly, since the original training problem may not be trainable in polynomial-time, we propose an approximate algorithm with a fully polynomial-time complexity in all data dimensions. Then, we prove strong glob
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20844;&#24335;&#65292;&#31216;&#20026;&#28145;&#24230;&#29983;&#23384;&#21058;&#37327;&#21453;&#24212;&#20989;&#25968;&#65288;DeepSDRF&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#29983;&#23384;&#25968;&#25454;&#20013;&#30340;&#36830;&#32493;&#27835;&#30103;&#25512;&#33616;&#38382;&#39064;&#12290;&#36890;&#36807;&#26657;&#27491;&#36873;&#25321;&#20559;&#24046;&#65292;DeepSDRF&#20272;&#35745;&#30340;&#27835;&#30103;&#25928;&#26524;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#25512;&#33616;&#31639;&#27861;&#12290;&#22312;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#21307;&#23398;&#25968;&#25454;&#24211;&#19978;&#30340;&#27979;&#35797;&#20013;&#65292;DeepSDRF&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2108.10453</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#23384;&#21058;&#37327;&#21453;&#24212;&#20989;&#25968;&#30340;&#36830;&#32493;&#27835;&#30103;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Continuous Treatment Recommendation with Deep Survival Dose Response Function. (arXiv:2108.10453v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.10453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20844;&#24335;&#65292;&#31216;&#20026;&#28145;&#24230;&#29983;&#23384;&#21058;&#37327;&#21453;&#24212;&#20989;&#25968;&#65288;DeepSDRF&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#29983;&#23384;&#25968;&#25454;&#20013;&#30340;&#36830;&#32493;&#27835;&#30103;&#25512;&#33616;&#38382;&#39064;&#12290;&#36890;&#36807;&#26657;&#27491;&#36873;&#25321;&#20559;&#24046;&#65292;DeepSDRF&#20272;&#35745;&#30340;&#27835;&#30103;&#25928;&#26524;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#25512;&#33616;&#31639;&#27861;&#12290;&#22312;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#21307;&#23398;&#25968;&#25454;&#24211;&#19978;&#30340;&#27979;&#35797;&#20013;&#65292;DeepSDRF&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#20020;&#24202;&#29983;&#23384;&#25968;&#25454;&#35774;&#32622;&#20013;&#30340;&#36830;&#32493;&#27835;&#30103;&#25512;&#33616;&#38382;&#39064;&#30340;&#36890;&#29992;&#20844;&#24335;&#65292;&#31216;&#20026;&#28145;&#24230;&#29983;&#23384;&#21058;&#37327;&#21453;&#24212;&#20989;&#25968;&#65288;DeepSDRF&#65289;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20174;&#21382;&#21490;&#25968;&#25454;&#20013;&#20165;&#20165;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#22240;&#32032;&#65288;&#28151;&#26434;&#22240;&#23376;&#65289;&#23545;&#35266;&#23519;&#21040;&#30340;&#27835;&#30103;&#21644;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#32467;&#26524;&#37117;&#26377;&#24433;&#21709;&#30340;&#26465;&#20214;&#24179;&#22343;&#21058;&#37327;&#21453;&#24212;&#65288;CADR&#65289;&#20989;&#25968;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#20174;DeepSDRF&#20013;&#20272;&#35745;&#30340;&#27835;&#30103;&#25928;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#20855;&#26377;&#36873;&#25321;&#20559;&#24046;&#26657;&#27491;&#30340;&#25512;&#33616;&#31639;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20004;&#31181;&#25512;&#33616;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#24739;&#32773;&#32467;&#26524;&#26041;&#38754;&#34920;&#29616;&#30456;&#20284;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;eICU&#30740;&#31350;&#26426;&#26500;&#65288;eRI&#65289;&#25968;&#25454;&#24211;&#19978;&#27979;&#35797;&#20102;DeepSDRF&#21644;&#30456;&#24212;&#30340;&#25512;&#33616;&#22120;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#22312;&#21307;&#23398;&#32972;&#26223;&#19979;&#20351;&#29992;&#22240;&#26524;&#27169;&#22411;&#26469;&#35299;&#20915;&#35266;&#23519;&#25968;&#25454;&#20013;&#30340;&#36830;&#32493;&#27835;&#30103;&#25928;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a general formulation for continuous treatment recommendation problems in settings with clinical survival data, which we call the Deep Survival Dose Response Function (DeepSDRF). That is, we consider the problem of learning the conditional average dose response (CADR) function solely from historical data in which observed factors (confounders) affect both observed treatment and time-to-event outcomes. The estimated treatment effect from DeepSDRF enables us to develop recommender algorithms with the correction for selection bias. We compared two recommender approaches based on random search and reinforcement learning and found similar performance in terms of patient outcome. We tested the DeepSDRF and the corresponding recommender on extensive simulation studies and the eICU Research Institute (eRI) database. To the best of our knowledge, this is the first time that causal models are used to address the continuous treatment effect with observational data in a medical context.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#21382;&#21490;&#39550;&#39542;&#32463;&#39564;&#30340;&#20154;&#31867;&#21270;&#33021;&#37327;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#39550;&#39542;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#28151;&#21512;&#21160;&#21147;&#30005;&#21160;&#36710;&#33021;&#37327;&#31649;&#29702;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2007.10126</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#21382;&#21490;&#39550;&#39542;&#32463;&#39564;&#30340;&#20154;&#31867;&#21270;&#33021;&#37327;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Human-like Energy Management Based on Deep Reinforcement Learning and Historical Driving Experiences. (arXiv:2007.10126v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.10126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#21382;&#21490;&#39550;&#39542;&#32463;&#39564;&#30340;&#20154;&#31867;&#21270;&#33021;&#37327;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#39550;&#39542;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#28151;&#21512;&#21160;&#21147;&#30005;&#21160;&#36710;&#33021;&#37327;&#31649;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#21160;&#21147;&#30005;&#21160;&#36710;&#30340;&#21457;&#23637;&#20381;&#36182;&#20110;&#20808;&#36827;&#39640;&#25928;&#30340;&#33021;&#37327;&#31649;&#29702;&#31574;&#30053;&#65288;EMS&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21644;&#37319;&#38598;&#30340;&#21382;&#21490;&#39550;&#39542;&#25968;&#25454;&#30340;&#20154;&#31867;&#21270;&#33021;&#37327;&#31649;&#29702;&#26694;&#26550;&#12290;&#35813;&#30740;&#31350;&#20013;&#30340;&#28151;&#21512;&#21160;&#21147;&#20256;&#21160;&#31995;&#32479;&#37319;&#29992;&#20018;&#32852;-&#24182;&#32852;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#39318;&#20808;&#24314;&#31435;&#20102;&#38754;&#21521;&#25511;&#21046;&#30340;&#27169;&#22411;&#12290;&#28982;&#21518;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#12290;&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#21151;&#29575;&#20998;&#37197;&#25511;&#21046;&#65292;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#33719;&#24471;&#30340;&#20840;&#23616;&#26368;&#20248;&#25511;&#21046;&#36712;&#36857;&#20316;&#20026;&#19987;&#23478;&#30693;&#35782;&#26469;&#35757;&#32451;DDPG&#27169;&#22411;&#12290;&#36825;&#20010;&#25805;&#20316;&#30830;&#20445;&#20102;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#26550;&#26500;&#30340;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#22522;&#20110;&#26377;&#32463;&#39564;&#39550;&#39542;&#21592;&#30340;&#37319;&#38598;&#30340;&#21382;&#21490;&#39550;&#39542;&#25968;&#25454;&#26469;&#26367;&#20195;&#22522;&#20110;DP&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#21319;&#33021;&#37327;&#31649;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Development of hybrid electric vehicles depends on an advanced and efficient energy management strategy (EMS). With online and real-time requirements in mind, this article presents a human-like energy management framework for hybrid electric vehicles according to deep reinforcement learning methods and collected historical driving data. The hybrid powertrain studied has a series-parallel topology, and its control-oriented modeling is founded first. Then, the distinctive deep reinforcement learning (DRL) algorithm, named deep deterministic policy gradient (DDPG), is introduced. To enhance the derived power split controls in the DRL framework, the global optimal control trajectories obtained from dynamic programming (DP) are regarded as expert knowledge to train the DDPG model. This operation guarantees the optimality of the proposed control architecture. Moreover, the collected historical driving data based on experienced drivers are employed to replace the DP-based controls, and thus c
&lt;/p&gt;</description></item></channel></rss>