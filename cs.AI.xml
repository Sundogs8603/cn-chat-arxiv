<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05935</link><description>&lt;p&gt;
SPHINX-X: &#25193;&#23637;&#25968;&#25454;&#21644;&#21442;&#25968;&#29992;&#20110;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;SPHINX-X&#65292;&#19968;&#31181;&#22522;&#20110;SPHINX&#24320;&#21457;&#30340;&#24191;&#27867;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#31995;&#21015;&#12290;&#20026;&#20102;&#25913;&#21892;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#31227;&#38500;&#20887;&#20313;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#32469;&#36807;&#23436;&#20840;&#22635;&#20805;&#30340;&#23376;&#22270;&#20687;&#65292;&#24182;&#23558;&#22810;&#38454;&#27573;&#35757;&#32451;&#31616;&#21270;&#25104;&#20026;&#19968;&#38454;&#27573;&#30340;&#20840;&#38598;&#21512;&#27169;&#24335;&#65292;&#20462;&#25913;&#20102;SPHINX&#26694;&#26550;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;MLLM&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#32452;&#35013;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#36328;&#35821;&#35328;&#12289;&#36328;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#30340;&#22810;&#39046;&#22495;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#30340;OCR&#23494;&#38598;&#21644;Mark&#25968;&#25454;&#38598;&#20016;&#23500;&#36825;&#20010;&#25910;&#38598;&#65292;&#25193;&#23637;&#20102;&#22810;&#26679;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#22522;&#30784;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;TinyLlama1.1B&#12289;InternLM2-7B&#12289;LLaMA2-13B&#21644;Mixtral8x7B&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#21464;&#21270;&#30340;MLLMs&#12290;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#24615;&#33021;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#22312;&#39057;&#22495;&#20013;&#30340;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#25552;&#20986;&#20102;&#39057;&#22495;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#32463;&#20856;&#30340;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2402.05933</link><description>&lt;p&gt;
&#39057;&#22495;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Time Series Diffusion in the Frequency Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#22312;&#39057;&#22495;&#20013;&#30340;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#25552;&#20986;&#20102;&#39057;&#22495;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#32463;&#20856;&#30340;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20613;&#37324;&#21494;&#20998;&#26512;&#22312;&#20449;&#21495;&#22788;&#29702;&#30340;&#21457;&#23637;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36825;&#20351;&#25105;&#20204;&#24819;&#30693;&#36947;&#36825;&#20010;&#26694;&#26550;&#26159;&#21542;&#33021;&#22815;&#21516;&#26679;&#26377;&#30410;&#20110;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#27169;&#22411;&#30340;&#33539;&#22260;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#39057;&#22495;&#20013;&#34920;&#31034;&#26102;&#38388;&#24207;&#21015;&#26159;&#21542;&#23545;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36890;&#36807;&#20174;&#26102;&#38388;&#22495;&#20013;&#25193;&#25955;&#30340;&#32463;&#20856;SDE&#20844;&#24335;&#20986;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39057;&#22495;&#20013;&#21457;&#29983;&#20102;&#19968;&#31181;&#21452;&#37325;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#20855;&#26377;&#19968;&#20010;&#37325;&#35201;&#30340;&#32454;&#24494;&#24046;&#21035;&#65306;&#24067;&#26391;&#36816;&#21160;&#34987;&#25105;&#20204;&#31216;&#20043;&#20026;&#38236;&#20687;&#24067;&#26391;&#36816;&#21160;&#25152;&#21462;&#20195;&#65292;&#20854;&#29305;&#24449;&#26159;&#20854;&#32452;&#20998;&#20043;&#38388;&#30340;&#38236;&#20687;&#23545;&#31216;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36866;&#24212;&#21435;&#22122;&#35780;&#20998;&#21305;&#37197;&#26041;&#27861;&#26469;&#23454;&#29616;&#39057;&#22495;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#36825;&#23548;&#33268;&#20102;&#39057;&#22495;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#32463;&#20856;&#30340;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#24037;&#20316;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fourier analysis has been an instrumental tool in the development of signal processing. This leads us to wonder whether this framework could similarly benefit generative modelling. In this paper, we explore this question through the scope of time series diffusion models. More specifically, we analyze whether representing time series in the frequency domain is a useful inductive bias for score-based diffusion models. By starting from the canonical SDE formulation of diffusion in the time domain, we show that a dual diffusion process occurs in the frequency domain with an important nuance: Brownian motions are replaced by what we call mirrored Brownian motions, characterized by mirror symmetries among their components. Building on this insight, we show how to adapt the denoising score matching approach to implement diffusion models in the frequency domain. This results in frequency diffusion models, which we compare to canonical time diffusion models. Our empirical evaluation on real-wor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LLaDA&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#20351;&#24471;&#39550;&#39542;&#21592;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33021;&#22815;&#22312;&#21508;&#22320;&#39550;&#39542;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21644;&#36816;&#21160;&#35745;&#21010;&#35843;&#25972;&#21040;&#26032;&#20301;&#32622;&#30340;&#20132;&#36890;&#35268;&#21017;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;LLaDA&#25351;&#23548;&#22312;&#35299;&#20915;&#24847;&#22806;&#24773;&#20917;&#21644;&#36866;&#24212;AV&#36816;&#21160;&#35745;&#21010;&#31574;&#30053;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05932</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25919;&#31574;&#36866;&#24212;&#22312;&#21508;&#22788;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Driving Everywhere with Large Language Model Policy Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LLaDA&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#20351;&#24471;&#39550;&#39542;&#21592;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33021;&#22815;&#22312;&#21508;&#22320;&#39550;&#39542;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21644;&#36816;&#21160;&#35745;&#21010;&#35843;&#25972;&#21040;&#26032;&#20301;&#32622;&#30340;&#20132;&#36890;&#35268;&#21017;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;LLaDA&#25351;&#23548;&#22312;&#35299;&#20915;&#24847;&#22806;&#24773;&#20917;&#21644;&#36866;&#24212;AV&#36816;&#21160;&#35745;&#21010;&#31574;&#30053;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#23558;&#39550;&#39542;&#34892;&#20026;&#36866;&#24212;&#26032;&#29615;&#22659;&#12289;&#20064;&#20439;&#21644;&#27861;&#24459;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;(AVs)&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LLaDA&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20351;&#20154;&#31867;&#39550;&#39542;&#21592;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#37117;&#33021;&#22312;&#21508;&#22788;&#34892;&#39542;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21644;&#36816;&#21160;&#35745;&#21010;&#35843;&#25972;&#21040;&#26032;&#20301;&#32622;&#30340;&#20132;&#36890;&#35268;&#21017;&#12290;LLaDA&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#37322;&#26412;&#22320;&#39550;&#39542;&#25163;&#20876;&#20013;&#30340;&#20132;&#36890;&#35268;&#21017;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLaDA&#30340;&#25351;&#23548;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24847;&#22806;&#24773;&#20917;&#26102;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLaDA&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#35843;&#25972;AV&#36816;&#21160;&#35745;&#21010;&#31574;&#30053;&#30340;&#33021;&#21147;&#65307;LLaDA&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#20248;&#20110;&#22522;&#32447;&#35268;&#21010;&#26041;&#27861;&#12290;&#35831;&#35775;&#38382;&#25105;&#20204;&#30340;&#32593;&#31449;&#20102;&#35299;&#26356;&#22810;&#35814;&#32454;&#20449;&#24687;&#65306;https://boyiliee.github.io/llada.
&lt;/p&gt;
&lt;p&gt;
Adapting driving behavior to new environments, customs, and laws is a long-standing problem in autonomous driving, precluding the widespread deployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a simple yet powerful tool that enables human drivers and autonomous vehicles alike to drive everywhere by adapting their tasks and motion plans to traffic rules in new locations. LLaDA achieves this by leveraging the impressive zero-shot generalizability of large language models (LLMs) in interpreting the traffic rules in the local driver handbook. Through an extensive user study, we show that LLaDA's instructions are useful in disambiguating in-the-wild unexpected situations. We also demonstrate LLaDA's ability to adapt AV motion planning policies in real-world datasets; LLaDA outperforms baseline planning approaches on all our metrics. Please check our website for more details: https://boyiliee.github.io/llada.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#33021;&#22815;&#36328;&#39046;&#22495;&#12289;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#23637;&#29616;&#20986;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#19988;&#22312;&#26426;&#22120;&#20154;&#12289;&#28216;&#25103; AI &#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.05929</link><description>&lt;p&gt;
&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Interactive Agent Foundation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05929
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#33021;&#22815;&#36328;&#39046;&#22495;&#12289;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#23637;&#29616;&#20986;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#19988;&#22312;&#26426;&#22120;&#20154;&#12289;&#28216;&#25103; AI &#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#27491;&#22312;&#20174;&#21019;&#24314;&#38745;&#24577;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#36716;&#21464;&#20026;&#33021;&#22815;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#21160;&#24577;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#26234;&#33021;&#20307;&#35757;&#32451;&#33539;&#24335;&#65292;&#29992;&#20110;&#35757;&#32451;&#36328;&#39046;&#22495;&#12289;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#30340; AI &#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#33539;&#24335;&#32479;&#19968;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#35270;&#35273;&#36974;&#25377;&#33258;&#32534;&#30721;&#22120;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#19968;&#27493;&#34892;&#21160;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#36890;&#29992;&#32780;&#36866;&#24212;&#24615;&#24378;&#30340; AI &#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#29420;&#31435;&#39046;&#22495; - &#26426;&#22120;&#20154;&#12289;&#28216;&#25103; AI &#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27599;&#20010;&#39046;&#22495;&#37117;&#23637;&#31034;&#20102;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#24191;&#27867;&#24615;&#65292;&#21033;&#29992;&#20102;&#21508;&#31181;&#25968;&#25454;&#28304;&#65292;&#22914;&#26426;&#22120;&#20154;&#24207;&#21015;&#12289;&#28216;&#25103;&#25968;&#25454;&#12289;&#22823;&#35268;&#27169;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. We propose an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. Our training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. We demonstrate the performance of our framework across three separate domains -- Robotics, Gaming AI, and Healthcare. Our model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of our approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#32858;&#21512;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#39118;&#38505;&#25935;&#24863;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#20351;&#29992;&#20102;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#20316;&#20026;&#39118;&#38505;&#24230;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23884;&#22871;CPT-AC&#31639;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;&#20110;&#29702;&#35299;&#20154;&#31867;&#30340;&#25439;&#22833;&#35268;&#36991;&#21644;&#23545;&#27010;&#29575;&#30340;&#39640;&#20272;/&#20302;&#20272;&#20542;&#21521;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.05906</link><description>&lt;p&gt;
&#32593;&#32476;&#32858;&#21512;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#39118;&#38505;&#25935;&#24863;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#32858;&#21512;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#39118;&#38505;&#25935;&#24863;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#20351;&#29992;&#20102;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#20316;&#20026;&#39118;&#38505;&#24230;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23884;&#22871;CPT-AC&#31639;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;&#20110;&#29702;&#35299;&#20154;&#31867;&#30340;&#25439;&#22833;&#35268;&#36991;&#21644;&#23545;&#27010;&#29575;&#30340;&#39640;&#20272;/&#20302;&#20272;&#20542;&#21521;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20551;&#35774;&#26234;&#33021;&#20307;&#23545;&#39118;&#38505;&#20013;&#24615;&#24182;&#20855;&#26377;&#23436;&#20840;&#23458;&#35266;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#26234;&#33021;&#20307;&#38656;&#35201;&#32771;&#34385;&#25110;&#24314;&#27169;&#20154;&#31867;&#32463;&#27982;&#25110;&#31038;&#20250;&#20559;&#22909;&#30340;&#24773;&#26223;&#20013;&#65292;&#24517;&#39035;&#23558;&#39118;&#38505;&#27010;&#24565;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#22312;&#20854;&#20182;&#20154;&#31867;&#25110;&#38750;&#20154;&#31867;&#26234;&#33021;&#20307;&#21442;&#19982;&#65292;&#21487;&#33021;&#20855;&#26377;&#20854;&#33258;&#24049;&#30340;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#30340;MARL&#20013;&#65292;&#36825;&#23558;&#26356;&#21152;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#65288;CPT&#65289;&#30340;&#39118;&#38505;&#25935;&#24863;&#21644;&#38750;&#21512;&#20316;MARL&#65292;CPT&#26159;&#19968;&#31181;&#38750;&#20984;&#39118;&#38505;&#24230;&#37327;&#65292;&#24182;&#19988;&#26159;&#39118;&#38505;&#21327;&#21516;&#24230;&#37327;&#30340;&#25193;&#23637;&#12290;CPT&#33021;&#22815;&#35299;&#37322;&#20154;&#31867;&#30340;&#25439;&#22833;&#35268;&#36991;&#21644;&#20182;&#20204;&#23545;&#23567;&#27010;&#29575;/&#22823;&#27010;&#29575;&#30340;&#39640;&#20272;/&#20302;&#20272;&#20542;&#21521;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;CPT&#39118;&#38505;&#30340;&#20998;&#24067;&#24335;&#22522;&#20110;&#37319;&#26679;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;AC&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#32593;&#32476;&#32858;&#21512;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65288;NAMGs&#65289;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20998;&#24067;&#24335;&#23884;&#22871;CPT-AC&#12290;&#22312;&#19968;&#31995;&#21015;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#25910;&#25947;&#21040;&#19968;&#31181;&#20027;&#35266;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical multi-agent reinforcement learning (MARL) assumes risk neutrality and complete objectivity for agents. However, in settings where agents need to consider or model human economic or social preferences, a notion of risk must be incorporated into the RL optimization problem. This will be of greater importance in MARL where other human or non-human agents are involved, possibly with their own risk-sensitive policies. In this work, we consider risk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT), a non-convex risk measure and a generalization of coherent measures of risk. CPT is capable of explaining loss aversion in humans and their tendency to overestimate/underestimate small/large probabilities. We propose a distributed sampling-based actor-critic (AC) algorithm with CPT risk for network aggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC. Under a set of assumptions, we prove the convergence of the algorithm to a subjective notion 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ClickSAM&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;Segment Anything Model&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#22122;&#22768;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05902</link><description>&lt;p&gt;
&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#36827;&#34892;&#31934;&#35843;&#30340;Segment Anything Model&#65288;SAM&#65289;
&lt;/p&gt;
&lt;p&gt;
ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ClickSAM&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;Segment Anything Model&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#22122;&#22768;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#12289;&#22810;&#26679;&#30340;&#36755;&#20837;&#25552;&#31034;&#12289;&#35757;&#32451;&#33021;&#21147;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#35774;&#35745;&#65292;&#26032;&#21457;&#24067;&#30340;Segment Anything Model&#65288;SAM&#65289;&#25104;&#20026;&#22270;&#20687;&#22788;&#29702;&#20013;&#27969;&#34892;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;SAM&#24403;&#21069;&#30340;&#27169;&#22411;&#26159;&#22312;&#19968;&#20010;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#38598;&#24182;&#27809;&#26377;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#65292;&#23588;&#20854;&#26159;&#36229;&#22768;&#22270;&#20687;&#12290;&#36229;&#22768;&#22270;&#20687;&#24448;&#24448;&#26377;&#24456;&#22810;&#22122;&#22768;&#65292;&#36825;&#20351;&#24471;&#20998;&#21106;&#37325;&#35201;&#32467;&#26500;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ClickSAM&#65292;&#23427;&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;Segment Anything Model&#30340;&#31934;&#32454;&#35843;&#25972;&#12290;ClickSAM&#26377;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#65306;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#20301;&#20110;&#30495;&#23454;&#36718;&#24275;&#20013;&#24515;&#30340;&#21333;&#20987;&#25552;&#31034;&#36827;&#34892;&#35757;&#32451;&#65292;&#31532;&#20108;&#38454;&#27573;&#36890;&#36807;&#39069;&#22806;&#30340;&#27491;&#36127;&#28857;&#20987;&#25552;&#31034;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#31532;&#19968;&#38454;&#27573;&#30340;&#39044;&#27979;&#19982;&#30495;&#23454;&#25513;&#33180;&#36827;&#34892;&#27604;&#36739;&#65292;&#35745;&#31639;&#20986;&#30495;&#27491;&#27491;&#12289;&#20551;&#27491;&#21644;&#20551;&#36127;&#27573;&#12290;&#27491;&#28857;&#20987;&#20351;&#29992;&#30495;&#23454;&#25513;&#33180;&#20013;&#30340;&#30495;&#23454;
&lt;/p&gt;
&lt;p&gt;
The newly released Segment Anything Model (SAM) is a popular tool used in image processing due to its superior segmentation accuracy, variety of input prompts, training capabilities, and efficient model design. However, its current model is trained on a diverse dataset not tailored to medical images, particularly ultrasound images. Ultrasound images tend to have a lot of noise, making it difficult to segment out important structures. In this project, we developed ClickSAM, which fine-tunes the Segment Anything Model using click prompts for ultrasound images. ClickSAM has two stages of training: the first stage is trained on single-click prompts centered in the ground-truth contours, and the second stage focuses on improving the model performance through additional positive and negative click prompts. By comparing the first stage predictions to the ground-truth masks, true positive, false positive, and false negative segments are calculated. Positive clicks are generated using the true 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.05894</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#36935;&#35265;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Meets Graph Neural Network in Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#26399;&#23398;&#26415;&#30028;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#65288;TAG&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#28508;&#21147;&#26377;&#25152;&#25259;&#38706;&#65292;&#20294;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#21463;&#21040;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#39640;&#65292;&#25512;&#29702;&#36807;&#31243;&#20013;&#24310;&#36831;&#38271;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34429;&#28982;&#36731;&#37327;&#19988;&#25797;&#38271;&#23398;&#20064;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#20294;&#23545;&#20110;&#30495;&#23454;&#24212;&#29992;&#20013;TAG&#22797;&#26434;&#35821;&#20041;&#30340;&#25226;&#25569;&#26377;&#25152;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;TAG&#20013;&#33410;&#28857;&#20998;&#31867;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#35328;&#22270;&#30693;&#35782;&#33976;&#39311;&#65288;LinguGKD&#65289;&#65292;&#20351;&#29992;LLMs&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#65292;GNNs&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#20854;&#20013;&#21253;&#25324;&#23545;LLM&#36827;&#34892;TAG&#23450;&#21521;&#25351;&#23548;&#35843;&#25972;&#20197;&#24212;&#23545;&#35774;&#35745;&#30340;&#33410;&#28857;&#20998;&#31867;&#25552;&#31034;&#65292;&#28982;&#21518;&#23545;&#23618;&#27425;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent community revelations about the advancements and potential of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the deployment of LLMs for production is hindered by their high computational and storage requirements, as well as long latencies during inference. Simultaneously, although traditional Graph Neural Networks (GNNs) are light weight and adept at learning structural features of graphs, their ability to grasp the complex semantics in TAGs is somewhat constrained for real applications. To address these limitations, we concentrate on the downstream task of node classification in TAG and propose a novel graph knowledge distillation framework, termed Linguistic Graph Knowledge Distillation (LinguGKD), using LLMs as teacher models and GNNs as student models for knowledge distillation. It involves TAG-oriented instruction tuning of LLM on designed node classification prompts, followed by aligning the hierarchically learned node features of the t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.05889</link><description>&lt;p&gt;
CREMA: &#36890;&#36807;&#26377;&#25928;&#30340;&#27169;&#22359;&#21270;&#36866;&#24212;&#21644;&#34701;&#21512;&#36827;&#34892;&#22810;&#27169;&#24577;&#32452;&#21512;&#35270;&#39057;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#22788;&#29702;&#22266;&#23450;&#27169;&#24577;&#36755;&#20837;&#24182;&#26356;&#26032;&#35768;&#22810;&#27169;&#22411;&#21442;&#25968;&#65292;&#20173;&#28982;&#23384;&#22312;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;CREMA&#65292;&#19968;&#31181;&#29992;&#20110;&#23558;&#20219;&#20309;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65288;&#22914;&#20809;&#27969;&#12289;3D&#28857;&#20113;&#12289;&#38899;&#39057;&#65289;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26597;&#35810;&#36716;&#25442;&#22120;&#65292;&#35813;&#36716;&#25442;&#22120;&#19982;&#27599;&#20010;&#21487;&#20197;&#35775;&#38382;&#30340;&#27169;&#24577;&#30456;&#20851;&#32852;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22359;&#12290;&#23427;&#23558;&#22810;&#31181;&#27169;&#24577;&#29305;&#24449;&#25237;&#24433;&#21040;LLM&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#25968;&#25454;&#31867;&#22411;&#20197;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#27169;&#22359;&#65292;&#29992;&#20110;&#21387;&#32553;&#22810;&#27169;&#24577;&#26597;&#35810;&#65292;&#22312;LLM&#20013;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#36827;&#34892;&#34701;&#21512;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive advancements in multimodal compositional reasoning approaches, they are still limited in their flexibility and efficiency by processing fixed modality inputs while updating a lot of model parameters. This paper tackles these critical challenges and proposes CREMA, an efficient and modular modality-fusion framework for injecting any new modality into video reasoning. We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio) from given videos without extra human annotation by leveraging existing pre-trained models. Next, we introduce a query transformer with multiple parameter-efficient modules associated with each accessible modality. It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation. Furthermore, we propose a fusion module designed to compress multimodal queries, maintaining computational efficiency in the LLM while combining additio
&lt;/p&gt;</description></item><item><title>LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#22686;&#21152;&#20102;&#36873;&#25321;&#24615;&#26333;&#20809;&#65292;&#19988;&#25903;&#25345;&#29992;&#25143;&#35266;&#28857;&#30340;&#26377;&#20559;&#35265;LLM&#21152;&#21095;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.05880</link><description>&lt;p&gt;
&#29983;&#25104;&#24615;&#22238;&#38899;&#23460;&#65311;LLM&#39537;&#21160;&#30340;&#25628;&#32034;&#31995;&#32479;&#23545;&#22810;&#26679;&#21270;&#20449;&#24687;&#25628;&#32034;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05880
&lt;/p&gt;
&lt;p&gt;
LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#22686;&#21152;&#20102;&#36873;&#25321;&#24615;&#26333;&#20809;&#65292;&#19988;&#25903;&#25345;&#29992;&#25143;&#35266;&#28857;&#30340;&#26377;&#20559;&#35265;LLM&#21152;&#21095;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20159;&#20154;&#24050;&#32463;&#20351;&#29992;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#65292;&#24182;&#19988;&#30456;&#20449;&#36825;&#20123;&#31995;&#32479;&#30456;&#27604;&#20256;&#32479;&#25628;&#32034;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#21644;&#20844;&#20849;&#35752;&#35770;&#37117;&#35843;&#26597;&#20102;&#25628;&#32034;&#31995;&#32479;&#22312;&#22686;&#21152;&#36873;&#25321;&#24615;&#26333;&#20809;&#21644;&#20135;&#29983;&#22238;&#38899;&#23460;&#26041;&#38754;&#30340;&#39118;&#38505;&#65292;&#21363;&#38480;&#21046;&#25509;&#35302;&#22810;&#26679;&#21270;&#24847;&#35265;&#24182;&#23548;&#33268;&#24847;&#35265;&#20559;&#25191;&#65292;&#20294;&#23545;&#20110;LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#30340;&#36825;&#31181;&#39118;&#38505;&#30693;&#20043;&#29978;&#23569;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#23454;&#39564;&#26469;&#30740;&#31350;&#65306;1&#65289;LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#30456;&#36739;&#20110;&#20256;&#32479;&#25628;&#32034;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#22686;&#21152;&#36873;&#25321;&#24615;&#26333;&#20809;&#65307;2&#65289;&#20855;&#26377;&#25903;&#25345;&#25110;&#25361;&#25112;&#29992;&#25143;&#35266;&#28857;&#30340;&#24847;&#35265;&#20559;&#35265;&#30340;LLM&#22914;&#20309;&#25913;&#21464;&#36825;&#31181;&#24433;&#21709;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#21442;&#19982;&#32773;&#22312;LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#26356;&#20542;&#21521;&#20110;&#36827;&#34892;&#20559;&#35265;&#30340;&#20449;&#24687;&#26597;&#35810;&#65292;&#24182;&#19988;&#25903;&#25345;&#20182;&#20204;&#35266;&#28857;&#30340;&#26377;&#20559;&#35265;&#30340;LLM&#21152;&#21095;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#21576;&#29616;&#20102;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) powered conversational search systems have already been used by hundreds of millions of people, and are believed to bring many benefits over conventional search. However, while decades of research and public discourse interrogated the risk of search systems in increasing selective exposure and creating echo chambers -- limiting exposure to diverse opinions and leading to opinion polarization, little is known about such a risk of LLM-powered conversational search. We conduct two experiments to investigate: 1) whether and how LLM-powered conversational search increases selective exposure compared to conventional search; 2) whether and how LLMs with opinion biases that either reinforce or challenge the user's view change the effect. Overall, we found that participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias. These results present critical implicatio
&lt;/p&gt;</description></item><item><title>PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;</title><link>https://arxiv.org/abs/2402.05868</link><description>&lt;p&gt;
PromptCrypt: &#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#30340;&#25552;&#31034;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05868
&lt;/p&gt;
&lt;p&gt;
PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#26085;&#24120;&#25805;&#20316;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#21487;&#35775;&#38382;&#24615;&#21644;&#21151;&#33021;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#22909;&#22788;&#65292;&#20294;&#23427;&#20204;&#20063;&#24341;&#20837;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#38382;&#39064;&#65306;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#20013;&#20256;&#36755;&#21644;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#20250;&#20135;&#29983;&#37325;&#22823;&#30340;&#25968;&#25454;&#27844;&#38706;&#21644;&#26410;&#32463;&#25480;&#26435;&#35775;&#38382;&#25935;&#24863;&#20449;&#24687;&#30340;&#39118;&#38505;&#65307;&#21363;&#20351;&#25968;&#25454;&#30340;&#20256;&#36755;&#21644;&#23384;&#20648;&#34987;&#21152;&#23494;&#65292;LLM&#26381;&#21153;&#25552;&#20379;&#21830;&#20173;&#28982;&#30693;&#36947;&#25968;&#25454;&#30340;&#30495;&#23454;&#20869;&#23481;&#65292;&#20174;&#32780;&#38459;&#27490;&#20010;&#20154;&#25110;&#23454;&#20307;&#25918;&#24515;&#20351;&#29992;&#27492;&#31867;LLM&#26381;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26426;&#21046;PromptCrypt&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#23427;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#21040;LLM&#65292;&#26377;&#25928;&#22320;&#20351;&#20854;&#23545;&#20154;&#31867;&#25110;LLM&#30340;&#26816;&#26597;&#26080;&#27861;&#29702;&#35299;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25552;&#31034;&#30340;&#24847;&#22270;&#65292;&#20174;&#32780;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#20195;&#29702;&#20043;&#38388;&#30340;&#35848;&#21028;&#33021;&#21147;&#65292;&#24320;&#21457;&#20102;NegotiationArena&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#21644;&#25506;&#32034;LLM&#20195;&#29702;&#30340;&#35848;&#21028;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#36816;&#29992;&#29305;&#23450;&#30340;&#34892;&#20026;&#31574;&#30053;&#26174;&#33879;&#25552;&#39640;&#35848;&#21028;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05863</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#36827;&#34892;&#33391;&#22909;&#30340;&#35848;&#21028;&#21527;&#65311;NegotiationArena&#24179;&#21488;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#20195;&#29702;&#20043;&#38388;&#30340;&#35848;&#21028;&#33021;&#21147;&#65292;&#24320;&#21457;&#20102;NegotiationArena&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#21644;&#25506;&#32034;LLM&#20195;&#29702;&#30340;&#35848;&#21028;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#36816;&#29992;&#29305;&#23450;&#30340;&#34892;&#20026;&#31574;&#30053;&#26174;&#33879;&#25552;&#39640;&#35848;&#21028;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35848;&#21028;&#26159;&#31038;&#20250;&#20132;&#24448;&#30340;&#22522;&#30784;&#65307;&#20154;&#20204;&#35848;&#21028;&#20174;&#27773;&#36710;&#20215;&#26684;&#21040;&#22914;&#20309;&#20849;&#20139;&#20849;&#21516;&#36164;&#28304;&#30340;&#19968;&#20999;&#12290;&#38543;&#30528;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#20154;&#31867;&#29992;&#25143;&#34892;&#21160;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#65292;&#36825;&#20123;LLM&#20195;&#29702;&#20063;&#38656;&#35201;&#20855;&#22791;&#35848;&#21028;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#20043;&#38388;&#30340;&#35848;&#21028;&#33021;&#21147;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;NegotiationArena&#65306;&#19968;&#20010;&#28789;&#27963;&#30340;&#35780;&#20272;&#21644;&#25506;&#32034;LLM&#20195;&#29702;&#35848;&#21028;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;NegotiationArena&#20013;&#23454;&#26045;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#22330;&#26223;&#65292;&#20197;&#35780;&#20272;LLMs&#22312;&#20998;&#37197;&#20849;&#20139;&#36164;&#28304;&#65288;&#32456;&#26497;&#21338;&#24328;&#65289;&#12289;&#32858;&#21512;&#36164;&#28304;&#65288;&#20132;&#26131;&#21338;&#24328;&#65289;&#21644;&#20080;&#21334;&#21830;&#21697;&#65288;&#20215;&#26684;&#35848;&#21028;&#65289;&#26041;&#38754;&#30340;&#34892;&#20026;&#12290;&#27599;&#20010;&#22330;&#26223;&#37117;&#20801;&#35768;LLM&#20195;&#29702;&#20043;&#38388;&#36827;&#34892;&#22810;&#36718;&#28789;&#27963;&#23545;&#35805;&#65292;&#20197;&#36827;&#34892;&#26356;&#22797;&#26434;&#30340;&#35848;&#21028;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36890;&#36807;&#37319;&#29992;&#26576;&#20123;&#34892;&#20026;&#31574;&#30053;&#65292;LLM&#20195;&#29702;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35848;&#21028;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;&#36890;&#36807;&#20551;&#35013;&#22788;&#22659;&#22256;&#39039;&#21644;&#32477;&#26395;&#65292;LLM&#20195;&#29702;&#21487;&#20197;&#22686;&#21152;&#35848;&#21028;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Negotiation is the basis of social interactions; humans negotiate everything from the price of cars to how to share common resources. With rapidly growing interest in using large language models (LLMs) to act as agents on behalf of human users, such LLM agents would also need to be able to negotiate. In this paper, we study how well LLMs can negotiate with each other. We develop NegotiationArena: a flexible framework for evaluating and probing the negotiation abilities of LLM agents. We implemented three types of scenarios in NegotiationArena to assess LLM's behaviors in allocating shared resources (ultimatum games), aggregate resources (trading games) and buy/sell goods (price negotiations). Each scenario allows for multiple turns of flexible dialogues between LLM agents to allow for more complex negotiations. Interestingly, LLM agents can significantly boost their negotiation outcomes by employing certain behavioral tactics. For example, by pretending to be desolate and desperate, LL
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26174;&#24335;&#22320;&#34920;&#31034;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#22312;&#22270;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.05862</link><description>&lt;p&gt;
&#35753;&#20320;&#30340;&#22270;&#26469;&#35828;&#35805;&#65306;&#20026;LLMs&#32534;&#30721;&#32467;&#26500;&#21270;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Let Your Graph Do the Talking: Encoding Structured Data for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26174;&#24335;&#22320;&#34920;&#31034;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#22312;&#22270;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32534;&#30721;&#25104;&#24207;&#21015;&#24418;&#24335;&#65292;&#20197;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#29992;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#34920;&#31034;LLMs&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;GraphToken&#65292;&#23398;&#20064;&#20102;&#19968;&#31181;&#32534;&#30721;&#20989;&#25968;&#65292;&#20197;&#26174;&#24335;&#32467;&#26500;&#21270;&#20449;&#24687;&#25193;&#23637;&#25552;&#31034;&#35821;&#12290;&#19982;&#20854;&#20182;&#19987;&#27880;&#20110;&#26377;&#38480;&#39046;&#22495;&#65288;&#20363;&#22914;&#30693;&#35782;&#22270;&#34920;&#31034;&#65289;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#38024;&#23545;&#19968;&#33324;&#32467;&#26500;&#21270;&#25968;&#25454;&#32534;&#30721;&#36827;&#34892;&#30740;&#31350;&#65292;&#29992;&#20110;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26126;&#30830;&#34920;&#31034;&#22270;&#32467;&#26500;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#22270;&#25512;&#29702;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;GraphQA&#22522;&#20934;&#27979;&#35797;&#20013;&#30475;&#21040;&#20102;&#25972;&#20307;&#30340;&#25913;&#36827; - &#22312;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#32423;&#20219;&#21153;&#19978;&#39640;&#36798;73%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we best encode structured data into sequential form for use in large language models (LLMs)? In this work, we introduce a parameter-efficient method to explicitly represent structured data for LLMs. Our method, GraphToken, learns an encoding function to extend prompts with explicit structured information. Unlike other work which focuses on limited domains (e.g. knowledge graph representation), our work is the first effort focused on the general encoding of structured data to be used for various reasoning tasks. We show that explicitly representing the graph structure allows significant improvements to graph reasoning tasks. Specifically, we see across the board improvements - up to 73% points - on node, edge and, graph-level tasks from the GraphQA benchmark.
&lt;/p&gt;</description></item><item><title>Sparse-VQ&#26159;&#19968;&#31181;&#26080;&#21069;&#39304;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21644;&#21453;&#23454;&#20363;&#24402;&#19968;&#21270;&#26469;&#20943;&#23569;&#22122;&#22768;&#24433;&#21709;&#24182;&#25429;&#33719;&#36275;&#22815;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05830</link><description>&lt;p&gt;
&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#21464;&#21387;&#22120;&#65306;&#19968;&#31181;&#26080;&#21069;&#39304;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05830
&lt;/p&gt;
&lt;p&gt;
Sparse-VQ&#26159;&#19968;&#31181;&#26080;&#21069;&#39304;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21644;&#21453;&#23454;&#20363;&#24402;&#19968;&#21270;&#26469;&#20943;&#23569;&#22122;&#22768;&#24433;&#21709;&#24182;&#25429;&#33719;&#36275;&#22815;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#21464;&#21387;&#22120;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#39046;&#20808;&#30340;&#26041;&#27861;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#23450;&#21046;&#20102;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#21033;&#29992;&#20462;&#34917;&#25216;&#26415;&#23558;&#36830;&#32493;&#20449;&#21495;&#36716;&#25442;&#20026;&#29255;&#27573;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#20869;&#22312;&#22122;&#22768;&#27700;&#24179;&#30340;&#26174;&#33879;&#21464;&#21270;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20855;&#26377;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#30340;FFN-Free&#21464;&#21387;&#22120;&#65288;Sparse-VQ&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21644;&#21453;&#23454;&#20363;&#24402;&#19968;&#21270;&#65288;RevIN&#65289;&#26469;&#20943;&#23569;&#22122;&#22768;&#24433;&#21709;&#65292;&#24182;&#25429;&#33719;&#36275;&#22815;&#30340;&#32479;&#35745;&#20449;&#24687;&#29992;&#20110;&#39044;&#27979;&#65292;&#20316;&#20026;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#21069;&#39304;&#23618;&#65288;FFN&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26080;FFN&#26041;&#27861;&#21066;&#20943;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#12290;&#36890;&#36807;&#23545;&#21313;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#26032;&#24341;&#20837;&#30340;CAISO&#25968;&#25454;&#38598;&#65292;Sparse-VQ&#21462;&#24471;&#20102;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is vital for numerous applications, and transformers have become increasingly prominent in this domain. Leading methods customize the transformer architecture from NLP and CV, utilizing a patching technique to convert continuous signals into segments. Yet, time series data are uniquely challenging due to significant distribution shifts and intrinsic noise levels. To address these two challenges,we introduce the Sparse Vector Quantized FFN-Free Transformer (Sparse-VQ). Our methodology capitalizes on a sparse vector quantization technique coupled with Reverse Instance Normalization (RevIN) to reduce noise impact and capture sufficient statistics for forecasting, serving as an alternative to the Feed-Forward layer (FFN) in the transformer architecture. Our FFN-free approach trims the parameter count, enhancing computational efficiency and reducing overfitting. Through evaluations across ten benchmark datasets, including the newly introduced CAISO dataset, Sparse-VQ su
&lt;/p&gt;</description></item><item><title>&#39044;&#27979;&#27169;&#22411;&#27169;&#25311;&#30340;&#20195;&#29702;&#23384;&#22312;&#20004;&#20010;&#32467;&#26500;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#20998;&#21035;&#26159;&#33258;&#21160;&#24314;&#35758;&#22916;&#24819;&#30151;&#21644;&#39044;&#27979;-&#31574;&#30053;&#19981;&#19968;&#33268;&#24615;&#12290;&#21069;&#32773;&#26159;&#30001;&#20110;&#38544;&#34255;&#35266;&#27979;&#20316;&#20026;&#28151;&#28102;&#21464;&#37327;&#65292;&#27169;&#22411;&#23558;&#29983;&#25104;&#30340;&#21160;&#20316;&#35270;&#20026;&#19981;&#23384;&#22312;&#35266;&#27979;&#30340;&#35777;&#25454;&#65307;&#21518;&#32773;&#26159;&#30001;&#20110;&#27169;&#22411;&#30340;&#38544;&#21547;&#39044;&#27979;&#23548;&#33268;&#36873;&#25321;&#36807;&#20110;&#20445;&#23432;&#12290;&#36825;&#20123;&#25925;&#38556;&#21487;&#20197;&#36890;&#36807;&#32435;&#20837;&#20462;&#22797;&#12290;</title><link>https://arxiv.org/abs/2402.05829</link><description>&lt;p&gt;
&#39044;&#27979;&#27169;&#22411;&#27169;&#25311;&#30340;&#20195;&#29702;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Limitations of Agents Simulated by Predictive Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05829
&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27169;&#22411;&#27169;&#25311;&#30340;&#20195;&#29702;&#23384;&#22312;&#20004;&#20010;&#32467;&#26500;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#20998;&#21035;&#26159;&#33258;&#21160;&#24314;&#35758;&#22916;&#24819;&#30151;&#21644;&#39044;&#27979;-&#31574;&#30053;&#19981;&#19968;&#33268;&#24615;&#12290;&#21069;&#32773;&#26159;&#30001;&#20110;&#38544;&#34255;&#35266;&#27979;&#20316;&#20026;&#28151;&#28102;&#21464;&#37327;&#65292;&#27169;&#22411;&#23558;&#29983;&#25104;&#30340;&#21160;&#20316;&#35270;&#20026;&#19981;&#23384;&#22312;&#35266;&#27979;&#30340;&#35777;&#25454;&#65307;&#21518;&#32773;&#26159;&#30001;&#20110;&#27169;&#22411;&#30340;&#38544;&#21547;&#39044;&#27979;&#23548;&#33268;&#36873;&#25321;&#36807;&#20110;&#20445;&#23432;&#12290;&#36825;&#20123;&#25925;&#38556;&#21487;&#20197;&#36890;&#36807;&#32435;&#20837;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#26159;&#23558;&#39044;&#27979;&#27169;&#22411;&#24212;&#29992;&#20110;&#31867;&#20284;&#20110;&#20195;&#29702;&#30340;&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#21161;&#25163;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#36716;&#21464;&#25104;&#20195;&#29702;&#26102;&#21487;&#33021;&#22833;&#36133;&#30340;&#20004;&#20010;&#32467;&#26500;&#19978;&#30340;&#21407;&#22240;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#33258;&#21160;&#24314;&#35758;&#22916;&#24819;&#30151;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20174;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#22914;&#26524;&#20195;&#29702;&#20381;&#36182;&#20110;&#38544;&#34255;&#35266;&#27979;&#25968;&#25454;&#65292;&#27169;&#22411;&#26080;&#27861;&#27169;&#20223;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#20195;&#29702;:&#38544;&#34255;&#35266;&#27979;&#20316;&#20026;&#28151;&#28102;&#21464;&#37327;&#65292;&#27169;&#22411;&#23558;&#20854;&#29983;&#25104;&#30340;&#21160;&#20316;&#35270;&#20026;&#19981;&#23384;&#22312;&#35266;&#27979;&#30340;&#35777;&#25454;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#27491;&#24335;&#30740;&#31350;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#26032;&#38480;&#21046;:&#39044;&#27979;-&#31574;&#30053;&#19981;&#19968;&#33268;&#24615;&#12290;&#24403;&#19968;&#20010;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#21160;&#20316;&#26102;&#65292;&#27169;&#22411;&#23545;&#29983;&#25104;&#36825;&#20123;&#21160;&#20316;&#30340;&#31574;&#30053;&#30340;&#38544;&#21547;&#39044;&#27979;&#21487;&#20197;&#20316;&#20026;&#28151;&#28102;&#21464;&#37327;&#12290;&#32467;&#26524;&#26159;&#65292;&#27169;&#22411;&#36873;&#25321;&#21160;&#20316;&#26102;&#65292;&#22909;&#20687;&#23427;&#20204;&#39044;&#26399;&#26410;&#26469;&#30340;&#21160;&#20316;&#26159;&#27425;&#20248;&#30340;&#65292;&#23548;&#33268;&#23427;&#20204;&#36807;&#20110;&#20445;&#23432;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21253;&#21547;&#36825;&#20004;&#31181;&#25925;&#38556;&#30340;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is increasing focus on adapting predictive models into agent-like systems, most notably AI assistants based on language models. We outline two structural reasons for why these models can fail when turned into agents. First, we discuss auto-suggestive delusions. Prior work has shown theoretically that models fail to imitate agents that generated the training data if the agents relied on hidden observations: the hidden observations act as confounding variables, and the models treat actions they generate as evidence for nonexistent observations. Second, we introduce and formally study a related, novel limitation: predictor-policy incoherence. When a model generates a sequence of actions, the model's implicit prediction of the policy that generated those actions can serve as a confounding variable. The result is that models choose actions as if they expect future actions to be suboptimal, causing them to be overly conservative. We show that both of those failures are fixed by includi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21457;&#29616;&#20855;&#26377;&#26102;&#38388;&#24847;&#35782;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25163;&#21160;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#34920;&#36798;&#20986;&#23398;&#20064;&#30340;&#26032;&#21407;&#21017;&#65292;&#24182;&#36866;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.05828</link><description>&lt;p&gt;
&#21457;&#29616;&#20855;&#26377;&#26102;&#38388;&#24847;&#35782;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discovering Temporally-Aware Reinforcement Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21457;&#29616;&#20855;&#26377;&#26102;&#38388;&#24847;&#35782;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25163;&#21160;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#34920;&#36798;&#20986;&#23398;&#20064;&#30340;&#26032;&#21407;&#21017;&#65292;&#24182;&#36866;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20803;&#23398;&#20064;&#36827;&#23637;&#20351;&#24471;&#26681;&#25454;&#20195;&#29702;&#30446;&#26631;&#20989;&#25968;&#33258;&#21160;&#21457;&#29616;&#21442;&#25968;&#21270;&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;&#20026;&#20102;&#25913;&#36827;&#25163;&#21160;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#24517;&#39035;&#23545;&#36825;&#20010;&#23398;&#20064;&#21040;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#21442;&#25968;&#21270;&#36827;&#34892;&#25913;&#36827;&#65292;&#20351;&#20854;&#33021;&#22815;&#34920;&#36798;&#20986;&#23398;&#20064;&#30340;&#26032;&#21407;&#21017;&#65288;&#32780;&#19981;&#20165;&#20165;&#26159;&#24674;&#22797;&#24050;&#32463;&#24314;&#31435;&#30340;&#21407;&#21017;&#65289;&#65292;&#21516;&#26102;&#20173;&#28982;&#36866;&#29992;&#20110;&#20854;&#20803;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#21508;&#31181;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38598;&#20013;&#20110;&#21457;&#29616;&#31867;&#20284;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#36825;&#20123;&#30446;&#26631;&#20989;&#25968;&#19981;&#32771;&#34385;&#35757;&#32451;&#25152;&#20801;&#35768;&#30340;&#24635;&#27493;&#25968;&#25110;&#8220;&#35757;&#32451;&#35270;&#37326;&#8221;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22312;&#33719;&#21462;&#26032;&#33021;&#21147;&#30340;&#36807;&#31243;&#20013;&#20250;&#20351;&#29992;&#21508;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;&#20363;&#22914;&#65292;&#23398;&#29983;&#21487;&#33021;&#20250;&#26681;&#25454;&#32771;&#35797;&#25130;&#27490;&#26085;&#26399;&#21644;&#33258;&#25105;&#35780;&#20272;&#30340;&#33021;&#21147;&#26469;&#25913;&#21464;&#20182;&#20204;&#30340;&#23398;&#20064;&#25216;&#24039;&#12290;&#26412;&#25991;&#35748;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Recent advancements in meta-learning have enabled the automatic discovery of novel reinforcement learning algorithms parameterized by surrogate objective functions. To improve upon manually designed algorithms, the parameterization of this learned objective function must be expressive enough to represent novel principles of learning (instead of merely recovering already established ones) while still generalizing to a wide range of settings outside of its meta-training distribution. However, existing methods focus on discovering objective functions that, like many widely used objective functions in reinforcement learning, do not take into account the total number of steps allowed for training, or "training horizon". In contrast, humans use a plethora of different learning objectives across the course of acquiring a new ability. For instance, students may alter their studying techniques based on the proximity to exam deadlines and their self-assessed capabilities. This paper contends tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#23558;&#21382;&#21490;&#21151;&#29575;&#25968;&#25454;&#12289;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#21644;&#21355;&#26143;&#22270;&#20687;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22826;&#38451;&#33021;&#21457;&#30005;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#23545;&#20110;&#26032;&#23433;&#35013;&#30340;&#30005;&#31449;&#23588;&#20854;&#26377;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05823</link><description>&lt;p&gt;
FusionSF&#65306;&#22312;&#30690;&#37327;&#37327;&#21270;&#26694;&#26550;&#20013;&#34701;&#21512;&#24322;&#36136;&#27169;&#24577;&#20197;&#23454;&#29616;&#21487;&#38752;&#30340;&#22826;&#38451;&#33021;&#21457;&#30005;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#23558;&#21382;&#21490;&#21151;&#29575;&#25968;&#25454;&#12289;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#21644;&#21355;&#26143;&#22270;&#20687;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22826;&#38451;&#33021;&#21457;&#30005;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#23545;&#20110;&#26032;&#23433;&#35013;&#30340;&#30005;&#31449;&#23588;&#20854;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#22826;&#38451;&#33021;&#21457;&#30005;&#39044;&#27979;&#23545;&#20110;&#23558;&#20809;&#20239;&#30005;&#31449;&#25972;&#21512;&#21040;&#30005;&#32593;&#20013;&#65292;&#35843;&#24230;&#21644;&#30830;&#20445;&#30005;&#32593;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#32570;&#20047;&#36275;&#22815;&#25968;&#25454;&#30340;&#26032;&#23433;&#35013;&#30340;&#22826;&#38451;&#33021;&#30005;&#31449;&#26469;&#35828;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#32039;&#36843;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#21382;&#21490;&#22826;&#38451;&#33021;&#21457;&#30005;&#25968;&#25454;&#25110;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65292;&#20197;&#21333;&#19968;&#27169;&#24577;&#30340;&#24418;&#24335;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;&#27169;&#24577;&#25552;&#20379;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#23558;&#21382;&#21490;&#21151;&#29575;&#25968;&#25454;&#12289;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#21644;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#25972;&#21512;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30690;&#37327;&#37327;&#21270;&#26694;&#26550;&#65292;&#20351;&#20855;&#26377;&#19981;&#21516;&#20449;&#24687;&#23494;&#24230;&#30340;&#27169;&#24577;&#23545;&#40784;&#65292;&#24179;&#34913;&#20102;&#25972;&#21512;&#36275;&#22815;&#20449;&#24687;&#21644;&#36991;&#20813;&#27169;&#22411;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#23545;&#20110;&#26032;&#23433;&#35013;&#30340;&#30005;&#31449;&#23588;&#20854;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate solar power forecasting is crucial to integrate photovoltaic plants into the electric grid, schedule and secure the power grid safety. This problem becomes more demanding for those newly installed solar plants which lack sufficient data. Current research predominantly relies on historical solar power data or numerical weather prediction in a single-modality format, ignoring the complementary information provided in different modalities. In this paper, we propose a multi-modality fusion framework to integrate historical power data, numerical weather prediction, and satellite images, significantly improving forecast performance. We introduce a vector quantized framework that aligns modalities with varying information densities, striking a balance between integrating sufficient information and averting model overfitting. Our framework demonstrates strong zero-shot forecasting capability, which is especially useful for those newly installed plants. Moreover, we collect and release
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#21644;&#36873;&#25321;&#24615;&#30340;&#36951;&#24536;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#27169;&#22411;&#24847;&#22806;&#20445;&#30041;&#20010;&#20154;&#25110;&#25935;&#24863;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#26088;&#22312;&#34913;&#37327;&#25935;&#24863;&#20449;&#24687;&#28040;&#38500;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#24378;&#21270;&#36951;&#24536;&#26694;&#26550;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26631;&#27880;&#25935;&#24863;&#33539;&#22260;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05813</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#36951;&#24536;&#65306;&#25512;&#36827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#36951;&#24536;&#25216;&#26415;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#21644;&#36873;&#25321;&#24615;&#30340;&#36951;&#24536;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#27169;&#22411;&#24847;&#22806;&#20445;&#30041;&#20010;&#20154;&#25110;&#25935;&#24863;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#26088;&#22312;&#34913;&#37327;&#25935;&#24863;&#20449;&#24687;&#28040;&#38500;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#24378;&#21270;&#36951;&#24536;&#26694;&#26550;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26631;&#27880;&#25935;&#24863;&#33539;&#22260;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#33268;&#21147;&#20110;&#35299;&#20915;&#31070;&#32463;&#27169;&#22411;&#24847;&#22806;&#20445;&#30041;&#20010;&#20154;&#25110;&#25935;&#24863;&#25968;&#25454;&#30340;&#38382;&#39064;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#22312;&#36825;&#37324;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#31934;&#30830;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#12290;&#19982;&#20197;&#24448;&#23436;&#20840;&#30456;&#21453;&#30340;&#35757;&#32451;&#30446;&#26631;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20943;&#36731;&#23545;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65306;&#25935;&#24863;&#20449;&#24687;&#25552;&#21462;&#21487;&#33021;&#24615;&#65288;S-EL&#65289;&#21644;&#25935;&#24863;&#20449;&#24687;&#23384;&#20648;&#20934;&#30830;&#24615;&#65288;S-MA&#65289;&#65292;&#26088;&#22312;&#34913;&#37327;&#25935;&#24863;&#20449;&#24687;&#28040;&#38500;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#21152;&#24378;&#36951;&#24536;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26631;&#27880;&#25935;&#24863;&#33539;&#22260;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#12290;&#22312;&#32447;&#36873;&#25321;&#26426;&#21046;&#21033;&#29992;&#35821;&#35328;&#27010;&#29575;&#24471;&#20998;&#30830;&#20445;&#35745;&#31639;&#25928;&#29575;&#65292;&#32780;&#31163;&#32447;&#31574;&#30053;&#21017;&#21033;&#29992;&#22522;&#20110;&#36317;&#31163;&#30340;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this study is to investigate Machine Unlearning (MU), a burgeoning field focused on addressing concerns related to neural models inadvertently retaining personal or sensitive data. Here, a novel approach is introduced to achieve precise and selective forgetting within language models. Unlike previous methodologies that adopt completely opposing training objectives, this approach aims to mitigate adverse effects on language model performance, particularly in generation tasks. Furthermore, two innovative evaluation metrics are proposed: Sensitive Information Extraction Likelihood (S-EL) and Sensitive Information Memory Accuracy (S-MA), designed to gauge the effectiveness of sensitive information elimination. To reinforce the forgetting framework, an effective method for annotating sensitive scopes is presented, involving both online and offline strategies. The online selection mechanism leverages language probability scores to ensure computational efficiency, while the offline
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#39640;&#25928;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#27700;&#24179;/&#22402;&#30452;&#24378;&#24230;&#65288;HVI&#65289;&#39068;&#33394;&#31354;&#38388;&#26469;&#35299;&#32806;&#20142;&#24230;&#21644;&#39068;&#33394;&#65292;&#24182;&#35774;&#35745;&#20102;&#39068;&#33394;&#21644;&#24378;&#24230;&#35299;&#32806;&#32593;&#32476;&#65288;CIDNet&#65289;&#20197;&#25913;&#21892;&#22686;&#24378;&#36807;&#31243;&#20013;&#30340;&#31283;&#23450;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#39068;&#33394;&#21644;&#20142;&#24230;&#20266;&#24433;&#12290;</title><link>https://arxiv.org/abs/2402.05809</link><description>&lt;p&gt;
&#21482;&#38656;&#19968;&#20010;&#39068;&#33394;&#31354;&#38388;&#65306;&#19968;&#31181;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#39640;&#25928;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
You Only Need One Color Space: An Efficient Network for Low-light Image Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#39640;&#25928;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#27700;&#24179;/&#22402;&#30452;&#24378;&#24230;&#65288;HVI&#65289;&#39068;&#33394;&#31354;&#38388;&#26469;&#35299;&#32806;&#20142;&#24230;&#21644;&#39068;&#33394;&#65292;&#24182;&#35774;&#35745;&#20102;&#39068;&#33394;&#21644;&#24378;&#24230;&#35299;&#32806;&#32593;&#32476;&#65288;CIDNet&#65289;&#20197;&#25913;&#21892;&#22686;&#24378;&#36807;&#31243;&#20013;&#30340;&#31283;&#23450;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#39068;&#33394;&#21644;&#20142;&#24230;&#20266;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#65288;Low-Light Image Enhancement&#65292;LLIE&#65289;&#20219;&#21153;&#26088;&#22312;&#20174;&#21463;&#25439;&#30340;&#20302;&#20809;&#22270;&#20687;&#20013;&#24674;&#22797;&#32454;&#33410;&#21644;&#35270;&#35273;&#20449;&#24687;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;sRGB&#21644;HSV&#39068;&#33394;&#31354;&#38388;&#19978;&#23398;&#20064;&#20302;/&#27491;&#24120;&#20809;&#22270;&#20687;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22686;&#24378;&#28041;&#21450;&#25918;&#22823;&#22270;&#20687;&#20449;&#21495;&#65292;&#24182;&#19988;&#23558;&#36825;&#20123;&#39068;&#33394;&#31354;&#38388;&#24212;&#29992;&#20110;&#20449;&#22122;&#27604;&#20302;&#30340;&#20302;&#20809;&#22270;&#20687;&#21487;&#33021;&#20250;&#24341;&#20837;&#28789;&#25935;&#24230;&#21644;&#19981;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#22686;&#24378;&#22270;&#20687;&#20013;&#23384;&#22312;&#39068;&#33394;&#20266;&#24433;&#21644;&#20142;&#24230;&#20266;&#24433;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35757;&#32451;&#39068;&#33394;&#31354;&#38388;&#65292;&#31216;&#20026;&#27700;&#24179;/&#22402;&#30452;&#24378;&#24230;&#65288;HVI&#65289;&#12290;&#23427;&#19981;&#20165;&#23558;&#20142;&#24230;&#21644;&#39068;&#33394;&#20174;RGB&#36890;&#36947;&#20998;&#31163;&#20986;&#26469;&#20197;&#20943;&#36731;&#22686;&#24378;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#32780;&#19988;&#30001;&#20110;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#23427;&#36824;&#36866;&#24212;&#19981;&#21516;&#20809;&#29031;&#33539;&#22260;&#30340;&#20302;&#20809;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39068;&#33394;&#21644;&#24378;&#24230;&#35299;&#32806;&#32593;&#32476;&#65288;CIDNet&#65289;&#65292;&#21547;&#26377;&#20004;&#20010;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Light Image Enhancement (LLIE) task tends to restore the details and visual information from corrupted low-light images. Most existing methods learn the mapping function between low/normal-light images by Deep Neural Networks (DNNs) on sRGB and HSV color space. Nevertheless, enhancement involves amplifying image signals, and applying these color spaces to low-light images with a low signal-to-noise ratio can introduce sensitivity and instability into the enhancement process. Consequently, this results in the presence of color artifacts and brightness artifacts in the enhanced images. To alleviate this problem, we propose a novel trainable color space, named Horizontal/Vertical-Intensity (HVI). It not only decouples brightness and color from RGB channels to mitigate the instability during enhancement but also adapts to low-light images in different illumination ranges due to the trainable parameters. Further, we design a novel Color and Intensity Decoupling Network (CIDNet) with two
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#24182;&#24314;&#31435;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#32467;&#26524;&#30417;&#30563;&#21644;&#36807;&#31243;&#30417;&#30563;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.05808</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#24182;&#24314;&#31435;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#32467;&#26524;&#30417;&#30563;&#21644;&#36807;&#31243;&#30417;&#30563;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;R$^3$&#65306;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#25512;&#29702;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21482;&#20351;&#29992;&#32467;&#26524;&#30417;&#30563;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36807;&#31243;&#30417;&#30563;&#30340;&#22909;&#22788;&#12290;&#23558;RL&#24212;&#29992;&#20110;&#22797;&#26434;&#25512;&#29702;&#30340;&#26680;&#24515;&#25361;&#25112;&#26159;&#30830;&#23450;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#20197;&#33719;&#24471;&#27491;&#21521;&#22870;&#21169;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#20248;&#21270;&#30417;&#30563;&#12290;&#32467;&#26524;&#30417;&#30563;&#20026;&#26368;&#32456;&#32467;&#26524;&#25552;&#20379;&#20102;&#31232;&#30095;&#22870;&#21169;&#65292;&#32780;&#19981;&#35782;&#21035;&#38169;&#35823;&#20301;&#32622;&#65292;&#32780;&#36807;&#31243;&#30417;&#30563;&#25552;&#20379;&#20102;&#36880;&#27493;&#22870;&#21169;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#12290;R$^3$&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;R$^3$&#23558;&#25512;&#29702;&#30340;&#36215;&#22987;&#29366;&#24577;&#20174;&#28436;&#31034;&#30340;&#32467;&#26463;&#28369;&#21160;&#21040;&#24320;&#22987;&#65292;&#20174;&#32780;&#22312;&#25152;&#26377;&#38454;&#27573;&#37117;&#20419;&#36827;&#20102;&#26356;&#23481;&#26131;&#30340;&#27169;&#22411;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;R$^3$&#24314;&#31435;&#20102;&#19968;&#20010;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#20351;&#32467;&#26524;&#30417;&#30563;&#33021;&#22815;&#25552;&#20379;&#38454;&#27573;&#32423;&#20449;&#21495;&#24182;&#31934;&#30830;&#23450;&#20301;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7
&lt;/p&gt;</description></item><item><title>InkSight&#26159;&#19968;&#20010;&#21487;&#20197;&#23558;&#31163;&#32447;&#25163;&#20889;&#36716;&#25442;&#20026;&#22312;&#32447;&#25163;&#20889;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#38405;&#35835;&#21644;&#20070;&#20889;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#22810;&#26679;&#21270;&#30340;&#29031;&#29255;&#20013;&#26377;&#25928;&#22320;Derendering&#25163;&#20889;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.05804</link><description>&lt;p&gt;
InkSight&#65306;&#36890;&#36807;&#23398;&#20064;&#38405;&#35835;&#21644;&#20070;&#20889;&#23454;&#29616;&#31163;&#32447;&#21040;&#22312;&#32447;&#25163;&#20889;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
InkSight: Offline-to-Online Handwriting Conversion by Learning to Read and Write
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05804
&lt;/p&gt;
&lt;p&gt;
InkSight&#26159;&#19968;&#20010;&#21487;&#20197;&#23558;&#31163;&#32447;&#25163;&#20889;&#36716;&#25442;&#20026;&#22312;&#32447;&#25163;&#20889;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#38405;&#35835;&#21644;&#20070;&#20889;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#22810;&#26679;&#21270;&#30340;&#29031;&#29255;&#20013;&#26377;&#25928;&#22320;Derendering&#25163;&#20889;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#31508;&#35760;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32784;&#29992;&#12289;&#21487;&#32534;&#36753;&#21644;&#26131;&#20110;&#32034;&#24341;&#30340;&#23384;&#20648;&#31508;&#35760;&#30340;&#26041;&#24335;&#65292;&#21363;&#30690;&#37327;&#21270;&#24418;&#24335;&#30340;&#25968;&#23383;&#22696;&#27700;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31508;&#35760;&#26041;&#24335;&#19982;&#20256;&#32479;&#30340;&#32440;&#31508;&#35760;&#26041;&#24335;&#20043;&#38388;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#32780;&#20256;&#32479;&#32440;&#31508;&#35760;&#26041;&#24335;&#20173;&#21463;&#21040;&#32477;&#22823;&#22810;&#25968;&#20154;&#30340;&#38738;&#30544;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;InkSight&#26088;&#22312;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#20351;&#23454;&#20307;&#31508;&#35760;&#32773;&#33021;&#22815;&#36731;&#26494;&#22320;&#23558;&#20182;&#20204;&#30340;&#20316;&#21697;&#65288;&#31163;&#32447;&#25163;&#20889;&#65289;&#36716;&#25442;&#20026;&#25968;&#23383;&#22696;&#27700;&#65288;&#22312;&#32447;&#25163;&#20889;&#65289;&#65292;&#36825;&#20010;&#36807;&#31243;&#25105;&#20204;&#31216;&#20043;&#20026;Derendering&#12290;&#20043;&#21069;&#20851;&#20110;&#27492;&#20027;&#39064;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#22270;&#20687;&#30340;&#20960;&#20309;&#23646;&#24615;&#19978;&#65292;&#23548;&#33268;&#20102;&#22312;&#35757;&#32451;&#39046;&#22495;&#20043;&#22806;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#38405;&#35835;&#21644;&#20070;&#20889;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20801;&#35768;&#22312;&#32570;&#20047;&#22823;&#37327;&#37197;&#23545;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#37197;&#23545;&#26679;&#26412;&#24456;&#38590;&#33719;&#21462;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26377;&#25928;&#22320;&#23545;&#20855;&#26377;&#22810;&#26679;&#21270;&#35270;&#35273;&#29305;&#24449;&#21644;&#32972;&#26223;&#30340;&#20219;&#24847;&#29031;&#29255;&#20013;&#30340;&#25163;&#20889;&#25991;&#26412;&#36827;&#34892;Derendering&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital note-taking is gaining popularity, offering a durable, editable, and easily indexable way of storing notes in the vectorized form, known as digital ink. However, a substantial gap remains between this way of note-taking and traditional pen-and-paper note-taking, a practice still favored by a vast majority. Our work, InkSight, aims to bridge the gap by empowering physical note-takers to effortlessly convert their work (offline handwriting) to digital ink (online handwriting), a process we refer to as Derendering. Prior research on the topic has focused on the geometric properties of images, resulting in limited generalization beyond their training domains. Our approach combines reading and writing priors, allowing training a model in the absence of large amounts of paired samples, which are difficult to obtain. To our knowledge, this is the first work that effectively derenders handwritten text in arbitrary photos with diverse visual characteristics and backgrounds. Furthermore,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#38899;&#26631;&#20016;&#23500;&#35821;&#26009;&#24211;&#26500;&#24314;&#26041;&#27861;&#65292;&#21253;&#25324;&#25910;&#38598;&#25991;&#26412;&#25968;&#25454;&#38598;&#12289;&#22522;&#20110;&#19977;&#38899;&#20998;&#24067;&#30340;&#21477;&#23376;&#36873;&#25321;&#31639;&#27861;&#21644;&#26681;&#25454;&#22768;&#23398;-&#21457;&#38899;&#35821;&#38899;&#29305;&#24449;&#36827;&#34892;&#26032;&#30340;&#38899;&#32032;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.05794</link><description>&lt;p&gt;
&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#30340;&#38899;&#26631;&#20016;&#23500;&#35821;&#26009;&#24211;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Phonetically rich corpus construction for a low-resourced language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#38899;&#26631;&#20016;&#23500;&#35821;&#26009;&#24211;&#26500;&#24314;&#26041;&#27861;&#65292;&#21253;&#25324;&#25910;&#38598;&#25991;&#26412;&#25968;&#25454;&#38598;&#12289;&#22522;&#20110;&#19977;&#38899;&#20998;&#24067;&#30340;&#21477;&#23376;&#36873;&#25321;&#31639;&#27861;&#21644;&#26681;&#25454;&#22768;&#23398;-&#21457;&#38899;&#35821;&#38899;&#29305;&#24449;&#36827;&#34892;&#26032;&#30340;&#38899;&#32032;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#25216;&#26415;&#20381;&#36182;&#20110;&#25429;&#25417;&#35828;&#35805;&#20154;&#30340;&#22768;&#38899;&#21464;&#24322;&#24615;&#21644;&#33719;&#21462;&#20840;&#38754;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#25991;&#26412;&#25552;&#31034;&#21644;&#21477;&#23376;&#36873;&#25321;&#26041;&#27861;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#34987;&#25552;&#20986;&#65292;&#20197;&#32452;&#25104;&#36866;&#24230;&#30340;&#38899;&#26631;&#25968;&#25454;&#65292;&#34987;&#31216;&#20026;&#38899;&#26631;&#20016;&#23500;&#30340;&#35821;&#26009;&#24211;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#32780;&#35328;&#65292;&#23427;&#20204;&#20173;&#28982;&#23545;&#22768;&#23398;&#24314;&#27169;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#27010;&#36848;&#20102;&#20026;&#36164;&#28304;&#26377;&#38480;&#30340;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#21019;&#24314;&#20855;&#26377;&#24191;&#27867;&#38899;&#26631;&#35206;&#30422;&#30340;&#35821;&#26009;&#24211;&#25152;&#38656;&#30340;&#26041;&#27861;&#35770;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20174;&#25991;&#26412;&#25968;&#25454;&#38598;&#25910;&#38598;&#21040;&#22522;&#20110;&#19977;&#38899;&#20998;&#24067;&#30340;&#21477;&#23376;&#36873;&#25321;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#32032;&#20998;&#31867;&#26041;&#27861;&#65292;&#26681;&#25454;&#22768;&#23398;-&#21457;&#38899;&#35821;&#38899;&#29305;&#24449;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#19977;&#38899;&#30340;&#32477;&#23545;&#25968;&#37327;&#25110;&#20302;&#27010;&#29575;&#19977;&#38899;&#24182;&#19981;&#33021;&#20445;&#35777;&#23545;&#27599;&#31181;&#21487;&#33021;&#30340;&#32452;&#21512;&#36827;&#34892;&#36866;&#24403;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech technologies rely on capturing a speaker's voice variability while obtaining comprehensive language information. Textual prompts and sentence selection methods have been proposed in the literature to comprise such adequate phonetic data, referred to as a phonetically rich \textit{corpus}. However, they are still insufficient for acoustic modeling, especially critical for languages with limited resources. Hence, this paper proposes a novel approach and outlines the methodological aspects required to create a \textit{corpus} with broad phonetic coverage for a low-resourced language, Brazilian Portuguese. Our methodology includes text dataset collection up to a sentence selection algorithm based on triphone distribution. Furthermore, we propose a new phonemic classification according to acoustic-articulatory speech features since the absolute number of distinct triphones, or low-probability triphones, does not guarantee an adequate representation of every possible combination. Usin
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#28216;&#25103;&#29609;&#23478;&#33021;&#22815;&#23637;&#29616;&#20986;&#24378;&#28872;&#30340;&#20844;&#24179;&#24847;&#35782;&#65292;&#21462;&#20915;&#20110;&#20854;&#23545;&#28216;&#25103;&#20249;&#20276;&#30340;&#20449;&#20219;&#31243;&#24230;&#12289;&#28216;&#25103;&#26694;&#26550;&#23545;&#20110;&#32473;&#20104;&#25509;&#21463;&#32773;&#30340;&#24433;&#21709;&#20197;&#21450;&#20854;&#21487;&#33021;&#23384;&#22312;&#21388;&#24694;&#19981;&#24179;&#31561;&#30340;&#24773;&#24863;&#12290;</title><link>https://arxiv.org/abs/2402.05786</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#28216;&#25103;&#29609;&#23478;&#65306;&#20419;&#36827;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompting Fairness: Artificial Intelligence as Game Players
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05786
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#28216;&#25103;&#29609;&#23478;&#33021;&#22815;&#23637;&#29616;&#20986;&#24378;&#28872;&#30340;&#20844;&#24179;&#24847;&#35782;&#65292;&#21462;&#20915;&#20110;&#20854;&#23545;&#28216;&#25103;&#20249;&#20276;&#30340;&#20449;&#20219;&#31243;&#24230;&#12289;&#28216;&#25103;&#26694;&#26550;&#23545;&#20110;&#32473;&#20104;&#25509;&#21463;&#32773;&#30340;&#24433;&#21709;&#20197;&#21450;&#20854;&#21487;&#33021;&#23384;&#22312;&#21388;&#24694;&#19981;&#24179;&#31561;&#30340;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#21313;&#24180;&#26469;&#65292;&#31038;&#20250;&#31185;&#23398;&#23478;&#20204;&#19968;&#30452;&#22312;&#30740;&#31350;&#27979;&#37327;&#20844;&#24179;&#24615;&#30340;&#21151;&#21033;&#20027;&#20041;&#28216;&#25103;&#65292;&#27604;&#22914;&#29420;&#35009;&#32773;&#28216;&#25103;&#12290;&#36825;&#20123;&#28216;&#25103;&#19981;&#20165;&#35753;&#25105;&#20204;&#20102;&#35299;&#20102;&#20154;&#31867;&#23545;&#20844;&#24179;&#24615;&#30340;&#30475;&#27861;&#65292;&#36824;&#25581;&#31034;&#20102;&#20844;&#24179;&#24615;&#12289;&#21033;&#20182;&#20027;&#20041;&#21644;&#36138;&#23146;&#22312;&#20309;&#31181;&#26465;&#20214;&#19979;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;&#23613;&#31649;&#36825;&#20123;&#28216;&#25103;&#20256;&#32479;&#19978;&#20851;&#27880;&#20154;&#31867;&#65292;&#20294;&#20154;&#24037;&#26234;&#33021;&#30340;&#23835;&#36215;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29609;&#36825;&#20123;&#28216;&#25103;&#12290;&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#25104;&#20026;&#20154;&#38469;&#20114;&#21160;&#20013;&#30340;&#24120;&#24577;&#65292;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#22312;&#28216;&#25103;&#20013;&#26174;&#31034;&#30340;&#20844;&#24179;&#24615;&#21487;&#20197;&#35753;&#25105;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20915;&#31574;&#36807;&#31243;&#26377;&#25152;&#20102;&#35299;&#12290;&#36890;&#36807;101&#36718;&#30340;&#29420;&#35009;&#32773;&#28216;&#25103;&#65292;&#25105;&#24471;&#20986;&#32467;&#35770;&#65306;&#20154;&#24037;&#26234;&#33021;&#20855;&#26377;&#24378;&#28872;&#30340;&#20844;&#24179;&#24847;&#35782;&#65292;&#36825;&#19982;&#20854;&#35748;&#20026;&#23427;&#30340;&#28216;&#25103;&#20249;&#20276;&#26159;&#21542;&#20540;&#24471;&#20449;&#20219;&#26377;&#20851;&#65307;&#35774;&#32622;&#23545;&#28216;&#25103;&#30340;&#26694;&#26550;&#22312;&#20154;&#24037;&#26234;&#33021;&#32473;&#20104;&#25509;&#21463;&#32773;&#30340;&#25968;&#37327;&#19978;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#32780;&#19988;&#26377;&#21487;&#33021;&#23384;&#22312;&#35777;&#25454;&#34920;&#26126;&#20154;&#24037;&#26234;&#33021;&#20063;&#20687;&#20154;&#31867;&#19968;&#26679;&#20855;&#26377;&#21388;&#24694;&#19981;&#24179;&#31561;&#30340;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilitarian games such as dictator games to measure fairness have been studied in the social sciences for decades. These games have given us insight into not only how humans view fairness but also in what conditions the frequency of fairness, altruism and greed increase or decrease. While these games have traditionally been focused on humans, the rise of AI gives us the ability to study how these models play these games. AI is becoming a constant in human interaction and examining how these models portray fairness in game play can give us some insight into how AI makes decisions. Over 101 rounds of the dictator game, I conclude that AI has a strong sense of fairness that is dependant of it it deems the person it is playing with as trustworthy, framing has a strong effect on how much AI gives a recipient when designated the trustee, and there may be evidence that AI experiences inequality aversion just as humans.
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.05785</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#23398;&#20064;&#19978;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Transformer Language Models on Algorithmic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05785
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#35201;&#27714;&#32452;&#21512;&#22810;&#20010;&#31163;&#25955;&#23376;&#20219;&#21153;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLaMA&#27169;&#22411;&#21644;&#22312;GPT-4&#21644;Gemini&#19978;&#25552;&#31034;&#26469;&#34913;&#37327;&#23398;&#20064;&#23398;&#20064;&#21407;&#35821;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#35268;&#27169;&#26041;&#38754;&#27604;&#20026;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#25928;&#26524;&#26356;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25351;&#25968;&#32423;&#22320;&#28010;&#36153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25163;&#22609;&#36896;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#21512;&#29702;&#35770;&#20998;&#26512;&#30340;&#34920;&#26684;&#21270;&#29256;&#26412;R-FOS&#12290;</title><link>https://arxiv.org/abs/2402.05782</link><description>&lt;p&gt;
&#20998;&#26512;&#23545;&#25163;&#22609;&#36896;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analysing the Sample Complexity of Opponent Shaping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25163;&#22609;&#36896;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#21512;&#29702;&#35770;&#20998;&#26512;&#30340;&#34920;&#26684;&#21270;&#29256;&#26412;R-FOS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#33324;&#21644;&#21338;&#24328;&#20013;&#65292;&#23398;&#20064;&#36890;&#24120;&#20250;&#23548;&#33268;&#38598;&#20307;&#24615;&#30340;&#27425;&#20248;&#32467;&#26524;&#12290;&#23545;&#27492;&#36827;&#34892;&#25913;&#36827;&#65292;&#23545;&#25163;&#22609;&#36896;&#65288;OS&#65289;&#26041;&#27861;&#31215;&#26497;&#24341;&#23548;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#32463;&#39564;&#24615;&#22320;&#25552;&#39640;&#20102;&#20010;&#20307;&#21644;&#32676;&#20307;&#22312;&#35768;&#22810;&#24773;&#26223;&#19979;&#30340;&#34920;&#29616;&#12290;&#26089;&#26399;&#30340;OS&#26041;&#27861;&#20351;&#29992;&#39640;&#38454;&#23548;&#25968;&#26469;&#22609;&#36896;&#21512;&#20316;&#29609;&#23478;&#30340;&#23398;&#20064;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#22609;&#36896;&#22810;&#20010;&#23398;&#20064;&#27493;&#39588;&#12290;&#21518;&#32493;&#30340;&#24037;&#20316;&#65292;&#21363;&#26080;&#27169;&#22411;&#23545;&#25163;&#22609;&#36896;&#65288;M-FOS&#65289;&#65292;&#36890;&#36807;&#23558;OS&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#20803;&#21338;&#24328;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#19982;&#26089;&#26399;&#30340;OS&#26041;&#27861;&#30456;&#27604;&#65292;&#23545;&#20110;M-FOS&#26694;&#26550;&#30340;&#29702;&#35770;&#29702;&#35299;&#36824;&#24456;&#23569;&#12290;&#20026;M-FOS&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;A&#65289;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#30340;&#25991;&#29486;&#24456;&#23569; B&#65289;M-FOS&#22312;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#20013;&#36816;&#20316;&#65292;&#25152;&#20197;&#29702;&#35770;&#20998;&#26512;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;R-FOS&#65292;&#36825;&#26159;M-FOS&#30340;&#34920;&#26684;&#21270;&#29256;&#26412;&#65292;&#26356;&#36866;&#21512;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#12290;R-FOS&#31163;&#25955;&#21270;&#20102;
&lt;/p&gt;
&lt;p&gt;
Learning in general-sum games often yields collectively sub-optimal results. Addressing this, opponent shaping (OS) methods actively guide the learning processes of other agents, empirically leading to improved individual and group performances in many settings. Early OS methods use higher-order derivatives to shape the learning of co-players, making them unsuitable for shaping multiple learning steps. Follow-up work, Model-free Opponent Shaping (M-FOS), addresses these by reframing the OS problem as a meta-game. In contrast to early OS methods, there is little theoretical understanding of the M-FOS framework. Providing theoretical guarantees for M-FOS is hard because A) there is little literature on theoretical sample complexity bounds for meta-reinforcement learning B) M-FOS operates in continuous state and action spaces, so theoretical analysis is challenging. In this work, we present R-FOS, a tabular version of M-FOS that is more suitable for theoretical analysis. R-FOS discretises
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#31283;&#23450;&#24615;&#24037;&#20855;&#20110;&#26102;&#38388;&#29420;&#31435;&#31995;&#32479;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#29289;&#29702;&#31283;&#23450;&#25968;&#25454;&#28857;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#19982;&#25511;&#21046;&#29702;&#35770;&#21407;&#29702;&#36827;&#34892;&#20102;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.05774</link><description>&lt;p&gt;
&#31283;&#23450;&#30340;&#33258;&#20027;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Stable Autonomous Flow Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#31283;&#23450;&#24615;&#24037;&#20855;&#20110;&#26102;&#38388;&#29420;&#31435;&#31995;&#32479;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#29289;&#29702;&#31283;&#23450;&#25968;&#25454;&#28857;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#19982;&#25511;&#21046;&#29702;&#35770;&#21407;&#29702;&#36827;&#34892;&#20102;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#31034;&#29289;&#29702;&#31283;&#23450;&#29366;&#24577;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#65292;&#36890;&#24120;&#20551;&#35774;&#25968;&#25454;&#28857;&#20195;&#34920;&#33021;&#37327;&#26223;&#35266;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;&#22312;&#25511;&#21046;&#35770;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#33021;&#37327;&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25511;&#21046;&#35770;&#19982;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#22312;&#25991;&#29486;&#20013;&#24456;&#23569;&#65292;&#23613;&#31649;&#26377;&#20960;&#20010;&#20855;&#26377;&#29289;&#29702;&#31283;&#23450;&#25968;&#25454;&#28857;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#36825;&#26679;&#30340;&#25968;&#25454;&#21644;&#19968;&#31181;&#26368;&#36817;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#65292;&#31216;&#20026;&#27969;&#21305;&#37197;&#12290;&#25105;&#20204;&#24212;&#29992;&#38543;&#26426;&#31283;&#23450;&#24615;&#24037;&#20855;&#20110;&#26102;&#38388;&#29420;&#31435;&#31995;&#32479;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#36866;&#24212;&#36825;&#31181;&#22788;&#29702;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#30340;&#31354;&#38388;&#65292;&#20197;&#21450;&#19982;&#20854;&#20182;&#25511;&#21046;&#29702;&#35770;&#21407;&#29702;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#31034;&#20363;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contexts where data samples represent a physically stable state, it is often assumed that the data points represent the local minima of an energy landscape. In control theory, it is well-known that energy can serve as an effective Lyapunov function. Despite this, connections between control theory and generative models in the literature are sparse, even though there are several machine learning applications with physically stable data points. In this paper, we focus on such data and a recent class of deep generative models called flow matching. We apply tools of stochastic stability for time-independent systems to flow matching models. In doing so, we characterize the space of flow matching models that are amenable to this treatment, as well as draw connections to other control theory principles. We demonstrate our theoretical results on two examples.
&lt;/p&gt;</description></item><item><title>&#24191;&#20041;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#26159;&#19968;&#31181;&#31163;&#32447;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#19968;&#31867;&#20984;&#20989;&#25968;&#26469;&#23454;&#29616;&#32479;&#19968;&#30340;&#20559;&#22909;&#20248;&#21270;&#35270;&#35282;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#31639;&#27861;&#24037;&#20855;&#21644;&#23454;&#35777;&#27934;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.05749</link><description>&lt;p&gt;
&#24191;&#20041;&#20559;&#22909;&#20248;&#21270;&#65306;&#31163;&#32447;&#23545;&#40784;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generalized Preference Optimization: A Unified Approach to Offline Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05749
&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#26159;&#19968;&#31181;&#31163;&#32447;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#19968;&#31867;&#20984;&#20989;&#25968;&#26469;&#23454;&#29616;&#32479;&#19968;&#30340;&#20559;&#22909;&#20248;&#21270;&#35270;&#35282;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#31639;&#27861;&#24037;&#20855;&#21644;&#23454;&#35777;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20559;&#22909;&#20248;&#21270;&#20801;&#35768;&#30452;&#25509;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23545;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#26368;&#36817;&#30340;&#23545;&#40784;&#23454;&#36341;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31867;&#36890;&#36807;&#19968;&#33324;&#30340;&#20984;&#20989;&#25968;&#21442;&#25968;&#21270;&#30340;&#31163;&#32447;&#25439;&#22833;&#20989;&#25968;&#12290;GPO&#25552;&#20379;&#20102;&#23545;&#20559;&#22909;&#20248;&#21270;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#28085;&#30422;&#20102;&#29616;&#26377;&#31639;&#27861;&#65288;DPO&#12289;IPO&#21644;SLiC&#65289;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65292;&#21516;&#26102;&#33258;&#28982;&#24341;&#20837;&#20102;&#26032;&#30340;&#21464;&#20307;&#12290;GPO&#26694;&#26550;&#36824;&#25581;&#31034;&#20102;&#31163;&#32447;&#31639;&#27861;&#22914;&#20309;&#36890;&#36807;&#23450;&#20041;&#25439;&#22833;&#30340;&#20984;&#20989;&#25968;&#26469;&#23454;&#26045;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#25581;&#31034;&#20102;&#31163;&#32447;&#27491;&#21017;&#21270;&#21644;&#35268;&#33539;&#30340;RLHF&#20844;&#24335;&#25152;&#24847;&#22270;&#30340;KL&#25955;&#24230;&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#24494;&#22937;&#24046;&#24322;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#23545;&#40784;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#26032;&#30340;&#31639;&#27861;&#24037;&#20855;&#21644;&#23454;&#35777;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline preference optimization allows fine-tuning large models directly from offline data, and has proved effective in recent alignment practices. We propose generalized preference optimization (GPO), a family of offline losses parameterized by a general class of convex functions. GPO enables a unified view over preference optimization, encompassing existing algorithms such as DPO, IPO and SLiC as special cases, while naturally introducing new variants. The GPO framework also sheds light on how offline algorithms enforce regularization, through the design of the convex function that defines the loss. Our analysis and experiments reveal the connections and subtle differences between the offline regularization and the KL divergence regularization intended by the canonical RLHF formulation. In all, our results present new algorithmic toolkits and empirical insights to alignment practitioners.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#26426;&#20132;&#20114;&#25968;&#25454;&#32416;&#38169;&#26041;&#27861;&#26469;&#25913;&#36827;&#25968;&#25454;&#38598;&#36136;&#37327;&#30340;&#26041;&#26696;&#65292;&#20197;&#25552;&#21319;&#35270;&#35273;&#35782;&#21035;&#20934;&#30830;&#24615;&#21644;&#20248;&#21270;&#35270;&#35273;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05747</link><description>&lt;p&gt;
Jacquard V2: &#20351;&#29992;&#20154;&#26426;&#20132;&#20114;&#25968;&#25454;&#32416;&#38169;&#26041;&#27861;&#25913;&#36827;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Jacquard V2: Refining Datasets using the Human In the Loop Data Correction Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#26426;&#20132;&#20114;&#25968;&#25454;&#32416;&#38169;&#26041;&#27861;&#26469;&#25913;&#36827;&#25968;&#25454;&#38598;&#36136;&#37327;&#30340;&#26041;&#26696;&#65292;&#20197;&#25552;&#21319;&#35270;&#35273;&#35782;&#21035;&#20934;&#30830;&#24615;&#21644;&#20248;&#21270;&#35270;&#35273;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#33258;&#21160;&#21270;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#22522;&#20110;&#35270;&#35273;&#30340;&#26426;&#22120;&#20154;&#25235;&#21462;&#22312;&#20854;&#20013;&#25198;&#28436;&#30528;&#36234;&#26469;&#36234;&#20851;&#38190;&#30340;&#35282;&#33394;&#12290;&#20026;&#20102;&#25552;&#21319;&#35270;&#35273;&#35782;&#21035;&#20934;&#30830;&#24615;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#33719;&#21462;&#19982;&#22788;&#29702;&#21508;&#31181;&#29289;&#20307;&#30456;&#20851;&#30340;&#38544;&#24335;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;&#20174;&#22836;&#24320;&#22987;&#21019;&#24314;&#25968;&#25454;&#38598;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#24120;&#24120;&#30001;&#20110;&#20026;&#20102;&#24555;&#36895;&#26631;&#27880;&#32780;&#20135;&#29983;&#38169;&#35823;&#65292;&#22240;&#27492;&#25913;&#21892;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#27969;&#34892;&#30340;Jacquard Grasp&#20013;&#65292;&#24050;&#32463;&#30830;&#23450;&#20102;&#19968;&#20123;&#22312;&#25235;&#21462;&#36793;&#30028;&#26694;&#27880;&#37322;&#20013;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19968;&#31181;&#20154;&#26426;&#20132;&#20114;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#39592;&#24178;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#26469;&#39044;&#27979;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#29289;&#20307;&#20301;&#32622;&#21644;&#26041;&#21521;&#12290;&#19982;Intersection over Union&#65288;IOU&#65289;&#20540;&#20302;&#20110;0.2&#30340;&#39044;&#27979;&#32463;&#36807;&#20154;&#24037;&#25805;&#20316;&#21592;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of rapid advancements in industrial automation, vision-based robotic grasping plays an increasingly crucial role. In order to enhance visual recognition accuracy, the utilization of large-scale datasets is imperative for training models to acquire implicit knowledge related to the handling of various objects. Creating datasets from scratch is a time and labor-intensive process. Moreover, existing datasets often contain errors due to automated annotations aimed at expediency, making the improvement of these datasets a substantial research challenge. Consequently, several issues have been identified in the annotation of grasp bounding boxes within the popular Jacquard Grasp. We propose utilizing a Human-In-The-Loop(HIL) method to enhance dataset quality. This approach relies on backbone deep learning networks to predict object positions and orientations for robotic grasping. Predictions with Intersection over Union (IOU) values below 0.2 undergo an assessment by human oper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#26367;&#25442;&#29616;&#26377;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#32452;&#20214;&#12290;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12289;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#26041;&#38754;&#25198;&#28436;&#20102;&#37325;&#35201;&#35282;&#33394;&#12290;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#23545;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24433;&#21709;&#20063;&#34987;&#35752;&#35770;&#21040;&#12290;</title><link>https://arxiv.org/abs/2402.05741</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Real-World Robot Applications of Foundation Models: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#26367;&#25442;&#29616;&#26377;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#32452;&#20214;&#12290;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12289;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#26041;&#38754;&#25198;&#28436;&#20102;&#37325;&#35201;&#35282;&#33394;&#12290;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#23545;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24433;&#21709;&#20063;&#34987;&#35752;&#35770;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#31561;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#20026;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#24577;&#30340;&#28789;&#27963;&#24212;&#29992;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;&#23427;&#20204;&#30340;&#24433;&#21709;&#28085;&#30422;&#20102;&#21253;&#25324;&#21307;&#30103;&#12289;&#25945;&#32946;&#21644;&#26426;&#22120;&#20154;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#65292;&#37325;&#28857;&#26159;&#26367;&#25442;&#29616;&#26377;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#32452;&#20214;&#12290;&#24635;&#32467;&#28085;&#30422;&#20102;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#30340;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#25361;&#25112;&#21644;&#23545;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#37096;&#20998;&#22522;&#20110;&#27169;&#22411;&#30340;Eluder&#32500;&#24230;&#65288;P-MBED&#65289;&#27010;&#24565;&#26469;&#34913;&#37327;&#27169;&#22411;&#31867;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#22522;&#26412;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#24179;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#24182;&#19981;&#27604;&#35299;&#20915;&#23545;&#25968;&#20010;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26356;&#20855;&#32479;&#35745;&#25361;&#25112;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05724</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#24182;&#19981;&#27604;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26356;&#21152;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#37096;&#20998;&#22522;&#20110;&#27169;&#22411;&#30340;Eluder&#32500;&#24230;&#65288;P-MBED&#65289;&#27010;&#24565;&#26469;&#34913;&#37327;&#27169;&#22411;&#31867;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#22522;&#26412;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#24179;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#24182;&#19981;&#27604;&#35299;&#20915;&#23545;&#25968;&#20010;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26356;&#20855;&#32479;&#35745;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#20989;&#25968;&#36924;&#36817;&#19979;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#35813;&#26041;&#27861;&#38656;&#35201;&#31574;&#30053;&#24615;&#25506;&#32034;&#20197;&#25214;&#21040;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#37096;&#20998;&#22522;&#20110;&#27169;&#22411;&#30340;Eluder&#32500;&#24230;&#65288;P-MBED&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#27010;&#24565;&#26469;&#25551;&#36848;&#27169;&#22411;&#31867;&#22797;&#26434;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;P-MBED&#21487;&#20197;&#34913;&#37327;&#20174;&#32473;&#23450;&#30340;&#24179;&#22343;&#22330;&#27169;&#22411;&#31867;&#36716;&#25442;&#32780;&#26469;&#30340;&#21333;&#20010;&#26234;&#33021;&#20307;&#27169;&#22411;&#31867;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#28508;&#22312;&#19978;&#21487;&#33021;&#27604;\citet{huang2023statistical}&#25552;&#20986;&#30340;MBED&#25351;&#25968;&#32423;&#20302;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#28040;&#38500;&#31639;&#27861;&#65292;&#20855;&#26377;&#26032;&#39062;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#24182;&#24314;&#31435;&#20102;&#19982;P-MBED&#30456;&#20851;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22522;&#26412;&#21487;&#23454;&#29616;&#24615;&#21644;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#24179;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#24182;&#19981;&#27604;&#35299;&#20915;&#23545;&#25968;&#20010;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26356;&#20855;&#32479;&#35745;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#22810;&#31867;&#22411;&#24179;&#22343;&#22330;&#21338;&#24328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the sample complexity of reinforcement learning (RL) in Mean-Field Games (MFGs) with model-based function approximation that requires strategic exploration to find a Nash Equilibrium policy. We introduce the Partial Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize the model class complexity. Notably, P-MBED measures the complexity of the single-agent model class converted from the given mean-field model class, and potentially, can be exponentially lower than the MBED proposed by \citet{huang2023statistical}. We contribute a model elimination algorithm featuring a novel exploration strategy and establish sample complexity results polynomial w.r.t.~P-MBED. Crucially, our results reveal that, under the basic realizability and Lipschitz continuity assumptions, \emph{learning Nash Equilibrium in MFGs is no more statistically challenging than solving a logarithmic number of single-agent RL problems}. We further extend our results to Multi-Type MFGs, gen
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#20154;&#32676;&#30340;&#26631;&#31614;&#27745;&#26579;&#25915;&#20987;&#26469;&#30772;&#22351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#20154;&#32676;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30452;&#25509;&#30456;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.05713</link><description>&lt;p&gt;
&#26126;&#26126;&#23601;&#22312;&#30524;&#21069;&#65306;&#23545;&#24369;&#21183;&#24739;&#32773;&#32676;&#20307;&#36827;&#34892;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#20154;&#32676;&#30340;&#26631;&#31614;&#27745;&#26579;&#25915;&#20987;&#26469;&#30772;&#22351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#20154;&#32676;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30452;&#25509;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25918;&#23556;&#23398;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#21095;&#23545;&#24369;&#21183;&#24739;&#32773;&#32676;&#20307;&#30340;&#20020;&#24202;&#20559;&#35265;&#30340;&#39118;&#38505;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#23637;&#31034;&#30340;&#20559;&#35265;&#30340;&#37327;&#21270;&#65292;&#20294;&#38024;&#23545;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#20197;&#21450;&#20854;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38024;&#23545;&#20154;&#21475;&#32479;&#35745;&#23398;&#26631;&#31614;&#30340;&#27602;&#21270;&#25915;&#20987;&#21487;&#20197;&#21521;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#65292;&#24182;&#22312;&#19981;&#24433;&#21709;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#38477;&#20302;&#23545;&#34987;&#20302;&#20272;&#32676;&#20307;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#22810;&#20010;&#24615;&#33021;&#25351;&#26631;&#21644;&#20154;&#21475;&#32676;&#20307;&#65288;&#22914;&#24615;&#21035;&#12289;&#24180;&#40836;&#20197;&#21450;&#20854;&#20132;&#21449;&#23376;&#32676;&#65289;&#19978;&#34920;&#26126;&#65292;&#32676;&#20307;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#19982;&#20854;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#24449;&#30452;&#25509;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data.
&lt;/p&gt;</description></item><item><title>DiffSpeaker&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#21464;&#25442;&#22120;&#21644;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#35821;&#38899;&#39537;&#21160;3D&#38754;&#37096;&#21160;&#30011;&#32593;&#32476;&#65292;&#36890;&#36807;&#35299;&#20915;&#37197;&#23545;&#38899;&#39057;-4D&#25968;&#25454;&#30340;&#32570;&#20047;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#21767;&#37096;&#21516;&#27493;&#21644;&#38750;&#35821;&#35328;&#38754;&#37096;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2402.05712</link><description>&lt;p&gt;
DiffSpeaker: &#20351;&#29992;&#25193;&#25955;&#21464;&#25442;&#22120;&#36827;&#34892;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05712
&lt;/p&gt;
&lt;p&gt;
DiffSpeaker&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#21464;&#25442;&#22120;&#21644;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#35821;&#38899;&#39537;&#21160;3D&#38754;&#37096;&#21160;&#30011;&#32593;&#32476;&#65292;&#36890;&#36807;&#35299;&#20915;&#37197;&#23545;&#38899;&#39057;-4D&#25968;&#25454;&#30340;&#32570;&#20047;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#21767;&#37096;&#21516;&#27493;&#21644;&#38750;&#35821;&#35328;&#38754;&#37096;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#23545;&#20110;&#35768;&#22810;&#22810;&#23186;&#20307;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25110;&#21464;&#25442;&#22120;&#26550;&#26500;&#37117;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#31616;&#21333;&#32858;&#21512;&#24182;&#27809;&#26377;&#24102;&#26469;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24576;&#30097;&#36825;&#26159;&#30001;&#20110;&#32570;&#20047;&#37197;&#23545;&#30340;&#38899;&#39057;-4D&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#21464;&#25442;&#22120;&#22312;&#25193;&#25955;&#26694;&#26550;&#20869;&#26377;&#25928;&#22320;&#20316;&#20026;&#21435;&#22122;&#22120;&#36827;&#34892;&#24037;&#20316;&#38750;&#24120;&#20851;&#38190;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffSpeaker&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#32593;&#32476;&#65292;&#37197;&#22791;&#20102;&#26032;&#39062;&#30340;&#26377;&#20559;&#26465;&#20214;&#27880;&#24847;&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#20316;&#20026;&#20256;&#32479;&#21464;&#25442;&#22120;&#20013;&#33258;&#27880;&#24847;&#21147;/&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#21697;&#65292;&#34701;&#20837;&#20102;&#32463;&#36807;&#28145;&#24605;&#29087;&#34385;&#30340;&#20559;&#35265;&#65292;&#20197;&#20351;&#27880;&#24847;&#26426;&#21046;&#38598;&#20013;&#22312;&#26082;&#19982;&#20219;&#21153;&#30456;&#20851;&#21448;&#19982;&#25193;&#25955;&#30456;&#20851;&#30340;&#26465;&#20214;&#19978;&#12290;&#25105;&#20204;&#36824;&#22312;&#25193;&#25955;&#33539;&#24335;&#20013;&#25506;&#35752;&#20102;&#31934;&#30830;&#30340;&#21767;&#37096;&#21516;&#27493;&#21644;&#38750;&#35821;&#35328;&#38754;&#37096;&#34920;&#24773;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#21767;&#37096;&#21516;&#27493;&#65292;&#32780;&#19988;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#38750;&#35821;&#35328;&#38754;&#37096;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-driven 3D facial animation is important for many multimedia applications. Recent work has shown promise in using either Diffusion models or Transformer architectures for this task. However, their mere aggregation does not lead to improved performance. We suspect this is due to a shortage of paired audio-4D data, which is crucial for the Transformer to effectively perform as a denoiser within the Diffusion framework. To tackle this issue, we present DiffSpeaker, a Transformer-based network equipped with novel biased conditional attention modules. These modules serve as substitutes for the traditional self/cross-attention in standard Transformers, incorporating thoughtfully designed biases that steer the attention mechanisms to concentrate on both the relevant task-specific and diffusion-related conditions. We also explore the trade-off between accurate lip synchronization and non-verbal facial expressions within the Diffusion paradigm. Experiments show our model not only achieves
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20154;-&#26426;&#32452;&#21512;&#20013;&#30340;&#24615;&#33021;&#25552;&#20986;&#20102;&#31163;&#32447;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#24314;&#27169;&#65292;&#35299;&#20915;&#20102;&#22810;&#26679;&#21270;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.05703</link><description>&lt;p&gt;
&#31163;&#32447;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#20197;&#25552;&#39640;&#20154;-&#26426;&#32452;&#21512;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Offline Risk-sensitive RL with Partial Observability to Enhance Performance in Human-Robot Teaming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20154;-&#26426;&#32452;&#21512;&#20013;&#30340;&#24615;&#33021;&#25552;&#20986;&#20102;&#31163;&#32447;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#24314;&#27169;&#65292;&#35299;&#20915;&#20102;&#22810;&#26679;&#21270;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#29983;&#29702;&#35745;&#31639;&#25972;&#21512;&#21040;&#28151;&#21512;&#20513;&#35758;&#30340;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#23558;&#23454;&#26102;&#29305;&#24449;&#20316;&#20026;&#20154;&#31867;&#29366;&#24577;&#35266;&#27979;&#34701;&#20837;&#20915;&#31574;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#33258;&#20027;&#20219;&#21153;&#20998;&#37197;&#20013;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20248;&#21183;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26234;&#33021;&#22320;&#20998;&#37197;&#20219;&#21153;&#26469;&#20943;&#36731;&#20154;&#31867;&#25805;&#20316;&#21592;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;&#28982;&#32780;&#65292;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#29983;&#29702;&#21644;&#34892;&#20026;&#27979;&#37327;&#32467;&#26524;&#30340;&#22810;&#26679;&#21270;&#20154;&#31867;&#21442;&#19982;&#32773;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24517;&#39035;&#21033;&#29992;&#27010;&#29575;&#26694;&#26550;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#29366;&#24577;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;ORL&#65289;&#26041;&#27861;&#20174;&#20808;&#21069;&#25910;&#38598;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#20165;&#24378;&#35843;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#34920;&#31034;&#30340;&#28508;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
The integration of physiological computing into mixed-initiative human-robot interaction systems offers valuable advantages in autonomous task allocation by incorporating real-time features as human state observations into the decision-making system. This approach may alleviate the cognitive load on human operators by intelligently allocating mission tasks between agents. Nevertheless, accommodating a diverse pool of human participants with varying physiological and behavioral measurements presents a substantial challenge. To address this, resorting to a probabilistic framework becomes necessary, given the inherent uncertainty and partial observability on the human's state. Recent research suggests to learn a Partially Observable Markov Decision Process (POMDP) model from a data set of previously collected experiences that can be solved using Offline Reinforcement Learning (ORL) methods. In the present work, we not only highlight the potential of partially observable representations an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#26469;&#33258;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#12290;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#21487;&#20197;&#22312;&#22238;&#31572;&#26597;&#35810;&#21069;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#65292;&#24182;&#36890;&#36807;MATRIX-simulated&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#20445;&#25345;&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#36981;&#20174;&#21644;&#25512;&#29702;&#36895;&#24230;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;</title><link>https://arxiv.org/abs/2402.05699</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#22404;&#26029;&#23545;&#35805;&#30340;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#26469;&#33258;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#12290;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#21487;&#20197;&#22312;&#22238;&#31572;&#26597;&#35810;&#21069;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#65292;&#24182;&#36890;&#36807;MATRIX-simulated&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#20445;&#25345;&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#36981;&#20174;&#21644;&#25512;&#29702;&#36895;&#24230;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#65292;&#20197;&#20943;&#36731;&#20854;&#34987;&#28389;&#29992;&#36896;&#25104;&#30340;&#28508;&#22312;&#19981;&#33391;&#24433;&#21709;&#65292;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#20511;&#37492;&#31038;&#20250;&#23398;&#30340;&#35265;&#35299;&#65292;&#21363;&#35748;&#35782;&#21040;&#25152;&#26377;&#21508;&#26041;&#30340;&#20851;&#20999;&#26159;&#22609;&#36896;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23545;&#40784;LLMs&#30340;&#26032;&#26041;&#21521;&#65306;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MATRIX&#30340;&#21019;&#26032;&#31038;&#20132;&#22330;&#26223;&#27169;&#25311;&#22120;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#36755;&#20837;&#26597;&#35810;&#21608;&#22260;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#20351;LLM&#22312;&#22238;&#31572;&#21069;&#33021;&#22815;&#32771;&#34385;&#31038;&#20132;&#21518;&#26524;&#12290;MATRIX&#31867;&#20284;&#20110;&#19968;&#20010;&#8220;&#22404;&#26029;&#23545;&#35805;&#8221;&#19979;&#30340;&#34394;&#25311;&#25490;&#32451;&#31354;&#38388;&#65292;LLM&#22312;&#20854;&#20013;&#25198;&#28436;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#22810;&#20010;&#35282;&#33394;&#24182;&#36827;&#34892;&#33258;&#25105;&#23454;&#36341;&#12290;&#20026;&#20102;&#24341;&#20837;&#36825;&#31181;&#23545;&#40784;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;MATRIX&#27169;&#25311;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#30830;&#20445;&#20854;&#22312;&#19981;&#24433;&#21709;&#25512;&#29702;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#31526;&#21512;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#24102;&#26377;MATRIX&#30340;LLM&#32988;&#36807;&#20102;&#23466;&#27861;AI&#12290;&#26368;&#21518;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that ou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20934;&#30830;&#21448;&#26131;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#19978;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#23454;&#38469;&#19978;&#36229;&#36807;&#20102;&#21442;&#32771;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05680</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#21487;&#35299;&#37322;&#24615;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Interpretable classifiers for tabular data via discretization and feature selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20934;&#30830;&#21448;&#26131;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#19978;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#23454;&#38469;&#19978;&#36229;&#36807;&#20102;&#21442;&#32771;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#25152;&#24471;&#21040;&#30340;&#20998;&#31867;&#22120;&#26159;&#31616;&#30701;&#30340;DNF&#20844;&#24335;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#25968;&#25454;&#31163;&#25955;&#21270;&#20026;&#24067;&#23572;&#24418;&#24335;&#65292;&#28982;&#21518;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#32467;&#21512;&#38750;&#24120;&#24555;&#36895;&#30340;&#31639;&#27861;&#26469;&#20135;&#29983;&#26368;&#20339;&#30340;&#24067;&#23572;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;14&#20010;&#23454;&#39564;&#26469;&#28436;&#31034;&#35813;&#26041;&#27861;&#65292;&#24471;&#21040;&#30340;&#32467;&#26524;&#30340;&#20934;&#30830;&#24230;&#20027;&#35201;&#19982;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#20197;&#21450;&#25991;&#29486;&#20013;&#30456;&#21516;&#25968;&#25454;&#38598;&#30340;&#29616;&#26377;&#32467;&#26524;&#30456;&#20284;&#12290;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#38469;&#19978;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#21442;&#32771;&#32467;&#26524;&#65292;&#23613;&#31649;&#25105;&#20204;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#30340;&#21363;&#26102;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#20010;&#20851;&#20110;&#20174;&#29616;&#23454;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#20998;&#31867;&#22120;&#19982;&#26469;&#33258;&#25968;&#25454;&#32972;&#26223;&#20998;&#24067;&#30340;&#26368;&#20339;&#20998;&#31867;&#22120;&#30456;&#23545;&#24212;&#30340;&#27010;&#29575;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method for computing immediately human interpretable yet accurate classifiers from tabular data. The classifiers obtained are short DNF-formulas, computed via first discretizing the original data to Boolean form and then using feature selection coupled with a very fast algorithm for producing the best possible Boolean classifier for the setting. We demonstrate the approach via 14 experiments, obtaining results with accuracies mainly similar to ones obtained via random forests, XGBoost, and existing results for the same datasets in the literature. In several cases, our approach in fact outperforms the reference results in relation to accuracy, even though the main objective of our study is the immediate interpretability of our classifiers. We also prove a new result on the probability that the classifier we obtain from real-life data corresponds to the ideally best classifier with respect to the background distribution the data comes from.
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05668</link><description>&lt;p&gt;
&#23545;LLMs&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Assessment of Jailbreak Attacks Against LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05668
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28389;&#29992;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#37319;&#21462;&#20102;&#23433;&#20840;&#25514;&#26045;&#20197;&#30830;&#20445;LLMs&#31526;&#21512;&#31038;&#20250;&#20262;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#32469;&#36807;LLMs&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#65292;&#34987;&#31216;&#20026;&#36234;&#29425;&#25915;&#20987;&#12290;&#36890;&#36807;&#24212;&#29992;&#25216;&#26415;&#65292;&#22914;&#35282;&#33394;&#25198;&#28436;&#22330;&#26223;&#12289;&#23545;&#25239;&#24615;&#26679;&#26412;&#25110;&#23545;&#23433;&#20840;&#30446;&#26631;&#30340;&#24494;&#22937;&#30772;&#22351;&#20316;&#20026;&#25552;&#31034;&#65292;LLMs&#21487;&#20197;&#20135;&#29983;&#19981;&#36866;&#24403;&#29978;&#33267;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#34429;&#28982;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#30740;&#31350;&#20102;&#20960;&#31181;&#36234;&#29425;&#25915;&#20987;&#30340;&#31867;&#21035;&#65292;&#20294;&#20182;&#20204;&#37117;&#26159;&#23396;&#31435;&#22320;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#21508;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#27979;&#37327;&#12290;&#25105;&#20204;&#38598;&#20013;&#22312;&#26469;&#33258;&#22235;&#20010;&#31867;&#21035;&#30340;13&#31181;&#23574;&#31471;&#36234;&#29425;&#26041;&#27861;&#12289;16&#31181;&#36829;&#35268;&#31867;&#21035;&#30340;160&#20010;&#38382;&#39064;&#20197;&#21450;&#20845;&#31181;&#27969;&#34892;&#30340;LLMs&#19978;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#22987;&#32456;&#33021;&#22815;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
Misuse of the Large Language Models (LLMs) has raised widespread concern. To address this issue, safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, LLMs can produce an inappropriate or even harmful response. While researchers have studied several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular LLMs. Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#25928;&#26524;&#30340;&#28145;&#24230;&#39044;&#27979;&#26041;&#27861;SA-LSTM&#65292;&#36890;&#36807;&#23558;&#33258;&#27880;&#24847;&#21147;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27493;&#39044;&#27979;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.05663</link><description>&lt;p&gt;
&#23454;&#26102;&#29942;&#39048;&#21644;&#28608;&#27874;&#39044;&#27979;&#30340;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05663
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#25928;&#26524;&#30340;&#28145;&#24230;&#39044;&#27979;&#26041;&#27861;SA-LSTM&#65292;&#36890;&#36807;&#23558;&#33258;&#27880;&#24847;&#21147;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27493;&#39044;&#27979;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#23454;&#26102;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#22312;&#20132;&#36890;&#25511;&#21046;&#30740;&#31350;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29305;&#21035;&#26159;CIRCLES&#32852;&#21512;&#39033;&#30446;&#38656;&#35201;&#39044;&#27979;&#25216;&#26415;&#26469;&#20943;&#36731;&#25968;&#25454;&#28304;&#24310;&#36831;&#30340;&#24433;&#21709;&#12290;&#22312;MegaVanderTest&#23454;&#39564;&#21462;&#24471;&#25104;&#21151;&#20043;&#21518;&#65292;&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#24403;&#21069;&#31995;&#32479;&#38480;&#21046;&#65292;&#24320;&#21457;&#26356;&#36866;&#21512;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#19979;&#19968;&#36718;&#23454;&#39564;&#30340;&#23454;&#26102;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SA-LSTM&#65292;&#36825;&#26159;&#19968;&#31181;&#28145;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#65288;SA&#65289;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#22312;&#31354;&#38388;&#32500;&#24230;&#19978;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#27493;&#39044;&#27979;&#65292;&#20351;&#29992;n-step SA-LSTM&#65292;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#20043;&#38388;&#30340;&#24179;&#34913;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#22810;&#27493;&#39044;&#27979;&#26041;&#27861;&#65292;&#21516;&#26102;&#23454;&#26102;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate real-time traffic state forecasting plays a pivotal role in traffic control research. In particular, the CIRCLES consortium project necessitates predictive techniques to mitigate the impact of data source delays. After the success of the MegaVanderTest experiment, this paper aims at overcoming the current system limitations and develop a more suited approach to improve the real-time traffic state estimation for the next iterations of the experiment. In this paper, we introduce the SA-LSTM, a deep forecasting method integrating Self-Attention (SA) on the spatial dimension with Long Short-Term Memory (LSTM) yielding state-of-the-art results in real-time mesoscale traffic forecasting. We extend this approach to multi-step forecasting with the n-step SA-LSTM, which outperforms traditional multi-step forecasting methods in the trade-off between short-term and long-term predictions, all while operating in real-time.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#26032;&#35780;&#20272;GNN&#22312;&#22270;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#20316;&#29992;&#65292;&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#20256;&#25773;&#36807;&#31243;&#23545;&#20110;&#36866;&#24212;&#19981;&#21516;&#22270;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25552;&#20379;&#20102;&#22810;&#23618;GNN&#30340;&#27867;&#21270;&#30028;&#38480;&#30340;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.05660</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#26080;&#30417;&#30563;&#22270;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#20256;&#25773;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Propagation for Unsupervised Graph Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05660
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#35780;&#20272;GNN&#22312;&#22270;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#20316;&#29992;&#65292;&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#20256;&#25773;&#36807;&#31243;&#23545;&#20110;&#36866;&#24212;&#19981;&#21516;&#22270;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25552;&#20379;&#20102;&#22810;&#23618;GNN&#30340;&#27867;&#21270;&#30028;&#38480;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#39046;&#22495;&#36866;&#24212;&#65288;UGDA&#65289;&#26088;&#22312;&#23558;&#26631;&#35760;&#28304;&#22270;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22270;&#20013;&#65292;&#20197;&#35299;&#20915;&#22270;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23398;&#20064;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#23545;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#30340;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;GNN&#30340;&#20869;&#22312;&#27867;&#21270;&#33021;&#21147;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#20102;&#12290;&#22312;&#32463;&#39564;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#37325;&#26032;&#35780;&#20272;&#20102;GNN&#22312;&#22270;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#20256;&#25773;&#36807;&#31243;&#22312;GNN&#20013;&#36866;&#24212;&#19981;&#21516;&#22270;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#23545;UGDA&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25512;&#23548;&#20986;&#22810;&#23618;GNN&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#36890;&#36807;&#23558;GNN Lipschitz&#24212;&#29992;&#20110;k&#23618;GNNs&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#22312;&#28304;&#22270;&#20013;&#21024;&#38500;&#20256;&#25773;&#23618;&#24182;&#22312;&#30446;&#26631;&#22270;&#20013;&#22534;&#21472;&#22810;&#20010;&#20256;&#25773;&#23618;&#65292;&#21487;&#20197;&#20351;&#30446;&#26631;&#39118;&#38505;&#30028;&#38480;&#26356;&#32039;&#23494;&#12290;&#22522;&#20110;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Graph Domain Adaptation (UGDA) aims to transfer knowledge from a labelled source graph to an unlabelled target graph in order to address the distribution shifts between graph domains. Previous works have primarily focused on aligning data from the source and target graph in the representation space learned by graph neural networks (GNNs). However, the inherent generalization capability of GNNs has been largely overlooked. Motivated by our empirical analysis, we reevaluate the role of GNNs in graph domain adaptation and uncover the pivotal role of the propagation process in GNNs for adapting to different graph domains. We provide a comprehensive theoretical analysis of UGDA and derive a generalization bound for multi-layer GNNs. By formulating GNN Lipschitz for k-layer GNNs, we show that the target risk bound can be tighter by removing propagation layers in source graph and stacking multiple propagation layers in target graph. Based on the empirical and theoretical analysis
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20174;&#26410;&#26631;&#35760;&#30340;&#22810;&#24863;&#23448;&#25968;&#25454;&#20013;&#23398;&#20064;&#20016;&#23500;&#32780;&#26377;&#24847;&#20041;&#30340;3D&#22330;&#26223;&#34920;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#24863;&#35273;&#27169;&#24577;&#20043;&#38388;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#20960;&#20309;&#23545;&#40784;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21040;&#24378;&#22823;&#32780;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;3D&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#19982;&#30417;&#30563;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#23398;&#21040;&#30340;&#34920;&#31034;&#22312;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#35774;&#32622;&#19979;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05650</link><description>&lt;p&gt;
&#12298;&#23721;&#30707;&#32534;&#30721;&#65292;&#19981;&#26159;&#24320;&#21457;-&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;LLM&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#35780;&#20272;&#12299;
&lt;/p&gt;
&lt;p&gt;
Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05650
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20174;&#26410;&#26631;&#35760;&#30340;&#22810;&#24863;&#23448;&#25968;&#25454;&#20013;&#23398;&#20064;&#20016;&#23500;&#32780;&#26377;&#24847;&#20041;&#30340;3D&#22330;&#26223;&#34920;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#24863;&#35273;&#27169;&#24577;&#20043;&#38388;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#20960;&#20309;&#23545;&#40784;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21040;&#24378;&#22823;&#32780;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;3D&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#19982;&#30417;&#30563;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#23398;&#21040;&#30340;&#34920;&#31034;&#22312;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#35774;&#32622;&#19979;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29983;&#25104;&#22411;AI&#22240;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39640;&#36136;&#37327;&#34920;&#29616;&#32780;&#22791;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;ChatGPT&#21457;&#24067;&#20043;&#21518;&#12290;&#35768;&#22810;&#20154;&#35748;&#20026;&#23427;&#20204;&#26377;&#28508;&#21147;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#25191;&#34892;&#36890;&#29992;&#38382;&#39064;&#35299;&#20915;&#65292;&#24182;&#21462;&#20195;&#20154;&#31867;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#23545;&#36825;&#20123;LLM&#25216;&#26415;&#22312;&#23436;&#25104;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#36827;&#34892;&#28145;&#20837;&#35843;&#26597;&#12290;&#22312;&#19968;&#39033;&#26377;109&#21517;&#21442;&#19982;&#32773;&#30340;&#21463;&#25511; 2x2 &#21463;&#35797;&#32773;&#38388;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;ChatGPT&#21512;&#20316;&#22312;&#32534;&#30721;&#20219;&#21153;&#21644;&#20856;&#22411;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#20013;&#30340;&#25928;&#29992;&#31243;&#24230;&#20197;&#21450;&#20154;&#20204;&#22914;&#20309;&#20351;&#29992;ChatGPT&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;ChatGPT&#22312;&#35299;&#20915;&#31616;&#21333;&#30340;&#32534;&#30721;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#22312;&#25903;&#25345;&#20856;&#22411;&#30340;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#24182;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#20102;&#21442;&#19982;&#32773;&#19982;ChatGPT&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#24182;&#25214;&#21040;&#20102;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLM) based generative AI has been gaining momentum for their impressive high-quality performances in multiple domains, particularly after the release of the ChatGPT. Many believe that they have the potential to perform general-purpose problem-solving in software development and replace human software developers. Nevertheless, there are in a lack of serious investigation into the capability of these LLM techniques in fulfilling software development tasks. In a controlled 2 $\times$ 2 between-subject experiment with 109 participants, we examined whether and to what degree working with ChatGPT was helpful in the coding task and typical software development task and how people work with ChatGPT. We found that while ChatGPT performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good. We also observed the interactions between participants and ChatGPT and found the relations between the i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.05643</link><description>&lt;p&gt;
&#36890;&#36807;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Token-Based World Models with Parallel Observation Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23558;Transformer&#24212;&#29992;&#20110;&#31163;&#25955;&#31526;&#21495;&#24207;&#21015;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;TBWMs&#65289;&#20316;&#20026;&#39640;&#25928;&#26679;&#26412;&#26041;&#27861;&#12290;&#22312;TBWMs&#20013;&#65292;&#19990;&#30028;&#27169;&#22411;&#23558;&#20195;&#29702;&#32463;&#39564;&#20316;&#20026;&#19968;&#31181;&#31867;&#20284;&#35821;&#35328;&#30340;&#20196;&#29260;&#24207;&#21015;&#36827;&#34892;&#28040;&#32791;&#65292;&#20854;&#20013;&#27599;&#20010;&#35266;&#27979;&#26500;&#25104;&#19968;&#20010;&#23376;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#22312;&#24819;&#35937;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#20196;&#29260;&#36880;&#20010;&#29983;&#25104;&#19979;&#19968;&#20010;&#35266;&#27979;&#30340;&#20018;&#34892;&#26041;&#24335;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#38271;&#12289;GPU&#21033;&#29992;&#29575;&#20302;&#21644;&#34920;&#31034;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#65288;POP&#65289;&#26426;&#21046;&#12290;POP&#36890;&#36807;&#19968;&#31181;&#38024;&#23545;&#25105;&#20204;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#35774;&#35745;&#30340;&#26032;&#22411;&#21069;&#21521;&#27169;&#24335;&#26469;&#25193;&#20805;&#20102;&#20445;&#25345;&#32593;&#32476;&#65288;RetNet&#65289;&#12290;&#25105;&#20204;&#23558;POP&#38598;&#25104;&#21040;&#19968;&#31181;&#21517;&#20026;REM&#65288;&#20445;&#25345;&#29615;&#22659;&#27169;&#22411;&#65289;&#30340;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#65292;&#23637;&#31034;&#20102;&#27604;&#20197;&#21069;&#30340;TBWMs&#24555;15.4&#20493;&#30340;&#24819;&#35937;&#33021;&#21147;&#12290;REM&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#30340;26&#20010;&#28216;&#25103;&#20013;&#30340;12&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#23436;&#25104;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28595;&#26032;&#38134;&#34892;&#36825;&#26679;&#19968;&#20010;&#22823;&#22411;&#32452;&#32455;&#20013;&#65292;&#23558;AI&#24037;&#20855;GitHub Copilot&#25972;&#21512;&#21040;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#35813;&#24037;&#20855;&#22312;&#23454;&#38469;&#24037;&#31243;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#22823;&#35268;&#27169;&#37319;&#29992;&#21518;&#23545;&#29983;&#20135;&#21147;&#30340;&#25913;&#21892;&#26159;&#26174;&#33879;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.05636</link><description>&lt;p&gt;
AI&#24037;&#20855;&#23545;&#28595;&#26032;&#38134;&#34892;&#24037;&#31243;&#30340;&#24433;&#21709;&#8212;&#8212;&#20851;&#20110;GitHub Copilot&#22312;&#20225;&#19994;&#29615;&#22659;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on GitHub Copilot within Coporate Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28595;&#26032;&#38134;&#34892;&#36825;&#26679;&#19968;&#20010;&#22823;&#22411;&#32452;&#32455;&#20013;&#65292;&#23558;AI&#24037;&#20855;GitHub Copilot&#25972;&#21512;&#21040;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#35813;&#24037;&#20855;&#22312;&#23454;&#38469;&#24037;&#31243;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#22823;&#35268;&#27169;&#37319;&#29992;&#21518;&#23545;&#29983;&#20135;&#21147;&#30340;&#25913;&#21892;&#26159;&#26174;&#33879;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#30340;&#26085;&#30410;&#27969;&#34892;&#24050;&#32463;&#23545;&#21253;&#25324;&#36719;&#20214;&#24037;&#31243;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#32452;&#32455;&#20013;&#23558;AI&#24037;&#20855;&#25972;&#21512;&#21040;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#20013;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#28966;&#28857;&#26159;&#28595;&#26032;&#38134;&#34892;&#65292;&#35813;&#38134;&#34892;&#25317;&#26377;&#36229;&#36807;5000&#21517;&#24037;&#31243;&#24072;&#65292;&#28085;&#30422;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#20351;&#29992;&#19968;&#27454;&#33879;&#21517;&#30340;AI&#24037;&#20855;GitHub Copilot&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#23454;&#38469;&#24037;&#31243;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#20998;&#20139;&#20102;&#22312;&#22823;&#35268;&#27169;&#37319;&#29992;GitHub Copilot&#21518;&#35266;&#23519;&#21040;&#30340;&#29983;&#20135;&#21147;&#25913;&#21892;&#30340;&#21021;&#27493;&#21457;&#29616;&#65292;&#32422;&#26377;1000&#21517;&#24037;&#31243;&#24072;&#22312;&#20351;&#29992;&#35813;&#24037;&#20855;&#12290;&#28595;&#26032;&#38134;&#34892;&#23545;GitHub Copilot&#36827;&#34892;&#20102;&#20026;&#26399;&#20845;&#21608;&#30340;&#23454;&#39564;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#21608;&#30340;&#20934;&#22791;&#21644;&#22235;&#21608;&#30340;&#20027;&#21160;&#27979;&#35797;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#21442;&#19982;&#32773;&#30340;&#24773;&#32490;&#20197;&#21450;&#35813;&#24037;&#20855;&#23545;&#29983;&#20135;&#21147;&#12289;&#20195;&#30721;&#36136;&#37327;&#21644;&#23433;&#20840;&#24615;&#30340;&#24433;&#21709;&#12290;&#26368;&#21021;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;GitHub Copilot&#36827;&#34892;&#36845;&#20195;&#21644;&#25913;&#36827;&#65292;&#21516;&#26102;&#35760;&#24405;&#20854;&#21453;&#39304;&#21644;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing popularity of AI, particularly Large Language Models (LLMs), has significantly impacted various domains, including Software Engineering. This study explores the integration of AI tools in software engineering practices within a large organization. We focus on ANZ Bank, which employs over 5000 engineers covering all aspects of the software development life cycle. This paper details an experiment conducted using GitHub Copilot, a notable AI tool, within a controlled environment to evaluate its effectiveness in real-world engineering tasks. Additionally, this paper shares initial findings on the productivity improvements observed after GitHub Copilot was adopted on a large scale, with about 1000 engineers using it. ANZ Bank's six-week experiment with GitHub Copilot included two weeks of preparation and four weeks of active testing. The study evaluated participant sentiment and the tool's impact on productivity, code quality, and security. Initially, participants used GitHub
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26059;&#36716;&#29305;&#24449;&#20013;&#30340;&#32465;&#23450;&#21160;&#21147;&#23398;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#20313;&#24358;&#32465;&#23450;&#8221;&#26426;&#21046;&#65292;&#20197;&#26367;&#20195;&#20256;&#32479;&#30340;&#8220;$\chi$-binding&#8221;&#26426;&#21046;&#12290;&#36890;&#36807;&#26174;&#24335;&#35745;&#31639;&#29305;&#24449;&#20043;&#38388;&#30340;&#23545;&#40784;&#21644;&#30456;&#24212;&#30340;&#26435;&#37325;&#35843;&#25972;&#65292;&#36825;&#19968;&#26032;&#26426;&#21046;&#33021;&#22815;&#36798;&#21040;&#19982;&#20256;&#32479;&#26426;&#21046;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#19982;&#33258;&#27880;&#24847;&#21147;&#21644;&#29983;&#29289;&#31070;&#32463;&#23398;&#26377;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.05627</link><description>&lt;p&gt;
&#26059;&#36716;&#29305;&#24449;&#20013;&#30340;&#32465;&#23450;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Binding Dynamics in Rotating Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26059;&#36716;&#29305;&#24449;&#20013;&#30340;&#32465;&#23450;&#21160;&#21147;&#23398;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#20313;&#24358;&#32465;&#23450;&#8221;&#26426;&#21046;&#65292;&#20197;&#26367;&#20195;&#20256;&#32479;&#30340;&#8220;$\chi$-binding&#8221;&#26426;&#21046;&#12290;&#36890;&#36807;&#26174;&#24335;&#35745;&#31639;&#29305;&#24449;&#20043;&#38388;&#30340;&#23545;&#40784;&#21644;&#30456;&#24212;&#30340;&#26435;&#37325;&#35843;&#25972;&#65292;&#36825;&#19968;&#26032;&#26426;&#21046;&#33021;&#22815;&#36798;&#21040;&#19982;&#20256;&#32479;&#26426;&#21046;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#19982;&#33258;&#27880;&#24847;&#21147;&#21644;&#29983;&#29289;&#31070;&#32463;&#23398;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#35748;&#30693;&#20013;&#65292;&#32465;&#23450;&#38382;&#39064;&#25551;&#36848;&#20102;&#22823;&#33041;&#22914;&#20309;&#28789;&#27963;&#22320;&#23558;&#21508;&#31181;&#20449;&#24687;&#25972;&#21512;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#23545;&#35937;&#34920;&#31034;&#30340;&#26410;&#35299;&#20043;&#35868;&#12290;&#31867;&#20284;&#22320;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20154;&#20204;&#36861;&#27714;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20197;&#23398;&#20064;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#26469;&#23454;&#29616;&#24378;&#22823;&#30340;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#27169;&#22411;&#12290;&#20511;&#37492;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#65292;&#26059;&#36716;&#29305;&#24449;&#36890;&#36807;&#24341;&#20837;&#30690;&#37327;&#29305;&#24449;&#26469;&#23398;&#20064;&#36825;&#31181;&#34920;&#31034;&#65292;&#30690;&#37327;&#29305;&#24449;&#30340;&#22823;&#23567;&#21253;&#21547;&#23545;&#35937;&#29305;&#24449;&#65292;&#26041;&#21521;&#21253;&#21547;&#23545;&#35937;&#20851;&#32852;&#12290;&#22312;&#26550;&#26500;&#30340;&#27599;&#20010;&#23618;&#20013;&#37117;&#23884;&#20837;&#20102;&#8220;$\chi$-binding&#8221;&#26426;&#21046;&#65292;&#24050;&#34987;&#35777;&#26126;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20102;&#35299;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26367;&#20195;&#30340;&#8220;&#20313;&#24358;&#32465;&#23450;&#8221;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#26174;&#24335;&#35745;&#31639;&#29305;&#24449;&#20043;&#38388;&#30340;&#23545;&#40784;&#24182;&#30456;&#24212;&#22320;&#35843;&#25972;&#26435;&#37325;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#23427;&#21487;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#19982;&#33258;&#27880;&#24847;&#21147;&#21644;&#29983;&#29289;&#31070;&#32463;&#23398;&#20135;&#29983;&#30452;&#25509;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In human cognition, the binding problem describes the open question of how the brain flexibly integrates diverse information into cohesive object representations. Analogously, in machine learning, there is a pursuit for models capable of strong generalization and reasoning by learning object-centric representations in an unsupervised manner. Drawing from neuroscientific theories, Rotating Features learn such representations by introducing vector-valued features that encapsulate object characteristics in their magnitudes and object affiliation in their orientations. The "$\chi$-binding" mechanism, embedded in every layer of the architecture, has been shown to be crucial, but remains poorly understood. In this paper, we propose an alternative "cosine binding" mechanism, which explicitly computes the alignment between features and adjusts weights accordingly, and we show that it achieves equivalent performance. This allows us to draw direct connections to self-attention and biological neu
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#30340;&#23567;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24207;&#21015;&#22411;&#20219;&#21153;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20219;&#21153;&#20013;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05616</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24207;&#21015;&#22411;&#20219;&#21153;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05616
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#23567;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24207;&#21015;&#22411;&#20219;&#21153;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20219;&#21153;&#20013;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#20855;&#26377;&#25968;&#30334;&#19975;&#21442;&#25968;&#30340;&#23567;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20316;&#24207;&#21015;&#22411;&#20219;&#21153;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#35299;&#20915;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#25216;&#33021;&#38656;&#27714;&#21644;&#26102;&#38388;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#21019;&#24314;&#23567;&#22411;&#19988;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#22522;&#20110;&#20219;&#21153;&#27169;&#22411;&#26080;&#27861;&#23436;&#25104;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;125M&#12289;350M&#21644;1.3B&#21442;&#25968;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65292;&#20351;&#29992;10,000&#21040;1,000,000&#20010;&#25351;&#20196;&#31034;&#20363;&#21487;&#20197;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21270;&#23398;&#20449;&#24687;&#23398;&#20219;&#21153;&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36830;&#32493;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26102;&#26399;&#23545;&#25913;&#36827;&#32467;&#26524;&#30340;&#20316;&#29992;&#65292;&#20197;&#21450;&#25968;&#25454;&#26684;&#24335;&#21644;&#39044;&#35757;&#32451;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#23545;&#25351;&#20196;&#24494;&#35843;&#25104;&#21151;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose that small pretrained foundational generative language models with millions of parameters can be utilized as a general learning framework for sequence-based tasks. Our proposal overcomes the computational resource, skill set, and timeline challenges associated with training neural networks and language models from scratch. Further, our approach focuses on creating small and highly specialized models that can accurately execute a challenging task of which the base model is incapable of performing. We demonstrate that 125M, 350M, and 1.3B parameter pretrained foundational language models can be instruction fine-tuned with 10,000-to-1,000,000 instruction examples to achieve near state-of-the-art results on challenging cheminformatics tasks. We also demonstrate the role of successive language model fine-tuning epochs on improved outcomes, as well as the importance of both data formatting and pretrained foundational language model selection for instruction fine-tuning success.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#29992;&#20110;&#31435;&#20307;&#35270;&#35273;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#31435;&#20307;&#35270;&#35273;&#25552;&#20379;&#30340;&#39069;&#22806;&#35270;&#35282;&#21644;&#30452;&#25509;&#25512;&#27979;&#29289;&#20307;&#30340;&#36317;&#31163;&#65292;&#35813;&#26041;&#27861;&#22312;6D&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#19988;&#21487;&#36866;&#29992;&#20110;&#20854;&#20182;&#22522;&#20110;&#23494;&#38598;&#29305;&#24449;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05610</link><description>&lt;p&gt;
&#25193;&#23637;&#29992;&#20110;&#31435;&#20307;&#35270;&#35273;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Extending 6D Object Pose Estimators for Stereo Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05610
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#29992;&#20110;&#31435;&#20307;&#35270;&#35273;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#31435;&#20307;&#35270;&#35273;&#25552;&#20379;&#30340;&#39069;&#22806;&#35270;&#35282;&#21644;&#30452;&#25509;&#25512;&#27979;&#29289;&#20307;&#30340;&#36317;&#31163;&#65292;&#35813;&#26041;&#27861;&#22312;6D&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#19988;&#21487;&#36866;&#29992;&#20110;&#20854;&#20182;&#22522;&#20110;&#23494;&#38598;&#29305;&#24449;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#31283;&#20581;&#22320;&#20272;&#35745;&#29289;&#20307;&#30340;6D&#23039;&#24577;&#20173;&#28982;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30452;&#25509;&#20174;RGB&#22270;&#20687;&#20013;&#20351;&#29992;&#23494;&#38598;&#29305;&#24449;&#22238;&#24402;&#23039;&#24577;&#30340;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#31435;&#20307;&#35270;&#35273;&#25552;&#20379;&#20102;&#23545;&#29289;&#20307;&#30340;&#39069;&#22806;&#35270;&#35282;&#65292;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#23039;&#24577;&#27495;&#20041;&#21644;&#36974;&#25377;&#12290;&#27492;&#22806;&#65292;&#31435;&#20307;&#22270;&#20687;&#21487;&#20197;&#30452;&#25509;&#25512;&#27979;&#29289;&#20307;&#30340;&#36317;&#31163;&#65292;&#32780;&#21333;&#30446;&#35270;&#35273;&#21017;&#38656;&#35201;&#20869;&#32622;&#23545;&#35937;&#23610;&#23544;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#23558;&#26368;&#20808;&#36827;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#25193;&#23637;&#21040;&#31435;&#20307;&#35270;&#35273;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#19982;BOP&#20860;&#23481;&#30340;YCB-V&#25968;&#25454;&#38598;&#30340;&#31435;&#20307;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#31435;&#20307;&#35270;&#35273;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;6D&#23039;&#24577;&#20272;&#35745;&#31639;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#20854;&#20182;&#22522;&#20110;&#23494;&#38598;&#29305;&#24449;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the 6D pose of objects accurately, quickly, and robustly remains a difficult task. However, recent methods for directly regressing poses from RGB images using dense features have achieved state-of-the-art results. Stereo vision, which provides an additional perspective on the object, can help reduce pose ambiguity and occlusion. Moreover, stereo can directly infer the distance of an object, while mono-vision requires internalized knowledge of the object's size. To extend the state-of-the-art in 6D object pose estimation to stereo, we created a BOP compatible stereo version of the YCB-V dataset. Our method outperforms state-of-the-art 6D pose estimation algorithms by utilizing stereo vision and can easily be adopted for other dense feature-based algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#21327;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#28151;&#21512;&#22242;&#38431;&#25480;&#26435;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;AI&#32463;&#29702;&#65288;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65289;&#20316;&#20026;&#22242;&#38431;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#65292;&#23398;&#20064;&#22242;&#38431;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#27169;&#22411;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#12290;</title><link>https://arxiv.org/abs/2402.05605</link><description>&lt;p&gt;
&#20248;&#21270;&#21327;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#28151;&#21512;&#22242;&#38431;&#20013;&#30340;&#25480;&#26435;
&lt;/p&gt;
&lt;p&gt;
Optimizing Delegation in Collaborative Human-AI Hybrid Teams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#21327;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#28151;&#21512;&#22242;&#38431;&#25480;&#26435;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;AI&#32463;&#29702;&#65288;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65289;&#20316;&#20026;&#22242;&#38431;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#65292;&#23398;&#20064;&#22242;&#38431;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#27169;&#22411;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#21644;&#33258;&#20027;&#31995;&#32479;&#20316;&#20026;&#28151;&#21512;&#22242;&#38431;&#20849;&#21516;&#36816;&#20316;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#30830;&#20445;&#22242;&#38431;&#30340;&#25104;&#21151;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#23558;&#22242;&#38431;&#25104;&#21592;&#31216;&#20026;&#20195;&#29702;&#20154;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#28151;&#21512;&#22242;&#38431;&#30340;&#24773;&#20917;&#65292;&#21363;&#22312;&#20219;&#20309;&#26102;&#20505;&#65292;&#21482;&#26377;&#19968;&#20010;&#22242;&#38431;&#25104;&#21592;&#65288;&#25511;&#21046;&#20195;&#29702;&#20154;&#65289;&#34987;&#25480;&#26435;&#20026;&#22242;&#38431;&#30340;&#25511;&#21046;&#32773;&#12290;&#20026;&#20102;&#30830;&#23450;&#26368;&#20339;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#36873;&#25321;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24341;&#20837;AI&#32463;&#29702;&#65288;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65289;&#30340;&#24819;&#27861;&#65292;&#35813;&#32463;&#29702;&#20316;&#20026;&#22242;&#38431;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#23398;&#20064;&#12290;&#32463;&#29702;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#20154;&#30340;&#34920;&#29616;&#21644;&#22242;&#38431;&#25152;&#22788;&#30340;&#29615;&#22659;/&#19990;&#30028;&#26469;&#23398;&#20064;&#34892;&#20026;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#36873;&#25321;&#20986;&#26368;&#29702;&#24819;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#12290;&#20026;&#20102;&#38480;&#23450;&#32463;&#29702;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#32422;&#26463;&#26465;&#20214;&#12290;&#32463;&#29702;&#30340;&#32422;&#26463;&#26465;&#20214;&#25351;&#31034;&#22242;&#38431;&#30340;&#21487;&#25509;&#21463;&#36816;&#20316;&#26041;&#24335;&#65292;&#22240;&#27492;&#22914;&#26524;&#22242;&#38431;&#36827;&#20837;&#19981;&#21487;&#25509;&#21463;&#24182;&#38656;&#35201;&#32463;&#29702;&#20171;&#20837;&#30340;&#29366;&#24577;&#65292;&#23601;&#20250;&#36829;&#21453;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
When humans and autonomous systems operate together as what we refer to as a hybrid team, we of course wish to ensure the team operates successfully and effectively. We refer to team members as agents. In our proposed framework, we address the case of hybrid teams in which, at any time, only one team member (the control agent) is authorized to act as control for the team. To determine the best selection of a control agent, we propose the addition of an AI manager (via Reinforcement Learning) which learns as an outside observer of the team. The manager learns a model of behavior linking observations of agent performance and the environment/world the team is operating in, and from these observations makes the most desirable selection of a control agent. We restrict the manager task by introducing a set of constraints. The manager constraints indicate acceptable team operation, so a violation occurs if the team enters a condition which is unacceptable and requires manager intervention. To
&lt;/p&gt;</description></item><item><title>AttnLRP&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#26469;&#35299;&#20915;&#20102;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#24402;&#22240;&#38382;&#39064;&#65292;&#20855;&#26377;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05602</link><description>&lt;p&gt;
AttnLRP: &#27880;&#24847;&#21147;&#24863;&#30693;&#30340;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#29992;&#20110;Transformer
&lt;/p&gt;
&lt;p&gt;
AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05602
&lt;/p&gt;
&lt;p&gt;
AttnLRP&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#26469;&#35299;&#20915;&#20102;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#24402;&#22240;&#38382;&#39064;&#65292;&#20855;&#26377;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#20559;&#35265;&#30340;&#39044;&#27979;&#21644;&#24187;&#35937;&#65292;&#36825;&#31361;&#26174;&#20102;&#29702;&#35299;&#20854;&#27169;&#22411;&#20869;&#37096;&#25512;&#29702;&#36807;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#23545;&#25972;&#20010;&#40657;&#30418;Transformer&#27169;&#22411;&#30340;&#20934;&#30830;&#24402;&#22240;&#24182;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#25193;&#23637;&#36880;&#23618;&#30456;&#20851;&#20256;&#36882;&#24402;&#22240;&#26041;&#27861;&#20197;&#22788;&#29702;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#34429;&#28982;&#23384;&#22312;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#33021;&#22815;&#24544;&#23454;&#19988;&#20840;&#38754;&#22320;&#24402;&#22240;Transformer&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#28508;&#22312;&#34920;&#31034;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#19982;&#21333;&#19968;&#21453;&#21521;&#20256;&#25773;&#30456;&#20284;&#12290;&#36890;&#36807;&#23545;Llama 2&#12289;Flan-T5&#21644;Vision Transformer&#26550;&#26500;&#19978;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#29702;&#35299;&#28508;&#22312;&#34920;&#31034;&#65292;&#20026;&#27010;&#24565;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a singular backward pass. Through extensive evaluations against existing methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#20165;&#36890;&#36807;&#21382;&#21490;&#32032;&#25551;&#37325;&#24314;&#28784;&#27877;&#38613;&#20687;&#30340;&#20840;&#33258;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#26102;&#22312;&#29616;&#22330;&#36827;&#34892;&#37325;&#24314;&#65292;&#24182;&#20026;&#19987;&#23478;&#25552;&#20379;&#19968;&#20010;&#26377;&#29992;&#30340;&#36215;&#28857;&#20197;&#25163;&#21160;&#37325;&#24314;&#38613;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.05593</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#20165;&#20174;&#21382;&#21490;&#32032;&#25551;&#20013;&#37325;&#24314;&#28784;&#27877;&#38613;&#20687;&#30340;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
A Concept for Reconstructing Stucco Statues from historic Sketches using synthetic Data only
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#20165;&#36890;&#36807;&#21382;&#21490;&#32032;&#25551;&#37325;&#24314;&#28784;&#27877;&#38613;&#20687;&#30340;&#20840;&#33258;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#26102;&#22312;&#29616;&#22330;&#36827;&#34892;&#37325;&#24314;&#65292;&#24182;&#20026;&#19987;&#23478;&#25552;&#20379;&#19968;&#20010;&#26377;&#29992;&#30340;&#36215;&#28857;&#20197;&#25163;&#21160;&#37325;&#24314;&#38613;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20013;&#19990;&#32426;&#65292;&#28784;&#27877;&#24037;&#20154;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#36763;&#35834;&#27604;&#20122;&#30340;&#32418;&#33394;&#39068;&#26009;&#22312;&#22681;&#19978;&#20808;&#21046;&#20316;&#38613;&#20687;&#30340;&#32032;&#25551;&#12290;&#22914;&#20170;&#65292;&#35768;&#22810;&#36825;&#20123;&#38613;&#20687;&#24050;&#32463;&#34987;&#25703;&#27585;&#65292;&#20294;&#26159;&#21033;&#29992;&#21407;&#22987;&#30340;&#36763;&#35834;&#27604;&#20122;&#32032;&#25551;&#65292;&#25105;&#20204;&#21487;&#20197;&#37325;&#24314;&#26368;&#32456;&#38613;&#20687;&#30340;&#26679;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#26041;&#27861;&#26469;&#37325;&#24314;&#28857;&#20113;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24425;&#33394;&#22270;&#20687;&#12289;&#28145;&#24230;&#22270;&#21644;&#34920;&#38754;&#27861;&#32447;&#26469;&#23637;&#31034;&#21021;&#27493;&#32467;&#26524;&#65292;&#21482;&#38656;&#35201;&#19968;&#24352;&#31616;&#21333;&#30340;&#32032;&#25551;&#65292;&#32780;&#19981;&#38656;&#35201;&#20854;&#20182;&#31867;&#20284;&#26679;&#26412;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#23454;&#26102;&#22312;&#29616;&#22330;&#36827;&#34892;&#37325;&#24314;&#65292;&#20363;&#22914;&#22312;&#23637;&#35272;&#20013;&#65292;&#25110;&#32773;&#20026;&#19987;&#23478;&#25552;&#20379;&#19968;&#20010;&#26377;&#29992;&#30340;&#36215;&#28857;&#65292;&#35797;&#22270;&#25163;&#21160;&#37325;&#24314;&#38613;&#20687;&#65292;&#21516;&#26102;&#21482;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
In medieval times, stuccoworkers used a red color, called sinopia, to first create a sketch of the to-be-made statue on the wall. Today, many of these statues are destroyed, but using the original drawings, deriving from the red color also called sinopia, we can reconstruct how the final statue might have looked.We propose a fully-automated approach to reconstruct a point cloud and show preliminary results by generating a color-image, a depth-map, as well as surface normals requiring only a single sketch, and without requiring a collection of other, similar samples. Our proposed solution allows real-time reconstruction on-site, for instance, within an exhibition, or to generate a useful starting point for an expert, trying to manually reconstruct the statue, all while using only synthetic data for training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SoftEDA&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36719;&#26631;&#31614;&#22312;&#22686;&#24378;&#25968;&#25454;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21487;&#33021;&#30772;&#22351;&#25991;&#26412;&#21407;&#22987;&#21547;&#20041;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05591</link><description>&lt;p&gt;
SoftEDA: &#29992;&#36719;&#26631;&#31614;&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#35268;&#21017;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SoftEDA&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36719;&#26631;&#31614;&#22312;&#22686;&#24378;&#25968;&#25454;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21487;&#33021;&#30772;&#22351;&#25991;&#26412;&#21407;&#22987;&#21547;&#20041;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#22240;&#20854;&#31616;&#21333;&#24615;&#32780;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#30772;&#22351;&#25991;&#26412;&#30340;&#21407;&#22987;&#21547;&#20041;&#65292;&#26368;&#32456;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36719;&#26631;&#31614;&#24212;&#29992;&#20110;&#22686;&#24378;&#25968;&#25454;&#30340;&#31616;&#21333;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19971;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24050;&#32463;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#20197;&#20415;&#22797;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rule-based text data augmentation is widely used for NLP tasks due to its simplicity. However, this method can potentially damage the original meaning of the text, ultimately hurting the performance of the model. To overcome this limitation, we propose a straightforward technique for applying soft labels to augmented data. We conducted experiments across seven different classification tasks and empirically demonstrated the effectiveness of our proposed approach. We have publicly opened our source code for reproducibility.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;AutoAugment&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#20013;&#30340;&#35821;&#20041;&#25439;&#23475;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#24378;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#24182;&#25552;&#21319;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05584</link><description>&lt;p&gt;
AutoAugment&#26159;&#20320;&#38656;&#35201;&#30340;&#65306;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#22686;&#24378;&#22522;&#20110;&#35268;&#21017;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AutoAugment Is What You Need: Enhancing Rule-based Augmentation Methods in Low-resource Regimes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;AutoAugment&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#20013;&#30340;&#35821;&#20041;&#25439;&#23475;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#24378;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#24182;&#25552;&#21319;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#21477;&#23376;&#30340;&#31163;&#25955;&#24615;&#36136;&#12290;&#23613;&#31649;&#22522;&#20110;&#35268;&#21017;&#30340;&#22686;&#24378;&#26041;&#27861;&#22240;&#20854;&#31616;&#21333;&#24615;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#35821;&#20041;&#25439;&#23475;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;&#26631;&#31614;&#65288;softEDA&#65289;&#36827;&#34892;&#31616;&#21333;&#25968;&#25454;&#22686;&#24378;&#65292;&#24182;&#37319;&#29992;&#26631;&#31614;&#24179;&#28369;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#27599;&#20010;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#25214;&#21040;&#26368;&#20339;&#22240;&#23376;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#27492;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;softEDA&#20173;&#28982;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;AutoAugment&#24212;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#21319;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#19988;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text data augmentation is a complex problem due to the discrete nature of sentences. Although rule-based augmentation methods are widely adopted in real-world applications because of their simplicity, they suffer from potential semantic damage. Previous researchers have suggested easy data augmentation with soft labels (softEDA), employing label smoothing to mitigate this problem. However, finding the best factor for each model and dataset is challenging; therefore, using softEDA in real-world applications is still difficult. In this paper, we propose adapting AutoAugment to solve this problem. The experimental results suggest that the proposed method can boost existing augmentation methods and that rule-based methods can enhance cutting-edge pre-trained language models. We offer the source code.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#21516;&#26102;&#23454;&#29616;&#32676;&#20307;&#26333;&#20809;&#20844;&#24179;&#24615;&#21644;&#32676;&#20869;&#31934;&#33521;&#20027;&#20041;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#38543;&#26102;&#30340;&#32676;&#20307;&#26333;&#20809;&#20844;&#24179;&#24615;&#20445;&#35777;&#21644;&#22312;&#27599;&#20010;&#32676;&#20307;&#20013;&#23454;&#29616;&#20010;&#20307;&#23618;&#38754;&#30340;&#31934;&#33521;&#20027;&#20041;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05575</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#21516;&#26102;&#23454;&#29616;&#32676;&#20307;&#26333;&#20809;&#20844;&#24179;&#24615;&#21644;&#32676;&#20869;&#31934;&#33521;&#20027;&#20041;
&lt;/p&gt;
&lt;p&gt;
Simultaneously Achieving Group Exposure Fairness and Within-Group Meritocracy in Stochastic Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05575
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#21516;&#26102;&#23454;&#29616;&#32676;&#20307;&#26333;&#20809;&#20844;&#24179;&#24615;&#21644;&#32676;&#20869;&#31934;&#33521;&#20027;&#20041;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#38543;&#26102;&#30340;&#32676;&#20307;&#26333;&#20809;&#20844;&#24179;&#24615;&#20445;&#35777;&#21644;&#22312;&#27599;&#20010;&#32676;&#20307;&#20013;&#23454;&#29616;&#20010;&#20307;&#23618;&#38754;&#30340;&#31934;&#33521;&#20027;&#20041;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#20013;&#30340;&#20844;&#24179;&#24615;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23545;&#21508;&#20010;&#33218;&#30340;&#26333;&#20809;&#20445;&#35777;&#12290;&#24403;&#33218;&#26681;&#25454;&#26576;&#20123;&#23646;&#24615;&#33258;&#28982;&#20998;&#32452;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#23618;&#20844;&#24179;&#24615;&#65292;&#23427;&#32771;&#34385;&#20102;&#20004;&#20010;&#23618;&#38754;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;&#31532;&#19968;&#23618;&#38754;&#65292;&#21452;&#23618;&#20844;&#24179;&#24615;&#20445;&#35777;&#27599;&#20010;&#32676;&#20307;&#26377;&#19968;&#23450;&#30340;&#26368;&#20302;&#26333;&#20809;&#12290;&#20026;&#20102;&#35299;&#20915;&#32676;&#20869;&#20010;&#20307;&#33218;&#30340;&#20998;&#37197;&#19981;&#22343;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#31532;&#20108;&#23618;&#38754;&#32771;&#34385;&#20102;&#31934;&#33521;&#20027;&#20041;&#20844;&#24179;&#24615;&#65292;&#30830;&#20445;&#27599;&#20010;&#33218;&#26681;&#25454;&#20854;&#22312;&#32676;&#20307;&#20013;&#30340;&#20248;&#21183;&#34987;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#20379;(i) &#38543;&#26102;&#30340;&#32676;&#20307;&#26333;&#20809;&#20844;&#24179;&#24615;&#20445;&#35777;&#21644;(ii) &#22312;&#27599;&#20010;&#32676;&#20307;&#20013;&#23454;&#29616;&#20010;&#20307;&#23618;&#38754;&#30340;&#31934;&#33521;&#20027;&#20041;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#35843;&#25972;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#26469;&#23454;&#29616;&#21452;&#23618;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches to fairness in stochastic multi-armed bandits (MAB) primarily focus on exposure guarantee to individual arms. When arms are naturally grouped by certain attribute(s), we propose Bi-Level Fairness, which considers two levels of fairness. At the first level, Bi-Level Fairness guarantees a certain minimum exposure to each group. To address the unbalanced allocation of pulls to individual arms within a group, we consider meritocratic fairness at the second level, which ensures that each arm is pulled according to its merit within the group. Our work shows that we can adapt a UCB-based algorithm to achieve a Bi-Level Fairness by providing (i) anytime Group Exposure Fairness guarantees and (ii) ensuring individual-level Meritocratic Fairness within each group. We first show that one can decompose regret bounds into two components: (a) regret due to anytime group exposure fairness and (b) regret due to meritocratic fairness within each group. Our proposed algorithm BF-UCB 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05569</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Node Classification With Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25104;&#21151;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#25104;&#23545;&#20132;&#20114;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36825;&#28608;&#21457;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#25968;&#25454;&#30340;&#24819;&#27861;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HyperGNNs&#65289;&#30340;&#21457;&#23637;&#12290;GNNs&#21644;HyperGNNs&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21516;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#34987;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#20960;&#20309;&#25299;&#25169;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#33410;&#28857;&#20998;&#31867;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#22823;&#22810;&#25968;HyperGNNs&#21487;&#20197;&#20351;&#29992;&#24102;&#26377;&#36229;&#22270;&#30340;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;GNN&#26469;&#36817;&#20284;&#12290;&#36825;&#23548;&#33268;&#20102;WCE-GNN&#65292;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;GNN&#21644;&#19968;&#20010;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#65288;WCE&#65289;&#65292;&#29992;&#20110;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23545;&#20110;&#20061;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WCE-GNN&#19981;&#20165;&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#65292;&#32780;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification. Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not o
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#65292;&#36890;&#36807;&#21442;&#25968;&#20849;&#20139;&#21644;&#23618;&#24207;&#21015;&#21270;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#20855;&#26377;&#25104;&#21315;&#19978;&#19975;&#20010;&#26410;&#30693;&#25968;&#30340;&#32447;&#24615;&#38382;&#39064;&#20197;&#21450;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#26410;&#30693;&#25968;&#30340;&#38382;&#39064;&#19978;&#20445;&#25345;&#20854;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#22312;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#20013;&#65292;&#35813;&#32593;&#32476;&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#23547;&#25214;&#26368;&#20339;&#30340;&#20809;&#28369;&#22120;&#65292;&#20197;&#25552;&#39640;&#20960;&#20309;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20108;&#38454;&#26925;&#22278;&#26041;&#31243;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#35823;&#24046;&#20256;&#25773;&#30697;&#38453;&#30340;&#35889;&#21322;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.05563</link><description>&lt;p&gt;
&#31070;&#32463;&#22810;&#37325;&#32593;&#26684;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Neural Multigrid Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05563
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#65292;&#36890;&#36807;&#21442;&#25968;&#20849;&#20139;&#21644;&#23618;&#24207;&#21015;&#21270;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#20855;&#26377;&#25104;&#21315;&#19978;&#19975;&#20010;&#26410;&#30693;&#25968;&#30340;&#32447;&#24615;&#38382;&#39064;&#20197;&#21450;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#26410;&#30693;&#25968;&#30340;&#38382;&#39064;&#19978;&#20445;&#25345;&#20854;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#22312;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#20013;&#65292;&#35813;&#32593;&#32476;&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#23547;&#25214;&#26368;&#20339;&#30340;&#20809;&#28369;&#22120;&#65292;&#20197;&#25552;&#39640;&#20960;&#20309;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20108;&#38454;&#26925;&#22278;&#26041;&#31243;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#35823;&#24046;&#20256;&#25773;&#30697;&#38453;&#30340;&#35889;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#26080;&#30697;&#38453;&#31070;&#32463;&#32593;&#32476;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#12290;&#35813;&#26550;&#26500;&#36275;&#22815;&#31616;&#21333;&#65292;&#21487;&#20197;&#22312;&#19981;&#21040;&#20116;&#21313;&#34892;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#23454;&#26045;&#65292;&#24182;&#19988;&#21253;&#21547;&#22823;&#37327;&#19981;&#21516;&#30340;&#22810;&#37325;&#32593;&#26684;&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#27809;&#26377;&#23494;&#38598;&#23618;&#30340;&#22266;&#23450;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#36845;&#20195;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#26631;&#20934;&#30340;&#35757;&#32451;&#21327;&#35758;&#19981;&#33021;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#27714;&#35299;&#22120;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#22256;&#38590;&#65292;&#25105;&#20204;&#20351;&#29992;&#21442;&#25968;&#20849;&#20139;&#21644;&#23618;&#30340;&#24207;&#21015;&#21270;&#12290;&#25152;&#24471;&#21040;&#30340;&#32593;&#32476;&#21487;&#20197;&#22312;&#25317;&#26377;&#25968;&#21315;&#20010;&#26410;&#30693;&#25968;&#30340;&#32447;&#24615;&#38382;&#39064;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#26410;&#30693;&#25968;&#30340;&#38382;&#39064;&#19978;&#20445;&#25345;&#20854;&#25928;&#29575;&#12290;&#20174;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#32593;&#32476;&#30340;&#35757;&#32451;&#23545;&#24212;&#20110;&#20026;&#20960;&#20309;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#23547;&#25214;&#26368;&#20339;&#20809;&#28369;&#22120;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20108;&#38454;&#26925;&#22278;&#26041;&#31243;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#27979;&#35797;&#30340;&#32447;&#24615;&#31995;&#32479;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20004;&#21040;&#20116;&#20493;&#36739;&#23567;&#30340;&#35823;&#24046;&#20256;&#25773;&#30697;&#38453;&#30340;&#35889;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a convenient matrix-free neural architecture for the multigrid method. The architecture is simple enough to be implemented in less than fifty lines of code, yet it encompasses a large number of distinct multigrid solvers. We argue that a fixed neural network without dense layers can not realize an efficient iterative method. Because of that, standard training protocols do not lead to competitive solvers. To overcome this difficulty, we use parameter sharing and serialization of layers. The resulting network can be trained on linear problems with thousands of unknowns and retains its efficiency on problems with millions of unknowns. From the point of view of numerical linear algebra network's training corresponds to finding optimal smoothers for the geometric multigrid method. We demonstrate our approach on a few second-order elliptic equations. For tested linear systems, we obtain from two to five times smaller spectral radius of the error propagation matrix compare to a bas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#36951;&#24536;&#22312;&#24322;&#36136;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#20851;&#38190;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;"&#38378;&#22238;"&#31639;&#27861;&#26469;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#24182;&#21462;&#24471;&#20248;&#24322;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05558</link><description>&lt;p&gt;
&#38378;&#22238;&#65306;&#29702;&#35299;&#21644;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Flashback: Understanding and Mitigating Forgetting in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#36951;&#24536;&#22312;&#24322;&#36136;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#20851;&#38190;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;"&#38378;&#22238;"&#31639;&#27861;&#26469;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#24182;&#21462;&#24471;&#20248;&#24322;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36951;&#24536;&#25110;&#32773;&#35828;&#22312;&#19981;&#21516;&#36718;&#27425;&#20013;&#30340;&#30693;&#35782;&#20002;&#22833;&#38459;&#30861;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#65292;&#23588;&#20854;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;&#26356;&#20026;&#26126;&#26174;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24378;&#35843;&#20102;&#36951;&#24536;&#22312;&#24322;&#36136;&#25968;&#25454;&#29615;&#22659;&#20013;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#20302;&#25928;&#23398;&#20064;&#36215;&#21040;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#30693;&#35782;&#20002;&#22833;&#26082;&#21457;&#29983;&#22312;&#23458;&#25143;&#31471;&#23616;&#37096;&#26356;&#26032;&#20013;&#65292;&#20063;&#21457;&#29983;&#22312;&#26381;&#21153;&#22120;&#31471;&#30340;&#32858;&#21512;&#27493;&#39588;&#20013;&#65307;&#21482;&#35299;&#20915;&#20854;&#20013;&#19968;&#20010;&#32780;&#24573;&#30053;&#21478;&#19968;&#20010;&#26080;&#27861;&#26377;&#25928;&#20943;&#36731;&#36951;&#24536;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24230;&#37327;&#36951;&#24536;&#30340;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#22312;&#26032;&#30693;&#35782;&#33719;&#21462;&#20013;&#26126;&#30830;&#35782;&#21035;&#36951;&#24536;&#12290;&#20511;&#21161;&#36825;&#20123;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#38378;&#22238;"&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#21160;&#24577;&#33976;&#39311;&#26041;&#27861;&#26469;&#35268;&#33539;&#21270;&#23616;&#37096;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#32858;&#21512;&#23427;&#20204;&#30340;&#30693;&#35782;&#12290;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;"&#38378;&#22238;"&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20943;&#36731;&#20102;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;6&#21040;16&#20010;&#36718;&#27425;&#20869;&#36798;&#21040;&#26356;&#24555;&#30340;&#30446;&#26631;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Federated Learning (FL), forgetting, or the loss of knowledge across rounds, hampers algorithm convergence, particularly in the presence of severe data heterogeneity among clients. This study explores the nuances of this issue, emphasizing the critical role of forgetting in FL's inefficient learning within heterogeneous data contexts. Knowledge loss occurs in both client-local updates and server-side aggregation steps; addressing one without the other fails to mitigate forgetting. We introduce a metric to measure forgetting granularly, ensuring distinct recognition amid new knowledge acquisition. Leveraging these insights, we propose Flashback, an FL algorithm with a dynamic distillation approach that is used to regularize the local models, and effectively aggregate their knowledge. Across different benchmarks, Flashback outperforms other methods, mitigates forgetting, and achieves faster round-to-target-accuracy, by converging in 6 to 16 rounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#8220;ChatCoach&#8221;&#65292;&#19968;&#20010;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21307;&#29983;&#21512;&#20316;&#30340;&#26694;&#26550;&#65292;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#26102;&#21453;&#39304;&#65292;&#20197;&#24110;&#21161;&#21307;&#23398;&#23398;&#21592;&#25552;&#39640;&#27807;&#36890;&#25216;&#24039;&#12290;</title><link>https://arxiv.org/abs/2402.05547</link><description>&lt;p&gt;
&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65306;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#8220;ChatCoach&#8221;&#65292;&#19968;&#20010;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21307;&#29983;&#21512;&#20316;&#30340;&#26694;&#26550;&#65292;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#26102;&#21453;&#39304;&#65292;&#20197;&#24110;&#21161;&#21307;&#23398;&#23398;&#21592;&#25552;&#39640;&#27807;&#36890;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20256;&#32479;&#24212;&#29992;&#20027;&#35201;&#38598;&#20013;&#22312;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#26381;&#21153;&#19978;&#65292;&#22686;&#24378;&#24739;&#32773;&#20114;&#21160;&#21644;&#25252;&#29702;&#20132;&#20184;&#65292;&#20363;&#22914;&#21307;&#23398;&#23545;&#35805;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;NLP&#22312;&#24110;&#21161;&#32463;&#39564;&#19981;&#20016;&#23500;&#30340;&#21307;&#29983;&#65292;&#29305;&#21035;&#26159;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#31561;&#39046;&#22495;&#30340;&#28508;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;ChatCoach&#8221;&#65292;&#19968;&#20010;&#38598;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#19968;&#20010;&#24739;&#32773;&#20195;&#29702;&#21644;&#19968;&#20010;&#36741;&#23548;&#20195;&#29702;&#20849;&#21516;&#25903;&#25345;&#21307;&#23398;&#23398;&#21592;&#22312;&#20250;&#35786;&#36807;&#31243;&#20013;&#32451;&#20064;&#21307;&#23398;&#27807;&#36890;&#25216;&#24039;&#12290;&#19982;&#20256;&#32479;&#30340;&#23545;&#35805;&#31995;&#32479;&#19981;&#21516;&#65292;ChatCoach&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#29615;&#22659;&#65292;&#21307;&#29983;&#21487;&#20197;&#22312;&#20854;&#20013;&#19982;&#24739;&#32773;&#20195;&#29702;&#36827;&#34892;&#21307;&#23398;&#23545;&#35805;&#12290;&#21516;&#26102;&#65292;&#36741;&#23548;&#20195;&#29702;&#20250;&#25552;&#20379;&#23454;&#26102;&#21453;&#39304;&#32473;&#21307;&#29983;&#12290;&#20026;&#20102;&#26500;&#24314;ChatCoach&#31995;&#32479;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#38598;&#25104;&#20102;ChatGPT&#21644;Llama2&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35780;&#20272;&#23427;&#20204;&#22312;&#20132;&#27969;&#21307;&#30103;&#36741;&#23548;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical dialogue systems. However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored. We introduce ``ChatCoach,'' an integrated human-AI cooperative framework. Within this framework, both a patient agent and a coaching agent collaboratively support medical learners in practicing their medical communication skills during consultations. Unlike traditional dialogue systems, ChatCoach provides a simulated environment where a human doctor can engage in medical dialogue with a patient agent. Simultaneously, a coaching agent provides real-time feedback to the doctor. To construct the ChatCoach system, we developed a dataset and integrated Large Language Models such as ChatGPT and Llama2, aiming to assess 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#32773;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#22411;&#27169;&#22411;&#65292;&#24182;&#19988;&#27604;&#22522;&#32447;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#35757;&#32451;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#24341;&#20837;Perceiver-based&#28436;&#21592;-&#35780;&#35770;&#32773;&#27169;&#22411;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#36328;&#27880;&#24847;&#21147;&#27169;&#22359;&#37197;&#21512;&#30340;&#20851;&#38190;&#27169;&#22411;&#29305;&#24449;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#21457;&#29616;&#34920;&#26126;&#65306;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#32773;&#31639;&#27861;&#26159;&#36880;&#28176;&#25670;&#33073;&#34892;&#20026;&#20811;&#38534;&#33539;&#24335;&#30340;&#19968;&#31181;&#33258;&#28982;&#36873;&#25321;&#65292;&#24182;&#19988;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#20174;&#27425;&#20248;&#31034;&#33539;&#25110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#25484;&#25569;&#22810;&#20010;&#39046;&#22495;&#30340;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.05546</link><description>&lt;p&gt;
&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#32773;&#24378;&#21270;&#23398;&#20064;&#25193;&#23637;&#21040;&#22823;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Offline Actor-Critic Reinforcement Learning Scales to Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#32773;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#22411;&#27169;&#22411;&#65292;&#24182;&#19988;&#27604;&#22522;&#32447;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#35757;&#32451;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#24341;&#20837;Perceiver-based&#28436;&#21592;-&#35780;&#35770;&#32773;&#27169;&#22411;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#36328;&#27880;&#24847;&#21147;&#27169;&#22359;&#37197;&#21512;&#30340;&#20851;&#38190;&#27169;&#22411;&#29305;&#24449;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#21457;&#29616;&#34920;&#26126;&#65306;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#32773;&#31639;&#27861;&#26159;&#36880;&#28176;&#25670;&#33073;&#34892;&#20026;&#20811;&#38534;&#33539;&#24335;&#30340;&#19968;&#31181;&#33258;&#28982;&#36873;&#25321;&#65292;&#24182;&#19988;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#20174;&#27425;&#20248;&#31034;&#33539;&#25110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#25484;&#25569;&#22810;&#20010;&#39046;&#22495;&#30340;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#32773;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#22411;&#27169;&#22411;&#65292;&#22914;transformer&#65292;&#24182;&#19988;&#36981;&#24490;&#19982;&#30417;&#30563;&#23398;&#20064;&#31867;&#20284;&#30340;&#25193;&#23637;&#35268;&#24459;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#32773;&#31639;&#27861;&#22312;&#21253;&#21547;132&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#20013;&#65292;&#21487;&#20197;&#32988;&#36807;&#24378;&#22823;&#30340;&#30417;&#30563;&#24335;&#34892;&#20026;&#20811;&#38534;&#22522;&#32447;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#27425;&#20248;&#21644;&#19987;&#23478;&#34892;&#20026;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Perceiver&#30340;&#28436;&#21592;-&#35780;&#35770;&#32773;&#27169;&#22411;&#65292;&#24182;&#38416;&#26126;&#20102;&#20351;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#36328;&#27880;&#24847;&#21147;&#27169;&#22359;&#37197;&#21512;&#24037;&#20316;&#25152;&#38656;&#30340;&#20851;&#38190;&#27169;&#22411;&#29305;&#24449;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;i&#65289;&#31616;&#21333;&#30340;&#31163;&#32447;&#28436;&#21592;&#35780;&#35770;&#32773;&#31639;&#27861;&#26159;&#36880;&#28176;&#36828;&#31163;&#24403;&#21069;&#20027;&#27969;&#34892;&#20026;&#20811;&#38534;&#33539;&#24335;&#30340;&#33258;&#28982;&#36873;&#25321;&#65292;ii&#65289;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#21487;&#20197;&#20174;&#27425;&#20248;&#31034;&#33539;&#25110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#25484;&#25569;&#35768;&#22810;&#39046;&#22495;&#30340;&#22810;&#20219;&#21153;&#31574;&#30053;&#65292;&#21253;&#25324;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that offline actor-critic reinforcement learning can scale to large models - such as transformers - and follows similar scaling laws as supervised learning. We find that offline actor-critic algorithms can outperform strong, supervised, behavioral cloning baselines for multi-task training on a large dataset containing both sub-optimal and expert behavior on 132 continuous control tasks. We introduce a Perceiver-based actor-critic model and elucidate the key model features needed to make offline RL work with self- and cross-attention modules. Overall, we find that: i) simple offline actor critic algorithms are a natural choice for gradually moving away from the currently predominant paradigm of behavioral cloning, and ii) via offline RL it is possible to learn multi-task policies that master many domains simultaneously, including real robotics tasks, from sub-optimal demonstrations or self-generated data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;RFL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#23458;&#25143;&#36129;&#29486;&#30340;&#32858;&#21512;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#22312;&#38750;&#30456;&#21516;&#20998;&#24067;&#29615;&#22659;&#19979;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05541</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#40065;&#26834;&#21644;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#30340;&#20652;&#21270;&#21058;&#65306;&#35299;&#23494;&#23458;&#25143;&#36129;&#29486;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning as a Catalyst for Robust and Fair Federated Learning: Deciphering the Dynamics of Client Contributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;RFL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#23458;&#25143;&#36129;&#29486;&#30340;&#32858;&#21512;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#22312;&#38750;&#30456;&#21516;&#20998;&#24067;&#29615;&#22659;&#19979;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#20135;&#29983;&#20102;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#20998;&#25955;&#30340;&#35774;&#22791;&#25110;&#31995;&#32479;&#19978;&#35757;&#32451;&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#24182;&#20445;&#30041;&#26412;&#22320;&#25968;&#25454;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31574;&#30053;&#32463;&#24120;&#24573;&#35270;&#32479;&#35745;&#24322;&#36136;&#24615;&#21644;&#23545;&#25932;&#23545;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#25152;&#24102;&#26469;&#30340;&#22256;&#38590;&#65292;&#36825;&#20123;&#22240;&#32032;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#20010;&#24615;&#21270;&#30340;FL&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#26469;&#36866;&#24212;&#20010;&#21035;&#23458;&#25143;&#30340;&#29305;&#28857;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#30340;&#33030;&#24369;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;RFL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#36866;&#24212;&#20248;&#21270;&#32858;&#21512;&#36807;&#31243;&#20013;&#23458;&#25143;&#36129;&#29486;&#30340;&#26032;&#26694;&#26550;&#65292;&#20174;&#32780;&#22686;&#24378;&#24694;&#24847;&#23458;&#25143;&#19979;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#22312;&#38750;&#30456;&#21516;&#20998;&#24067;&#29615;&#22659;&#19979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#33268;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#20197;&#20248;&#21270;&#23458;&#25143;&#36129;&#29486;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in federated learning (FL) have produced models that retain user privacy by training across multiple decentralized devices or systems holding local data samples. However, these strategies often neglect the inherent challenges of statistical heterogeneity and vulnerability to adversarial attacks, which can degrade model robustness and fairness. Personalized FL strategies offer some respite by adjusting models to fit individual client profiles, yet they tend to neglect server-side aggregation vulnerabilities. To address these issues, we propose Reinforcement Federated Learning (RFL), a novel framework that leverages deep reinforcement learning to adaptively optimize client contribution during aggregation, thereby enhancing both model robustness against malicious clients and fairness across participants under non-identically distributed settings. To achieve this goal, we propose a meticulous approach involving a Deep Deterministic Policy Gradient-based algorithm for co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#38544;&#31169;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#24635;&#32467;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;</title><link>https://arxiv.org/abs/2402.05525</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Model-Based Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#38544;&#31169;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#24635;&#32467;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20855;&#26377;&#38544;&#31169;&#20445;&#35777;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#35757;&#32451;&#19968;&#20010;&#30456;&#23545;&#20110;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#36712;&#36857;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DP-MORL&#65292;&#19968;&#31181;&#24102;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;MBRL&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;DP-FedAvg&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#29615;&#22659;&#30340;&#38544;&#31169;&#27169;&#22411;&#65292;DP-FedAvg&#26159;&#19968;&#31181;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#36712;&#36857;&#32423;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#20174;&#65288;&#21463;&#32602;&#30340;&#65289;&#38544;&#31169;&#27169;&#22411;&#20013;&#25512;&#23548;&#20986;&#31574;&#30053;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#19982;&#31995;&#32479;&#20132;&#20114;&#25110;&#35775;&#38382;&#36755;&#20837;&#25968;&#25454;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;DP-MORL&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;RL&#20195;&#29702;&#65292;&#24182;&#36827;&#19968;&#27493;&#27010;&#36848;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address offline reinforcement learning with privacy guarantees, where the goal is to train a policy that is differentially private with respect to individual trajectories in the dataset. To achieve this, we introduce DP-MORL, an MBRL algorithm coming with differential privacy guarantees. A private model of the environment is first learned from offline data using DP-FedAvg, a training method for neural networks that provides differential privacy guarantees at the trajectory level. Then, we use model-based policy optimization to derive a policy from the (penalized) private model, without any further interaction with the system or access to the input data. We empirically show that DP-MORL enables the training of private RL agents from offline data and we furthermore outline the price of privacy in this setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RLNet&#30340;&#40065;&#26834;&#32447;&#24615;&#21270;&#32593;&#32476;&#65292;&#36890;&#36807;&#20943;&#23569;&#24310;&#36831;&#24182;&#25913;&#21892;&#27169;&#22411;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#32780;&#40065;&#26834;&#30340;&#38544;&#31169;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.05521</link><description>&lt;p&gt;
&#32447;&#24615;&#21270;&#27169;&#22411;&#20197;&#23454;&#29616;&#39640;&#25928;&#32780;&#40065;&#26834;&#30340;&#38544;&#31169;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Linearizing Models for Efficient yet Robust Private Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RLNet&#30340;&#40065;&#26834;&#32447;&#24615;&#21270;&#32593;&#32476;&#65292;&#36890;&#36807;&#20943;&#23569;&#24310;&#36831;&#24182;&#25913;&#21892;&#27169;&#22411;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#32780;&#40065;&#26834;&#30340;&#38544;&#31169;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#26085;&#30410;&#20851;&#27880;&#23548;&#33268;&#20102;&#23458;&#25143;&#31471;-&#26381;&#21153;&#22120;&#24212;&#29992;&#20013;&#31169;&#26377;&#25512;&#29702;&#65288;PI&#65289;&#26694;&#26550;&#30340;&#21457;&#23637;&#65292;&#35813;&#26694;&#26550;&#26082;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21448;&#20445;&#25252;&#27169;&#22411;&#30693;&#35782;&#20135;&#26435;&#12290;&#28982;&#32780;&#65292;&#25152;&#38656;&#30340;&#23494;&#30721;&#21407;&#35821;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#24310;&#36831;&#24320;&#38144;&#65292;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#35201;&#27714;PI&#26381;&#21153;&#23545;&#21508;&#31181;&#33258;&#28982;&#21457;&#29983;&#30340;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#24050;&#26377;&#19968;&#20123;&#24037;&#20316;&#19987;&#27880;&#20110;&#24320;&#21457;&#36866;&#29992;&#20110;PI&#30340;&#24310;&#36831;-&#39640;&#25928;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#23545;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;RLNet&#65292;&#19968;&#31181;&#40065;&#26834;&#30340;&#32447;&#24615;&#21270;&#32593;&#32476;&#65292;&#36890;&#36807;&#20943;&#23569;&#39640;&#24310;&#36831;&#30340;ReLU&#25805;&#20316;&#25552;&#20379;&#24310;&#36831;&#25913;&#36827;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#22312;&#28165;&#26224;&#22270;&#20687;&#21644;&#25439;&#22351;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;RLNet&#27169;&#22411;&#25552;&#20379;&#20102;&#22312;&#28165;&#26224;&#22270;&#20687;&#12289;&#33258;&#28982;&#25200;&#21160;&#22270;&#20687;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#25200;&#21160;&#22270;&#20687;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#25913;&#21892;&#30340;&#8220;&#19977;&#36830;&#36194;&#8221;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing concern about data privacy has led to the development of private inference (PI) frameworks in client-server applications which protects both data privacy and model IP. However, the cryptographic primitives required yield significant latency overhead which limits its wide-spread application. At the same time, changing environments demand the PI service to be robust against various naturally occurring and gradient-based perturbations. Despite several works focused on the development of latency-efficient models suitable for PI, the impact of these models on robustness has remained unexplored. Towards this goal, this paper presents RLNet, a class of robust linearized networks that can yield latency improvement via reduction of high-latency ReLU operations while improving the model performance on both clean and corrupted images. In particular, RLNet models provide a "triple win ticket" of improved classification accuracy on clean, naturally perturbed, and gradient-based perturbe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT 4.0&#35780;&#20272;&#30740;&#31350;&#36136;&#37327;&#30340;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#20135;&#29983;&#31526;&#21512;&#26631;&#20934;&#30340;&#25991;&#26723;&#25688;&#35201;&#21644;&#36136;&#37327;&#35780;&#20272;&#29702;&#30001;&#12290;&#28982;&#32780;&#65292;&#19982;&#20316;&#32773;&#33258;&#25105;&#35780;&#20215;&#24471;&#20998;&#30456;&#27604;&#65292;ChatGPT-4&#30340;&#35780;&#20998;&#30456;&#20851;&#24615;&#36739;&#24369;&#65292;&#20294;&#22810;&#36718;&#35780;&#20998;&#30340;&#24179;&#22343;&#24471;&#20998;&#20855;&#26377;&#26174;&#33879;&#27491;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05519</link><description>&lt;p&gt;
ChatGPT&#33021;&#35780;&#20272;&#30740;&#31350;&#36136;&#37327;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT evaluate research quality?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT 4.0&#35780;&#20272;&#30740;&#31350;&#36136;&#37327;&#30340;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#20135;&#29983;&#31526;&#21512;&#26631;&#20934;&#30340;&#25991;&#26723;&#25688;&#35201;&#21644;&#36136;&#37327;&#35780;&#20272;&#29702;&#30001;&#12290;&#28982;&#32780;&#65292;&#19982;&#20316;&#32773;&#33258;&#25105;&#35780;&#20215;&#24471;&#20998;&#30456;&#27604;&#65292;ChatGPT-4&#30340;&#35780;&#20998;&#30456;&#20851;&#24615;&#36739;&#24369;&#65292;&#20294;&#22810;&#36718;&#35780;&#20998;&#30340;&#24179;&#22343;&#24471;&#20998;&#20855;&#26377;&#26174;&#33879;&#27491;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;: &#35780;&#20272;ChatGPT 4.0&#26159;&#21542;&#36275;&#22815;&#20934;&#30830;&#65292;&#33021;&#22815;&#33258;&#21160;&#35780;&#20272;&#26399;&#21002;&#35770;&#25991;&#30340;&#30740;&#31350;&#36136;&#37327;&#65292;&#20197;&#33410;&#30465;&#26102;&#38388;&#12290;&#35774;&#35745;/&#26041;&#27861;: &#36890;&#36807;&#20351;&#29992;2021&#24180;&#33521;&#22269;&#30740;&#31350;&#21331;&#36234;&#26694;&#26550;&#65288;REF&#65289;&#30340;&#21457;&#24067;&#35780;&#20998;&#25351;&#21335;&#21019;&#24314;&#19968;&#20010;&#30740;&#31350;&#35780;&#20272;ChatGPT&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25105;&#33258;&#24049;&#30340;51&#31687;&#25991;&#31456;&#20013;&#65292;&#24182;&#19982;&#25105;&#33258;&#24049;&#30340;&#36136;&#37327;&#21028;&#26029;&#36827;&#34892;&#27604;&#36739;&#65292;&#27979;&#35797;ChatGPT-4&#35780;&#20272;&#26399;&#21002;&#35770;&#25991;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;: ChatGPT-4&#21487;&#20197;&#29983;&#25104;&#31526;&#21512;REF&#26631;&#20934;&#30340;&#21512;&#29702;&#30340;&#25991;&#26723;&#25688;&#35201;&#21644;&#36136;&#37327;&#35780;&#20272;&#29702;&#30001;&#12290;&#23427;&#30340;&#24635;&#20307;&#35780;&#20998;&#19982;&#25105;&#23545;&#30456;&#21516;&#25991;&#31456;&#30340;&#33258;&#25105;&#35780;&#20215;&#24471;&#20998;&#20043;&#38388;&#23384;&#22312;&#36739;&#24369;&#30340;&#30456;&#20851;&#24615;&#65288;&#24179;&#22343;&#20026;r=0.281&#65292;&#22312;15&#27425;&#36845;&#20195;&#20013;&#24179;&#22343;&#26377;8&#27425;&#19982;0&#26174;&#33879;&#19981;&#21516;&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;15&#27425;&#36845;&#20195;&#30340;&#24179;&#22343;&#24471;&#20998;&#21576;&#29616;&#20986;&#20102;0.509&#30340;&#26174;&#33879;&#27491;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#23545;&#22810;&#36718;ChatGPT-4&#35780;&#20998;&#27714;&#24179;&#22343;&#20284;&#20046;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Assess whether ChatGPT 4.0 is accurate enough to perform research evaluations on journal articles to automate this time-consuming task. Design/methodology/approach: Test the extent to which ChatGPT-4 can assess the quality of journal articles using a case study of the published scoring guidelines of the UK Research Excellence Framework (REF) 2021 to create a research evaluation ChatGPT. This was applied to 51 of my own articles and compared against my own quality judgements. Findings: ChatGPT-4 can produce plausible document summaries and quality evaluation rationales that match the REF criteria. Its overall scores have weak correlations with my self-evaluation scores of the same documents (averaging r=0.281 over 15 iterations, with 8 being statistically significantly different from 0). In contrast, the average scores from the 15 iterations produced a statistically significant positive correlation of 0.509. Thus, averaging scores from multiple ChatGPT-4 rounds seems more effec
&lt;/p&gt;</description></item><item><title>NoisyICL&#36890;&#36807;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#24341;&#20837;&#22122;&#38899;&#65292;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#26657;&#20934;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;NoisyICL&#21487;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#12289;&#26356;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.05515</link><description>&lt;p&gt;
NoisyICL: &#19968;&#28857;&#22122;&#38899;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#12289;
&lt;/p&gt;
&lt;p&gt;
NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05515
&lt;/p&gt;
&lt;p&gt;
NoisyICL&#36890;&#36807;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#24341;&#20837;&#22122;&#38899;&#65292;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#26657;&#20934;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;NoisyICL&#21487;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#12289;&#26356;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064; (ICL) &#22312;&#39640;&#20808;&#39564;&#20559;&#24046;&#21644;&#19981;&#21487;&#20449;&#20219;&#30340;&#32622;&#20449;&#24230;&#30340;&#24433;&#21709;&#19979;&#65292;&#34920;&#29616;&#19981;&#20339;&#19988;&#26657;&#20934;&#19981;&#36275;&#12290;&#20197;&#24448;&#30340;&#19968;&#20123;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#25104;&#26412;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#25913;&#21892; ICL &#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; NoisyICL&#65292;&#36890;&#36807;&#38543;&#26426;&#22122;&#38899;&#25200;&#21160;&#27169;&#22411;&#21442;&#25968;&#26469;&#21162;&#21147;&#25552;&#39640;&#24615;&#33021;&#21644;&#26657;&#20934;&#24615;&#12290;&#25105;&#20204;&#22312;2&#20010;&#27169;&#22411;&#21644;12&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NoisyICL&#21487;&#20197;&#24110;&#21161;ICL&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;NoisyICL&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#26356;&#20844;&#24179;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#32622;&#20449;&#24230;&#26356;&#21487;&#20449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;NoisyICL&#26159;ICL&#30340;&#19968;&#31181;&#26377;&#25928;&#26657;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20195;&#30721;&#24050;&#19978;&#20256;&#33267;Github&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL) is suffering from unsatisfactory performance and under-calibration due to high prior bias and unfaithful confidence. Some previous works fine-tuned language models for better ICL performance with enormous datasets and computing costs. In this paper, we propose NoisyICL, simply perturbing the model parameters by random noises to strive for better performance and calibration. Our experiments on 2 models and 12 downstream datasets show that NoisyICL can help ICL produce more accurate predictions. Our further analysis indicates that NoisyICL enables the model to provide more fair predictions, and also with less unfaithful confidence. Therefore, we believe that NoisyICL is an effective calibration of ICL. Our experimental code is uploaded to Github.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36153;&#29992;&#25928;&#30410;&#39640;&#21644;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#27880;&#37322;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#26500;&#24314;&#20102;&#19968;&#20010;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;&#24182;&#24320;&#25918;&#20102;&#28304;&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.05512</link><description>&lt;p&gt;
GPT&#23545;&#20110;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20855;&#26377;&#22810;&#35821;&#35328;&#27880;&#37322;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
GPTs Are Multilingual Annotators for Sequence Generation Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36153;&#29992;&#25928;&#30410;&#39640;&#21644;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#27880;&#37322;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#26500;&#24314;&#20102;&#19968;&#20010;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;&#24182;&#24320;&#25918;&#20102;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27880;&#37322;&#26159;&#26500;&#24314;&#26032;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#36890;&#36807;&#20247;&#21253;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#30340;&#26041;&#27861;&#26082;&#32791;&#26102;&#21448;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#24403;&#22788;&#29702;&#20302;&#36164;&#28304;&#35821;&#35328;&#26102;&#65292;&#30001;&#20110;&#20247;&#21253;&#24037;&#20316;&#32773;&#30340;&#35821;&#35328;&#27744;&#24046;&#24322;&#65292;&#36825;&#20010;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#36153;&#29992;&#25928;&#30410;&#39640;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;&#65292;&#24182;&#33268;&#21147;&#20110;&#23558;&#27492;&#25968;&#25454;&#38598;&#24320;&#25918;&#32473;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#24050;&#32463;&#24320;&#25918;&#20102;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#65292;&#20197;&#20415;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#21487;&#22797;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data annotation is an essential step for constructing new datasets. However, the conventional approach of data annotation through crowdsourcing is both time-consuming and expensive. In addition, the complexity of this process increases when dealing with low-resource languages owing to the difference in the language pool of crowdworkers. To address these issues, this study proposes an autonomous annotation method by utilizing large language models, which have been recently demonstrated to exhibit remarkable performance. Through our experiments, we demonstrate that the proposed method is not just cost-efficient but also applicable for low-resource language annotation. Additionally, we constructed an image captioning dataset using our approach and are committed to open this dataset for future study. We have opened our source code for further study and reproducibility.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#24037;&#31243;&#26694;&#26550;(REOM)&#20197;&#23558;&#32534;&#35793;&#21518;&#30340;&#35774;&#22791;&#19978;TFLite&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35843;&#35797;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.05493</link><description>&lt;p&gt;
&#25506;&#32034;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Investigating White-Box Attacks for On-Device Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#24037;&#31243;&#26694;&#26550;(REOM)&#20197;&#23558;&#32534;&#35793;&#21518;&#30340;&#35774;&#22791;&#19978;TFLite&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35843;&#35797;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#21033;&#29992;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20174;&#30456;&#24212;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20013;&#36731;&#26131;&#25552;&#21462;&#20986;&#26469;&#12290;&#29616;&#26377;&#30340;&#35774;&#22791;&#19978;&#25915;&#20987;&#26041;&#27861;&#21482;&#33021;&#29983;&#25104;&#40657;&#30418;&#25915;&#20987;&#65292;&#36825;&#31181;&#26041;&#27861;&#36828;&#19981;&#22914;&#30333;&#30418;&#31574;&#30053;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;&#36825;&#26159;&#22240;&#20026;&#31227;&#21160;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22914;TFLite&#19981;&#25903;&#25345;&#26799;&#24230;&#35745;&#31639;&#65292;&#32780;&#26799;&#24230;&#35745;&#31639;&#23545;&#20110;&#30333;&#30418;&#25915;&#20987;&#31639;&#27861;&#26159;&#24517;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#21457;&#29616;&#21487;&#33021;&#20302;&#20272;&#20102;&#35774;&#22791;&#19978;&#25915;&#20987;&#30340;&#21361;&#23475;&#24615;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65306;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#30333;&#30418;&#31574;&#30053;&#30452;&#25509;&#21463;&#21040;&#25915;&#20987;&#65311;&#25105;&#20204;&#39318;&#20808;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#23558;&#35774;&#22791;&#19978;&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35843;&#35797;&#29256;&#26412;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#36870;&#21521;&#24037;&#31243;&#26694;&#26550;(REOM)&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#23558;&#32534;&#35793;&#21518;&#30340;&#35774;&#22791;&#19978;TFLite&#27169;&#22411;&#36870;&#21521;&#20026;&#21487;&#35843;&#35797;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;REOM
&lt;/p&gt;
&lt;p&gt;
Numerous mobile apps have leveraged deep learning capabilities. However, on-device models are vulnerable to attacks as they can be easily extracted from their corresponding mobile apps. Existing on-device attacking approaches only generate black-box attacks, which are far less effective and efficient than white-box strategies. This is because mobile deep learning frameworks like TFLite do not support gradient computing, which is necessary for white-box attacking algorithms. Thus, we argue that existing findings may underestimate the harmfulness of on-device attacks. To this end, we conduct a study to answer this research question: Can on-device models be directly attacked via white-box strategies? We first systematically analyze the difficulties of transforming the on-device model to its debuggable version, and propose a Reverse Engineering framework for On-device Models (REOM), which automatically reverses the compiled on-device TFLite model to the debuggable model. Specifically, REOM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#24037;&#20316;&#37327;&#20272;&#35745;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#23545;&#20110;&#39033;&#30446;&#35268;&#21010;&#21644;&#36164;&#28304;&#20998;&#37197;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.05484</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25552;&#21319;&#36719;&#20214;&#24037;&#20316;&#37327;&#20272;&#35745;&#65306;&#19968;&#39033;&#20840;&#38754;&#30340;&#30740;&#31350;&#21644;&#26694;&#26550;&#25552;&#35758;
&lt;/p&gt;
&lt;p&gt;
Leveraging AI for Enhanced Software Effort Estimation: A Comprehensive Study and Framework Proposal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#24037;&#20316;&#37327;&#20272;&#35745;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#23545;&#20110;&#39033;&#30446;&#35268;&#21010;&#21644;&#36164;&#28304;&#20998;&#37197;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36807;&#21435;&#20116;&#24180;&#65288;2017&#24180;&#33267;2023&#24180;&#65289;&#20013;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#36719;&#20214;&#24037;&#20316;&#37327;&#20272;&#35745;&#30340;&#24191;&#27867;&#30740;&#31350;&#12290;&#36890;&#36807;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#21644;&#19982;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#21253;&#25324;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#32447;&#24615;&#22238;&#24402;&#65292;&#38543;&#26426;&#26862;&#26519;&#31561;&#25216;&#26415;&#65289;&#30340;&#27604;&#36739;&#65292;&#25214;&#21040;&#20102;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#25552;&#20986;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26694;&#26550;&#26377;&#28508;&#21147;&#25552;&#21319;&#39033;&#30446;&#35268;&#21010;&#21644;&#36164;&#28304;&#20998;&#37197;&#65292;&#20026;&#36719;&#20214;&#39033;&#30446;&#24037;&#20316;&#37327;&#20272;&#35745;&#30340;&#30740;&#31350;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an extensive study on the application of AI techniques for software effort estimation in the past five years from 2017 to 2023. By overcoming the limitations of traditional methods, the study aims to improve accuracy and reliability. Through performance evaluation and comparison with diverse Machine Learning models, including Artificial Neural Network (ANN), Support Vector Machine (SVM), Linear Regression, Random Forest and other techniques, the most effective method is identified. The proposed AI-based framework holds the potential to enhance project planning and resource allocation, contributing to the research area of software project effort estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RIPPLE&#30340;&#24555;&#36895;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28508;&#24847;&#35782;&#21033;&#29992;&#21644;&#27169;&#20223;&#21160;&#20316;&#30340;&#24605;&#24819;&#65292;&#35299;&#20915;&#20102;&#36890;&#36807;&#36234;&#29425;&#25552;&#31034;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05467</link><description>&lt;p&gt;
&#24555;&#36895;&#20248;&#21270;LLM&#36234;&#29425;&#26041;&#27861;&#65306;&#36890;&#36807;&#28508;&#24847;&#35782;&#21033;&#29992;&#21644;&#27169;&#20223;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RIPPLE&#30340;&#24555;&#36895;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28508;&#24847;&#35782;&#21033;&#29992;&#21644;&#27169;&#20223;&#21160;&#20316;&#30340;&#24605;&#24819;&#65292;&#35299;&#20915;&#20102;&#36890;&#36807;&#36234;&#29425;&#25552;&#31034;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#36890;&#36807;&#20854;&#38750;&#20961;&#30340;&#25512;&#29702;&#21644;&#29702;&#35299;&#33021;&#21147;&#25913;&#21464;&#20102;&#20154;&#31867;&#29983;&#27963;&#12290;&#38543;&#30528;&#23427;&#20204;&#22312;&#25935;&#24863;&#20219;&#21153;&#20013;&#30340;&#22686;&#21152;&#20351;&#29992;&#65292;&#23433;&#20840;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20154;&#20204;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20197;&#30830;&#20445;LLMs&#19982;&#20154;&#31867;&#36947;&#24503;&#21407;&#21017;&#30456;&#19968;&#33268;&#65292;&#20197;&#30830;&#20445;&#20854;&#23433;&#20840;&#37096;&#32626;&#12290;&#23613;&#31649;&#26377;&#28508;&#21147;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#40784;&#30340;LLMs&#23481;&#26131;&#21463;&#21040;&#19987;&#38376;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25552;&#31034;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#65292;&#24341;&#21457;&#26292;&#21147;&#21644;&#26377;&#23475;&#20869;&#23481;&#12290;&#24403;&#20195;LLMs&#30340;&#31163;&#25955;&#26412;&#36136;&#21644;&#24222;&#22823;&#35268;&#27169;&#20351;&#24471;&#33258;&#21160;&#29983;&#25104;&#22810;&#26679;&#21270;&#12289;&#39640;&#25928;&#21644;&#24378;&#25928;&#30340;&#36234;&#29425;&#25552;&#31034;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RIPPLE&#65288;&#22522;&#20110;&#28508;&#24847;&#35782;&#21033;&#29992;&#21644;&#27169;&#20223;&#21160;&#20316;&#30340;&#24555;&#36895;&#20248;&#21270;&#65289;&#30340;&#26032;&#22411;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#20102;&#20004;&#20010;&#24515;&#29702;&#23398;&#27010;&#24565;&#30340;&#21551;&#21457;&#65306;&#28508;&#24847;&#35782;&#21644;&#27169;&#20223;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have become prevalent across diverse sectors, transforming human life with their extraordinary reasoning and comprehension abilities. As they find increased use in sensitive tasks, safety concerns have gained widespread attention. Extensive efforts have been dedicated to aligning LLMs with human moral principles to ensure their safe deployment. Despite their potential, recent research indicates aligned LLMs are prone to specialized jailbreaking prompts that bypass safety measures to elicit violent and harmful content. The intrinsic discrete nature and substantial scale of contemporary LLMs pose significant challenges in automatically generating diverse, efficient, and potent jailbreaking prompts, representing a continuous obstacle. In this paper, we introduce RIPPLE (Rapid Optimization via Subconscious Exploitation and Echopraxia), a novel optimization-based method inspired by two psychological concepts: subconsciousness and echopraxia, which describe the p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#22768;&#23398;&#20449;&#24687;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21160;&#24577;&#34701;&#21512;&#65288;UADF&#65289;&#30340;&#21518;&#26399;&#34701;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20811;&#26381;&#29983;&#25104;&#24615;&#38169;&#35823;&#32416;&#27491;&#65288;GER&#65289;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#23454;&#26045;UADF&#26041;&#27861;&#65292;&#22312;LLM&#20915;&#31574;&#30340;&#26631;&#35760;&#32423;&#20998;&#26512;&#21644;&#26657;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#21160;&#24577;&#22320;&#34701;&#21512;&#22768;&#23398;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;ASR&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05457</link><description>&lt;p&gt;
&#27704;&#36828;&#19981;&#23244;&#26202;&#65306;&#23558;&#22768;&#23398;&#20449;&#24687;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#22768;&#23398;&#20449;&#24687;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21160;&#24577;&#34701;&#21512;&#65288;UADF&#65289;&#30340;&#21518;&#26399;&#34701;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20811;&#26381;&#29983;&#25104;&#24615;&#38169;&#35823;&#32416;&#27491;&#65288;GER&#65289;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#23454;&#26045;UADF&#26041;&#27861;&#65292;&#22312;LLM&#20915;&#31574;&#30340;&#26631;&#35760;&#32423;&#20998;&#26512;&#21644;&#26657;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#21160;&#24577;&#22320;&#34701;&#21512;&#22768;&#23398;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;ASR&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25104;&#21151;&#22320;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#25104;&#21151;&#29992;&#20110;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#36755;&#20986;&#20043;&#19978;&#36827;&#34892;&#29983;&#25104;&#24615;&#38169;&#35823;&#32416;&#27491;&#65288;GER&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLM&#34987;&#29992;&#20110;&#23545;&#30001;ASR&#31995;&#32479;&#29983;&#25104;&#30340;N&#26368;&#20339;&#20551;&#35774;&#21015;&#34920;&#36827;&#34892;&#30452;&#25509;&#26144;&#23556;&#65292;&#29983;&#25104;&#39044;&#27979;&#30340;&#36755;&#20986;&#36716;&#24405;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;GER&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#20026;LLM&#22312;&#35757;&#32451;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#35821;&#38899;&#20449;&#21495;&#20013;&#21487;&#29992;&#30340;&#22768;&#23398;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21160;&#24577;&#34701;&#21512;&#65288;UADF&#65289;&#30340;&#26032;&#22411;&#21518;&#26399;&#34701;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#27880;&#20837;&#22768;&#23398;&#20449;&#24687;&#20197;&#29983;&#25104;&#39044;&#27979;&#36716;&#24405;&#26469;&#20811;&#26381;&#36825;&#31181;&#38480;&#21046;&#12290;UADF&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#24182;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#23427;&#39318;&#20808;&#20998;&#26512;&#21644;&#26657;&#20934;&#26631;&#35760;&#32423;LLM&#20915;&#31574;&#65292;&#28982;&#21518;&#65288;ii&#65289;&#23427;&#21160;&#24577;&#22320;&#21560;&#25910;&#22768;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have successfully shown that large language models (LLMs) can be successfully used for generative error correction (GER) on top of the automatic speech recognition (ASR) output. Specifically, an LLM is utilized to carry out a direct mapping from the N-best hypotheses list generated by an ASR system to the predicted output transcription. However, despite its effectiveness, GER introduces extra data uncertainty since the LLM is trained without taking into account acoustic information available in the speech signal. In this work, we aim to overcome such a limitation by infusing acoustic information before generating the predicted transcription through a novel late fusion solution termed Uncertainty-Aware Dynamic Fusion (UADF). UADF is a multimodal fusion approach implemented into an auto-regressive decoding process and works in two stages: (i) It first analyzes and calibrates the token-level LLM decision, and (ii) it then dynamically assimilates the information from the aco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Minecraft&#28216;&#25103;&#24212;&#29992;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#31995;&#32479;"Minecraft-ify"&#65292;&#33021;&#22815;&#29983;&#25104;&#38024;&#23545;3D&#34394;&#25311;&#35282;&#33394;&#30340;&#38754;&#37096;&#32858;&#28966;&#22270;&#20687;&#65292;&#24182;&#25903;&#25345;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#65292;&#25552;&#20379;&#20102;&#26356;&#33258;&#30001;&#21644;&#20248;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.05448</link><description>&lt;p&gt;
Minecraft-ify&#65306;&#29992;&#20110;&#28216;&#25103;&#24212;&#29992;&#30340;Minecraft&#39118;&#26684;&#22270;&#20687;&#29983;&#25104;&#19982;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Minecraft-ify: Minecraft Style Image Generation with Text-guided Image Editing for In-Game Application
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Minecraft&#28216;&#25103;&#24212;&#29992;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#31995;&#32479;"Minecraft-ify"&#65292;&#33021;&#22815;&#29983;&#25104;&#38024;&#23545;3D&#34394;&#25311;&#35282;&#33394;&#30340;&#38754;&#37096;&#32858;&#28966;&#22270;&#20687;&#65292;&#24182;&#25903;&#25345;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#65292;&#25552;&#20379;&#20102;&#26356;&#33258;&#30001;&#21644;&#20248;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#38754;&#21521;Minecraft&#35270;&#39057;&#28216;&#25103;&#30340;&#35282;&#33394;&#32441;&#29702;&#29983;&#25104;&#31995;&#32479;"Minecraft-ify"&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#29983;&#25104;&#38024;&#23545;&#20855;&#26377;&#31435;&#26041;&#20307;&#27969;&#24418;&#30340;3D&#34394;&#25311;&#35282;&#33394;&#30340;&#38754;&#37096;&#32858;&#28966;&#22270;&#20687;&#20197;&#36827;&#34892;&#32441;&#29702;&#26144;&#23556;&#12290;&#19982;&#29616;&#26377;&#39033;&#30446;&#25110;&#20316;&#21697;&#21482;&#29983;&#25104;&#32441;&#29702;&#19981;&#21516;&#65292;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#21453;&#36716;&#29992;&#25143;&#25552;&#20379;&#30340;&#30495;&#23454;&#22270;&#20687;&#65292;&#25110;&#20174;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#29983;&#25104;&#24179;&#22343;/&#38543;&#26426;&#22806;&#35266;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;StyleGAN&#21644;StyleCLIP&#36827;&#34892;&#25991;&#26412;&#24341;&#23548;&#30340;&#25805;&#20316;&#12290;&#36825;&#20123;&#21151;&#33021;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#29992;&#25143;&#20307;&#39564;&#21644;&#26356;&#22810;&#30340;&#33258;&#30001;&#65292;&#26159;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;AI&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we first present the character texture generation system \textit{Minecraft-ify}, specified to Minecraft video game toward in-game application. Ours can generate face-focused image for texture mapping tailored to 3D virtual character having cube manifold. While existing projects or works only generate texture, proposed system can inverse the user-provided real image, or generate average/random appearance from learned distribution. Moreover, it can be manipulated with text-guidance using StyleGAN and StyleCLIP. These features provide a more extended user experience with enlarged freedom as a user-friendly AI-tool. Project page can be found at https://gh-bumsookim.github.io/Minecraft-ify/
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#65292;&#39564;&#35777;&#20102;GPT-4&#29983;&#25104;&#30340;&#21465;&#36848;&#22312;&#20256;&#36798;&#29983;&#27963;&#20107;&#20214;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#21465;&#36848;&#33021;&#22815;&#36275;&#22815;&#20256;&#36798;&#25552;&#31034;&#30340;&#24847;&#22270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#21465;&#36848;&#12290;</title><link>https://arxiv.org/abs/2402.05435</link><description>&lt;p&gt;
GPT-4&#20351;&#29992;&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#29983;&#25104;&#29983;&#27963;&#20107;&#20214;&#30340;&#21465;&#36848;&#65306;&#19968;&#39033;&#39564;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#65292;&#39564;&#35777;&#20102;GPT-4&#29983;&#25104;&#30340;&#21465;&#36848;&#22312;&#20256;&#36798;&#29983;&#27963;&#20107;&#20214;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#21465;&#36848;&#33021;&#22815;&#36275;&#22815;&#20256;&#36798;&#25552;&#31034;&#30340;&#24847;&#22270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#21465;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#21508;&#31181;&#21465;&#36848;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#20419;&#36827;&#20102;&#23545;&#20854;&#22312;&#21465;&#36848;&#24418;&#24335;&#20013;&#20256;&#36798;&#29983;&#27963;&#20107;&#20214;&#25928;&#26524;&#30340;&#31995;&#32479;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#38646;-shot&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#65292;&#20351;&#29992;OpenAI&#30340;GPT-4&#29983;&#25104;&#20102;24,000&#20010;&#21465;&#36848;&#12290;&#20174;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#25163;&#21160;&#20998;&#31867;&#20102;2,880&#20010;&#21465;&#36848;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#20256;&#36798;&#20986;&#29983;&#12289;&#27515;&#20129;&#12289;&#25307;&#32856;&#21644;&#35299;&#38599;&#20107;&#20214;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;87.43%&#30340;&#21465;&#36848;&#36275;&#22815;&#20256;&#36798;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#24847;&#22270;&#12290;&#20026;&#20102;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#21465;&#36848;&#65292;&#25105;&#20204;&#23545;&#20998;&#31867;&#25968;&#25454;&#38598;&#35757;&#32451;&#21644;&#39564;&#35777;&#20102;&#20061;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#23545;&#21097;&#20313;21,120&#20010;&#21465;&#36848;&#30340;&#20998;&#31867;&#39044;&#27979;&#20998;&#26512;&#12290;&#25152;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23558;&#26377;&#25928;&#30340;&#21465;&#36848;&#20998;&#31867;&#20026;&#26377;&#25928;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21516;&#26102;&#23558;&#26080;&#25928;&#30340;&#21465;&#36848;&#20998;&#31867;&#20026;&#26080;&#25928;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#20165;&#25512;&#36827;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#36824;&#25552;&#20379;&#20102;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21465;&#36848;&#30340;&#26377;&#30410;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) play a pivotal role in generating vast arrays of narratives, facilitating a systematic exploration of their effectiveness for communicating life events in narrative form. In this study, we employ a zero-shot structured narrative prompt to generate 24,000 narratives using OpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and evaluate their validity in conveying birth, death, hiring, and firing events. Remarkably, 87.43% of the narratives sufficiently convey the intention of the structured prompt. To automate the identification of valid and invalid narratives, we train and validate nine Machine Learning models on the classified datasets. Leveraging these models, we extend our analysis to predict the classifications of the remaining 21,120 narratives. All the ML models excelled at classifying valid narratives as valid, but experienced challenges at simultaneously classifying invalid narratives as invalid. Our findings not only advance th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#36890;&#36807;&#25311;&#21512;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#24182;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#36827;&#34892;&#20998;&#31867;&#65292;&#25928;&#26524;&#30053;&#20248;&#20110;&#25110;&#19982;&#20116;&#20010;&#22522;&#20934;&#20998;&#31867;&#27169;&#22411;&#30456;&#24403;&#12290;&#22312;&#23454;&#38469;&#30340;&#20135;&#21697;&#25414;&#32465;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23398;&#20064;&#20135;&#21697;&#25903;&#20184;&#24847;&#24895;&#20998;&#24067;&#26041;&#38754;&#23637;&#29616;&#20986;&#30495;&#23454;&#30340;&#23454;&#29992;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05428</link><description>&lt;p&gt;
&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#29992;&#20110;&#20998;&#31867;&#21450;&#20854;&#22312;&#20135;&#21697;&#25414;&#32465;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Mixture Density Networks for Classification with an Application to Product Bundling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#36890;&#36807;&#25311;&#21512;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#24182;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#36827;&#34892;&#20998;&#31867;&#65292;&#25928;&#26524;&#30053;&#20248;&#20110;&#25110;&#19982;&#20116;&#20010;&#22522;&#20934;&#20998;&#31867;&#27169;&#22411;&#30456;&#24403;&#12290;&#22312;&#23454;&#38469;&#30340;&#20135;&#21697;&#25414;&#32465;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23398;&#20064;&#20135;&#21697;&#25903;&#20184;&#24847;&#24895;&#20998;&#24067;&#26041;&#38754;&#23637;&#29616;&#20986;&#30495;&#23454;&#30340;&#23454;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;(MDNs)&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21364;&#24456;&#23569;&#20351;&#29992;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;MDNs&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21487;&#29992;&#24615;&#19981;&#26126;&#30830;&#21644;&#30452;&#25509;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;MDN&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#23545;&#25968;&#25454;&#36827;&#34892;&#39640;&#26031;&#28151;&#21512;&#25311;&#21512;&#65292;&#24182;&#20351;&#29992;&#25311;&#21512;&#30340;&#20998;&#24067;&#36890;&#36807;&#35780;&#20272;&#32473;&#23450;&#36755;&#20837;&#29305;&#24449;&#30340;&#23398;&#20064;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#26469;&#23545;&#32473;&#23450;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;MDN&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#30053;&#20248;&#20110;&#25110;&#19982;&#20116;&#20010;&#22522;&#20934;&#20998;&#31867;&#27169;&#22411;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#30495;&#23454;&#25928;&#29992;&#36890;&#36807;&#19968;&#20010;&#23454;&#38469;&#30340;&#20135;&#21697;&#25414;&#32465;&#24212;&#29992;&#20013;&#24471;&#20197;&#23637;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#20110;MDN&#30340;&#27169;&#22411;&#20174;&#21512;&#25104;&#38144;&#21806;&#25968;&#25454;&#20013;&#23398;&#20064;&#20004;&#20010;&#20135;&#21697;&#30340;&#25903;&#20184;&#24847;&#24895;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
While mixture density networks (MDNs) have been extensively used for regression tasks, they have not been used much for classification tasks. One reason for this is that the usability of MDNs for classification is not clear and straightforward. In this paper, we propose two MDN-based models for classification tasks. Both models fit mixtures of Gaussians to the the data and use the fitted distributions to classify a given sample by evaluating the learnt cumulative distribution function for the given input features. While the proposed MDN-based models perform slightly better than, or on par with, five baseline classification models on three publicly available datasets, the real utility of our models comes out through a real-world product bundling application. Specifically, we use our MDN-based models to learn the willingness-to-pay (WTP) distributions for two products from synthetic sales data of the individual products. The Gaussian mixture representation of the learnt WTP distributions
&lt;/p&gt;</description></item><item><title>DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.05421</link><description>&lt;p&gt;
DiffTOP: &#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05421
&lt;/p&gt;
&lt;p&gt;
DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffTOP&#65292;&#23427;&#21033;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#65292;&#20026;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#29983;&#25104;&#21160;&#20316;&#12290;&#36712;&#36857;&#20248;&#21270;&#26159;&#19968;&#31181;&#22312;&#25511;&#21046;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65292;&#30001;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#25439;&#22833;&#23545;&#20110;&#36712;&#36857;&#20248;&#21270;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#36712;&#36857;&#20248;&#21270;&#30340;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#12290;DiffTOP&#35299;&#20915;&#20102;&#20043;&#21069;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#22240;&#20026;DiffTOP&#20013;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36890;&#36807;&#36712;&#36857;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#25439;&#22833;&#30452;&#25509;&#26368;&#22823;&#21270;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23545;DiffTOP&#22312;&#26631;&#20934;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#22871;&#20214;&#20013;&#36827;&#34892;&#20102;&#27169;&#20223;&#23398;&#20064;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;LEAP&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#29359;&#38169;&#35823;&#65292;&#28982;&#21518;&#21453;&#24605;&#24182;&#23398;&#20064;&#20934;&#21017;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.05403</link><description>&lt;p&gt;
&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20934;&#21017;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Principle Learning from Mistakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;LEAP&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#29359;&#38169;&#35823;&#65292;&#28982;&#21518;&#21453;&#24605;&#24182;&#23398;&#20064;&#20934;&#21017;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65292;&#20063;&#31216;&#20026;&#23569;&#26679;&#26412;&#25552;&#31034;&#65289;&#24050;&#25104;&#20026;&#23558;LLMs&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#30340;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#22522;&#20110;ICL&#30340;&#26041;&#27861;&#21482;&#20174;&#27491;&#30830;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#20013;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#36825;&#19968;&#33539;&#20363;&#65292;&#36890;&#36807;&#20174;&#23569;&#32473;&#23450;&#30340;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#20013;&#23398;&#20064;&#26356;&#22810;&#20869;&#23481;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23398;&#20064;&#20934;&#21017;&#65288;LEAP&#65289;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#26377;&#24847;&#35825;&#20351;&#27169;&#22411;&#22312;&#36825;&#20123;&#23569;&#37327;&#31034;&#20363;&#20013;&#29359;&#38169;&#35823;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#21453;&#24605;&#36825;&#20123;&#38169;&#35823;&#65292;&#24182;&#20174;&#20013;&#23398;&#20064;&#26174;&#24335;&#30340;&#20219;&#21153;&#29305;&#23450;&#8220;&#20934;&#21017;&#8221;&#65292;&#36825;&#20123;&#20934;&#21017;&#26377;&#21161;&#20110;&#35299;&#20915;&#31867;&#20284;&#30340;&#38382;&#39064;&#24182;&#36991;&#20813;&#24120;&#35265;&#30340;&#38169;&#35823;&#65307;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21407;&#22987;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#36825;&#20123;&#23398;&#21040;&#30340;&#36890;&#29992;&#20934;&#21017;&#26469;&#25552;&#31034;&#27169;&#22411;&#22238;&#31572;&#26410;&#35265;&#36807;&#30340;&#27979;&#35797;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65288;Hotpot QA&#65289;&#12289;&#25991;&#26412;&#38382;&#39064;&#22238;&#31572;&#65288;DROP&#65289;&#12289;Big-Bench&#22256;&#38590;&#25512;&#29702;&#21644;&#25968;&#23398;&#38382;&#39064;&#65288;GSM8K&#21644;MATH&#65289;&#22312;&#20869;&#30340;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;LEAP&#65307;&#22312;&#25152;&#26377;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;LEAP&#37117;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL, also known as few-shot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples. Nonetheless, all ICL-based approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then we reflect on these mistakes, and learn explicit task-specific "principles" from them, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#36741;&#21161;&#30340;&#33258;&#21160;&#35843;&#33410;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#39640;&#24230;&#21487;&#37197;&#32622;&#21442;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#35299;&#20915;&#36719;&#30828;&#20214;&#20043;&#38388;&#37197;&#32622;&#36873;&#39033;&#30340;&#20132;&#20114;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#26426;&#22120;&#20154;&#24179;&#21488;&#20043;&#38388;&#30340;&#24615;&#33021;&#36801;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.05399</link><description>&lt;p&gt;
CURE: &#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#27169;&#25311;&#36741;&#21161;&#33258;&#21160;&#35843;&#33410;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
CURE: Simulation-Augmented Auto-Tuning in Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#36741;&#21161;&#30340;&#33258;&#21160;&#35843;&#33410;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#39640;&#24230;&#21487;&#37197;&#32622;&#21442;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#35299;&#20915;&#36719;&#30828;&#20214;&#20043;&#38388;&#37197;&#32622;&#36873;&#39033;&#30340;&#20132;&#20114;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#26426;&#22120;&#20154;&#24179;&#21488;&#20043;&#38388;&#30340;&#24615;&#33021;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#31995;&#32479;&#36890;&#24120;&#30001;&#22810;&#20010;&#23376;&#31995;&#32479;&#32452;&#25104;&#65292;&#20363;&#22914;&#23450;&#20301;&#21644;&#23548;&#33322;&#65292;&#27599;&#20010;&#23376;&#31995;&#32479;&#21448;&#21253;&#21547;&#35768;&#22810;&#21487;&#37197;&#32622;&#30340;&#32452;&#20214;&#65288;&#20363;&#22914;&#36873;&#25321;&#19981;&#21516;&#30340;&#35268;&#21010;&#31639;&#27861;&#65289;&#12290;&#19968;&#26086;&#36873;&#25321;&#20102;&#26576;&#20010;&#31639;&#27861;&#65292;&#23601;&#38656;&#35201;&#35774;&#32622;&#30456;&#20851;&#30340;&#37197;&#32622;&#36873;&#39033;&#20197;&#36798;&#21040;&#36866;&#24403;&#30340;&#20540;&#12290;&#31995;&#32479;&#22534;&#26632;&#20013;&#30340;&#37197;&#32622;&#36873;&#39033;&#20250;&#20135;&#29983;&#22797;&#26434;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;&#22312;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#26426;&#22120;&#20154;&#20013;&#25214;&#21040;&#26368;&#20339;&#37197;&#32622;&#26469;&#23454;&#29616;&#26399;&#26395;&#30340;&#24615;&#33021;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36719;&#20214;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#37197;&#32622;&#36873;&#39033;&#20132;&#20114;&#23548;&#33268;&#20102;&#24222;&#22823;&#19988;&#22797;&#26434;&#30340;&#37197;&#32622;&#31354;&#38388;&#12290;&#24615;&#33021;&#36801;&#31227;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#21644;&#26426;&#22120;&#20154;&#24179;&#21488;&#20043;&#38388;&#20063;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#25968;&#25454;&#39640;&#25928;&#20248;&#21270;&#31639;&#27861;&#65288;&#20363;&#22914;&#36125;&#21494;&#26031;&#20248;&#21270;&#65289;&#24050;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#33258;&#21160;&#21270;&#35843;&#25972;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#21487;&#37197;&#32622;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20248;&#21270;&#31639;&#27861;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#24212;&#29992;&#20173;&#26377;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic systems are typically composed of various subsystems, such as localization and navigation, each encompassing numerous configurable components (e.g., selecting different planning algorithms). Once an algorithm has been selected for a component, its associated configuration options must be set to the appropriate values. Configuration options across the system stack interact non-trivially. Finding optimal configurations for highly configurable robots to achieve desired performance poses a significant challenge due to the interactions between configuration options across software and hardware that result in an exponentially large and complex configuration space. These challenges are further compounded by the need for transferability between different environments and robotic platforms. Data efficient optimization algorithms (e.g., Bayesian optimization) have been increasingly employed to automate the tuning of configurable parameters in cyber-physical systems. However, such optimiz
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TASER&#26041;&#27861;&#65292;&#23427;&#26159;&#38024;&#23545;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05396</link><description>&lt;p&gt;
TASER: &#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#24555;&#36895;&#20934;&#30830;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05396
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TASER&#26041;&#27861;&#65292;&#23427;&#26159;&#38024;&#23545;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGNN&#65289;&#22312;&#21253;&#25324;&#27450;&#35784;&#26816;&#27979;&#21644;&#20869;&#23481;&#25512;&#33616;&#22312;&#20869;&#30340;&#21508;&#31181;&#37325;&#35201;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;TGNN&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#26102;&#38388;&#36807;&#26102;&#30340;&#38142;&#25509;&#21644;&#20559;&#26012;&#30340;&#20132;&#20114;&#20998;&#24067;&#12290;&#36825;&#20123;&#22122;&#22768;&#23548;&#33268;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20005;&#37325;&#25439;&#23475;&#20102;TGNN&#30340;&#20934;&#30830;&#24615;&#65306;&#65288;1&#65289;&#27169;&#22411;&#21463;&#21040;&#36739;&#24046;&#20132;&#20114;&#30340;&#30417;&#30563;&#65292;&#65288;2&#65289;&#22122;&#22768;&#36755;&#20837;&#23548;&#33268;&#32858;&#21512;&#28040;&#24687;&#30340;&#39640;&#26041;&#24046;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;TGNN&#21435;&#22122;&#25216;&#26415;&#24182;&#26410;&#32771;&#34385;&#27599;&#20010;&#33410;&#28857;&#30340;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#22122;&#22768;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#38754;&#20020;&#30528;&#36941;&#21382;&#26356;&#22810;&#37051;&#23621;&#23548;&#33268;&#20135;&#29983;&#36807;&#22810;&#23567;&#25209;&#37327;&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#30456;&#20449;&#24555;&#36895;&#20934;&#30830;&#30340;TGNN&#30340;&#35299;&#20915;&#26041;&#27861;&#22312;&#20110;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TASER&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#36827;&#34892;&#20248;&#21270;&#30340;TGNN&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content recommendation. Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic graphs like time-deprecated links and skewed interaction distribution. The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are supervised by inferior interactions, and (2) noisy input induces high variance in the aggregated messages. However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node. In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors. We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling. In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and sc
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.05391</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05391
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#25512;&#21160;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#35821;&#20041;&#32593;&#32476;&#31038;&#21306;&#23545;&#22810;&#27169;&#24577;&#32500;&#24230;&#30340;&#25506;&#32034;&#20026;&#21019;&#26032;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;300&#22810;&#31687;&#25991;&#31456;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#24863;&#30693;&#30740;&#31350;&#65306;&#20197;&#30693;&#35782;&#22270;&#35889;&#25903;&#25345;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;KG&#39537;&#21160;&#22810;&#27169;&#24577;&#65288;KG4MM&#65289;&#23398;&#20064;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#30740;&#31350;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MM4KG&#65289;&#39046;&#22495;&#12290;&#25105;&#20204;&#20174;&#23450;&#20041;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#65292;&#28982;&#21518;&#25506;&#32034;&#23427;&#20204;&#30340;&#26500;&#24314;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#31867;&#21035;&#65306;KG&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#20197;&#21450;&#20869;&#22312;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#22914;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#23454;&#20307;&#23545;&#40784;&#65292;&#31361;&#20986;&#20102;&#20855;&#20307;&#30340;&#30740;&#31350;&#36712;&#36857;&#12290;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#20041;&#12289;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#20986;&#36827;&#34892;&#30456;&#20851;&#30740;&#31350;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;cu
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#29992;&#25143;&#28789;&#27963;&#21452;&#24037;&#32593;&#32476;&#20013;&#30340;&#29289;&#29702;&#23618;&#23433;&#20840;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#20854;&#22312;&#24615;&#33021;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.05378</link><description>&lt;p&gt;
&#29289;&#29702;&#23618;&#23433;&#20840;&#22312;&#22810;&#29992;&#25143;&#28789;&#27963;&#21452;&#24037;&#32593;&#32476;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Physical-Layer Security in Multi-User Flexible-Duplex Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#29992;&#25143;&#28789;&#27963;&#21452;&#24037;&#32593;&#32476;&#20013;&#30340;&#29289;&#29702;&#23618;&#23433;&#20840;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#20854;&#22312;&#24615;&#33021;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#28789;&#27963;&#21452;&#24037;&#32593;&#32476;&#20013;&#30340;&#29289;&#29702;&#23618;&#23433;&#20840;&#65288;PLS&#65289;&#65292;&#32771;&#34385;&#21040;&#31363;&#21548;&#32773;&#30340;&#24773;&#26223;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22260;&#32469;&#30528;&#27714;&#35299;&#24635;&#20445;&#23494;&#36895;&#29575;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#23637;&#24320;&#65292;&#29305;&#21035;&#26159;&#38754;&#23545;&#37319;&#29992;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#65288;MMSE&#65289;&#25509;&#25910;&#22120;&#30340;&#21327;&#35843;&#21644;&#20998;&#24067;&#24335;&#31363;&#21548;&#32773;&#26102;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#22522;&#20110;&#36845;&#20195;&#32463;&#20856;&#20248;&#21270;&#35299;&#21644;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26631;&#24535;&#30528;GNN&#22312;PLS&#24212;&#29992;&#19978;&#30340;&#21021;&#27493;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;GNN&#26041;&#27861;&#25193;&#23637;&#21040;&#35299;&#20915;&#31363;&#21548;&#32773;&#30340;&#20449;&#36947;&#20449;&#24687;&#32570;&#22833;&#38382;&#39064;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#24615;&#33021;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#26041;&#38754;&#65292;&#28789;&#27963;&#21452;&#24037;&#36890;&#20449;&#20248;&#20110;&#21322;&#21452;&#24037;&#65288;HD&#65289;&#36890;&#20449;&#65292;GNN&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores Physical-Layer Security (PLS) in Flexible Duplex (FlexD) networks, considering scenarios involving eavesdroppers. Our investigation revolves around the intricacies of the sum secrecy rate maximization problem, particularly when faced with coordinated and distributed eavesdroppers employing a Minimum Mean Square Error (MMSE) receiver. Our contributions include an iterative classical optimization solution and an unsupervised learning strategy based on Graph Neural Networks (GNNs). To the best of our knowledge, this work marks the initial exploration of GNNs for PLS applications. Additionally, we extend the GNN approach to address the absence of eavesdroppers' channel knowledge. Extensive numerical simulations highlight FlexD's superiority over Half-Duplex (HD) communications and the GNN approach's superiority over the classical method in both performance and time complexity.
&lt;/p&gt;</description></item><item><title>CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;</title><link>https://arxiv.org/abs/2402.05374</link><description>&lt;p&gt;
CIC&#65306;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CIC: A framework for Culturally-aware Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05374
&lt;/p&gt;
&lt;p&gt;
CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VLPs&#65289;&#22914;BLIP&#20174;&#22270;&#20687;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#65292;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#25152;&#25551;&#32472;&#30340;&#25991;&#21270;&#20803;&#32032;&#65288;&#20363;&#22914;&#20122;&#27954;&#25991;&#21270;&#32676;&#20307;&#30340;&#20256;&#32479;&#26381;&#35013;&#65289;&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#24615;&#23383;&#24149;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;\textbf{&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#65288;CIC&#65289;}&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#20195;&#34920;&#19981;&#21516;&#25991;&#21270;&#30340;&#22270;&#20687;&#20013;&#29983;&#25104;&#23383;&#24149;&#24182;&#25551;&#36848;&#25991;&#21270;&#20803;&#32032;&#12290;&#21463;&#21040;&#23558;&#35270;&#35273;&#27169;&#24577;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#36827;&#34892;&#32452;&#21512;&#30340;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#65288;1&#65289;&#26681;&#25454;&#22270;&#20687;&#20013;&#30340;&#25991;&#21270;&#31867;&#21035;&#29983;&#25104;&#38382;&#39064;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29983;&#25104;&#30340;&#38382;&#39064;&#20174;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20013;&#25552;&#21462;&#25991;&#21270;&#35270;&#35273;&#20803;&#32032;&#65292;&#65288;3&#65289;&#20351;&#29992;&#24102;&#26377;&#25552;&#31034;&#30340;LLMs&#29983;&#25104;&#25991;&#21270;&#24863;&#30693;&#23383;&#24149;&#12290;&#25105;&#20204;&#22312;4&#20010;&#19981;&#21516;&#22823;&#23398;&#30340;45&#21517;&#21442;&#19982;&#32773;&#19978;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 dif
&lt;/p&gt;</description></item><item><title>&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#27880;&#24847;&#21147;&#26435;&#37325;&#25552;&#21319;&#20026;&#20027;&#35201;&#34920;&#31034;&#65292;&#20351;&#29992;&#20840;&#23616;&#26631;&#24535;&#21644;&#23616;&#37096;&#31383;&#21475;&#26500;&#24314;&#30340;&#27880;&#24847;&#21147;&#22270;&#20316;&#20026;&#31283;&#20581;&#26680;&#34920;&#31034;&#26469;&#20811;&#26381;&#22122;&#22768;&#21644;&#20998;&#24067;&#21464;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021; improvement.</title><link>https://arxiv.org/abs/2402.05370</link><description>&lt;p&gt;
&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31283;&#20581;&#34920;&#31034;&#30340;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention as Robust Representation for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05370
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#27880;&#24847;&#21147;&#26435;&#37325;&#25552;&#21319;&#20026;&#20027;&#35201;&#34920;&#31034;&#65292;&#20351;&#29992;&#20840;&#23616;&#26631;&#24535;&#21644;&#23616;&#37096;&#31383;&#21475;&#26500;&#24314;&#30340;&#27880;&#24847;&#21147;&#22270;&#20316;&#20026;&#31283;&#20581;&#26680;&#34920;&#31034;&#26469;&#20811;&#26381;&#22122;&#22768;&#21644;&#20998;&#24067;&#21464;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021; improvement.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#30001;&#20110;Transformer&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#65292;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#36880;&#28176;&#22686;&#22810;&#12290;Transformers&#30340;&#20851;&#38190;&#29305;&#24615;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21160;&#24577;&#22320;&#34701;&#21512;&#23884;&#20837;&#20197;&#22686;&#24378;&#25968;&#25454;&#34920;&#31034;&#65292;&#36890;&#24120;&#23558;&#27880;&#24847;&#21147;&#26435;&#37325;&#20316;&#20026;&#21103;&#20135;&#21697;&#12290;&#28982;&#32780;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20855;&#26377;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#24615;&#65292;&#32473;&#39044;&#27979;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27880;&#24847;&#21147;&#26435;&#37325;&#25552;&#21319;&#20026;&#26102;&#38388;&#24207;&#21015;&#30340;&#20027;&#35201;&#34920;&#31034;&#65292;&#21033;&#29992;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#26469;&#25913;&#21892;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20840;&#23616;&#26631;&#24535;&#21644;&#23616;&#37096;&#31383;&#21475;&#26500;&#24314;&#30340;&#27880;&#24847;&#21147;&#22270;&#20805;&#24403;&#25968;&#25454;&#28857;&#30340;&#31283;&#20581;&#26680;&#34920;&#31034;&#65292;&#33021;&#22815;&#25269;&#25239;&#22122;&#22768;&#21644;&#20998;&#24067;&#31227;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#23558;&#22343;&#26041;&#35823;&#24046;(MSE)&#26174;&#33879;&#38477;&#20302;&#20102;3.6%&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32780;&#19981;&#25913;&#21464;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting is essential for many practical applications, with the adoption of transformer-based models on the rise due to their impressive performance in NLP and CV. Transformers' key feature, the attention mechanism, dynamically fusing embeddings to enhance data representation, often relegating attention weights to a byproduct role. Yet, time series data, characterized by noise and non-stationarity, poses significant forecasting challenges. Our approach elevates attention weights as the primary representation for time series, capitalizing on the temporal relationships among data points to improve forecasting accuracy. Our study shows that an attention map, structured using global landmarks and local windows, acts as a robust kernel representation for data points, withstanding noise and shifts in distribution. Our method outperforms state-of-the-art models, reducing mean squared error (MSE) in multivariate time series forecasting by a notable 3.6% without altering the core
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05359</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27835;&#31243;&#24207;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#27714;&#35299;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#21487;&#20197;&#37322;&#25918;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#35745;&#31639;&#21644;&#25991;&#31456;&#32423;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#35201;&#20040;&#34920;&#29616;&#20986;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#65292;&#35201;&#20040;&#30001;&#24187;&#35273;&#24341;&#21457;&#20013;&#38388;&#38169;&#35823;&#12290;&#20026;&#20102;&#20351;LLM&#23545;&#36825;&#20123;&#20013;&#38388;&#38169;&#35823;&#26356;&#20855;&#36776;&#21035;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;LLM&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#20248;&#36234;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#20219;&#21153;&#20998;&#35299;&#12289;&#23376;&#20219;&#21153;&#35299;&#20915;&#21644;&#35299;&#20915;&#32452;&#35013;&#36807;&#31243;&#30340;&#20998;&#31163;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#24341;&#23548;LLM&#25193;&#23637;&#22266;&#23450;&#28145;&#24230;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#24403;&#21069;&#21457;&#23637;&#29366;&#24577;&#19979;&#30340;&#20851;&#38190;&#38480;&#21046;&#36827;&#34892;&#20102;&#23457;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.05355</link><description>&lt;p&gt;
&#23433;&#20840;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey on Safe Multi-Modal Learning System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05355
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#24403;&#21069;&#21457;&#23637;&#29366;&#24577;&#19979;&#30340;&#20851;&#38190;&#38480;&#21046;&#36827;&#34892;&#20102;&#23457;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#23433;&#20840;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#23545;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#23433;&#20840;&#38382;&#39064;&#32570;&#20047;&#31995;&#32479;&#24615;&#30740;&#31350;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#30340;&#20998;&#31867;&#27861;&#65292;&#30830;&#23450;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#22235;&#20010;&#20851;&#38190;&#25903;&#26609;&#12290;&#20511;&#21161;&#36825;&#19968;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#25903;&#26609;&#36827;&#34892;&#20102;&#28145;&#20837;&#23457;&#26597;&#65292;&#31361;&#20986;&#20102;&#24403;&#21069;&#21457;&#23637;&#29366;&#24577;&#30340;&#20851;&#38190;&#38480;&#21046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the wide deployment of multimodal learning systems (MMLS) in real-world scenarios, safety concerns have become increasingly prominent. The absence of systematic research into their safety is a significant barrier to progress in this field. To bridge the gap, we present the first taxonomy for MMLS safety, identifying four essential pillars of these concerns. Leveraging this taxonomy, we conduct in-depth reviews for each pillar, highlighting key limitations based on the current state of development. Finally, we pinpoint unique challenges in MMLS safety and provide potential directions for future research.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32570;&#20047;&#36890;&#29992;&#34892;&#20026;&#65292;&#38656;&#35201;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;KIX&#65292;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#30693;&#35782;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05346</link><description>&lt;p&gt;
KIX: &#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KIX: A Metacognitive Generalization Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05346
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32570;&#20047;&#36890;&#29992;&#34892;&#20026;&#65292;&#38656;&#35201;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;KIX&#65292;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#30693;&#35782;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#20854;&#20182;&#21160;&#29289;&#33021;&#22815;&#28789;&#27963;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#21644;&#24212;&#29992;&#38271;&#26399;&#31215;&#32047;&#30340;&#39640;&#32423;&#30693;&#35782;&#26469;&#36866;&#24212;&#26032;&#39062;&#24773;&#22659;&#65292;&#36825;&#34920;&#29616;&#20102;&#19968;&#31181;&#27867;&#21270;&#26234;&#33021;&#34892;&#20026;&#12290;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26356;&#22810;&#22320;&#26159;&#19987;&#23478;&#65292;&#32570;&#20047;&#36825;&#31181;&#36890;&#29992;&#34892;&#20026;&#12290;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#38656;&#35201;&#29702;&#35299;&#21644;&#21033;&#29992;&#20851;&#38190;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;Knowledge-Interaction-eXecution (KIX)&#65292;&#24182;&#19988;&#35748;&#20026;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#26469;&#21033;&#29992;&#31867;&#22411;&#31354;&#38388;&#21487;&#20197;&#20419;&#36827;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#26159;&#23558;&#30693;&#35782;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#24182;&#26377;&#26395;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#23454;&#29616;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#30340;&#25512;&#24191;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans and other animals aptly exhibit general intelligence behaviors in solving a variety of tasks with flexibility and ability to adapt to novel situations by reusing and applying high level knowledge acquired over time. But artificial agents are more of a specialist, lacking such generalist behaviors. Artificial agents will require understanding and exploiting critical structured knowledge representations. We present a metacognitive generalization framework, Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects leveraging type space facilitate the learning of transferable interaction concepts and generalization. It is a natural way of integrating knowledge into reinforcement learning and promising to act as an enabler for autonomous and generalist behaviors in artificial intelligence systems.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#23545;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#30340;&#24050;&#26377;&#24037;&#20316;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#65292;&#38416;&#26126;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#26041;&#24335;&#21644;&#20027;&#27969;&#25216;&#26415;&#29305;&#28857;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#37325;&#35201;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.05322</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#22270;&#19978;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learning on Multimodal Graphs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05322
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#23545;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#30340;&#24050;&#26377;&#24037;&#20316;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#65292;&#38416;&#26126;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#26041;&#24335;&#21644;&#20027;&#27969;&#25216;&#26415;&#29305;&#28857;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#37325;&#35201;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#24191;&#27867;&#23384;&#22312;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#20132;&#36890;&#36816;&#36755;&#31561;&#65292;&#20854;&#20013;&#22810;&#27169;&#24577;&#22270;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#27169;&#24577;&#22270;&#19978;&#30340;&#24212;&#29992;&#65292;&#34987;&#31216;&#20026;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064; (MGL)&#65292;&#23545;&#20110;&#25104;&#21151;&#30340;&#20154;&#24037;&#26234;&#33021; (AI) &#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#23545;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#20013;&#24050;&#26377;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#65292;&#38416;&#26126;&#20102;&#22312;&#19981;&#21516;&#22270;&#31867;&#22411;&#19978;&#23454;&#29616;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#24182;&#25506;&#32034;&#20102;&#20027;&#27969;&#23398;&#20064;&#25216;&#26415;&#30340;&#29305;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#30340;&#37325;&#35201;&#24212;&#29992;&#65292;&#24182;&#23545;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20026;&#24076;&#26395;&#20102;&#35299;&#29616;&#26377; MGL &#25216;&#26415;&#21450;&#20854;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#36866;&#29992;&#24615;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22522;&#30784;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal data pervades various domains, including healthcare, social media, and transportation, where multimodal graphs play a pivotal role. Machine learning on multimodal graphs, referred to as multimodal graph learning (MGL), is essential for successful artificial intelligence (AI) applications. The burgeoning research in this field encompasses diverse graph data types and modalities, learning techniques, and application scenarios. This survey paper conducts a comparative analysis of existing works in multimodal graph learning, elucidating how multimodal learning is achieved across different graph types and exploring the characteristics of prevalent learning techniques. Additionally, we delineate significant applications of multimodal graph learning and offer insights into future directions in this domain. Consequently, this paper serves as a foundational resource for researchers seeking to comprehend existing MGL techniques and their applicability across diverse scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23454;&#29616;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#30340;&#31070;&#32463;&#31526;&#21495;&#24378;&#21270;&#23398;&#20064;&#30340;&#19977;&#20010;&#36335;&#24452;&#65292;&#24182;&#25581;&#31034;&#20102;&#23398;&#20064;&#30340;&#36830;&#32493;&#24615;&#21644;&#21487;&#24494;&#24615;&#30340;&#30410;&#22788;&#65292;&#20197;&#21450;&#23558;&#36923;&#36753;&#19982;&#25968;&#20540;&#20223;&#30495;&#32467;&#21512;&#30340;&#38590;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.05307</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#21644;&#31574;&#30053;&#32593;&#32476;&#30340;&#31070;&#32463;&#31526;&#21495;&#24378;&#21270;&#23398;&#20064;&#30340;&#19977;&#20010;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Three Pathways to Neurosymbolic Reinforcement Learning with Interpretable Model and Policy Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23454;&#29616;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#30340;&#31070;&#32463;&#31526;&#21495;&#24378;&#21270;&#23398;&#20064;&#30340;&#19977;&#20010;&#36335;&#24452;&#65292;&#24182;&#25581;&#31034;&#20102;&#23398;&#20064;&#30340;&#36830;&#32493;&#24615;&#21644;&#21487;&#24494;&#24615;&#30340;&#30410;&#22788;&#65292;&#20197;&#21450;&#23558;&#36923;&#36753;&#19982;&#25968;&#20540;&#20223;&#30495;&#32467;&#21512;&#30340;&#38590;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#23558;&#32463;&#20856;&#31526;&#21495;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#12289;&#31616;&#32422;&#24615;&#21644;&#26174;&#24335;&#25512;&#29702;&#19982;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#26041;&#27861;&#30340;&#32479;&#35745;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#21516;&#26102;&#21487;&#24494;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#21487;&#33021;&#26159;&#36825;&#31181;&#32467;&#21512;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#21644;&#31574;&#30053;&#30340;&#19977;&#31181;&#36335;&#24452;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#20854;&#26550;&#26500;&#20013;&#30452;&#25509;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#12290;&#25105;&#20204;&#25581;&#31034;&#21644;&#24378;&#35843;&#20102;&#23558;&#36923;&#36753;&#12289;&#20223;&#30495;&#21644;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#30340;&#28508;&#21147;&#21644;&#22522;&#26412;&#22256;&#38590;&#12290;&#19968;&#20010;&#25945;&#35757;&#26159;&#23398;&#20064;&#21463;&#30410;&#20110;&#36830;&#32493;&#24615;&#21644;&#21487;&#24494;&#24615;&#65292;&#20294;&#32463;&#20856;&#36923;&#36753;&#26159;&#31163;&#25955;&#19988;&#19981;&#21487;&#24494;&#30340;&#12290;&#23558;&#36923;&#36753;&#26494;&#24347;&#20026;&#23454;&#20540;&#30340;&#21487;&#24494;&#34920;&#31034;&#23384;&#22312;&#19968;&#20010;&#26435;&#34913;&#65307;&#36234;&#21487;&#23398;&#20064;&#65292;&#36234;&#19981;&#21487;&#35299;&#37322;&#12290;&#21478;&#19968;&#20010;&#25945;&#35757;&#26159;&#22312;&#25968;&#20540;&#20223;&#30495;&#29615;&#22659;&#20013;&#20351;&#29992;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI combines the interpretability, parsimony, and explicit reasoning of classical symbolic approaches with the statistical learning of data-driven neural approaches. Models and policies that are simultaneously differentiable and interpretable may be key enablers of this marriage. This paper demonstrates three pathways to implementing such models and policies in a real-world reinforcement learning setting. Specifically, we study a broad class of neural networks that build interpretable semantics directly into their architecture. We reveal and highlight both the potential and the essential difficulties of combining logic, simulation, and learning. One lesson is that learning benefits from continuity and differentiability, but classical logic is discrete and non-differentiable. The relaxation to real-valued, differentiable representations presents a trade-off; the more learnable, the less interpretable. Another lesson is that using logic in the context of a numerical simulati
&lt;/p&gt;</description></item><item><title>Sym-Q&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31526;&#21495;&#22238;&#24402;&#37325;&#26032;&#23450;&#20041;&#20026;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#26469;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#27867;&#21270;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#30417;&#30563;&#28436;&#31034;&#21644;&#22870;&#21169;&#20449;&#21495;&#65292;Sym-Q&#33021;&#22815;&#26681;&#25454;&#25311;&#21512;&#31934;&#24230;&#30340;&#36136;&#37327;&#25913;&#36827;&#34920;&#36798;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.05306</link><description>&lt;p&gt;
Sym-Q&#65306;&#36890;&#36807;&#39034;&#24207;&#20915;&#31574;&#36827;&#34892;&#33258;&#36866;&#24212;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05306
&lt;/p&gt;
&lt;p&gt;
Sym-Q&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31526;&#21495;&#22238;&#24402;&#37325;&#26032;&#23450;&#20041;&#20026;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#26469;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#27867;&#21270;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#30417;&#30563;&#28436;&#31034;&#21644;&#22870;&#21169;&#20449;&#21495;&#65292;Sym-Q&#33021;&#22815;&#26681;&#25454;&#25311;&#21512;&#31934;&#24230;&#30340;&#36136;&#37327;&#25913;&#36827;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#20855;&#26377;&#20174;&#23454;&#35777;&#25968;&#25454;&#20013;&#25581;&#31034;&#28508;&#22312;&#25968;&#23398;&#21644;&#29289;&#29702;&#20851;&#31995;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#27867;&#21270;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#36890;&#24120;&#65292;&#24403;&#36755;&#20986;&#34920;&#36798;&#24335;&#19981;&#36275;&#20197;&#36866;&#24212;&#23454;&#39564;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#26377;&#25928;&#30340;&#26426;&#21046;&#26469;&#36866;&#24212;&#25110;&#20462;&#25913;&#34920;&#36798;&#24335;&#12290;&#36825;&#31181;&#32570;&#20047;&#28789;&#27963;&#24615;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#21457;&#29616;&#26410;&#30693;&#30340;&#29289;&#29702;&#25110;&#29983;&#29289;&#20851;&#31995;&#26041;&#38754;&#12290;&#21463;&#21040;&#20154;&#31867;&#19987;&#23478;&#22914;&#20309;&#25913;&#36827;&#21644;&#35843;&#25972;&#34920;&#36798;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;Symbolic Q-network&#65288;Sym-Q&#65289;&#65292;&#23558;&#31526;&#21495;&#22238;&#24402;&#37325;&#26032;&#23450;&#20041;&#20026;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#12290;Sym-Q&#21033;&#29992;&#30417;&#30563;&#28436;&#31034;&#24182;&#26681;&#25454;&#22870;&#21169;&#20449;&#21495;&#26469;&#25913;&#36827;&#34920;&#36798;&#24335;&#65292;&#22870;&#21169;&#20449;&#21495;&#25351;&#31034;&#25311;&#21512;&#31934;&#24230;&#30340;&#36136;&#37327;&#12290;&#23427;&#29420;&#29305;&#30340;&#33021;&#21147;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression holds great potential for uncovering underlying mathematical and physical relationships from empirical data. While existing transformer-based models have recently achieved significant success in this domain, they face challenges in terms of generalizability and adaptability. Typically, in cases where the output expressions do not adequately fit experimental data, the models lack efficient mechanisms to adapt or modify the expression. This inflexibility hinders their application in real-world scenarios, particularly in discovering unknown physical or biological relationships. Inspired by how human experts refine and adapt expressions, we introduce Symbolic Q-network (Sym-Q), a novel reinforcement learning-based model that redefines symbolic regression as a sequential decision-making task. Sym-Q leverages supervised demonstrations and refines expressions based on reward signals indicating the quality of fitting precision. Its distinctive ability to manage the complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BIKED++&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;140&#19975;&#20010;&#33258;&#34892;&#36710;&#35774;&#35745;&#30340;&#22270;&#20687;&#21644;&#21442;&#25968;&#21270;CAD&#25991;&#20214;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#36328;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#65292;&#20363;&#22914;&#20351;&#29992;&#21442;&#25968;&#21270;&#34920;&#31034;&#26469;&#20934;&#30830;&#20272;&#35745;&#22270;&#20687;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#35813;&#25968;&#25454;&#38598;&#20063;&#24050;&#20844;&#24320;&#65292;&#21487;&#20379;&#30740;&#31350;&#32773;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05301</link><description>&lt;p&gt;
BIKED++&#65306;&#19968;&#20010;&#21253;&#21547;140&#19975;&#20010;&#33258;&#34892;&#36710;&#22270;&#20687;&#21644;&#21442;&#25968;&#21270;CAD&#35774;&#35745;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BIKED++&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;140&#19975;&#20010;&#33258;&#34892;&#36710;&#35774;&#35745;&#30340;&#22270;&#20687;&#21644;&#21442;&#25968;&#21270;CAD&#25991;&#20214;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#36328;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#65292;&#20363;&#22914;&#20351;&#29992;&#21442;&#25968;&#21270;&#34920;&#31034;&#26469;&#20934;&#30830;&#20272;&#35745;&#22270;&#20687;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#35813;&#25968;&#25454;&#38598;&#20063;&#24050;&#20844;&#24320;&#65292;&#21487;&#20379;&#30740;&#31350;&#32773;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;140&#19975;&#20010;&#36890;&#36807;&#21442;&#25968;&#21270;&#34920;&#31034;&#21644;JSON&#25991;&#20214;&#20197;&#21450;&#26629;&#26684;&#21270;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#34892;&#36710;&#35774;&#35745;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#28210;&#26579;&#24341;&#25806;&#21644;BikeCAD&#36719;&#20214;&#29983;&#25104;&#21442;&#25968;&#21270;&#35774;&#35745;&#30340;&#30690;&#37327;&#22270;&#24418;&#32780;&#21019;&#24314;&#30340;&#12290;&#26412;&#25991;&#36824;&#20844;&#24320;&#20102;&#35813;&#28210;&#26579;&#24341;&#25806;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#22810;&#31181;&#24212;&#29992;&#65292;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#35757;&#32451;&#21442;&#25968;&#21270;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35774;&#35745;&#34920;&#31034;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#30452;&#25509;&#20174;&#21442;&#25968;&#21270;&#34920;&#31034;&#20934;&#30830;&#20272;&#35745;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#23884;&#20837;&#12290;&#36825;&#26679;&#21487;&#20197;&#24314;&#31435;&#21442;&#25968;&#21270;&#33258;&#34892;&#36710;&#35774;&#35745;&#19982;&#25991;&#26412;&#23383;&#31526;&#20018;&#25110;&#21442;&#32771;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#20851;&#31995;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#20063;&#24050;&#20844;&#24320;&#12290;&#35813;&#25968;&#25454;&#38598;&#21152;&#20837;&#20102;BIKED&#25968;&#25454;&#38598;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a public dataset of 1.4 million procedurally-generated bicycle designs represented parametrically, as JSON files, and as rasterized images. The dataset is created through the use of a rendering engine which harnesses the BikeCAD software to generate vector graphics from parametric designs. This rendering engine is discussed in the paper and also released publicly alongside the dataset. Though this dataset has numerous applications, a principal motivation is the need to train cross-modal predictive models between parametric and image-based design representations. For example, we demonstrate that a predictive model can be trained to accurately estimate Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric representation directly. This allows similarity relations to be established between parametric bicycle designs and text strings or reference images. Trained predictive models are also made public. The dataset joins the BIKED dataset family whic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#21644;&#22522;&#20110;&#20027;&#39064;&#30340;&#26041;&#27861;&#23545;&#22403;&#22334;&#37038;&#20214;&#36827;&#34892;&#20998;&#31867;&#30340;&#26032;&#24605;&#36335;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SPEMC-15K-E&#21644;SPEMC-15K-S&#65292;&#24182;&#20351;&#29992;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#23558;&#20854;&#21010;&#20998;&#20026;11&#20010;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TF-IDF&#21644;LR&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#20013;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05296</link><description>&lt;p&gt;
&#20351;&#29992;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#21644;&#22522;&#20110;&#20027;&#39064;&#30340;&#26041;&#27861;&#23545;&#22403;&#22334;&#37038;&#20214;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying spam emails using agglomerative hierarchical clustering and a topic-based approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05296
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#21644;&#22522;&#20110;&#20027;&#39064;&#30340;&#26041;&#27861;&#23545;&#22403;&#22334;&#37038;&#20214;&#36827;&#34892;&#20998;&#31867;&#30340;&#26032;&#24605;&#36335;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SPEMC-15K-E&#21644;SPEMC-15K-S&#65292;&#24182;&#20351;&#29992;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#23558;&#20854;&#21010;&#20998;&#20026;11&#20010;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TF-IDF&#21644;LR&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#20013;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22403;&#22334;&#37038;&#20214;&#26159;&#26410;&#32463;&#35831;&#27714;&#30340;&#24700;&#20154;&#19988;&#26377;&#26102;&#26377;&#23475;&#30340;&#28040;&#24687;&#65292;&#21487;&#33021;&#21547;&#26377;&#24694;&#24847;&#36719;&#20214;&#12289;&#38035;&#40060;&#25110;&#24694;&#20316;&#21095;&#12290;&#19982;&#22823;&#22810;&#25968;&#30740;&#31350;&#35299;&#20915;&#39640;&#25928;&#21453;&#22403;&#22334;&#37038;&#20214;&#36807;&#28388;&#22120;&#35774;&#35745;&#30340;&#38382;&#39064;&#19981;&#21516;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#21644;&#26032;&#39062;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#22403;&#22334;&#37038;&#20214;&#38382;&#39064;&#12290;&#37325;&#28857;&#20851;&#27880;&#32593;&#32476;&#23433;&#20840;&#21333;&#20301;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20027;&#39064;&#30340;&#26041;&#27861;&#26469;&#23545;&#22403;&#22334;&#37038;&#20214;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;SPEMC-15K-E&#21644;SPEMC-15K-S&#65292;&#20998;&#21035;&#21253;&#21547;&#22823;&#32422;15K&#23553;&#33521;&#25991;&#21644;&#35199;&#29677;&#29273;&#25991;&#37038;&#20214;&#65292;&#24182;&#20351;&#29992;&#20957;&#32858;&#23618;&#27425;&#32858;&#31867;&#23558;&#20854;&#21010;&#20998;&#20026;11&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;16&#31181;&#27969;&#27700;&#32447;&#65292;&#32467;&#21512;&#20102;&#22235;&#31181;&#25991;&#26412;&#34920;&#31034;&#25216;&#26415;-&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#12289;&#35789;&#34955;&#27169;&#22411;&#12289;Word2Vec&#21644;BERT-&#20197;&#21450;&#22235;&#31181;&#20998;&#31867;&#22120;&#65306;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#36923;&#36753;&#22238;&#24402;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#20013;&#65292;TF-IDF&#21644;LR&#30340;&#24615;&#33021;&#26368;&#22909;&#65292;
&lt;/p&gt;
&lt;p&gt;
Spam emails are unsolicited, annoying and sometimes harmful messages which may contain malware, phishing or hoaxes. Unlike most studies that address the design of efficient anti-spam filters, we approach the spam email problem from a different and novel perspective. Focusing on the needs of cybersecurity units, we follow a topic-based approach for addressing the classification of spam email into multiple categories. We propose SPEMC-15K-E and SPEMC-15K-S, two novel datasets with approximately 15K emails each in English and Spanish, respectively, and we label them using agglomerative hierarchical clustering into 11 classes. We evaluate 16 pipelines, combining four text representation techniques -Term Frequency-Inverse Document Frequency (TF-IDF), Bag of Words, Word2Vec and BERT- and four classifiers: Support Vector Machine, N\"aive Bayes, Random Forest and Logistic Regression. Experimental results show that the highest performance is achieved with TF-IDF and LR for the English dataset, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#25490;&#24207;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35780;&#20272;&#19981;&#21516;&#31639;&#27861;&#32467;&#26524;&#20013;&#30340;&#29305;&#24449;&#25490;&#24207;&#30340;&#31283;&#23450;&#24615;&#65292;&#21253;&#25324;&#23436;&#25972;&#30340;&#25490;&#21517;&#21015;&#34920;&#12289;&#29305;&#24449;&#23376;&#38598;&#21644;&#37096;&#20998;&#25490;&#21517;&#21015;&#34920;&#12290;</title><link>https://arxiv.org/abs/2402.05295</link><description>&lt;p&gt;
&#19968;&#31181;&#37327;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#25490;&#24207;&#31639;&#27861;&#31283;&#23450;&#24615;&#30340;&#20449;&#24687;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An information theoretic approach to quantify the stability of feature selection and ranking algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#25490;&#24207;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35780;&#20272;&#19981;&#21516;&#31639;&#27861;&#32467;&#26524;&#20013;&#30340;&#29305;&#24449;&#25490;&#24207;&#30340;&#31283;&#23450;&#24615;&#65292;&#21253;&#25324;&#23436;&#25972;&#30340;&#25490;&#21517;&#21015;&#34920;&#12289;&#29305;&#24449;&#23376;&#38598;&#21644;&#37096;&#20998;&#25490;&#21517;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26159;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#26102;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#29305;&#21035;&#26159;&#36825;&#20123;&#25216;&#26415;&#36890;&#36807;&#20174;&#22024;&#26434;&#12289;&#20887;&#20313;&#21644;&#26080;&#20851;&#30340;&#29305;&#24449;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#31616;&#21270;&#20102;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#30693;&#35782;&#30340;&#36807;&#31243;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#20986;&#29616;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#30340;&#32467;&#26524;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#29305;&#24449;&#25490;&#24207;&#12290;&#22312;&#19978;&#36848;&#24773;&#20917;&#20013;&#65292;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Jensen Shannon&#36317;&#31163;&#30340;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#40065;&#26834;&#24615;&#12290;&#19982;&#20854;&#20182;&#31283;&#23450;&#24615;&#24230;&#37327;&#19981;&#21516;&#65292;&#36825;&#20010;&#24230;&#37327;&#25351;&#26631;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#31639;&#27861;&#32467;&#26524;&#65306;&#23436;&#25972;&#30340;&#25490;&#21517;&#21015;&#34920;&#12289;&#29305;&#24449;&#23376;&#38598;&#20197;&#21450;&#36739;&#23569;&#30740;&#31350;&#30340;&#37096;&#20998;&#25490;&#21517;&#21015;&#34920;&#12290;&#36825;&#20010;&#24191;&#20041;&#24230;&#37327;&#20197;&#27010;&#29575;&#26041;&#27861;&#37327;&#21270;&#20102;&#30456;&#21516;&#22823;&#23567;&#30340;&#25972;&#20010;&#21015;&#34920;&#38598;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#21453;&#26144;&#20854;&#20013;&#30340;&#25490;&#24207;&#31283;&#23450;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature selection is a key step when dealing with high dimensional data. In particular, these techniques simplify the process of knowledge discovery from the data by selecting the most relevant features out of the noisy, redundant and irrelevant features. A problem that arises in many of these practical applications is that the outcome of the feature selection algorithm is not stable. Thus, small variations in the data may yield very different feature rankings. Assessing the stability of these methods becomes an important issue in the previously mentioned situations. We propose an information theoretic approach based on the Jensen Shannon divergence to quantify this robustness. Unlike other stability measures, this metric is suitable for different algorithm outcomes: full ranked lists, feature subsets as well as the lesser studied partial ranked lists. This generalized metric quantifies the difference among a whole set of lists with the same size, following a probabilistic approach and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20998;&#26512;&#20102;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#19982;&#21442;&#19982;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#32771;&#34385;&#19981;&#19968;&#33268;&#24615;&#30340;&#20449;&#24687;&#34701;&#21512;&#26426;&#21046;&#21644;&#27169;&#24577;&#25554;&#20540;&#32593;&#32476;&#65292;&#22312;&#35299;&#20915;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05294</link><description>&lt;p&gt;
&#26816;&#39564;&#21307;&#30103;&#35270;&#35273;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#30142;&#30149;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Examining Modality Incongruity in Multimodal Federated Learning for Medical Vision and Language-based Disease Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20998;&#26512;&#20102;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#19982;&#21442;&#19982;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#32771;&#34385;&#19981;&#19968;&#33268;&#24615;&#30340;&#20449;&#24687;&#34701;&#21512;&#26426;&#21046;&#21644;&#27169;&#24577;&#25554;&#20540;&#32593;&#32476;&#65292;&#22312;&#35299;&#20915;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65288;MMFL&#65289;&#21033;&#29992;&#27599;&#20010;&#23458;&#25143;&#31471;&#20013;&#30340;&#22810;&#20010;&#27169;&#24577;&#26500;&#24314;&#27604;&#20854;&#21333;&#27169;&#24577;&#23545;&#24212;&#29289;&#26356;&#24378;&#22823;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#23458;&#25143;&#31471;&#32570;&#22833;&#27169;&#24577;&#30340;&#24433;&#21709;&#65292;&#20063;&#31216;&#20026;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#65292;&#19968;&#30452;&#34987;&#22823;&#22823;&#24573;&#35270;&#12290;&#26412;&#25991;&#39318;&#27425;&#20998;&#26512;&#20102;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#19982;&#21442;&#19982;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#29305;&#21035;&#26816;&#26597;&#20102;&#19981;&#19968;&#33268;&#30340;MMFL&#19982;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#23458;&#25143;&#31471;&#30456;&#27604;&#26159;&#21542;&#26356;&#26377;&#30410;&#20110;&#21333;&#27169;&#24577;FL&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19977;&#20010;&#28508;&#22312;&#36884;&#24452;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#33258;&#27880;&#24847;&#26426;&#21046;&#23545;&#20110;&#19981;&#32771;&#34385;&#19981;&#19968;&#33268;&#24615;&#30340;&#20449;&#24687;&#34701;&#21512;&#22312;MMFL&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#23458;&#25143;&#31471;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#24577;&#25554;&#20540;&#32593;&#32476;&#65288;MIN&#65289;&#26469;&#35299;&#20915;&#21333;&#27169;&#24577;&#23458;&#25143;&#31471;&#20013;&#30340;&#27169;&#24577;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20854;&#20943;&#36731;&#32570;&#22833;&#27169;&#24577;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Multimodal Federated Learning (MMFL) utilizes multiple modalities in each client to build a more powerful Federated Learning (FL) model than its unimodal counterpart. However, the impact of missing modality in different clients, also called modality incongruity, has been greatly overlooked. This paper, for the first time, analyses the impact of modality incongruity and reveals its connection with data heterogeneity across participating clients. We particularly inspect whether incongruent MMFL with unimodal and multimodal clients is more beneficial than unimodal FL. Furthermore, we examine three potential routes of addressing this issue. Firstly, we study the effectiveness of various self-attention mechanisms towards incongruity-agnostic information fusion in MMFL. Secondly, we introduce a modality imputation network (MIN) pre-trained in a multimodal client for modality translation in unimodal clients and investigate its potential towards mitigating the missing modality problem. Thirdly
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#32467;&#30452;&#32928;&#30284;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#35270;&#35273;&#26041;&#27861;&#35780;&#20272;&#29305;&#24449;&#25490;&#24207;&#25216;&#26415;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05293</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#32467;&#30452;&#32928;&#30284;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#29305;&#24449;&#36873;&#25321;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A comparative study on feature selection for a risk prediction model for colorectal cancer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05293
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#32467;&#30452;&#32928;&#30284;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#35270;&#35273;&#26041;&#27861;&#35780;&#20272;&#29305;&#24449;&#25490;&#24207;&#25216;&#26415;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#21644;&#30446;&#26631;  &#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#26088;&#22312;&#35782;&#21035;&#24739;&#19978;&#26576;&#31181;&#30142;&#30149;&#39118;&#38505;&#26356;&#39640;&#30340;&#20154;&#32676;&#12290;&#29305;&#24449;&#36873;&#25321;&#23545;&#20110;&#25913;&#21892;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12289;&#36991;&#20813;&#36807;&#25311;&#21512;&#20197;&#21450;&#35782;&#21035;&#23548;&#33268;&#30284;&#30151;&#39118;&#38505;&#65288;&#21644;&#20445;&#25252;&#65289;&#30340;&#22240;&#32032;&#23588;&#20026;&#37325;&#35201;&#12290;&#23545;&#29305;&#24449;&#36873;&#25321;/&#25490;&#24207;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#35780;&#20272;&#25104;&#20026;&#20998;&#26512;&#20855;&#26377;&#26356;&#22810;&#39044;&#27979;&#33021;&#21147;&#29305;&#24449;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#26041;&#27861; &#26412;&#30740;&#31350;&#38598;&#20013;&#22312;&#32467;&#30452;&#32928;&#30284;&#19978;&#65292;&#35780;&#20272;&#20102;&#20960;&#31181;&#29305;&#24449;&#25490;&#24207;&#31639;&#27861;&#22312;&#19968;&#32452;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65288;&#31070;&#32463;&#32593;&#32476;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#36923;&#36753;&#22238;&#24402;&#12289;k-&#26368;&#36817;&#37051;&#21644;&#25552;&#21319;&#26641;&#65289;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#26631;&#37327;&#31283;&#23450;&#24230;&#25351;&#26631;&#21644;&#26412;&#25991;&#25552;&#20986;&#30340;&#35270;&#35273;&#26041;&#27861;&#35780;&#20272;&#20102;&#20854;&#31283;&#23450;&#24615;&#65292;&#26082;&#21487;&#20197;&#30740;&#31350;&#29305;&#24449;&#25490;&#24207;&#25216;&#26415;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20063;&#21487;&#20197;&#30740;&#31350;&#23427;&#20204;&#30340;&#21333;&#29420;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background and objective   Risk prediction models aim at identifying people at higher risk of developing a target disease. Feature selection is particularly important to improve the prediction model performance avoiding overfitting and to identify the leading cancer risk (and protective) factors. Assessing the stability of feature selection/ranking algorithms becomes an important issue when the aim is to analyze the features with more prediction power. Methods   This work is focused on colorectal cancer, assessing several feature ranking algorithms in terms of performance for a set of risk prediction models (Neural Networks, Support Vector Machines (SVM), Logistic Regression, k-Nearest Neighbors and Boosted Trees). Additionally, their robustness is evaluated following a conventional approach with scalar stability metrics and a visual approach proposed in this work to study both similarity among feature ranking techniques as well as their individual stability. A comparative analysis is 
&lt;/p&gt;</description></item><item><title>&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#22870;&#21169;&#24182;&#36827;&#34892;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#36890;&#24120;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#24120;&#29992;&#30340;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#20250;&#20135;&#29983;&#36802;&#22238;&#30340;&#26799;&#24230;&#36335;&#24452;&#65292;&#23545;&#20110;&#38271;&#36317;&#31163;&#30340;&#31574;&#30053;&#26799;&#24230;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Actions World Models (AWMs)&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#30452;&#25509;&#30340;&#26799;&#24230;&#20256;&#25773;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.05290</link><description>&lt;p&gt;
&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#32473;&#20986;&#26356;&#22909;&#30340;&#31574;&#30053;&#26799;&#24230;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Transformer World Models Give Better Policy Gradients?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05290
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#22870;&#21169;&#24182;&#36827;&#34892;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#36890;&#24120;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#24120;&#29992;&#30340;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#20250;&#20135;&#29983;&#36802;&#22238;&#30340;&#26799;&#24230;&#36335;&#24452;&#65292;&#23545;&#20110;&#38271;&#36317;&#31163;&#30340;&#31574;&#30053;&#26799;&#24230;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Actions World Models (AWMs)&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#30452;&#25509;&#30340;&#26799;&#24230;&#20256;&#25773;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#26469;&#35828;&#65292;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#23637;&#24320;&#31070;&#32463;&#32593;&#32476;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#22270;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#20197;&#23398;&#20064;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20856;&#22411;&#30340;&#19990;&#30028;&#27169;&#22411;&#20135;&#29983;&#20102;&#38590;&#20197;&#20248;&#21270;&#30340;&#25439;&#22833;&#22320;&#24418;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#19978;&#36890;&#24120;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#21464;&#24418;&#22120;&#24050;&#30693;&#21487;&#20197;&#39640;&#25928;&#22320;&#20256;&#25773;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#26799;&#24230;&#65306;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21602;&#65311;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24120;&#29992;&#30340;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#20250;&#20135;&#29983;&#36802;&#22238;&#30340;&#26799;&#24230;&#36335;&#24452;&#65292;&#36825;&#23545;&#20110;&#38271;&#36317;&#31163;&#30340;&#31574;&#30053;&#26799;&#24230;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#31216;&#20026;Actions World Models (AWMs)&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#26356;&#30452;&#25509;&#30340;&#26799;&#24230;&#20256;&#25773;&#36335;&#24452;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;AWMs&#38598;&#25104;&#21040;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#30340;&#26694;&#26550;&#20013;&#65292;&#24378;&#35843;&#20102;&#32593;&#32476;&#26550;&#26500;&#19982;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;AWMs&#21487;&#20197;&#20135;&#29983;&#21487;&#20248;&#21270;&#30340;&#26799;&#24230;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
A natural approach for reinforcement learning is to predict future rewards by unrolling a neural network world model, and to backpropagate through the resulting computational graph to learn a policy. However, this method often becomes impractical for long horizons since typical world models induce hard-to-optimize loss landscapes. Transformers are known to efficiently propagate gradients overlong horizons: could they be the solution to this problem? Surprisingly, we show that commonly-used transformer world models produce circuitous gradient paths, which can be detrimental to long-range policy gradients. To tackle this challenge, we propose a class of world models called Actions World Models (AWMs), designed to provide more direct routes for gradient propagation. We integrate such AWMs into a policy gradient framework that underscores the relationship between network architectures and the policy gradient updates they inherently represent. We demonstrate that AWMs can generate optimizat
&lt;/p&gt;</description></item><item><title>&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05271</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#24341;&#21457;&#20102;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#26435;&#37325;&#19982;&#32463;&#39564;NTK&#20043;&#38388;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05271
&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#65292;&#22312;&#19968;&#33324;&#32467;&#26500;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#20010;&#35828;&#27861;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#37327;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22914;&#20309;&#30456;&#20851;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#24341;&#20837;&#30340;NFA&#26159;&#30001;&#38548;&#31163;&#36825;&#31181;&#23545;&#40784;&#30340;&#20013;&#24515;&#21270;NFA&#39537;&#21160;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in supervised learning. Prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the Neural Feature Ansatz (NFA). However, the reason these quantities become correlated during training is poorly understood. In this work, we explain the emergence of this correlation. We identify that the NFA is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights. We establish that the NFA introduced in prior works is driven by a centered NFA that isolates this alignment. We show that the speed of NFA development can be predicted analytically at early training times in terms of sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#26377;&#24207;&#21152;&#26435;&#24179;&#22343;&#20540;&#20989;&#25968;&#65292;&#22312;LTR&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#38598;&#25104;&#39640;&#25928;&#30340;&#20844;&#24179;&#25490;&#21517;&#27169;&#22411;&#65292;&#23454;&#29616;&#20844;&#24179;&#24615;&#12289;&#29992;&#25143;&#25928;&#29992;&#21644;&#36816;&#34892;&#26102;&#25928;&#29575;&#20043;&#38388;&#30340;&#26377;&#21033;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.05252</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#24494;&#20998;&#20248;&#21270;&#26377;&#24207;&#21152;&#26435;&#24179;&#22343;&#20540;&#23398;&#20064;&#20844;&#24179;&#25490;&#21517;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Fair Ranking Policies via Differentiable Optimization of Ordered Weighted Averages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#26377;&#24207;&#21152;&#26435;&#24179;&#22343;&#20540;&#20989;&#25968;&#65292;&#22312;LTR&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#38598;&#25104;&#39640;&#25928;&#30340;&#20844;&#24179;&#25490;&#21517;&#27169;&#22411;&#65292;&#23454;&#29616;&#20844;&#24179;&#24615;&#12289;&#29992;&#25143;&#25928;&#29992;&#21644;&#36816;&#34892;&#26102;&#25928;&#29575;&#20043;&#38388;&#30340;&#26377;&#21033;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#26159;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20043;&#19968;&#12290;&#23427;&#26159;&#20855;&#26377;&#28145;&#36828;&#31038;&#20250;&#24433;&#21709;&#30340;&#24179;&#21488;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#27714;&#32844;&#25628;&#32034;&#12289;&#21307;&#30103;&#20449;&#24687;&#26816;&#32034;&#21644;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#25512;&#36865;&#12290;&#20256;&#32479;&#30340;LTR&#27169;&#22411;&#24050;&#32463;&#26174;&#31034;&#20986;&#20135;&#29983;&#20559;&#35265;&#32467;&#26524;&#65292;&#24341;&#21457;&#20102;&#22914;&#20309;&#35299;&#20915;&#20165;&#20248;&#20808;&#32771;&#34385;&#29992;&#25143;&#30456;&#20851;&#24615;&#30340;&#25490;&#21517;&#31995;&#32479;&#24341;&#20837;&#30340;&#24046;&#24322;&#30340;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#20844;&#24179;&#23398;&#20064;&#25490;&#24207;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#25110;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#25490;&#21517;&#24179;&#21488;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#26377;&#24207;&#21152;&#26435;&#24179;&#22343;&#65288;OWA&#65289;&#20989;&#25968;&#30340;&#39640;&#25928;&#21487;&#35299;&#30340;&#20844;&#24179;&#25490;&#21517;&#27169;&#22411;&#38598;&#25104;&#21040;LTR&#27169;&#22411;&#30340;&#35757;&#32451;&#24490;&#29615;&#20013;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#24615;&#12289;&#29992;&#25143;&#25928;&#29992;&#21644;&#36816;&#34892;&#26102;&#25928;&#29575;&#20043;&#38388;&#30340;&#26377;&#21033;&#24179;&#34913;&#12290;&#29305;&#21035;&#26159;&#65292;&#26412;&#25991;&#39318;&#27425;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#21453;&#21521;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank (LTR) is one of the most widely used machine learning applications. It is a key component in platforms with profound societal impacts, including job search, healthcare information retrieval, and social media content feeds. Conventional LTR models have been shown to produce biases results, stimulating a discourse on how to address the disparities introduced by ranking systems that solely prioritize user relevance. However, while several models of fair learning to rank have been proposed, they suffer from deficiencies either in accuracy or efficiency, thus limiting their applicability to real-world ranking platforms. This paper shows how efficiently-solvable fair ranking models, based on the optimization of Ordered Weighted Average (OWA) functions, can be integrated into the training loop of an LTR model to achieve favorable balances between fairness, user utility, and runtime efficiency. In particular, this paper is the first to show how to backpropagate through constra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#31070;&#32463;&#21151;&#33021;&#65288;UNFs&#65289;&#65292;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#20219;&#20309;&#26435;&#37325;&#31354;&#38388;&#30340;&#32622;&#25442;&#31561;&#21464;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20248;&#21270;&#23567;&#22411;&#22270;&#20687;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;UNFs&#33021;&#22815;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#25913;&#36827;&#65292;&#20026;&#23398;&#20064;&#20248;&#21270;&#22120;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.05232</link><description>&lt;p&gt;
&#36890;&#29992;&#31070;&#32463;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
Universal Neural Functionals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#31070;&#32463;&#21151;&#33021;&#65288;UNFs&#65289;&#65292;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#20219;&#20309;&#26435;&#37325;&#31354;&#38388;&#30340;&#32622;&#25442;&#31561;&#21464;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20248;&#21270;&#23567;&#22411;&#22270;&#20687;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;UNFs&#33021;&#22815;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#25913;&#36827;&#65292;&#20026;&#23398;&#20064;&#20248;&#21270;&#22120;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26159;&#22788;&#29702;&#26435;&#37325;&#31354;&#38388;&#29305;&#24449;&#65292;&#21363;&#20174;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#26799;&#24230;&#20013;&#36716;&#25442;&#25110;&#25552;&#21462;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#26435;&#37325;&#31354;&#38388;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#31616;&#21333;&#30340;&#21069;&#39304;&#32593;&#32476;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#26159;&#31561;&#21464;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#26222;&#36890;&#26550;&#26500;&#24182;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#26435;&#37325;&#31354;&#38388;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#21487;&#33021;&#20250;&#22240;&#24490;&#29615;&#25110;&#27531;&#24046;&#36830;&#25509;&#32780;&#21464;&#24471;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#33258;&#21160;&#26500;&#24314;&#32622;&#25442;&#31561;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36890;&#29992;&#31070;&#32463;&#21151;&#33021;&#65288;UNFs&#65289;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#26435;&#37325;&#31354;&#38388;&#12290;&#22312;&#20854;&#20182;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;UNFs&#26367;&#20195;&#29616;&#26377;&#30340;&#23398;&#20064;&#20248;&#21270;&#22120;&#35774;&#35745;&#65292;&#24182;&#22312;&#20248;&#21270;&#23567;&#22411;&#22270;&#20687;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#26102;&#21457;&#29616;&#26377;&#24076;&#26395;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#20248;&#21270;&#22120;&#21487;&#20197;&#20174;&#32771;&#34385;&#65288;&#23545;&#31216;&#65289;&#32467;&#26500;&#30340;&#35282;&#24230;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
A challenging problem in many modern machine learning tasks is to process weight-space features, i.e., to transform or extract information from the weights and gradients of a neural network. Recent works have developed promising weight-space models that are equivariant to the permutation symmetries of simple feedforward networks. However, they are not applicable to general architectures, since the permutation symmetries of a weight space can be complicated by recurrence or residual connections. This work proposes an algorithm that automatically constructs permutation equivariant models, which we refer to as universal neural functionals (UNFs), for any weight space. Among other applications, we demonstrate how UNFs can be substituted into existing learned optimizer designs, and find promising improvements over prior methods when optimizing small image classifiers and language models. Our results suggest that learned optimizers can benefit from considering the (symmetry) structure of the
&lt;/p&gt;</description></item><item><title>VerAs&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#35780;&#20272;STEM&#23454;&#39564;&#25253;&#21578;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#32500;&#24230;&#30340;&#20998;&#26512;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#38024;&#23545;&#23398;&#29983;&#25552;&#20379;&#35814;&#32454;&#21453;&#39304;&#65292;&#24110;&#21161;&#20182;&#20204;&#25552;&#39640;&#31185;&#23398;&#20889;&#20316;&#25216;&#24039;&#12290;</title><link>https://arxiv.org/abs/2402.05224</link><description>&lt;p&gt;
VerAs: &#39564;&#35777;&#28982;&#21518;&#35780;&#20272;STEM&#23454;&#39564;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
VerAs: Verify then Assess STEM Lab Reports
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05224
&lt;/p&gt;
&lt;p&gt;
VerAs&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#35780;&#20272;STEM&#23454;&#39564;&#25253;&#21578;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#32500;&#24230;&#30340;&#20998;&#26512;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#38024;&#23545;&#23398;&#29983;&#25552;&#20379;&#35814;&#32454;&#21453;&#39304;&#65292;&#24110;&#21161;&#20182;&#20204;&#25552;&#39640;&#31185;&#23398;&#20889;&#20316;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;STEM&#25945;&#32946;&#23545;&#25209;&#21028;&#24615;&#24605;&#32500;&#33021;&#21147;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#31185;&#23398;&#20889;&#20316;&#22312;&#27880;&#37325;&#25506;&#31350;&#25216;&#33021;&#30340;&#35838;&#31243;&#20013;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;&#19968;&#20221;&#25968;&#25454;&#38598;&#26159;&#22522;&#20110;&#19968;&#22871;&#25506;&#31350;&#22411;&#29289;&#29702;&#35838;&#31243;&#30340;&#20004;&#32452;&#22823;&#23398;&#27700;&#24179;&#30340;&#23454;&#39564;&#25253;&#21578;&#65292;&#20381;&#36182;&#20110;&#21033;&#29992;&#22810;&#20010;&#32500;&#24230;&#30340;&#20998;&#26512;&#35780;&#20272;&#26631;&#20934;&#65292;&#25351;&#23450;&#23398;&#31185;&#30693;&#35782;&#21644;&#20248;&#31168;&#35299;&#37322;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#12290;&#27599;&#20010;&#20998;&#26512;&#32500;&#24230;&#37117;&#20197;6&#20998;&#21046;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#25552;&#20379;&#35814;&#32454;&#21453;&#39304;&#65292;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#31185;&#23398;&#20889;&#20316;&#25216;&#24039;&#12290;&#25163;&#21160;&#35780;&#20272;&#21487;&#33021;&#36739;&#24930;&#65292;&#24182;&#19988;&#22312;&#22823;&#29677;&#20013;&#23545;&#25152;&#26377;&#23398;&#29983;&#36827;&#34892;&#19968;&#33268;&#24615;&#26657;&#20934;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#23613;&#31649;&#22312;STEM&#23398;&#31185;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#30340;&#33258;&#21160;&#35780;&#20272;&#19978;&#24050;&#32463;&#26377;&#24456;&#22810;&#24037;&#20316;&#65292;&#20294;&#22312;&#23454;&#39564;&#25253;&#21578;&#31561;&#38271;&#31687;&#20889;&#20316;&#20013;&#30340;&#24037;&#20316;&#35201;&#23569;&#24471;&#22810;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#29420;&#31435;&#30340;&#39564;&#35777;&#22120;&#21644;&#35780;&#20272;&#27169;&#22359;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#24320;&#25918;&#39046;&#22495;&#38382;&#39064;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With an increasing focus in STEM education on critical thinking skills, science writing plays an ever more important role in curricula that stress inquiry skills. A recently published dataset of two sets of college level lab reports from an inquiry-based physics curriculum relies on analytic assessment rubrics that utilize multiple dimensions, specifying subject matter knowledge and general components of good explanations. Each analytic dimension is assessed on a 6-point scale, to provide detailed feedback to students that can help them improve their science writing skills. Manual assessment can be slow, and difficult to calibrate for consistency across all students in large classes. While much work exists on automated assessment of open-ended questions in STEM subjects, there has been far less work on long-form writing such as lab reports. We present an end-to-end neural architecture that has separate verifier and assessment modules, inspired by approaches to Open Domain Question Answ
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#23545;&#35299;&#39064;&#20219;&#21153;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.05201</link><description>&lt;p&gt;
&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Sampling Temperature on Problem Solving in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05201
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#23545;&#35299;&#39064;&#20219;&#21153;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35843;&#26597;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35299;&#39064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#26631;&#20934;LLM&#22522;&#20934;&#20013;&#38543;&#26426;&#25277;&#21462;&#38382;&#39064;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65288;MCQA&#65289;&#32771;&#35797;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22235;&#31181;&#24120;&#35265;&#30340;LLM&#20197;&#21450;&#20116;&#31181;&#25552;&#31034;&#24341;&#25806;&#25216;&#26415;&#26469;&#35299;&#20915;MCQA&#38382;&#39064;&#65292;&#21516;&#26102;&#23558;&#37319;&#26679;&#28201;&#24230;&#20174;0.0&#22686;&#21152;&#21040;1.0&#12290;&#23613;&#31649;&#26377;&#20851;&#30340;&#25253;&#36947;&#19982;&#20043;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#22312;&#35299;&#39064;&#20219;&#21153;&#20013;&#30340;&#21464;&#21270;&#27809;&#26377;&#32479;&#35745;&#23398;&#19978;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#20284;&#20046;&#19981;&#21463;LLM&#12289;&#25552;&#31034;&#24341;&#25806;&#25216;&#26415;&#25110;&#38382;&#39064;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#25152;&#26377;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#34917;&#20805;&#36164;&#26009;&#37117;&#21487;&#20197;&#22312;GitHub&#19978;&#25214;&#21040;&#65306;https://github.com/matthewrenze/jhu-llm-temperature&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used four popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.0. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature in the range 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to hold regardless of the LLM, the prompt-engineering technique, or the problem domain. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature.
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#21463;&#38480;&#65292;&#26080;&#27861;&#23454;&#29616;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#21644;&#20551;&#35774;&#27979;&#35797;&#30340;MatSci-LLMs&#26694;&#26550;&#65292;&#24182;&#25551;&#36848;&#20102;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.05200</link><description>&lt;p&gt;
LLMs&#26159;&#21542;&#20934;&#22791;&#22909;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#26448;&#26009;&#21457;&#29616;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Ready for Real-World Materials Discovery?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05200
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#21463;&#38480;&#65292;&#26080;&#27861;&#23454;&#29616;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#21644;&#20551;&#35774;&#27979;&#35797;&#30340;MatSci-LLMs&#26694;&#26550;&#65292;&#24182;&#25551;&#36848;&#20102;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24378;&#22823;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;&#65292;&#21152;&#24555;&#20102;&#26448;&#26009;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20173;&#23384;&#22312;&#19981;&#36275;&#65292;&#26080;&#27861;&#25104;&#20026;&#23454;&#29992;&#30340;&#26448;&#26009;&#31185;&#23398;&#24037;&#20855;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#30456;&#20851;&#22833;&#36133;&#26696;&#20363;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#29702;&#35299;&#21644;&#25512;&#29702;&#22797;&#26434;&#12289;&#30456;&#20114;&#20851;&#32852;&#30340;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#26041;&#38754;&#30340;&#29616;&#26377;&#38480;&#21046;&#12290;&#37492;&#20110;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21457;&#22522;&#20110;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#21644;&#20551;&#35774;&#29983;&#25104;&#19982;&#27979;&#35797;&#30340;&#26448;&#26009;&#31185;&#23398;LLMs&#65288;MatSci-LLMs&#65289;&#30340;&#26694;&#26550;&#12290;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;MatSci-LLMs&#30340;&#36335;&#24452;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#24314;&#31435;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#31185;&#23398;&#25991;&#29486;&#65292;&#20854;&#20013;&#23384;&#22312;&#21508;&#31181;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) create exciting possibilities for powerful language processing tools to accelerate research in materials science. While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge. Given those shortcomings, we outline a framework for developing Materials Science LLMs (MatSci-LLMs) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing. The path to attaining performant MatSci-LLMs rests in large part on building high-quality, multi-modal datasets sourced from scientific literature where various information extraction challenges persist. As such, we describe key materials science information extraction cha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InCoRo&#31995;&#32479;&#65292;&#20351;&#29992;&#32463;&#20856;&#30340;&#26426;&#22120;&#20154;&#21453;&#39304;&#24490;&#29615;&#65292;&#36890;&#36807;LLM&#25511;&#21046;&#22120;&#12289;&#22330;&#26223;&#29702;&#35299;&#21333;&#20803;&#21644;&#26426;&#22120;&#20154;&#30340;&#21327;&#21516;&#24037;&#20316;&#65292;&#23454;&#29616;&#23545;&#21160;&#24577;&#29615;&#22659;&#20013;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#25345;&#32493;&#20998;&#26512;&#29615;&#22659;&#29366;&#24577;&#24182;&#25552;&#20379;&#36866;&#24212;&#24615;&#25191;&#34892;&#21629;&#20196;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#24182;&#32416;&#27491;&#25511;&#21046;&#22120;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.05188</link><description>&lt;p&gt;
InCoRo&#65306;&#24102;&#26377;&#21453;&#39304;&#24490;&#29615;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#26426;&#22120;&#20154;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
InCoRo: In-Context Learning for Robotics Control with Feedback Loops
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InCoRo&#31995;&#32479;&#65292;&#20351;&#29992;&#32463;&#20856;&#30340;&#26426;&#22120;&#20154;&#21453;&#39304;&#24490;&#29615;&#65292;&#36890;&#36807;LLM&#25511;&#21046;&#22120;&#12289;&#22330;&#26223;&#29702;&#35299;&#21333;&#20803;&#21644;&#26426;&#22120;&#20154;&#30340;&#21327;&#21516;&#24037;&#20316;&#65292;&#23454;&#29616;&#23545;&#21160;&#24577;&#29615;&#22659;&#20013;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#25345;&#32493;&#20998;&#26512;&#29615;&#22659;&#29366;&#24577;&#24182;&#25552;&#20379;&#36866;&#24212;&#24615;&#25191;&#34892;&#21629;&#20196;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#24182;&#32416;&#27491;&#25511;&#21046;&#22120;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#20351;&#26426;&#22120;&#20154;&#20855;&#22791;&#36275;&#22815;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;LLM&#36827;&#23637;&#23558;&#23427;&#20204;&#23450;&#20301;&#20026;&#31616;&#21333;&#25512;&#29702;&#20219;&#21153;&#30340;&#39318;&#36873;&#24037;&#20855;&#65292;&#28608;&#21457;&#20102;Liang&#31561;&#20154;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;[35]&#65292;&#35813;&#24037;&#20316;&#20351;&#29992;LLM&#23558;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#36716;&#21270;&#20026;&#26426;&#22120;&#20154;&#21333;&#20301;&#30340;&#20302;&#32423;&#38745;&#24577;&#25191;&#34892;&#35745;&#21010;&#12290;&#22312;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#20351;&#29992;LLM&#23558;&#20854;&#27867;&#21270;&#33021;&#21147;&#25552;&#21319;&#21040;&#19968;&#20010;&#26032;&#30340;&#27700;&#24179;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#26412;&#25991;&#23558;&#36825;&#39033;&#20808;&#21069;&#24037;&#20316;&#25193;&#23637;&#21040;&#20102;&#21160;&#24577;&#29615;&#22659;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;InCoRo&#65292;&#19968;&#20010;&#20351;&#29992;&#32463;&#20856;&#30340;&#26426;&#22120;&#20154;&#21453;&#39304;&#24490;&#29615;&#30340;&#31995;&#32479;&#65292;&#30001;LLM&#25511;&#21046;&#22120;&#12289;&#22330;&#26223;&#29702;&#35299;&#21333;&#20803;&#21644;&#26426;&#22120;&#20154;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#25345;&#32493;&#20998;&#26512;&#29615;&#22659;&#29366;&#24577;&#24182;&#25552;&#20379;&#36866;&#24212;&#24615;&#25191;&#34892;&#21629;&#20196;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#26465;&#20214;&#24182;&#32416;&#27491;&#25511;&#21046;&#22120;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#19981;&#38656;&#35201;&#20219;&#20309;&#36845;&#20195;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges in robotics is to enable robotic units with the reasoning capability that would be robust enough to execute complex tasks in dynamic environments. Recent advances in LLMs have positioned them as go-to tools for simple reasoning tasks, motivating the pioneering work of Liang et al. [35] that uses an LLM to translate natural language commands into low-level static execution plans for robotic units. Using LLMs inside robotics systems brings their generalization to a new level, enabling zero-shot generalization to new tasks. This paper extends this prior work to dynamic environments. We propose InCoRo, a system that uses a classical robotic feedback loop composed of an LLM controller, a scene understanding unit, and a robot. Our system continuously analyzes the state of the environment and provides adapted execution commands, enabling the robot to adjust to changing environmental conditions and correcting for controller errors. Our system does not require any iterativ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#32553;&#25918;&#24459;&#30340;&#36164;&#28304;&#27169;&#22411;&#65292;&#36890;&#36807;&#35266;&#23519;&#23454;&#35777;&#21457;&#29616;&#65292;&#23376;&#20219;&#21153;&#30340;&#25439;&#22833;&#19982;&#20998;&#37197;&#30340;&#31070;&#32463;&#20803;&#25104;&#21453;&#27604;&#65292;&#22797;&#21512;&#20219;&#21153;&#20013;&#23376;&#20219;&#21153;&#33719;&#24471;&#30340;&#36164;&#28304;&#38543;&#27169;&#22411;&#21464;&#22823;&#32780;&#22686;&#38271;&#65292;&#20445;&#25345;&#36164;&#28304;&#27604;&#20363;&#19981;&#21464;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#22797;&#21512;&#20219;&#21153;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#65292;&#24182;&#25104;&#21151;&#22797;&#21046;&#20102;Chinchilla&#27169;&#22411;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#12290;&#35813;&#36164;&#28304;&#27169;&#22411;&#26159;&#34920;&#24449;&#21644;&#35786;&#26029;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.05164</link><description>&lt;p&gt;
&#31070;&#32463;&#32553;&#25918;&#24459;&#30340;&#36164;&#28304;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Resource Model For Neural Scaling Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#32553;&#25918;&#24459;&#30340;&#36164;&#28304;&#27169;&#22411;&#65292;&#36890;&#36807;&#35266;&#23519;&#23454;&#35777;&#21457;&#29616;&#65292;&#23376;&#20219;&#21153;&#30340;&#25439;&#22833;&#19982;&#20998;&#37197;&#30340;&#31070;&#32463;&#20803;&#25104;&#21453;&#27604;&#65292;&#22797;&#21512;&#20219;&#21153;&#20013;&#23376;&#20219;&#21153;&#33719;&#24471;&#30340;&#36164;&#28304;&#38543;&#27169;&#22411;&#21464;&#22823;&#32780;&#22686;&#38271;&#65292;&#20445;&#25345;&#36164;&#28304;&#27604;&#20363;&#19981;&#21464;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#22797;&#21512;&#20219;&#21153;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#65292;&#24182;&#25104;&#21151;&#22797;&#21046;&#20102;Chinchilla&#27169;&#22411;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#12290;&#35813;&#36164;&#28304;&#27169;&#22411;&#26159;&#34920;&#24449;&#21644;&#35786;&#26029;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32553;&#25918;&#24459;&#25551;&#36848;&#20102;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#27169;&#22411;&#24615;&#33021;&#22914;&#20309;&#25552;&#39640;&#12290;&#21463;&#23454;&#35777;&#35266;&#23519;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#32553;&#25918;&#30340;&#36164;&#28304;&#27169;&#22411;&#12290;&#19968;&#20010;&#20219;&#21153;&#36890;&#24120;&#26159;&#22797;&#21512;&#20219;&#21153;&#65292;&#21487;&#20197;&#20998;&#35299;&#20026;&#35768;&#22810;&#23376;&#20219;&#21153;&#65292;&#36825;&#20123;&#23376;&#20219;&#21153;&#31454;&#20105;&#36164;&#28304;&#65288;&#20197;&#20998;&#37197;&#32473;&#23376;&#20219;&#21153;&#30340;&#31070;&#32463;&#20803;&#25968;&#37327;&#26469;&#34913;&#37327;&#65289;&#12290;&#22312;&#29609;&#20855;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#65306;&#65288;1&#65289;&#23376;&#20219;&#21153;&#30340;&#25439;&#22833;&#19982;&#20854;&#20998;&#37197;&#30340;&#31070;&#32463;&#20803;&#25104;&#21453;&#27604;&#12290;&#65288;2&#65289;&#24403;&#22797;&#21512;&#20219;&#21153;&#20013;&#23384;&#22312;&#22810;&#20010;&#23376;&#20219;&#21153;&#26102;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#22823;&#65292;&#27599;&#20010;&#23376;&#20219;&#21153;&#33719;&#24471;&#30340;&#36164;&#28304;&#22343;&#21248;&#22686;&#38271;&#65292;&#20445;&#25345;&#33719;&#24471;&#36164;&#28304;&#30340;&#27604;&#20363;&#19981;&#21464;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#21457;&#29616;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#19968;&#33324;&#22797;&#21512;&#20219;&#21153;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#65292;&#24182;&#25104;&#21151;&#22797;&#21046;&#20102;arXiv:2203.15556&#20013;&#25253;&#21578;&#30340;Chinchilla&#27169;&#22411;&#30340;&#31070;&#32463;&#32553;&#25918;&#24459;&#12290;&#25105;&#20204;&#30456;&#20449;&#26412;&#25991;&#20013;&#20351;&#29992;&#30340;&#36164;&#28304;&#27010;&#24565;&#23558;&#26159;&#34920;&#24449;&#21644;&#35786;&#26029;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural scaling laws characterize how model performance improves as the model size scales up. Inspired by empirical observations, we introduce a resource model of neural scaling. A task is usually composite hence can be decomposed into many subtasks, which compete for resources (measured by the number of neurons allocated to subtasks). On toy problems, we empirically find that: (1) The loss of a subtask is inversely proportional to its allocated neurons. (2) When multiple subtasks are present in a composite task, the resources acquired by each subtask uniformly grow as models get larger, keeping the ratios of acquired resources constants. We hypothesize these findings to be generally true and build a model to predict neural scaling laws for general composite tasks, which successfully replicates the neural scaling law of Chinchilla models reported in arXiv:2203.15556. We believe that the notion of resource used in this paper will be a useful tool for characterizing and diagnosing neural 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#26426;&#21046;&#22266;&#26377;&#26131;&#30862;&#24615;&#65292;&#21435;&#38500;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#20294;&#23545;&#25928;&#29992;&#24433;&#21709;&#19981;&#22823;&#65292;&#38656;&#35201;&#26356;&#24378;&#20581;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.05162</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#35780;&#20272;&#23433;&#20840;&#23545;&#40784;&#30340;&#26131;&#30862;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#26426;&#21046;&#22266;&#26377;&#26131;&#30862;&#24615;&#65292;&#21435;&#38500;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#20294;&#23545;&#25928;&#29992;&#24433;&#21709;&#19981;&#22823;&#65292;&#38656;&#35201;&#26356;&#24378;&#20581;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20854;&#23433;&#20840;&#26426;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#22266;&#26377;&#30340;&#26131;&#30862;&#24615;&#65292;&#36825;&#21487;&#20174;&#23427;&#20204;&#26131;&#21463;&#36234;&#29425;&#21644;&#21363;&#20351;&#26159;&#38750;&#24694;&#24847;&#24494;&#35843;&#20063;&#26131;&#21463;&#24433;&#21709;&#26469;&#35828;&#26126;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#25506;&#35752;&#20102;&#23433;&#20840;&#23545;&#40784;&#30340;&#26131;&#30862;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#23545;&#20110;&#23433;&#20840;&#38450;&#25252;&#33267;&#20851;&#37325;&#35201;&#65292;&#19988;&#22312;&#31070;&#32463;&#20803;&#21644;&#31209;&#32423;&#21035;&#19978;&#19982;&#25928;&#29992;&#30456;&#20851;&#30340;&#21306;&#22495;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#30340;&#23396;&#31435;&#21306;&#22495;&#26159;&#31232;&#30095;&#30340;&#65292;&#32422;&#21344;&#21442;&#25968;&#32423;&#21035;&#30340;$3\%$&#21644;&#25490;&#21517;&#32423;&#21035;&#30340;$2.5\%$&#12290;&#21435;&#38500;&#36825;&#20123;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#32780;&#23545;&#25928;&#29992;&#30340;&#24433;&#21709;&#19981;&#22823;&#65292;&#20174;&#32780;&#35777;&#23454;&#20102;&#35813;&#27169;&#22411;&#23433;&#20840;&#26426;&#21046;&#30340;&#22266;&#26377;&#26131;&#30862;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#38480;&#21046;&#23545;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#36827;&#34892;&#20462;&#25913;&#65292;LLMs&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#20302;&#25104;&#26412;&#30340;&#24494;&#35843;&#25915;&#20987;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#26356;&#24378;&#22823;&#30340;&#23433;&#20840;&#31574;&#30053;&#30340;&#32039;&#36843;&#24615;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;Hugging Face&#24179;&#21488;&#19978;&#30340;32,111&#20221;AI&#27169;&#22411;&#25991;&#26723;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#25552;&#20379;&#20102;&#27169;&#22411;&#21345;&#65292;&#20294;&#20449;&#24687;&#37327;&#19981;&#19968;&#33268;&#12290;&#26377;&#20851;&#29615;&#22659;&#24433;&#21709;&#12289;&#38480;&#21046;&#21644;&#35780;&#20272;&#30340;&#37096;&#20998;&#22635;&#20889;&#29575;&#26368;&#20302;&#65292;&#35757;&#32451;&#37096;&#20998;&#21017;&#22635;&#20889;&#29575;&#26368;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.05160</link><description>&lt;p&gt;
AI&#30340;&#25991;&#26723;&#21270;&#24773;&#20917;&#22914;&#20309;&#65311;&#23545;32,000&#20221;AI&#27169;&#22411;&#21345;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
What's documented in AI? Systematic Analysis of 32K AI Model Cards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;Hugging Face&#24179;&#21488;&#19978;&#30340;32,111&#20221;AI&#27169;&#22411;&#25991;&#26723;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#25552;&#20379;&#20102;&#27169;&#22411;&#21345;&#65292;&#20294;&#20449;&#24687;&#37327;&#19981;&#19968;&#33268;&#12290;&#26377;&#20851;&#29615;&#22659;&#24433;&#21709;&#12289;&#38480;&#21046;&#21644;&#35780;&#20272;&#30340;&#37096;&#20998;&#22635;&#20889;&#29575;&#26368;&#20302;&#65292;&#35757;&#32451;&#37096;&#20998;&#21017;&#22635;&#20889;&#29575;&#26368;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#27169;&#22411;&#30340;&#36805;&#36895;&#22686;&#21152;&#31361;&#26174;&#20102;&#20805;&#20998;&#30340;&#25991;&#26723;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#36825;&#26679;&#21487;&#20197;&#20351;&#29992;&#25143;&#20102;&#35299;&#12289;&#20449;&#20219;&#24182;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#12290;&#34429;&#28982;&#24320;&#21457;&#32773;&#40723;&#21169;&#21046;&#20316;&#27169;&#22411;&#21345;&#65292;&#20294;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#36825;&#20123;&#21345;&#21253;&#21547;&#22810;&#23569;&#20449;&#24687;&#25110;&#32773;&#21253;&#21547;&#21738;&#20123;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;Hugging Face&#24179;&#21488;&#19978;&#30340;32,111&#20221;AI&#27169;&#22411;&#25991;&#26723;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#35813;&#24179;&#21488;&#26159;&#20998;&#21457;&#21644;&#37096;&#32626;AI&#27169;&#22411;&#30340;&#39046;&#20808;&#24179;&#21488;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#26222;&#36941;&#30340;&#27169;&#22411;&#21345;&#25991;&#26723;&#21270;&#23454;&#36341;&#12290;&#22823;&#22810;&#25968;&#19979;&#36733;&#37327;&#36739;&#22823;&#30340;AI&#27169;&#22411;&#25552;&#20379;&#20102;&#27169;&#22411;&#21345;&#65292;&#20294;&#36825;&#20123;&#21345;&#30340;&#20449;&#24687;&#37327;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26377;&#20851;&#29615;&#22659;&#24433;&#21709;&#12289;&#38480;&#21046;&#21644;&#35780;&#20272;&#30340;&#37096;&#20998;&#22635;&#20889;&#29575;&#26368;&#20302;&#65292;&#32780;&#35757;&#32451;&#37096;&#20998;&#21017;&#26159;&#22635;&#20889;&#24471;&#26368;&#20840;&#38754;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#23545;&#27599;&#20010;&#37096;&#20998;&#30340;&#20869;&#23481;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;&#20174;&#19994;&#32773;&#30340;&#37325;&#28857;&#20851;&#27880;&#20869;&#23481;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26377;&#30456;&#24403;&#22810;&#30340;&#27169;&#22411;&#21345;&#22312;&#30456;&#20851;&#37096;&#20998;&#23384;&#22312;&#36739;&#22823;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid proliferation of AI models has underscored the importance of thorough documentation, as it enables users to understand, trust, and effectively utilize these models in various applications. Although developers are encouraged to produce model cards, it's not clear how much information or what information these cards contain. In this study, we conduct a comprehensive analysis of 32,111 AI model documentations on Hugging Face, a leading platform for distributing and deploying AI models. Our investigation sheds light on the prevailing model card documentation practices. Most of the AI models with substantial downloads provide model cards, though the cards have uneven informativeness. We find that sections addressing environmental impact, limitations, and evaluation exhibit the lowest filled-out rates, while the training section is the most consistently filled-out. We analyze the content of each section to characterize practitioners' priorities. Interestingly, there are substantial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23391;&#21152;&#25289;OCR&#31995;&#32479;&#65292;&#20855;&#26377;&#37325;&#26500;&#25991;&#26723;&#24067;&#23616;&#12289;&#31934;&#30830;&#25552;&#21462;&#12289;&#22810;&#26679;&#21270;&#25991;&#26723;&#31867;&#22411;&#25903;&#25345;&#21644;&#20248;&#21270;&#23383;&#31526;&#19982;&#21333;&#35789;&#35782;&#21035;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.05158</link><description>&lt;p&gt;
&#25552;&#39640;&#23391;&#21152;&#25289;OCR&#30340;&#19987;&#29992;&#27169;&#22411;&#21644;&#20808;&#36827;&#25216;&#26415;&#22312;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Enhancement of Bengali OCR by Specialized Models and Advanced Techniques for Diverse Document Types
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23391;&#21152;&#25289;OCR&#31995;&#32479;&#65292;&#20855;&#26377;&#37325;&#26500;&#25991;&#26723;&#24067;&#23616;&#12289;&#31934;&#30830;&#25552;&#21462;&#12289;&#22810;&#26679;&#21270;&#25991;&#26723;&#31867;&#22411;&#25903;&#25345;&#21644;&#20248;&#21270;&#23383;&#31526;&#19982;&#21333;&#35789;&#35782;&#21035;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#23391;&#21152;&#25289;OCR&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;&#19968;&#20123;&#33021;&#21147;&#12290;&#35813;&#31995;&#32479;&#22312;&#20445;&#30041;&#32467;&#26500;&#12289;&#23545;&#40784;&#21644;&#22270;&#20687;&#30340;&#21516;&#26102;&#65292;&#20248;&#31168;&#22320;&#37325;&#26500;&#25991;&#26723;&#24067;&#23616;&#12290;&#23427;&#32467;&#21512;&#20102;&#20808;&#36827;&#30340;&#22270;&#20687;&#21644;&#31614;&#21517;&#26816;&#27979;&#25216;&#26415;&#65292;&#20197;&#36827;&#34892;&#31934;&#30830;&#30340;&#25552;&#21462;&#12290;&#38024;&#23545;&#21253;&#25324;&#35745;&#31639;&#26426;&#25490;&#29256;&#12289;&#20984;&#29256;&#21360;&#21047;&#12289;&#25171;&#23383;&#26426;&#21644;&#25163;&#20889;&#25991;&#26723;&#22312;&#20869;&#30340;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#65292;&#35813;&#31995;&#32479;&#36824;&#21253;&#25324;&#20102;&#19987;&#38376;&#30340;&#21333;&#35789;&#20998;&#21106;&#27169;&#22411;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#38745;&#24577;&#21644;&#21160;&#24577;&#25163;&#20889;&#36755;&#20837;&#65292;&#24182;&#35782;&#21035;&#21508;&#31181;&#20070;&#20889;&#39118;&#26684;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#33021;&#22815;&#35782;&#21035;&#23391;&#21152;&#25289;&#22797;&#21512;&#23383;&#31526;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#25454;&#25910;&#38598;&#24037;&#20316;&#65292;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;&#35821;&#26009;&#24211;&#65292;&#32780;&#20808;&#36827;&#30340;&#25216;&#26415;&#32452;&#20214;&#21017;&#20248;&#21270;&#20102;&#23383;&#31526;&#21644;&#21333;&#35789;&#35782;&#21035;&#12290;&#20854;&#20182;&#36129;&#29486;&#21253;&#25324;&#22270;&#20687;&#12289;&#26631;&#24535;&#12289;&#31614;&#21517;&#21644;&#34920;&#26684;&#35782;&#21035;&#12289;&#36879;&#35270;&#26657;&#27491;&#12289;&#24067;&#23616;&#37325;&#24314;&#20197;&#21450;&#29992;&#20110;&#39640;&#25928;&#21487;&#25193;&#23637;&#22788;&#29702;&#30340;&#25490;&#38431;&#27169;&#22359;&#12290;&#35813;&#31995;&#32479;&#22312;&#39640;&#25928;&#20934;&#30830;&#30340;&#25991;&#26412;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper presents a unique Bengali OCR system with some capabilities. The system excels in reconstructing document layouts while preserving structure, alignment, and images. It incorporates advanced image and signature detection for accurate extraction. Specialized models for word segmentation cater to diverse document types, including computer-composed, letterpress, typewriter, and handwritten documents. The system handles static and dynamic handwritten inputs, recognizing various writing styles. Furthermore, it has the ability to recognize compound characters in Bengali. Extensive data collection efforts provide a diverse corpus, while advanced technical components optimize character and word recognition. Additional contributions include image, logo, signature and table recognition, perspective correction, layout reconstruction, and a queuing module for efficient and scalable processing. The system demonstrates outstanding performance in efficient and accurate text extract
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36827;&#34892;&#19968;&#39033;&#20851;&#20110;AI&#31995;&#32479;&#25968;&#25454;&#24037;&#31243;&#30340;&#26144;&#23556;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;AI&#25968;&#25454;&#24037;&#31243;&#27963;&#21160;&#30340;&#29983;&#21629;&#21608;&#26399;&#38454;&#27573;&#65292;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#21644;&#26550;&#26500;&#20197;&#21450;&#32463;&#39564;&#25945;&#35757;&#12290;</title><link>https://arxiv.org/abs/2402.05156</link><description>&lt;p&gt;
&#25968;&#25454;&#22914;&#20309;&#65311;&#20851;&#20110;AI&#31995;&#32479;&#25968;&#25454;&#24037;&#31243;&#30340;&#26144;&#23556;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What About the Data? A Mapping Study on Data Engineering for AI Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36827;&#34892;&#19968;&#39033;&#20851;&#20110;AI&#31995;&#32479;&#25968;&#25454;&#24037;&#31243;&#30340;&#26144;&#23556;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;AI&#25968;&#25454;&#24037;&#31243;&#27963;&#21160;&#30340;&#29983;&#21629;&#21608;&#26399;&#38454;&#27573;&#65292;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#21644;&#26550;&#26500;&#20197;&#21450;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#31163;&#19981;&#24320;&#25968;&#25454;&#12290;&#29616;&#22312;&#65292;&#38543;&#30528;AI&#27169;&#22411;&#65288;&#25968;&#25454;&#31185;&#23398;&#21644;AI&#65289;&#30340;&#25104;&#29087;&#65292;&#24182;&#21487;&#20197;&#23454;&#38469;&#24212;&#29992;&#65292;&#22823;&#37096;&#20998;&#32452;&#32455;&#22312;&#25968;&#25454;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#38656;&#35201;&#20102;&#35299;&#22914;&#20309;&#20026;AI&#31995;&#32479;&#20934;&#22791;&#25968;&#25454;&#30340;&#25968;&#25454;&#24037;&#31243;&#24072;&#25110;&#21487;&#20197;&#20026;&#20998;&#26512;&#39033;&#30446;&#35774;&#32622;&#20225;&#19994;&#32423;&#25968;&#25454;&#26550;&#26500;&#30340;&#24037;&#31243;&#24072;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#22312;&#36807;&#21435;&#65292;AI&#24037;&#31243;&#30340;&#25968;&#25454;&#24037;&#31243;&#37096;&#20998;&#27809;&#26377;&#24471;&#21040;&#22826;&#22810;&#20851;&#27880;&#65292;&#32780;&#26356;&#22810;&#35752;&#35770;&#24314;&#27169;&#37096;&#20998;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#36827;&#34892;&#19968;&#39033;&#20851;&#20110;AI&#31995;&#32479;&#25968;&#25454;&#24037;&#31243;&#30340;&#26144;&#23556;&#30740;&#31350;&#65288;&#21363;AI&#25968;&#25454;&#24037;&#31243;&#65289;&#26469;&#25913;&#21464;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#22312;2019&#24180;1&#26376;&#33267;2023&#24180;6&#26376;&#20043;&#38388;&#25214;&#21040;&#20102;25&#31687;&#30456;&#20851;&#35770;&#25991;&#65292;&#35299;&#37322;&#20102;AI&#25968;&#25454;&#24037;&#31243;&#27963;&#21160;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#28085;&#30422;&#30340;&#29983;&#21629;&#21608;&#26399;&#38454;&#27573;&#65292;&#25552;&#20986;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#25110;&#26550;&#26500;&#20197;&#21450;&#25152;&#21576;&#29616;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#35770;&#25991;&#36827;&#34892;&#20102;&#25972;&#20307;&#35752;&#35770;&#65292;&#24182;&#23545;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#20123;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI systems cannot exist without data. Now that AI models (data science and AI) have matured and are readily available to apply in practice, most organizations struggle with the data infrastructure to do so. There is a growing need for data engineers that know how to prepare data for AI systems or that can setup enterprise-wide data architectures for analytical projects. But until now, the data engineering part of AI engineering has not been getting much attention, in favor of discussing the modeling part. In this paper we aim to change this by perform a mapping study on data engineering for AI systems, i.e., AI data engineering. We found 25 relevant papers between January 2019 and June 2023, explaining AI data engineering activities. We identify which life cycle phases are covered, which technical solutions or architectures are proposed and which lessons learned are presented. We end by an overall discussion of the papers with implications for practitioners and researchers. This paper 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36229;&#22270;&#32593;&#32476;&#65288;AHNTP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39640;&#38454;&#30456;&#20851;&#24615;&#26469;&#25913;&#21892;&#20449;&#20219;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;AHNTP&#21033;&#29992;&#22522;&#20110;&#27169;&#24335;&#30340;PageRank&#26469;&#25429;&#25417;&#39640;&#38454;&#31038;&#20132;&#24433;&#21709;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#36229;&#32676;&#20174;&#33410;&#28857;&#32423;&#21644;&#32467;&#26500;&#32423;&#23646;&#24615;&#34701;&#20837;&#22797;&#26434;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.05154</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#36229;&#22270;&#32593;&#32476;&#29992;&#20110;&#20449;&#20219;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Hypergraph Network for Trust Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36229;&#22270;&#32593;&#32476;&#65288;AHNTP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39640;&#38454;&#30456;&#20851;&#24615;&#26469;&#25913;&#21892;&#20449;&#20219;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;AHNTP&#21033;&#29992;&#22522;&#20110;&#27169;&#24335;&#30340;PageRank&#26469;&#25429;&#25417;&#39640;&#38454;&#31038;&#20132;&#24433;&#21709;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#36229;&#32676;&#20174;&#33410;&#28857;&#32423;&#21644;&#32467;&#26500;&#32423;&#23646;&#24615;&#34701;&#20837;&#22797;&#26434;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#22312;&#20010;&#20307;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;&#20449;&#20219;&#39044;&#27979;&#27169;&#22411;&#20381;&#36182;&#20110;&#37197;&#23545;&#30456;&#20851;&#24615;&#26469;&#25512;&#26029;&#29992;&#25143;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#36890;&#24120;&#26159;&#22797;&#26434;&#30340;&#65292;&#19981;&#20165;&#20165;&#26159;&#37197;&#23545;&#20851;&#31995;&#12290;&#36229;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#36825;&#20123;&#22797;&#26434;&#30340;&#39640;&#38454;&#20851;&#32852;&#20851;&#31995;&#65292;&#22240;&#20026;&#36229;&#22270;&#21487;&#20197;&#21033;&#29992;&#36229;&#36793;&#26469;&#36830;&#25509;&#20004;&#20010;&#20197;&#19978;&#30340;&#33410;&#28857;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#36229;&#22270;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#24212;&#29992;&#21040;&#20449;&#20219;&#39044;&#27979;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20449;&#20219;&#39044;&#27979;&#30340;&#33258;&#36866;&#24212;&#36229;&#22270;&#32593;&#32476;&#65288;AHNTP&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#38454;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#20449;&#20219;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;AHNTP&#21033;&#29992;&#22522;&#20110;&#27169;&#24335;&#30340;PageRank&#26469;&#25429;&#25417;&#39640;&#38454;&#31038;&#20132;&#24433;&#21709;&#20449;&#24687;&#12290;&#21478;&#22806;&#65292;&#23427;&#36824;&#20174;&#33410;&#28857;&#32423;&#21644;&#32467;&#26500;&#32423;&#23646;&#24615;&#26500;&#24314;&#36229;&#32676;&#65292;&#20197;&#34701;&#20837;&#22797;&#26434;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust plays an essential role in an individual's decision-making. Traditional trust prediction models rely on pairwise correlations to infer potential relationships between users. However, in the real world, interactions between users are usually complicated rather than pairwise only. Hypergraphs offer a flexible approach to modeling these complex high-order correlations (not just pairwise connections), since hypergraphs can leverage hyperedeges to link more than two nodes. However, most hypergraph-based methods are generic and cannot be well applied to the trust prediction task. In this paper, we propose an Adaptive Hypergraph Network for Trust Prediction (AHNTP), a novel approach that improves trust prediction accuracy by using higher-order correlations. AHNTP utilizes Motif-based PageRank to capture high-order social influence information. In addition, it constructs hypergroups from both node-level and structure-level attributes to incorporate complex correlation information. Furthe
&lt;/p&gt;</description></item><item><title>CrashFormer&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#21033;&#29992;&#20840;&#38754;&#30340;&#36755;&#20837;&#25968;&#25454;&#65288;&#22914;&#20107;&#25925;&#21382;&#21490;&#12289;&#22825;&#27668;&#20449;&#24687;&#12289;&#22320;&#22270;&#22270;&#20687;&#21644;&#20154;&#21475;&#20449;&#24687;&#65289;&#65292;&#21487;&#20197;&#27599;6&#23567;&#26102;&#39044;&#27979;5.161&#24179;&#26041;&#20844;&#37324;&#22320;&#29702;&#33539;&#22260;&#20869;&#30340;&#26410;&#26469;&#20107;&#25925;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.05151</link><description>&lt;p&gt;
CrashFormer: &#19968;&#31181;&#22810;&#27169;&#24577;&#26550;&#26500;&#29992;&#20110;&#39044;&#27979;&#20107;&#25925;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
CrashFormer: A Multimodal Architecture to Predict the Risk of Crash
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05151
&lt;/p&gt;
&lt;p&gt;
CrashFormer&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#21033;&#29992;&#20840;&#38754;&#30340;&#36755;&#20837;&#25968;&#25454;&#65288;&#22914;&#20107;&#25925;&#21382;&#21490;&#12289;&#22825;&#27668;&#20449;&#24687;&#12289;&#22320;&#22270;&#22270;&#20687;&#21644;&#20154;&#21475;&#20449;&#24687;&#65289;&#65292;&#21487;&#20197;&#27599;6&#23567;&#26102;&#39044;&#27979;5.161&#24179;&#26041;&#20844;&#37324;&#22320;&#29702;&#33539;&#22260;&#20869;&#30340;&#26410;&#26469;&#20107;&#25925;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#20132;&#36890;&#20107;&#25925;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#20840;&#29699;&#20844;&#20849;&#23433;&#20840;&#38382;&#39064;&#12290;&#20107;&#25925;&#39044;&#27979;&#23545;&#20110;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#22312;&#20107;&#25925;&#21457;&#29983;&#20043;&#21069;&#37319;&#21462;&#31215;&#26497;&#30340;&#25514;&#26045;&#65292;&#24182;&#25552;&#20379;&#23433;&#20840;&#25919;&#31574;&#12289;&#27861;&#35268;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#25514;&#26045;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#36807;&#21435;&#20960;&#21313;&#24180;&#36827;&#34892;&#20102;&#35768;&#22810;&#20851;&#20110;&#20107;&#25925;&#39044;&#27979;&#30340;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#36755;&#20837;&#25968;&#25454;&#25110;&#38382;&#39064;&#24418;&#24335;&#30340;&#38480;&#21046;&#65292;&#35768;&#22810;&#30740;&#31350;&#22312;&#21487;&#25512;&#24191;&#24615;&#12289;&#21487;&#37325;&#29616;&#24615;&#25110;&#23454;&#38469;&#24212;&#29992;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CrashFormer&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#21033;&#29992;&#20840;&#38754;&#65288;&#20294;&#30456;&#23545;&#23481;&#26131;&#33719;&#21462;&#65289;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#22914;&#20107;&#25925;&#21382;&#21490;&#12289;&#22825;&#27668;&#20449;&#24687;&#12289;&#22320;&#22270;&#22270;&#20687;&#21644;&#20154;&#21475;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20197;&#30456;&#23545;&#21487;&#25509;&#21463;&#30340;&#39057;&#29575;&#65288;&#21363;&#27599;6&#23567;&#26102;&#65289;&#39044;&#27979;5.161&#24179;&#26041;&#20844;&#37324;&#22320;&#29702;&#33539;&#22260;&#20869;&#30340;&#26410;&#26469;&#20107;&#25925;&#39118;&#38505;&#12290;CrashFormer&#30001;&#20116;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#24207;&#21015;&#32534;&#30721;&#22120;&#29992;&#20110;&#21033;&#29992;&#21382;&#21490;&#20107;&#25925;&#21644;&#22825;&#27668;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing traffic accidents is a crucial global public safety concern. Accident prediction is key to improving traffic safety, enabling proactive measures to be taken before a crash occurs, and informing safety policies, regulations, and targeted interventions. Despite numerous studies on accident prediction over the past decades, many have limitations in terms of generalizability, reproducibility, or feasibility for practical use due to input data or problem formulation. To address existing shortcomings, we propose CrashFormer, a multi-modal architecture that utilizes comprehensive (but relatively easy to obtain) inputs such as the history of accidents, weather information, map images, and demographic information. The model predicts the future risk of accidents on a reasonably acceptable cadence (i.e., every six hours) for a geographical location of 5.161 square kilometers. CrashFormer is composed of five components: a sequential encoder to utilize historical accidents and weather data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#30340;&#21160;&#20316;&#32422;&#26463;&#31574;&#30053;&#26799;&#24230;&#65288;FlowPG&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21160;&#20316;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#21487;&#36870;&#26144;&#23556;&#21644;&#24320;&#21457;&#22810;&#31181;&#21160;&#20316;&#37319;&#26679;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22312;&#27599;&#20010;&#24378;&#21270;&#23398;&#20064;&#27493;&#39588;&#20013;&#30830;&#20445;&#20195;&#29702;&#37319;&#21462;&#21512;&#29702;&#21160;&#20316;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05149</link><description>&lt;p&gt;
FlowPG: &#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#30340;&#21160;&#20316;&#32422;&#26463;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
FlowPG: Action-constrained Policy Gradient with Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#30340;&#21160;&#20316;&#32422;&#26463;&#31574;&#30053;&#26799;&#24230;&#65288;FlowPG&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21160;&#20316;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#21487;&#36870;&#26144;&#23556;&#21644;&#24320;&#21457;&#22810;&#31181;&#21160;&#20316;&#37319;&#26679;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22312;&#27599;&#20010;&#24378;&#21270;&#23398;&#20064;&#27493;&#39588;&#20013;&#30830;&#20445;&#20195;&#29702;&#37319;&#21462;&#21512;&#29702;&#21160;&#20316;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#20316;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#65288;ACRL&#65289;&#26159;&#35299;&#20915;&#23433;&#20840;&#20851;&#38190;&#21644;&#36164;&#28304;&#20998;&#37197;&#30456;&#20851;&#20915;&#31574;&#38382;&#39064;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;ACRL&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#30830;&#20445;&#20195;&#29702;&#22312;&#27599;&#20010;&#24378;&#21270;&#23398;&#20064;&#27493;&#39588;&#20013;&#37319;&#21462;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#26377;&#25928;&#21160;&#20316;&#12290;&#36890;&#24120;&#20351;&#29992;&#22312;&#31574;&#30053;&#32593;&#32476;&#20043;&#19978;&#30340;&#25237;&#24433;&#23618;&#30340;&#26041;&#27861;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#36739;&#38271;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#38646;&#26799;&#24230;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#27169;&#22411;&#23398;&#20064;&#19968;&#20010;&#21487;&#36870;&#30340;&#12289;&#21487;&#24494;&#20998;&#30340;&#26144;&#23556;&#65292;&#23558;&#21487;&#34892;&#21160;&#20316;&#31354;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#28508;&#21464;&#37327;&#19978;&#30340;&#31616;&#21333;&#20998;&#24067;&#30340;&#25903;&#25745;&#38598;&#21512;&#65292;&#20363;&#22914;&#39640;&#26031;&#20998;&#24067;&#12290;&#20854;&#27425;&#65292;&#23398;&#20064;&#27969;&#27169;&#22411;&#38656;&#35201;&#20174;&#21487;&#34892;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#20063;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#27931;&#21644;&#27010;&#29575;&#34920;&#20915;&#22270;&#65292;&#29992;&#20110;&#20984;&#32422;&#26463;&#21644;&#38750;&#20984;&#32422;&#26463;&#19979;&#30340;&#21160;&#20316;&#37319;&#26679;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#23398;&#20064;&#30340;&#27969;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#32593;&#32476;&#30456;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action-constrained reinforcement learning (ACRL) is a popular approach for solving safety-critical and resource-allocation related decision making problems. A major challenge in ACRL is to ensure agent taking a valid action satisfying constraints in each RL step. Commonly used approach of using a projection layer on top of the policy network requires solving an optimization program which can result in longer training time, slow convergence, and zero gradient problem. To address this, first we use a normalizing flow model to learn an invertible, differentiable mapping between the feasible action space and the support of a simple distribution on a latent variable, such as Gaussian. Second, learning the flow model requires sampling from the feasible action space, which is also challenging. We develop multiple methods, based on Hamiltonian Monte-Carlo and probabilistic sentential decision diagrams for such action sampling for convex and non-convex constraints. Third, we integrate the learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#26041;&#27861;&#30340;&#20998;&#25955;&#24335;&#35843;&#24230;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#27169;&#22359;&#21270;&#30005;&#35299;&#26893;&#29289;&#30340;&#36816;&#33829;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#24179;&#34913;&#27682;&#27668;&#20135;&#37327;&#21644;&#27874;&#21160;&#38656;&#27714;&#65292;&#20197;&#26368;&#23567;&#21270;&#36793;&#38469;&#21270;&#27682;&#27668;&#25104;&#26412;&#65292;&#24182;&#30830;&#20445;&#36866;&#24212;&#36816;&#33829;&#24178;&#25200;&#12290;</title><link>https://arxiv.org/abs/2402.05148</link><description>&lt;p&gt;
&#27169;&#22359;&#21270;&#30005;&#35299;&#26893;&#29289;&#30340;&#25104;&#26412;&#20248;&#21270;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Cost Optimized Scheduling in Modular Electrolysis Plants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#26041;&#27861;&#30340;&#20998;&#25955;&#24335;&#35843;&#24230;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#27169;&#22359;&#21270;&#30005;&#35299;&#26893;&#29289;&#30340;&#36816;&#33829;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#24179;&#34913;&#27682;&#27668;&#20135;&#37327;&#21644;&#27874;&#21160;&#38656;&#27714;&#65292;&#20197;&#26368;&#23567;&#21270;&#36793;&#38469;&#21270;&#27682;&#27668;&#25104;&#26412;&#65292;&#24182;&#30830;&#20445;&#36866;&#24212;&#36816;&#33829;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20840;&#29699;&#36716;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#36235;&#21183;&#65292;&#36890;&#36807;&#30005;&#35299;&#20135;&#29983;&#32511;&#33394;&#27682;&#27668;&#27491;&#22312;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35774;&#35745;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22359;&#21270;&#30005;&#35299;&#26893;&#29289;&#65292;&#22312;&#36866;&#24212;&#21487;&#20877;&#29983;&#33021;&#28304;&#27874;&#21160;&#30340;&#21516;&#26102;&#65292;&#20026;&#27682;&#27668;&#26085;&#30410;&#22686;&#38271;&#30340;&#38656;&#27714;&#25552;&#20379;&#21160;&#24577;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#23427;&#20204;&#30340;&#36816;&#33829;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#21327;&#35843;&#22823;&#37327;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#29305;&#24615;&#30340;&#30005;&#35299;&#27169;&#22359;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#26041;&#27861;&#30340;&#20998;&#25955;&#24335;&#35843;&#24230;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#27169;&#22359;&#21270;&#30005;&#35299;&#26893;&#29289;&#30340;&#36816;&#33829;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#24179;&#34913;&#27682;&#27668;&#20135;&#37327;&#21644;&#27874;&#21160;&#38656;&#27714;&#65292;&#20197;&#26368;&#23567;&#21270;&#36793;&#38469;&#21270;&#27682;&#27668;&#25104;&#26412;&#65292;&#24182;&#30830;&#20445;&#36866;&#24212;&#36816;&#33829;&#24178;&#25200;&#12290;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#22312;&#35745;&#31639;&#36793;&#38469;&#21270;&#27682;&#27668;&#25104;&#26412;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to the global shift towards renewable energy resources, the production of green hydrogen through electrolysis is emerging as a promising solution. Modular electrolysis plants, designed for flexibility and scalability, offer a dynamic response to the increasing demand for hydrogen while accommodating the fluctuations inherent in renewable energy sources. However, optimizing their operation is challenging, especially when a large number of electrolysis modules needs to be coordinated, each with potentially different characteristics.   To address these challenges, this paper presents a decentralized scheduling model to optimize the operation of modular electrolysis plants using the Alternating Direction Method of Multipliers. The model aims to balance hydrogen production with fluctuating demand, to minimize the marginal Levelized Cost of Hydrogen (mLCOH), and to ensure adaptability to operational disturbances. A case study validates the accuracy of the model in calculating mLC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#21387;&#32553;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32593;&#32476;&#65292;&#20197;&#20415;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#33258;&#21160;&#39550;&#39542;&#35774;&#22791;&#20013;&#23454;&#29616;&#39640;&#25928;&#37096;&#32626;&#12290;&#36890;&#36807;&#36880;&#28176;&#21024;&#38500;&#19981;&#37325;&#35201;&#30340;&#31070;&#32463;&#20803;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23567;&#20102;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.05146</link><description>&lt;p&gt;
&#29992;&#21160;&#24577;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#21387;&#32553;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32593;&#32476;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Compressing Deep Reinforcement Learning Networks with a Dynamic Structured Pruning Method for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#21387;&#32553;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32593;&#32476;&#65292;&#20197;&#20415;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#33258;&#21160;&#39550;&#39542;&#35774;&#22791;&#20013;&#23454;&#29616;&#39640;&#25928;&#37096;&#32626;&#12290;&#36890;&#36807;&#36880;&#28176;&#21024;&#38500;&#19981;&#37325;&#35201;&#30340;&#31070;&#32463;&#20803;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23567;&#20102;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#22797;&#26434;&#30340;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#19981;&#21487;&#36991;&#20813;&#22320;&#24102;&#26469;&#20102;&#39640;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#37327;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#33258;&#21160;&#39550;&#39542;&#35774;&#22791;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#29992;&#30340;&#26041;&#27861;&#26469;&#21387;&#32553;&#21644;&#21152;&#36895;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#26159;&#20272;&#35745;&#19968;&#20010;&#21442;&#25968;&#65288;&#21363;&#31070;&#32463;&#20803;&#65289;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#36129;&#29486;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#36880;&#28176;&#21024;&#38500;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#19981;&#37325;&#35201;&#30340;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65292;&#21363;&#20351;&#29992;&#32452;&#31232;&#30095;&#27491;&#21017;&#21270;&#22120;&#35757;&#32451;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#21160;&#24577;&#21098;&#26525;&#38408;&#20540;&#21435;&#38500;&#19981;&#37325;&#35201;&#30340;&#31070;&#32463;&#20803;&#12290;&#20026;&#20102;&#20351;&#29992;&#23569;&#37327;&#37325;&#35201;&#30340;&#31070;&#32463;&#20803;&#26377;&#25928;&#22320;&#35757;&#32451;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#31070;&#32463;&#20803;&#37325;&#35201;&#24615;&#32452;&#31232;&#30095;&#27491;&#21017;&#21270;&#22120;&#12290;&#19982;&#20256;&#32479;&#30340;&#27491;&#21017;&#21270;&#22120;&#19981;&#21516;&#65292;&#36825;&#20010;&#27491;&#21017;&#21270;&#22120;&#23545;&#20887;&#20313;&#21442;&#25968;&#26045;&#21152;&#20102;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) has shown remarkable success in complex autonomous driving scenarios. However, DRL models inevitably bring high memory consumption and computation, which hinders their wide deployment in resource-limited autonomous driving devices. Structured Pruning has been recognized as a useful method to compress and accelerate DRL models, but it is still challenging to estimate the contribution of a parameter (i.e., neuron) to DRL models. In this paper, we introduce a novel dynamic structured pruning approach that gradually removes a DRL model's unimportant neurons during the training stage. Our method consists of two steps, i.e. training DRL models with a group sparse regularizer and removing unimportant neurons with a dynamic pruning threshold. To efficiently train the DRL model with a small number of important neurons, we employ a neuron-importance group sparse regularizer. In contrast to conventional regularizers, this regularizer imposes a penalty on redundan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#31639;&#23376;&#30340;&#24378;&#30423;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#21033;&#29992;&#37096;&#20998;&#35757;&#32451;&#21644;&#20934;&#30830;&#24615;&#20316;&#20026;&#22870;&#21169;&#65292;&#26368;&#32456;&#30340;&#31639;&#27861;Mutant-UCB&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#22266;&#23450;&#39044;&#31639;&#19979;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.05144</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#31639;&#23376;&#30340;&#24378;&#30423;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
A Bandit Approach with Evolutionary Operators for Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#31639;&#23376;&#30340;&#24378;&#30423;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#21033;&#29992;&#37096;&#20998;&#35757;&#32451;&#21644;&#20934;&#30830;&#24615;&#20316;&#20026;&#22870;&#21169;&#65292;&#26368;&#32456;&#30340;&#31639;&#27861;Mutant-UCB&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#22266;&#23450;&#39044;&#31639;&#19979;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#27169;&#22411;&#26159;&#33218;&#65292;&#36873;&#25321;&#19968;&#20010;&#33218;&#23545;&#24212;&#37096;&#20998;&#35757;&#32451;&#27169;&#22411;&#65288;&#36164;&#28304;&#20998;&#37197;&#65289;&#12290;&#22870;&#21169;&#26159;&#36873;&#25321;&#27169;&#22411;&#22312;&#37096;&#20998;&#35757;&#32451;&#21518;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#20010;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#20013;&#65292;&#36951;&#25022;&#26159;&#26368;&#20248;&#27169;&#22411;&#30340;&#39044;&#26399;&#20934;&#30830;&#24615;&#19982;&#26368;&#32456;&#36873;&#25321;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;UCB-E&#22312;&#38543;&#26426;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#30340;&#30452;&#25509;&#25512;&#24191;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#22522;&#26412;&#20551;&#35774;&#19979;&#65292;&#26399;&#26395;&#36951;&#25022;&#30340;&#39034;&#24207;&#26159;$T^{-\alpha}$&#65292;&#20854;&#20013;$\alpha \in (0,1/5)$&#65292;$T$&#26159;&#35201;&#20998;&#37197;&#30340;&#36164;&#28304;&#25968;&#37327;&#12290;&#20174;&#36825;&#20010;&#22522;&#26412;&#31639;&#27861;&#20986;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;Mutant-UCB&#65292;&#23427;&#32467;&#21512;&#20102;&#36827;&#21270;&#31639;&#27861;&#30340;&#25805;&#20316;&#31526;&#12290;&#22312;&#19977;&#20010;&#24320;&#28304;&#22270;&#29255;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#27979;&#35797;&#34920;&#26126;&#20102;&#36825;&#31181;&#26032;&#39062;&#30340;&#32452;&#21512;&#26041;&#27861;&#30340;&#30456;&#20851;&#24615;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22266;&#23450;&#39044;&#31639;&#19979;&#30340;&#22269;&#38469;&#39046;&#20808;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper formulates model selection as an infinite-armed bandit problem. The models are arms, and picking an arm corresponds to a partial training of the model (resource allocation). The reward is the accuracy of the selected model after its partial training. In this best arm identification problem, regret is the gap between the expected accuracy of the optimal model and that of the model finally chosen. We first consider a straightforward generalization of UCB-E to the stochastic infinite-armed bandit problem and show that, under basic assumptions, the expected regret order is $T^{-\alpha}$ for some $\alpha \in (0,1/5)$ and $T$ the number of resources to allocate. From this vanilla algorithm, we introduce the algorithm Mutant-UCB that incorporates operators from evolutionary algorithms. Tests carried out on three open source image classification data sets attest to the relevance of this novel combining approach, which outperforms the state-of-the-art for a fixed budget.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35745;&#31639;&#31649;&#29702;&#65292;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#21040;&#29616;&#26377;&#24037;&#20316;&#27969;&#31243;&#20013;&#12290;&#23427;&#25552;&#20379;&#20102;&#19977;&#20010;&#31616;&#21333;&#30340;&#27493;&#39588;&#26469;&#24320;&#22987;&#23454;&#26045;&#20154;&#24037;&#26234;&#33021;&#65292;&#21253;&#25324;&#20219;&#21153;&#65288;&#37325;&#26032;&#65289;&#32452;&#32455;&#65292;&#35780;&#20272;&#33258;&#21160;&#21270;&#28508;&#21147;&#21644;&#23436;&#25104;&#20154;&#24037;&#26234;&#33021;&#36873;&#25321;&#21644;&#36866;&#24212;&#30340;&#20219;&#21153;&#35268;&#33539;&#27169;&#26495;&#12290;</title><link>https://arxiv.org/abs/2402.05142</link><description>&lt;p&gt;
&#35745;&#31639;&#31649;&#29702;&#30340;&#22522;&#30784;&#65306;&#31995;&#32479;&#26041;&#27861;&#29992;&#20110;&#23558;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#21040;&#29616;&#26377;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
The Foundations of Computational Management: A Systematic Approach to Task Automation for the Integration of Artificial Intelligence into Existing Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35745;&#31639;&#31649;&#29702;&#65292;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#21040;&#29616;&#26377;&#24037;&#20316;&#27969;&#31243;&#20013;&#12290;&#23427;&#25552;&#20379;&#20102;&#19977;&#20010;&#31616;&#21333;&#30340;&#27493;&#39588;&#26469;&#24320;&#22987;&#23454;&#26045;&#20154;&#24037;&#26234;&#33021;&#65292;&#21253;&#25324;&#20219;&#21153;&#65288;&#37325;&#26032;&#65289;&#32452;&#32455;&#65292;&#35780;&#20272;&#33258;&#21160;&#21270;&#28508;&#21147;&#21644;&#23436;&#25104;&#20154;&#24037;&#26234;&#33021;&#36873;&#25321;&#21644;&#36866;&#24212;&#30340;&#20219;&#21153;&#35268;&#33539;&#27169;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#23835;&#36215;&#25512;&#21160;&#19979;&#65292;&#32452;&#32455;&#38754;&#20020;&#30528;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22914;&#20309;&#25104;&#21151;&#22320;&#23558;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#21040;&#29616;&#26377;&#36816;&#33829;&#20013;&#65311;&#20026;&#20102;&#24110;&#21161;&#35299;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#31649;&#29702;&#39044;&#26399;&#24182;&#20943;&#36731;&#27822;&#20007;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#35745;&#31639;&#31649;&#29702;&#65292;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#32452;&#32455;&#22312;&#29616;&#26377;&#24037;&#20316;&#27969;&#31243;&#20013;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#28508;&#21147;&#30340;&#33021;&#21147;&#12290;&#35745;&#31639;&#31649;&#29702;&#20316;&#20026;&#31649;&#29702;&#31185;&#23398;&#30340;&#25112;&#30053;&#27934;&#23519;&#21644;&#35745;&#31639;&#24605;&#32500;&#30340;&#20998;&#26512;&#20005;&#35880;&#24615;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19977;&#20010;&#31616;&#21333;&#30340;&#36880;&#27493;&#31243;&#24207;&#65292;&#20197;&#24320;&#22987;&#22312;&#24037;&#20316;&#27969;&#31243;&#20013;&#23454;&#26045;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#31243;&#12290;&#36825;&#20123;&#31243;&#24207;&#20391;&#37325;&#20110;&#20219;&#21153;&#30340;&#65288;&#37325;&#26032;&#65289;&#32452;&#32455;&#65292;&#35780;&#20272;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#28508;&#21147;&#65292;&#23436;&#25104;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#36873;&#25321;&#21644;&#36866;&#24212;&#30340;&#20219;&#21153;&#35268;&#33539;&#27169;&#26495;&#12290;&#26412;&#25991;&#21253;&#21547;&#20102;&#25163;&#21160;&#21644;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#21457;&#34920;&#24314;&#35758;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by the rapid ascent of artificial intelligence (AI), organizations are at the epicenter of a seismic shift, facing a crucial question: How can AI be successfully integrated into existing operations? To help answer it, manage expectations and mitigate frustration, this article introduces Computational Management, a systematic approach to task automation for enhancing the ability of organizations to harness AI's potential within existing workflows. Computational Management acts as a bridge between the strategic insights of management science with the analytical rigor of computational thinking. The article offers three easy step-by-step procedures to begin the process of implementing AI within a workflow. Such procedures focus on task (re)formulation, on the assessment of the automation potential of tasks, on the completion of task specification templates for AI selection and adaptation. Included in the article there are manual and automated methods, with prompt suggestions for pub
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05140</link><description>&lt;p&gt;
Tag-LLM: &#23558;&#36890;&#29992;&#30340;LLM&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#20877;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#19987;&#38376;&#39046;&#22495;&#20013;&#65292;&#22914;&#29289;&#29702;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26410;&#20805;&#20998;&#28085;&#30422;&#30340;&#39046;&#22495;&#65292;&#23427;&#20204;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#36890;&#29992;LLMs&#37325;&#26032;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26377;&#25928;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#34987;&#21442;&#25968;&#21270;&#20026;&#36830;&#32493;&#21521;&#37327;&#24182;&#38468;&#21152;&#21040;LLMs&#30340;&#23884;&#20837;&#23618;&#65292;&#20197;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#26631;&#31614;&#65306;&#39046;&#22495;&#26631;&#31614;&#29992;&#20110;&#38480;&#23450;&#19987;&#19994;&#34920;&#31034;&#65288;&#20363;&#22914;&#21270;&#23398;&#24335;&#65289;&#24182;&#25552;&#20379;&#39046;&#22495;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65307;&#21151;&#33021;&#26631;&#31614;&#29992;&#20110;&#34920;&#31034;&#29305;&#23450;&#30340;&#21151;&#33021;&#65288;&#20363;&#22914;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#65289;&#24182;&#21387;&#32553;&#21151;&#33021;&#35299;&#20915;&#25351;&#20196;&#12290;&#25105;&#20204;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#39046;&#22495;&#30693;&#35782;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#36825;&#20123;&#26631;&#31614;&#30340;&#21327;&#35758;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from tas
&lt;/p&gt;</description></item><item><title>SceMQA&#26159;&#19968;&#31181;&#31185;&#23398;&#31867;&#22823;&#23398;&#20837;&#23398;&#32423;&#22810;&#27169;&#24577;&#38382;&#39064;&#22238;&#31572;&#30340;&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#20013;&#34987;&#24573;&#35270;&#30340;&#25945;&#32946;&#38454;&#27573;&#30340;&#31354;&#30333;&#12290;&#23427;&#21253;&#21547;&#26680;&#24515;&#31185;&#23398;&#31185;&#30446;&#65292;&#34701;&#21512;&#20102;&#22810;&#39033;&#36873;&#25321;&#21644;&#33258;&#30001;&#22238;&#31572;&#30340;&#26684;&#24335;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#38382;&#39064;&#35299;&#26512;&#21644;&#31572;&#26696;&#35299;&#37322;&#12290;&#35813;&#22522;&#20934;&#36824;&#36890;&#36807;&#30456;&#21516;&#32972;&#26223;&#20294;&#38382;&#39064;&#19981;&#21516;&#30340;&#26041;&#24335;&#65292;&#20419;&#36827;&#20102;&#23545;&#25512;&#29702;&#33021;&#21147;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.05138</link><description>&lt;p&gt;
SceMQA&#65306;&#19968;&#31181;&#31185;&#23398;&#31867;&#22823;&#23398;&#20837;&#23398;&#32423;&#22810;&#27169;&#24577;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05138
&lt;/p&gt;
&lt;p&gt;
SceMQA&#26159;&#19968;&#31181;&#31185;&#23398;&#31867;&#22823;&#23398;&#20837;&#23398;&#32423;&#22810;&#27169;&#24577;&#38382;&#39064;&#22238;&#31572;&#30340;&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#20013;&#34987;&#24573;&#35270;&#30340;&#25945;&#32946;&#38454;&#27573;&#30340;&#31354;&#30333;&#12290;&#23427;&#21253;&#21547;&#26680;&#24515;&#31185;&#23398;&#31185;&#30446;&#65292;&#34701;&#21512;&#20102;&#22810;&#39033;&#36873;&#25321;&#21644;&#33258;&#30001;&#22238;&#31572;&#30340;&#26684;&#24335;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#38382;&#39064;&#35299;&#26512;&#21644;&#31572;&#26696;&#35299;&#37322;&#12290;&#35813;&#22522;&#20934;&#36824;&#36890;&#36807;&#30456;&#21516;&#32972;&#26223;&#20294;&#38382;&#39064;&#19981;&#21516;&#30340;&#26041;&#24335;&#65292;&#20419;&#36827;&#20102;&#23545;&#25512;&#29702;&#33021;&#21147;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SceMQA&#65292;&#36825;&#26159;&#19968;&#31181;&#38754;&#21521;&#22823;&#23398;&#20837;&#23398;&#32423;&#31185;&#23398;&#31867;&#22810;&#27169;&#24577;&#38382;&#39064;&#22238;&#31572;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#23427;&#22635;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#20013;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#20851;&#38190;&#25945;&#32946;&#38454;&#27573;&#65292;&#28085;&#30422;&#20102;&#39640;&#20013;&#21040;&#22823;&#23398;&#39044;&#31185;&#30340;&#27700;&#24179;&#12290;SceMQA&#19987;&#27880;&#20110;&#26680;&#24515;&#31185;&#23398;&#31185;&#30446;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#29289;&#29702;&#23398;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#12290;&#23427;&#34701;&#21512;&#20102;&#22810;&#39033;&#36873;&#25321;&#21644;&#33258;&#30001;&#22238;&#31572;&#30340;&#26684;&#24335;&#65292;&#30830;&#20445;&#23545;AI&#27169;&#22411;&#30340;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#20026;&#27599;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#30693;&#35782;&#28857;&#21644;&#35814;&#32454;&#30340;&#31572;&#26696;&#35299;&#37322;&#12290;SceMQA&#36824;&#29420;&#29305;&#22320;&#25552;&#20379;&#20102;&#30456;&#21516;&#32972;&#26223;&#20294;&#38382;&#39064;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#20197;&#20419;&#36827;&#23545;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;&#30340;&#26368;&#26032;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24320;&#21457;&#31185;&#23398;&#31867;&#22823;&#23398;&#20837;&#23398;&#32423;&#22810;&#27169;&#24577;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper introduces SceMQA, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. SceMQA focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and free-response formats, ensuring a comprehensive evaluation of AI models' abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. SceMQA also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both open-source and close-source state-of-the-art Multimodal Large Language Models (MLLMs), across various experimental settings. The results show that further research and development are needed in developi
&lt;/p&gt;</description></item><item><title>CADReN&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#38170;&#28857;&#39537;&#21160;&#30340;&#20851;&#31995;&#32593;&#32476;&#65292;&#29992;&#20110;&#21487;&#25511;&#30340;&#36328;&#22270;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#38170;&#28857;&#26426;&#21046;&#65292;&#32771;&#34385;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#24320;&#28304;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;RIC200&#21644;WK1K&#12290;</title><link>https://arxiv.org/abs/2402.05135</link><description>&lt;p&gt;
CADReN: &#19978;&#19979;&#25991;&#38170;&#28857;&#39537;&#21160;&#30340;&#20851;&#31995;&#32593;&#32476;&#29992;&#20110;&#21487;&#25511;&#36328;&#22270;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CADReN: Contextual Anchor-Driven Relational Network for Controllable Cross-Graphs Node Importance Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05135
&lt;/p&gt;
&lt;p&gt;
CADReN&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#38170;&#28857;&#39537;&#21160;&#30340;&#20851;&#31995;&#32593;&#32476;&#65292;&#29992;&#20110;&#21487;&#25511;&#30340;&#36328;&#22270;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#38170;&#28857;&#26426;&#21046;&#65292;&#32771;&#34385;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#24320;&#28304;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;RIC200&#21644;WK1K&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;(NIE)&#23545;&#20110;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#23558;&#22806;&#37096;&#20449;&#24687;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#20391;&#37325;&#20110;&#38745;&#24577;&#30340;&#21333;&#19968;&#22270;&#29305;&#24449;&#65292;&#22312;&#26032;&#22270;&#21644;&#29992;&#25143;&#29305;&#23450;&#35201;&#27714;&#26041;&#38754;&#32570;&#20047;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;CADReN&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#38170;&#28857;(CA)&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#20351;&#32593;&#32476;&#33021;&#22815;&#30456;&#23545;&#20110;CA&#35780;&#20272;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#65292;&#32771;&#34385;&#30693;&#35782;&#22270;&#35889;(KGs)&#20013;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CADReN&#22312;&#36328;&#22270;NIE&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#12290;CADReN&#36824;&#34987;&#35777;&#26126;&#22312;&#21333;&#19968;&#22270;NIE&#20219;&#21153;&#19978;&#19982;&#20197;&#21069;&#30340;&#27169;&#22411;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#24182;&#24320;&#28304;&#20102;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;RIC200&#21644;WK1K&#65292;&#19987;&#38376;&#29992;&#20110;&#36328;&#22270;NIE&#30740;&#31350;&#65292;&#20026;&#36825;&#20010;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node Importance Estimation (NIE) is crucial for integrating external information into Large Language Models through Retriever-Augmented Generation. Traditional methods, focusing on static, single-graph characteristics, lack adaptability to new graphs and user-specific requirements. CADReN, our proposed method, addresses these limitations by introducing a Contextual Anchor (CA) mechanism. This approach enables the network to assess node importance relative to the CA, considering both structural and semantic features within Knowledge Graphs (KGs). Extensive experiments show that CADReN achieves better performance in cross-graph NIE task, with zero-shot prediction ability. CADReN is also proven to match the performance of previous models on single-graph NIE task. Additionally, we introduce and opensource two new datasets, RIC200 and WK1K, specifically designed for cross-graph NIE research, providing a valuable resource for future developments in this domain.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05133</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Personalized Language Modeling from Personalized Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#30446;&#21069;&#20027;&#27969;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#24320;&#21457;&#30340;&#31639;&#27861;&#30340;&#22522;&#26412;&#21069;&#25552;&#22312;&#29992;&#25143;&#20559;&#22909;&#22810;&#26679;&#21270;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#20986;&#29616;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#20171;&#32461;&#20102;&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#20219;&#21153;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26222;&#36890;&#30340;RLHF&#21487;&#33021;&#20250;&#23384;&#22312;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20010;&#24615;&#21270;-RLHF&#65288;P-RLHF&#65289;&#26694;&#26550;&#65292;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#27169;&#22411;&#21644;&#35821;&#35328;&#65288;&#25110;&#22870;&#21169;&#65289;&#27169;&#22411;&#12290;&#29992;&#25143;&#27169;&#22411;&#25509;&#25910;&#29992;&#25143;&#20449;&#24687;&#24182;&#36755;&#20986;&#29992;&#25143;&#34920;&#31034;&#12290;&#20854;&#32467;&#26500;&#32534;&#30721;&#20102;&#25105;&#20204;&#23545;&#21453;&#39304;&#25968;&#25454;&#20013;&#29992;&#25143;&#20559;&#22909;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20026;&#20010;&#24615;&#21270;&#22870;&#21169;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#24320;&#21457;&#20102;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is the current dominating framework to fine-tune large language models to better align with human preferences. However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse. In this work, we aim to address this problem by developing methods for building personalized language models. We first formally introduce the task of learning from personalized human feedback and explain why vanilla RLHF can be problematic in this context. We then propose a general Personalized-RLHF (P-RLHF) framework, which requires one to jointly learn a user model and a language (or reward) model. The user model takes in user information and outputs user representations. Its structure encodes our assumptions about user preferences underlying the feedback data. We develop new learning objectives for personalized reward modeling and personalized Direct Preference Optimizat
&lt;/p&gt;</description></item><item><title>LB-KBQA&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;BERT&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24110;&#21161;&#65292;&#33021;&#22815;&#25552;&#39640;&#24847;&#22270;&#35782;&#21035;&#30340;&#24615;&#33021;&#21644;&#35299;&#20915;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05130</link><description>&lt;p&gt;
LB-KBQA: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;BERT&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05130
&lt;/p&gt;
&lt;p&gt;
LB-KBQA&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;BERT&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24110;&#21161;&#65292;&#33021;&#22815;&#25552;&#39640;&#24847;&#22270;&#35782;&#21035;&#30340;&#24615;&#33021;&#21644;&#35299;&#20915;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22240;&#20854;&#26032;&#20852;&#30340;&#33021;&#21147;&#32780;&#36171;&#20104;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#21147;&#37327;&#65292;&#20854;&#20013;&#19968;&#20010;&#20856;&#22411;&#30340;&#24212;&#29992;&#39046;&#22495;&#26159;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;AI&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#29983;&#25104;&#24335;AI&#30340;&#20856;&#22411;&#24212;&#29992;&#39046;&#22495;&#20043;&#19968;&#26159;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#19988;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#65292;LLM&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;&#24847;&#22270;&#35782;&#21035;&#24615;&#33021;&#30340;&#38556;&#30861;&#65292;&#36825;&#28304;&#33258;&#35821;&#35328;&#22810;&#26679;&#24615;&#21644;&#26032;&#20986;&#29616;&#30340;&#24847;&#22270;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;AI&#30340;&#24847;&#22270;&#35782;&#21035;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#22312;&#24847;&#22270;&#35782;&#21035;&#26041;&#38754;&#21463;&#21040;&#26377;&#38480;&#30340;&#36164;&#28304;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;BERT&#30340;&#26032;&#22411;KBQA&#31995;&#32479;&#65288;LB-KBQA&#65289;&#12290;&#22312;&#29983;&#25104;&#24335;AI&#30340;&#24110;&#21161;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#21040;&#8230;&#8230;&#65288;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI), because of its emergent abilities, has empowered various fields, one typical of which is large language models (LLMs). One of the typical application fields of Generative AI is large language models (LLMs), and the natural language understanding capability of LLM is dramatically improved when compared with conventional AI-based methods. The natural language understanding capability has always been a barrier to the intent recognition performance of the Knowledge-Based-Question-and-Answer (KBQA) system, which arises from linguistic diversity and the newly appeared intent. Conventional AI-based methods for intent recognition can be divided into semantic parsing-based and model-based approaches. However, both of the methods suffer from limited resources in intent recognition. To address this issue, we propose a novel KBQA system based on a Large Language Model(LLM) and BERT (LB-KBQA). With the help of generative AI, our proposed method could detect 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#21644;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#20026;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.05128</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25552;&#21319;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#21644;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#20026;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#31185;&#20070;&#38382;&#31572;&#65288;TQA&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#19978;&#19979;&#25991;&#21644;&#22810;&#27169;&#24335;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#22312;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#21644;&#26080;&#27861;&#25429;&#25417;&#38271;&#25991;&#26412;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24341;&#20837;&#38761;&#21629;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;LLMs&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;TQA&#20013;&#39046;&#22495;&#22806;&#24773;&#26223;&#65292;&#21363;&#27010;&#24565;&#20998;&#24067;&#22312;&#19981;&#21516;&#35838;&#31243;&#20013;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#24182;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;LLM&#27169;&#22411;Llama-2&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#24182;&#21152;&#20837;RAG&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#20248;&#20110;&#22522;&#32447;&#65292;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;4.12%&#65292;&#22312;&#27979;&#35797;&#38598;&#19978;&#25552;&#39640;&#20102;9.84%&#12290;
&lt;/p&gt;
&lt;p&gt;
Textbook question answering (TQA) is a challenging task in artificial intelligence due to the complex nature of context and multimodal data. Although previous research has significantly improved the task, there are still some limitations including the models' weak reasoning and inability to capture contextual information in the lengthy context. The introduction of large language models (LLMs) has revolutionized the field of AI, however, directly applying LLMs often leads to inaccurate answers. This paper proposes a methodology that handle the out-of-domain scenario in TQA where concepts are spread across different lessons by incorporating the retrieval augmented generation (RAG) technique and utilize transfer learning to handle the long context and enhance reasoning abilities. Through supervised fine-tuning of the LLM model Llama-2 and the incorporation of RAG, our architecture outperforms the baseline, achieving a 4.12% accuracy improvement on validation set and 9.84% on test set for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#21644;&#27835;&#30103;&#33539;&#24335;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32463;&#36807;&#29305;&#23450;&#25552;&#31034;&#24494;&#35843;&#20197;&#35786;&#26029;&#12289;&#35299;&#37322;&#21644;&#24314;&#35758;&#27835;&#30103;&#24178;&#39044;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#25968;&#25454;&#24211;&#65292;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#24314;&#35758;&#12290;&#27492;&#26041;&#27861;&#19982;&#24739;&#32773;&#36827;&#34892;&#20849;&#24773;&#23545;&#35805;&#31649;&#29702;&#65292;&#26377;&#25928;&#25903;&#25345;&#25233;&#37057;&#30151;&#24739;&#32773;&#12290;</title><link>https://arxiv.org/abs/2402.05127</link><description>&lt;p&gt;
Illuminate&#65306;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#20998;&#26512;&#21644;&#31215;&#26497;&#27835;&#30103;&#30340;&#26032;&#26041;&#27861;&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#21644;&#27835;&#30103;&#33539;&#24335;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32463;&#36807;&#29305;&#23450;&#25552;&#31034;&#24494;&#35843;&#20197;&#35786;&#26029;&#12289;&#35299;&#37322;&#21644;&#24314;&#35758;&#27835;&#30103;&#24178;&#39044;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#25968;&#25454;&#24211;&#65292;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#24314;&#35758;&#12290;&#27492;&#26041;&#27861;&#19982;&#24739;&#32773;&#36827;&#34892;&#20849;&#24773;&#23545;&#35805;&#31649;&#29702;&#65292;&#26377;&#25928;&#25903;&#25345;&#25233;&#37057;&#30151;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65306;Generative Pre-trained Transformer 4&#65288;GPT-4&#65289;&#12289;Llama 2 chat&#21644;Gemini&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#21644;&#27835;&#30103;&#26032;&#33539;&#24335;&#12290;&#36825;&#20123;LLMs&#36890;&#36807;&#29305;&#23450;&#30340;&#25552;&#31034;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#35786;&#26029;&#12289;&#35299;&#37322;&#21644;&#24314;&#35758;&#25233;&#37057;&#30151;&#30340;&#27835;&#30103;&#24178;&#39044;&#12290;&#19968;&#31181;&#29420;&#29305;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#26041;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#26681;&#25454;DSM-5&#26631;&#20934;&#20998;&#26512;&#21644;&#35299;&#37322;&#25233;&#37057;&#30151;&#29366;&#30340;&#33021;&#21147;&#12290;&#22312;&#20132;&#20114;&#38454;&#27573;&#65292;&#27169;&#22411;&#37319;&#29992;&#20849;&#24773;&#23545;&#35805;&#31649;&#29702;&#65292;&#21033;&#29992;PsychDB&#21644;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#65288;CBT&#65289;&#25351;&#21335;&#31561;&#36164;&#28304;&#65292;&#19982;&#24739;&#26377;&#37325;&#24230;&#25233;&#37057;&#30151;&#30340;&#20010;&#20307;&#36827;&#34892;&#25903;&#25345;&#24615;&#20114;&#21160;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#20171;&#32461;&#20102;Illuminate&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;CBT&#27169;&#22359;&#65292;&#21487;&#24110;&#21161;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#24314;&#35758;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;F1&#20998;&#25968;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#21484;&#22238;&#29575;&#23548;&#21521;&#30340;&#24046;&#38169;&#36827;&#34892;LLM&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel paradigm for depression detection and treatment using advanced Large Language Models (LLMs): Generative Pre-trained Transformer 4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized prompts to diagnose, explain, and suggest therapeutic interventions for depression. A unique few-shot prompting method enhances the models' ability to analyze and explain depressive symptoms based on the DSM-5 criteria. In the interaction phase, the models engage in empathetic dialogue management, drawing from resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide, fostering supportive interactions with individuals experiencing major depressive disorders. Additionally, the research introduces the Illuminate Database, enriched with various CBT modules, aiding in personalized therapy recommendations. The study evaluates LLM performance using metrics such as F1 scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;LLMs&#24320;&#21457;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20837;&#36873;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#27969;&#31243;&#25552;&#39640;&#20102;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05125</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#19982;LLMs
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Clinical Trial Patient Matching with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;LLMs&#24320;&#21457;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20837;&#36873;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#27969;&#31243;&#25552;&#39640;&#20102;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#26159;&#25512;&#20986;&#26032;&#33647;&#30340;&#20851;&#38190;&#38590;&#39064;&#12290;&#30446;&#21069;&#65292;&#35782;&#21035;&#31526;&#21512;&#35797;&#39564;&#20837;&#36873;&#26631;&#20934;&#30340;&#24739;&#32773;&#26159;&#39640;&#24230;&#25163;&#21160;&#30340;&#65292;&#27599;&#20301;&#24739;&#32773;&#38656;&#33457;&#36153;&#38271;&#36798;1&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#31579;&#36873;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#29702;&#35299;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23427;&#20204;&#22312;&#35797;&#39564;&#21305;&#37197;&#20013;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#19968;&#20010;&#24739;&#32773;&#30340;&#30149;&#21490;&#20316;&#20026;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#26102;&#65292;&#35780;&#20272;&#35813;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#19968;&#32452;&#21253;&#21547;&#26631;&#20934;&#65288;&#20063;&#20197;&#33258;&#30001;&#25991;&#26412;&#24418;&#24335;&#25351;&#23450;&#65289;&#12290;&#25105;&#20204;&#30340;&#38646;&#26679;&#26412;&#31995;&#32479;&#22312;n2c2 2018&#38431;&#21015;&#36873;&#25321;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#19968;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#65292;&#35813;&#31574;&#30053;&#19982;&#29616;&#29366;&#30456;&#27604;&#21487;&#20197;&#23558;&#24739;&#32773;&#21305;&#37197;&#26102;&#38388;&#21644;&#25104;&#26412;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26816;&#32034;&#27969;&#31243;&#65292;&#20943;&#23569;&#20102;&#21305;&#37197;&#28040;&#38500;&#30340;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matching patients to clinical trials is a key unsolved challenge in bringing new drugs to market. Today, identifying patients who meet a trial's eligibility criteria is highly manual, taking up to 1 hour per patient. Automated screening is challenging, however, as it requires understanding unstructured clinical text. Large language models (LLMs) offer a promising solution. In this work, we explore their application to trial matching. First, we design an LLM-based system which, given a patient's medical history as unstructured clinical text, evaluates whether that patient meets a set of inclusion criteria (also specified as free text). Our zero-shot system achieves state-of-the-art scores on the n2c2 2018 cohort selection benchmark. Second, we improve the data and cost efficiency of our method by identifying a prompting strategy which matches patients an order of magnitude faster and more cheaply than the status quo, and develop a two-stage retrieval pipeline that reduces the number of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#21644;&#20107;&#23454;&#39564;&#35777;&#65292;&#20197;&#21450;&#26032;&#20852;&#30340;&#34920;&#26684;&#25805;&#20316;&#21644;&#39640;&#32423;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#12290;&#36824;&#35752;&#35770;&#20102;LLMs&#30340;&#26368;&#26032;&#33539;&#20363;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#25351;&#23548;&#35843;&#25972;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05121</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Table Processing: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#21644;&#20107;&#23454;&#39564;&#35777;&#65292;&#20197;&#21450;&#26032;&#20852;&#30340;&#34920;&#26684;&#25805;&#20316;&#21644;&#39640;&#32423;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#12290;&#36824;&#35752;&#35770;&#20102;LLMs&#30340;&#26368;&#26032;&#33539;&#20363;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#25351;&#23548;&#35843;&#25972;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#36890;&#24120;&#26159;&#20108;&#32500;&#32467;&#26500;&#21270;&#30340;&#65292;&#29992;&#20110;&#23384;&#20648;&#22823;&#37327;&#25968;&#25454;&#65292;&#22312;&#25968;&#25454;&#24211;&#26597;&#35810;&#12289;&#30005;&#23376;&#34920;&#26684;&#35745;&#31639;&#21644;&#20174;&#32593;&#32476;&#34920;&#26684;&#29983;&#25104;&#25253;&#21578;&#31561;&#26085;&#24120;&#27963;&#21160;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#36825;&#20123;&#20197;&#34920;&#26684;&#20026;&#20013;&#24515;&#30340;&#20219;&#21153;&#21487;&#20197;&#24102;&#26469;&#37325;&#22823;&#30340;&#20844;&#20247;&#21033;&#30410;&#65292;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#20852;&#36259;&#12290;&#35813;&#35843;&#26597;&#23545;&#34920;&#26684;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27010;&#36848;&#65292;&#19981;&#20165;&#28085;&#30422;&#20256;&#32479;&#39046;&#22495;&#22914;&#34920;&#26684;&#38382;&#39064;&#22238;&#31572;&#65288;Table QA&#65289;&#21644;&#20107;&#23454;&#39564;&#35777;&#65292;&#36824;&#21253;&#25324;&#26368;&#36817;&#24378;&#35843;&#30340;&#26041;&#38754;&#65292;&#22914;&#34920;&#26684;&#25805;&#20316;&#21644;&#39640;&#32423;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#36229;&#36234;&#20102;&#26089;&#26399;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;LLM&#20351;&#29992;&#20013;&#30340;&#26368;&#26032;&#33539;&#20363;&#12290;&#37325;&#28857;&#26159;LLMs&#39046;&#22495;&#20869;&#30340;&#25351;&#23548;&#35843;&#25972;&#12289;&#25552;&#31034;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#20960;&#20010;&#25361;&#25112;&#65292;&#28085;&#30422;&#31169;&#26377;&#37096;&#32626;&#12289;&#39640;&#25928;&#25512;&#26029;&#21644; LLMS &#21457;&#23637;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet calculations, and generating reports from web tables. Automating these table-centric tasks with Large Language Models (LLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides an extensive overview of table tasks, encompassing not only the traditional areas like table question answering (Table QA) and fact verification, but also newly emphasized aspects such as table manipulation and advanced table data analysis. Additionally, it goes beyond the early strategies of pre-training and fine-tuning small language models, to include recent paradigms in LLM usage. The focus here is particularly on instruction-tuning, prompting, and agent-based approaches within the realm of LLMs. Finally, we highlight several challenges, ranging from private deployment and efficient inference to the developmen
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#37319;&#26679;&#21644;&#25237;&#31080;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#27491;&#20132;&#12290;</title><link>https://arxiv.org/abs/2402.05120</link><description>&lt;p&gt;
&#26356;&#22810;&#30340;&#20195;&#29702;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
More Agents Is All You Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05120
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#37319;&#26679;&#21644;&#25237;&#31080;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#27491;&#20132;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#36890;&#36807;&#19968;&#31181;&#37319;&#26679;&#21644;&#25237;&#31080;&#30340;&#26041;&#27861;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#30340;&#24615;&#33021;&#19982;&#23454;&#20363;&#21270;&#30340;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#24050;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#26159;&#27491;&#20132;&#30340;&#65292;&#32780;&#22686;&#24378;&#30340;&#31243;&#24230;&#19982;&#20219;&#21153;&#30340;&#22256;&#38590;&#31243;&#24230;&#30456;&#20851;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#33021;&#22815;&#20419;&#36827;&#20854;&#21457;&#29983;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#22312;&#20197;&#19979;&#32593;&#22336;: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}
&lt;/p&gt;
&lt;p&gt;
We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.05119</link><description>&lt;p&gt;
&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Limitations of Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#65288;IT&#65289;&#26159;&#20351;&#29992;&#25351;&#20196;-&#22238;&#24212;&#23545;&#26469;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36807;&#31243;&#65292;&#24050;&#25104;&#20026;&#23558;&#22522;&#30784;&#39044;&#35757;&#32451;LLM&#36716;&#21270;&#20026;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#34429;&#28982;IT&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#24182;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#23616;&#38480;&#24615;&#21644;&#19981;&#36275;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#23545;LLM&#36890;&#36807;IT&#21457;&#29983;&#30340;&#21464;&#21270;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;IT&#30340;&#22810;&#31181;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;IT&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#25110;&#25216;&#33021;&#12290;LoRA&#24494;&#35843;&#20165;&#38480;&#20110;&#23398;&#20064;&#22238;&#24212;&#30340;&#21551;&#21160;&#21644;&#26679;&#24335;&#20196;&#29260;&#65292;&#32780;&#20840;&#21442;&#25968;&#24494;&#35843;&#20250;&#23548;&#33268;&#30693;&#35782;&#36864;&#21270;&#12290;&#65288;2&#65289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;IT&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#20250;&#23548;&#33268;&#22238;&#24212;&#36136;&#37327;&#19979;&#38477;&#12290;&#65288;3&#65289;&#20840;&#21442;&#25968;&#24494;&#35843;&#36890;&#36807;&#19981;&#20934;&#30830;&#22320;&#20174;IT&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#27010;&#24565;&#19978;&#30456;&#20284;&#23454;&#20363;&#30340;&#26631;&#35760;&#65292;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#36816;&#21160;&#37325;&#23450;&#20301;&#29992;&#20110;&#20154;&#26426;&#20223;&#30495;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#39046;&#22495;&#21040;&#39046;&#22495;&#30340;&#36716;&#25442;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26080;&#23545;&#24212;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20154;&#26426;&#20223;&#30495;&#12290;</title><link>https://arxiv.org/abs/2402.05115</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#36816;&#21160;&#37325;&#23450;&#20301;&#29992;&#20110;&#20154;&#26426;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Motion Retargeting for Human-Robot Imitation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05115
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#36816;&#21160;&#37325;&#23450;&#20301;&#29992;&#20110;&#20154;&#26426;&#20223;&#30495;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#39046;&#22495;&#21040;&#39046;&#22495;&#30340;&#36716;&#25442;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26080;&#23545;&#24212;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20154;&#26426;&#20223;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#26089;&#26399;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23558;&#20154;&#31867;&#21160;&#20316;&#39046;&#22495;&#30340;&#20851;&#33410;&#20301;&#32622;&#24207;&#21015;&#36716;&#21270;&#20026;&#29305;&#23450;&#26426;&#22120;&#20154;&#21487;&#23454;&#29616;&#30340;&#21160;&#20316;&#39046;&#22495;&#65292;&#20174;&#32780;&#25913;&#36827;&#22312;&#32447;&#20154;&#26426;&#20223;&#30495;&#12290;&#20511;&#21161;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#39046;&#22495;&#21040;&#39046;&#22495;&#30340;&#36716;&#25442;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#23545;&#24212;&#30340;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21160;&#20316;&#23545;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#36825;&#26679;&#30340;&#37197;&#23545;&#25968;&#25454;&#38750;&#24120;&#31232;&#32570;&#19988;&#25910;&#38598;&#36215;&#26469;&#32321;&#29712;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36716;&#21521;&#36866;&#24212;&#20110;&#26080;&#37197;&#23545;&#39046;&#22495;&#21040;&#39046;&#22495;&#36716;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#36827;&#34892;&#20154;&#26426;&#20223;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This early-stage research work aims to improve online human-robot imitation by translating sequences of joint positions from the domain of human motions to a domain of motions achievable by a given robot, thus constrained by its embodiment. Leveraging the generalization capabilities of deep learning methods, we address this problem by proposing an encoder-decoder neural network model performing domain-to-domain translation. In order to train such a model, one could use pairs of associated robot and human motions. Though, such paired data is extremely rare in practice, and tedious to collect. Therefore, we turn towards deep learning methods for unpaired domain-to-domain translation, that we adapt in order to perform human-robot imitation.
&lt;/p&gt;</description></item><item><title>SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2402.05044</link><description>&lt;p&gt;
SALAD-Bench: &#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#27425;&#21270;&#21644;&#20840;&#38754;&#24615;&#23433;&#20840;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05044
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#24378;&#22823;&#30340;&#23433;&#20840;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#23433;&#20840;&#22522;&#20934;&#65292;&#31216;&#20026;SALAD-Bench&#12290;SALAD-Bench&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#22810;&#26679;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#36328;&#19977;&#20010;&#23618;&#27425;&#30340;&#32454;&#33268;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#22522;&#20934;&#12290;SALAD-Bench&#36890;&#36807;&#23545;&#26631;&#20934;&#26597;&#35810;&#21644;&#22797;&#26434;&#26597;&#35810;&#65288;&#21253;&#25324;&#25915;&#20987;&#12289;&#38450;&#24481;&#20462;&#25913;&#21644;&#22810;&#39033;&#36873;&#25321;&#65289;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#26377;&#25928;&#31649;&#29702;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#26080;&#32541;&#21487;&#38752;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#22120;&#65306;&#22522;&#20110;LLM&#30340;MD-Judge&#65292;&#19987;&#27880;&#20110;&#25915;&#20987;&#22686;&#24378;&#26597;&#35810;&#30340;&#38382;&#31572;&#23545;&#35780;&#20272;&#12290;&#20197;&#19978;&#32452;&#20214;&#23558;SALAD-Bench&#20174;&#26631;&#20934;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#25193;&#23637;&#21040;&#20102;LLM&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#35780;&#20272;&#65292;&#30830;&#20445;&#20102;&#32852;&#21512;&#30446;&#26631;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#22810;&#20010;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#21457;&#20449;&#32773;&#21521;&#21333;&#20010;&#33258;&#31169;&#34892;&#20026;&#32773;&#20256;&#36882;&#20449;&#21495;&#20197;&#24433;&#21709;&#20854;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36229;&#36234;&#24050;&#26377;&#26041;&#27861;&#30340;&#23616;&#37096;&#22343;&#34913;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.04971</link><description>&lt;p&gt;
&#22810;&#21457;&#20449;&#32773;&#35828;&#26381; - &#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
Multi-Sender Persuasion -- A Computational Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04971
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#22810;&#20010;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#21457;&#20449;&#32773;&#21521;&#21333;&#20010;&#33258;&#31169;&#34892;&#20026;&#32773;&#20256;&#36882;&#20449;&#21495;&#20197;&#24433;&#21709;&#20854;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36229;&#36234;&#24050;&#26377;&#26041;&#27861;&#30340;&#23616;&#37096;&#22343;&#34913;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#21040;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#22810;&#20010;&#21457;&#20449;&#32773;&#21521;&#21333;&#20010;&#33258;&#31169;&#34892;&#20026;&#32773;&#20256;&#36882;&#20449;&#21495;&#20197;&#20351;&#20854;&#37319;&#21462;&#26576;&#20123;&#34892;&#21160;&#12290;&#36825;&#20123;&#35774;&#32622;&#26159;&#35745;&#31639;&#32463;&#27982;&#23398;&#65292;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#21644;&#20855;&#26377;&#22810;&#20010;&#30446;&#26631;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#26680;&#24515;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#26159;&#21457;&#20449;&#32773;&#20449;&#21495;&#31574;&#30053;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#19968;&#33324;&#24773;&#20917;&#19979;&#25214;&#21040;&#19968;&#20010;&#22343;&#34913;&#26159;PPAD-Hard&#30340;;&#23454;&#38469;&#19978;&#65292;&#35745;&#31639;&#19968;&#20010;&#21457;&#20449;&#32773;&#30340;&#26368;&#20339;&#21709;&#24212;&#29978;&#33267;&#26159;NP-Hard&#30340;&#12290;&#37492;&#20110;&#36825;&#20123;&#22266;&#26377;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#36716;&#32780;&#23547;&#25214;&#23616;&#37096;&#32435;&#20160;&#22343;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#35813;&#28216;&#25103;&#30340;&#38750;&#32447;&#24615;&#21644;&#19981;&#36830;&#32493;&#25928;&#29992;&#12290;&#32467;&#21512;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36229;&#36234;&#23436;&#20840;&#23637;&#31034;&#22343;&#34913;&#21644;&#29616;&#26377;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#30340;&#23616;&#37096;&#22343;&#34913;&#12290;&#24191;&#20041;&#19978;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#36129;&#29486;&#23545;&#24191;&#27867;&#30340;&#31867;&#21035;&#24863;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider multiple senders with informational advantage signaling to convince a single self-interested actor towards certain actions. Generalizing the seminal Bayesian Persuasion framework, such settings are ubiquitous in computational economics, multi-agent learning, and machine learning with multiple objectives. The core solution concept here is the Nash equilibrium of senders' signaling policies. Theoretically, we prove that finding an equilibrium in general is PPAD-Hard; in fact, even computing a sender's best response is NP-Hard. Given these intrinsic difficulties, we turn to finding local Nash equilibria. We propose a novel differentiable neural network to approximate this game's non-linear and discontinuous utilities. Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria and those found by existing neural networks. Broadly, our theoretical and empirical contributions are of interest to a large class 
&lt;/p&gt;</description></item><item><title>S-Agents&#26159;&#19968;&#20010;&#33258;&#32452;&#32455;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#20195;&#29702;&#26641;&#32467;&#26500;&#12289;&#27801;&#28431;&#20195;&#29702;&#26550;&#26500;&#21644;&#38750;&#38459;&#22622;&#21327;&#20316;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#39640;&#25928;&#21327;&#35843;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#21327;&#20316;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.04578</link><description>&lt;p&gt;
S-Agents: &#33258;&#32452;&#32455;&#20195;&#29702;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
S-Agents: self-organizing agents in open-ended environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04578
&lt;/p&gt;
&lt;p&gt;
S-Agents&#26159;&#19968;&#20010;&#33258;&#32452;&#32455;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#20195;&#29702;&#26641;&#32467;&#26500;&#12289;&#27801;&#28431;&#20195;&#29702;&#26550;&#26500;&#21644;&#38750;&#38459;&#22622;&#21327;&#20316;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#39640;&#25928;&#21327;&#35843;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#21327;&#20316;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#33258;&#20027;&#20195;&#29702;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#65292;&#20855;&#22791;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#20248;&#21270;&#21327;&#20316;&#38656;&#35201;&#28789;&#27963;&#30340;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;&#22266;&#23450;&#30340;&#12289;&#20219;&#21153;&#23548;&#21521;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#24573;&#35270;&#20102;&#20197;&#20195;&#29702;&#20026;&#20013;&#24515;&#30340;&#32452;&#32455;&#32467;&#26500;&#12290;&#21463;&#20154;&#31867;&#32452;&#32455;&#34892;&#20026;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#32452;&#32455;&#20195;&#29702;&#31995;&#32479;&#65288;S-Agents&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#21160;&#24577;&#24037;&#20316;&#27969;&#31243;&#30340;&#8220;&#20195;&#29702;&#26641;&#8221;&#32467;&#26500;&#12289;&#24179;&#34913;&#20449;&#24687;&#20248;&#20808;&#32423;&#30340;&#8220;&#27801;&#28431;&#20195;&#29702;&#26550;&#26500;&#8221;&#20197;&#21450;&#20801;&#35768;&#20195;&#29702;&#20043;&#38388;&#24322;&#27493;&#25191;&#34892;&#20219;&#21153;&#30340;&#8220;&#38750;&#38459;&#22622;&#21327;&#20316;&#8221;&#26041;&#27861;&#12290;&#36825;&#31181;&#32467;&#26500;&#21487;&#20197;&#33258;&#20027;&#21327;&#35843;&#19968;&#32452;&#20195;&#29702;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#26080;&#38480;&#19988;&#21160;&#24577;&#30340;&#29615;&#22659;&#25361;&#25112;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;S-Agents&#33021;&#22815;&#29087;&#32451;&#22320;&#25191;&#34892;&#21327;&#20316;&#24314;&#31569;&#20219;&#21153;&#21644;&#36164;&#28304;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging large language models (LLMs), autonomous agents have significantly improved, gaining the ability to handle a variety of tasks. In open-ended settings, optimizing collaboration for efficiency and effectiveness demands flexible adjustments. Despite this, current research mainly emphasizes fixed, task-oriented workflows and overlooks agent-centric organizational structures. Drawing inspiration from human organizational behavior, we introduce a self-organizing agent system (S-Agents) with a "tree of agents" structure for dynamic workflow, an "hourglass agent architecture" for balancing information priorities, and a "non-obstructive collaboration" method to allow asynchronous task execution among agents. This structure can autonomously coordinate a group of agents, efficiently addressing the challenges of an open and dynamic environment without human intervention. Our experiments demonstrate that S-Agents proficiently execute collaborative building tasks and resource collection i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)</title><link>https://arxiv.org/abs/2402.04154</link><description>&lt;p&gt;
&#12298;&#35835;&#29609;&#28216;&#25103;&#65288;R2-Play&#65289;: &#22810;&#27169;&#24577;&#28216;&#25103;&#25351;&#23548;&#19979;&#30340;&#20915;&#31574; Transformer&#12299;
&lt;/p&gt;
&lt;p&gt;
Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24320;&#21457;&#19968;&#27454;&#36890;&#29992;&#26234;&#33021;&#20307;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30446;&#26631;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#20219;&#21153;&#30340;&#22823;&#37327;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#25193;&#23637;&#21040;&#26032;&#20219;&#21153;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#25972;&#21512;&#21040;&#20915;&#31574;&#32593;&#32476;&#20013;&#65292;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#20165;&#20381;&#36182;&#20110;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#23545;&#20110;&#20934;&#30830;&#20256;&#36798;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#22686;&#24378;&#26234;&#33021;&#20307;&#20219;&#21153;&#25351;&#23548;&#30340;&#24418;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#65292;&#20174;&#32780;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#38271;&#26399;&#35270;&#35273;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;
Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#22312;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#30340;&#21516;&#26102;&#23454;&#29616;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#24341;&#23548;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;LSTS&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.03678</link><description>&lt;p&gt;
&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#22312;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#30340;&#21516;&#26102;&#23454;&#29616;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#24341;&#23548;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;LSTS&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#20351;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#23398;&#20064;&#22810;&#26679;&#21270;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#29615;&#22659;&#20132;&#20114;&#12290;&#20026;&#20102;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#65292;&#22914;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL$_f$&#65289;&#20844;&#24335;&#25110;&#22870;&#21169;&#26426;&#22120;&#65288;RM&#65289;&#65292;&#26469;&#25351;&#23548;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;LSTS&#19981;&#20551;&#35774;&#29615;&#22659;&#21160;&#21147;&#23398;&#25110;&#22870;&#21169;&#26426;&#22120;&#30340;&#20449;&#24687;&#65292;&#24182;&#21160;&#24577;&#37319;&#26679;&#23548;&#33268;&#25104;&#21151;&#30446;&#26631;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#19978;&#35780;&#20272;&#20102;LSTS&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has made significant strides in enabling artificial agents to learn diverse behaviors. However, learning an effective policy often requires a large number of environment interactions. To mitigate sample complexity issues, recent approaches have used high-level task specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward Machines (RM), to guide the learning progress of the agent. In this work, we propose a novel approach, called Logical Specifications-guided Dynamic Task Sampling (LSTS), that learns a set of RL policies to guide an agent from an initial state to a goal state based on a high-level task specification, while minimizing the number of environmental interactions. Unlike previous work, LSTS does not assume information about the environment dynamics or the Reward Machine, and dynamically samples promising tasks that lead to successful goal policies. We evaluate LSTS on a gridworld and show that it achieves improved time-to-threshol
&lt;/p&gt;</description></item><item><title>IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.03227</link><description>&lt;p&gt;
IGUANe: &#19968;&#31181;&#36866;&#29992;&#20110;&#33041;MR&#22270;&#20687;&#22810;&#20013;&#24515;&#21327;&#35843;&#30340;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03227
&lt;/p&gt;
&lt;p&gt;
IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;MRI&#30740;&#31350;&#20013;&#65292;&#26469;&#33258;&#22810;&#20010;&#37319;&#38598;&#28857;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#32858;&#21512;&#21487;&#20197;&#22686;&#21152;&#26679;&#26412;&#22823;&#23567;&#65292;&#20294;&#21487;&#33021;&#24341;&#20837;&#38459;&#30861;&#21518;&#32493;&#20998;&#26512;&#19968;&#33268;&#24615;&#30340;&#19982;&#37319;&#38598;&#28857;&#30456;&#20851;&#30340;&#21464;&#24322;&#12290;&#22270;&#20687;&#32763;&#35793;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#21327;&#35843;MR&#22270;&#20687;&#36328;&#31449;&#28857;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;IGUANe&#65288;&#20855;&#26377;&#32479;&#19968;&#23545;&#25239;&#32593;&#32476;&#30340;&#22270;&#20687;&#29983;&#25104;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21407;&#22987;&#30340;&#19977;&#32500;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#22495;&#36716;&#25442;&#30340;&#20248;&#21183;&#21644;&#30452;&#25509;&#24212;&#29992;&#26679;&#24335;&#36716;&#31227;&#26041;&#27861;&#26469;&#23454;&#29616;&#22810;&#20013;&#24515;&#33041;MR&#22270;&#20687;&#21327;&#35843;&#12290;IGUANe&#36890;&#36807;&#22810;&#23545;&#19968;&#31574;&#30053;&#65292;&#38598;&#25104;&#20102;&#20219;&#24847;&#25968;&#37327;&#30340;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#25193;&#23637;&#20102;CycleGAN&#26550;&#26500;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22270;&#20687;&#65292;&#29978;&#33267;&#26469;&#33258;&#26410;&#30693;&#37319;&#38598;&#28857;&#65292;&#20351;&#20854;&#25104;&#20026;&#21327;&#35843;&#30340;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;&#22312;&#30001;11&#21488;&#19981;&#21516;&#25195;&#25551;&#20202;&#30340;T1&#21152;&#26435;&#22270;&#20687;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;IGUANe&#22312;&#26410;&#35265;&#31449;&#28857;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the
&lt;/p&gt;</description></item><item><title>BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03216</link><description>&lt;p&gt;
BGE M3-&#23884;&#20837;&#65306;&#36890;&#36807;&#33258;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03216
&lt;/p&gt;
&lt;p&gt;
BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;M3-&#23884;&#20837;&#65292;&#20197;&#20854;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#32780;&#33879;&#31216;&#12290;&#23427;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#23884;&#20837;&#27169;&#22411;&#30340;&#19977;&#31181;&#24120;&#35265;&#26816;&#32034;&#21151;&#33021;&#65306;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;IR&#24212;&#29992;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#27169;&#22411;&#22522;&#30784;&#12290;&#23427;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#30701;&#21477;&#21040;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26723;&#12290;M3-&#23884;&#20837;&#30340;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20197;&#19979;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#19981;&#21516;&#26816;&#32034;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#25972;&#21512;&#20026;&#25945;&#24072;&#20449;&#21495;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#20855;&#26377;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#30340;&#20004;&#20010;&#27493;&#39588;&#12290;&#36890;&#36807;&#23545;&#35813;&#25915;&#20987;&#23545; GPT &#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616; GPT-4 &#23545;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02987</link><description>&lt;p&gt;
GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Conversation Reconstruction Attack Against GPT Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#20855;&#26377;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#30340;&#20004;&#20010;&#27493;&#39588;&#12290;&#36890;&#36807;&#23545;&#35813;&#25915;&#20987;&#23545; GPT &#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616; GPT-4 &#23545;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20854;&#20013; GPT &#31995;&#21015;&#27169;&#22411;&#20195;&#34920;&#30528;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#25104;&#26524;&#12290;&#20026;&#20102;&#20248;&#21270;&#20219;&#21153;&#25191;&#34892;&#65292;&#29992;&#25143;&#32463;&#24120;&#19982;&#25176;&#31649;&#22312;&#20113;&#29615;&#22659;&#20013;&#30340; GPT &#27169;&#22411;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290;&#36825;&#20123;&#22810;&#36718;&#23545;&#35805;&#24448;&#24448;&#21253;&#21547;&#31169;&#20154;&#20449;&#24687;&#65292;&#38656;&#35201;&#22312;&#20113;&#20013;&#36827;&#34892;&#20256;&#36755;&#21644;&#23384;&#20648;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25805;&#20316;&#27169;&#24335;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#25915;&#20987;&#38754;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#29305;&#23450;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65306;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#24403; GPT &#27169;&#22411;&#36973;&#21463;&#35813;&#25915;&#20987;&#26102;&#23545;&#35805;&#20013;&#22266;&#26377;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#20102;&#35814;&#23613;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;GPT-4 &#23545;&#20110;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#39640;&#32423;&#25915;&#20987;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#37325;&#26500;&#20197;&#21069;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, significant advancements have been made in the field of large language models (LLMs), represented by GPT series models. To optimize task execution, users often engage in multi-round conversations with GPT models hosted in cloud environments. These multi-round conversations, potentially replete with private information, require transmission and storage within the cloud. However, this operational paradigm introduces additional attack surfaces. In this paper, we first introduce a specific Conversation Reconstruction Attack targeting GPT models. Our introduced Conversation Reconstruction Attack is composed of two steps: hijacking a session and reconstructing the conversations. Subsequently, we offer an exhaustive evaluation of the privacy risks inherent in conversations when GPT models are subjected to the proposed attack. However, GPT-4 demonstrates certain robustness to the proposed attacks. We then introduce two advanced attacks aimed at better reconstructing previous c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32508;&#36848;&#21644;&#35843;&#26597;&#30740;&#31350;&#29289;&#32852;&#32593;&#22312;&#20919;&#38142;&#20013;&#30340;&#23454;&#26045;&#38556;&#30861;&#65292;&#21457;&#29616;&#20102;13&#20010;&#20851;&#38190;&#38556;&#30861;&#12290;&#20854;&#20013;&#65292;&#21512;&#35268;&#24615;&#21644;&#20919;&#38142;&#32593;&#32476;&#26159;&#29289;&#32852;&#32593;&#37319;&#29992;&#31574;&#30053;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;MICMAC&#21644;DEMATEL&#26041;&#27861;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#35780;&#20272;&#38556;&#30861;&#20043;&#38388;&#30340;&#20114;&#21160;&#20851;&#31995;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.01804</link><description>&lt;p&gt;
&#20919;&#38142;&#20013;&#29289;&#32852;&#32593;&#23454;&#26045;&#38556;&#30861;&#30340;&#20998;&#26512;&#65306;&#19968;&#31181;&#38598;&#25104;&#30340;ISM-MICMAC&#21644;DEMATEL&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Analysis of Internet of Things implementation barriers in the cold supply chain: an integrated ISM-MICMAC and DEMATEL approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32508;&#36848;&#21644;&#35843;&#26597;&#30740;&#31350;&#29289;&#32852;&#32593;&#22312;&#20919;&#38142;&#20013;&#30340;&#23454;&#26045;&#38556;&#30861;&#65292;&#21457;&#29616;&#20102;13&#20010;&#20851;&#38190;&#38556;&#30861;&#12290;&#20854;&#20013;&#65292;&#21512;&#35268;&#24615;&#21644;&#20919;&#38142;&#32593;&#32476;&#26159;&#29289;&#32852;&#32593;&#37319;&#29992;&#31574;&#30053;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;MICMAC&#21644;DEMATEL&#26041;&#27861;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#35780;&#20272;&#38556;&#30861;&#20043;&#38388;&#30340;&#20114;&#21160;&#20851;&#31995;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#29289;&#32852;&#32593;&#25216;&#26415;&#25972;&#21512;&#21040;&#20919;&#38142;&#20013;&#21487;&#20197;&#25552;&#39640;&#36879;&#26126;&#24230;&#12289;&#25928;&#29575;&#21644;&#36136;&#37327;&#65292;&#20248;&#21270;&#36816;&#33829;&#27969;&#31243;&#24182;&#25552;&#39640;&#29983;&#20135;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#29615;&#22659;&#19979;&#65292;&#29289;&#32852;&#32593;&#22312;&#20919;&#38142;&#20013;&#30340;&#38598;&#25104;&#21463;&#21040;&#29305;&#23450;&#30340;&#38556;&#30861;&#30340;&#38459;&#30861;&#65292;&#38656;&#35201;&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;&#29289;&#32852;&#32593;&#23454;&#26045;&#30340;&#30456;&#20851;&#25991;&#29486;&#36827;&#34892;&#32508;&#36848;&#65292;&#20849;&#21457;&#29616;&#20102;13&#20010;&#38556;&#30861;&#12290;&#35843;&#26597;&#25968;&#25454;&#32463;&#36807;&#20132;&#21449;&#39564;&#35777;&#20197;&#30830;&#20445;&#36136;&#37327;&#65292;&#24182;&#37319;&#29992;Cronbach's alpha&#27979;&#35797;&#26469;&#30830;&#20445;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#22312;&#31532;&#19968;&#38454;&#27573;&#24212;&#29992;&#35299;&#37322;&#24615;&#32467;&#26500;&#24314;&#27169;&#25216;&#26415;&#20197;&#35782;&#21035;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#36825;&#20123;&#38556;&#30861;&#20013;&#65292;&#8220;&#21512;&#35268;&#24615;&#8221;&#21644;&#8220;&#20919;&#38142;&#32593;&#32476;&#8221;&#26159;&#29289;&#32852;&#32593;&#37319;&#29992;&#31574;&#30053;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;MICMAC&#30340;&#39537;&#21160;&#21644;&#20381;&#36182;&#21147;&#37327;&#20803;&#32032;&#20998;&#31867;&#26377;&#21161;&#20110;&#35780;&#20272;&#38556;&#30861;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;&#22312;&#26412;&#30740;&#31350;&#30340;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#36890;&#36807;DEMATEL&#26041;&#27861;&#30830;&#23450;&#20102;&#25152;&#35782;&#21035;&#38556;&#30861;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating Internet of Things (IoT) technology inside the cold supply chain can enhance transparency, efficiency, and quality, optimizing operating procedures and increasing productivity. The integration of IoT in this complicated setting is hindered by specific barriers that need a thorough examination. Prominent barriers to IoT implementation in the cold supply chain are identified using a two-stage model. After reviewing the available literature on the topic of IoT implementation, a total of 13 barriers were found. The survey data was cross-validated for quality, and Cronbach's alpha test was employed to ensure validity. This research applies the interpretative structural modeling technique in the first phase to identify the main barriers. Among those barriers, "regularity compliance" and "cold chain networks" are key drivers for IoT adoption strategies. MICMAC's driving and dependence power element categorization helps evaluate the barrier interactions. In the second phase of this
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#21253;&#25324;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#22797;&#26434;&#21644;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.01703</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#21253;&#25324;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#22797;&#26434;&#21644;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#24220;&#23448;&#21592;&#19982;&#24066;&#27665;&#20043;&#38388;&#30340;&#20114;&#21160;&#24433;&#21709;&#20844;&#20849;&#31119;&#31049;&#21644;&#27665;&#20027;&#31038;&#20250;&#30340;&#27491;&#24403;&#24615;&#12290;&#35686;&#23519;&#26159;&#22269;&#23478;&#26368;&#26174;&#32780;&#26131;&#35265;&#12289;&#26368;&#25509;&#35302;&#24066;&#27665;&#30340;&#20195;&#29702;&#20154;&#65292;&#22312;&#20132;&#36890;&#31449;&#20572;&#26399;&#38388;&#65292;&#20182;&#20204;&#27599;&#24180;&#19982;&#20844;&#20247;&#20114;&#21160;&#36229;&#36807;2000&#19975;&#27425;&#12290;&#22914;&#20170;&#65292;&#36825;&#20123;&#20114;&#21160;&#32463;&#24120;&#34987;&#25140;&#22312;&#36523;&#19978;&#30340;&#25668;&#20687;&#26426;&#35760;&#24405;&#19979;&#26469;&#65292;&#36825;&#34987;&#35270;&#20026;&#25552;&#39640;&#35686;&#23519;&#38382;&#36131;&#21046;&#21644;&#25913;&#21892;&#35686;&#27665;&#20114;&#21160;&#30340;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#26469;&#20998;&#26512;&#36825;&#20123;&#22797;&#26434;&#32780;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#65292;&#36825;&#20123;&#35760;&#24405;&#30340;&#21450;&#26102;&#20998;&#26512;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35282;&#24230;&#12289;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24037;&#20855;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#26469;&#33258;&#36825;&#20123;&#36523;&#19978;&#25668;&#20687;&#26426;&#35760;&#24405;&#30340;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#30830;&#23450;&#19982;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#26368;&#30456;&#20851;&#30340;&#27807;&#36890;&#26041;&#38754;&#65292;&#21253;&#25324;&#20849;&#21516;&#24863;&#30693;&#20114;&#21160;&#30340;&#26631;&#24535;&#26631;&#35760;&#20197;&#21450;&#20855;&#26377;&#36825;&#20123;&#26631;&#35760;&#30340;&#31526;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions between the government officials and civilians affect public wellbeing and the state legitimacy that is necessary for the functioning of democratic society. Police officers, the most visible and contacted agents of the state, interact with the public more than 20 million times a year during traffic stops. Today, these interactions are regularly recorded by body-worn cameras (BWCs), which are lauded as a means to enhance police accountability and improve police-public interactions. However, the timely analysis of these recordings is hampered by a lack of reliable automated tools that can enable the analysis of these complex and contested police-public interactions. This article proposes an approach to developing new multi-perspective, multimodal machine learning (ML) tools to analyze the audio, video, and transcript information from this BWC footage. Our approach begins by identifying the aspects of communication most salient to different stakeholders, including both commun
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#31216;&#20026;ReAGent&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#25928;&#29575;&#26356;&#39640;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.00794</link><description>&lt;p&gt;
ReAGent: &#19968;&#20010;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#31216;&#20026;ReAGent&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#25928;&#29575;&#26356;&#39640;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;FAs&#65289;&#65292;&#22914;&#26799;&#24230;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30830;&#23450;&#25152;&#26377;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#29616;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20026;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24320;&#21457;&#21644;&#27979;&#35797;FAs&#65292;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#22312;&#25991;&#26412;&#29983;&#25104;&#19978;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;FAs&#26469;&#22788;&#29702;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#22240;&#20026;&#27169;&#22411;&#26550;&#26500;&#21644;&#20219;&#21153;&#35774;&#32622;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#27809;&#26377;&#19968;&#20010;&#36890;&#29992;&#30340;FA&#36866;&#29992;&#20110;&#25152;&#26377;&#27169;&#22411;&#21644;&#20219;&#21153;&#12290;&#36825;&#20351;&#24471;&#38024;&#23545;&#22823;&#22411;LMs&#36873;&#25321;FA&#35745;&#31639;&#19978;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#20026;&#36755;&#20837;&#37325;&#35201;&#24615;&#30340;&#25512;&#23548;&#36890;&#24120;&#38656;&#35201;&#22810;&#20010;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#36882;&#65292;&#21253;&#25324;&#21487;&#33021;&#26159;&#38480;&#21046;&#24615;&#30340;&#26799;&#24230;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;LMs&#30340;&#27169;&#22411;&#26080;&#20851;FA&#65292;&#31216;&#20026;&#36882;&#24402;&#24402;&#22240;&#29983;&#25104;&#22120;&#65288;ReAGent&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent)
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#26435;&#34913;&#22788;&#29702;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#20013;&#30340;&#32039;&#24352;&#20851;&#31995;&#30340;&#20116;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23454;&#26045;&#20840;&#38754;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2401.08103</link><description>&lt;p&gt;
&#35299;&#20915;&#22312;&#23454;&#26045;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20262;&#29702;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Resolving Ethics Trade-offs in Implementing Responsible AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#26435;&#34913;&#22788;&#29702;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#20013;&#30340;&#32039;&#24352;&#20851;&#31995;&#30340;&#20116;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23454;&#26045;&#20840;&#38754;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25226;&#39640;&#32423;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#24212;&#29992;&#21040;&#23454;&#38469;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#22788;&#29702;&#24213;&#23618;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26041;&#38754;&#30340;&#32039;&#24352;&#20851;&#31995;&#26041;&#38754;&#20173;&#23384;&#22312;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20116;&#31181;&#22788;&#29702;&#36825;&#20123;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#19981;&#31561;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#32771;&#34385;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#12289;&#33539;&#22260;&#12289;&#34913;&#37327;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#21644;&#35777;&#26126;&#31243;&#24230;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#20123;&#26041;&#27861;&#20013;&#27809;&#26377;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#32452;&#32455;&#12289;&#31995;&#32479;&#25110;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#31215;&#26497;&#35782;&#21035;&#32039;&#24352;&#20851;&#31995;&#65292;&#65288;ii&#65289;&#20248;&#20808;&#22788;&#29702;&#21644;&#26435;&#34913;&#20262;&#29702;&#26041;&#38754;&#65292;&#65288;iii&#65289;&#35777;&#26126;&#21644;&#35760;&#24405;&#26435;&#34913;&#20915;&#31574;&#12290;&#35813;&#25552;&#35758;&#30340;&#26694;&#26550;&#26088;&#22312;&#20419;&#36827;&#23454;&#26045;&#31526;&#21512;&#28508;&#22312;&#30417;&#31649;&#35201;&#27714;&#30340;&#20840;&#38754;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the operationalisation of high-level AI ethics principles into practical AI/ML systems has made progress, there is still a theory-practice gap in managing tensions between the underlying AI ethics aspects. We cover five approaches for addressing the tensions via trade-offs, ranging from rudimentary to complex. The approaches differ in the types of considered context, scope, methods for measuring contexts, and degree of justification. None of the approaches is likely to be appropriate for all organisations, systems, or applications. To address this, we propose a framework which consists of: (i) proactive identification of tensions, (ii) prioritisation and weighting of ethics aspects, (iii) justification and documentation of trade-off decisions. The proposed framework aims to facilitate the implementation of well-rounded AI/ML systems that are appropriate for potential regulatory requirements.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#25490;&#21517;&#26597;&#35810;&#21644;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26597;&#35810;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#22914;&#21307;&#23398;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#20854;&#20182;&#26377;&#36259;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.14877</link><description>&lt;p&gt;
&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#31283;&#20581;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Robust Knowledge Extraction from Large Language Models using Social Choice Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#25490;&#21517;&#26597;&#35810;&#21644;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26597;&#35810;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#22914;&#21307;&#23398;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#20854;&#20182;&#26377;&#36259;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#25903;&#25345;&#24456;&#22810;&#24212;&#29992;&#65292;&#22914;&#23545;&#35805;&#20195;&#29702;&#12289;&#21019;&#24847;&#20889;&#20316;&#25110;&#19968;&#33324;&#26597;&#35810;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#65288;&#22914;&#21307;&#23398;&#65289;&#20013;&#65292;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#26597;&#35810;&#22238;&#31572;&#65292;&#22240;&#20026;&#24403;&#22810;&#27425;&#25552;&#31034;&#30456;&#21516;&#26597;&#35810;&#26102;&#65292;&#23427;&#20204;&#36890;&#24120;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615; - &#32467;&#26524;&#21487;&#33021;&#19981;&#21516;&#12290;&#20026;&#20102;&#25552;&#39640;LLM&#26597;&#35810;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25490;&#21517;&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#26041;&#27861;&#26469;&#27719;&#24635;&#26597;&#35810;&#32467;&#26524;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35786;&#26029;&#22330;&#26223;&#20013;&#30340;&#25490;&#21517;&#26597;&#35810;&#65292;&#22914;&#21307;&#23398;&#21644;&#25925;&#38556;&#35786;&#26029;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#24212;&#29992;&#25991;&#29486;&#20013;&#30340;Partial Borda Choice&#20989;&#25968;&#26469;&#21512;&#24182;&#22810;&#20010;&#26597;&#35810;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#35774;&#32622;&#20013;&#30340;&#19968;&#20123;&#20854;&#20182;&#26377;&#36259;&#23646;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-language models (LLMs) can support a wide range of applications like conversational agents, creative writing or general query answering. However, they are ill-suited for query answering in high-stake domains like medicine because they are typically not robust - even the same query can result in different answers when prompted multiple times. In order to improve the robustness of LLM queries, we propose using ranking queries repeatedly and to aggregate the queries using methods from social choice theory. We study ranking queries in diagnostic settings like medical and fault diagnosis and discuss how the Partial Borda Choice function from the literature can be applied to merge multiple query results. We discuss some additional interesting properties in our setting and evaluate the robustness of our approach empirically.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20154;&#33080;&#39564;&#35777;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#32534;&#30721;&#22120;&#21021;&#22987;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#23545;&#26631;&#35760;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#20943;&#23569;&#65292;&#22312;&#20154;&#33080;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.14301</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20154;&#33080;&#39564;&#35777;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Autoencoder Based Face Verification System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14301
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20154;&#33080;&#39564;&#35777;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#32534;&#30721;&#22120;&#21021;&#22987;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#23545;&#26631;&#35760;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#20943;&#23569;&#65292;&#22312;&#20154;&#33080;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#20943;&#23569;&#23545;&#26631;&#35760;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#22312;&#20154;&#33080;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#65292;&#37319;&#29992;&#20004;&#20010;&#27493;&#39588;&#30340;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#30340;&#21021;&#22987;&#21270;&#21442;&#25968;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20010;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#20197;&#26377;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#65292;&#20351;&#29992;&#30456;&#23545;&#26377;&#38480;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#35780;&#20272;&#38454;&#27573;&#65292;&#29983;&#25104;&#20154;&#33080;&#22270;&#20687;&#23884;&#20837;&#20316;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#22312;CelebA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#32780;&#35780;&#20272;&#21017;&#20351;&#29992;&#20102;&#35832;&#22914;Labeled Faces in the Wild (LFW)&#21644;YouTube Faces (YTF)&#31561;&#22522;&#20934;&#20154;&#33080;&#35782;&#21035;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#32534;&#30721;&#22120;&#26469;&#21021;&#22987;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36798;&#21040;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary objective of this work is to present an alternative approach aimed at reducing the dependency on labeled data. Our proposed method involves utilizing autoencoder pre-training within a face image recognition task with two step processes. Initially, an autoencoder is trained in an unsupervised manner using a substantial amount of unlabeled training dataset. Subsequently, a deep learning model is trained with initialized parameters from the pre-trained autoencoder. This deep learning training process is conducted in a supervised manner, employing relatively limited labeled training dataset. During evaluation phase, face image embeddings is generated as the output of deep neural network layer. Our training is executed on the CelebA dataset, while evaluation is performed using benchmark face recognition datasets such as Labeled Faces in the Wild (LFW) and YouTube Faces (YTF). Experimental results demonstrate that by initializing the deep neural network with pre-trained autoencod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2312.13327</link><description>&lt;p&gt;
&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#30340;&#24773;&#22659;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Reinforcement Learning for Variable Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#20808;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19978;&#19979;&#25991;&#22810;&#24773;&#33410;&#35757;&#32451;&#30340;&#21464;&#24418;&#37329;&#21018;&#32593;&#32476;&#21487;&#20197;&#22312;&#24773;&#22659;&#20013;&#27867;&#21270;&#21040;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#34892;&#21160;&#31354;&#38388;&#22823;&#23567;&#21644;&#32467;&#26500;&#12290;&#24341;&#20837;&#26032;&#30340;&#34892;&#21160;&#31354;&#38388;&#36890;&#24120;&#38656;&#35201;&#25968;&#25454;&#37325;&#26032;&#25910;&#38598;&#21644;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#24212;&#29992;&#26469;&#35828;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21482;&#35757;&#32451;&#19968;&#27425;&#30340;Headless-AD&#27169;&#22411;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#20855;&#26377;&#21487;&#21464;&#22823;&#23567;&#12289;&#35821;&#20041;&#20869;&#23481;&#21644;&#39034;&#24207;&#30340;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#12290;&#36890;&#36807;&#22312;&#20271;&#21162;&#21033;&#21644;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#20197;&#21450;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Headless-AD&#22312;&#20174;&#26410;&#36935;&#21040;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#20960;&#20010;&#29615;&#22659;&#37197;&#32622;&#19978;&#32988;&#36807;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been shown that transformers pre-trained on diverse datasets with multi-episode contexts can generalize to new reinforcement learning tasks in-context. A key limitation of previously proposed models is their reliance on a predefined action space size and structure. The introduction of a new action space often requires data re-collection and model re-training, which can be costly for some applications. In our work, we show that it is possible to mitigate this issue by proposing the Headless-AD model that, despite being trained only once, is capable of generalizing to discrete action spaces of variable size, semantic content and order. By experimenting with Bernoulli and contextual bandits, as well as a gridworld environment, we show that Headless-AD exhibits significant capability to generalize to action spaces it has never encountered, even outperforming specialized models trained for a specific set of actions on several environment configurations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CLLM&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#21644;&#25968;&#25454;&#31579;&#36873;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#36827;&#34892;&#34920;&#26684;&#22686;&#24378;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#20197;&#21450;&#22522;&#20110;&#23398;&#20064;&#21160;&#24577;&#12289;&#32622;&#20449;&#24230;&#21644;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#30340;&#31579;&#36873;&#26426;&#21046;&#65292;CLLM&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.12112</link><description>&lt;p&gt;
LLM&#31934;&#36873;&#65306;&#22312;&#36229;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#21033;&#29992;LLMs&#21644;&#25968;&#25454;&#31579;&#36873;&#36827;&#34892;&#34920;&#26684;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CLLM&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#21644;&#25968;&#25454;&#31579;&#36873;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#36827;&#34892;&#34920;&#26684;&#22686;&#24378;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#20197;&#21450;&#22522;&#20110;&#23398;&#20064;&#21160;&#24577;&#12289;&#32622;&#20449;&#24230;&#21644;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#30340;&#31579;&#36873;&#26426;&#21046;&#65292;CLLM&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#34987;&#20302;&#20272;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22686;&#21152;ML&#25152;&#38656;&#30340;&#25968;&#25454;&#26679;&#26412;&#22823;&#23567;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#23545;&#20110;&#37322;&#25918;ML&#22312;&#25968;&#25454;&#21294;&#20047;&#30340;&#22320;&#21306;&#21644;&#39046;&#22495;&#30340;&#21464;&#38761;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26377;&#38480;&#30340;&#35757;&#32451;&#38598;&#38480;&#21046;&#20102;&#20256;&#32479;&#30340;&#34920;&#26684;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#22312;&#29983;&#25104;ML&#20219;&#21153;&#25152;&#38656;&#30340;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CLLM&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20687;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#19968;&#26679;&#65292;&#24182;&#38750;LLMs&#29983;&#25104;&#30340;&#25152;&#26377;&#25968;&#25454;&#37117;&#33021;&#25552;&#39640;&#19979;&#28216;&#30340;&#25928;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#21160;&#24577;&#12289;&#32622;&#20449;&#24230;&#21644;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#30340;&#21407;&#21017;&#24615;&#31579;&#36873;&#26426;&#21046;&#65292;&#20197;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CLLM&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) in low-data settings remains an underappreciated yet crucial problem. Hence, data augmentation methods to increase the sample size of datasets needed for ML are key to unlocking the transformative potential of ML in data-deprived regions and domains. Unfortunately, the limited training set constrains traditional tabular synthetic data generators in their ability to generate a large and diverse augmented dataset needed for ML tasks. To address this challenge, we introduce CLLM, which leverages the prior knowledge of Large Language Models (LLMs) for data augmentation in the low-data regime. However, not all the data generated by LLMs will improve downstream utility, as for any generative model. Consequently, we introduce a principled curation mechanism, leveraging learning dynamics, coupled with confidence and uncertainty metrics, to obtain a high-quality dataset. Empirically, on multiple real-world datasets, we demonstrate the superior performance of CLLM in the lo
&lt;/p&gt;</description></item><item><title>&#20844;&#24179;&#24615;&#32422;&#26463;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#33021;&#22815;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.10396</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#32422;&#26463;&#33021;&#22815;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#24110;&#21161;&#20174;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#20013;&#24674;&#22797;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Can Fairness Constraints Help Recover From Biased Data?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10396
&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#32422;&#26463;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#33021;&#22815;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#35748;&#20026;&#65292;&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#20844;&#24179;&#24615;&#32422;&#26463;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#30340;&#20943;&#23569;&#65292;&#32780;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#21487;&#33021;&#20250;&#21152;&#21095;&#36825;&#31181;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;Blum&#65286;Stangl&#65288;2019&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#65292;&#21363;&#20351;&#37319;&#29992;&#24179;&#31561;&#26426;&#20250;&#32422;&#26463;&#65292;&#20063;&#21487;&#20197;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;&#20182;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24456;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#35777;&#26126;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#21487;&#20197;&#38544;&#24335;&#20462;&#27491;&#25968;&#25454;&#20559;&#24046;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#20844;&#24179;&#24615;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#20182;&#20204;&#30340;&#25968;&#25454;&#20559;&#24046;&#27169;&#22411;&#27169;&#25311;&#20102;&#21463;&#21387;&#36843;&#20154;&#32676;&#30340;&#34920;&#24449;&#21644;&#26631;&#31614;&#20559;&#35265;&#65292;&#24182;&#22312;&#20855;&#26377;&#29420;&#31435;&#26631;&#31614;&#22122;&#22768;&#30340;&#31616;&#21333;&#26465;&#20214;&#19979;&#65292;&#38024;&#23545;&#19968;&#20010;&#29702;&#24819;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#23637;&#31034;&#20102;&#19978;&#36848;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;Blum&#65286;Stangl&#65288;2019&#65289;&#30340;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#12289;&#25968;&#25454;&#20559;&#24046;&#27169;&#22411;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#20551;&#35774;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
A general belief in fair classification is that fairness constraints incur a trade-off with accuracy, which biased data may worsen. Contrary to this belief, Blum &amp; Stangl (2019) show that fair classification with equal opportunity constraints even on extremely biased data can recover optimally accurate and fair classifiers on the original data distribution. Their result is interesting because it demonstrates that fairness constraints can implicitly rectify data bias and simultaneously overcome a perceived fairness-accuracy trade-off. Their data bias model simulates under-representation and label bias in underprivileged population, and they show the above result on a stylized data distribution with i.i.d. label noise, under simple conditions on the data distribution and bias parameters. We propose a general approach to extend the result of Blum &amp; Stangl (2019) to different fairness constraints, data bias models, data distributions, and hypothesis classes. We strengthen their result, and
&lt;/p&gt;</description></item><item><title>CIDR&#26159;&#19968;&#31181;&#35299;&#20915;&#26368;&#23567;&#29305;&#24449;&#21024;&#38500;&#38382;&#39064;&#30340;&#21512;&#20316;&#24335;&#38598;&#25104;&#21160;&#24577;&#20462;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#20316;&#24335;&#38598;&#25104;&#26799;&#24230;&#26469;&#26816;&#27979;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#32972;&#21253;&#38382;&#39064;&#65292;&#20174;&#20247;&#22810;&#20505;&#36873;&#38598;&#20013;&#30830;&#23450;&#26368;&#23567;&#29305;&#24449;&#38598;&#12290;</title><link>https://arxiv.org/abs/2312.08157</link><description>&lt;p&gt;
CIDR: &#19968;&#31181;&#29992;&#20110;&#26368;&#23567;&#29305;&#24449;&#21024;&#38500;&#38382;&#39064;&#30340;&#21512;&#20316;&#24335;&#38598;&#25104;&#21160;&#24577;&#20462;&#27491;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CIDR: A Cooperative Integrated Dynamic Refining Method for Minimal Feature Removal Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08157
&lt;/p&gt;
&lt;p&gt;
CIDR&#26159;&#19968;&#31181;&#35299;&#20915;&#26368;&#23567;&#29305;&#24449;&#21024;&#38500;&#38382;&#39064;&#30340;&#21512;&#20316;&#24335;&#38598;&#25104;&#21160;&#24577;&#20462;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#20316;&#24335;&#38598;&#25104;&#26799;&#24230;&#26469;&#26816;&#27979;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#32972;&#21253;&#38382;&#39064;&#65292;&#20174;&#20247;&#22810;&#20505;&#36873;&#38598;&#20013;&#30830;&#23450;&#26368;&#23567;&#29305;&#24449;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#32493;&#35299;&#37322;&#39046;&#22495;&#20013;&#30340;&#26368;&#23567;&#29305;&#24449;&#21024;&#38500;&#38382;&#39064;&#26088;&#22312;&#35782;&#21035;&#26368;&#23567;&#29305;&#24449;&#38598;&#65288;MFS&#65289;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#36138;&#23146;&#31639;&#27861;&#35745;&#31639;&#26368;&#23567;&#29305;&#24449;&#38598;&#26102;&#32570;&#20047;&#23545;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#30340;&#25506;&#32034;&#65292;&#32780;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26080;&#27861;&#28385;&#36275;&#21333;&#35843;&#24615;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#24335;&#38598;&#25104;&#21160;&#24577;&#20462;&#27491;&#26041;&#27861;&#65288;CIDR&#65289;&#26469;&#39640;&#25928;&#22320;&#21457;&#29616;&#26368;&#23567;&#29305;&#24449;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21512;&#20316;&#24335;&#38598;&#25104;&#26799;&#24230;&#65288;CIG&#65289;&#26469;&#26816;&#27979;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#23558;CIG&#19982;&#26368;&#23567;&#29305;&#24449;&#38598;&#30340;&#29305;&#24449;&#29305;&#24615;&#30456;&#32467;&#21512;&#65292;&#23558;&#26368;&#23567;&#29305;&#24449;&#21024;&#38500;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#32972;&#21253;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#36741;&#21161;&#26368;&#23567;&#29305;&#24449;&#20462;&#27491;&#31639;&#27861;&#26469;&#20174;&#20247;&#22810;&#20505;&#36873;&#38598;&#20013;&#30830;&#23450;&#26368;&#23567;&#29305;&#24449;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#39318;&#27425;&#25552;&#20986;&#20102;&#26368;&#23567;&#29305;&#24449;&#21024;&#38500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The minimal feature removal problem in the post-hoc explanation area aims to identify the minimal feature set (MFS). Prior studies using the greedy algorithm to calculate the minimal feature set lack the exploration of feature interactions under a monotonic assumption which cannot be satisfied in general scenarios. In order to address the above limitations, we propose a Cooperative Integrated Dynamic Refining method (CIDR) to efficiently discover minimal feature sets. Specifically, we design Cooperative Integrated Gradients (CIG) to detect interactions between features. By incorporating CIG and characteristics of the minimal feature set, we transform the minimal feature removal problem into a knapsack problem. Additionally, we devise an auxiliary Minimal Feature Refinement algorithm to determine the minimal feature set from numerous candidate sets. To the best of our knowledge, our work is the first to address the minimal feature removal problem in the field of natural language process
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;5G&#21450;&#20854;&#21518;&#22826;&#38451;&#33021;&#23567;&#22411;&#34562;&#31389;&#32593;&#32476;&#30340;&#26368;&#20339;&#26080;&#20154;&#26426;&#36127;&#36733;&#37325;&#20998;&#37197;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#20154;&#26426;&#19978;&#30340;&#31354;&#20013;&#22522;&#31449;&#36827;&#34892;&#21487;&#38752;&#23433;&#20840;&#30340;&#30005;&#21147;&#20877;&#20998;&#37197;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.12944</link><description>&lt;p&gt;
DroneOptiNet: &#19968;&#31181;&#29992;&#20110;5G&#21450;&#20854;&#21518;&#22826;&#38451;&#33021;&#23567;&#22411;&#34562;&#31389;&#32593;&#32476;&#30340;&#26368;&#20339;&#26080;&#20154;&#26426;&#36127;&#36733;&#37325;&#20998;&#37197;&#26426;&#21046;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DroneOptiNet: A Framework for Optimal Drone-based Load Redistribution Mechanism for 5G and Beyond Solar Small Cell Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;5G&#21450;&#20854;&#21518;&#22826;&#38451;&#33021;&#23567;&#22411;&#34562;&#31389;&#32593;&#32476;&#30340;&#26368;&#20339;&#26080;&#20154;&#26426;&#36127;&#36733;&#37325;&#20998;&#37197;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#20154;&#26426;&#19978;&#30340;&#31354;&#20013;&#22522;&#31449;&#36827;&#34892;&#21487;&#38752;&#23433;&#20840;&#30340;&#30005;&#21147;&#20877;&#20998;&#37197;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20116;&#20195;&#21450;&#20854;&#21518;&#30340;&#34562;&#31389;&#32593;&#32476;&#23545;&#21151;&#29575;&#38656;&#27714;&#25552;&#20986;&#20102;&#37325;&#35201;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#33021;&#22815;&#39640;&#25928;&#21033;&#29992;&#33021;&#28304;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20351;&#29992;&#26080;&#20154;&#26426;&#19978;&#30340;&#31354;&#20013;&#22522;&#31449;&#65288;BS&#65289;&#36827;&#34892;&#21487;&#38752;&#23433;&#20840;&#30340;&#30005;&#21147;&#20877;&#20998;&#37197;&#30340;&#29992;&#25143;&#36127;&#36733;&#36716;&#31227;&#26041;&#27861;&#65292;&#20197;&#36328;&#36234;&#30001;&#32511;&#33394;&#23567;&#22411;&#34562;&#31389;BS&#32452;&#25104;&#30340;&#24494;&#32593;&#32593;&#32476;&#12290;&#26681;&#25454;&#29992;&#25143;&#23494;&#24230;&#21644;&#31354;&#20013;&#22522;&#31449;&#30340;&#21487;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;&#31354;&#20013;&#22522;&#31449;&#20174;&#39640;&#33021;&#32791;&#23567;&#21306;&#36801;&#31227;&#21040;&#20302;&#33021;&#32791;&#23567;&#21306;&#65292;&#26469;&#28385;&#36275;&#33021;&#37327;&#19981;&#36275;&#30340;&#23567;&#21306;&#30340;&#33021;&#37327;&#38656;&#27714;&#12290;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#26080;&#20154;&#26426;&#26694;&#26550;&#23558;&#38271;&#30701;&#26399;&#35760;&#24518;&#19982;&#29420;&#29305;&#30340;&#25104;&#26412;&#20989;&#25968;&#32467;&#21512;&#65292;&#20351;&#29992;&#36827;&#21270;&#31070;&#32463;&#32593;&#32476;&#26469;&#26377;&#25928;&#22320;&#31649;&#29702;&#26080;&#20154;&#26426;&#21644;&#22522;&#31449;&#30340;&#33021;&#37327;&#21644;&#36127;&#36733;&#37325;&#20998;&#37197;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20943;&#23569;&#20102;&#22522;&#31449;&#20572;&#30005;&#65292;&#24182;&#20445;&#25345;&#20102;&#19968;&#33268;&#30340;&#21534;&#21520;&#37327;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#23637;&#31034;&#20102;&#20854;&#25552;&#21319;&#26080;&#32447;&#32593;&#32476;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The power requirements posed by the fifth-generation and beyond cellular networks are an important constraint in network deployment and require energy-efficient solutions. In this work, we propose a novel user load transfer approach using airborne base stations (BS) mounted on drones for reliable and secure power redistribution across the micro-grid network comprising green small cell BSs. Depending on the user density and the availability of an aerial BS, the energy requirement of a cell with an energy deficit is accommodated by migrating the aerial BS from a high-energy to a low-energy cell. The proposed hybrid drone-based framework integrates long short-term memory with unique cost functions using an evolutionary neural network for drones and BSs and efficiently manages energy and load redistribution. The proposed algorithm reduces power outages at BSs and maintains consistent throughput stability, thereby demonstrating its capability to boost the reliability and robustness of wirel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#36825;&#26159;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#36873;&#25321;&#24314;&#27169;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2311.07607</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#24314;&#27169;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Modeling Choice via Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#36825;&#26159;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#36873;&#25321;&#24314;&#27169;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#27169;&#22411;&#26159;&#36816;&#33829;&#31649;&#29702;&#39046;&#22495;&#20013;&#35768;&#22810;&#32463;&#20856;&#20248;&#21270;&#38382;&#39064;&#30340;&#22522;&#30784;&#36755;&#20837;&#65292;&#21253;&#25324;&#32452;&#21512;&#12289;&#24211;&#23384;&#21644;&#23450;&#20215;&#20248;&#21270;&#12290;&#20934;&#30830;&#22320;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#23454;&#36341;&#20013;&#24212;&#29992;&#36825;&#20123;&#20248;&#21270;&#38382;&#39064;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#36215;&#20102;&#23558;&#36825;&#20123;&#25216;&#26415;&#25972;&#21512;&#21040;&#36873;&#25321;&#24314;&#27169;&#20013;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#36873;&#25321;&#24314;&#27169;&#30340;&#20132;&#21449;&#28857;&#19978;&#23384;&#22312;&#26126;&#26174;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#23588;&#20854;&#26159;&#22312;&#29702;&#35770;&#21644;&#32463;&#39564;&#22522;&#30784;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#27169;&#22411;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25104;&#21151;&#65288;&#20174;&#29702;&#35770;&#21644;&#23454;&#36341;&#20004;&#20010;&#26041;&#38754;&#65289;&#21033;&#29992;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#27010;&#24565;&#65288;&#33258;&#27880;&#24847;&#21147;&#65289;&#30340;&#27169;&#22411;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36873;&#25321;&#27169;&#22411;&#26159;Halo&#22810;&#39033;&#24335;&#36923;&#36753;&#65288;Halo-MNL&#65289;&#27169;&#22411;&#30340;&#20302;&#31209;&#25512;&#24191;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Halo-MNL&#27169;&#22411;&#38656;&#35201;$\Omega(m^2)$&#30340;&#35745;&#31639;&#37327;&#65292;&#32780;&#25105;&#20204;&#30340;&#27169;&#22411;&#21482;&#38656;&#35201;$\Omega(m)$&#30340;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models of choice are a fundamental input to many now-canonical optimization problems in the field of Operations Management, including assortment, inventory, and price optimization. Naturally, accurate estimation of these models from data is a critical step in the application of these optimization problems in practice. Concurrently, recent advancements in deep learning have sparked interest in integrating these techniques into choice modeling. However, there is a noticeable research gap at the intersection of deep learning and choice modeling, particularly with both theoretical and empirical foundations. Thus motivated, we first propose a choice model that is the first to successfully (both theoretically and practically) leverage a modern neural network architectural concept (self-attention). Theoretically, we show that our attention-based choice model is a low-rank generalization of the Halo Multinomial Logit (Halo-MNL) model. We prove that whereas the Halo-MNL requires $\Omega(m^2)$ d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Lie&#31070;&#32463;&#20803;&#32593;&#32476;&#65292;&#33021;&#22815;&#20197;&#20219;&#20309;&#21322;&#21333;Lie&#20195;&#25968;&#25968;&#25454;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#20276;&#38543;&#25805;&#20316;&#20351;&#20854;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;&#36890;&#36807;&#25512;&#24191;&#21521;&#37327;&#31070;&#32463;&#20803;&#32593;&#32476;&#21644;&#24341;&#20837;&#26032;&#30340;&#23618;&#65292;&#35813;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.04521</link><description>&lt;p&gt;
Lie&#31070;&#32463;&#20803;&#65306;&#21322;&#21333;Lie&#20195;&#25968;&#30340;&#20276;&#38543;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Lie&#31070;&#32463;&#20803;&#32593;&#32476;&#65292;&#33021;&#22815;&#20197;&#20219;&#20309;&#21322;&#21333;Lie&#20195;&#25968;&#25968;&#25454;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#20276;&#38543;&#25805;&#20316;&#20351;&#20854;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;&#36890;&#36807;&#25512;&#24191;&#21521;&#37327;&#31070;&#32463;&#20803;&#32593;&#32476;&#21644;&#24341;&#20837;&#26032;&#30340;&#23618;&#65292;&#35813;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#35813;&#25968;&#25454;&#23384;&#22312;&#20110;&#20219;&#20309;&#21322;&#21333;Lie&#20195;&#25968;&#20013;&#12290;&#23545;&#24212;&#30340;&#32676;&#36890;&#36807;&#20276;&#38543;&#25805;&#20316;&#20316;&#29992;&#20110;Lie&#20195;&#25968;&#19978;&#65292;&#20351;&#24471;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#20855;&#26377;&#20276;&#38543;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#31616;&#21333;&#30340;$\mathrm{SO}(3)$-&#31561;&#21464;&#32593;&#32476;&#8212;&#8212;&#21521;&#37327;&#31070;&#32463;&#20803;&#20174;3&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#25512;&#24191;&#21040;Lie&#20195;&#25968;&#31354;&#38388;&#65292;&#21033;&#29992;Killing&#24418;&#24335;&#30340;&#19981;&#21464;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;Lie&#25324;&#21495;&#23618;&#21644;&#20960;&#20309;&#36890;&#36947;&#28151;&#21512;&#23618;&#26469;&#25193;&#23637;&#24314;&#27169;&#33021;&#21147;&#12290;&#23454;&#39564;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23545;$\mathfrak{so}(3)$&#21644;$\mathfrak{sl}(3)$ Lie&#20195;&#25968;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#25311;&#21512;&#31561;&#21464;&#21644;&#19981;&#21464;&#20989;&#25968;&#12289;&#23398;&#20064;&#31995;&#32479;&#21160;&#21147;&#23398;&#12289;&#28857;&#20113;&#37197;&#20934;&#21644;&#22522;&#20110;&#21333;&#24212;&#24615;&#30340;&#24418;&#29366;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31561;&#21464;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#34920;&#29616;&#20986;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an equivariant neural network that takes data in any semi-simple Lie algebra as input. The corresponding group acts on the Lie algebra as adjoint operations, making our proposed network adjoint-equivariant. Our framework generalizes the Vector Neurons, a simple $\mathrm{SO}(3)$-equivariant network, from 3-D Euclidean space to Lie algebra spaces, building upon the invariance property of the Killing form. Furthermore, we propose novel Lie bracket layers and geometric channel mixing layers that extend the modeling capacity. Experiments are conducted for the $\mathfrak{so}(3)$ and $\mathfrak{sl}(3)$ Lie algebras on various tasks, including fitting equivariant and invariant functions, learning system dynamics, point cloud registration, and homography-based shape classification. Our proposed equivariant network shows wide applicability and competitive performance in various domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2309.01945</link><description>&lt;p&gt;
OHQ: &#33455;&#29255;&#19978;&#30340;&#30828;&#20214;&#24863;&#30693;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
OHQ: On-chip Hardware-aware Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#20808;&#36827;&#28145;&#24230;&#27169;&#22411;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21033;&#29992;&#22810;&#20301;&#23485;&#26550;&#26500;&#26469;&#37322;&#25918;&#37327;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#23384;&#22312;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#37327;&#21270;&#36807;&#31243;&#20381;&#36182;&#20110;&#29420;&#31435;&#30340;&#39640;&#24615;&#33021;&#35774;&#22791;&#65292;&#32780;&#19981;&#26159;&#26412;&#22320;&#36827;&#34892;&#65292;&#36825;&#20063;&#23548;&#33268;&#20102;&#32771;&#34385;&#30340;&#30828;&#20214;&#25351;&#26631;&#19982;&#23454;&#38469;&#37096;&#32626;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#22312;&#32447;&#35774;&#22791;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33455;&#29255;&#19978;&#30340;&#37327;&#21270;&#24863;&#30693;&#65288;OQA&#65289;&#27969;&#27700;&#32447;&#65292;&#33021;&#22815;&#24863;&#30693;&#37327;&#21270;&#31639;&#23376;&#22312;&#30828;&#20214;&#19978;&#30340;&#23454;&#38469;&#25928;&#29575;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#65288;MQE&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lookbehind-SAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#27425;&#19978;&#21319;&#27493;&#39588;&#21644;&#32447;&#24615;&#25554;&#20540;&#26469;&#22686;&#24378;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25439;&#22833;&#38160;&#24230;&#25240;&#34935;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26377;&#22810;&#31181;&#20248;&#28857;&#65292;&#21253;&#25324;&#25552;&#39640;&#30340;&#27867;&#21270;&#24615;&#33021;&#12289;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#21644;&#25913;&#36827;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2307.16704</link><description>&lt;p&gt;
Lookbehind-SAM: k&#27493;&#22238;&#26395;&#65292;1&#27493;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Lookbehind-SAM: k steps back, 1 step forward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.16704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lookbehind-SAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#27425;&#19978;&#21319;&#27493;&#39588;&#21644;&#32447;&#24615;&#25554;&#20540;&#26469;&#22686;&#24378;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25439;&#22833;&#38160;&#24230;&#25240;&#34935;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26377;&#22810;&#31181;&#20248;&#28857;&#65292;&#21253;&#25324;&#25552;&#39640;&#30340;&#27867;&#21270;&#24615;&#33021;&#12289;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#21644;&#25913;&#36827;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#65288;SAM&#65289;&#26041;&#27861;&#36890;&#36807;&#23558;&#26368;&#23567;&#21270;&#25439;&#22833;&#20540;&#21644;&#25439;&#22833;&#38160;&#24230;&#38382;&#39064;&#34920;&#36848;&#20026;&#26497;&#23567;&#26497;&#22823;&#22411;&#30446;&#26631;&#65292;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;SAM&#30446;&#26631;&#20013;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#37096;&#20998;&#30340;&#25928;&#29575;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25439;&#22833;&#38160;&#24230;&#25240;&#34935;&#12290;&#21463;Lookahead&#20248;&#21270;&#22120;&#30340;&#21551;&#21457;&#65292;&#35813;&#20248;&#21270;&#22120;&#20351;&#29992;&#22810;&#20010;&#21521;&#21069;&#30340;&#19979;&#38477;&#27493;&#39588;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Lookbehind&#65292;&#23427;&#22312;&#21518;&#38754;&#25191;&#34892;&#22810;&#20010;&#19978;&#21319;&#27493;&#39588;&#65292;&#22686;&#24378;&#20102;SAM&#30340;&#26368;&#22823;&#21270;&#27493;&#39588;&#65292;&#24182;&#25214;&#21040;&#20102;&#19968;&#20010;&#20855;&#26377;&#26356;&#39640;&#25439;&#22833;&#30340;&#26368;&#22351;&#24773;&#20917;&#25200;&#21160;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20943;&#23567;&#30001;&#20110;&#25910;&#38598;&#21040;&#30340;&#22810;&#20010;&#19978;&#21319;&#27493;&#39588;&#30340;&#26799;&#24230;&#25152;&#24341;&#36215;&#30340;&#19979;&#38477;&#27493;&#39588;&#30340;&#26041;&#24046;&#65292;&#25105;&#20204;&#37319;&#29992;&#32447;&#24615;&#25554;&#20540;&#26469;&#25913;&#36827;&#26368;&#23567;&#21270;&#36807;&#31243;&#12290;Lookbehind&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#39640;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#23545;&#22122;&#22768;&#26435;&#37325;&#30340;&#26356;&#39640;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#25913;&#36827;&#30340;&#25928;&#26524;&#21644;&#36739;&#23569;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-aware minimization (SAM) methods have gained increasing popularity by formulating the problem of minimizing both loss value and loss sharpness as a minimax objective. In this work, we increase the efficiency of the maximization and minimization parts of SAM's objective to achieve a better loss-sharpness trade-off. By taking inspiration from the Lookahead optimizer, which uses multiple descent steps ahead, we propose Lookbehind, which performs multiple ascent steps behind to enhance the maximization step of SAM and find a worst-case perturbation with higher loss. Then, to mitigate the variance in the descent step arising from the gathered gradients across the multiple ascent steps, we employ linear interpolation to refine the minimization step. Lookbehind leads to a myriad of benefits across a variety of tasks. Particularly, we show increased generalization performance, greater robustness against noisy weights, as well as improved learning and less catastrophic forgetting in l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SecurityBERT&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#21644;&#38544;&#31169;&#20445;&#25252;&#32534;&#30721;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#26816;&#27979;&#32593;&#32476;&#23041;&#32961;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#39640;&#31934;&#24230;&#35782;&#21035;&#32593;&#32476;&#25915;&#20987;&#65292;&#24182;&#20855;&#26377;&#26368;&#23567;&#30340;&#35745;&#31639;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2306.14263</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#38761;&#26032;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#65306;&#19968;&#31181;&#38754;&#21521;&#29289;&#32852;&#32593;/&#24037;&#19994;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#38544;&#31169;&#20445;&#25252;BERT&#36731;&#37327;&#32423;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Cyber Threat Detection with Large Language Models: A privacy-preserving BERT-based Lightweight Model for IoT/IIoT Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.14263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SecurityBERT&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#21644;&#38544;&#31169;&#20445;&#25252;&#32534;&#30721;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#26816;&#27979;&#32593;&#32476;&#23041;&#32961;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#39640;&#31934;&#24230;&#35782;&#21035;&#32593;&#32476;&#25915;&#20987;&#65292;&#24182;&#20855;&#26377;&#26368;&#23567;&#30340;&#35745;&#31639;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65288;NLP&#65289;&#30446;&#21069;&#27491;&#22312;&#32463;&#21382;&#19968;&#22330;&#30001;&#22522;&#20110;&#38761;&#21629;&#24615;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#38761;&#21629;&#24615;&#36716;&#21464;&#12290;&#38543;&#30528;&#32593;&#32476;&#23433;&#20840;&#25915;&#20987;&#30340;&#39057;&#29575;&#21644;&#22810;&#26679;&#24615;&#25345;&#32493;&#22686;&#21152;&#65292;&#20107;&#20214;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;&#29289;&#32852;&#32593;&#35774;&#22791;&#27491;&#22312;&#36805;&#36895;&#25193;&#23637;&#65292;&#22240;&#27492;&#38656;&#35201;&#39640;&#31934;&#24230;&#21644;&#26368;&#23567;&#35745;&#31639;&#35201;&#27714;&#30340;&#33258;&#20027;&#35782;&#21035;&#29289;&#32852;&#32593;&#20013;&#30340;&#22522;&#20110;&#32593;&#32476;&#30340;&#25915;&#20987;&#30340;&#26377;&#25928;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SecurityBERT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;Transformer&#65288;BERT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#30340;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#12290;&#22312;SecurityBERT&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#22266;&#23450;&#38271;&#24230;&#32534;&#30721;&#65288;PPFLE&#65289;&#30340;&#26032;&#22411;&#38544;&#31169;&#20445;&#25252;&#32534;&#30721;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#20197;&#32467;&#26500;&#21270;&#26684;&#24335;&#36827;&#34892;&#26377;&#25928;&#34920;&#31034;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#20854;&#19982;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Natural Language Processing (NLP) is currently undergoing a revolutionary transformation driven by the power of pre-trained Large Language Models (LLMs) based on groundbreaking Transformer architectures. As the frequency and diversity of cybersecurity attacks continue to rise, the importance of incident detection has significantly increased. IoT devices are expanding rapidly, resulting in a growing need for efficient techniques to autonomously identify network-based attacks in IoT networks with both high precision and minimal computational requirements. This paper presents SecurityBERT, a novel architecture that leverages the Bidirectional Encoder Representations from Transformers (BERT) model for cyber threat detection in IoT networks. During the training of SecurityBERT, we incorporated a novel privacy-preserving encoding technique called Privacy-Preserving Fixed-Length Encoding (PPFLE). We effectively represented network traffic data in a structured format by combining 
&lt;/p&gt;</description></item><item><title>HardSATGEN&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#24674;&#22797;&#24037;&#19994;&#22522;&#20934;&#30340;&#32467;&#26500;&#21644;&#35745;&#31639;&#29305;&#24615;&#65292;&#27492;&#26041;&#27861;&#22312;&#24037;&#19994;SAT&#20844;&#24335;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2302.02104</link><description>&lt;p&gt;
HardSATGEN&#65306;&#29702;&#35299;&#38590;SAT&#20844;&#24335;&#29983;&#25104;&#30340;&#22256;&#38590;&#21644;&#24378;&#30340;&#32467;&#26500;&#22256;&#38590;&#24863;&#30693;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
HardSATGEN: Understanding the Difficulty of Hard SAT Formula Generation and A Strong Structure-Hardness-Aware Baseline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02104
&lt;/p&gt;
&lt;p&gt;
HardSATGEN&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#24674;&#22797;&#24037;&#19994;&#22522;&#20934;&#30340;&#32467;&#26500;&#21644;&#35745;&#31639;&#29305;&#24615;&#65292;&#27492;&#26041;&#27861;&#22312;&#24037;&#19994;SAT&#20844;&#24335;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;SAT&#20844;&#24335;&#29983;&#25104;&#26159;&#19968;&#39033;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;SAT&#29983;&#25104;&#26041;&#27861;&#24456;&#38590;&#21516;&#26102;&#25429;&#25417;&#20840;&#23616;&#32467;&#26500;&#29305;&#24615;&#24182;&#20445;&#25345;&#21512;&#29702;&#30340;&#35745;&#31639;&#22256;&#38590;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#20043;&#21069;&#23398;&#20064;&#26041;&#27861;&#22312;&#37325;&#29616;&#21407;&#22987;&#23454;&#20363;&#30340;&#35745;&#31639;&#22256;&#38590;&#24230;&#26041;&#38754;&#30340;&#38480;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#36825;&#21487;&#33021;&#28304;&#33258;&#20854;&#37319;&#29992;&#30340;&#20998;&#35010;&#21512;&#24182;&#36807;&#31243;&#30340;&#20869;&#22312;&#21516;&#36136;&#24615;&#12290;&#22522;&#20110;&#24037;&#19994;&#20844;&#24335;&#21576;&#29616;&#26126;&#30830;&#31038;&#21306;&#32467;&#26500;&#21644;&#36807;&#24230;&#20998;&#21106;&#23376;&#32467;&#26500;&#23548;&#33268;&#36923;&#36753;&#32467;&#26500;&#35821;&#20041;&#24418;&#25104;&#22256;&#38590;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HardSATGEN&#65292;&#22312;SAT&#20844;&#24335;&#29983;&#25104;&#30340;&#31070;&#32463;&#20998;&#35010;&#21512;&#24182;&#33539;&#20363;&#20013;&#24341;&#20837;&#20102;&#31934;&#32454;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#24674;&#22797;&#24037;&#19994;&#22522;&#20934;&#30340;&#32467;&#26500;&#21644;&#35745;&#31639;&#29305;&#24615;&#12290;&#21253;&#25324;&#31169;&#20154;&#21644;&#23454;&#38469;&#20225;&#19994;&#27979;&#35797;&#22522;&#20934;&#35780;&#20272;&#22312;&#20869;&#30340;&#23454;&#39564;&#34920;&#26126;HardSATGEN&#30340;&#20248;&#36234;&#24615;&#65292;&#25104;&#20026;&#21807;&#19968;&#19968;&#31181;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
Industrial SAT formula generation is a critical yet challenging task. Existing SAT generation approaches can hardly simultaneously capture the global structural properties and maintain plausible computational hardness. We first present an in-depth analysis for the limitation of previous learning methods in reproducing the computational hardness of original instances, which may stem from the inherent homogeneity in their adopted split-merge procedure. On top of the observations that industrial formulae exhibit clear community structure and oversplit substructures lead to the difficulty in semantic formation of logical structures, we propose HardSATGEN, which introduces a fine-grained control mechanism to the neural split-merge paradigm for SAT formula generation to better recover the structural and computational properties of the industrial benchmarks. Experiments including evaluations on private and practical corporate testbed show the superiority of HardSATGEN being the only method to
&lt;/p&gt;</description></item><item><title>&#24067;&#23572;&#35266;&#23519;&#21338;&#24328;&#26159;&#19968;&#31181;&#26377;&#38480;&#30340;&#22810;&#20154;&#31574;&#30053;&#21338;&#24328;&#65292;&#20855;&#26377;&#19981;&#23436;&#20840;&#20449;&#24687;&#21644;&#23450;&#24615;&#30446;&#26631;&#12290;&#23427;&#26159;&#24067;&#23572;&#21338;&#24328;&#30340;&#19968;&#31181;&#27867;&#21270;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#19981;&#23436;&#32654;&#21644;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2202.03637</link><description>&lt;p&gt;
&#24067;&#23572;&#35266;&#23519;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Boolean Observation Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.03637
&lt;/p&gt;
&lt;p&gt;
&#24067;&#23572;&#35266;&#23519;&#21338;&#24328;&#26159;&#19968;&#31181;&#26377;&#38480;&#30340;&#22810;&#20154;&#31574;&#30053;&#21338;&#24328;&#65292;&#20855;&#26377;&#19981;&#23436;&#20840;&#20449;&#24687;&#21644;&#23450;&#24615;&#30446;&#26631;&#12290;&#23427;&#26159;&#24067;&#23572;&#21338;&#24328;&#30340;&#19968;&#31181;&#27867;&#21270;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#19981;&#23436;&#32654;&#21644;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#24067;&#23572;&#35266;&#23519;&#21338;&#24328;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#38480;&#30340;&#22810;&#20154;&#31574;&#30053;&#21338;&#24328;&#65292;&#20855;&#26377;&#19981;&#23436;&#20840;&#20449;&#24687;&#21644;&#23450;&#24615;&#30446;&#26631;&#12290;&#22312;&#24067;&#23572;&#35266;&#23519;&#21338;&#24328;&#20013;&#65292;&#27599;&#20010;&#29609;&#23478;&#19982;&#19968;&#32452;&#21629;&#39064;&#21464;&#37327;&#30456;&#20851;&#32852;&#65292;&#21482;&#26377;&#23427;&#33021;&#22815;&#35266;&#23519;&#21040;&#21464;&#37327;&#30340;&#20540;&#65292;&#24182;&#19988;&#23427;&#25511;&#21046;&#30528;&#26159;&#21542;&#20197;&#21450;&#21521;&#35841;&#36879;&#38706;&#35813;&#20540;&#12290;&#23427;&#19981;&#33021;&#25511;&#21046;&#32473;&#23450;&#30340;&#22266;&#23450;&#21464;&#37327;&#20540;&#12290;&#24067;&#23572;&#35266;&#23519;&#21338;&#24328;&#26159;&#24067;&#23572;&#21338;&#24328;&#30340;&#19968;&#31181;&#27867;&#21270;&#65292;&#24067;&#23572;&#21338;&#24328;&#26159;&#19968;&#31181;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#31574;&#30053;&#21338;&#24328;&#23376;&#31867;&#65292;&#20294;&#20855;&#26377;&#23436;&#20840;&#20449;&#24687;&#65292;&#27599;&#20010;&#29609;&#23478;&#25511;&#21046;&#20854;&#21464;&#37327;&#30340;&#20540;&#12290;&#22312;&#24067;&#23572;&#35266;&#23519;&#21338;&#24328;&#20013;&#65292;&#29609;&#23478;&#30340;&#30446;&#26631;&#25551;&#36848;&#20102;&#22810;&#26234;&#33021;&#20307;&#23545;&#21464;&#37327;&#30340;&#30693;&#35782;&#12290;&#19982;&#32463;&#20856;&#30340;&#31574;&#30053;&#21338;&#24328;&#31867;&#20284;&#65292;&#29609;&#23478;&#21516;&#26102;&#36873;&#25321;&#20182;&#20204;&#30340;&#31574;&#30053;&#65292;&#22240;&#27492;&#35266;&#23519;&#21338;&#24328;&#25429;&#25417;&#21040;&#20102;&#19981;&#23436;&#32654;&#21644;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#26041;&#38754;&#12290;&#23427;&#20204;&#38656;&#35201;&#25512;&#29702;&#20851;&#20110;&#32473;&#23450;&#21464;&#37327;&#26080;&#27861;&#21306;&#20998;&#30340;&#19968;&#32452;&#20272;&#20540;&#30340;&#32467;&#26524;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Boolean Observation Games, a subclass of multi-player finite strategic games with incomplete information and qualitative objectives. In Boolean observation games, each player is associated with a finite set of propositional variables of which only it can observe the value, and it controls whether and to whom it can reveal that value. It does not control the given, fixed, value of variables. Boolean observation games are a generalization of Boolean games, a well-studied subclass of strategic games but with complete information, and wherein each player controls the value of its variables.   In Boolean observation games, player goals describe multi-agent knowledge of variables. As in classical strategic games, players choose their strategies simultaneously and therefore observation games capture aspects of both imperfect and incomplete information. They require reasoning about sets of outcomes given sets of indistinguishable valuations of variables. An outcome relation betwee
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#24049;&#25552;&#20379;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#65292;&#36824;&#21487;&#20197;&#20026;&#33258;&#24049;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#23545;Llama 2 70B&#27169;&#22411;&#30340;&#19977;&#27425;&#36845;&#20195;&#24494;&#35843;&#65292;&#32467;&#26524;&#22312;AlpacaEval 2.0&#25490;&#34892;&#27036;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#29616;&#26377;&#31995;&#32479;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#23454;&#29616;&#33021;&#22815;&#19981;&#26029;&#33258;&#25105;&#25913;&#36827;&#30340;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10020</link><description>&lt;p&gt;
&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Rewarding Language Models. (arXiv:2401.10020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10020
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#24049;&#25552;&#20379;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#65292;&#36824;&#21487;&#20197;&#20026;&#33258;&#24049;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#23545;Llama 2 70B&#27169;&#22411;&#30340;&#19977;&#27425;&#36845;&#20195;&#24494;&#35843;&#65292;&#32467;&#26524;&#22312;AlpacaEval 2.0&#25490;&#34892;&#27036;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#29616;&#26377;&#31995;&#32479;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#23454;&#29616;&#33021;&#22815;&#19981;&#26029;&#33258;&#25105;&#25913;&#36827;&#30340;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20551;&#35774;&#35201;&#23454;&#29616;&#36229;&#20154;&#32423;&#30340;&#26234;&#33021;&#20307;&#65292;&#26410;&#26469;&#30340;&#27169;&#22411;&#38656;&#35201;&#36229;&#20154;&#32423;&#30340;&#21453;&#39304;&#65292;&#20197;&#25552;&#20379;&#36275;&#22815;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#26159;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#20250;&#21463;&#21040;&#20154;&#31867;&#34920;&#29616;&#27700;&#24179;&#30340;&#38480;&#21046;&#65292;&#32780;&#19988;&#36825;&#20123;&#29420;&#31435;&#30340;&#20923;&#32467;&#22870;&#21169;&#27169;&#22411;&#22312;LLM&#35757;&#32451;&#36807;&#31243;&#20013;&#26080;&#27861;&#23398;&#20064;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#36890;&#36807;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#30340;&#25552;&#31034;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#33258;&#24049;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36845;&#20195;DPO&#35757;&#32451;&#20013;&#65292;&#19981;&#20165;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#32780;&#19988;&#33021;&#22815;&#20026;&#33258;&#24049;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#23545;Llama 2 70B&#36827;&#34892;&#25105;&#20204;&#26041;&#27861;&#30340;&#19977;&#27425;&#36845;&#20195;&#30340;&#24494;&#35843;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;AlpacaEval 2.0&#25490;&#34892;&#27036;&#19978;&#32988;&#36807;&#35768;&#22810;&#29616;&#26377;&#31995;&#32479;&#65292;&#21253;&#25324;Claude 2&#12289;Gemini Pro&#21644;GPT-4 0613&#12290;&#34429;&#28982;&#36825;&#21482;&#26159;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;&#21487;&#33021;&#23454;&#29616;&#33021;&#22815;&#19981;&#26029;&#33258;&#25105;&#25913;&#36827;&#30340;&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continuall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09071</link><description>&lt;p&gt;
&#29992;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#37325;&#26032;&#24605;&#32771;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering. (arXiv:2401.09071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35770;&#19978;&#22312;&#35889;&#22495;&#20013;&#26377;&#24456;&#22909;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#23454;&#38469;&#19978;&#20381;&#36182;&#20110;&#22810;&#39033;&#24335;&#36924;&#36817;&#65292;&#24847;&#21619;&#30528;&#23427;&#20204;&#19982;&#31354;&#38388;&#22495;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;&#30001;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#24456;&#23569;&#20174;&#31354;&#38388;&#35282;&#24230;&#30740;&#31350;&#35889;&#22270;GNN&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#31354;&#38388;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#65292;&#20363;&#22914;&#65292;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#22495;&#20013;&#23454;&#38469;&#19978;&#32534;&#30721;&#20102;&#21738;&#20123;&#20449;&#24687;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#22312;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#30340;&#20869;&#22312;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#26126;&#30830;&#22320;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#36866;&#24212;&#24615;&#26032;&#22270;&#12290;&#29702;&#35770;&#21644;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24212;&#24615;&#26032;&#22270;&#19981;&#20165;&#34920;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#36824;&#33021;&#22815;&#23481;&#32435;&#26377;&#31526;&#21495;&#30340;&#36793;&#26435;&#37325;&#20197;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded in the spectral domain, their practical reliance on polynomial approximation implies a profound linkage to the spatial domain. As previous studies rarely examine spectral GNNs from the spatial perspective, their spatial-domain interpretability remains elusive, e.g., what information is essentially encoded by spectral GNNs in the spatial domain? In this paper, to answer this question, we establish a theoretical connection between spectral filtering and spatial aggregation, unveiling an intrinsic interaction that spectral filtering implicitly leads the original graph to an adapted new graph, explicitly computed for spatial aggregation. Both theoretical and empirical investigations reveal that the adapted new graph not only exhibits non-locality but also accommodates signed edge weights to reflect label consistency between nodes. These findings thus highlight the interpretable role of spectral GNNs in the spatial 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36965;&#24863;&#22270;&#20687;&#21644;&#22810;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#22478;&#24066;&#21151;&#33021;&#21306;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#28385;&#36275;&#31227;&#21160;&#20114;&#32852;&#32593;&#22312;&#32447;&#21040;&#31163;&#32447;&#19994;&#21153;&#30340;&#31934;&#30830;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.06550</link><description>&lt;p&gt;
&#36890;&#36807;&#36965;&#24863;&#22270;&#20687;&#21644;&#22810;&#35821;&#20041;&#20449;&#24687;&#26816;&#27979;&#22478;&#24066;&#21151;&#33021;&#21306;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multimodal Learning for detecting urban functional zones using remote sensing image and multi-semantic information. (arXiv:2401.06550v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36965;&#24863;&#22270;&#20687;&#21644;&#22810;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#22478;&#24066;&#21151;&#33021;&#21306;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#28385;&#36275;&#31227;&#21160;&#20114;&#32852;&#32593;&#22312;&#32447;&#21040;&#31163;&#32447;&#19994;&#21153;&#30340;&#31934;&#30830;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#20852;&#36259;&#21306;&#65288;AOI&#65289;&#26159;&#25351;&#20855;&#26377;&#23450;&#20041;&#36793;&#30028;&#30340;&#25972;&#21512;&#30340;&#22478;&#24066;&#21151;&#33021;&#21306;&#22495;&#12290;&#22478;&#24066;&#21830;&#19994;&#30340;&#36805;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#23545;&#23450;&#20041;AOI&#30340;&#26356;&#31934;&#30830;&#35201;&#27714;&#30340;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#22478;&#24066;&#35268;&#21010;&#25110;&#21306;&#22495;&#32463;&#27982;&#20998;&#26512;&#30340;&#24191;&#27867;AOI&#25366;&#25496;&#65292;&#26410;&#33021;&#28385;&#36275;&#31227;&#21160;&#20114;&#32852;&#32593;&#22312;&#32447;&#21040;&#31163;&#32447;&#19994;&#21153;&#30340;&#31934;&#30830;&#35201;&#27714;&#12290;&#36825;&#20123;&#19994;&#21153;&#38656;&#35201;&#21040;&#20855;&#20307;&#30340;&#31038;&#21306;&#12289;&#23398;&#26657;&#25110;&#21307;&#38498;&#30340;&#20934;&#30830;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#36965;&#24863;&#22270;&#20687;&#21644;&#22810;&#35821;&#20041;&#21442;&#32771;&#20449;&#24687;&#26816;&#27979;AOI&#22260;&#26639;&#22810;&#36793;&#24418;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21253;&#21547;&#21160;&#24577;&#20154;&#21592;&#27969;&#21160;&#21644;&#29289;&#27969;&#22320;&#22336;&#20449;&#24687;&#30340;&#32423;&#32852;&#27169;&#22359;&#26469;&#35780;&#20272;&#20854;&#26102;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#36873;&#25321;&#29305;&#23450;&#31867;&#21035;&#30340;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#24320;&#22987;&#65292;&#24182;&#29992;&#23427;&#26469;&#21484;&#22238;&#30456;&#24212;&#30340;&#36965;&#24863;&#22270;&#20687;&#12289;&#38468;&#36817;&#30340;POI&#12289;&#36947;&#36335;n
&lt;/p&gt;
&lt;p&gt;
Urban area-of-interest (AOI) refers to an integrated urban functional zone with defined boundaries. The rapid development of urban commerce has resulted in an increased demand for more precise requirements in defining AOIs. However, existing research primarily concentrates on broad AOI mining for urban planning or regional economic analysis, failing to cater to the precise requirements of mobile Internet online-to-offline businesses. These businesses necessitate accuracy down to a specific community, school, or hospital. In this paper, we propose an end-to-end multimodal deep learning algorithm for detecting AOI fence polygon using remote sensing images and multi-semantics reference information. We then evaluate its timeliness through a cascaded module that incorporates dynamic human mobility and logistics address information. Specifically, we begin by selecting a point-of-interest (POI) of specific category, and use it to recall corresponding remote sensing images, nearby POIs, road n
&lt;/p&gt;</description></item><item><title>RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04679</link><description>&lt;p&gt;
RoSA: &#36890;&#36807;&#40065;&#26834;&#36866;&#24212;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04679
&lt;/p&gt;
&lt;p&gt;
RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32972;&#26223;&#19979;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#39044;&#31639;&#19979;&#25552;&#20379;&#33391;&#22909;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843; (PEFT) &#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;RoSA&#65292;&#21463;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#30340;&#21551;&#21457;&#65292;&#23427;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#20849;&#21516;&#35757;&#32451;$\textit{&#20302;&#31209;}$&#21644;$\textit{&#39640;&#24230;&#31232;&#30095;}$&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RoSA&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#23567;&#23398;&#25968;&#23398;&#21644;SQL&#26597;&#35810;&#29983;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30456;&#21516;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#65292;RoSA&#20248;&#20110;LoRA&#21644;&#32431;&#31929;&#30340;&#31232;&#30095;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;GPU&#20869;&#26680;&#20026;RoSA&#25552;&#20379;&#31995;&#32479;&#25903;&#25345;&#65292;&#20197;&#34917;&#20805;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;https://github.com/IST-DASLab&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#22266;&#23450;&#39044;&#31639;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#26469;&#25512;&#33616;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#26469;&#34913;&#37327;&#25512;&#33616;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03756</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#22266;&#23450;&#39044;&#31639;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65306;&#36866;&#24212;&#24615;&#23454;&#39564;&#35774;&#35745;&#19982;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Fixed-Budget Best Arm Identification: Adaptive Experimental Design with Policy Learning. (arXiv:2401.03756v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#22266;&#23450;&#39044;&#31639;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#26469;&#25512;&#33616;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#26469;&#34913;&#37327;&#25512;&#33616;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#26159;&#22522;&#20110;&#35777;&#25454;&#30340;&#20915;&#31574;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#24102;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;Best Arm Identification, BAI&#65289;&#38382;&#39064;&#26469;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#32473;&#23450;&#22810;&#20010;&#27835;&#30103;&#33218;&#30340;&#33258;&#36866;&#24212;&#35797;&#39564;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#20915;&#31574;&#32773;&#35266;&#23519;&#19968;&#20010;&#21051;&#30011;&#23454;&#39564;&#21333;&#20301;&#30340;&#19978;&#19979;&#25991;&#65288;&#21327;&#21464;&#37327;&#65289;&#65292;&#24182;&#23558;&#35813;&#21333;&#20301;&#20998;&#37197;&#32473;&#20854;&#20013;&#19968;&#20010;&#27835;&#30103;&#33218;&#12290;&#22312;&#23454;&#39564;&#32467;&#26463;&#26102;&#65292;&#20915;&#31574;&#32773;&#25512;&#33616;&#19968;&#20010;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#39044;&#35745;&#20135;&#29983;&#26368;&#39640;&#26399;&#26395;&#32467;&#26524;&#30340;&#27835;&#30103;&#33218;&#65288;&#26368;&#20339;&#27835;&#30103;&#33218;&#65289;&#12290;&#35813;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#65288;&#31574;&#30053;&#36951;&#25022;&#65289;&#26469;&#34913;&#37327;&#65292;&#35813;&#36951;&#25022;&#34920;&#31034;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#65292;&#26368;&#20339;&#27835;&#30103;&#33218;&#21644;&#25512;&#33616;&#27835;&#30103;&#33218;&#30340;&#26465;&#20214;&#26399;&#26395;&#32467;&#26524;&#20043;&#38388;&#30340;&#26368;&#22823;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#21021;&#22987;&#27493;&#39588;&#26159;&#25512;&#23548;&#26368;&#22351;&#24773;&#20917;&#19979;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#30340;&#28176;&#36817;&#19979;&#30028;&#65292;&#35813;&#19979;&#30028;&#36824;&#26263;&#31034;&#30528;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19968;&#20123;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individualized treatment recommendation is a crucial task in evidence-based decision-making. In this study, we formulate this task as a fixed-budget best arm identification (BAI) problem with contextual information. In this setting, we consider an adaptive experiment given multiple treatment arms. At each round, a decision-maker observes a context (covariate) that characterizes an experimental unit and assigns the unit to one of the treatment arms. At the end of the experiment, the decision-maker recommends a treatment arm estimated to yield the highest expected outcome conditioned on a context (best treatment arm). The effectiveness of this decision is measured in terms of the worst-case expected simple regret (policy regret), which represents the largest difference between the conditional expected outcomes of the best and recommended treatment arms given a context. Our initial step is to derive asymptotic lower bounds for the worst-case expected simple regret, which also implies idea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.02740</link><description>&lt;p&gt;
&#20026;&#22810;&#20219;&#21153;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20844;&#24179;&#24615;&#24863;&#30693;&#30340;&#20316;&#19994;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fairness-Aware Job Scheduling for Multi-Job Federated Learning. (arXiv:2401.02740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#65288;&#21363;FL&#23458;&#25143;&#31471;&#65289;&#33021;&#22815;&#22312;&#19981;&#27844;&#38706;&#25935;&#24863;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FL&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22404;&#26029;&#22330;&#26223;&#65292;&#22312;&#35813;&#22330;&#26223;&#20013;&#65292;&#21333;&#20010;FL&#26381;&#21153;&#22120;&#22312;&#27599;&#36718;&#35757;&#32451;&#20013;&#36873;&#25321;&#19968;&#37096;&#20998;FL&#23458;&#25143;&#31471;&#26469;&#26356;&#26032;&#20854;&#26412;&#22320;&#27169;&#22411;&#12290;&#23454;&#38469;&#19978;&#65292;&#21487;&#33021;&#20250;&#26377;&#22810;&#20010;FL&#26381;&#21153;&#22120;&#21516;&#26102;&#23581;&#35797;&#20174;&#21516;&#19968;&#20010;&#27744;&#20013;&#36873;&#25321;&#23458;&#25143;&#31471;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#22522;&#20110;Lyapunov&#20248;&#21270;&#65292;&#23427;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#24403;&#21069;&#38656;&#27714;&#21644;&#20316;&#19994;&#20184;&#27454;&#20986;&#20215;&#65292;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#20197;&#38450;&#27490;&#31561;&#24453;&#26102;&#38388;&#36807;&#38271;&#12290;&#22522;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#23545;FairFedJS&#19982;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#20248;&#21183;&#12290;&#23427;&#22312;&#24179;&#22343;&#19978;&#20987;&#36133;&#20102;&#26368;&#20339;&#22522;&#20934;&#32447;31.9%&#21644;1.0%&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to collaboratively train machine learning models without disclosing sensitive private data. Existing FL research mostly focuses on the monopoly scenario in which a single FL server selects a subset of FL clients to update their local models in each round of training. In practice, there can be multiple FL servers simultaneously trying to select clients from the same pool. In this paper, we propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS) approach to bridge this gap. Based on Lyapunov optimization, it ensures fair allocation of high-demand FL client datasets to FL jobs in need of them, by jointly considering the current demand and the job payment bids, in order to prevent prolonged waiting. Extensive experiments comparing FairFedJS against four state-of-the-art approaches on two datasets demonstrate its significant advantages. It outperforms the best baseline by 31.9% and 1.0% on avera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#35786;&#26029;&#24102;&#26377;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#36724;&#25215;&#25925;&#38556;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.01172</link><description>&lt;p&gt;
&#25391;&#21160;&#20449;&#21495;&#30340;&#20108;&#27425;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#29992;&#20110;&#35786;&#26029;&#36724;&#25215;&#25925;&#38556;
&lt;/p&gt;
&lt;p&gt;
Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing Bearing Faults. (arXiv:2401.01172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#35786;&#26029;&#24102;&#26377;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#36724;&#25215;&#25925;&#38556;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36724;&#25215;&#25925;&#38556;&#30340;&#35786;&#26029;&#23545;&#20110;&#38477;&#20302;&#32500;&#20462;&#25104;&#26412;&#21644;&#35774;&#22791;&#20572;&#26426;&#33267;&#20851;&#37325;&#35201;&#12290;&#36724;&#25215;&#25925;&#38556;&#26159;&#26426;&#22120;&#25391;&#21160;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#20998;&#26512;&#20854;&#20449;&#21495;&#24418;&#24577;&#21487;&#20197;&#25581;&#31034;&#20854;&#20581;&#24247;&#29366;&#20917;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#25511;&#21046;&#29615;&#22659;&#36827;&#34892;&#20248;&#21270;&#65292;&#24573;&#30053;&#20102;&#23454;&#38469;&#26465;&#20214;&#19979;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#36716;&#36895;&#21644;&#25391;&#21160;&#30340;&#38750;&#24179;&#31283;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#19979;&#35786;&#26029;&#36724;&#25215;&#25925;&#38556;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#36724;&#25215;&#25925;&#38556;&#24341;&#36215;&#30340;&#25391;&#21160;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#38750;&#24179;&#31283;&#24615;&#19982;&#36724;&#25215;&#22266;&#26377;&#21644;&#25805;&#20316;&#21442;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#38416;&#36848;&#20102;&#20108;&#27425;&#26102;&#38388;&#39057;&#29575;&#20998;&#24067;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38388;&#39057;&#29575;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosis of bearing faults is paramount to reducing maintenance costs and operational breakdowns. Bearing faults are primary contributors to machine vibrations, and analyzing their signal morphology offers insights into their health status. Unfortunately, existing approaches are optimized for controlled environments, neglecting realistic conditions such as time-varying rotational speeds and the vibration's non-stationary nature. This paper presents a fusion of time-frequency analysis and deep learning techniques to diagnose bearing faults under time-varying speeds and varying noise levels. First, we formulate the bearing fault-induced vibrations and discuss the link between their non-stationarity and the bearing's inherent and operational parameters. We also elucidate quadratic time-frequency distributions and validate their effectiveness in resolving distinctive dynamic patterns associated with different bearing faults. Based on this, we design a time-frequency convolutional neural n
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31639;&#27861;&#36924;&#30495;&#24615;&#21644;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;LLMs&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#24635;&#32479;&#25237;&#31080;&#34892;&#20026;&#65292;&#20294;&#22312;&#20934;&#30830;&#34920;&#31034;&#20840;&#29699;&#21464;&#26262;&#35266;&#28857;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;LLMs&#23545;&#26576;&#20123;&#32676;&#20307;&#30340;&#35266;&#28857;&#20272;&#35745;&#23384;&#22312;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27954;&#35028;&#32654;&#22269;&#20154;&#23545;&#20840;&#29699;&#21464;&#26262;&#30340;&#25285;&#24551;&#20302;&#20272;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;LLMs&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#20934;&#30830;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00217</link><description>&lt;p&gt;
&#33021;&#21542;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#20840;&#29699;&#21464;&#26262;&#30340;&#20844;&#20247;&#24847;&#35265;&#65311;&#19968;&#39033;&#20851;&#20110;&#31639;&#27861;&#36924;&#30495;&#24615;&#21644;&#20559;&#35265;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias. (arXiv:2311.00217v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31639;&#27861;&#36924;&#30495;&#24615;&#21644;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;LLMs&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#24635;&#32479;&#25237;&#31080;&#34892;&#20026;&#65292;&#20294;&#22312;&#20934;&#30830;&#34920;&#31034;&#20840;&#29699;&#21464;&#26262;&#35266;&#28857;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;LLMs&#23545;&#26576;&#20123;&#32676;&#20307;&#30340;&#35266;&#28857;&#20272;&#35745;&#23384;&#22312;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27954;&#35028;&#32654;&#22269;&#20154;&#23545;&#20840;&#29699;&#21464;&#26262;&#30340;&#25285;&#24551;&#20302;&#20272;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;LLMs&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#20934;&#30830;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#30340;&#24863;&#30693;&#21644;&#34892;&#20026;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#34987;&#31216;&#20026;&#31639;&#27861;&#36924;&#30495;&#24615;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;&#27668;&#20505;&#21464;&#21270;&#35843;&#26597;&#35780;&#20272;LLMs&#30340;&#31639;&#27861;&#36924;&#30495;&#24615;&#21644;&#20559;&#35265;&#12290;LLMs&#34987;&#26465;&#20214;&#21270;&#20026;&#22522;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;/&#25110;&#24515;&#29702;&#21327;&#21464;&#37327;&#26469;&#27169;&#25311;&#35843;&#26597;&#22238;&#31572;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#30456;&#20851;&#21327;&#21464;&#37327;&#27809;&#26377;&#21253;&#21547;&#22312;&#20869;&#26102;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#24635;&#32479;&#25237;&#31080;&#34892;&#20026;&#65292;&#20294;&#22312;&#20934;&#30830;&#34920;&#31034;&#20840;&#29699;&#21464;&#26262;&#35266;&#28857;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#24403;LLMs&#34987;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#21327;&#21464;&#37327;&#21516;&#26102;&#26465;&#20214;&#21270;&#26102;&#65292;GPT-4&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#29305;&#23450;&#32676;&#20307;&#30340;&#35266;&#28857;&#20272;&#35745;&#23384;&#22312;&#24046;&#24322;&#65292;LLMs&#20542;&#21521;&#20110;&#20302;&#20272;&#38750;&#27954;&#35028;&#32654;&#22269;&#20154;&#23545;&#20840;&#29699;&#21464;&#26262;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#31361;&#20986;&#20102;LLMs&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#31934;&#30830;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated their potential in social science research by emulating human perceptions and behaviors, a concept referred to as algorithmic fidelity. This study assesses the algorithmic fidelity and bias of LLMs by utilizing two nationally representative climate change surveys. The LLMs were conditioned on demographics and/or psychological covariates to simulate survey responses. The findings indicate that LLMs can effectively capture presidential voting behaviors but encounter challenges in accurately representing global warming perspectives when relevant covariates are not included. GPT-4 exhibits improved performance when conditioned on both demographics and covariates. However, disparities emerge in LLM estimations of the views of certain groups, with LLMs tending to underestimate worry about global warming among Black Americans. While highlighting the potential of LLMs to aid social science research, these results underscore the importance of metic
&lt;/p&gt;</description></item><item><title>VFedMH&#26159;&#19968;&#31181;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#32858;&#21512;&#21442;&#19982;&#32773;&#30340;&#23884;&#20837;&#26469;&#22788;&#29702;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#24322;&#26500;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;VFL&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.13367</link><description>&lt;p&gt;
VFedMH: &#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#29992;&#20110;&#35757;&#32451;&#22810;&#21442;&#19982;&#26041;&#24322;&#26500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VFedMH: Vertical Federated Learning for Training Multi-party Heterogeneous Models. (arXiv:2310.13367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13367
&lt;/p&gt;
&lt;p&gt;
VFedMH&#26159;&#19968;&#31181;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#32858;&#21512;&#21442;&#19982;&#32773;&#30340;&#23884;&#20837;&#26469;&#22788;&#29702;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#24322;&#26500;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;VFL&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#20316;&#20026;&#19968;&#31181;&#38598;&#25104;&#26679;&#26412;&#23545;&#40784;&#21644;&#29305;&#24449;&#21512;&#24182;&#30340;&#26032;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;VFL&#26041;&#27861;&#22312;&#22788;&#29702;&#21442;&#19982;&#32773;&#20043;&#38388;&#23384;&#22312;&#24322;&#26500;&#26412;&#22320;&#27169;&#22411;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#24433;&#21709;&#20102;&#20248;&#21270;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VFedMH&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#26041;&#24322;&#26500;&#27169;&#22411;&#12290;VFedMH&#30340;&#37325;&#28857;&#26159;&#22312;&#21069;&#21521;&#20256;&#25773;&#26399;&#38388;&#32858;&#21512;&#27599;&#20010;&#21442;&#19982;&#32773;&#30693;&#35782;&#30340;&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#20013;&#38388;&#32467;&#26524;&#12290;&#20027;&#21160;&#26041;&#65292;&#25317;&#26377;&#26679;&#26412;&#30340;&#26631;&#31614;&#21644;&#29305;&#24449;&#65292;&#22312;VFedMH&#20013;&#23433;&#20840;&#22320;&#32858;&#21512;&#26412;&#22320;&#23884;&#20837;&#20197;&#33719;&#24471;&#20840;&#23616;&#30693;&#35782;&#23884;&#20837;&#65292;&#24182;&#23558;&#20854;&#21457;&#36865;&#32473;&#34987;&#21160;&#26041;&#12290;&#34987;&#21160;&#26041;&#20165;&#25317;&#26377;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#21033;&#29992;&#20840;&#23616;&#23884;&#20837;&#22312;&#20854;&#26412;&#22320;&#24322;&#26500;&#32593;&#32476;&#19978;&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#34987;&#21160;&#26041;&#19981;&#25317;&#26377;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) has gained increasing attention as a novel training paradigm that integrates sample alignment and feature union. However, existing VFL methods face challenges when dealing with heterogeneous local models among participants, which affects optimization convergence and generalization. To address this issue, this paper proposes a novel approach called Vertical Federated learning for training Multi-parties Heterogeneous models (VFedMH). VFedMH focuses on aggregating the embeddings of each participant's knowledge instead of intermediate results during forward propagation. The active party, who possesses labels and features of the sample, in VFedMH securely aggregates local embeddings to obtain global knowledge embeddings, and sends them to passive parties. The passive parties, who own only features of the sample, then utilize the global embeddings to propagate forward on their local heterogeneous networks. However, the passive party does not own the labels, 
&lt;/p&gt;</description></item><item><title>Lag-Llama&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35757;&#32451;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#27169;&#22411;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#25193;&#23637;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.08278</link><description>&lt;p&gt;
Lag-Llama: &#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lag-Llama: Towards Foundation Models for Time Series Forecasting. (arXiv:2310.08278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08278
&lt;/p&gt;
&lt;p&gt;
Lag-Llama&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35757;&#32451;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#27169;&#22411;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#25193;&#23637;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#24182;&#30740;&#31350;&#20854;&#25193;&#23637;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#20013;&#30340; Lag-Llama &#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#8220;&#20998;&#24067;&#22806;&#8221;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#20248;&#20110;&#26377;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#25193;&#23637;&#34892;&#20026;&#12290;&#24320;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/kashif/pytorch-transformer-ts &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming to build foundation models for time-series forecasting and study their scaling behavior, we present here our work-in-progress on Lag-Llama, a general-purpose univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The model shows good zero-shot prediction capabilities on unseen "out-of-distribution" time-series datasets, outperforming supervised baselines. We use smoothly broken power-laws to fit and predict model scaling behavior. The open source code is made available at https://github.com/kashif/pytorch-transformer-ts.
&lt;/p&gt;</description></item><item><title>COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04353</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04353
&lt;/p&gt;
&lt;p&gt;
COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#19982;&#22806;&#37096;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25511;&#21046;&#20219;&#21153;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents, which use a large language model (LLM) capable of in-context learning to interact with an external environment, have recently emerged as a promising approach to control tasks. We present the first language-agent approach to formal theorem-proving. Our method, COPRA, uses a high-capacity, black-box LLM (GPT-4) as part of a policy for a stateful backtracking search. During the search, the policy can select proof tactics and retrieve lemmas and definitions from an external database. Each selected tactic is executed in the underlying proof framework, and the execution feedback is used to build the prompt for the next policy invocation. The search also tracks selected information from its history and uses it to reduce hallucinations and unnecessary LLM queries.  We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the Compcert project. On these benchmarks, COPRA is significantly better than one-shot invocations of GPT-4, as well as state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#29983;&#25104;&#22270;&#20687;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#32473;&#23450;&#23545;&#35805;&#32972;&#26223;&#19979;&#29983;&#25104;&#19968;&#33268;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2309.15516</link><description>&lt;p&gt;
&#25945;&#25480;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Teaching Text-to-Image Models to Communicate. (arXiv:2309.15516v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#29983;&#25104;&#22270;&#20687;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#32473;&#23450;&#23545;&#35805;&#32972;&#26223;&#19979;&#29983;&#25104;&#19968;&#33268;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#20013;&#65292;&#21508;&#31181;&#24037;&#20316;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#34429;&#28982;&#29616;&#26377;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#26159;&#22312;&#30452;&#25509;&#24212;&#29992;&#20110;&#23545;&#35805;&#29983;&#25104;&#22270;&#20687;&#26102;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#31361;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#23545;&#35805;&#21040;&#22270;&#20687;&#29983;&#25104;&#65292;&#21363;&#22312;&#32473;&#23450;&#23545;&#35805;&#32972;&#26223;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#24212;&#35813;&#29983;&#25104;&#19968;&#20010;&#19982;&#25351;&#23450;&#23545;&#35805;&#20869;&#23481;&#19968;&#33268;&#30340;&#36924;&#30495;&#22270;&#20687;&#20316;&#20026;&#22238;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20013;&#38388;&#36716;&#25442;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#21462;&#23545;&#35805;&#20013;&#21253;&#21547;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#32771;&#34385;&#21040;&#23545;&#35805;&#32467;&#26500;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#22312;&#23545;&#35805;&#20013;&#30340;&#27599;&#20010;&#35828;&#35805;&#22238;&#21512;&#20043;&#21069;&#25918;&#32622;&#20998;&#21106;&#26631;&#35760;&#65292;&#20197;&#21306;&#20998;&#19981;&#21516;&#30340;&#21457;&#35328;&#32773;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#22788;&#29702;&#21518;&#30340;&#23545;&#35805;&#32972;&#26223;&#29983;&#25104;&#22270;&#20687;&#12290;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#19982;&#22788;&#29702;&#21518;&#23545;&#35805;&#29615;&#22659;&#30456;&#19968;&#33268;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various works have been extensively studied in the research of text-to-image generation. Although existing models perform well in text-to-image generation, there are significant challenges when directly employing them to generate images in dialogs. In this paper, we first highlight a new problem: dialog-to-image generation, that is, given the dialog context, the model should generate a realistic image which is consistent with the specified conversation as response. To tackle the problem, we propose an efficient approach for dialog-to-image generation without any intermediate translation, which maximizes the extraction of the semantic information contained in the dialog. Considering the characteristics of dialog structure, we put segment token before each sentence in a turn of a dialog to differentiate different speakers. Then, we fine-tune pre-trained text-to-image models to enable them to generate images conditioning on processed dialog context. After fine-tuning, our approach can con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE&#26041;&#27861;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;ELBO&#65292;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21644;PatchGAN&#37492;&#21035;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#12290;</title><link>http://arxiv.org/abs/2309.13160</link><description>&lt;p&gt;
GAMIX-VAE: &#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE
&lt;/p&gt;
&lt;p&gt;
GAMIX-VAE: A VAE with Gaussian Mixture Based Posterior. (arXiv:2309.13160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE&#26041;&#27861;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;ELBO&#65292;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21644;PatchGAN&#37492;&#21035;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#29983;&#25104;&#24314;&#27169;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#22522;&#30707;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;VAEs&#30340;&#19968;&#20010;&#32454;&#24494;&#26041;&#38754;&#65292;&#37325;&#28857;&#26159;&#35299;&#37322;KL Divergence&#65292;&#36825;&#26159;Evidence Lower Bound&#65288;ELBO&#65289;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#25511;&#21046;&#20102;&#37325;&#26500;&#20934;&#30830;&#24615;&#21644;&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#34429;&#28982;KL Divergence&#35753;&#28508;&#21464;&#37327;&#20998;&#24067;&#19982;&#20808;&#39564;&#20998;&#24067;&#23545;&#40784;&#65292;&#32473;&#25972;&#20010;&#28508;&#31354;&#38388;&#21152;&#19978;&#32467;&#26500;&#32422;&#26463;&#65292;&#20294;&#21364;&#19981;&#38480;&#21046;&#21508;&#20010;&#21464;&#37327;&#20998;&#24067;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37325;&#26032;&#23450;&#20041;&#20102;&#24102;&#26377;&#39640;&#26031;&#28151;&#21512;&#30340;&#21518;&#39564;&#27010;&#29575;&#30340;ELBO&#65292;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#39033;&#20197;&#38450;&#27490;&#26041;&#24046;&#23849;&#28291;&#65292;&#24182;&#20351;&#29992;PatchGAN&#37492;&#21035;&#22120;&#26469;&#22686;&#24378;&#32441;&#29702;&#36924;&#30495;&#24230;&#12290;&#23454;&#29616;&#32454;&#33410;&#28041;&#21450;Encoder&#21644;Decoder&#30340;ResNetV2&#26550;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#30340;&#33021;&#21147;&#65292;&#20026;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Autoencoders (VAEs) have become a cornerstone in generative modeling and representation learning within machine learning. This paper explores a nuanced aspect of VAEs, focusing on interpreting the Kullback Leibler (KL) Divergence, a critical component within the Evidence Lower Bound (ELBO) that governs the trade-off between reconstruction accuracy and regularization. While the KL Divergence enforces alignment between latent variable distributions and a prior imposing a structure on the overall latent space but leaves individual variable distributions unconstrained. The proposed method redefines the ELBO with a mixture of Gaussians for the posterior probability, introduces a regularization term to prevent variance collapse, and employs a PatchGAN discriminator to enhance texture realism. Implementation details involve ResNetV2 architectures for both the Encoder and Decoder. The experiments demonstrate the ability to generate realistic faces, offering a promising solution for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiscoSCMs&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#26597;&#35810;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#26469;&#35299;&#20915;&#19968;&#33268;&#24615;&#35268;&#21017;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#20998;&#26512;&#20010;&#24615;&#21270;&#28608;&#21169;&#22330;&#26223;&#20013;&#30340;&#28508;&#22312;&#32467;&#26524;&#26102;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#29420;&#31435;&#28508;&#22312;&#22122;&#22768;&#26465;&#20214;&#65292;&#21487;&#20197;&#25552;&#39640;&#35299;&#20915;Layer 3&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09323</link><description>&lt;p&gt;
&#29992;DiscoSCMs&#22238;&#31572;Layer 3&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Answering Layer 3 queries with DiscoSCMs. (arXiv:2309.09323v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiscoSCMs&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#26597;&#35810;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#26469;&#35299;&#20915;&#19968;&#33268;&#24615;&#35268;&#21017;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#20998;&#26512;&#20010;&#24615;&#21270;&#28608;&#21169;&#22330;&#26223;&#20013;&#30340;&#28508;&#22312;&#32467;&#26524;&#26102;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#29420;&#31435;&#28508;&#22312;&#22122;&#22768;&#26465;&#20214;&#65292;&#21487;&#20197;&#25552;&#39640;&#35299;&#20915;Layer 3&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#20013;&#65292;&#35299;&#20915;Pearl&#22240;&#26524;&#23618;&#27425;&#65288;PCH&#65289;&#19979;&#30340;&#20851;&#32852;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#30340;&#22240;&#26524;&#26597;&#35810;&#26159;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#33268;&#24615;&#35268;&#21017;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20998;&#24067;&#19968;&#33268;&#24615;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;DiscoSCMs&#65289;&#65292;&#25193;&#23637;&#20102;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#21644;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#12290;&#20197;&#20010;&#24615;&#21270;&#28608;&#21169;&#22330;&#26223;&#20013;&#28508;&#22312;&#32467;&#26524;&#30340;&#30456;&#20851;&#27169;&#24335;$P(y_x, y'_{x'})$&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#23613;&#31649;&#21453;&#20107;&#23454;&#19981;&#20877;&#36864;&#21270;&#65292;&#20294;&#20173;&#26080;&#27861;&#30830;&#23450;&#12290;&#22240;&#27492;&#65292;&#23558;&#29420;&#31435;&#28508;&#22312;&#22122;&#22768;&#26465;&#20214;&#32435;&#20837;DiscoSCM&#12290;&#21457;&#29616;&#36890;&#36807;&#36866;&#24212;&#20998;&#24067;&#30340;&#23884;&#20837;&#24335;&#25512;&#26029;&#65292;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#35299;&#20915;Layer 3&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing causal queries across the Pearl Causal Hierarchy (PCH) (i.e., associational, interventional and counterfactual), which is formalized as \Layer{} Valuations, is a central task in contemporary causal inference research. Counterfactual questions, in particular, pose a significant challenge as they often necessitate a complete knowledge of structural equations. This paper identifies \textbf{the degeneracy problem} caused by the consistency rule. To tackle this, the \textit{Distribution-consistency Structural Causal Models} (DiscoSCMs) is introduced, which extends both the structural causal models (SCM) and the potential outcome framework. The correlation pattern of potential outcomes in personalized incentive scenarios, described by $P(y_x, y'_{x'})$, is used as a case study for elucidation. Although counterfactuals are no longer degenerate, they remain indeterminable. As a result, the condition of independent potential noise is incorporated into DiscoSCM. It is found that by ad
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#65292;&#21457;&#29616;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#25216;&#26415;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#26377;&#25928;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07034</link><description>&lt;p&gt;
&#22914;&#20309;&#65288;&#19981;&#65289;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#20351;&#29992;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
How (Not) to Use Sociodemographic Information for Subjective NLP Tasks. (arXiv:2309.07034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#65292;&#21457;&#29616;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#25216;&#26415;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#26377;&#25928;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#37322;&#32773;&#30340;&#31038;&#20250;&#20154;&#21475;&#32972;&#26223;&#65288;&#21363;&#24615;&#21035;&#65292;&#24180;&#40836;&#65292;&#25945;&#32946;&#32972;&#26223;&#31561;&#20010;&#20307;&#32452;&#25104;&#65289;&#23545;&#20854;&#22312;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#27604;&#22914;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12290;&#36890;&#24120;&#65292;&#24322;&#36136;&#30340;&#32972;&#26223;&#20250;&#23548;&#33268;&#39640;&#24230;&#20998;&#27495;&#12290;&#20026;&#20102;&#24314;&#27169;&#36825;&#31181;&#24046;&#24322;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#31181;&#25216;&#26415;&#23558;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#20855;&#26377;&#29305;&#23450;&#31038;&#20250;&#20154;&#21475;&#29305;&#24449;&#30340;&#20154;&#31867;&#21487;&#33021;&#32473;&#20986;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NLP&#25991;&#29486;&#23545;&#36825;&#31181;&#25216;&#26415;&#30340;&#25928;&#26524;&#23384;&#22312;&#20998;&#27495; - &#23427;&#20173;&#28982;&#19981;&#28165;&#26970;&#23427;&#33021;&#22312;&#21738;&#20123;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#26377;&#24110;&#21161;&#65292;&#24182;&#19988;&#35780;&#20272;&#20165;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#21644;&#26368;&#20840;&#38754;&#30340;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#30740;&#31350;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19971;&#20010;&#25968;&#25454;&#38598;&#21644;&#20845;&#20010;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#30340;&#27169;&#22411;&#23478;&#26063;&#20013;&#30340;&#20960;&#20010;&#25552;&#31034;&#24418;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#31038;&#20250;&#20154;&#21475;&#25552;&#31034;&#23545;&#26576;&#20123;&#20219;&#21153;&#26377;&#25928;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotators' sociodemographic backgrounds (i.e., the individual compositions of their gender, age, educational background, etc.) have a strong impact on their decisions when working on subjective NLP tasks, such as hate speech detection. Often, heterogeneous backgrounds result in high disagreements. To model this variation, recent work has explored sociodemographic prompting, a technique, which steers the output of prompt-based models towards answers that humans with specific sociodemographic profiles would give. However, the available NLP literature disagrees on the efficacy of this technique -- it remains unclear, for which tasks and scenarios it can help and evaluations are limited to specific tasks only. We address this research gap by presenting the largest and most comprehensive study of sociodemographic prompting today. Concretely, we evaluate several prompt formulations across seven datasets and six instruction-tuned model families. We find that (1) while sociodemographic prompt
&lt;/p&gt;</description></item><item><title>chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.14120</link><description>&lt;p&gt;
&#25480;&#26435;&#20020;&#24202;&#21307;&#29983;&#24182;&#27665;&#20027;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#20020;&#24202;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290; (arXiv:2308.14120v2 [cs.LG] &#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14120
&lt;/p&gt;
&lt;p&gt;
chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24320;&#21457;&#32773;&#65288;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#65289;&#21644;&#20174;&#19994;&#32773;&#65288;&#22914;&#20020;&#24202;&#21307;&#29983;&#65289;&#20043;&#38388;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#65292;&#38459;&#30861;&#20102;ML&#22312;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;chatGPT Advanced Data Analysis&#65288;ADA&#65289;&#65292;&#21363;GPT-4&#30340;&#25193;&#23637;&#65292;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#24182;&#39640;&#25928;&#25191;&#34892;ML&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21521;chatGPT ADA&#25552;&#20379;&#20102;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#30340;&#22823;&#22411;&#35797;&#39564;&#30340;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21644;&#30740;&#31350;&#35814;&#32454;&#20449;&#24687;&#65292;&#27809;&#26377;&#32473;&#20986;&#20855;&#20307;&#25351;&#23548;&#12290;ChatGPT ADA&#22522;&#20110;&#21407;&#22987;&#30740;&#31350;&#30340;&#35757;&#32451;&#25968;&#25454;&#33258;&#20027;&#24320;&#21457;&#20102;&#26368;&#20808;&#36827;&#30340;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#65292;&#22914;&#30284;&#30151;&#21457;&#23637;&#12289;&#30284;&#30151;&#36827;&#23637;&#12289;&#30142;&#30149;&#24182;&#21457;&#30151;&#25110;&#33268;&#30149;&#22522;&#22240;&#24207;&#21015;&#31561;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;ML&#27169;&#22411;&#19982;&#20854;&#24050;&#21457;&#34920;&#30340;&#23545;&#24212;&#29289;&#30456;&#21305;&#37197;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;chatGPT ADA&#20026;&#27665;&#20027;&#21270;&#21307;&#23398;&#20013;&#30340;ML&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#20351;&#38750;ML&#19987;&#23478;&#33021;&#22815;&#33719;&#24471;&#20808;&#36827;&#30340;&#20998;&#26512;&#24037;&#20855;&#24182;&#25512;&#21160;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;DRL&#26041;&#27861;&#22312;MAPF&#20013;&#30340;&#25972;&#21512;&#65292;&#24182;&#35299;&#20915;&#20102;MAPF&#35299;&#20915;&#26041;&#26696;&#35780;&#20272;&#25351;&#26631;&#32570;&#20047;&#32479;&#19968;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;DRL&#20316;&#20026;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;MAPF&#24403;&#21069;&#25361;&#25112;&#25152;&#38656;&#30340;&#22522;&#30784;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.05893</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#22242;&#38431;&#23548;&#33322;&#65306;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22312;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding. (arXiv:2308.05893v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;DRL&#26041;&#27861;&#22312;MAPF&#20013;&#30340;&#25972;&#21512;&#65292;&#24182;&#35299;&#20915;&#20102;MAPF&#35299;&#20915;&#26041;&#26696;&#35780;&#20272;&#25351;&#26631;&#32570;&#20047;&#32479;&#19968;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;DRL&#20316;&#20026;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;MAPF&#24403;&#21069;&#25361;&#25112;&#25152;&#38656;&#30340;&#22522;&#30784;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#26159;&#35768;&#22810;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#36890;&#24120;&#26159;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#22522;&#26412;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#21644;&#25317;&#25380;&#30340;&#29615;&#22659;&#20013;&#65292;MAPF&#30340;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#24050;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#38477;&#20302;&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#26412;&#32508;&#36848;&#35770;&#25991;&#20013;&#37325;&#28857;&#20171;&#32461;&#20102;DRL&#26041;&#27861;&#22312;MAPF&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#30446;&#21069;&#22312;&#35780;&#20272;MAPF&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#32570;&#21475;&#65292;&#36890;&#36807;&#35299;&#20915;&#32570;&#20047;&#32479;&#19968;&#35780;&#20272;&#25351;&#26631;&#30340;&#38382;&#39064;&#24182;&#23545;&#36825;&#20123;&#25351;&#26631;&#36827;&#34892;&#20840;&#38754;&#38416;&#37322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#35752;&#35770;&#20102;&#20316;&#20026;&#26410;&#26469;&#26041;&#21521;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;DRL&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#22522;&#30784;&#29702;&#35299;&#20197;&#24212;&#23545;MAPF&#20013;&#30340;&#24403;&#21069;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation metrics and providing comprehensive clarification on these metrics. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;ChatGPT&#21644;Stack Overflow&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#30340;&#29305;&#28857;&#21644;&#21487;&#29992;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22238;&#31572;&#20013;&#26377;52%&#38169;&#35823;&#65292;77%&#20887;&#38271;&#65292;&#20294;&#30001;&#20110;&#20854;&#32508;&#21512;&#24615;&#21644;&#28165;&#26224;&#30340;&#35821;&#35328;&#34920;&#36798;&#65292;&#20173;&#28982;&#22312;39.34%&#30340;&#24773;&#20917;&#19979;&#34987;&#20351;&#29992;&#32773;&#20559;&#22909;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.02312</link><description>&lt;p&gt;
&#35841;&#22238;&#31572;&#30340;&#26356;&#22909;&#65311;&#23545;ChatGPT&#21644;Stack Overflow&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Who Answers It Better? An In-Depth Analysis of ChatGPT and Stack Overflow Answers to Software Engineering Questions. (arXiv:2308.02312v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;ChatGPT&#21644;Stack Overflow&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#30340;&#29305;&#28857;&#21644;&#21487;&#29992;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22238;&#31572;&#20013;&#26377;52%&#38169;&#35823;&#65292;77%&#20887;&#38271;&#65292;&#20294;&#30001;&#20110;&#20854;&#32508;&#21512;&#24615;&#21644;&#28165;&#26224;&#30340;&#35821;&#35328;&#34920;&#36798;&#65292;&#20173;&#28982;&#22312;39.34%&#30340;&#24773;&#20917;&#19979;&#34987;&#20351;&#29992;&#32773;&#20559;&#22909;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Q&amp;A&#24179;&#21488;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#19968;&#30452;&#26159;&#31243;&#24207;&#21592;&#32593;&#19978;&#27714;&#21161;&#34892;&#20026;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;ChatGPT&#30340;&#25512;&#20986;&#65292;&#32593;&#19978;&#27714;&#21161;&#34892;&#20026;&#30340;&#33539;&#24335;&#27491;&#22312;&#21457;&#29983;&#21464;&#21270;&#12290;&#23613;&#31649;ChatGPT&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#23578;&#26410;&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#26469;&#35780;&#20272;ChatGPT&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#30340;&#29305;&#28857;&#25110;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;ChatGPT&#22238;&#31572;517&#20010;Stack Overflow&#65288;SO&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#23545;ChatGPT&#22238;&#31572;&#30340;&#27491;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#32508;&#21512;&#24615;&#21644;&#31616;&#27905;&#24615;&#36827;&#34892;&#20102;&#26816;&#26597;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#20998;&#26512;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;ChatGPT&#22238;&#31572;&#22312;&#35821;&#35328;&#21644;&#20154;&#31867;&#26041;&#38754;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;52&#65285;&#30340;ChatGPT&#22238;&#31572;&#26159;&#38169;&#35823;&#30340;&#65292;77&#65285;&#30340;&#22238;&#31572;&#20887;&#38271;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30001;&#20110;&#20854;&#32508;&#21512;&#24615;&#21644;&#28165;&#26224;&#30340;&#35821;&#35328;&#34920;&#36798;&#65292;ChatGPT&#22238;&#31572;&#20173;&#28982;&#22312;39.34&#65285;&#30340;&#24773;&#20917;&#19979;&#21463;&#21040;&#38738;&#30544;&#12290;
&lt;/p&gt;
&lt;p&gt;
Q&amp;A platforms have been an integral part of the web-help-seeking behavior of programmers over the past decade. However, with the recent introduction of ChatGPT, the paradigm of web-help-seeking behavior is experiencing a shift. Despite the popularity of ChatGPT, no comprehensive study has been conducted to evaluate the characteristics or usability of ChatGPT's answers to software engineering questions. To bridge the gap, we conducted the first in-depth analysis of ChatGPT's answers to 517 Stack Overflow (SO) questions and examined the correctness, consistency, comprehensiveness, and conciseness of ChatGPT's answers. Furthermore, we conducted a large-scale linguistic analysis, and a user study to understand the characteristics of ChatGPT answers from linguistic and human aspects. Our analysis shows that 52\% of ChatGPT answers are incorrect and 77\% are verbose. Nonetheless, ChatGPT answers are still preferred 39.34\% of the time due to their comprehensiveness and well-articulated langu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.00031</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00031
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;&#36817;&#21313;&#24180;&#26469;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#26368;&#20196;&#20154;&#20852;&#22859;&#30340;&#21457;&#23637;&#20043;&#19968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24050;&#32463;&#25104;&#20026;&#38750;&#24120;&#25104;&#21151;&#30340;&#33539;&#24335;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;RL&#24212;&#29992;&#20110;&#29983;&#25104;AI&#20013;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35752;&#35770;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65292;&#21363;&#20316;&#20026;&#19968;&#31181;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#65292;&#20316;&#20026;&#19968;&#31181;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#65292;&#20197;&#21450;&#20316;&#20026;&#19968;&#31181;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#36731;&#26494;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#35843;&#26597;&#32467;&#26524;&#20013;&#23545;&#36825;&#20010;&#36855;&#20154;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.07084</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;Wasserstein&#21464;&#20998;&#25512;&#29702;&#65306;&#21487;&#35299;&#37322;&#24615;&#30340;&#24418;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#25110;&#26368;&#20248;&#25511;&#21046;&#21487;&#20197;&#20026;&#20855;&#26377;&#21487;&#21464;&#21160;&#24577;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#25552;&#20379;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#23454;&#26045;&#20013;&#65292;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#21644;&#30456;&#24212;&#30340;&#26368;&#20248;&#31574;&#30053;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#23558;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#22240;&#20026;&#27010;&#29575;&#25512;&#29702;&#21407;&#21017;&#19978;&#25552;&#20379;&#20102;&#22810;&#26679;&#19988;&#24378;&#22823;&#30340;&#25968;&#23398;&#24037;&#20855;&#26469;&#25512;&#26029;&#38543;&#26426;&#21160;&#24577;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#26469;&#35299;&#37322;&#22870;&#21169;&#35774;&#35745;&#65292;&#36879;&#26126;&#22320;&#35757;&#32451;&#25910;&#25947;&#65292;&#20197;&#21450;&#23545;&#39034;&#24207;&#20915;&#31574;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#20026;&#20102;&#35777;&#26126;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25910;&#25947;&#35757;&#32451;&#24182;&#20445;&#35777;&#20102;&#25910;&#25947;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning or optimal control can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and corresponding optimal policy. Consequently, formalizing the sequential decision-making problems as inference has a considerable value, as probabilistic inference in principle offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of the reward design and policy convergence. In this study, we propose a novel Adaptive Wasserstein Variational Optimization (AWaVO) to tackle these challenges in sequential decision-making. Our approach utilizes formal methods to provide interpretations of reward design, transparency of training convergence, and probabilistic interpretation of sequential decisions. To demonstrate practicality, we show convergent training with guara
&lt;/p&gt;</description></item><item><title>S2vNTM&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20851;&#38190;&#35789;&#30340;&#27169;&#24335;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#20027;&#39064;&#65292;&#24182;&#20248;&#21270;&#20027;&#39064;&#20851;&#38190;&#35789;&#38598;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#36895;&#24230;&#33267;&#23569;&#27604;&#22522;&#32447;&#27169;&#22411;&#24555;&#20004;&#20493;&#12290;</title><link>http://arxiv.org/abs/2307.04804</link><description>&lt;p&gt;
S2vNTM: &#21322;&#30417;&#30563;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
S2vNTM: Semi-supervised vMF Neural Topic Modeling. (arXiv:2307.04804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04804
&lt;/p&gt;
&lt;p&gt;
S2vNTM&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20851;&#38190;&#35789;&#30340;&#27169;&#24335;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#20027;&#39064;&#65292;&#24182;&#20248;&#21270;&#20027;&#39064;&#20851;&#38190;&#35789;&#38598;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#36895;&#24230;&#33267;&#23569;&#27604;&#22522;&#32447;&#27169;&#22411;&#24555;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#26469;&#35828;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65306;&#65288;1&#65289;&#24456;&#38590;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#65292;&#27604;&#22914;&#20851;&#38190;&#35789;&#65307;&#65288;2&#65289;&#35757;&#32451;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65307;&#65288;3&#65289;&#20381;&#36182;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21322;&#30417;&#30563;vMF&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#65288;S2vNTM&#65289;&#26469;&#20811;&#26381;&#36825;&#20123;&#22256;&#38590;&#12290;S2vNTM&#23558;&#19968;&#20123;&#31181;&#23376;&#20851;&#38190;&#35789;&#20316;&#20026;&#20027;&#39064;&#30340;&#36755;&#20837;&#12290;S2vNTM&#21033;&#29992;&#20851;&#38190;&#35789;&#30340;&#27169;&#24335;&#26469;&#35782;&#21035;&#28508;&#22312;&#30340;&#20027;&#39064;&#65292;&#24182;&#20248;&#21270;&#20027;&#39064;&#20851;&#38190;&#35789;&#38598;&#30340;&#36136;&#37327;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;S2vNTM&#22312;&#25552;&#20379;&#26377;&#38480;&#20851;&#38190;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20998;&#31867;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;S2vNTM&#33267;&#23569;&#27604;&#22522;&#32447;&#27169;&#22411;&#24555;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model based methods are powerful techniques for text classification. However, the models have several shortcomings. (1) It is difficult to integrate human knowledge such as keywords. (2) It needs a lot of resources to train the models. (3) It relied on large text data to pretrain. In this paper, we propose Semi-Supervised vMF Neural Topic Modeling (S2vNTM) to overcome these difficulties. S2vNTM takes a few seed keywords as input for topics. S2vNTM leverages the pattern of keywords to identify potential topics, as well as optimize the quality of topics' keywords sets. Across a variety of datasets, S2vNTM outperforms existing semi-supervised topic modeling methods in classification accuracy with limited keywords provided. S2vNTM is at least twice as fast as baselines.
&lt;/p&gt;</description></item><item><title>NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14405</link><description>&lt;p&gt;
NeuralMatrix: &#23558;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#31227;&#21160;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix: Moving Entire Neural Networks to General Matrix Multiplication for Efficient Inference. (arXiv:2305.14405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14405
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NeuralMatrix&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#22810;&#21151;&#33021;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#22522;&#20110;ASIC&#30340;&#21152;&#36895;&#22120;&#30340;&#19987;&#29992;&#24615;&#38480;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;CPU&#21644;GPU&#31561;&#36890;&#29992;&#22788;&#29702;&#22120;&#30456;&#27604;&#30340;&#24212;&#29992;&#29305;&#23450;&#21152;&#36895;&#27700;&#24179;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;DNN&#35745;&#31639;&#20013;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36816;&#31639;&#26144;&#23556;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#21450;&#20351;&#29992;GEMM&#21152;&#36895;&#22120;&#23545;DNN&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19977;&#31181;&#27969;&#34892;&#31867;&#21035;&#30340;&#21508;&#31181;DNN&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65288;&#21363;CNN&#65292;Transformers&#21644;GNN&#65289;&#20316;&#20026;&#31034;&#20363;&#30340;&#25903;&#25745;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;DNN&#36716;&#25442;&#20026;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21518;&#20165;&#20250;&#20986;&#29616;&#39640;&#36798;2.02&#65285;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#65292;&#21516;&#26102;&#23558;&#21534;&#21520;&#37327;&#19982;&#21151;&#29575;&#30340;&#27604;&#20540;&#19982;CPU&#21644;GPU&#30456;&#27604;&#25552;&#39640;&#20102;113&#20493;&#21040;19.44&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce NeuralMatrix, a novel framework that enables the computation of versatile deep neural networks (DNNs) on a single general matrix multiplication (GEMM) accelerator. The proposed approach overcomes the specificity limitations of ASIC-based accelerators while achieving application-specific acceleration levels compared to general-purpose processors such as CPUs and GPUs. We address the challenges of mapping both linear and nonlinear operations in DNN computation to general matrix multiplications and the impact of using a GEMM accelerator on DNN inference accuracy. Extensive experiments are conducted on various DNN models from three popular categories (i.e., CNN, Transformers, and GNN) as illustrative backbone models. Our results demonstrate that DNNs suffer only up to a 2.02% accuracy loss after being converted to general matrix multiplication, while achieving 113x to 19.44x improvements in throughput per power compared to CPUs and GPUs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#20851;&#27880;&#26426;&#21046;&#21644;&#21452;&#21521;LSTM&#32593;&#32476;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13105</link><description>&lt;p&gt;
&#21033;&#29992;&#23460;&#20869;WiFi&#31995;&#32479;&#36827;&#34892;&#26080;&#35774;&#22791;&#31359;&#22681;&#23384;&#22312;&#26816;&#27979;&#30340;&#27880;&#24847;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System. (arXiv:2304.13105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#20851;&#27880;&#26426;&#21046;&#21644;&#21452;&#21521;LSTM&#32593;&#32476;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20934;&#30830;&#26816;&#27979;&#20154;&#21592;&#23384;&#22312;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#33021;&#28304;&#31649;&#29702;&#21644;&#23433;&#20840;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#30340;&#36890;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21517;&#20026;&#27880;&#24847;&#21147;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#65288;ALPD&#65289;&#65292;&#37319;&#29992;&#20851;&#27880;&#26426;&#21046;&#20174;CSI&#25968;&#25454;&#20013;&#33258;&#21160;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#23376;&#36733;&#27874;&#65292;&#24182;&#37319;&#29992;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#25429;&#25417;CSI&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#38745;&#24577;&#29305;&#24449;&#26469;&#25552;&#39640;&#38745;&#24577;&#29366;&#24577;&#19979;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#37096;&#32626;&#19968;&#23545;WiFi&#25509;&#20837;&#28857;&#65288;AP&#65289;&#26469;&#25910;&#38598;CSI&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;ALPD&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36827;&#19968;&#27493;&#19982;&#20960;&#20010;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ALPD&#31995;&#32479;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#21452;&#21521;&#20256;&#36755;&#25968;&#25454;&#19981;&#20250;&#24433;&#21709;&#25105;&#20204;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate detection of human presence in indoor environments is important for various applications, such as energy management and security. In this paper, we propose a novel system for human presence detection using the channel state information (CSI) of WiFi signals. Our system named attention-enhanced deep learning for presence detection (ALPD) employs an attention mechanism to automatically select informative subcarriers from the CSI data and a bidirectional long short-term memory (LSTM) network to capture temporal dependencies in CSI. Additionally, we utilize a static feature to improve the accuracy of human presence detection in static states. We evaluate the proposed ALPD system by deploying a pair of WiFi access points (APs) for collecting CSI dataset, which is further compared with several benchmarks. The results demonstrate that our ALPD system outperforms the benchmarks in terms of accuracy, especially in the presence of interference. Moreover, bidirectional transmission data 
&lt;/p&gt;</description></item><item><title>Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.10557</link><description>&lt;p&gt;
Transformer&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
An Introduction to Transformers. (arXiv:2304.10557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10557
&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12290;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;Transformer&#30340;&#20171;&#32461;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#32570;&#23569;&#23545;&#20854;&#26550;&#26500;&#30340;&#31934;&#30830;&#25968;&#23398;&#25551;&#36848;&#65292;&#20854;&#35774;&#35745;&#36873;&#25321;&#30340;&#30452;&#35273;&#20063;&#24120;&#24120;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#30740;&#31350;&#36335;&#24452;&#30340;&#26354;&#25240;&#65292;Transformer&#37096;&#20214;&#30340;&#35299;&#37322;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer is a neural network component that can be used to learn useful representations of sequences or sets of datapoints. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;Bradley-Terry-Luce&#21644;Plackett-Luce&#27169;&#22411;&#19979;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#25552;&#20379;&#20102;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#22312;&#35777;&#26126;&#20102;&#30495;&#23454;MLE&#21644;&#20197;&#25104;&#23545;&#27604;&#36739;&#24418;&#24335;&#26367;&#20195;&#30340;&#22791;&#36873;MLE&#37117;&#21487;&#20197;&#22312;PL&#27169;&#22411;&#19979;&#25910;&#25947;&#30340;&#21516;&#26102;&#65292;&#20063;&#34920;&#26126;&#20102;&#30495;&#23454;MLE&#30340;&#39640;&#25928;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;RLHF&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;IRL&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11270</link><description>&lt;p&gt;
&#20351;&#29992;&#26469;&#33258;&#25104;&#23545;&#25110;$K$&#20803;&#27604;&#36739;&#30340;&#20154;&#31867;&#21453;&#39304;&#30340;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons. (arXiv:2301.11270v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;Bradley-Terry-Luce&#21644;Plackett-Luce&#27169;&#22411;&#19979;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#25552;&#20379;&#20102;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#22312;&#35777;&#26126;&#20102;&#30495;&#23454;MLE&#21644;&#20197;&#25104;&#23545;&#27604;&#36739;&#24418;&#24335;&#26367;&#20195;&#30340;&#22791;&#36873;MLE&#37117;&#21487;&#20197;&#22312;PL&#27169;&#22411;&#19979;&#25910;&#25947;&#30340;&#21516;&#26102;&#65292;&#20063;&#34920;&#26126;&#20102;&#30495;&#23454;MLE&#30340;&#39640;&#25928;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;RLHF&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;IRL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#20026;&#32447;&#24615;&#20989;&#25968;&#26102;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#22312;Bradley-Terry-Luce&#65288;BTL&#65289;&#27169;&#22411;&#21644;Plackett-Luce&#65288;PL&#65289;&#27169;&#22411;&#19979;&#22343;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#22522;&#20110;&#23398;&#24471;&#30340;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#26102;&#65292;MLE&#20250;&#22833;&#36133;&#65292;&#32780;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#25552;&#20379;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;PL&#27169;&#22411;&#19979;&#65292;&#30495;&#23454;MLE&#21644;&#23558;$k$&#20803;&#27604;&#36739;&#25286;&#20998;&#20026;&#25104;&#23545;&#27604;&#36739;&#30340;&#22791;&#36873;MLE&#37117;&#25910;&#25947;&#12290;&#32780;&#30495;&#23454;MLE&#26159;&#28176;&#36817;&#26356;&#20026;&#39640;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#29616;&#26377;RLHF&#31639;&#27861;&#65288;&#22914;InstructGPT&#65289;&#30340;&#23454;&#39564;&#25104;&#21151;&#65292;&#24182;&#20026;&#31639;&#27861;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;&#26368;&#22823;&#29109;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;(IRL)&#38382;&#39064;&#65292;&#24182;&#20026;&#20854;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the $K$-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and max-entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#38656;&#27714;&#21644;&#21512;&#20316;&#20248;&#21270;&#22522;&#20110;&#20998;&#37197;&#31574;&#30053;&#30340;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#65292;&#20174;&#32780;&#25913;&#21892;SAMS&#36710;&#38431;&#30340;&#26381;&#21153;&#36136;&#37327;&#21644;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#36739;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.08659</link><description>&lt;p&gt;
&#38754;&#21521;&#20849;&#20139;&#33258;&#21160;&#39550;&#39542;&#20986;&#34892;&#26381;&#21153;&#30340;&#39044;&#27979;&#24335;&#36710;&#38431;&#35843;&#24230;&#65306;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Anticipatory Fleet Repositioning for Shared-use Autonomous Mobility Services: An Optimization and Learning-Based Approach. (arXiv:2210.08659v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#38656;&#27714;&#21644;&#21512;&#20316;&#20248;&#21270;&#22522;&#20110;&#20998;&#37197;&#31574;&#30053;&#30340;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#65292;&#20174;&#32780;&#25913;&#21892;SAMS&#36710;&#38431;&#30340;&#26381;&#21153;&#36136;&#37327;&#21644;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#36739;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20986;&#34892;&#26381;&#21153;&#12289;&#20016;&#23500;&#30340;&#20132;&#36890;&#25968;&#25454;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#21457;&#23637;&#20026;&#20849;&#20139;&#33258;&#21160;&#39550;&#39542;&#20986;&#34892;&#26381;&#21153;&#65288;SAMS&#65289;&#25552;&#20379;&#20102;&#37325;&#35201;&#26426;&#36935;&#65292;&#20197;&#25552;&#20379;&#21487;&#35775;&#38382;&#21644;&#38656;&#27714;&#21709;&#24212;&#24615;&#30340;&#20010;&#20154;&#20986;&#34892;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#39044;&#27979;&#24615;&#22320;&#37325;&#26032;&#35843;&#24230;&#31354;&#38386;&#30340;&#36710;&#36742;&#65292;&#25552;&#39640;SAMS&#36710;&#38431;&#30340;&#25928;&#29575;&#21644;&#26381;&#21153;&#36136;&#37327;&#12290;&#25226;&#20877;&#24179;&#34913;&#38382;&#39064;&#20316;&#20026;&#39532;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#20248;&#21183;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;A2C&#65289;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#65292;&#23427;&#39044;&#27979;&#26410;&#26469;&#38656;&#27714;&#24182;&#19982;&#22522;&#20110;&#20248;&#21270;&#30340;&#20998;&#37197;&#31574;&#30053;&#21512;&#20316;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#38598;&#20013;&#24335;&#30340;&#37325;&#26032;&#23450;&#20301;&#20915;&#31574;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#22823;&#22411;&#27773;&#36710;&#36710;&#38431;&#65292;&#22240;&#20026;&#38382;&#39064;&#22823;&#23567;&#19981;&#36229;&#36807;Fleetsize^2&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of mobility-on-demand services, rich transportation data sources, and autonomous vehicles (AVs) creates significant opportunities for shared-use AV mobility services (SAMSs) to provide accessible and demand-responsive personal mobility. SAMS fleet operation involves multiple interrelated decisions, with a primary focus on efficiently fulfilling passenger ride requests with a high level of service quality. This paper focuses on improving the efficiency and service quality of a SAMS vehicle fleet via anticipatory repositioning of idle vehicles. The rebalancing problem is formulated as a Markov Decision Process, which we propose solving using an advantage actor critic (A2C) reinforcement learning-based method. The proposed approach learns a rebalancing policy that anticipates future demand and cooperates with an optimization-based assignment strategy. The approach allows for centralized repositioning decisions and can handle large vehicle fleets since the problem size does
&lt;/p&gt;</description></item></channel></rss>