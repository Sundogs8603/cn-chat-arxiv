<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01586</link><description>&lt;p&gt;
TrustAgent: &#36890;&#36807;&#20195;&#29702;&#26500;&#25104;&#23454;&#29616;&#23433;&#20840;&#21487;&#20449;&#36182;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#21487;&#20449;&#24230;&#20173;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#30001;&#20110;&#20195;&#29702;&#21487;&#20197;&#30452;&#25509;&#19982;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#65292;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#23545;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#32500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#31181;&#31574;&#30053;&#65306;&#39044;&#20808;&#35268;&#21010;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#20043;&#21069;&#21521;&#27169;&#22411;&#27880;&#20837;&#23433;&#20840;&#30693;&#35782;&#65307;&#35268;&#21010;&#36807;&#31243;&#20013;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#26102;&#22686;&#24378;&#23433;&#20840;&#24615;&#65307;&#35745;&#21010;&#21518;&#26816;&#26597;&#31574;&#30053;&#65292;&#36890;&#36807;&#35745;&#21010;&#21518;&#26816;&#26597;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#36890;&#36807;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#26377;&#25928;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#20854;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
&lt;/p&gt;</description></item><item><title>&#23454;&#29616;&#20102;&#39318;&#20010;&#22522;&#20110;GPU&#30340;LTL&#23398;&#20064;&#22120;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#26522;&#20030;&#24335;&#31243;&#24207;&#21512;&#25104;&#65292;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#22120;&#65292;&#22788;&#29702;&#36319;&#36394;&#33267;&#23569;&#22810;2048&#20493;&#65292;&#36895;&#24230;&#24179;&#22343;&#24555;46&#20493;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#20855;&#26377;$O(\log n)$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#26080;&#20998;&#25903;LTL semantics&#12290;</title><link>https://arxiv.org/abs/2402.12373</link><description>&lt;p&gt;
&#22522;&#20110;GPU&#30340;LTL&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LTL learning on GPUs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12373
&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20102;&#39318;&#20010;&#22522;&#20110;GPU&#30340;LTL&#23398;&#20064;&#22120;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#26522;&#20030;&#24335;&#31243;&#24207;&#21512;&#25104;&#65292;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#22120;&#65292;&#22788;&#29702;&#36319;&#36394;&#33267;&#23569;&#22810;2048&#20493;&#65292;&#36895;&#24230;&#24179;&#22343;&#24555;46&#20493;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#20855;&#26377;$O(\log n)$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#26080;&#20998;&#25903;LTL semantics&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#24207;&#36923;&#36753;&#65288;LTL&#65289;&#22312;&#24037;&#19994;&#39564;&#35777;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;LTL&#20844;&#24335;&#21487;&#20197;&#20174;&#36319;&#36394;&#20013;&#23398;&#20064;&#12290;&#25193;&#23637;LTL&#20844;&#24335;&#23398;&#20064;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#31532;&#19968;&#31181;&#22522;&#20110;GPU&#30340;LTL&#23398;&#20064;&#22120;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26522;&#20030;&#24335;&#31243;&#24207;&#21512;&#25104;&#12290;&#35813;&#23398;&#20064;&#22120;&#26159;&#23436;&#22791;&#21644;&#27491;&#30830;&#30340;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;&#23427;&#22788;&#29702;&#30340;&#36319;&#36394;&#33267;&#23569;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#22120;&#22810;2048&#20493;&#65292;&#24179;&#22343;&#33267;&#23569;&#24555;46&#20493;&#12290;&#36825;&#26159;&#36890;&#36807;&#35832;&#22810;&#26041;&#27861;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#20855;&#26377;$O(\log n)$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#26032;&#22411;&#26080;&#20998;&#25903;LTL&#35821;&#20041;&#65292;&#20854;&#20013;$n$&#26159;&#36319;&#36394;&#38271;&#24230;&#65292;&#32780;&#20197;&#21069;&#30340;&#23454;&#29616;&#26159;$O(n^2)$&#25110;&#26356;&#31967;&#65288;&#20551;&#35774;&#25353;&#20301;&#24067;&#23572;&#36816;&#31639;&#21644;&#25353;2&#30340;&#24130;&#31227;&#20301;&#20855;&#26377;&#21333;&#20301;&#25104;&#26412;&#8212;&#8212;&#36825;&#26159;&#23545;&#29616;&#20195;&#22788;&#29702;&#22120;&#30340;&#29616;&#23454;&#20551;&#35774;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12373v1 Announce Type: cross  Abstract: Linear temporal logic (LTL) is widely used in industrial verification. LTL formulae can be learned from traces. Scaling LTL formula learning is an open problem. We implement the first GPU-based LTL learner using a novel form of enumerative program synthesis. The learner is sound and complete. Our benchmarks indicate that it handles traces at least 2048 times more numerous, and on average at least 46 times faster than existing state-of-the-art learners. This is achieved with, among others, novel branch-free LTL semantics that has $O(\log n)$ time complexity, where $n$ is trace length, while previous implementations are $O(n^2)$ or worse (assuming bitwise boolean operations and shifts by powers of 2 have unit costs -- a realistic assumption on modern processors).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;ANALOBENCH&#22522;&#20934;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#31867;&#27604;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25193;&#23637;LMs&#35268;&#27169;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#38271;&#22330;&#26223;&#25110;&#30456;&#20851;&#32463;&#39564;&#22238;&#24518;&#30340;&#31867;&#27604;&#26102;&#24102;&#26469;&#30340;&#24615;&#33021;&#25552;&#21319;&#36739;&#23567;&#12290;</title><link>https://arxiv.org/abs/2402.12370</link><description>&lt;p&gt;
AnaloBench&#65306;&#35780;&#20272;&#25277;&#35937;&#21644;&#38271;&#19978;&#19979;&#25991;&#31867;&#27604;&#35782;&#21035;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12370
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;ANALOBENCH&#22522;&#20934;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#31867;&#27604;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25193;&#23637;LMs&#35268;&#27169;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#38271;&#22330;&#26223;&#25110;&#30456;&#20851;&#32463;&#39564;&#22238;&#24518;&#30340;&#31867;&#27604;&#26102;&#24102;&#26469;&#30340;&#24615;&#33021;&#25552;&#21319;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#32463;&#24120;&#36827;&#34892;&#31867;&#27604;&#24605;&#32500;&#65292;&#23558;&#20010;&#20154;&#32463;&#39564;&#19982;&#24403;&#21069;&#24773;&#20917;&#32852;&#31995;&#36215;&#26469;&#65288;$X$&#31867;&#20284;&#20110;$Y$&#26159;&#22240;&#20026;$Z$&#65289;&#12290;&#31867;&#27604;&#24605;&#32500;&#20351;&#20154;&#31867;&#33021;&#22815;&#29992;&#21019;&#36896;&#24615;&#26041;&#24335;&#35299;&#20915;&#38382;&#39064;&#65292;&#29702;&#35299;&#22256;&#38590;&#27010;&#24565;&#65292;&#26356;&#26377;&#25928;&#22320;&#34920;&#36798;&#24819;&#27861;&#12290;&#33021;&#21542;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20063;&#33021;&#20570;&#21040;&#36825;&#19968;&#28857;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ANALOBENCH&#65292;&#19968;&#20010;&#29992;&#20110;&#30830;&#23450;LMs&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#26041;&#27861;&#19987;&#27880;&#20110;&#20154;&#31867;&#20043;&#38388;&#20849;&#21516;&#30340;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#65306;&#65288;i&#65289;&#20174;&#22823;&#37327;&#20449;&#24687;&#20013;&#22238;&#24518;&#30456;&#20851;&#32463;&#39564;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#23558;&#31867;&#27604;&#25512;&#29702;&#24212;&#29992;&#20110;&#22797;&#26434;&#21644;&#38271;&#24230;&#36739;&#38271;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#22823;&#37327;&#19987;&#26377;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;GPT&#31995;&#21015;&#65292;Claude V2&#65289;&#21644;&#24320;&#28304;&#27169;&#22411;&#65292;&#22914;LLaMA2&#12290;&#19982;&#20808;&#21069;&#30340;&#32467;&#26524;&#19968;&#26679;&#65292;&#25193;&#23637;LMs&#20250;&#24102;&#26469;&#19968;&#20123;&#24615;&#33021;&#25552;&#21319;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#31867;&#27604;&#28041;&#21450;&#38271;&#22330;&#26223;&#25110;&#22238;&#24518;&#30456;&#20851;&#32463;&#39564;&#26102;&#65292;&#35268;&#27169;&#30340;&#25552;&#21319;&#24102;&#26469;&#30340;&#22686;&#30410;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12370v1 Announce Type: cross  Abstract: Humans regularly engage in analogical thinking, relating personal experiences to current situations ($X$ is analogous to $Y$ because of $Z$). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose ANALOBENCH, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We test a broad collection of proprietary models (e.g., GPT family, Claude V2) and open source models such as LLaMA2. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) rec
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36136;&#30097;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;AI&#21453;&#39304;&#20013;&#30340;&#24517;&#35201;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#26356;&#24378;&#30340;&#25945;&#24072;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#21487;&#20197;&#36229;&#36234;&#29616;&#26377;&#30340;RLAIF&#31649;&#36947;&#12290;</title><link>https://arxiv.org/abs/2402.12366</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;AI&#21453;&#39304;&#30340;&#20851;&#38190;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Critical Evaluation of AI Feedback for Aligning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12366
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36136;&#30097;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;AI&#21453;&#39304;&#20013;&#30340;&#24517;&#35201;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#26356;&#24378;&#30340;&#25945;&#24072;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#21487;&#20197;&#36229;&#36234;&#29616;&#26377;&#30340;RLAIF&#31649;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#19982;AI&#21453;&#39304;&#65288;RLAIF&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#25552;&#39640;&#24378;&#22823;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290; RLAIF&#39318;&#20808;&#20351;&#29992;&#26469;&#33258;&#25945;&#24072;&#27169;&#22411;&#30340;&#31034;&#33539;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#65292;&#28982;&#21518;&#20877;&#20351;&#29992;&#26469;&#33258;&#35780;&#35770;&#27169;&#22411;&#30340;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#19968;&#27493;&#24494;&#35843;&#27169;&#22411;&#12290;&#23613;&#31649;&#26368;&#36817;&#27969;&#34892;&#30340;&#24320;&#28304;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#20102;&#20174;RL&#27493;&#39588;&#20013;&#33719;&#24471;&#30340;&#24615;&#33021;&#26174;&#30528;&#25552;&#39640;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#26159;&#21542;&#22797;&#26434;&#30340;RL&#27493;&#39588;&#30495;&#27491;&#26377;&#24517;&#35201;&#20026;AI&#21453;&#39304;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RL&#27493;&#39588;&#30340;&#25913;&#36827;&#20960;&#20046;&#23436;&#20840;&#26159;&#22240;&#20026;&#20351;&#29992;&#36739;&#24369;&#30340;&#25945;&#24072;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-3.5&#65289;&#29992;&#20110;SFT&#25968;&#25454;&#25910;&#38598;&#32780;&#19981;&#26159;&#29992;&#20110;AI&#21453;&#39304;&#29983;&#25104;&#30340;&#35780;&#35770;&#32773;&#65288;&#20363;&#22914;GPT-4&#65289;&#30340;&#24191;&#27867;&#23454;&#36341;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#20197;GPT-4&#20316;&#20026;&#25945;&#24072;&#30340;&#30417;&#30563;&#24494;&#35843;&#20248;&#20110;&#29616;&#26377;&#30340;RLAIF&#31649;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12366v1 Announce Type: cross  Abstract: Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF pipelines. More generally, w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;</title><link>https://arxiv.org/abs/2402.12365</link><description>&lt;p&gt;
&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Universal Physics Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26367;&#20195;&#32773;&#36817;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#23427;&#20204;&#30340;&#25968;&#20540;&#23545;&#24212;&#29289;&#65292;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#21363;&#20351;&#31995;&#32479;&#30340;&#22522;&#30784;&#21160;&#24577;&#30456;&#20284;&#12290;&#19968;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#26159;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#34920;&#36848;&#65292;&#36825;&#20026;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#24314;&#27169;&#22522;&#20110;&#31890;&#23376;&#32780;&#19981;&#26159;&#32593;&#26684;&#30340;&#21160;&#24577;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#27169;&#25311;&#20102;&#19968;&#31995;&#21015;&#26102;&#31354;&#38382;&#39064; - &#23545;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#12290;UPTs&#22312;&#27809;&#26377;&#22522;&#20110;&#32593;&#26684;&#25110;&#22522;&#20110;&#31890;&#23376;&#30340;&#28508;&#22312;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#20174;&#32780;&#22312;&#32593;&#26684;&#21644;&#31890;&#23376;&#20043;&#38388;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;UPTs&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#20256;&#25773;&#21160;&#24577;&#65292;&#24378;&#35843;&#20102;&#36870;&#32534;&#30721;&#21644;&#35299;&#30721;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;UPTs&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12365v1 Announce Type: cross  Abstract: Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space repre
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#35266;&#27979;&#22120;&#65292;&#22312;&#21333;&#27493;&#31934;&#30830;&#35266;&#27979;&#22120;&#32447;&#24615;&#21270;&#26694;&#26550;&#20869;&#23398;&#20064;&#38750;&#32447;&#24615;&#29366;&#24577;&#36716;&#25442;&#26144;&#23556;&#65292;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#19982;&#20256;&#32479;&#24130;&#32423;&#25968;&#25968;&#20540;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;</title><link>https://arxiv.org/abs/2402.12360</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#35266;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Discrete-Time Observers with Physics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12360
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#35266;&#27979;&#22120;&#65292;&#22312;&#21333;&#27493;&#31934;&#30830;&#35266;&#27979;&#22120;&#32447;&#24615;&#21270;&#26694;&#26550;&#20869;&#23398;&#20064;&#38750;&#32447;&#24615;&#29366;&#24577;&#36716;&#25442;&#26144;&#23556;&#65292;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#19982;&#20256;&#32479;&#24130;&#32423;&#25968;&#25968;&#20540;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26469;&#35299;&#20915;&#31163;&#25955;&#26102;&#38388;&#38750;&#32447;&#24615;&#35266;&#27979;&#22120;&#29366;&#24577;&#20272;&#35745;&#38382;&#39064;&#12290;&#25552;&#20986;&#30340;PINN&#26041;&#27861;&#38598;&#25104;&#22312;&#21333;&#27493;&#31934;&#30830;&#35266;&#27979;&#22120;&#32447;&#24615;&#21270;&#26694;&#26550;&#20869;&#65292;&#26088;&#22312;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#38750;&#40784;&#27425;&#27867;&#20989;&#26041;&#31243;&#32452;&#26469;&#23398;&#20064;&#38750;&#32447;&#24615;&#29366;&#24577;&#36716;&#25442;&#26144;&#23556;&#12290;&#36890;&#36807;&#20004;&#20010;&#35828;&#26126;&#24615;&#26696;&#20363;&#30740;&#31350;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;PINN&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#36825;&#20123;&#26696;&#20363;&#65292;&#35266;&#27979;&#22120;&#32447;&#24615;&#21270;&#36716;&#25442;&#26144;&#23556;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#26041;&#27861;&#24471;&#20986;&#12290;&#25105;&#20204;&#36824;&#20026;&#25152;&#25552;&#20986;&#30340;PINN&#26041;&#26696;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20998;&#26512;&#65292;&#24182;&#23558;&#20854;&#19982;&#20381;&#36182;&#20110;&#35745;&#31639;&#24130;&#32423;&#25968;&#35299;&#30340;&#20256;&#32479;&#24130;&#32423;&#25968;&#25968;&#20540;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12360v1 Announce Type: cross  Abstract: We use Physics-Informed Neural Networks (PINNs) to solve the discrete-time nonlinear observer state estimation problem. Integrated within a single-step exact observer linearization framework, the proposed PINN approach aims at learning a nonlinear state transformation map by solving a system of inhomogeneous functional equations. The performance of the proposed PINN approach is assessed via two illustrative case studies for which the observer linearizing transformation map can be derived analytically. We also perform an uncertainty quantification analysis for the proposed PINN scheme and we compare it with conventional power-series numerical implementations, which rely on the computation of a power series solution.
&lt;/p&gt;</description></item><item><title>LoRA+&#36890;&#36807;&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#25913;&#36827;&#21407;&#22987;LoRA&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#24494;&#35843;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.12354</link><description>&lt;p&gt;
LoRA+: &#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#39640;&#25928;&#20302;&#31209;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
LoRA+: Efficient Low Rank Adaptation of Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12354
&lt;/p&gt;
&lt;p&gt;
LoRA+&#36890;&#36807;&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#25913;&#36827;&#21407;&#22987;LoRA&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#24494;&#35843;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26368;&#21021;&#30001;&#32993;&#31561;&#20154;&#65288;2021&#24180;&#65289;&#24341;&#20837;&#65292;&#23548;&#33268;&#23545;&#20855;&#26377;&#22823;&#23485;&#24230;&#65288;&#23884;&#20837;&#32500;&#24230;&#65289;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#34920;&#29616;&#20122;&#20248;&#12290;&#36825;&#26159;&#22240;&#20026;LoRA&#20013;&#30340;&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#20351;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#29575;&#36827;&#34892;&#26356;&#26032;&#12290;&#36890;&#36807;&#23545;&#22823;&#23485;&#24230;&#32593;&#32476;&#36827;&#34892;&#32553;&#25918;&#21442;&#25968;&#30340;&#35770;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#20351;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#29575;&#19981;&#21033;&#20110;&#26377;&#25928;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;LoRA&#30340;&#36825;&#31181;&#27425;&#20248;&#24615;&#21487;&#20197;&#31616;&#21333;&#22320;&#36890;&#36807;&#20026;LoRA&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#20197;&#21450;&#19968;&#20010;&#31934;&#24515;&#36873;&#25321;&#30340;&#27604;&#29575;&#26469;&#36827;&#34892;&#26657;&#27491;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25552;&#20986;&#30340;&#31639;&#27861;&#31216;&#20026;LoRA$+$&#12290;&#22312;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;LoRA$+$&#22312;&#30456;&#21516;&#35745;&#31639;&#25104;&#26412;&#19979;&#25552;&#39640;&#20102;&#24615;&#33021;&#65288;1-2&#65285;&#30340;&#25913;&#36827;&#65289;&#21644;&#24494;&#35843;&#36895;&#24230;&#65288;&#26368;&#22810;&#25552;&#36895;&#32422;2&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12354v1 Announce Type: cross  Abstract: In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;&#20102;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35266;&#23519;&#21040;LLMs&#22312;&#19981;&#21516;&#28216;&#25103;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12348</link><description>&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#35780;&#20272;&#25581;&#31034;LLM&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#30340;GTBench
&lt;/p&gt;
&lt;p&gt;
GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12348
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;&#20102;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35266;&#23519;&#21040;LLMs&#22312;&#19981;&#21516;&#28216;&#25103;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25972;&#21512;&#21040;&#20851;&#38190;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#30340;&#25112;&#30053;&#21644;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#26412;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20363;&#22914;&#65292;&#38656;&#35201;&#32431;&#36923;&#36753;&#21644;&#25112;&#30053;&#25512;&#29702;&#26469;&#19982;&#23545;&#25163;&#31454;&#20105;&#30340;&#26827;&#30424;&#28216;&#25103;&#21644;&#32440;&#29260;&#28216;&#25103;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;GTBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#35821;&#35328;&#39537;&#21160;&#30340;&#29615;&#22659;&#65292;&#21253;&#25324;10&#20010;&#24191;&#27867;&#35748;&#21487;&#30340;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#20840;&#38754;&#30340;&#28216;&#25103;&#20998;&#31867;&#27861;&#65306;&#23436;&#25972;&#20449;&#24687;&#19982;&#19981;&#23436;&#25972;&#20449;&#24687;&#65292;&#21160;&#24577;&#19982;&#38745;&#24577;&#65292;&#20197;&#21450;&#27010;&#29575;&#19982;&#30830;&#23450;&#24615;&#22330;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#34920;&#24449;LLMs&#30340;&#21338;&#24328;&#35770;&#25512;&#29702;&#65307;&#65288;2&#65289;LLM&#23545;&#25239;LLM&#30340;&#27604;&#36187;&#20316;&#20026;&#25512;&#29702;&#35780;&#20272;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65288;1&#65289;LLMs&#22312;&#21508;&#31181;&#28216;&#25103;&#22330;&#26223;&#19979;&#26377;&#19981;&#21516;&#30340;&#34892;&#20026;&#65307;&#20363;&#22914;&#65292;LLMs&#22312;&#23436;&#25972;&#21644;&#30830;&#23450;&#24615;&#28216;&#25103;&#20013;&#22833;&#36133;&#65292;&#20294;&#23427;&#20204;&#22312;&#27010;&#29575;&#28216;&#25103;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12348v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we investigate two key problems: (1) Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#24694;&#24847;&#31532;&#19977;&#26041;&#25552;&#20379;&#25805;&#32437;&#22270;&#20687;&#30340;&#29992;&#25143;&#38544;&#24418;&#25915;&#20987;&#24471;&#20197;&#26460;&#32477;&#12290;</title><link>https://arxiv.org/abs/2402.12336</link><description>&lt;p&gt;
Robust CLIP: &#23545;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#20197;&#33719;&#24471;&#24378;&#22823;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12336
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#24694;&#24847;&#31532;&#19977;&#26041;&#25552;&#20379;&#25805;&#32437;&#22270;&#20687;&#30340;&#29992;&#25143;&#38544;&#24418;&#25915;&#20987;&#24471;&#20197;&#26460;&#32477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;OpenFlamingo&#12289;LLaVA&#21644;GPT-4&#20043;&#31867;&#30340;&#22810;&#27169;&#22411;&#22522;&#30784;&#27169;&#22411;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35270;&#35273;&#27169;&#24577;&#19978;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#29992;&#26469;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#25110;&#27450;&#39575;&#29992;&#25143;&#65292;&#22240;&#27492;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#39118;&#38505;&#65292;&#36825;&#20351;&#24471;&#22823;&#22411;&#22810;&#27169;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#25104;&#20026;&#19968;&#39033;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#26041;&#26696;&#65292;&#20197;&#33719;&#24471;&#24378;&#22823;&#30340;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#22312;&#25152;&#26377;&#20381;&#36182;&#20110;CLIP&#30340;&#35270;&#35273;&#19979;&#28216;&#20219;&#21153;&#65288;VLMs&#12289;&#38646;&#26679;&#26412;&#20998;&#31867;&#65289;&#19978;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#26086;&#26356;&#25442;&#21407;&#22987;&#30340;CLIP&#27169;&#22411;&#65292;&#29992;&#25143;&#22312;&#20351;&#29992;VLMs&#26102;&#20250;&#21463;&#21040;&#24694;&#24847;&#31532;&#19977;&#26041;&#25552;&#20379;&#30340;&#25805;&#32437;&#22270;&#20687;&#30340;&#28508;&#22312;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12336v1 Announce Type: cross  Abstract: Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP mo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#29983;&#23384;&#36712;&#36857;&#21644;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#32467;&#26500;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#35299;&#20915;&#20102;&#39044;&#27979;&#12289;&#25968;&#25454;&#34917;&#20805;&#21644;&#29983;&#25104;&#21407;&#22411;&#26102;&#38388;&#30456;&#20851;&#36712;&#36857;&#31561;&#20219;&#21153;</title><link>https://arxiv.org/abs/2402.12331</link><description>&lt;p&gt;
&#29983;&#25104;&#29983;&#23384;&#21487;&#35299;&#37322;&#36712;&#36857;&#21644;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating Survival Interpretable Trajectories and Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12331
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#29983;&#23384;&#36712;&#36857;&#21644;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#32467;&#26500;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#35299;&#20915;&#20102;&#39044;&#27979;&#12289;&#25968;&#25454;&#34917;&#20805;&#21644;&#29983;&#25104;&#21407;&#22411;&#26102;&#38388;&#30456;&#20851;&#36712;&#36857;&#31561;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24212;&#29992;&#29305;&#23450;&#32467;&#26500;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#29983;&#25104;&#29983;&#23384;&#36712;&#36857;&#21644;&#25968;&#25454;&#30340;&#26032;&#27169;&#22411;&#12290; &#23427;&#35299;&#20915;&#20102;&#19977;&#20010;&#20219;&#21153;&#12290; &#39318;&#20808;&#65292;&#23427;&#22522;&#20110;Beran&#20272;&#35745;&#22120;&#20026;&#26032;&#29983;&#25104;&#30340;&#29305;&#24449;&#21521;&#37327;&#25552;&#20379;&#20107;&#20214;&#26102;&#38388;&#30340;&#39044;&#27979;&#21644;&#29983;&#23384;&#20989;&#25968;&#30340;&#24418;&#24335;&#12290; &#31532;&#20108;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#32473;&#23450;&#30340;&#35757;&#32451;&#38598;&#29983;&#25104;&#39069;&#22806;&#25968;&#25454;&#65292;&#21487;&#20197;&#34917;&#20805;&#21407;&#22987;&#25968;&#25454;&#38598;&#12290; &#31532;&#19977;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20026;&#23545;&#35937;&#29983;&#25104;&#20102;&#19968;&#20010;&#21407;&#22411;&#26102;&#38388;&#30456;&#20851;&#36712;&#36857;&#65292;&#25551;&#36848;&#20102;&#22914;&#20309;&#25913;&#21464;&#23545;&#35937;&#30340;&#29305;&#24449;&#20197;&#23454;&#29616;&#19981;&#21516;&#26102;&#38388;&#20107;&#20214;&#30340;&#26102;&#38388;&#12290; &#36712;&#36857;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290; &#30001;&#20110;&#23558;&#29305;&#23450;&#21152;&#26435;&#26041;&#26696;&#32435;&#20837;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290; &#35813;&#27169;&#22411;&#36824;&#36890;&#36807;&#35299;&#20915;&#20998;&#31867;&#38382;&#39064;&#30830;&#23450;&#20102;&#26032;&#29983;&#25104;&#25968;&#25454;&#30340;&#34987;&#23457;&#26597;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12331v1 Announce Type: cross  Abstract: A new model for generating survival trajectories and data based on applying an autoencoder of a specific structure is proposed. It solves three tasks. First, it provides predictions in the form of the expected event time and the survival function for a new generated feature vector on the basis of the Beran estimator. Second, the model generates additional data based on a given training set that would supplement the original dataset. Third, the most important, it generates a prototype time-dependent trajectory for an object, which characterizes how features of the object could be changed to achieve a different time to an event. The trajectory can be viewed as a type of the counterfactual explanation. The proposed model is robust during training and inference due to a specific weighting scheme incorporating into the variational autoencoder. The model also determines the censored indicators of new generated data by solving a classificatio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340; API &#35775;&#38382;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#26356;&#39640;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#38750;&#20165;&#20165;&#22522;&#20110;&#27169;&#22411;&#20043;&#38388;&#30340;&#36716;&#31227;&#24615;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.12329</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Query-Based Adversarial Prompt Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340; API &#35775;&#38382;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#26356;&#39640;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#38750;&#20165;&#20165;&#22522;&#20110;&#27169;&#22411;&#20043;&#38388;&#30340;&#36716;&#31227;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#23548;&#33268;&#19968;&#20010;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#23383;&#31526;&#20018;&#25110;&#25191;&#34892;&#26377;&#23475;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#35201;&#20040;&#22312;&#30333;&#30418;&#35774;&#32622;&#20013;&#65288;&#23436;&#20840;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65289;&#65292;&#35201;&#20040;&#36890;&#36807;&#21487;&#36716;&#31227;&#24615;&#65306;&#19968;&#31181;&#29616;&#35937;&#65292;&#21363;&#22312;&#19968;&#20010;&#27169;&#22411;&#19978;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#36890;&#24120;&#22312;&#20854;&#20182;&#27169;&#22411;&#19978;&#20173;&#28982;&#26377;&#25928;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#25913;&#36827;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#21033;&#29992; API &#35775;&#38382;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#26469;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#65288;&#26126;&#26174;&#65289;&#26356;&#39640;&#30340;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#19981;&#33021;&#20165;&#20165;&#20351;&#29992;&#36716;&#31227;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312; GPT-3.5 &#21644; OpenAI &#30340;&#23433;&#20840;&#20998;&#31867;&#22120;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#65307;&#25105;&#20204;&#33021;&#22815;&#35753; GPT-3.5 &#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#30446;&#21069;&#30340;&#36716;&#31227;&#25915;&#20987;&#22833;&#36133;&#20102;&#65292;&#24182;&#19988;&#25105;&#20204;&#20960;&#20046;&#20197; 100% &#30340;&#27010;&#29575;&#35268;&#36991;&#20102;&#23433;&#20840;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12329v1 Announce Type: cross  Abstract: Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#24895;&#26223;&#65292;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20026;&#31038;&#20250;&#29616;&#35937;&#25552;&#20379;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2402.12327</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#20132;&#27969;&#21527;&#65306;&#25506;&#32034;&#31454;&#20105;LLM&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21457;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12327
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#24895;&#26223;&#65292;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20026;&#31038;&#20250;&#29616;&#35937;&#25552;&#20379;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#20195;&#29702;&#20855;&#26377;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#21644;&#31038;&#20250;&#21160;&#24577;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#30740;&#31350;LLM&#20195;&#29702;&#22312;&#27809;&#26377;&#26126;&#30830;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#33258;&#21457;&#24314;&#31435;&#21512;&#20316;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#19981;&#20165;&#23637;&#31034;&#20102;LLM&#20195;&#29702;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#31454;&#20105;&#19982;&#21512;&#20316;&#30340;&#33021;&#21147;&#65292;&#20063;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#24895;&#26223;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#37027;&#20123;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#31038;&#20250;&#29616;&#35937;&#30340;&#27934;&#23519;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/wuzengqing001225/SABM_ShallWe &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12327v1 Announce Type: new  Abstract: Recent advancements have shown that agents powered by large language models (LLMs) possess capabilities to simulate human behaviors and societal dynamics. However, the potential for LLM agents to spontaneously establish collaborative relationships in the absence of explicit instructions has not been studied. To address this gap, we conduct three case studies, revealing that LLM agents are capable of spontaneously forming collaborations even within competitive settings. This finding not only demonstrates the capacity of LLM agents to mimic competition and cooperation in human societies but also validates a promising vision of computational social science. Specifically, it suggests that LLM agents could be utilized to model human social interactions, including those with spontaneous collaborations, thus offering insights into social phenomena. The source codes for this study are available at https://github.com/wuzengqing001225/SABM_ShallWe
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;FairSAR&#65292;&#19968;&#31181;&#29420;&#29305;&#30340;&#36951;&#25022;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#20844;&#24179;&#24847;&#35782;&#22312;&#32447;&#23398;&#20064;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12319</link><description>&lt;p&gt;
&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#21160;&#24577;&#29615;&#22659;&#21709;&#24212;&#22411;&#22312;&#32447;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12319
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;FairSAR&#65292;&#19968;&#31181;&#29420;&#29305;&#30340;&#36951;&#25022;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#20844;&#24179;&#24847;&#35782;&#22312;&#32447;&#23398;&#20064;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#24050;&#32463;&#25104;&#20026;&#36830;&#32493;&#32456;&#36523;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#36880;&#28176;&#33719;&#21462;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#22312;&#24341;&#20837;&#26032;&#20219;&#21153;&#26102;&#30830;&#20445;&#21508;&#20010;&#21463;&#20445;&#25252;&#23376;&#20154;&#21475;&#65288;&#22914;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#20043;&#38388;&#30340;&#32479;&#35745;&#24179;&#31561;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#36951;&#25022;&#24230;&#37327;FairSAR&#65292;&#20197;&#24212;&#23545;&#19981;&#26029;&#21457;&#23637;&#30340;&#29615;&#22659;&#20013;&#30340;&#20844;&#24179;&#24847;&#35782;&#22312;&#32447;&#23398;&#20064;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12319v1 Announce Type: cross  Abstract: The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning. In this scenario, the learner's objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender, when it comes to the newly introduced tasks. A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework. Nevertheless, it's crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions. In this paper, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by inco
&lt;/p&gt;</description></item><item><title>ARKS&#26159;&#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#36890;&#36807;&#27963;&#36291;&#26816;&#32034;&#21644;&#25972;&#21512;&#21508;&#31181;&#20449;&#24687;&#28304;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20026;&#35299;&#20915;&#19982;&#39057;&#32321;&#26356;&#26032;&#30340;&#24211;&#21644;&#38271;&#23614;&#32534;&#31243;&#35821;&#35328;&#30456;&#20851;&#30340;&#29616;&#23454;&#32534;&#31243;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12317</link><description>&lt;p&gt;
ARKS&#65306;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#30693;&#35782;&#27748;&#20013;&#30340;&#27963;&#36291;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
ARKS: Active Retrieval in Knowledge Soup for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12317
&lt;/p&gt;
&lt;p&gt;
ARKS&#26159;&#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#36890;&#36807;&#27963;&#36291;&#26816;&#32034;&#21644;&#25972;&#21512;&#21508;&#31181;&#20449;&#24687;&#28304;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20026;&#35299;&#20915;&#19982;&#39057;&#32321;&#26356;&#26032;&#30340;&#24211;&#21644;&#38271;&#23614;&#32534;&#31243;&#35821;&#35328;&#30456;&#20851;&#30340;&#29616;&#23454;&#32534;&#31243;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#23558;&#22806;&#37096;&#30693;&#35782;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#12290;&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#25506;&#35752;&#65292;&#20294;&#23427;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21033;&#29992;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#27748;&#20013;&#30340;&#27963;&#36291;&#26816;&#32034;(ARKS)&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#29992;&#20110;&#27867;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#20195;&#30721;&#12290;&#19982;&#20381;&#38752;&#21333;&#19968;&#26469;&#28304;&#19981;&#21516;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23558;&#32593;&#39029;&#25628;&#32034;&#12289;&#25991;&#26723;&#12289;&#25191;&#34892;&#21453;&#39304;&#21644;&#36827;&#21270;&#20195;&#30721;&#29255;&#27573;&#25972;&#21512;&#22312;&#19968;&#36215;&#30340;&#30693;&#35782;&#27748;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31215;&#26497;&#30340;&#26816;&#32034;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36845;&#20195;&#22320;&#20248;&#21270;&#26597;&#35810;&#24182;&#26356;&#26032;&#30693;&#35782;&#27748;&#12290;&#20026;&#20102;&#35780;&#20272;ARKS&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#19982;&#39057;&#32321;&#26356;&#26032;&#30340;&#24211;&#21644;&#38271;&#23614;&#32534;&#31243;&#35821;&#35328;&#30456;&#20851;&#30340;&#29616;&#23454;&#32534;&#31243;&#38382;&#39064;&#12290;&#22312;ChatGPT&#21644;CodeLlama&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ARKS&#27604;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#20135;&#29983;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12317v1 Announce Type: cross  Abstract: Recently the retrieval-augmented generation (RAG) paradigm has raised much attention for its potential in incorporating external knowledge into large language models (LLMs) without further training. While widely explored in natural language applications, its utilization in code generation remains under-explored. In this paper, we introduce Active Retrieval in Knowledge Soup (ARKS), an advanced strategy for generalizing large language models for code. In contrast to relying on a single source, we construct a knowledge soup integrating web search, documentation, execution feedback, and evolved code snippets. We employ an active retrieval strategy that iteratively refines the query and updates the knowledge soup. To assess the performance of ARKS, we compile a new benchmark comprising realistic coding problems associated with frequently updated libraries and long-tail programming languages. Experimental results on ChatGPT and CodeLlama de
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#24322;&#26500;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#38598;&#21512;&#20132;&#38598;&#30340;&#22810;&#35270;&#35282;&#21322;&#19968;&#33268;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12307</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#19968;&#33268;&#23398;&#20064;&#29992;&#20110;&#24322;&#26500;&#20256;&#24863;&#22120;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Multi-View Conformal Learning for Heterogeneous Sensor Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12307
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#24322;&#26500;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#38598;&#21512;&#20132;&#38598;&#30340;&#22810;&#35270;&#35282;&#21322;&#19968;&#33268;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20010;&#21035;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#23545;&#20110;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#35832;&#22914;&#21307;&#30103;&#35786;&#26029;&#12289;&#23433;&#20840;&#21644;&#26080;&#20154;&#36710;&#31561;&#20851;&#38190;&#24212;&#29992;&#20013;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#65292;&#22797;&#26434;&#30340;&#39044;&#27979;&#27169;&#22411;&#22312;&#35299;&#20915;&#22256;&#38590;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#27599;&#22825;&#37117;&#26377;&#26032;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;&#32780;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#21457;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#19978;&#65292;&#23545;&#20010;&#21035;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#32771;&#37327;&#36739;&#23569;&#65292;&#29978;&#33267;&#22312;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;&#32972;&#26223;&#19979;&#26356;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#27979;&#35797;&#20102;&#29992;&#20110;&#24322;&#26500;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;&#22810;&#35270;&#35282;&#21644;&#21333;&#35270;&#35282;&#19968;&#33268;&#27169;&#22411;&#12290;&#30001;&#20110;&#22522;&#20110;&#19968;&#33268;&#24615;&#39044;&#27979;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#36793;&#38469;&#32622;&#20449;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#38598;&#21512;&#20132;&#38598;&#30340;&#22810;&#35270;&#35282;&#21322;&#19968;&#33268;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12307v1 Announce Type: cross  Abstract: Being able to assess the confidence of individual predictions in machine learning models is crucial for decision making scenarios. Specially, in critical applications such as medical diagnosis, security, and unmanned vehicles, to name a few. In the last years, complex predictive models have had great success in solving hard tasks and new methods are being proposed every day. While the majority of new developments in machine learning models focus on improving the overall performance, less effort is put on assessing the trustworthiness of individual predictions, and even to a lesser extent, in the context of sensor fusion. To this end, we build and test multi-view and single-view conformal models for heterogeneous sensor fusion. Our models provide theoretical marginal confidence guarantees since they are based on the conformal prediction framework. We also propose a multi-view semi-conformal model based on sets intersection. Through comp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#21830;&#19994;&#27169;&#22411;GPT-3.5 Turbo&#21644;GPT-4&#19982;&#24320;&#28304;&#27169;&#22411;Mistral-7B&#12289;Mixtral-8x7B&#12289;Llama2-13B&#12289;Llama2-70B&#12289;QWEN1.5-72B&#20197;&#21450;CheXbert&#21644;CheXpert-labeler&#22312;&#20934;&#30830;&#26631;&#35760;X&#23556;&#32447;&#25991;&#26412;&#25253;&#21578;&#20013;&#22810;&#21457;&#29616;&#23384;&#22312;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12298</link><description>&lt;p&gt;
&#24320;&#28304;&#21040;&#24213;&#22914;&#20309;&#20102;&#65311;&#20851;&#20110;&#21830;&#19994;&#21644;&#24320;&#28304;LLM&#22312;&#26631;&#35760;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#33021;&#21147;&#26041;&#38754;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12298
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#21830;&#19994;&#27169;&#22411;GPT-3.5 Turbo&#21644;GPT-4&#19982;&#24320;&#28304;&#27169;&#22411;Mistral-7B&#12289;Mixtral-8x7B&#12289;Llama2-13B&#12289;Llama2-70B&#12289;QWEN1.5-72B&#20197;&#21450;CheXbert&#21644;CheXpert-labeler&#22312;&#20934;&#30830;&#26631;&#35760;X&#23556;&#32447;&#25991;&#26412;&#25253;&#21578;&#20013;&#22810;&#21457;&#29616;&#23384;&#22312;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#28044;&#29616;&#20102;&#35768;&#22810;&#26032;&#30340;&#24320;&#28304;&#27169;&#22411;&#21644;&#21830;&#19994;&#27169;&#22411;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#20986;&#29256;&#29289;&#25506;&#35752;&#20102;GPT-4&#22312;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#24863;&#20852;&#36259;&#20449;&#24687;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20294;&#23578;&#26410;&#23545;GPT-4&#19982;&#19981;&#21516;&#39046;&#20808;&#30340;&#24320;&#28304;&#27169;&#22411;&#36827;&#34892;&#23454;&#38469;&#27604;&#36739;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#29420;&#31435;&#25968;&#25454;&#38598;&#12290;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#20102;2019&#24180;7&#26376;&#33267;2021&#24180;7&#26376;&#22312;&#39532;&#33832;&#35832;&#22622;&#24030;&#32508;&#21512;&#21307;&#38498;&#21019;&#24314;&#30340;540&#20221;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#12290;&#31532;&#20108;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;ImaGenome&#25968;&#25454;&#38598;&#30340;500&#20221;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21830;&#19994;&#27169;&#22411;GPT-3.5 Turbo&#21644;GPT-4&#19982;&#24320;&#28304;&#27169;&#22411;Mistral-7B&#12289;Mixtral-8x7B&#12289;Llama2-13B&#12289;Llama2-70B&#12289;QWEN1.5-72B&#20197;&#21450;CheXbert&#21644;CheXpert-labeler&#22312;&#20934;&#30830;&#26631;&#35760;X&#23556;&#32447;&#25991;&#26412;&#25253;&#21578;&#20013;&#22810;&#21457;&#29616;&#23384;&#22312;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12298v1 Announce Type: cross  Abstract: Introduction: With the rapid advances in large language models (LLMs), there have been numerous new open source as well as commercial models. While recent publications have explored GPT-4 in its application to extracting information of interest from radiology reports, there has not been a real-world comparison of GPT-4 to different leading open-source models.   Materials and Methods: Two different and independent datasets were used. The first dataset consists of 540 chest x-ray reports that were created at the Massachusetts General Hospital between July 2019 and July 2021. The second dataset consists of 500 chest x-ray reports from the ImaGenome dataset. We then compared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to the open-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B, QWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accurately label the presence of multiple findings in x-ray text repo
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#32423;&#21035;&#23436;&#32654;&#30340;MMR&#65288;BLP&#65289;&#65292;&#23427;&#26159;&#26497;&#23567;&#21270;&#36951;&#25022;&#30446;&#26631;&#30340;&#31934;&#30830;&#21270;&#65292;&#33021;&#22815;&#20811;&#26381;&#26497;&#23567;&#21270;&#36951;&#25022;&#31574;&#30053;&#22312;&#36951;&#25022;&#19978;&#30028;&#26102;&#23398;&#20064;&#20572;&#28382;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.12284</link><description>&lt;p&gt;
&#20026;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#20248;&#21270;&#26497;&#23567;&#21270;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Refining Minimax Regret for Unsupervised Environment Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12284
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#32423;&#21035;&#23436;&#32654;&#30340;MMR&#65288;BLP&#65289;&#65292;&#23427;&#26159;&#26497;&#23567;&#21270;&#36951;&#25022;&#30446;&#26631;&#30340;&#31934;&#30830;&#21270;&#65292;&#33021;&#22815;&#20811;&#26381;&#26497;&#23567;&#21270;&#36951;&#25022;&#31574;&#30053;&#22312;&#36951;&#25022;&#19978;&#30028;&#26102;&#23398;&#20064;&#20572;&#28382;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#36807;&#23545;&#23545;&#25163;&#26368;&#22823;&#21270;&#26576;&#20010;&#30446;&#26631;&#29983;&#25104;&#30340;&#29615;&#22659;&#37197;&#32622;&#65288;&#20851;&#21345;&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#36951;&#25022;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#30446;&#26631;&#65292;&#29702;&#35770;&#19978;&#23548;&#33268;&#20855;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#26497;&#23567;&#21270;&#36951;&#25022;&#65288;MMR&#65289;&#31574;&#30053;&#65307;&#29305;&#21035;&#26159;&#65292;&#20195;&#29702;&#30340;&#26368;&#22823;&#36951;&#25022;&#26159;&#26377;&#30028;&#30340;&#12290;&#28982;&#32780;&#65292;&#19968;&#26086;&#20195;&#29702;&#22312;&#25152;&#26377;&#20851;&#21345;&#19978;&#36798;&#21040;&#20102;&#36825;&#20010;&#36951;&#25022;&#19978;&#30028;&#65292;&#23545;&#25163;&#23558;&#21482;&#20250;&#23545;&#26080;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#36951;&#25022;&#30340;&#20851;&#21345;&#36827;&#34892;&#37319;&#26679;&#12290;&#23613;&#31649;&#22312;&#36825;&#20123;&#26368;&#22823;&#21270;&#36951;&#25022;&#30340;&#20851;&#21345;&#20043;&#22806;&#21487;&#33021;&#23384;&#22312;&#24615;&#33021;&#25913;&#36827;&#31354;&#38388;&#65292;&#20294;&#23398;&#20064;&#20572;&#28382;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#32423;&#21035;&#23436;&#32654;&#30340;MMR&#65288;BLP&#65289;&#65292;&#23427;&#26159;&#26497;&#23567;&#21270;&#36951;&#25022;&#30446;&#26631;&#30340;&#31934;&#30830;&#21270;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#65292;&#35299;&#20915;&#36825;&#20010;&#30446;&#26631;&#23558;&#23548;&#33268;MMR&#31574;&#30053;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;BLP&#31574;&#30053;&#22312;&#25152;&#26377;&#20851;&#21345;&#19978;&#37117;&#19982;&#23436;&#32654;&#36125;&#21494;&#26031;&#31574;&#30053;&#19968;&#33268;&#34892;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12284v1 Announce Type: cross  Abstract: In unsupervised environment design, reinforcement learning agents are trained on environment configurations (levels) generated by an adversary that maximises some objective. Regret is a commonly used objective that theoretically results in a minimax regret (MMR) policy with desirable robustness guarantees; in particular, the agent's maximum regret is bounded. However, once the agent reaches this regret bound on all levels, the adversary will only sample levels where regret cannot be further reduced. Although there are possible performance improvements to be made outside of these regret-maximising levels, learning stagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a refinement of the minimax regret objective that overcomes this limitation. We formally show that solving for this objective results in a subset of MMR policies, and that BLP policies act consistently with a Perfect Bayesian policy over all levels. We fur
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39592;&#26550;&#22270;&#35299;&#30721;&#65288;SGD&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#23376;&#38382;&#39064;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20449;&#24687;&#36716;&#21457;&#65292;&#25913;&#21892;&#21709;&#24212;&#36136;&#37327;&#19988;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12280</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#39592;&#26550;&#22270;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Adaptive Skeleton Graph Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12280
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39592;&#26550;&#22270;&#35299;&#30721;&#65288;SGD&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#23376;&#38382;&#39064;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20449;&#24687;&#36716;&#21457;&#65292;&#25913;&#21892;&#21709;&#24212;&#36136;&#37327;&#19988;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#25104;&#21151;&#24402;&#22240;&#20110;&#22823;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#65288;&#20363;&#22914;&#65292;70&#20159;+&#65289;&#65307;&#28982;&#32780;&#65292;LLM&#25512;&#26029;&#20250;&#20135;&#29983;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#24182;&#34892;&#35299;&#30721;&#31574;&#30053;&#65292;&#20363;&#22914;&#8220;&#24605;&#24819;&#39592;&#26550;&#8221;&#65288;SoT&#65289;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#20998;&#35299;&#20026;&#21487;&#20197;&#24182;&#34892;&#35299;&#30721;&#30340;&#23376;&#38382;&#39064;&#26469;&#25913;&#21892;&#24615;&#33021;&#65307;&#20294;&#26159;&#65292;&#23427;&#20204;&#24448;&#24448;&#22312;&#21709;&#24212;&#36136;&#37327;&#19978;&#36973;&#21463;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#22312;&#29983;&#25104;&#23376;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#35831;&#27714;&#39069;&#22806;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#20381;&#36182;&#20851;&#31995;&#21644;&#38590;&#24230;&#65292;&#20197;&#25552;&#39640;&#21709;&#24212;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39592;&#26550;&#22270;&#35299;&#30721;&#65288;SGD&#65289;&#65292;&#21033;&#29992;&#23376;&#38382;&#39064;&#20043;&#38388;&#26292;&#38706;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25903;&#25345;&#20381;&#36182;&#23376;&#38382;&#39064;&#20043;&#38388;&#30340;&#20449;&#24687;&#36716;&#21457;&#65292;&#20197;&#25552;&#39640;&#36136;&#37327;&#65292;&#21516;&#26102;&#26292;&#38706;&#29420;&#31435;&#23376;&#38382;&#39064;&#35299;&#30721;&#30340;&#24182;&#34892;&#21270;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12280v1 Announce Type: cross  Abstract: Large language models (LLMs) have seen significant adoption for natural language tasks, owing their success to massive numbers of model parameters (e.g., 70B+); however, LLM inference incurs significant computation and memory costs. Recent approaches propose parallel decoding strategies, such as Skeleton-of-Thought (SoT), to improve performance by breaking prompts down into sub-problems that can be decoded in parallel; however, they often suffer from reduced response quality. Our key insight is that we can request additional information, specifically dependencies and difficulty, when generating the sub-problems to improve both response quality and performance. In this paper, we propose Skeleton Graph Decoding (SGD), which uses dependencies exposed between sub-problems to support information forwarding between dependent sub-problems for improved quality while exposing parallelization opportunities for decoding independent sub-problems. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24494;&#35843;&#23398;&#20064;&#29575;&#21487;&#20197;&#32531;&#35299;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#20013;&#20197;&#38169;&#35823;&#35821;&#35328;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#26159;&#24456;&#24378;&#22823;&#30340;&#22522;&#32447;&#65292;mBART&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#31867;&#20284;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;mT5&#12290;</title><link>https://arxiv.org/abs/2402.12279</link><description>&lt;p&gt;
&#26377;&#25928;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12279
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#23398;&#20064;&#29575;&#21487;&#20197;&#32531;&#35299;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#20013;&#20197;&#38169;&#35823;&#35821;&#35328;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#26159;&#24456;&#24378;&#22823;&#30340;&#22522;&#32447;&#65292;mBART&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#31867;&#20284;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;mT5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#24847;&#21619;&#30528;&#22312;&#19968;&#20010;&#35821;&#35328;&#19978;&#24494;&#35843;&#22810;&#35821;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20110;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#36827;&#34892;&#27492;&#20219;&#21153;&#30340;&#39044;&#27979;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#25351;&#20986;&#19968;&#20010;&#32463;&#24120;&#20986;&#29616;&#30340;&#38382;&#39064;&#26159;&#20197;&#38169;&#35823;&#30340;&#35821;&#35328;&#29983;&#25104;&#65292;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#24120;&#20351;&#29992;mT5&#20316;&#20026;&#20027;&#24178;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#32479;&#19968;&#30340;&#35774;&#32622;&#20013;&#27604;&#36739;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#36824;&#21253;&#25324;&#26367;&#20195;&#24615;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#21363;mBART&#21644;NLLB-200&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#24494;&#35843;&#25152;&#20351;&#29992;&#30340;&#23398;&#20064;&#29575;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#22823;&#22823;&#32531;&#35299;&#20197;&#38169;&#35823;&#35821;&#35328;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#32454;&#33268;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#65292;&#23545;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#24494;&#35843;&#20316;&#20026;&#38750;&#24120;&#24378;&#22823;&#30340;&#22522;&#32447;&#65292;&#26367;&#20195;&#26041;&#27861;&#21482;&#24102;&#26469;&#24494;&#23567;&#30340;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;mBART&#19982;&#21516;&#31561;&#22823;&#23567;&#30340;mT5&#34920;&#29616;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12279v1 Announce Type: cross  Abstract: Zero-shot cross-lingual generation implies finetuning of the multilingual pretrained language model on a generation task in one language and then using it to make predictions for this task in other languages. Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model. In this work we compare various approaches proposed from the literature in unified settings, also including alternative backbone models, namely mBART and NLLB-200. We first underline the importance of tuning learning rate used for finetuning, which helps to substantially alleviate the problem of generation in the wrong language. Then, we show that with careful learning rate tuning, the simple full finetuning of the model acts as a very strong baseline and alternative approaches bring only marginal improvements. Finally, we find that mBART performs similarly to mT5 of the same size,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#28145;&#24230;RL&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20248;&#20110;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12275</link><description>&lt;p&gt;
WorldCoder&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#65306;&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12275
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#28145;&#24230;RL&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20248;&#20110;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26500;&#24314;&#20195;&#34920;&#20854;&#23545;&#19990;&#30028;&#30693;&#35782;&#30340;Python&#31243;&#24207;&#12290;&#35813;&#19990;&#30028;&#27169;&#22411;&#35797;&#22270;&#35299;&#37322;&#20854;&#20132;&#20114;&#65292;&#21516;&#26102;&#23545;&#33258;&#24049;&#33021;&#22815;&#33719;&#24471;&#30340;&#22870;&#21169;&#25345;&#20048;&#35266;&#24577;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;LLM&#30340;&#31243;&#24207;&#21512;&#25104;&#24037;&#20316;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#32593;&#26684;&#19990;&#30028;&#19978;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#27604;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26356;&#39640;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#27604;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12275v1 Announce Type: new  Abstract: We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via LLMs. We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26497;&#24378;&#30340;&#24377;&#24615;&#65292;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12265</link><description>&lt;p&gt;
&#35770;&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Byzantine-Resilience of Distillation-Based Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12265
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26497;&#24378;&#30340;&#24377;&#24615;&#65292;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#38544;&#31169;&#12289;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31639;&#27861;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;KD&#30340;FL&#31639;&#27861;&#30456;&#24403;&#20855;&#26377;&#24377;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#36807;&#31243;&#30456;&#23545;&#20110;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#12290;&#26681;&#25454;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#23545;&#20808;&#21069;&#30340;&#25308;&#21344;&#24237;&#24377;&#24615;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FilterExp&#65292;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12265v1 Announce Type: cross  Abstract: Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset. In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process. We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to Federated Averaging. Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods. Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilien
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LoRA&#38598;&#25104;&#22312;&#31934;&#35843;LLMs&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#22495;&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#20998;&#26512;&#65292;&#25512;&#27979;&#20102;&#27169;&#22411;&#23545;&#29305;&#23450;&#26550;&#26500;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2402.12264</link><description>&lt;p&gt;
&#20351;&#29992;LoRA&#38598;&#25104;&#22312;&#31934;&#35843;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification in fine-tuned LLMs using LoRA ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12264
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LoRA&#38598;&#25104;&#22312;&#31934;&#35843;LLMs&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#22495;&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#20998;&#26512;&#65292;&#25512;&#27979;&#20102;&#27169;&#22411;&#23545;&#29305;&#23450;&#26550;&#26500;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#23545;&#20110;&#31934;&#35843;&#27169;&#22411;&#23398;&#21040;&#20102;&#20160;&#20040;&#12289;&#36951;&#24536;&#20102;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#20449;&#20219;&#20854;&#39044;&#27979;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#19968;&#33324;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#23545;&#31934;&#35843;LLMs&#36827;&#34892;&#22522;&#20110;&#21518;&#39564;&#36924;&#36817;&#30340;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Mistral-7b&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#20998;&#26512;&#20102;&#19977;&#20010;&#24120;&#35265;&#30340;&#22810;&#39033;&#36873;&#25321;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#22312;&#31934;&#35843;&#36807;&#31243;&#20013;&#21644;&#20043;&#21518;&#23545;&#19981;&#21516;&#30446;&#26631;&#39046;&#22495;&#30340;&#24863;&#30693;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#25928;&#33021;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#32467;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#25968;&#20540;&#23454;&#39564;&#25903;&#25345;&#65292;&#25105;&#20204;&#23545;&#37027;&#20123;&#23545;&#20110;&#32473;&#23450;&#26550;&#26500;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#29109;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#25552;&#20986;&#20102;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12264v1 Announce Type: cross  Abstract: Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning. In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn.
&lt;/p&gt;</description></item><item><title>BEARS&#26159;&#19968;&#31181;&#38598;&#25104;&#25216;&#26415;&#65292;&#21487;&#20197;&#35753;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#24847;&#35782;&#21040;&#23427;&#20204;&#23398;&#20064;&#30340;&#27010;&#24565;&#30340;&#35821;&#20041;&#27169;&#31946;&#24615;&#65292;&#24110;&#21161;&#29992;&#25143;&#35782;&#21035;&#21644;&#24576;&#30097;&#20302;&#36136;&#37327;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2402.12240</link><description>&lt;p&gt;
BEARS &#35753;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#24847;&#35782;&#21040;&#23427;&#20204;&#30340;&#25512;&#29702;&#25463;&#24452;
&lt;/p&gt;
&lt;p&gt;
BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12240
&lt;/p&gt;
&lt;p&gt;
BEARS&#26159;&#19968;&#31181;&#38598;&#25104;&#25216;&#26415;&#65292;&#21487;&#20197;&#35753;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#24847;&#35782;&#21040;&#23427;&#20204;&#23398;&#20064;&#30340;&#27010;&#24565;&#30340;&#35821;&#20041;&#27169;&#31946;&#24615;&#65292;&#24110;&#21161;&#29992;&#25143;&#35782;&#21035;&#21644;&#24576;&#30097;&#20302;&#36136;&#37327;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic (NeSy)&#39044;&#27979;&#22120;&#31526;&#21512;&#31526;&#21495;&#30693;&#35782;-&#32534;&#30721;&#65292;&#20363;&#22914;&#23433;&#20840;&#32422;&#26463;&#65292;&#21487;&#33021;&#21463;&#21040;&#25512;&#29702;&#25463;&#24452;&#65288;RSs&#65289;&#30340;&#24433;&#21709;&#65306;&#23427;&#20204;&#36890;&#36807;&#21033;&#29992;&#38750;&#39044;&#26399;&#30340;&#35821;&#20041;&#26469;&#23398;&#20064;&#19982;&#31526;&#21495;&#30693;&#35782;&#19968;&#33268;&#30340;&#27010;&#24565;&#12290; RSs&#25439;&#23475;&#20102;&#21487;&#38752;&#24615;&#21644;&#27867;&#21270;&#65292;&#24182;&#19988;&#27491;&#22914;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25152;&#23637;&#31034;&#30340;&#65292;&#23427;&#20204;&#19982;NeSy&#27169;&#22411;&#23545;&#39044;&#27979;&#27010;&#24565;&#36807;&#20110;&#33258;&#20449;&#26377;&#20851;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21807;&#19968;&#21487;&#38752;&#30340;&#32531;&#35299;&#31574;&#30053;&#38656;&#35201;&#23545;&#27010;&#24565;&#36827;&#34892;&#26114;&#36149;&#30340;&#23494;&#38598;&#30417;&#30563;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#26159;&#35797;&#22270;&#23436;&#20840;&#36991;&#20813;RSs&#65292;&#32780;&#26159;&#35201;&#30830;&#20445;NeSy&#27169;&#22411;&#24847;&#35782;&#21040;&#23427;&#20204;&#23398;&#20064;&#30340;&#27010;&#24565;&#30340;&#35821;&#20041;&#27169;&#31946;&#24615;&#65292;&#20174;&#32780;&#20351;&#29992;&#25143;&#33021;&#22815;&#35782;&#21035;&#21644;&#24576;&#30097;&#20302;&#36136;&#37327;&#30340;&#27010;&#24565;&#12290;&#20174;&#19977;&#20010;&#31616;&#21333;&#30340;&#35774;&#35745;&#35201;&#27714;&#24320;&#22987;&#65292;&#25105;&#20204;&#24471;&#20986;bears&#65288;BE Aware of Reasoning Shortcuts&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38598;&#25104;&#25216;&#26415;&#65292;&#21487;&#20197;&#26657;&#20934;&#27169;&#22411;&#30340;&#27010;&#24565;&#32423;&#20449;&#24515;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12240v1 Announce Type: cross  Abstract: Neuro-Symbolic (NeSy) predictors that conform to symbolic knowledge - encoding, e.g., safety constraints - can be affected by Reasoning Shortcuts (RSs): They learn concepts consistent with the symbolic knowledge by exploiting unintended semantics. RSs compromise reliability and generalization and, as we show in this paper, they are linked to NeSy models being overconfident about the predicted concepts. Unfortunately, the only trustworthy mitigation strategy requires collecting costly dense supervision over the concepts. Rather than attempting to avoid RSs altogether, we propose to ensure NeSy models are aware of the semantic ambiguity of the concepts they learn, thus enabling their users to identify and distrust low-quality concepts. Starting from three simple desiderata, we derive bears (BE Aware of Reasoning Shortcuts), an ensembling technique that calibrates the model's concept-level confidence without compromising prediction accura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12237</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#20869;&#23481;&#23457;&#26680;&#20013;&#25512;&#36831;&#65306;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning to Defer in Content Moderation: The Human-AI Interplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#22312;&#32447;&#24179;&#21488;&#20869;&#23481;&#23457;&#26680;&#20381;&#36182;&#20110;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#31639;&#27861;&#35266;&#23519;&#21040;&#21363;&#23558;&#21457;&#24067;&#30340;&#24086;&#23376;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#20570;&#20986;&#20998;&#31867;&#21644;&#20934;&#20837;&#20915;&#31574;&#65292;&#24182;&#23433;&#25490;&#24086;&#23376;&#36827;&#34892;&#20154;&#24037;&#23457;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12237v1 Announce Type: cross  Abstract: Successful content moderation in online platforms relies on a human-AI collaboration approach. A typical heuristic estimates the expected harmfulness of a post and uses fixed thresholds to decide whether to remove it and whether to send it for human review. This disregards the prediction uncertainty, the time-varying element of human review capacity and post arrivals, and the selective sampling in the dataset (humans only review posts filtered by the admission algorithm).   In this paper, we introduce a model to capture the human-AI interplay in content moderation. The algorithm observes contextual information for incoming posts, makes classification and admission decisions, and schedules posts for human review. Only admitted posts receive human reviews on their harmfulness. These reviews help educate the machine-learning algorithms but are delayed due to congestion in the human review system. The classical learning-theoretic way to ca
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#20108;&#21449;&#26641;&#29992;&#20110;&#32858;&#31867;&#65292;&#31216;&#20026;Kauri&#65292;&#36890;&#36807;&#36138;&#23146;&#26368;&#22823;&#21270; kernel KMeans &#30446;&#26631;&#26469;&#25191;&#34892;&#65292;&#26080;&#38656;&#23450;&#20041;&#36136;&#24515;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20854;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12232</link><description>&lt;p&gt;
&#23558; Kernel KMeans &#32858;&#31867;&#25286;&#20998;&#29992;&#20110;&#31471;&#21040;&#31471;&#26080;&#30417;&#30563;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Kernel KMeans clustering splits for end-to-end unsupervised decision trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12232
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#20108;&#21449;&#26641;&#29992;&#20110;&#32858;&#31867;&#65292;&#31216;&#20026;Kauri&#65292;&#36890;&#36807;&#36138;&#23146;&#26368;&#22823;&#21270; kernel KMeans &#30446;&#26631;&#26469;&#25191;&#34892;&#65292;&#26080;&#38656;&#23450;&#20041;&#36136;&#24515;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20854;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26641;&#26159;&#33719;&#21462;&#23545;&#30456;&#23545;&#36739;&#23567;&#25968;&#25454;&#38598;&#36827;&#34892;&#21487;&#35299;&#37322;&#39044;&#27979;&#30340;&#20415;&#21033;&#27169;&#22411;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#20851;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#31471;&#21040;&#31471;&#26500;&#24314;&#36825;&#31181;&#26641;&#30340;&#25552;&#35758;&#65292;&#20294;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#29992;&#20110;&#32858;&#31867;&#30340;&#26641;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#20316;&#21697;&#20027;&#35201;&#38598;&#20013;&#20110;&#20351;&#29992;&#26641;&#26469;&#35299;&#37322;&#21478;&#19968;&#20010;&#32858;&#31867;&#31639;&#27861;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#20108;&#21449;&#26641;&#29992;&#20110;&#32858;&#31867;&#65306;Kauri&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36138;&#23146;&#26368;&#22823;&#21270; kernel KMeans &#30446;&#26631;&#26469;&#25191;&#34892;&#65292;&#32780;&#26080;&#38656;&#23450;&#20041;&#36136;&#24515;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23558;&#27492;&#27169;&#22411;&#19982;&#26368;&#36817;&#30340;&#26080;&#30417;&#30563;&#26641;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#24403;&#20351;&#29992;&#32447;&#24615;&#26680;&#26102;&#65292;Kauri &#30340;&#24615;&#33021;&#30456;&#21516;&#12290;&#23545;&#20110;&#20854;&#20182;&#20869;&#26680;&#65292;Kauri &#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#20869;&#26680; KMeans &#21644; CART &#20915;&#31574;&#26641;&#30340;&#20018;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12232v1 Announce Type: cross  Abstract: Trees are convenient models for obtaining explainable predictions on relatively small datasets. Although there are many proposals for the end-to-end construction of such trees in supervised learning, learning a tree end-to-end for clustering without labels remains an open challenge. As most works focus on interpreting with trees the result of another clustering algorithm, we present here a novel end-to-end trained unsupervised binary tree for clustering: Kauri. This method performs a greedy maximisation of the kernel KMeans objective without requiring the definition of centroids. We compare this model on multiple datasets with recent unsupervised trees and show that Kauri performs identically when using a linear kernel. For other kernels, Kauri often outperforms the concatenation of kernel KMeans and a CART decision tree.
&lt;/p&gt;</description></item><item><title>AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12226</link><description>&lt;p&gt;
AnyGPT&#65306;&#32479;&#19968;&#30340;&#22810;&#27169;&#24335;&#31163;&#25955;&#24207;&#21015;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12226
&lt;/p&gt;
&lt;p&gt;
AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; AnyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#24847;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#31163;&#25955;&#34920;&#31034;&#32479;&#19968;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#20048;&#12290;AnyGPT &#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26550;&#26500;&#25110;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#12290;&#30456;&#21453;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#32423;&#39044;&#22788;&#29702;&#65292;&#20419;&#36827;&#20102;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#31867;&#20284;&#20110;&#26032;&#35821;&#35328;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#25991;&#26412;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24335;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#20219;&#24847;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;108k&#20010;&#22810;&#36718;&#23545;&#35805;&#31034;&#20363;&#65292;&#31934;&#32454;&#22320;&#20132;&#32455;&#21508;&#31181;&#27169;&#24577;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AnyGPT&#33021;&#22815;&#20419;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReAlign&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#25351;&#23548;&#25968;&#25454;&#30340;&#21709;&#24212;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12219</link><description>&lt;p&gt;
&#37325;&#26032;&#26684;&#24335;&#21270;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Reformatted Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReAlign&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#25351;&#23548;&#25968;&#25454;&#30340;&#21709;&#24212;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#24494;&#35843;&#25968;&#25454;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#30340;&#26041;&#27861;&#35201;&#20040;&#32791;&#26102;&#36153;&#21147;&#65292;&#35201;&#20040;&#23481;&#26131;&#21463;&#21040;LLM&#24187;&#35273;&#24341;&#36215;&#30340;&#20107;&#23454;&#38169;&#35823;&#24433;&#21709;&#12290;&#26412;&#25991;&#25506;&#35752;&#25552;&#21319;&#29616;&#26377;&#25351;&#23548;&#25968;&#25454;&#36136;&#37327;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ReAlign&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#23427;&#23558;&#25351;&#23548;&#25968;&#25454;&#30340;&#21709;&#24212;&#37325;&#26032;&#26684;&#24335;&#21270;&#20026;&#26356;&#31526;&#21512;&#39044;&#20808;&#24314;&#31435;&#26631;&#20934;&#21644;&#32534;&#35793;&#35777;&#25454;&#30340;&#26684;&#24335;&#12290;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#20154;&#31867;&#27880;&#37322;&#12289;&#24187;&#35273;&#21644;&#25193;&#23637;&#22256;&#38590;&#65292;&#19982;&#29616;&#26377;&#23545;&#40784;&#25216;&#26415;&#27491;&#20132;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReAlign&#26174;&#33879;&#25552;&#21319;&#20102;LLMs&#30340;&#25972;&#20307;&#23545;&#40784;&#33021;&#21147;&#12289;&#25968;&#23398;&#25512;&#29702;&#12289;&#20107;&#23454;&#24615;&#21644;&#21487;&#35835;&#24615;&#12290;&#20196;&#20154;&#40723;&#33310;&#30340;&#26159;&#65292;&#22312;&#19981;&#24341;&#20837;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#25110;&#20808;&#36827;&#35757;&#32451;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#21709;&#24212;&#65292;LLaMA-2-13
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12219v1 Announce Type: cross  Abstract: The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.   Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#37319;&#29992;Copyleft&#20943;&#36731;AIGC&#29256;&#26435;&#22256;&#22659;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#20204;&#26222;&#36941;&#24863;&#30693;&#21040;&#22256;&#22659;&#24182;&#20542;&#21521;&#20110;&#20351;&#29992;&#33258;&#30001;&#25480;&#26435;&#12290;</title><link>https://arxiv.org/abs/2402.12216</link><description>&lt;p&gt;
&#20943;&#36731;AIGC&#29256;&#26435;&#22256;&#22659;&#30340;Copyleft&#65306;&#20551;&#35774;&#20998;&#26512;&#12289;&#20844;&#20247;&#35748;&#30693;&#21644;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Copyleft for Alleviating AIGC Copyright Dilemma: What-if Analysis, Public Perception and Implications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#37319;&#29992;Copyleft&#20943;&#36731;AIGC&#29256;&#26435;&#22256;&#22659;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#20204;&#26222;&#36941;&#24863;&#30693;&#21040;&#22256;&#22659;&#24182;&#20542;&#21521;&#20110;&#20351;&#29992;&#33258;&#30001;&#25480;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#19982;&#20840;&#29699;&#27835;&#29702;&#65288;AIGC&#65289;&#22312;&#36807;&#21435;&#20960;&#24180;&#28145;&#21051;&#24433;&#21709;&#25105;&#20204;&#30340;&#31038;&#20250;&#65292;&#20262;&#29702;&#38382;&#39064;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#20854;&#20013;&#26368;&#32039;&#36843;&#30340;&#38382;&#39064;&#26159;AIGC&#29256;&#26435;&#22256;&#22659;&#65292;&#36825;&#21487;&#33021;&#26497;&#22823;&#22320;&#38459;&#30861;AIGC&#30340;&#21457;&#23637;&#24182;&#32473;&#25972;&#20010;&#31038;&#20250;&#24102;&#26469;&#24040;&#22823;&#25104;&#26412;&#12290;&#37492;&#20110;AIGC&#29256;&#26435;&#27835;&#29702;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#24403;&#21069;&#27809;&#26377;&#23436;&#32654;&#35299;&#20915;&#26041;&#26696;&#30340;&#20107;&#23454;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#24352;&#22312;AI&#27835;&#29702;&#20013;&#37319;&#29992;Copyleft&#65292;&#20294;&#27809;&#26377;&#36827;&#34892;&#23454;&#36136;&#24615;&#20998;&#26512;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#37319;&#29992;Copyleft&#20943;&#36731;AIGC&#29256;&#26435;&#22256;&#22659;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#20174;&#20004;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#65306;&#22312;&#23450;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#27491;&#24335;&#30340;&#20551;&#35774;&#20998;&#26512;&#26469;&#28548;&#28165;&#22256;&#22659;&#65292;&#24182;&#25552;&#20379;&#26696;&#20363;&#30740;&#31350;&#26469;&#23637;&#31034;Copyleft&#30340;&#21487;&#34892;&#24615;&#65307;&#22312;&#23450;&#37327;&#26041;&#38754;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31934;&#24515;&#35774;&#35745;&#30340;&#35843;&#26597;&#65292;&#20197;&#20102;&#35299;&#20844;&#20247;&#23545;AIGC&#20351;&#29992;Copyleft&#30340;&#30475;&#27861;&#12290;&#20851;&#38190;&#21457;&#29616;&#21253;&#25324;&#65306;a&#65289;&#20154;&#20204;&#26222;&#36941;&#24863;&#30693;&#21040;&#36825;&#19968;&#22256;&#22659;&#65292;b&#65289;&#20182;&#20204;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#33258;&#30001;&#25480;&#26435;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12216v1 Announce Type: cross  Abstract: As AIGC has impacted our society profoundly in the past years, ethical issues have received tremendous attention. The most urgent one is the AIGC copyright dilemma, which can immensely stifle the development of AIGC and greatly cost the entire society. Given the complexity of AIGC copyright governance and the fact that no perfect solution currently exists, previous work advocated copyleft on AI governance but without substantive analysis. In this paper, we take a step further to explore the feasibility of copyleft to alleviate the AIGC copyright dilemma. We conduct a mixed-methods study from two aspects: qualitatively, we use a formal what-if analysis to clarify the dilemma and provide case studies to show the feasibility of copyleft; quantitatively, we perform a carefully designed survey to find out how the public feels about copylefting AIGC. The key findings include: a) people generally perceive the dilemma, b) they prefer to use au
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#24322;&#36136;&#24615;&#30340;&#28151;&#21512;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#26657;&#36873;&#20462;&#35838;&#31243;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#24322;&#26500;&#22270;&#21644;&#35774;&#35745;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#24322;&#36136;&#24615;&#24863;&#30693;&#34920;&#31034;&#65292;&#24182;&#22312;&#32852;&#37030;&#26041;&#26696;&#19979;&#35757;&#32451;&#20010;&#21035;&#23398;&#26657;&#27169;&#22411;&#20197;&#25512;&#33616;&#37327;&#36523;&#23450;&#21046;&#30340;&#36873;&#20462;&#35838;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.12202</link><description>&lt;p&gt;
&#32771;&#34385;&#24322;&#36136;&#24615;&#30340;&#36328;&#26657;&#36873;&#20462;&#35838;&#25512;&#33616;&#65306;&#19968;&#31181;&#28151;&#21512;&#32852;&#37030;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid Federated Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#24322;&#36136;&#24615;&#30340;&#28151;&#21512;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#26657;&#36873;&#20462;&#35838;&#31243;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#24322;&#26500;&#22270;&#21644;&#35774;&#35745;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#24322;&#36136;&#24615;&#24863;&#30693;&#34920;&#31034;&#65292;&#24182;&#22312;&#32852;&#37030;&#26041;&#26696;&#19979;&#35757;&#32451;&#20010;&#21035;&#23398;&#26657;&#27169;&#22411;&#20197;&#25512;&#33616;&#37327;&#36523;&#23450;&#21046;&#30340;&#36873;&#20462;&#35838;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25945;&#32946;&#26102;&#20195;&#65292;&#35299;&#20915;&#36328;&#26657;&#23398;&#20064;&#32773;&#22810;&#26679;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#20026;&#36873;&#20462;&#35838;&#31243;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#38382;&#39064;&#32463;&#24120;&#38480;&#21046;&#20102;&#36328;&#26657;&#25968;&#25454;&#20849;&#20139;&#65292;&#36825;&#38480;&#21046;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#31232;&#30095;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#21644;&#26377;&#25928;&#22788;&#29702;&#24322;&#36136;&#24615;&#30340;&#33021;&#21147;&#65292;&#26368;&#32456;&#23548;&#33268;&#23376;&#20248;&#21270;&#30340;&#25512;&#33616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HFRec&#65292;&#19968;&#31181;&#32771;&#34385;&#24322;&#36136;&#24615;&#30340;&#28151;&#21512;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#20026;&#36328;&#26657;&#36873;&#20462;&#35838;&#31243;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#20026;&#27599;&#20010;&#23398;&#26657;&#26500;&#24314;&#20102;&#24322;&#26500;&#22270;&#65292;&#23558;&#23398;&#29983;&#20043;&#38388;&#30340;&#21508;&#31181;&#20132;&#20114;&#21644;&#21382;&#21490;&#34892;&#20026;&#34701;&#20837;&#20854;&#20013;&#65292;&#20197;&#25972;&#21512;&#19978;&#19979;&#25991;&#21644;&#20869;&#23481;&#20449;&#24687;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#24322;&#36136;&#24615;&#24863;&#30693;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#22312;&#19968;&#20010;&#32852;&#37030;&#26041;&#26696;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#35774;&#32622;&#26469;&#35757;&#32451;&#20010;&#21035;&#23398;&#26657;&#27169;&#22411;&#20197;&#25512;&#33616;&#37327;&#36523;&#23450;&#21046;&#30340;&#36873;&#20462;&#35838;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12202v1 Announce Type: cross  Abstract: In the era of modern education, addressing cross-school learner diversity is crucial, especially in personalized recommender systems for elective course selection. However, privacy concerns often limit cross-school data sharing, which hinders existing methods' ability to model sparse data and address heterogeneity effectively, ultimately leading to suboptimal recommendations. In response, we propose HFRec, a heterogeneity-aware hybrid federated recommender system designed for cross-school elective course recommendations. The proposed model constructs heterogeneous graphs for each school, incorporating various interactions and historical behaviors between students to integrate context and content information. We design an attention mechanism to capture heterogeneity-aware representations. Moreover, under a federated scheme, we train individual school-based models with adaptive learning settings to recommend tailored electives. Our HFRec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MultiFIX&#65292;&#19968;&#31181;&#27880;&#37325;&#21487;&#35299;&#37322;&#24615;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#22270;&#26469;&#35299;&#37322;&#27599;&#20010;&#27169;&#24577;&#23545;&#26368;&#32456;&#39044;&#27979;&#30340;&#24433;&#21709;</title><link>https://arxiv.org/abs/2402.12183</link><description>&lt;p&gt;
MultiFIX:&#19968;&#31181;&#21451;&#22909;&#30340;XAI&#21151;&#33021;&#35825;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#26500;&#24314;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MultiFIX: An XAI-friendly feature inducing approach to building models from multimodal data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12183
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MultiFIX&#65292;&#19968;&#31181;&#27880;&#37325;&#21487;&#35299;&#37322;&#24615;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#22270;&#26469;&#35299;&#37322;&#27599;&#20010;&#27169;&#24577;&#23545;&#26368;&#32456;&#39044;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20581;&#24247;&#39046;&#22495;&#65292;&#20915;&#31574;&#24120;&#24120;&#22522;&#20110;&#19981;&#21516;&#30340;&#25968;&#25454;&#27169;&#24577;&#12290;&#22240;&#27492;&#65292;&#22312;&#21019;&#24314;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#20013;&#25552;&#21462;&#21644;&#32452;&#21512;&#30456;&#20851;&#29305;&#24449;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#38750;&#24120;&#26377;&#30410;&#12290;&#27492;&#22806;&#65292;&#37325;&#35201;&#30340;&#26159;&#35201;&#20102;&#35299;&#27599;&#20010;&#27169;&#24577;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#39044;&#27979;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#65292;&#20197;&#20415;&#21487;&#20197;&#20449;&#20219;&#21644;&#36127;&#36131;&#20219;&#22320;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MultiFIX&#65306;&#19968;&#31181;&#26032;&#30340;&#27880;&#37325;&#21487;&#35299;&#37322;&#24615;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#31649;&#36947;&#65292;&#26126;&#30830;&#20174;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#20013;&#35825;&#23548;&#20986;&#21333;&#29420;&#30340;&#29305;&#24449;&#65292;&#38543;&#21518;&#21487;&#20197;&#23558;&#20854;&#32452;&#21512;&#20197;&#36827;&#34892;&#26368;&#32456;&#39044;&#27979;&#12290;&#37319;&#29992;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#24182;&#25552;&#21462;&#27599;&#20010;&#27169;&#24577;&#30340;&#20195;&#34920;&#24615;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#35299;&#37322;&#27169;&#22411;&#30340;&#27599;&#20010;&#37096;&#20998;&#12290;&#27880;&#24847;&#21147;&#22270;&#29992;&#20110;&#31361;&#20986;&#22270;&#20687;&#20013;&#30340;&#37325;&#35201;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12183v1 Announce Type: new  Abstract: In the health domain, decisions are often based on different data modalities. Thus, when creating prediction models, multimodal fusion approaches that can extract and combine relevant features from different data modalities, can be highly beneficial. Furthermore, it is important to understand how each modality impacts the final prediction, especially in high-stake domains, so that these models can be used in a trustworthy and responsible manner. We propose MultiFIX: a new interpretability-focused multimodal data fusion pipeline that explicitly induces separate features from different data types that can subsequently be combined to make a final prediction. An end-to-end deep learning architecture is used to train a predictive model and extract representative features of each modality. Each part of the model is then explained using explainable artificial intelligence techniques. Attention maps are used to highlight important regions in ima
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20998;&#26512;&#19981;&#21516;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#22914;&#20309;&#26356;&#21152;&#21407;&#21017;&#22320;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.12181</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Revisiting Data Augmentation in Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12181
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20998;&#26512;&#19981;&#21516;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#22914;&#20309;&#26356;&#21152;&#21407;&#21017;&#22320;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#23454;&#35777;&#20013;&#35777;&#26126;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#25110;&#27867;&#21270;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#24182;&#19981;&#24635;&#26159;&#28165;&#26970;&#21738;&#31181;&#25216;&#26415;&#24212;&#35813;&#34987;&#20248;&#20808;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#20204;&#24182;&#25581;&#31034;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#34920;&#36798;&#36825;&#20123;&#26041;&#27861;&#30340;Q-targets&#21644;&#32463;&#39564;&#28436;&#21592;/&#35780;&#35770;&#23478;&#25439;&#22833;&#30340;&#26041;&#24046;&#65292;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#23427;&#20204;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#24182;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22914;&#20309;&#36873;&#25321;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#36716;&#25442;&#26469;&#35745;&#31639;&#30446;&#26631;Q&#20540;&#21487;&#33021;&#20250;&#24433;&#21709;&#36825;&#20123;&#26041;&#27861;&#30340;&#35299;&#37322;&#12290;&#36825;&#39033;&#20998;&#26512;&#25552;&#20986;&#20102;&#22914;&#20309;&#26356;&#21152;&#21407;&#21017;&#22320;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21253;&#25324;&#20102;&#19968;&#20010;&#31216;&#20026;&#20999;&#32447;prop&#30340;&#27491;&#21017;&#21270;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12181v1 Announce Type: cross  Abstract: Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL). Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear. To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected. Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them. We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values. This analysis suggests recommendations on how to exploit data augmentation in a more principled way. In addition, we include a regularization term called tangent prop,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35774;&#35745;&#30340;&#8220;&#26816;&#27979;&#30417;&#25511;&#31995;&#32479;&#8221;&#33021;&#22815;&#39640;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#22320;&#35782;&#21035;&#22312;&#32447;&#32771;&#35797;&#20013;&#30340;&#20316;&#24330;&#34892;&#20026;&#65292;&#24110;&#21161;&#30417;&#32771;&#21592;&#20570;&#20986;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.12179</link><description>&lt;p&gt;
&#26816;&#27979;&#30417;&#25511;&#31995;&#32479;&#65306;&#22312;&#32447;&#32771;&#35797;&#20013;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Examining Monitoring System: Detecting Abnormal Behavior In Online Examinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12179
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35774;&#35745;&#30340;&#8220;&#26816;&#27979;&#30417;&#25511;&#31995;&#32479;&#8221;&#33021;&#22815;&#39640;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#22320;&#35782;&#21035;&#22312;&#32447;&#32771;&#35797;&#20013;&#30340;&#20316;&#24330;&#34892;&#20026;&#65292;&#24110;&#21161;&#30417;&#32771;&#21592;&#20570;&#20986;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#22312;&#32447;&#32771;&#35797;&#20316;&#24330;&#24050;&#25104;&#20026;&#19968;&#20010;&#26222;&#36941;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23398;&#26415;&#19981;&#31471;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#8220;&#26816;&#27979;&#22312;&#32447;&#32771;&#35797;&#20013;&#24322;&#24120;&#34892;&#20026;&#30340;&#32771;&#35797;&#30417;&#25511;&#31995;&#32479;&#8221;&#65292;&#26088;&#22312;&#24110;&#21161;&#30417;&#32771;&#21592;&#35782;&#21035;&#24322;&#24120;&#23398;&#29983;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#23637;&#31034;&#20986;&#39640;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#65292;&#25552;&#20379;&#23453;&#36149;&#20449;&#24687;&#65292;&#36741;&#21161;&#30417;&#32771;&#21592;&#20570;&#20986;&#20915;&#31574;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#20197;&#21450;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#32531;&#35299;&#22312;&#32447;&#32771;&#35797;&#20316;&#24330;&#26222;&#36941;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12179v1 Announce Type: cross  Abstract: Cheating in online exams has become a prevalent issue over the past decade, especially during the COVID-19 pandemic. To address this issue of academic dishonesty, our "Exam Monitoring System: Detecting Abnormal Behavior in Online Examinations" is designed to assist proctors in identifying unusual student behavior. Our system demonstrates high accuracy and speed in detecting cheating in real-time scenarios, providing valuable information, and aiding proctors in decision-making. This article outlines our methodology and the effectiveness of our system in mitigating the widespread problem of cheating in online exams.
&lt;/p&gt;</description></item><item><title>Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12177</link><description>&lt;p&gt;
Mafin: &#29992;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#26469;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12177
&lt;/p&gt;
&lt;p&gt;
Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;RAG&#20013;&#30340;&#26816;&#32034;&#38454;&#27573;&#36890;&#24120;&#28041;&#21450;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#26597;&#35810;&#21644;&#27573;&#33853;&#36716;&#25442;&#20026;&#21521;&#37327;&#20197;&#25429;&#33719;&#23427;&#20204;&#30340;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#26102;&#65292;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#65292;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20165;&#33021;&#20174;&#40657;&#30418;&#27169;&#22411;&#33719;&#21462;&#23884;&#20837;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#65288;Mafin&#65289;--&#19968;&#31181;&#36890;&#36807;&#29992;&#21487;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;&#27169;&#22411;&#26469;&#36827;&#34892;&#24494;&#35843;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Mafin&#20165;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#23567;&#30340;&#22686;&#24378;&#27169;&#22411;&#23601;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40657;&#30418;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26723;&#65292;&#23454;&#29616;&#22312;&#26032;&#39046;&#22495;&#22238;&#31572;&#38382;&#39064;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.12170</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Unsupervised LLM Adaptation for Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12170
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26723;&#65292;&#23454;&#29616;&#22312;&#26032;&#39046;&#22495;&#22238;&#31572;&#38382;&#39064;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#26679;&#21270;&#30693;&#35782;&#12290;&#25509;&#30528;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;&#65292;LLM&#33021;&#22815;&#36820;&#22238;&#22810;&#26679;&#38382;&#39064;&#30340;&#27491;&#30830;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#39044;&#35757;&#32451;&#30340;LLM&#35843;&#25972;&#21040;&#26032;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#22914;&#19981;&#21516;&#32452;&#32455;&#25110;&#26102;&#26399;&#65292;&#29992;&#20110;&#38382;&#31572;&#20219;&#21153;&#20250;&#20135;&#29983;&#24456;&#39640;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21363;&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#12289;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;&#28304;&#25968;&#25454;&#65289;&#21644;&#30446;&#26631;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26723;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;LLM&#65292;&#20351;&#20854;&#33021;&#22815;&#22238;&#31572;&#20851;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#22312;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35265;&#35299;&#65307;&#65288;i&#65289;&#24494;&#35843;&#27169;&#22411;&#23637;&#31034;&#20102;&#25552;&#20379;&#27491;&#30830;&#31572;&#26696;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12170v1 Announce Type: cross  Abstract: Large language models (LLM) learn diverse knowledge present in the large-scale training dataset via self-supervised training. Followed by instruction-tuning, LLM acquires the ability to return correct information for diverse questions. However, adapting these pre-trained LLMs to new target domains, such as different organizations or periods, for the question-answering (QA) task incurs a substantial annotation cost. To tackle this challenge, we propose a novel task, unsupervised LLM adaptation for question answering. In this task, we leverage a pre-trained LLM, a publicly available QA dataset (source data), and unlabeled documents from the target domain. Our goal is to learn LLM that can answer questions about the target domain. We introduce one synthetic and two real datasets to evaluate models fine-tuned on the source and target data, and reveal intriguing insights; (i) fine-tuned models exhibit the ability to provide correct answers 
&lt;/p&gt;</description></item><item><title>PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#20026;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#31283;&#20581;&#38450;&#24481;</title><link>https://arxiv.org/abs/2402.12168</link><description>&lt;p&gt;
&#38024;&#23545;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12168
&lt;/p&gt;
&lt;p&gt;
PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#20026;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#31283;&#20581;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#25552;&#20986;&#24182;&#25104;&#21151;&#23454;&#26045;&#20102;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#38754;&#23545;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#26102;&#65292;&#20165;&#26356;&#26032;&#26377;&#38480;&#27169;&#22411;&#21442;&#25968;&#30340;PEFT&#26159;&#21542;&#26500;&#25104;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#39044;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#20173;&#28982;&#26131;&#21463;&#21033;&#29992;&#65292;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#22312;&#24494;&#35843;&#21518;&#20381;&#28982;&#20445;&#25345;&#39640;&#32622;&#20449;&#24230;&#12290;&#21463;&#21040;&#36825;&#19968;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21033;&#29992;PEFT&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#65292;&#25552;&#20379;&#38024;&#23545;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31283;&#20581;&#38450;&#24481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;PEFT&#35757;&#32451;PSIM&#65292;&#24102;&#26377;&#38543;&#26426;&#37325;&#32622;&#26679;&#26412;&#26631;&#31614;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12168v1 Announce Type: cross  Abstract: Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#37197;&#22120;&#35843;&#20248;&#26694;&#26550;&#65292;&#36171;&#20104;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20844;&#24179;&#24615;</title><link>https://arxiv.org/abs/2402.12161</link><description>&lt;p&gt;
&#36171;&#20104;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Endowing Pre-trained Graph Models with Provable Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#37197;&#22120;&#35843;&#20248;&#26694;&#26550;&#65292;&#36171;&#20104;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#26088;&#22312;&#25429;&#25417;&#21487;&#36716;&#31227;&#30340;&#22266;&#26377;&#32467;&#26500;&#23646;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#31867;&#20284;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;PGMs&#20063;&#20250;&#32487;&#25215;&#20154;&#31867;&#31038;&#20250;&#20013;&#30340;&#20559;&#35265;&#65292;&#23548;&#33268;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#20986;&#29616;&#27495;&#35270;&#34892;&#20026;&#12290;&#29616;&#26377;&#20844;&#24179;&#26041;&#27861;&#30340;&#21435;&#20559;&#35265;&#36807;&#31243;&#36890;&#24120;&#19982;GNNs&#30340;&#21442;&#25968;&#20248;&#21270;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#22312;&#29616;&#23454;&#20013;&#21487;&#33021;&#19982;&#19981;&#21516;&#30340;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#32852;&#65292;&#30452;&#25509;&#37319;&#29992;&#29616;&#26377;&#26041;&#27861;&#25913;&#21892;PGMs&#30340;&#20844;&#24179;&#24615;&#26159;&#19981;&#28789;&#27963;&#19988;&#20302;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#21363;&#23545;&#27169;&#22411;&#39044;&#27979;&#20844;&#24179;&#24615;&#30340;&#21487;&#35777;&#26126;&#19979;&#38480;&#65292;&#36825;&#30452;&#25509;&#25552;&#20379;&#20102;&#23454;&#38469;&#22330;&#26223;&#19979;&#30340;&#20445;&#35777;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#37197;&#22120;&#35843;&#20248;&#26694;&#26550;&#65292;&#36171;&#20104;&#39044;&#35757;&#32451;\textbf{&#22270;}&#27169;&#22411;&#20855;&#26377;\textbf{&#21487;&#35777;&#26126;}&#30340;\textbf{&#20844;}&#24179;\textbf{&#24615;}&#65288;&#31216;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12161v1 Announce Type: cross  Abstract: Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks. Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications. The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs. However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient. Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario. To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained \textbf{Graph} models with \textbf{P}rovable f\textbf{A}i\textbf{R}ness (called
&lt;/p&gt;</description></item><item><title>Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#38544;&#34255;&#31354;&#38388;&#20869;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#26469;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#22312;&#23398;&#20064;&#20013;&#21160;&#24577;&#28436;&#21464;&#65292;&#24182;&#26377;&#21161;&#20110;&#22788;&#29702;&#26410;&#35265;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12151</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Causal Language Models Perform Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12151
&lt;/p&gt;
&lt;p&gt;
Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#38544;&#34255;&#31354;&#38388;&#20869;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#26469;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#22312;&#23398;&#20064;&#20013;&#21160;&#24577;&#28436;&#21464;&#65292;&#24182;&#26377;&#21161;&#20110;&#22788;&#29702;&#26410;&#35265;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;LLM&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#39069;&#22806;&#35757;&#32451;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#24050;&#32463;&#26174;&#31034;&#20986;&#24456;&#22823;&#25913;&#36827;&#65292;&#28982;&#32780;&#65292;&#23548;&#33268;&#26377;&#25928;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#30340;&#26426;&#21046;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#20998;&#26512;&#20102;&#22522;&#20110;Transformer&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#27169;&#22411;&#36890;&#36807;&#22312;&#20854;&#38544;&#34255;&#31354;&#38388;&#20869;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#32780;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21160;&#24577;&#28436;&#21464;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#22914;&#20309;&#24110;&#21161;&#27169;&#22411;&#22788;&#29702;&#26410;&#35265;&#23454;&#20363;&#65292;&#24182;&#22312;&#26356;&#29616;&#23454;&#30340;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12151v1 Announce Type: cross  Abstract: Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks, the capability of an LLM to follow human instructions is still a concern. Recent works have shown great improvements in the instruction-following capability via additional training for instruction-following tasks. However, the mechanisms responsible for effective instruction-following capabilities remain inadequately understood. Here, we introduce a simplified instruction-following task and use synthetic datasets to analyze a Transformer-based causal language model. Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances and validate our results in a more realistic setting.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#29305;&#23450;&#35282;&#33394;&#20197;&#34920;&#36798;&#22810;&#26679;&#35266;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;FairThinking&#27969;&#27700;&#32447;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2402.12150</link><description>&lt;p&gt;
&#24744;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26263;&#20013;&#25903;&#25345;&#20844;&#24179;&#65292;&#24744;&#24212;&#35813;&#20687;&#23545;&#24453;&#19968;&#20010;&#20844;&#24179;&#32773;&#37027;&#26679;&#25552;&#31034;&#23427;
&lt;/p&gt;
&lt;p&gt;
Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12150
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#29305;&#23450;&#35282;&#33394;&#20197;&#34920;&#36798;&#22810;&#26679;&#35266;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;FairThinking&#27969;&#27700;&#32447;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#20984;&#26174;&#20102;&#30830;&#20445;&#20854;&#20844;&#24179;&#24615;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;&#28982;&#32780;&#65292;LLMs&#32463;&#24120;&#23637;&#31034;&#25903;&#37197;&#24615;&#35266;&#28857;&#65292;&#21516;&#26102;&#24573;&#35270;&#26469;&#33258;&#23569;&#25968;&#27966;&#30340;&#26367;&#20195;&#35266;&#28857;&#65292;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#20559;&#35265;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#36829;&#21453;&#20844;&#24179;&#30340;&#34892;&#20026;&#21457;&#29983;&#26159;&#22240;&#20026;LLMs&#20351;&#29992;&#20195;&#34920;&#35757;&#32451;&#25968;&#25454;&#22823;&#22810;&#25968;&#30340;&#20154;&#31867;&#20010;&#24615;&#26469;&#34920;&#36798;&#20182;&#20204;&#30340;&#35266;&#28857;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#39564;&#35777;&#25552;&#31034;LLMs&#20351;&#29992;&#29305;&#23450;&#35282;&#33394;&#21487;&#20197;&#20351;LLMs&#34920;&#36798;&#22810;&#26679;&#35266;&#28857;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#21644;&#35266;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;FairThinking&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#33021;&#35753;LLMs&#34920;&#36798;&#22810;&#26679;&#35266;&#28857;&#20197;&#23454;&#29616;&#20844;&#24179;&#34920;&#36798;&#30340;&#27969;&#27700;&#32447;&#12290;&#20026;&#20102;&#35780;&#20272;FairThinking&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#19977;&#20010;&#19982;&#20844;&#24179;&#30456;&#20851;&#20027;&#39064;&#30340;&#19968;&#21315;&#20010;&#39033;&#30446;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;GPT-3.5&#65292;GPT-4&#65292;Llama2&#21644;Mistral&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12150v1 Announce Type: cross  Abstract: The widespread adoption of large language models (LLMs) underscores the urgent need to ensure their fairness. However, LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases. We hypothesize that these fairness-violating behaviors occur because LLMs express their viewpoints using a human personality that represents the majority of training data. In response to this, we validate that prompting LLMs with specific roles can allow LLMs to express diverse viewpoints. Building on this insight and observation, we develop FairThinking, a pipeline designed to automatically generate roles that enable LLMs to articulate diverse perspectives for fair expressions. To evaluate FairThinking, we create a dataset with a thousand items covering three fairness-related topics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to demonstrate its superior performanc
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Factiverse AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#36328;&#35821;&#35328;&#30340;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20026;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12147</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#35268;&#27169;&#30340;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
End-to-end multilingual fact-checking at scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12147
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Factiverse AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#36328;&#35821;&#35328;&#30340;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20026;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;Factiverse AI&#27169;&#22411;&#22312;100&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#24615;&#22522;&#20934;&#27979;&#35797;&#23637;&#31034;&#65292;&#20026;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#32988;&#36807;GPT-4&#12289;GPT-3.5-Turbo&#21644;Mistral-7b&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12147v1 Announce Type: cross  Abstract: In this article, we describe how you can perform end-to-end fact-checking in over 100 languages using Factiverse AI models. We also show through an experimental benchmark that fine-tuned models tailored for fact-checking tasks outperform Large Language Models such as GPT-4, GPT-3.5-Turbo, and Mistral-7b.
&lt;/p&gt;</description></item><item><title>Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12146</link><description>&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12146
&lt;/p&gt;
&lt;p&gt;
Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24191;&#27867;&#20219;&#21153;&#20013;&#23637;&#29616;&#24378;&#22823;&#24615;&#33021;&#65292;&#20294;&#20173;&#38754;&#20020;&#24187;&#35273;&#31561;&#21487;&#38752;&#24615;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20687;GPT-4&#36825;&#26679;&#39640;&#33021;&#21147;&#30340;LLMs&#22312;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#32780;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#36890;&#24120;&#34987;&#35843;&#25972;&#26469;&#35780;&#20272;&#23545;&#30456;&#21516;&#26597;&#35810;&#30340;&#21709;&#24212;&#30340;&#30456;&#23545;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\textit{Meta}$ $\textit{Ranking}$&#65288;MR&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20808;&#21069;&#30452;&#25509;&#35780;&#20272;&#21709;&#24212;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#36827;&#34892;&#27604;&#36739;&#26469;&#23454;&#29616;&#21028;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#21709;&#24212;&#30340;&#38169;&#35823;&#26816;&#27979;&#20013;&#65292;MR&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#20063;&#33021;&#32988;&#36807;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;MR&#21487;&#20197;&#34987;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SSTKG&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#21644;&#25506;&#32034;&#26102;&#31354;&#30693;&#35782;&#22270;&#65292;&#20197;&#23558;&#31354;&#38388;&#21644;&#26102;&#38388;&#25968;&#25454;&#25972;&#21512;&#21040;&#30693;&#35782;&#22270;&#20013;&#65292;&#25552;&#20379;&#20102;&#29992;&#20110;&#26410;&#26469;&#26102;&#24207;&#39044;&#27979;&#21644;&#31354;&#38388;&#20449;&#24687;&#25512;&#33616;&#30340;&#36755;&#20986;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.12132</link><description>&lt;p&gt;
SSTKG&#65306;&#31616;&#21333;&#30340;&#21487;&#35299;&#37322;&#21644;&#22810;&#25165;&#22810;&#33402;&#30340;&#21160;&#24577;&#20449;&#24687;&#23884;&#20837;&#30340;&#26102;&#31354;&#30693;&#35782;&#22270;
&lt;/p&gt;
&lt;p&gt;
SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12132
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SSTKG&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#21644;&#25506;&#32034;&#26102;&#31354;&#30693;&#35782;&#22270;&#65292;&#20197;&#23558;&#31354;&#38388;&#21644;&#26102;&#38388;&#25968;&#25454;&#25972;&#21512;&#21040;&#30693;&#35782;&#22270;&#20013;&#65292;&#25552;&#20379;&#20102;&#29992;&#20110;&#26410;&#26469;&#26102;&#24207;&#39044;&#27979;&#21644;&#31354;&#38388;&#20449;&#24687;&#25512;&#33616;&#30340;&#36755;&#20986;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#21644;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#38745;&#24577;&#25968;&#25454;&#65292;&#24573;&#35270;&#20102;&#30495;&#23454;&#22330;&#26223;&#30340;&#21160;&#24577;&#24615;&#21644;&#38544;&#34255;&#30340;&#26102;&#31354;&#23646;&#24615;&#12290;&#36825;&#32463;&#24120;&#23548;&#33268;&#27425;&#20248;&#30340;&#39044;&#27979;&#21644;&#25512;&#33616;&#12290;&#23613;&#31649;&#26377;&#26377;&#25928;&#30340;&#26102;&#31354;&#25512;&#29702;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#25361;&#25112;&#65292;&#20363;&#22914;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#19981;&#20805;&#20998;&#30340;&#35821;&#20041;&#29702;&#35299;&#65292;&#36825;&#20250;&#24433;&#21709;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550; - &#31616;&#21333;&#30340;&#26102;&#31354;&#30693;&#35782;&#22270;&#65288;SSTKG&#65289;&#65292;&#29992;&#20110;&#26500;&#24314;&#21644;&#25506;&#32034;&#26102;&#31354;&#30693;&#35782;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12132v1 Announce Type: new  Abstract: Knowledge graphs (KGs) have been increasingly employed for link prediction and recommendation using real-world datasets. However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios. This often results in suboptimal predictions and recommendations. Although there are effective spatio-temporal inference methods, they face challenges such as scalability with large datasets and inadequate semantic understanding, which impede their performance. To address these limitations, this paper introduces a novel framework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing and exploring spatio-temporal KGs. To integrate spatial and temporal data into KGs, our framework exploited through a new 3-step embedding method. Output embeddings can be used for future temporal sequence prediction and spatial information recommendation, providing valua
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25490;&#21517;&#30456;&#20851;&#20998;&#26512;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#22312;&#29983;&#25104;&#22270;&#20687;&#35780;&#20215;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12121</link><description>&lt;p&gt;
&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#35780;&#20215;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Image Review Ability of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25490;&#21517;&#30456;&#20851;&#20998;&#26512;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#22312;&#29983;&#25104;&#22270;&#20687;&#35780;&#20215;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#26159;&#33021;&#22815;&#36890;&#36807;&#21333;&#20010;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;LVLM&#29983;&#25104;&#22270;&#20687;&#35780;&#20215;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;LVLM&#23545;&#22270;&#20687;&#30340;&#35780;&#20215;&#33021;&#21147;&#23578;&#26410;&#23436;&#20840;&#34987;&#29702;&#35299;&#65292;&#31361;&#26174;&#20102;&#23545;&#20854;&#35780;&#20215;&#33021;&#21147;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;&#19982;&#22270;&#20687;&#26631;&#39064;&#19981;&#21516;&#65292;&#35780;&#20215;&#25991;&#26412;&#21487;&#20197;&#20174;&#22270;&#20687;&#26500;&#22270;&#21644;&#26333;&#20809;&#31561;&#19981;&#21516;&#35270;&#35282;&#25776;&#20889;&#12290;&#36825;&#31181;&#35780;&#20215;&#35282;&#24230;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;&#38590;&#20197;&#21807;&#19968;&#30830;&#23450;&#22270;&#20687;&#30340;&#27491;&#30830;&#35780;&#20215;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#30456;&#20851;&#20998;&#26512;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#31867;&#21644;LVLM&#23545;&#35780;&#20215;&#25991;&#26412;&#36827;&#34892;&#25490;&#21517;&#65292;&#28982;&#21518;&#27979;&#37327;&#36825;&#20123;&#25490;&#21517;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#26368;&#26032;LVLM&#22270;&#20687;&#35780;&#20215;&#33021;&#21147;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12121v1 Announce Type: cross  Abstract: Large-scale vision language models (LVLMs) are language models that are capable of processing images and text inputs by a single model. This paper explores the use of LVLMs to generate review texts for images. The ability of LVLMs to review images is not fully understood, highlighting the need for a methodical evaluation of their review abilities. Unlike image captions, review texts can be written from various perspectives such as image composition and exposure. This diversity of review perspectives makes it difficult to uniquely determine a single correct review for an image. To address this challenge, we introduce an evaluation method based on rank correlation analysis, in which review texts are ranked by humans and LVLMs, then, measures the correlation between these rankings. We further validate this approach by creating a benchmark dataset aimed at assessing the image review ability of recent LVLMs. Our experiments with the dataset
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DualView&#65292;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#24314;&#27169;&#30340;&#21518;&#26399;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#21644;&#20248;&#36136;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12118</link><description>&lt;p&gt;
DualView&#65306;&#21452;&#37325;&#35270;&#35282;&#19979;&#30340;&#25968;&#25454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
DualView: Data Attribution from the Dual Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12118
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DualView&#65292;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#24314;&#27169;&#30340;&#21518;&#26399;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#21644;&#20248;&#36136;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DualView&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#24314;&#27169;&#30340;&#21518;&#26399;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#33391;&#22909;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#19982;&#25991;&#29486;&#30456;&#20851;&#30340;&#36866;&#24403;&#23450;&#37327;&#35780;&#20272;&#31574;&#30053;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#65292;&#27604;&#36739;&#20102;&#19982;&#30456;&#20851;&#20027;&#35201;&#26412;&#22320;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12118v1 Announce Type: cross  Abstract: Local data attribution (or influence estimation) techniques aim at estimating the impact that individual data points seen during training have on particular predictions of an already trained Machine Learning model during test time. Previous methods either do not perform well consistently across different evaluation criteria from literature, are characterized by a high computational demand, or suffer from both. In this work we present DualView, a novel method for post-hoc data attribution based on surrogate modelling, demonstrating both high computational efficiency, as well as good evaluation results. With a focus on neural networks, we evaluate our proposed technique using suitable quantitative evaluation strategies from the literature against related principal local data attribution methods. We find that DualView requires considerably lower computational resources than other methods, while demonstrating comparable performance to comp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30830;&#20445;&#24402;&#19968;&#21270;&#23545;&#24207;&#21015;&#38271;&#24230;&#19981;&#21464;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#31227;&#38500;&#24322;&#24120;&#20540;&#30340;&#21516;&#26102;&#20419;&#36827;&#20102;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.12102</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#31227;&#38500;&#24322;&#24120;&#20540;&#26159;&#21542;&#26377;&#20854;&#30410;&#22788;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is It a Free Lunch for Removing Outliers during Pretraining?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12102
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30830;&#20445;&#24402;&#19968;&#21270;&#23545;&#24207;&#21015;&#38271;&#24230;&#19981;&#21464;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#31227;&#38500;&#24322;&#24120;&#20540;&#30340;&#21516;&#26102;&#20419;&#36827;&#20102;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#38271;&#65292;&#37327;&#21270;&#30340;&#20316;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#26435;&#37325;&#25110;&#28608;&#27963;&#20013;&#23384;&#22312;&#30340;&#24322;&#24120;&#20540;&#26126;&#26174;&#24433;&#21709;&#20102;&#37327;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;qtransformer &#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#20197;&#26080;&#24322;&#24120;&#20540;&#26041;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#22411; softmax &#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23427;&#20204;&#36866;&#29992;&#20110;&#37327;&#21270;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20840;&#31934;&#24230;&#24615;&#33021;&#30340;&#19979;&#38477;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#36890;&#36807;&#30830;&#20445;&#20854;&#24402;&#19968;&#21270;&#23545;&#24207;&#21015;&#38271;&#24230;&#19981;&#21464;&#26469;&#22686;&#24378;&#35813;&#26041;&#27861;&#65292;&#36825;&#26159;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#24357;&#21512;&#24046;&#36317;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#36824;&#20419;&#36827;&#20102;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12102v1 Announce Type: cross  Abstract: With the growing size of large language models, the role of quantization becomes increasingly significant. However, outliers present in weights or activations notably influence the performance of quantized models. Recently, \citet{qtransformer} introduced a novel softmax function aimed at pretraining models in an outlier-free manner, thereby enhancing their suitability for quantization. Interestingly, we observed that such an approach leads to performance degradation in full precision. Building on this insight, we enhance the method by ensuring its normalization is invariant to sequence length, a crucial factor for bridging the gap between pretraining and fine-tuning. Moreover, this improved method also facilitates successful pretraining of causal language models.
&lt;/p&gt;</description></item><item><title>Groot&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#26641;&#30340;&#35821;&#20041;&#36716;&#25442;&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;93.66%&#12290;</title><link>https://arxiv.org/abs/2402.12100</link><description>&lt;p&gt;
Groot: &#20351;&#29992;&#22522;&#20110;&#26641;&#30340;&#35821;&#20041;&#36716;&#25442;&#23545;&#29983;&#25104;&#24335;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Groot: Adversarial Testing for Generative Text-to-Image Models with Tree-based Semantic Transformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12100
&lt;/p&gt;
&lt;p&gt;
Groot&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#26641;&#30340;&#35821;&#20041;&#36716;&#25442;&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;93.66%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26222;&#21450;&#65292;&#20854;&#23433;&#20840;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#25239;&#24615;&#27979;&#35797;&#25216;&#26415;&#24050;&#34987;&#24320;&#21457;&#29992;&#20110;&#25506;&#27979;&#36825;&#31867;&#27169;&#22411;&#26159;&#21542;&#33021;&#34987;&#28608;&#21457;&#20135;&#29983;&#19981;&#23433;&#20840;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30528;&#20302;&#25104;&#21151;&#29575;&#21644;&#20302;&#25928;&#29575;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; Groot&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#26641;&#30340;&#35821;&#20041;&#36716;&#25442;&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#25239;&#27979;&#35797;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#12290;Groot &#32467;&#21512;&#35821;&#20041;&#20998;&#35299;&#21644;&#25935;&#24863;&#20803;&#32032;&#28153;&#27809;&#31574;&#30053;&#65292;&#32467;&#21512; LLMs &#26469;&#31995;&#32479;&#22320;&#20248;&#21270;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35780;&#20272;&#39564;&#35777;&#20102; Groot &#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#19981;&#20165;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;&#39046;&#20808;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#22914; DALL-E 3 &#21644; Midjourney &#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#65288;93.66%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12100v1 Announce Type: cross  Abstract: With the prevalence of text-to-image generative models, their safety becomes a critical concern. adversarial testing techniques have been developed to probe whether such models can be prompted to produce Not-Safe-For-Work (NSFW) content. However, existing solutions face several challenges, including low success rate and inefficiency. We introduce Groot, the first automated framework leveraging tree-based semantic transformation for adversarial testing of text-to-image models. Groot employs semantic decomposition and sensitive element drowning strategies in conjunction with LLMs to systematically refine adversarial prompts. Our comprehensive evaluation confirms the efficacy of Groot, which not only exceeds the performance of current state-of-the-art approaches but also achieves a remarkable success rate (93.66%) on leading text-to-image models such as DALL-E 3 and Midjourney.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;pGS-CAM&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#30446;&#26631;&#23450;&#20301;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;LiDAR&#28857;&#20113;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#20013;&#27599;&#20010;&#28857;&#30340;&#36129;&#29486;&#65292;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#36807;&#31243;&#24182;&#35782;&#21035;&#25913;&#36827;&#30340;&#28508;&#22312;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.12098</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#30446;&#26631;&#23450;&#20301;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;LiDAR&#28857;&#20113;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable LiDAR Point Cloud Semantic Segmentation via Gradient Based Target Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12098
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;pGS-CAM&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#30446;&#26631;&#23450;&#20301;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;LiDAR&#28857;&#20113;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#20013;&#27599;&#20010;&#28857;&#30340;&#36129;&#29486;&#65292;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#36807;&#31243;&#24182;&#35782;&#21035;&#25913;&#36827;&#30340;&#28508;&#22312;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LiDAR&#28857;&#20113;&#30340;&#35821;&#20041;&#20998;&#21106;&#23545;&#22478;&#24066;&#35268;&#21010;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#35299;&#37322;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#35299;&#37322;&#28857;&#20113;&#35821;&#20041;&#20998;&#21106;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;pGS-CAM&#30340;&#26032;&#22411;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#23618;&#20013;&#29983;&#25104;&#26174;&#33879;&#24615;&#22270;&#12290;&#21463;Grad-CAM&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26799;&#24230;&#31361;&#20986;&#26174;&#31034;&#23616;&#37096;&#37325;&#35201;&#24615;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#65288;&#22914;SemanticKITTI&#12289;Paris-Lille3D&#12289;DALES&#65289;&#21644;3D&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65288;&#22914;KPConv&#12289;RandLANet&#65289;&#19978;&#34920;&#29616;&#20986;&#31283;&#20581;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;pGS-CAM&#36890;&#36807;&#31361;&#20986;&#26174;&#31034;&#27599;&#20010;&#28857;&#30340;&#36129;&#29486;&#65292;&#26377;&#25928;&#22320;&#24378;&#35843;&#20102;SS&#26550;&#26500;&#20013;&#30340;&#20013;&#38388;&#28608;&#27963;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;SS&#27169;&#22411;&#26159;&#22914;&#20309;&#36827;&#34892;&#39044;&#27979;&#30340;&#24182;&#30830;&#23450;&#28508;&#22312;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12098v1 Announce Type: cross  Abstract: Semantic Segmentation (SS) of LiDAR point clouds is essential for many applications, such as urban planning and autonomous driving. While much progress has been made in interpreting SS predictions for images, interpreting point cloud SS predictions remains a challenge. This paper introduces pGS-CAM, a novel gradient-based method for generating saliency maps in neural network activation layers. Inspired by Grad-CAM, which uses gradients to highlight local importance, pGS-CAM is robust and effective on a variety of datasets (SemanticKITTI, Paris-Lille3D, DALES) and 3D deep learning architectures (KPConv, RandLANet). Our experiments show that pGS-CAM effectively accentuates the feature learning in intermediate activations of SS architectures by highlighting the contribution of each point. This allows us to better understand how SS models make their predictions and identify potential areas for improvement. Relevant codes are available at h
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#24182;&#19981;&#30495;&#27491;&#29702;&#35299;&#36923;&#36753;&#35268;&#21017;&#65292;&#32780;&#26159;&#36890;&#36807;&#35821;&#22659;&#23398;&#20064;&#22686;&#24378;&#20102;&#27169;&#22411;&#21040;&#36798;&#32467;&#35770;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2402.12091</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30495;&#27491;&#29702;&#35299;&#36923;&#36753;&#36824;&#26159;&#20165;&#20165;&#27169;&#20223;&#35821;&#22659;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models Understand Logic or Just Mimick Context?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12091
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#24182;&#19981;&#30495;&#27491;&#29702;&#35299;&#36923;&#36753;&#35268;&#21017;&#65292;&#32780;&#26159;&#36890;&#36807;&#35821;&#22659;&#23398;&#20064;&#22686;&#24378;&#20102;&#27169;&#22411;&#21040;&#36798;&#32467;&#35770;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#20204;&#22312;&#22797;&#26434;&#22330;&#26223;&#65288;&#22914;&#36923;&#36753;&#25512;&#29702;&#21644;&#31526;&#21495;&#25512;&#29702;&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#26159;&#22312;&#35821;&#22659;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#35821;&#22659;&#25512;&#29702;&#30340;&#36825;&#31181;&#27169;&#22411;&#25104;&#21151;&#32972;&#21518;&#30340;&#21407;&#22240;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;LLMs&#26159;&#21542;&#20102;&#35299;&#36923;&#36753;&#35268;&#21017;&#20197;&#36827;&#34892;&#25512;&#29702;&#65292;&#36824;&#26159;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#27010;&#29575;&#26144;&#23556;&#36890;&#36807;&#35821;&#22659;&#8220;&#29468;&#27979;&#8221;&#31572;&#26696;&#65311;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#26041;&#27861;&#22312;&#20004;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#26367;&#25442;&#35821;&#22659;&#25991;&#26412;&#24182;&#20462;&#25913;&#36923;&#36753;&#27010;&#24565;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;LLMs&#24182;&#19981;&#30495;&#27491;&#29702;&#35299;&#36923;&#36753;&#35268;&#21017;&#65307;&#30456;&#21453;&#65292;&#22312;&#35821;&#22659;&#23398;&#20064;&#31616;&#21333;&#22686;&#24378;&#20102;&#36825;&#20123;&#27169;&#22411;&#21040;&#36798;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12091v1 Announce Type: cross  Abstract: Over the past few years, the abilities of large language models (LLMs) have received extensive attention, which have performed exceptionally well in complicated scenarios such as logical reasoning and symbolic inference. A significant factor contributing to this progress is the benefit of in-context learning and few-shot prompting. However, the reasons behind the success of such models using contextual reasoning have not been fully explored. Do LLMs have understand logical rules to draw inferences, or do they ``guess'' the answers by learning a type of probabilistic mapping through context? This paper investigates the reasoning capabilities of LLMs on two logical reasoning datasets by using counterfactual methods to replace context text and modify logical concepts. Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;HIP&#32593;&#32476;&#65292;&#36890;&#36807;&#20174;&#26102;&#38388;&#24615;&#12289;&#32467;&#26500;&#24615;&#21644;&#37325;&#22797;&#24615;&#35282;&#24230;&#20256;&#36882;&#20449;&#24687;&#65292;&#32508;&#21512;&#32771;&#34385;&#20102;&#26102;&#38388;&#21464;&#21270;&#32972;&#21518;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#26356;&#26032;&#34920;&#31034;&#24182;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.12074</link><description>&lt;p&gt;
HIP&#32593;&#32476;&#65306;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#22806;&#25512;&#25512;&#29702;&#30340;&#21382;&#21490;&#20449;&#24687;&#20256;&#36882;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HIP Network: Historical Information Passing Network for Extrapolation Reasoning on Temporal Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12074
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;HIP&#32593;&#32476;&#65292;&#36890;&#36807;&#20174;&#26102;&#38388;&#24615;&#12289;&#32467;&#26500;&#24615;&#21644;&#37325;&#22797;&#24615;&#35282;&#24230;&#20256;&#36882;&#20449;&#24687;&#65292;&#32508;&#21512;&#32771;&#34385;&#20102;&#26102;&#38388;&#21464;&#21270;&#32972;&#21518;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#26356;&#26032;&#34920;&#31034;&#24182;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#25512;&#29702;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#22312;&#35757;&#32451;&#26399;&#38388;&#25152;&#26377;&#26102;&#38388;&#25139;&#21644;&#30456;&#24212;&#30340;&#22270;&#24418;&#37117;&#26159;&#21487;&#29992;&#30340;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23398;&#20064;&#26681;&#25454;&#21382;&#21490;&#20449;&#24687;&#25512;&#26029;&#26410;&#26469;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#26410;&#20840;&#38754;&#32771;&#34385;&#26102;&#38388;&#21464;&#21270;&#32972;&#21518;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#20197;&#36873;&#25321;&#24615;&#22320;&#20256;&#36882;&#21382;&#21490;&#20449;&#24687;&#65292;&#36866;&#24403;&#26356;&#26032;&#34920;&#31034;&#24182;&#20934;&#30830;&#39044;&#27979;&#20107;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21382;&#21490;&#20449;&#24687;&#20256;&#36882;&#65288;HIP&#65289;&#32593;&#32476;&#26469;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#12290;HIP&#32593;&#32476;&#20174;&#26102;&#38388;&#12289;&#32467;&#26500;&#21644;&#37325;&#22797;&#30340;&#35282;&#24230;&#20256;&#36882;&#20449;&#24687;&#65292;&#29992;&#20110;&#27169;&#25311;&#20107;&#20214;&#30340;&#26102;&#38388;&#28436;&#21464;&#12289;&#21516;&#19968;&#26102;&#38388;&#27493;&#20013;&#20107;&#20214;&#30340;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#24050;&#30693;&#20107;&#20214;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#20851;&#31995;&#30340;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12074v1 Announce Type: new  Abstract: In recent years, temporal knowledge graph (TKG) reasoning has received significant attention. Most existing methods assume that all timestamps and corresponding graphs are available during training, which makes it difficult to predict future events. To address this issue, recent works learn to infer future events based on historical information. However, these methods do not comprehensively consider the latent patterns behind temporal changes, to pass historical information selectively, update representations appropriately and predict events accurately. In this paper, we propose the Historical Information Passing (HIP) network to predict future events. HIP network passes information from temporal, structural and repetitive perspectives, which are used to model the temporal evolution of events, the interactions of events at the same time step, and the known events respectively. In particular, our method considers the updating of relation 
&lt;/p&gt;</description></item><item><title>EmoBench&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;&#65292;&#21253;&#25324;&#24773;&#24863;&#29702;&#35299;&#21644;&#24773;&#24863;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12071</link><description>&lt;p&gt;
EmoBench: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
EmoBench: Evaluating the Emotional Intelligence of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12071
&lt;/p&gt;
&lt;p&gt;
EmoBench&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;&#65292;&#21253;&#25324;&#24773;&#24863;&#29702;&#35299;&#21644;&#24773;&#24863;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20984;&#26174;&#20102;&#38656;&#35201;&#31283;&#20581;&#12289;&#20840;&#38754;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#30340;&#24773;&#24863;&#26234;&#33021;&#65288;EI&#65289;&#36827;&#34892;&#35780;&#20272;&#30340;&#30740;&#31350;&#30456;&#24403;&#26377;&#38480;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#24773;&#24863;&#35782;&#21035;&#65292;&#24573;&#35270;&#20102;&#24773;&#24863;&#35843;&#33410;&#31561;&#37325;&#35201;&#30340;&#24773;&#24863;&#26234;&#33021;&#33021;&#21147;&#65292;&#32780;&#24773;&#24863;&#29702;&#35299;&#21017;&#20419;&#36827;&#24773;&#24863;; &#20854;&#27425;&#65292;&#23427;&#20204;&#20027;&#35201;&#22522;&#20110;&#29616;&#26377;&#25968;&#25454;&#38598;&#26500;&#24314;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#39057;&#32321;&#27169;&#24335;&#12289;&#26126;&#30830;&#20449;&#24687;&#21644;&#27880;&#37322;&#38169;&#35823;&#65292;&#23548;&#33268;&#35780;&#20272;&#19981;&#21487;&#38752;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EmoBench&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20511;&#37492;&#20102;&#24050;&#24314;&#31435;&#30340;&#24515;&#29702;&#29702;&#35770;&#65292;&#24182;&#20026;&#26426;&#22120;EI&#25552;&#20986;&#20102;&#32508;&#21512;&#23450;&#20041;&#65292;&#21253;&#25324;&#24773;&#24863;&#29702;&#35299;&#21644;&#24773;&#24863;&#24212;&#29992;&#12290;EmoBench&#21253;&#25324;&#19968;&#32452;400&#20010;&#29992;&#33521;&#35821;&#21644;&#20013;&#25991;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#65292;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#38656;&#35201;&#28145;&#20837;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12071v1 Announce Type: cross  Abstract: Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;WKVQuant&#65292;&#19968;&#31181;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12065</link><description>&lt;p&gt;
WKVQuant&#65306;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#20197;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12065
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;WKVQuant&#65292;&#19968;&#31181;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30528;&#37096;&#32626;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#38656;&#27714;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#27880;LLMs&#30340;&#37327;&#21270;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#37327;&#21270;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#21644;&#28608;&#27963;&#36716;&#25442;&#20026;&#20302;&#27604;&#29305;&#25972;&#25968;&#26469;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#35782;&#21035;&#20986;&#23427;&#20204;&#22312;&#24179;&#34913;&#37327;&#21270;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#36229;&#36234;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WKVQuant&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#37327;&#21270;LLMs&#30340;&#21442;&#25968;&#26435;&#37325;&#21644;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#32780;&#35774;&#35745;&#30340;PTQ&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20165;&#32771;&#34385;&#36807;&#21435;&#30340;&#37327;&#21270;&#20197;&#25913;&#21892;&#27880;&#24847;&#21147;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20108;&#32500;&#37327;&#21270;&#31574;&#30053;&#26469;&#22788;&#29702;KV&#32531;&#23384;&#30340;&#20998;&#24067;&#65292;&#20197;&#21450;&#19968;&#31181;&#36328;&#22359;&#37325;&#24314;&#27491;&#21017;&#21270;&#26041;&#27861;&#20197;&#24110;&#21161;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12065v1 Announce Type: cross  Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12062</link><description>&lt;p&gt;
&#22240;&#26524;&#24179;&#31561;&#20445;&#25252;&#19982;&#31639;&#27861;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Equal Protection as Algorithmic Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21746;&#23398;&#30340;&#25991;&#29486;&#24418;&#25104;&#20102;&#19981;&#21516;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#26631;&#20934;&#12290;&#20854;&#20013;&#26368;&#21463;&#20105;&#35758;&#30340;&#20998;&#31867;&#24179;&#31561;&#35201;&#27714;&#65292;&#39044;&#27979;&#31639;&#27861;&#30340;&#38169;&#35823;&#20998;&#31867;&#22312;&#34987;&#20445;&#25252;&#29305;&#24449;&#25152;&#25351;&#31034;&#30340;&#32676;&#20307;&#20013;&#20197;&#30456;&#31561;&#39057;&#29575;&#21457;&#29983;&#12290;&#23613;&#31649;&#20998;&#31867;&#24179;&#31561;&#20855;&#26377;&#30452;&#35266;&#21560;&#24341;&#21147;&#65292;&#20294;&#24050;&#21463;&#21040;&#25915;&#20987;&#12290;&#25105;&#20204;&#36716;&#21521;&#19968;&#20010;&#30456;&#20851;&#21407;&#21017;&#65292;&#21363;&#24179;&#31561;&#20445;&#25252;&#65292;&#35813;&#21407;&#21017;&#26368;&#21021;&#26159;&#22312;&#21009;&#20107;&#21496;&#27861;&#39046;&#22495;&#21457;&#23637;&#36215;&#26469;&#30340;&#12290;&#24179;&#31561;&#20445;&#25252;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#65288;&#23558;&#22312;&#35268;&#23450;&#30340;&#24847;&#20041;&#19978;&#20855;&#20307;&#35828;&#26126;&#65289;&#36827;&#34892;&#22343;&#31561;&#21270;&#65292;&#32780;&#19981;&#26159;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#27604;&#29575;&#22343;&#31561;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24179;&#31561;&#20445;&#25252;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20998;&#31867;&#24179;&#31561;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12062v1 Announce Type: cross  Abstract: Over the last ten years the literature in computer science and philosophy has formulated different criteria of algorithmic fairness. One of the most discussed, classification parity, requires that the erroneous classifications of a predictive algorithm occur with equal frequency for groups picked out by protected characteristics. Despite its intuitive appeal, classification parity has come under attack. Multiple scenarios can be imagined in which - intuitively - a predictive algorithm does not treat any individual unfairly, and yet classification parity is violated. To make progress, we turn to a related principle, equal protection, originally developed in the context of criminal justice. Key to equal protection is equalizing the risks of erroneous classifications (in a sense to be specified) as opposed to equalizing the rates of erroneous classifications. We show that equal protection avoids many of the counterexamples to classificati
&lt;/p&gt;</description></item><item><title>LONDI&#26694;&#26550;&#21487;&#20197;&#22312;&#38656;&#35201;&#22797;&#26434;&#20915;&#31574;&#21644;&#25512;&#29702;&#30340;&#22320;&#26041;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#36164;&#28304;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2402.12061</link><description>&lt;p&gt;
&#25152;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#37117;&#19968;&#26679;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
All Language Models Large and Small
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12061
&lt;/p&gt;
&lt;p&gt;
LONDI&#26694;&#26550;&#21487;&#20197;&#22312;&#38656;&#35201;&#22797;&#26434;&#20915;&#31574;&#21644;&#25512;&#29702;&#30340;&#22320;&#26041;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39046;&#20808;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#35757;&#32451;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#20351;&#29992;&#39640;&#24378;&#24230;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#23545;&#20110;&#38477;&#20302;&#37096;&#32626;&#36164;&#28304;&#25104;&#26412;&#21644;&#26356;&#24555;&#25191;&#34892;&#20915;&#31574;&#20219;&#21153;&#31561;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#20248;&#21270;&#32593;&#32476;&#20998;&#24067;&#65288;LONDI&#65289;&#26694;&#26550;&#30340;&#26032;&#22411;&#21363;&#25554;&#21363;&#29992;LM&#26694;&#26550;&#12290; LONDI&#23398;&#20250;&#20102;&#22312;&#38656;&#35201;&#36827;&#34892;&#22797;&#26434;&#20915;&#31574;&#21644;&#25512;&#29702;&#30340;&#22320;&#26041;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#22823;&#30340;LM&#65292;&#32780;&#22312;&#20854;&#20182;&#22320;&#26041;&#20351;&#29992;&#20302;&#36164;&#28304;&#30340;LM&#12290; LONDI&#30001;&#20004;&#20010;&#65288;&#31163;&#32447;&#65289;&#31574;&#30053;&#32593;&#32476;&#31995;&#32479;&#12289;&#19968;&#20010;LM&#12289;&#19968;&#20010;&#22823;&#30340;LM&#65288;LLM)&#21644;&#19968;&#20010;&#20351;&#29992;&#24320;&#20851;&#25511;&#21046;&#24555;&#36895;&#23398;&#20064;&#20309;&#26102;&#35843;&#29992;LLM&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22359;&#32452;&#25104;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;LLM&#35843;&#29992;&#21644;&#36164;&#28304;&#20351;&#29992;&#26041;&#38754;&#20445;&#25345;&#39044;&#31639;&#32422;&#26463;&#30340;LONDI&#21464;&#20307;&#12290; &#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LONDI&#23398;&#20064;&#28608;&#27963;&#25152;&#38656;&#35299;&#20915;&#20219;&#21153;&#30340;LLM&#30340;&#31995;&#32479;&#29366;&#24577;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12061v1 Announce Type: cross  Abstract: Many leading language models (LMs) use high-intensity computational resources both during training and execution. This poses the challenge of lowering resource costs for deployment and faster execution of decision-making tasks among others. We introduce a novel plug-and-play LM framework named Language Optimising Network Distribution (LONDI) framework. LONDI learns to selectively employ large LMs only where complex decision-making and reasoning are required while using low-resource LMs everywhere else. LONDI consists of a system of two (off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning module that uses switching controls to quickly learn which system states to call the LLM. We then introduce a variant of LONDI that maintains budget constraints on LLM calls and hence its resource usage. Theoretically, we prove LONDI learns the subset of system states to activate the LLM required to solve the task. We then prove
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#26368;&#23567;&#26497;&#23567;&#36951;&#25022;&#30340;&#22810;&#23545;&#25968;&#32553;&#25918;&#38382;&#39064;&#65292;&#36890;&#36807;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#23454;&#29616;&#23545;&#35774;&#35745;&#30697;&#38453;&#29305;&#24449;&#20540;&#20851;&#31995;&#30340;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#32047;&#31215;&#36951;&#25022;&#30340;&#23545;&#25968;&#32553;&#25918;&#12290;</title><link>https://arxiv.org/abs/2402.12042</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#23545;&#25968;&#26497;&#23567;&#26497;&#23567;&#36951;&#25022;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Linear bandits with polylogarithmic minimax regret
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#26368;&#23567;&#26497;&#23567;&#36951;&#25022;&#30340;&#22810;&#23545;&#25968;&#32553;&#25918;&#38382;&#39064;&#65292;&#36890;&#36807;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#23454;&#29616;&#23545;&#35774;&#35745;&#30697;&#38453;&#29305;&#24449;&#20540;&#20851;&#31995;&#30340;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#32047;&#31215;&#36951;&#25022;&#30340;&#23545;&#25968;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#32447;&#24615;&#38543;&#26426;&#36172;&#21338;&#26426;&#30340;&#22122;&#22768;&#27169;&#22411;&#65292;&#23545;&#20110;&#35813;&#27169;&#22411;&#65292;&#24403;&#25105;&#20204;&#36873;&#25321;&#36234;&#26469;&#36234;&#25509;&#36817;&#26410;&#30693;&#21521;&#37327;&#30340;&#21333;&#20301;&#29699;&#19978;&#30340;&#21160;&#20316;&#26102;&#65292;&#20122;&#39640;&#26031;&#22122;&#22768;&#21442;&#25968;&#20197;&#32447;&#24615;&#26041;&#24335;&#28040;&#22833;&#12290;&#25105;&#20204;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#24341;&#20837;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20854;&#22312;&#26102;&#38388;&#38271;&#24230;$T$&#30340;&#24773;&#20917;&#19979;&#21576;&#23545;&#25968;$^3&#65288;T&#65289;$&#30340;&#26368;&#23567;&#36951;&#25022;&#32553;&#25918;&#65292;&#19982;&#20856;&#22411;&#36172;&#21338;&#26426;&#31639;&#27861;&#30340;&#24179;&#26041;&#26681;&#36951;&#25022;&#32553;&#25918;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#22522;&#20110;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#36890;&#36807;&#20960;&#20309;&#35770;&#35777;&#23454;&#29616;&#20102;&#35774;&#35745;&#30697;&#38453;$V_t$&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;$t$&#22788;&#30340;&#29305;&#24449;&#20540;&#20851;&#31995;$\lambda_{\min} ( V_t ) = \Omega (\sqrt{\lambda_{\max}(V_t ) })$&#65292;&#36825;&#20123;&#20960;&#20309;&#35770;&#35777;&#19982;&#22122;&#22768;&#27169;&#22411;&#26080;&#20851;&#65292;&#24182;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20005;&#26684;&#25511;&#21046;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#26399;&#26395;&#36951;&#25022;&#20026;$O(\frac1{t})$&#30340;&#25968;&#37327;&#32423;&#65292;&#20174;&#32780;&#23548;&#33268;&#32047;&#31215;&#36951;&#25022;&#30340;&#23545;&#25968;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12042v1 Announce Type: cross  Abstract: We study a noise model for linear stochastic bandits for which the subgaussian noise parameter vanishes linearly as we select actions on the unit sphere closer and closer to the unknown vector. We introduce an algorithm for this problem that exhibits a minimax regret scaling as $\log^3(T)$ in the time horizon $T$, in stark contrast the square root scaling of this regret for typical bandit algorithms. Our strategy, based on weighted least-squares estimation, achieves the eigenvalue relation $\lambda_{\min} ( V_t ) = \Omega (\sqrt{\lambda_{\max}(V_t ) })$ for the design matrix $V_t$ at each time step $t$ through geometrical arguments that are independent of the noise model and might be of independent interest. This allows us to tightly control the expected regret in each time step to be of the order $O(\frac1{t})$, leading to the logarithmic scaling of the cumulative regret.
&lt;/p&gt;</description></item><item><title>&#26102;&#38388;&#24207;&#21015;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12035</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#22686;&#37327;&#24335;&#23398;&#20064;: &#22522;&#20934;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Class-incremental Learning for Time Series: Benchmark and Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12035
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29615;&#22659;&#26412;&#36136;&#19978;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#32463;&#24120;&#20250;&#38543;&#26102;&#38388;&#24341;&#20837;&#26032;&#30340;&#31867;&#21035;&#12290;&#36825;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#23588;&#20026;&#24120;&#35265;&#65292;&#27604;&#22914;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20986;&#29616;&#26032;&#30340;&#30142;&#30149;&#20998;&#31867;&#65292;&#25110;&#32773;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#28155;&#21152;&#26032;&#30340;&#27963;&#21160;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#19968;&#20010;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22320;&#21560;&#25910;&#26032;&#30340;&#31867;&#21035;&#65292;&#21516;&#26102;&#36991;&#20813;&#23545;&#26087;&#31867;&#21035;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#23601;&#24341;&#21457;&#20102;&#22686;&#37327;&#24335;&#23398;&#20064;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#36827;&#23637;&#65292;&#20294;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22686;&#37327;&#24335;&#23398;&#20064;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#12290;&#29616;&#26377;&#30740;&#31350;&#23384;&#22312;&#23454;&#39564;&#35774;&#35745;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#23545;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#26102;&#38388;&#24207;&#21015;&#22686;&#37327;&#23398;&#20064;&#65288;TSCIL&#65289;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#20854;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#35206;&#30422;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12035v1 Announce Type: cross  Abstract: Real-world environments are inherently non-stationary, frequently introducing new classes over time. This is especially common in time series classification, such as the emergence of new disease classification in healthcare or the addition of new activities in human activity recognition. In such cases, a learning system is required to assimilate novel classes effectively while avoiding catastrophic forgetting of the old ones, which gives rise to the Class-incremental Learning (CIL) problem. However, despite the encouraging progress in the image and language domains, CIL for time series data remains relatively understudied. Existing studies suffer from inconsistent experimental designs, necessitating a comprehensive evaluation and benchmarking of methods across a wide range of datasets. To this end, we first present an overview of the Time Series Class-incremental Learning (TSCIL) problem, highlight its unique challenges, and cover the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#39057;&#29575;&#31354;&#38388;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12026</link><description>&lt;p&gt;
&#20174;&#21518;&#38376;&#27602;&#21270;&#25968;&#25454;&#38598;&#20013;&#36890;&#36807;&#38477;&#39057;&#31354;&#38388;&#33719;&#21462;&#28165;&#27905;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12026
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#39057;&#29575;&#31354;&#38388;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;LMs&#30340;&#21487;&#38752;&#24615;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#27602;&#21270;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;LMs&#26102;&#20943;&#36731;&#21518;&#38376;&#23398;&#20064;&#65292;&#20294;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#25269;&#24481;&#22797;&#26434;&#30340;&#21518;&#38376;&#25915;&#20987;&#26102;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20613;&#37324;&#21494;&#20998;&#26512;&#30740;&#31350;&#20102;&#39057;&#29575;&#31354;&#38388;&#20013;&#21518;&#38376;LMs&#30340;&#23398;&#20064;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#27602;&#21270;&#25968;&#25454;&#38598;&#19978;&#21576;&#29616;&#30340;&#21518;&#38376;&#26144;&#23556;&#30456;&#27604;&#28165;&#27905;&#26144;&#23556;&#26356;&#20542;&#21521;&#20110;&#36739;&#20302;&#39057;&#29575;&#65292;&#23548;&#33268;&#21518;&#38376;&#26144;&#23556;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#65292;&#23427;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#37096;&#32626;&#22810;&#20010;&#24452;&#21521;&#32553;&#25918;&#65292;&#20302;&#31209;&#36866;&#24212;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#22312;&#26356;&#26032;&#21442;&#25968;&#26102;&#36827;&#19968;&#27493;&#35843;&#25972;&#26799;&#24230;&#12290;&#36890;&#36807;&#38477;&#39057;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12026v1 Announce Type: cross  Abstract: Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-4&#27169;&#22411;&#22686;&#24378;&#26234;&#33021;&#21512;&#32422;&#23433;&#20840;&#23457;&#35745;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#27604;&#36739;&#20854;&#22312;&#35782;&#21035;&#24120;&#35265;&#28431;&#27934;&#12289;&#20195;&#30721;&#35299;&#26512;&#21644;&#28431;&#27934;&#25429;&#33719;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#21319;&#26234;&#33021;&#21512;&#32422;&#23433;&#20840;&#24615;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.12023</link><description>&lt;p&gt;
&#22522;&#20110;Chain of Thought&#35780;&#20272;ChatGPT&#30340;&#26234;&#33021;&#21512;&#32422;&#23457;&#35745;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on Chain of Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-4&#27169;&#22411;&#22686;&#24378;&#26234;&#33021;&#21512;&#32422;&#23433;&#20840;&#23457;&#35745;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#27604;&#36739;&#20854;&#22312;&#35782;&#21035;&#24120;&#35265;&#28431;&#27934;&#12289;&#20195;&#30721;&#35299;&#26512;&#21644;&#28431;&#27934;&#25429;&#33719;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#21319;&#26234;&#33021;&#21512;&#32422;&#23433;&#20840;&#24615;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#21512;&#32422;&#20316;&#20026;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#30830;&#20445;&#20132;&#26131;&#33258;&#21160;&#21270;&#21644;&#36981;&#23432;&#21327;&#35758;&#35268;&#21017;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26234;&#33021;&#21512;&#32422;&#23481;&#26131;&#21463;&#21040;&#23433;&#20840;&#28431;&#27934;&#30340;&#24433;&#21709;&#65292;&#19968;&#26086;&#34987;&#21033;&#29992;&#65292;&#23601;&#21487;&#33021;&#23548;&#33268;&#37325;&#22823;&#36164;&#20135;&#25439;&#22833;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;GPT-4&#27169;&#22411;&#22686;&#24378;&#26234;&#33021;&#21512;&#32422;&#23433;&#20840;&#23457;&#35745;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#26469;&#33258;SolidiFI-benchmark&#28431;&#27934;&#24211;&#30340;35&#20010;&#26234;&#33021;&#21512;&#32422;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;732&#20010;&#28431;&#27934;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#20116;&#31181;&#28431;&#27934;&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#35780;&#20272;GPT-4&#35782;&#21035;&#19971;&#31181;&#24120;&#35265;&#28431;&#27934;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#20843;&#32452;&#26234;&#33021;&#21512;&#32422;&#23457;&#35745;&#25253;&#21578;&#30340;CoT (Chain of Thought)&#25552;&#31034;&#27169;&#25311;&#19987;&#19994;&#23457;&#35745;&#20154;&#21592;&#30340;&#23457;&#35745;&#36807;&#31243;&#65292;&#35780;&#20272;&#20102;GPT-4&#22312;&#20195;&#30721;&#35299;&#26512;&#21644;&#28431;&#27934;&#25429;&#33719;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;GPT-4&#32534;&#20889;Solidity Pr&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12023v1 Announce Type: cross  Abstract: Smart contracts, as a key component of blockchain technology, play a crucial role in ensuring the automation of transactions and adherence to protocol rules. However, smart contracts are susceptible to security vulnerabilities, which, if exploited, can lead to significant asset losses. This study explores the potential of enhancing smart contract security audits using the GPT-4 model. We utilized a dataset of 35 smart contracts from the SolidiFI-benchmark vulnerability library, containing 732 vulnerabilities, and compared it with five other vulnerability detection tools to evaluate GPT-4's ability to identify seven common types of vulnerabilities. Moreover, we assessed GPT-4's performance in code parsing and vulnerability capture by simulating a professional auditor's auditing process using CoT(Chain of Thought) prompts based on the audit reports of eight groups of smart contracts. We also evaluated GPT-4's ability to write Solidity Pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#31934;&#33521;&#35757;&#32451;&#26679;&#26412;&#65292;&#27604;&#36739;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#33021;&#25928;&#20248;&#21183;&#65292;&#25506;&#35752;&#20854;&#23545;&#21487;&#25345;&#32493;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12010</link><description>&lt;p&gt;
&#20351;&#29992;&#31934;&#33521;&#26679;&#26412;&#35757;&#32451;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Green AI Models Using Elite Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12010
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#31934;&#33521;&#35757;&#32451;&#26679;&#26412;&#65292;&#27604;&#36739;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#33021;&#25928;&#20248;&#21183;&#65292;&#25506;&#35752;&#20854;&#23545;&#21487;&#25345;&#32493;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#27169;&#22411;&#35757;&#32451;&#37327;&#30340;&#22823;&#24133;&#22686;&#21152;&#20855;&#26377;&#37325;&#35201;&#30340;&#29615;&#22659;&#24433;&#21709;&#65292;&#36825;&#38656;&#35201;&#26356;&#33410;&#33021;&#39640;&#25928;&#21644;&#21487;&#25345;&#32493;&#30340;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#12290;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#23637;&#29616;&#20986;&#35757;&#32451;&#33410;&#33021;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#32780;&#23454;&#20363;&#36873;&#25321;&#26041;&#27861;&#23637;&#31034;&#20102;&#20351;&#29992;&#26368;&#23567;&#21270;&#35757;&#32451;&#38598;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#33021;&#21147;&#19988;&#24615;&#33021;&#19979;&#38477;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#38024;&#23545;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#31934;&#33521;&#35757;&#32451;&#26679;&#26412;&#65292;&#27604;&#36739;&#27169;&#22411;&#24615;&#33021;&#21644;&#33410;&#33021;&#25928;&#30410;&#19982;&#20856;&#22411;&#27169;&#22411;&#35757;&#32451;&#23454;&#36341;&#30340;&#24046;&#24322;&#65292;&#24182;&#30740;&#31350;&#36825;&#19968;&#26694;&#26550;&#23545;&#20419;&#36827;&#21487;&#25345;&#32493;&#27169;&#22411;&#35757;&#32451;&#23454;&#36341;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12010v1 Announce Type: cross  Abstract: The substantial increase in AI model training has considerable environmental implications, mandating more energy-efficient and sustainable AI practices. On the one hand, data-centric approaches show great potential towards training energy-efficient AI models. On the other hand, instance selection methods demonstrate the capability of training AI models with minimised training sets and negligible performance degradation. Despite the growing interest in both topics, the impact of data-centric training set selection on energy efficiency remains to date unexplored. This paper presents an evolutionary-based sampling framework aimed at (i) identifying elite training samples tailored for datasets and model pairs, (ii) comparing model performance and energy efficiency gains against typical model training practice, and (iii) investigating the feasibility of this framework for fostering sustainable model training practices. To evaluate the propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#32676;&#38598;&#24615;&#33021;&#23545;&#28155;&#21152;&#21040;&#22522;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#22024;&#26434;&#19981;&#30456;&#20851;&#21464;&#37327;&#30340;&#25935;&#24863;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.12008</link><description>&lt;p&gt;
&#32676;&#38598;&#24230;&#37327;&#23545;&#26080;&#20851;&#29305;&#24449;&#30340;&#25935;&#24863;&#24230;
&lt;/p&gt;
&lt;p&gt;
Cluster Metric Sensitivity to Irrelevant Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#32676;&#38598;&#24615;&#33021;&#23545;&#28155;&#21152;&#21040;&#22522;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#22024;&#26434;&#19981;&#30456;&#20851;&#21464;&#37327;&#30340;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#31639;&#27861;&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#29992;&#20110;&#25968;&#25454;&#25506;&#32034;&#21644;&#21457;&#29616;&#12290;&#25216;&#26415;&#36827;&#27493;&#23548;&#33268;&#25968;&#25454;&#22312;&#23481;&#37327;&#12289;&#32500;&#24230;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#19981;&#26029;&#22686;&#38271;&#12290;&#36825;&#20026;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#20102;&#24040;&#22823;&#26426;&#20250;&#65292;&#22240;&#20026;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#30446;&#30340;&#30340;&#35810;&#38382;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#27604;&#22914;&#22312;&#32473;&#23450;&#20219;&#21153;&#20013;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#12290;&#22312;&#30417;&#30563;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#21033;&#29992;&#21508;&#31181;&#26041;&#27861;&#20248;&#21270;&#20219;&#21153;&#30446;&#26631;&#65288;&#20363;&#22914;&#20998;&#31867;&#20934;&#30830;&#24615;&#65289;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#22312;&#26080;&#30417;&#30563;&#38382;&#39064;&#20013;&#65292;&#36825;&#20123;&#24037;&#20855;&#24182;&#19981;readily available&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#26080;&#27861;&#23450;&#37327;&#22320;&#34913;&#37327;&#26080;&#26631;&#31614;&#20219;&#21153;&#20013;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32676;&#38598;&#24615;&#33021;&#23545;&#22024;&#26434;&#30340;&#19981;&#30456;&#20851;&#21464;&#37327;&#36827;&#34892;&#36845;&#20195;&#28155;&#21152;&#21040;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#32676;&#38598;&#30340;&#22522;&#32447;&#25968;&#25454;&#38598;&#30340;&#25935;&#24863;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#26080;&#20851;&#21464;&#37327;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12008v1 Announce Type: cross  Abstract: Clustering algorithms are used extensively in data analysis for data exploration and discovery. Technological advancements lead to continually growth of data in terms of volume, dimensionality and complexity. This provides great opportunities in data analytics as the data can be interrogated for many different purposes. This however leads challenges, such as identification of relevant features for a given task. In supervised tasks, one can utilise a number of methods to optimise the input features for the task objective (e.g. classification accuracy). In unsupervised problems, such tools are not readily available, in part due to an inability to quantify feature relevance in unlabeled tasks. In this paper, we investigate the sensitivity of clustering performance noisy uncorrelated variables iteratively added to baseline datasets with well defined clusters. We show how different types of irrelevant variables can impact the outcome of a c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25277;&#21462;&#24335;&#30693;&#35782;&#22270;&#35889;&#24635;&#32467;&#30340;&#24212;&#29992;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#20998;&#31867;&#65292;&#20026;&#26410;&#26469;&#26041;&#21521;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2402.12001</link><description>&lt;p&gt;
&#23545;&#25277;&#21462;&#24335;&#30693;&#35782;&#22270;&#35889;&#24635;&#32467;&#30340;&#35843;&#26597;&#65306;&#24212;&#29992;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey on Extractive Knowledge Graph Summarization: Applications, Approaches, Evaluation, and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25277;&#21462;&#24335;&#30693;&#35782;&#22270;&#35889;&#24635;&#32467;&#30340;&#24212;&#29992;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#20998;&#31867;&#65292;&#20026;&#26410;&#26469;&#26041;&#21521;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#19981;&#26029;&#22686;&#38271;&#65292;&#25277;&#21462;&#24335;KG&#24635;&#32467;&#25104;&#20026;&#19968;&#39033;&#28909;&#38376;&#20219;&#21153;&#12290;&#26088;&#22312;&#25552;&#28860;&#20855;&#26377;&#27987;&#32553;&#20449;&#24687;&#30340;&#32039;&#20945;&#23376;&#22270;&#65292;&#26377;&#21161;&#20110;&#21508;&#31181;&#19979;&#28216;&#22522;&#20110;KG&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26159;&#39318;&#25209;&#23545;&#20854;&#24212;&#29992;&#25552;&#20379;&#31995;&#32479;&#27010;&#36848;&#24182;&#20174;&#36328;&#23398;&#31185;&#30740;&#31350;&#20013;&#23450;&#20041;&#29616;&#26377;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#20043;&#19968;&#12290;&#22522;&#20110;&#25105;&#20204;&#24191;&#27867;&#32780;&#27604;&#36739;&#30340;&#35780;&#35770;&#65292;&#26410;&#26469;&#26041;&#21521;&#20063;&#24050;&#38138;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12001v1 Announce Type: new  Abstract: With the continuous growth of large Knowledge Graphs (KGs), extractive KG summarization becomes a trending task. Aiming at distilling a compact subgraph with condensed information, it facilitates various downstream KG-based tasks. In this survey paper, we are among the first to provide a systematic overview of its applications and define a taxonomy for existing methods from its interdisciplinary studies. Future directions are also laid out based on our extensive and comparative review.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#65292;&#38381;&#28304;&#27169;&#22411;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#19982;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.11997</link><description>&lt;p&gt;
&#22238;&#24518;&#37027;&#19968;&#24180;&#21457;&#29983;&#30340;&#20107;&#20214;&#65311;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11997
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#65292;&#38381;&#28304;&#27169;&#22411;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#19982;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#25512;&#29702;&#21644;&#20445;&#30041;&#26102;&#38388;&#20449;&#24687;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#20107;&#20214;&#30340;&#39034;&#24207;&#24615;&#23545;&#20851;&#38190;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#25968;&#25454;&#38598;\textbf{TempUN}&#19978;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#26102;&#38388;&#20445;&#30041;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26174;&#33879;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#38381;&#28304;&#27169;&#22411;&#26356;&#39057;&#32321;&#22320;&#26174;&#31034;&#20986;&#30693;&#35782;&#24046;&#36317;&#65292;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#21644;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25506;&#32034;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;&#24182;&#27809;&#26377;&#24102;&#26469;&#20027;&#35201;&#24615;&#33021;&#25913;&#36827;&#12290;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#33719;&#24471;&#65288;https://github.com/lingoiitgn/TempUN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11997v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, \textbf{TempUN}, to reveal significant limitations in temporal retention and reasoning abilities. Interestingly, closed-source models indicate knowledge gaps more frequently, potentially suggesting a trade-off between uncertainty awareness and incorrect responses. Further, exploring various fine-tuning approaches yielded no major performance improvements. The associated dataset and code are available at the following URL (https://github.com/lingoiitgn/TempUN).
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27178;&#21521;&#36830;&#25509;&#21644;Hebbian&#23398;&#20064;&#30340;&#31070;&#32463;&#25805;&#20316;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#25237;&#24433;&#20445;&#25252;&#30693;&#35782;</title><link>https://arxiv.org/abs/2402.11984</link><description>&lt;p&gt;
&#22522;&#20110;Hebbian&#23398;&#20064;&#30340;&#27491;&#20132;&#25237;&#24433;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11984
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27178;&#21521;&#36830;&#25509;&#21644;Hebbian&#23398;&#20064;&#30340;&#31070;&#32463;&#25805;&#20316;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#25237;&#24433;&#20445;&#25252;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#65292;&#23545;&#20110;&#33021;&#25928;&#39640;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#19982;&#32456;&#36523;&#19981;&#26029;&#23398;&#20064;&#19981;&#21516;&#65292;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#31070;&#32463;&#25805;&#20316;&#22914;&#20309;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#26159;&#20154;&#24037;&#26234;&#33021;&#21644;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27178;&#21521;&#36830;&#25509;&#21644;Hebbian&#23398;&#20064;&#30340;&#31070;&#32463;&#25805;&#20316;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25237;&#24433;&#20445;&#25252;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11984v1 Announce Type: cross  Abstract: Neuromorphic computing with spiking neural networks is promising for energy-efficient artificial intelligence (AI) applications. However, different from humans who continually learn different tasks in a lifetime, neural network models suffer from catastrophic forgetting. How could neuronal operations solve this problem is an important question for AI and neuroscience. Many previous studies draw inspiration from observed neuroscience phenomena and propose episodic replay or synaptic metaplasticity, but they are not guaranteed to explicitly preserve knowledge for neuron populations. Other works focus on machine learning methods with more mathematical grounding, e.g., orthogonal projection on high dimensional spaces, but there is no neural correspondence for neuromorphic computing. In this work, we develop a new method with neuronal operations based on lateral connections and Hebbian learning, which can protect knowledge by projecting act
&lt;/p&gt;</description></item><item><title>&#22238;&#24402;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#19968;&#30452;&#34987;&#24573;&#35270;&#65292;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23450;&#20041;&#65292;&#23637;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#20849;&#21516;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.11963</link><description>&lt;p&gt;
&#22238;&#24402;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Imbalance in Regression Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11963
&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#19968;&#30452;&#34987;&#24573;&#35270;&#65292;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23450;&#20041;&#65292;&#23637;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#20849;&#21516;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23601;&#20998;&#31867;&#32780;&#35328;&#65292;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#65292;&#24182;&#19988;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#22312;&#22238;&#24402;&#20013;&#23384;&#22312;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#21516;&#26679;&#37325;&#35201;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#34987;&#24573;&#35270;&#65306;&#30001;&#20110;&#25968;&#25454;&#38598;&#30446;&#26631;&#20998;&#24067;&#20013;&#30340;&#27424;&#34920;&#31034;&#21644;&#36807;&#22810;&#34920;&#31034;&#65292;&#22238;&#24402;&#22120;&#23481;&#26131;&#36864;&#21270;&#20026;&#26420;&#32032;&#27169;&#22411;&#65292;&#31995;&#32479;&#22320;&#24573;&#30053;&#19981;&#24120;&#35265;&#30340;&#35757;&#32451;&#25968;&#25454;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#32463;&#24120;&#35265;&#21040;&#30340;&#30446;&#26631;&#19978;&#36827;&#34892;&#36807;&#24230;&#34920;&#31034;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#24471;&#20986;&#30340;&#35265;&#35299;&#21046;&#23450;&#20102;&#23545;&#22238;&#24402;&#20013;&#19981;&#24179;&#34913;&#30340;&#39318;&#20010;&#23450;&#20041;&#65292;&#25105;&#20204;&#23637;&#31034;&#36825;&#26159;&#20998;&#31867;&#20013;&#24120;&#29992;&#30340;&#19981;&#24179;&#34913;&#24230;&#37327;&#30340;&#27867;&#21270;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#24076;&#26395;&#23558;&#20851;&#27880;&#28857;&#36716;&#21521;&#22238;&#24402;&#20013;&#34987;&#24573;&#35270;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#22880;&#23450;&#20849;&#21516;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11963v1 Announce Type: cross  Abstract: For classification, the problem of class imbalance is well known and has been extensively studied. In this paper, we argue that imbalance in regression is an equally important problem which has so far been overlooked: Due to under- and over-representations in a data set's target distribution, regressors are prone to degenerate to naive models, systematically neglecting uncommon training data and over-representing targets seen often during training. We analyse this problem theoretically and use resulting insights to develop a first definition of imbalance in regression, which we show to be a generalisation of the commonly employed imbalance measure in classification. With this, we hope to turn the spotlight on the overlooked problem of imbalance in regression and to provide common ground for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DB-LLM&#30340;&#26032;&#39062;&#21452;&#20108;&#20540;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28789;&#27963;&#21452;&#20108;&#20540;&#21270;(FDB)&#26469;&#24179;&#34913;2&#20301;&#23485;&#24230;&#30340;&#31934;&#24230;&#20248;&#21183;&#21644;&#20108;&#20540;&#21270;&#30340;&#25928;&#29575;&#20248;&#21183;&#65292;&#20174;&#32780;&#22312;&#25552;&#39640;LLMs&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11960</link><description>&lt;p&gt;
DB-LLM: &#39640;&#25928;LLM&#30340;&#20934;&#30830;&#21452;&#20108;&#20540;&#21270;
&lt;/p&gt;
&lt;p&gt;
DB-LLM: Accurate Dual-Binarization for Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DB-LLM&#30340;&#26032;&#39062;&#21452;&#20108;&#20540;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28789;&#27963;&#21452;&#20108;&#20540;&#21270;(FDB)&#26469;&#24179;&#34913;2&#20301;&#23485;&#24230;&#30340;&#31934;&#24230;&#20248;&#21183;&#21644;&#20108;&#20540;&#21270;&#30340;&#25928;&#29575;&#20248;&#21183;&#65292;&#20174;&#32780;&#22312;&#25552;&#39640;LLMs&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#33879;&#25512;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#28982;&#32780;&#39640;&#26114;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#37327;&#21270;&#25104;&#20026;&#25913;&#21892;LLMs&#35745;&#31639;&#25928;&#29575;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36229;&#20302;&#27604;&#29305;&#37327;&#21270;&#24635;&#26159;&#23548;&#33268;&#20005;&#37325;&#30340;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#32531;&#35299;&#20102;&#36229;&#20302;&#27604;&#29305;&#37327;&#21270;&#30340;&#24494;&#35266;&#21644;&#23439;&#35266;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLMs&#21452;&#20108;&#20540;&#21270;&#26041;&#27861;&#65292;&#21363;DB-LLM&#12290;&#23545;&#20110;&#24494;&#35266;&#23618;&#38754;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;2&#20301;&#23485;&#24230;&#30340;&#20934;&#30830;&#24615;&#20248;&#21183;&#21644;&#20108;&#20540;&#21270;&#30340;&#25928;&#29575;&#20248;&#21183;&#65292;&#24341;&#20837;&#20102;&#28789;&#27963;&#21452;&#20108;&#20540;&#21270;(FDB)&#12290;&#36890;&#36807;&#23558;2&#20301;&#37327;&#21270;&#26435;&#37325;&#20998;&#20026;&#20004;&#32452;&#29420;&#31435;&#30340;&#20108;&#36827;&#21046;&#25968;&#38598;&#65292;FDB&#30830;&#20445;&#20102;&#34920;&#31034;&#30340;&#20934;&#30830;&#24615;&#24182;&#24341;&#20837;&#20102;&#28789;&#27963;&#24615;&#65292;&#21033;&#29992;&#20108;&#20540;&#21270;&#30340;&#39640;&#25928;&#20301;&#25805;&#20316;&#21516;&#26102;&#20445;&#30041;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11960v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly advanced the field of natural language processing, while the expensive memory and computation consumption impede their practical deployment. Quantization emerges as one of the most effective methods for improving the computational efficiency of LLMs. However, existing ultra-low-bit quantization always causes severe accuracy drops. In this paper, we empirically relieve the micro and macro characteristics of ultra-low bit quantization and present a novel Dual-Binarization method for LLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage of 2-bit-width and the efficiency advantage of binarization into account, introducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized weights into two independent sets of binaries, FDB ensures the accuracy of representations and introduces flexibility, utilizing the efficient bitwise operations of binarization while reta
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;SEASON&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#27010;&#25324;&#24615;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#19982;BART&#12289;PEGASUS&#21644;ProphetNet&#31561;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#30528;&#37325;&#20998;&#26512;&#20102;&#36130;&#32463;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11955</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;&#27010;&#25324;&#24615;&#25688;&#35201;&#29983;&#25104;&#30340;&#20851;&#38190;&#24615;&#20998;&#37197;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Multidomain Abstractive Summarization Using Salience Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11955
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;SEASON&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#27010;&#25324;&#24615;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#19982;BART&#12289;PEGASUS&#21644;ProphetNet&#31561;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#30528;&#37325;&#20998;&#26512;&#20102;&#36130;&#32463;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;SEASON&#65288;Salience Allocation as Guidance for Abstractive SummarizatiON&#65289;&#25216;&#26415;&#30340;&#35270;&#35282;&#25506;&#35752;&#20102;&#27010;&#25324;&#24615;&#25991;&#26412;&#25688;&#35201;&#29983;&#25104;&#30340;&#39046;&#22495;&#65292;&#35813;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20851;&#38190;&#24615;&#20998;&#37197;&#25216;&#26415;&#26469;&#22686;&#24378;&#25688;&#35201;&#29983;&#25104;&#12290;&#30740;&#31350;&#36890;&#36807;&#19982;BART&#12289;PEGASUS&#21644;ProphetNet&#31561;&#30693;&#21517;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#36825;&#20123;&#27169;&#22411;&#22343;&#38024;&#23545;&#21508;&#31181;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#26469;&#35780;&#20272;SEASON&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#35780;&#20272;&#20351;&#29992;&#21253;&#25324;CNN/Dailymail&#12289;SAMSum&#21644;&#22522;&#20110;&#36130;&#32463;&#26032;&#38395;&#30340;Event-Driven Trading (EDT)&#22312;&#20869;&#30340;&#22810;&#31181;&#25968;&#25454;&#38598;&#36827;&#34892;&#65292;&#29305;&#21035;&#20851;&#27880;&#21253;&#21547;&#22823;&#37327;&#26032;&#38395;&#25991;&#31456;&#30340;&#36130;&#32463;&#25968;&#25454;&#38598;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2020/03/01&#33267;2021/05/06&#12290;&#26412;&#25991;&#37319;&#29992;&#22810;&#31181;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;ROUGE&#12289;METEOR&#12289;BERTScore&#21644;MoverScore&#65289;&#26469;&#35780;&#20272;&#36825;&#20123;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#29983;&#25104;&#27010;&#25324;&#24615;&#25688;&#35201;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#25351;&#26631;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#20248;&#21155;&#30340;&#28145;&#20837;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11955v1 Announce Type: cross  Abstract: This paper explores the realm of abstractive text summarization through the lens of the SEASON (Salience Allocation as Guidance for Abstractive SummarizatiON) technique, a model designed to enhance summarization by leveraging salience allocation techniques. The study evaluates SEASON's efficacy by comparing it with prominent models like BART, PEGASUS, and ProphetNet, all fine-tuned for various text summarization tasks. The assessment is conducted using diverse datasets including CNN/Dailymail, SAMSum, and Financial-news based Event-Driven Trading (EDT), with a specific focus on a financial dataset containing a substantial volume of news articles from 2020/03/01 to 2021/05/06. This paper employs various evaluation metrics such as ROUGE, METEOR, BERTScore, and MoverScore to evaluate the performance of these models fine-tuned for generating abstractive summaries. The analysis of these metrics offers a thorough insight into the strengths a
&lt;/p&gt;</description></item><item><title>Mini-Hes&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;mini-block&#23545;&#35282;&#40657;&#22622;&#26080;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#20108;&#38454;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22823;&#25968;&#25454;&#37327;&#19979;&#20108;&#38454;&#31639;&#27861;&#21487;&#34892;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11948</link><description>&lt;p&gt;
Mini-Hes&#65306;&#19968;&#31181;&#21487;&#24182;&#34892;&#21270;&#30340;&#20108;&#38454;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11948
&lt;/p&gt;
&lt;p&gt;
Mini-Hes&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;mini-block&#23545;&#35282;&#40657;&#22622;&#26080;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#20108;&#38454;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22823;&#25968;&#25454;&#37327;&#19979;&#20108;&#38454;&#31639;&#27861;&#21487;&#34892;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22823;&#37327;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#22312;&#35768;&#22810;&#19982;&#22823;&#25968;&#25454;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#33258;&#28982;&#26159;&#39640;&#32500;&#19988;&#19981;&#23436;&#25972;&#30340;&#65288;HDI&#65289;&#12290;&#29992;&#25143;&#30340;&#34892;&#20026;&#29305;&#24449;&#38544;&#34255;&#22312;&#36825;&#20123;&#20132;&#20114;&#20013;&#65292;&#22240;&#27492;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;HDI&#25968;&#25454;&#26159;&#29702;&#35299;&#29992;&#25143;&#34892;&#20026;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#65288;LFA&#65289;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#34920;&#31034;HDI&#25968;&#25454;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290; LFA&#27169;&#22411;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20854;&#35757;&#32451;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#20854;&#35757;&#32451;&#36807;&#31243;&#20013;&#21516;&#26102;&#21253;&#21547;&#23616;&#37096;&#26354;&#29575;&#21644;&#39044;&#22788;&#29702;&#26799;&#24230;&#21487;&#20197;&#27604;&#20351;&#29992;&#19968;&#38454;&#26041;&#27861;&#26500;&#24314;&#30340;LFA&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#20108;&#38454;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;LFA&#27169;&#22411;&#30340;mini-block&#23545;&#35282;&#40657;&#22622;&#26080;&#32422;&#26463;&#65288;Mini-Hes&#65289;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11948v1 Announce Type: cross  Abstract: Interactions among large number of entities is naturally high-dimensional and incomplete (HDI) in many big data related tasks. Behavioral characteristics of users are hidden in these interactions, hence, effective representation of the HDI data is a fundamental task for understanding user behaviors. Latent factor analysis (LFA) model has proven to be effective in representing HDI data. The performance of an LFA model relies heavily on its training process, which is a non-convex optimization. It has been proven that incorporating local curvature and preprocessing gradients during its training process can lead to superior performance compared to LFA models built with first-order family methods. However, with the escalation of data volume, the feasibility of second-order algorithms encounters challenges. To address this pivotal issue, this paper proposes a mini-block diagonal hessian-free (Mini-Hes) optimization for building an LFA model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22242;&#38431;QUST&#22312;SemEval-2024&#20219;&#21153;8&#20013;&#30340;&#21442;&#19982;&#24773;&#20917;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#28165;&#27927;&#25552;&#39640;&#20102;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#22312;&#21333;&#35821;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#22810;&#31181;&#26041;&#27861;&#24182;&#26368;&#32456;&#37319;&#29992;&#22534;&#21472;&#38598;&#25104;&#27169;&#22411;&#65292;&#26368;&#32456;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.11934</link><description>&lt;p&gt;
Team QUST&#22312;SemEval-2024&#20219;&#21153;8&#20013;&#30340;&#30740;&#31350;&#65306;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#26041;&#27861;&#23545;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22242;&#38431;QUST&#22312;SemEval-2024&#20219;&#21153;8&#20013;&#30340;&#21442;&#19982;&#24773;&#20917;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#28165;&#27927;&#25552;&#39640;&#20102;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#22312;&#21333;&#35821;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#22810;&#31181;&#26041;&#27861;&#24182;&#26368;&#32456;&#37319;&#29992;&#22534;&#21472;&#38598;&#25104;&#27169;&#22411;&#65292;&#26368;&#32456;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22242;&#38431;QUST&#22312;SemEval 2024&#20219;&#21153;8&#20013;&#30340;&#21442;&#19982;&#24773;&#20917;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22686;&#24378;&#21644;&#28165;&#27927;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#22312;&#21333;&#35821;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12289;&#22810;&#23610;&#24230;&#27491;&#36127;&#26080;&#26631;&#35760;&#26694;&#26550;&#65288;MPU&#65289;&#12289;&#24494;&#35843;&#12289;&#36866;&#37197;&#22120;&#21644;&#38598;&#25104;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#21333;&#35821;&#27169;&#22411;&#20013;&#36873;&#25321;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#23376;&#20219;&#21153;A&#21644;B&#20013;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#12290;&#26368;&#32456;&#27169;&#22411;&#37319;&#29992;&#20102;&#23558;&#24494;&#35843;&#19982;MPU&#30456;&#32467;&#21512;&#30340;&#22534;&#21472;&#38598;&#25104;&#12290;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#30340;&#23376;&#20219;&#21153;A&#20013;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#23448;&#26041;&#27979;&#35797;&#38598;&#20013;&#21462;&#24471;&#20102;&#31532;8&#21517;&#65288;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#25490;&#21517;&#31532;13&#65289;&#12290;&#25105;&#20204;&#22312;https://github.com/warmth27/SemEval2024_QUST&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11934v1 Announce Type: cross  Abstract: This paper presents the participation of team QUST in Task 8 SemEval 2024. We first performed data augmentation and cleaning on the dataset to enhance model training efficiency and accuracy. In the monolingual task, we evaluated traditional deep-learning methods, multiscale positive-unlabeled framework (MPU), fine-tuning, adapters and ensemble methods. Then, we selected the top-performing models based on their accuracy from the monolingual models and evaluated them in subtasks A and B. The final model construction employed a stacking ensemble that combined fine-tuning with MPU. Our system achieved 8th (scored 8th in terms of accuracy, officially ranked 13th) place in the official test set in multilingual settings of subtask A. We release our system code at:https://github.com/warmth27/SemEval2024_QUST
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JD2P&#30340;&#26032;&#22411;&#31163;&#32447;&#26550;&#26500;&#65292;&#36890;&#36807;&#32852;&#21512;&#25968;&#25454;&#28145;&#21270;&#21644;&#39044;&#21462;&#25216;&#26415;&#65292;&#25353;&#39034;&#24207;&#31163;&#32447;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#20197;&#20943;&#23569;&#29289;&#32852;&#32593;&#35774;&#22791;&#21521;&#36793;&#32536;&#26381;&#21153;&#22120;&#20256;&#36755;&#25968;&#25454;&#26102;&#25152;&#38656;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2402.11925</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#25968;&#25454;&#28145;&#21270;&#21644;&#39044;&#21462;&#23454;&#29616;&#33021;&#25928;&#36793;&#32536;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Energy-Efficient Edge Learning via Joint Data Deepening-and-Prefetching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11925
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JD2P&#30340;&#26032;&#22411;&#31163;&#32447;&#26550;&#26500;&#65292;&#36890;&#36807;&#32852;&#21512;&#25968;&#25454;&#28145;&#21270;&#21644;&#39044;&#21462;&#25216;&#26415;&#65292;&#25353;&#39034;&#24207;&#31163;&#32447;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#20197;&#20943;&#23569;&#29289;&#32852;&#32593;&#35774;&#22791;&#21521;&#36793;&#32536;&#26381;&#21153;&#22120;&#20256;&#36755;&#25968;&#25454;&#26102;&#25152;&#38656;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#30340;&#23454;&#26102;&#25968;&#25454;&#23545;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#23454;&#26102;&#35757;&#32451;&#65292;&#23454;&#29616;&#26080;&#22788;&#19981;&#22312;&#30340;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#30340;&#24895;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#33021;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#20256;&#36755;&#39640;&#32500;&#19988;&#24222;&#22823;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#26550;&#26500;&#65292;&#31216;&#20026;&#32852;&#21512;&#25968;&#25454;&#28145;&#21270;&#21644;&#39044;&#21462;&#65288;JD2P&#65289;&#65292;&#20854;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#25216;&#26415;&#65306;&#25968;&#25454;&#28145;&#21270;&#21644;&#39044;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11925v1 Announce Type: cross  Abstract: The vision of pervasive artificial intelligence (AI) services can be realized by training an AI model on time using real-time data collected by internet of things (IoT) devices. To this end, IoT devices require offloading their data to an edge server in proximity. However, transmitting high-dimensional and voluminous data from energy-constrained IoT devices poses a significant challenge. To address this limitation, we propose a novel offloading architecture, called joint data deepening-and-prefetching (JD2P), which is feature-by-feature offloading comprising two key techniques. The first one is data deepening, where each data sample's features are sequentially offloaded in the order of importance determined by the data embedding technique such as principle component analysis (PCA). Offloading is terminated once the already transmitted features are sufficient for accurate data classification, resulting in a reduction in the amount of tr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#22312;LLM&#20013;&#24341;&#20837;&#27714;&#35299;&#22120;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;</title><link>https://arxiv.org/abs/2402.11903</link><description>&lt;p&gt;
SoLA: &#20026;&#20102;&#26356;&#22909;&#30340;&#36923;&#36753;&#25512;&#29702;&#32780;&#23545;LLM&#36827;&#34892;&#27714;&#35299;&#23618;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11903
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#22312;LLM&#20013;&#24341;&#20837;&#27714;&#35299;&#22120;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36923;&#36753;&#25512;&#29702;&#19978;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#35797;&#22270;&#36890;&#36807;&#24037;&#20855;&#23398;&#20064;&#26469;&#25913;&#21464;&#38382;&#39064;&#27714;&#35299;&#12290;&#34429;&#28982;&#22312;&#23567;&#35268;&#27169;&#38382;&#39064;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#35268;&#27169;&#24222;&#22823;&#19988;&#34920;&#36798;&#22797;&#26434;&#65292;&#35299;&#20915;&#24037;&#19994;&#26696;&#20363;&#20173;&#28982;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;LLM&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#27714;&#35299;&#22120;&#20316;&#20026;&#26032;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;&#12290;&#22312;SoLA&#20013;&#65292;LLM&#26088;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#35782;&#21035;&#26368;&#39640;&#36136;&#37327;&#30340;&#23616;&#37096;&#35299;&#65292;&#32780;&#27714;&#35299;&#22120;&#23618;&#21017;&#19987;&#27880;&#20110;&#21021;&#22987;&#35299;&#19981;&#28385;&#36275;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#20511;&#21161;MaxSAT&#20316;&#20026;&#26725;&#26753;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#26799;&#24230;&#65292;&#20351;&#26368;&#32456;&#27169;&#22411;&#33021;&#22815;&#25910;&#25947;&#21040;&#19968;&#20010;&#28385;&#36275;&#30340;&#35299;&#25110;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#12290;&#21518;&#38376;&#29702;&#35770;&#30830;&#20445;SoLA&#33021;&#22815;&#33719;&#24471;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11903v1 Announce Type: cross  Abstract: Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurat
&lt;/p&gt;</description></item><item><title>Nyx&#26159;&#19968;&#31181;&#26032;&#22411;PDDL+&#35268;&#21010;&#22120;&#65292;&#24378;&#35843;&#36731;&#24039;&#12289;&#31616;&#21333;&#21644;&#36866;&#24212;&#24615;&#65292;&#21487;&#20197;&#23450;&#21046;&#23454;&#29616;&#36229;&#36234;PDDL+&#33539;&#22260;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11901</link><description>&lt;p&gt;
&#24102;&#26377;PDDL+&#21644;&#26356;&#22810;&#20869;&#23481;&#30340;&#23454;&#38469;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Real-World Planning with PDDL+ and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11901
&lt;/p&gt;
&lt;p&gt;
Nyx&#26159;&#19968;&#31181;&#26032;&#22411;PDDL+&#35268;&#21010;&#22120;&#65292;&#24378;&#35843;&#36731;&#24039;&#12289;&#31616;&#21333;&#21644;&#36866;&#24212;&#24615;&#65292;&#21487;&#20197;&#23450;&#21046;&#23454;&#29616;&#36229;&#36234;PDDL+&#33539;&#22260;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#35268;&#21010;&#30340;&#23454;&#38469;&#24212;&#29992;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#24314;&#27169;&#35821;&#35328;&#65292;&#20197;&#31934;&#30830;&#25429;&#25417;&#30446;&#26631;&#31995;&#32479;&#30340;&#37325;&#35201;&#22797;&#26434;&#24615;&#12290;&#28151;&#21512;&#31995;&#32479;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38543;&#22788;&#21487;&#35265;&#65292;&#32780;PDDL+&#26159;&#25429;&#25417;&#27492;&#31867;&#31995;&#32479;&#20316;&#20026;&#35268;&#21010;&#39046;&#22495;&#30340;&#26631;&#20934;&#21270;&#24314;&#27169;&#35821;&#35328;&#12290;PDDL+&#20351;&#24471;&#33021;&#22815;&#20934;&#30830;&#32534;&#30721;&#28151;&#21512;&#31163;&#25955;-&#36830;&#32493;&#31995;&#32479;&#21160;&#24577;&#12289;&#22806;&#29983;&#27963;&#21160;&#20197;&#21450;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#23637;&#31034;&#30340;&#35768;&#22810;&#20854;&#20182;&#26377;&#36259;&#29305;&#24615;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;PDDL+&#35268;&#21010;&#36719;&#20214;&#30340;&#26222;&#36941;&#30701;&#32570;&#21644;&#23569;&#25968;&#29616;&#26377;&#35268;&#21010;&#22120;&#30340;&#20005;&#26684;&#38480;&#21046;&#65292;PDDL+&#30340;&#20351;&#29992;&#24773;&#20917;&#19968;&#30452;&#36739;&#20026;&#32531;&#24930;&#21644;&#29369;&#35947;&#19981;&#20915;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#20998;&#27495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Nyx&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#35843;&#36731;&#24039;&#12289;&#31616;&#21333;&#21644;&#26368;&#37325;&#35201;&#30340;&#36866;&#24212;&#24615;&#30340;&#26032;&#22411;PDDL+&#35268;&#21010;&#22120;&#12290;&#35813;&#35268;&#21010;&#22120;&#34987;&#35774;&#35745;&#20026;&#21487;&#20197;&#36731;&#26494;&#23450;&#21046;&#65292;&#25193;&#23637;&#20854;&#33021;&#21147;&#36828;&#36828;&#36229;&#20986;PDDL+&#30340;&#33539;&#22260;&#12290;&#22240;&#27492;&#65292;Nyx&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#23450;&#21046;&#23454;&#29616;&#20960;&#20046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11901v1 Announce Type: new  Abstract: Real-world applications of AI Planning often require a highly expressive modeling language to accurately capture important intricacies of target systems. Hybrid systems are ubiquitous in the real-world, and PDDL+ is the standardized modeling language for capturing such systems as planning domains. PDDL+ enables accurate encoding of mixed discrete-continuous system dynamics, exogenous activity, and many other interesting features exhibited in realistic scenarios. However, the uptake in usage of PDDL+ has been slow and apprehensive, largely due to a general shortage of PDDL+ planning software, and rigid limitations of the few existing planners. To overcome this chasm, we present Nyx, a novel PDDL+ planner built to emphasize lightness, simplicity, and, most importantly, adaptability. The planner is designed to be effortlessly customizable to expand its capabilities well beyond the scope of PDDL+. As a result, Nyx can be tailored to virtuall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35299;&#30721;&#26041;&#27861;COIECD&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#20915;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20914;&#31361;&#19978;&#19979;&#25991;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#22312;&#38750;&#20914;&#31361;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11893</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#35299;&#30721;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;-&#29109;&#32422;&#26463;&#26469;&#35782;&#21035;&#21644;&#35299;&#20915;&#30693;&#35782;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11893
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35299;&#30721;&#26041;&#27861;COIECD&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#20915;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20914;&#31361;&#19978;&#19979;&#25991;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#22312;&#38750;&#20914;&#31361;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#20869;&#37096;&#21270;&#20102;&#22823;&#37327;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29616;&#23454;&#24212;&#29992;&#38656;&#35201;&#22806;&#37096;&#19978;&#19979;&#25991;&#30693;&#35782;&#26469;&#24110;&#21161;&#27169;&#22411;&#23436;&#25104;&#22522;&#26412;&#20219;&#21153;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#30693;&#35782;&#20914;&#31361;&#30340;&#20851;&#38190;&#22256;&#22659;&#65292;&#21363;&#19978;&#19979;&#25991;&#30693;&#35782;&#19982;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#30456;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#30721;&#26041;&#27861;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#20914;&#31361;&#65292;&#21487;&#33021;&#20250;&#22312;&#27809;&#26377;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#26080;&#24847;&#20013;&#38477;&#20302;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35299;&#30721;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#20449;&#24687;-&#29109;&#32422;&#26463;&#35299;&#30721;&#65288;COIECD&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;&#30693;&#35782;&#20914;&#31361;&#24182;&#35299;&#20915;&#23427;&#20204;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#20914;&#31361;&#19978;&#19979;&#25991;&#30340;&#24544;&#23454;&#24230;&#65292;&#21516;&#26102;&#22312;&#38750;&#20914;&#31361;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;COIECD&#22312;&#29616;&#23454;&#25968;&#25454;&#38598;&#20013;&#23637;&#29616;&#20986;&#36739;&#24378;&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;&#25552;&#20379;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11893v1 Announce Type: new  Abstract: Large language models internalize enormous parametric knowledge during pre-training. Concurrently, realistic applications necessitate external contextual knowledge to aid models on the underlying tasks. This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the However, existing decoding works are specialized in resolving knowledge conflicts and could inadvertently deteriorate performance in absence of conflicts. In this paper, we propose an adaptive decoding method, termed as contextual information-entropy constraint decoding (COIECD), to discern whether the knowledge conflicts occur and resolve them. It can improve the model's faithfulness to conflicting context, and simultaneously maintain high performance among non- Our experiments show that COIECD exhibits strong performance and robustness over knowledge conflicts in realistic datasets. Code is available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20445;&#30041;&#35821;&#20041;&#30340;&#36716;&#25442;&#30340;&#33258;&#28982;&#24615;&#21450;&#20854;&#23545;NPR&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;NPR&#31995;&#32479;&#22312;&#38754;&#23545;&#19981;&#33258;&#28982;&#30340;&#20195;&#30721;&#36716;&#25442;&#26102;&#20250;&#20135;&#29983;&#36739;&#39640;&#30340;&#35823;&#25253;&#29575;&#65292;&#19988;&#22312;&#20351;&#29992;&#33258;&#28982;&#36716;&#25442;&#36827;&#34892;&#35780;&#20272;&#26102;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.11892</link><description>&lt;p&gt;
&#29992;&#20445;&#30041;&#35821;&#20041;&#30340;&#36716;&#25442;&#35780;&#20272;&#31243;&#24207;&#20462;&#22797;&#65306;&#33258;&#28982;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Program Repair with Semantic-Preserving Transformations: A Naturalness Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20445;&#30041;&#35821;&#20041;&#30340;&#36716;&#25442;&#30340;&#33258;&#28982;&#24615;&#21450;&#20854;&#23545;NPR&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;NPR&#31995;&#32479;&#22312;&#38754;&#23545;&#19981;&#33258;&#28982;&#30340;&#20195;&#30721;&#36716;&#25442;&#26102;&#20250;&#20135;&#29983;&#36739;&#39640;&#30340;&#35823;&#25253;&#29575;&#65292;&#19988;&#22312;&#20351;&#29992;&#33258;&#28982;&#36716;&#25442;&#36827;&#34892;&#35780;&#20272;&#26102;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20445;&#30041;&#35821;&#20041;&#30340;&#36716;&#25442;&#30340;&#33258;&#28982;&#24615;&#21450;&#20854;&#23545;NPR&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#20154;&#31867;&#30740;&#31350;&#65292;&#21253;&#25324;(1)&#19982;&#36164;&#28145;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#30340;&#35775;&#35848;&#65292;&#20197;&#24314;&#31435;&#35780;&#20272;&#20195;&#30721;&#36716;&#25442;&#33258;&#28982;&#24615;&#30340;&#31532;&#19968;&#20010;&#20855;&#20307;&#26631;&#20934;&#65307;(2)&#36827;&#34892;&#20102;&#19968;&#39033;&#28041;&#21450;10&#21517;&#24320;&#21457;&#20154;&#21592;&#30340;&#35843;&#26597;&#65292;&#35780;&#20272;&#20102;&#24212;&#29992;&#20110;225&#20010;&#30495;&#23454;&#19990;&#30028;bug&#30340;1178&#20010;&#36716;&#25442;&#65288;&#21363;&#21407;&#22987;&#21644;&#36716;&#25442;&#31243;&#24207;&#25104;&#23545;&#30340;&#24773;&#20917;&#65289;&#30340;&#33258;&#28982;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20854;&#20013;&#25509;&#36817;60%&#30340;&#36716;&#25442;&#34987;&#35748;&#20026;&#26159;&#33258;&#28982;&#30340;&#65292;20%&#30340;&#36716;&#25442;&#34987;&#35748;&#20026;&#26159;&#19981;&#33258;&#28982;&#30340;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#26631;&#27880;&#32773;&#20043;&#38388;&#26377;&#30456;&#24403;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#19981;&#33258;&#28982;&#30340;&#20195;&#30721;&#36716;&#25442;&#24341;&#20837;&#20102;&#20116;&#20010;&#30693;&#21517;NPR&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#30340;25.2%&#35823;&#25253;&#29575;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#33258;&#28982;&#36716;&#25442;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;NPR&#31995;&#32479;&#30340;&#24615;&#33021;&#26174;&#30528;&#19979;&#38477;&#65292;&#21363;&#24615;&#33021;&#19979;&#38477;&#39640;&#36798;22.9%&#21644;23.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11892v1 Announce Type: cross  Abstract: In this paper, we investigate the naturalness of semantic-preserving transformations and their impacts on the evaluation of NPR. To achieve this, we conduct a two-stage human study, including (1) interviews with senior software developers to establish the first concrete criteria for assessing the naturalness of code transformations and (2) a survey involving 10 developers to assess the naturalness of 1178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs. Our findings reveal that nearly 60% and 20% of these transformations are considered natural and unnatural with substantially high agreement among human annotators. Furthermore, the unnatural code transformations introduce a 25.2% false alarm rate on robustness of five well-known NPR systems. Additionally, the performance of the NPR systems drops notably when evaluated using natural transformations, i.e., a drop of up to 22.9% and 23.6% i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#21644;&#25913;&#36827;LLM&#20316;&#20026;&#37239;&#20799;&#38738;&#23569;&#24180;&#24773;&#24863;&#25903;&#25345;&#32773;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;LLM&#19982;&#37239;&#20799;&#30456;&#20851;&#20869;&#23481;&#30340;&#20114;&#21160;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26631;&#20934;&#37327;&#34920;&#12290;</title><link>https://arxiv.org/abs/2402.11886</link><description>&lt;p&gt;
LLM&#30340;&#22810;&#24425;&#26410;&#26469;&#65306;&#35780;&#20272;&#21644;&#25913;&#36827;LLM&#20316;&#20026;&#37239;&#20799;&#38738;&#23569;&#24180;&#24773;&#24863;&#25903;&#25345;&#32773;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#21644;&#25913;&#36827;LLM&#20316;&#20026;&#37239;&#20799;&#38738;&#23569;&#24180;&#24773;&#24863;&#25903;&#25345;&#32773;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;LLM&#19982;&#37239;&#20799;&#30456;&#20851;&#20869;&#23481;&#30340;&#20114;&#21160;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26631;&#20934;&#37327;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37239;&#20799;&#38738;&#23569;&#24180;&#38754;&#20020;&#30528;&#22686;&#21152;&#30340;&#24515;&#29702;&#20581;&#24247;&#39118;&#38505;&#65292;&#22914;&#25233;&#37057;&#12289;&#28966;&#34385;&#21644;&#33258;&#26432;&#24847;&#24565;&#12290;&#21463;&#36127;&#38754;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#20182;&#20204;&#32463;&#24120;&#36991;&#20813;&#23547;&#27714;&#24110;&#21161;&#65292;&#20381;&#36182;&#22312;&#32447;&#36164;&#28304;&#65292;&#36825;&#21487;&#33021;&#25552;&#20379;&#19981;&#30456;&#23481;&#30340;&#20449;&#24687;&#12290;&#34429;&#28982;&#33719;&#24471;&#25903;&#25345;&#29615;&#22659;&#21644;&#21487;&#38752;&#20449;&#24687;&#26159;&#26080;&#20215;&#30340;&#65292;&#20294;&#20840;&#29699;&#35768;&#22810;&#37239;&#20799;&#38738;&#23569;&#24180;&#26080;&#27861;&#33719;&#24471;&#36825;&#31181;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#37319;&#29992;&#65292;&#24773;&#20917;&#21487;&#33021;&#24456;&#24555;&#21457;&#29983;&#21464;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#25506;&#35752;LLM&#25913;&#21464;&#37239;&#20799;&#24773;&#24863;&#25903;&#25345;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;LLM&#19982;&#37239;&#20799;&#30456;&#20851;&#20869;&#23481;&#30340;&#20114;&#21160;&#36827;&#34892;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#20026;&#20102;&#35780;&#20272;&#22238;&#24212;&#36136;&#37327;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#24515;&#29702;&#26631;&#20934;&#21644;&#19987;&#23478;&#24847;&#35265;&#21551;&#21457;&#30340;&#26032;&#39062;&#21313;&#20010;&#38382;&#39064;&#37327;&#34920;&#12290;&#25105;&#20204;&#23558;&#35813;&#37327;&#34920;&#24212;&#29992;&#20110;&#35780;&#20998;&#20960;&#20010;LLM&#21644;&#20154;&#31867;&#35780;&#35770;&#65292;&#36825;&#20123;&#35780;&#35770;&#26159;&#37239;&#20799;&#38738;&#23569;&#24180;&#23547;&#27714;&#24314;&#35758;&#21644;&#20998;&#20139;&#26102;&#21457;&#34920;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11886v1 Announce Type: cross  Abstract: Queer youth face increased mental health risks, such as depression, anxiety, and suicidal ideation. Hindered by negative stigma, they often avoid seeking help and rely on online resources, which may provide incompatible information. Although access to a supportive environment and reliable information is invaluable, many queer youth worldwide have no access to such support. However, this could soon change due to the rapid adoption of Large Language Models (LLMs) such as ChatGPT. This paper aims to comprehensively explore the potential of LLMs to revolutionize emotional support for queers. To this end, we conduct a qualitative and quantitative analysis of LLM's interactions with queer-related content. To evaluate response quality, we develop a novel ten-question scale that is inspired by psychological standards and expert input. We apply this scale to score several LLMs and human comments to posts where queer youth seek advice and share 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#20197;&#21450;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#38598;&#25104;&#27169;&#22411;&#26041;&#27861;&#30340;Q&#23398;&#20064;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.11877</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#25918;&#26494;&#37319;&#26679;&#27169;&#22411;&#30340;&#22312;&#32447;&#27169;&#22411;&#30340;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#20998;&#26512;&#19979;&#30340;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Finite-Time Error Analysis of Online Model-Based Q-Learning with a Relaxed Sampling Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#20197;&#21450;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#38598;&#25104;&#27169;&#22411;&#26041;&#27861;&#30340;Q&#23398;&#20064;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;Q&#23398;&#20064;&#22312;&#26080;&#27169;&#22411;&#35774;&#32622;&#20013;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23558;Q&#23398;&#20064;&#25193;&#23637;&#21040;&#22522;&#20110;&#27169;&#22411;&#30340;&#26694;&#26550;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;Q&#23398;&#20064;&#19982;&#22522;&#20110;&#27169;&#22411;&#26041;&#27861;&#30456;&#32467;&#21512;&#26102;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35797;&#22270;&#38416;&#26126;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;Q&#23398;&#20064;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#26080;&#27169;&#22411;&#23545;&#24212;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11877v1 Announce Type: cross  Abstract: Reinforcement learning has witnessed significant advancements, particularly with the emergence of model-based approaches. Among these, $Q$-learning has proven to be a powerful algorithm in model-free settings. However, the extension of $Q$-learning to a model-based framework remains relatively unexplored. In this paper, we delve into the sample complexity of $Q$-learning when integrated with a model-based approach. Through theoretical analyses and empirical evaluations, we seek to elucidate the conditions under which model-based $Q$-learning excels in terms of sample efficiency compared to its model-free counterpart.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26410;&#26631;&#35760;&#39640;&#32500;&#23454;&#20540;&#26426;&#22120;&#20154;&#36712;&#36857;&#24320;&#22987;&#33258;&#20027;&#23398;&#20064;&#36890;&#29992;&#30340;&#36923;&#36753;&#30456;&#20851;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#26500;&#25104;&#20102;&#33258;&#21160;&#21457;&#26126;&#30340;PDDL-like&#22495;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11871</link><description>&lt;p&gt;
&#20174;&#23454;&#38469;&#21040;&#36923;&#36753;&#20877;&#21040;&#23454;&#38469;&#65306;&#20026;&#35268;&#21010;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#21457;&#26126;&#31526;&#21495;&#35789;&#27719;&#12289;&#21160;&#20316;&#21644;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions and Models for Planning from Raw Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26410;&#26631;&#35760;&#39640;&#32500;&#23454;&#20540;&#26426;&#22120;&#20154;&#36712;&#36857;&#24320;&#22987;&#33258;&#20027;&#23398;&#20064;&#36890;&#29992;&#30340;&#36923;&#36753;&#30456;&#20851;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#26500;&#25104;&#20102;&#33258;&#21160;&#21457;&#26126;&#30340;PDDL-like&#22495;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#24037;&#21046;&#20316;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#20811;&#26381;&#38271;&#26399;&#20154;&#24037;&#26234;&#33021;&#26426;&#22120;&#20154;&#35268;&#21010;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#21019;&#24314;&#36825;&#26679;&#30340;&#34920;&#31034;&#38656;&#35201;&#20855;&#26377;&#24378;&#28872;&#30452;&#35273;&#21644;&#35814;&#32454;&#30693;&#35782;&#30340;&#19987;&#23478;&#65292;&#20182;&#20204;&#20102;&#35299;&#26426;&#22120;&#20154;&#21644;&#22312;&#29305;&#23450;&#29615;&#22659;&#20013;&#21487;&#33021;&#38656;&#35201;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290;&#28040;&#38500;&#23545;&#20154;&#31867;&#30452;&#35273;&#30340;&#20381;&#36182;&#26159;&#19968;&#20010;&#26497;&#20026;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#23398;&#20064;&#36890;&#29992;&#36923;&#36753;&#30456;&#20851;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#35813;&#34920;&#31034;&#20174;&#26410;&#26631;&#35760;&#30340;&#39640;&#32500;&#23454;&#20540;&#26426;&#22120;&#20154;&#36712;&#36857;&#24320;&#22987;&#12290;&#25152;&#23398;&#34920;&#31034;&#26500;&#25104;&#20102;&#33258;&#21160;&#21457;&#26126;&#30340;&#31867;PDDL&#22495;&#27169;&#22411;&#12290;&#30830;&#23450;&#24615;&#35774;&#32622;&#19979;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20174;&#23569;&#25968;&#26426;&#22120;&#20154;&#36712;&#36857;&#20013;&#21487;&#20197;&#23398;&#21040;&#24378;&#22823;&#30340;&#25277;&#35937;&#34920;&#31034;&#65307;&#25152;&#23398;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11871v1 Announce Type: cross  Abstract: Hand-crafted, logic-based state and action representations have been widely used to overcome the intractable computational complexity of long-horizon robot planning problems, including task and motion planning problems. However, creating such representations requires experts with strong intuitions and detailed knowledge about the robot and the tasks it may need to accomplish in a given setting. Removing this dependency on human intuition is a highly active research area.   This paper presents the first approach for autonomously learning generalizable, logic-based relational representations for abstract states and actions starting from unannotated high-dimensional, real-valued robot trajectories. The learned representations constitute auto-invented PDDL-like domain models. Empirical results in deterministic settings show that powerful abstract representations can be learned from just a handful of robot trajectories; the learned relation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#22522;&#20110;&#23618;&#27425;&#20998;&#26512;&#27861;&#21644;&#27169;&#31946;&#36923;&#36753;&#30340;&#20004;&#31181;&#26032;&#30340;&#22312;&#32447;&#22320;&#22270;&#21305;&#37197;&#31639;&#27861;&#65292;&#20854;&#20013;AHP&#24212;&#29992;&#20110;&#22320;&#22270;&#21305;&#37197;&#30340;&#26041;&#24335;&#26159;&#26032;&#24320;&#21457;&#30340;&#65292;&#21516;&#26102;&#27169;&#31946;&#36923;&#36753;&#34987;&#24212;&#29992;&#20110;&#22320;&#22270;&#21305;&#37197;&#19982;&#29616;&#26377;&#30740;&#31350;&#31867;&#20284;&#65292;&#20294;&#36827;&#34892;&#20102;&#19968;&#20123;&#25913;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.11866</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#27425;&#20998;&#26512;&#27861;&#21644;&#27169;&#31946;&#36923;&#36753;&#30340;&#20004;&#31181;&#22312;&#32447;&#22320;&#22270;&#21305;&#37197;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Two Online Map Matching Algorithms Based on Analytic Hierarchy Process and Fuzzy Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#22522;&#20110;&#23618;&#27425;&#20998;&#26512;&#27861;&#21644;&#27169;&#31946;&#36923;&#36753;&#30340;&#20004;&#31181;&#26032;&#30340;&#22312;&#32447;&#22320;&#22270;&#21305;&#37197;&#31639;&#27861;&#65292;&#20854;&#20013;AHP&#24212;&#29992;&#20110;&#22320;&#22270;&#21305;&#37197;&#30340;&#26041;&#24335;&#26159;&#26032;&#24320;&#21457;&#30340;&#65292;&#21516;&#26102;&#27169;&#31946;&#36923;&#36753;&#34987;&#24212;&#29992;&#20110;&#22320;&#22270;&#21305;&#37197;&#19982;&#29616;&#26377;&#30740;&#31350;&#31867;&#20284;&#65292;&#20294;&#36827;&#34892;&#20102;&#19968;&#20123;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36825;&#31687;&#35770;&#25991;&#30340;&#30446;&#30340;&#26159;&#24320;&#21457;&#26032;&#30340;&#22320;&#22270;&#21305;&#37197;&#31639;&#27861;&#24182;&#25913;&#36827;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#20851;&#38190;&#26041;&#27861;&#65306;&#23618;&#27425;&#20998;&#26512;&#27861;&#65288;AHP&#65289;&#22320;&#22270;&#21305;&#37197;&#21644;&#27169;&#31946;&#36923;&#36753;&#22320;&#22270;&#21305;&#37197;&#12290;AHP&#26159;&#19968;&#31181;&#23558;&#25968;&#23398;&#20998;&#26512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#32467;&#21512;&#30340;&#20915;&#31574;&#26041;&#27861;&#65292;&#27169;&#31946;&#36923;&#36753;&#26159;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#31243;&#24230;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#26088;&#22312;&#24314;&#27169;&#20174;0&#21040;1&#30340;&#19981;&#30830;&#23450;&#25512;&#29702;&#26041;&#24335;&#65292;&#32780;&#19981;&#26159;&#36890;&#24120;&#30340;&#24067;&#23572;&#36923;&#36753;&#12290;&#22312;&#36825;&#20123;&#31639;&#27861;&#20013;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#26032;&#24320;&#21457;&#20102;&#23558;AHP&#24212;&#29992;&#20110;&#22320;&#22270;&#21305;&#37197;&#30340;&#26041;&#24335;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#23558;&#27169;&#31946;&#36923;&#36753;&#24212;&#29992;&#20110;&#22320;&#22270;&#21305;&#37197;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30740;&#31350;&#22823;&#33268;&#30456;&#21516;&#65292;&#38500;&#20102;&#19968;&#20123;&#32454;&#24494;&#30340;&#25913;&#21464;&#12290;&#30001;&#20110;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20849;&#21516;&#29305;&#28857;&#26159;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#19981;&#31934;&#30830;&#20449;&#24687;&#24182;&#19988;&#26131;&#20110;&#23454;&#29616;&#65292;&#25105;&#20204;&#20915;&#23450;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11866v1 Announce Type: cross  Abstract: Our aim of this paper is to develop new map matching algorithms and to improve upon previous work. We address two key approaches: Analytic Hierarchy Process (AHP) map matching and fuzzy logic map matching. AHP is a decision-making method that combines mathematical analysis with human judgment, and fuzzy logic is an approach to computing based on the degree of truth and aims at modeling the imprecise modes of reasoning from 0 to 1 rather than the usual boolean logic. Of these algorithms, the way of our applying AHP to map matching is newly developed in this paper, meanwhile, our application of fuzzy logic to map matching is mostly the same as existing research except for some small changes. Because of the common characteristic that both methods are designed to handle imprecise information and simplicity for implementation, we decided to use these methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22312;&#31526;&#21495;&#32570;&#22833;&#26102;&#36890;&#36807;&#27880;&#24847;&#21147;&#35268;&#33539;&#21270;&#25913;&#36827;&#20195;&#30721;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#31243;&#24207;&#20998;&#26512;&#25552;&#21462;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#25513;&#30721;&#26041;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;&#20851;&#27880;&#24230;&#30340;&#37325;&#35201;&#24615;</title><link>https://arxiv.org/abs/2402.11842</link><description>&lt;p&gt;
CodeArt&#65306;&#24403;&#31526;&#21495;&#32570;&#22833;&#26102;&#36890;&#36807;&#27880;&#24847;&#21147;&#35268;&#33539;&#21270;&#25913;&#36827;&#20195;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeArt: Better Code Models by Attention Regularization When Symbols Are Lacking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11842
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22312;&#31526;&#21495;&#32570;&#22833;&#26102;&#36890;&#36807;&#27880;&#24847;&#21147;&#35268;&#33539;&#21270;&#25913;&#36827;&#20195;&#30721;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#31243;&#24207;&#20998;&#26512;&#25552;&#21462;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#25513;&#30721;&#26041;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;&#20851;&#27880;&#24230;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#20195;&#30721;&#27169;&#22411;&#22312;&#35768;&#22810;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24403;&#31526;&#21495;&#32570;&#22833;&#25110;&#32773;&#19981;&#20855;&#20449;&#24687;&#37327;&#26102;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20250;&#19979;&#38477;&#12290;&#36825;&#26159;&#22240;&#20026;&#27169;&#22411;&#21487;&#33021;&#27809;&#26377;&#23398;&#20250;&#22312;&#27809;&#26377;&#31526;&#21495;&#30340;&#24773;&#20917;&#19979;&#27491;&#30830;&#22320;&#20851;&#27880;&#30456;&#20851;&#24615;/&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#31526;&#21495;&#32570;&#22833;&#26102;&#39044;&#35757;&#32451;&#36890;&#29992;&#20195;&#30721;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31243;&#24207;&#20250;&#36864;&#21270;&#20026;&#29992;&#38750;&#24120;&#21407;&#22987;&#30340;&#35821;&#35328;&#32534;&#20889;&#30340;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#31243;&#24207;&#20998;&#26512;&#26469;&#20107;&#20808;&#25552;&#21462;&#19978;&#19979;&#25991;&#65288;&#32780;&#19981;&#26159;&#20687;&#20256;&#32479;&#27169;&#22411;&#20013;&#20381;&#36182;&#31526;&#21495;&#21644;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#21147;&#25513;&#30721;&#26041;&#27861;&#65292;&#21482;&#20801;&#35768;&#27169;&#22411;&#20851;&#27880;&#36825;&#20123;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#21452;&#21521;&#31243;&#24207;&#20381;&#36182;&#20256;&#36882;&#38381;&#21253;&#21644;&#20196;&#29260;&#20849;&#29616;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20869;&#22312;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#34987;&#29992;&#20110;&#23398;&#20064;&#20801;&#35768;&#30340;&#20851;&#27880;&#24230;&#21738;&#20123;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11842v1 Announce Type: cross  Abstract: Transformer based code models have impressive performance in many software engineering tasks. However, their effectiveness degrades when symbols are missing or not informative. The reason is that the model may not learn to pay attention to the right correlations/contexts without the help of symbols. We propose a new method to pre-train general code models when symbols are lacking. We observe that in such cases, programs degenerate to something written in a very primitive language. We hence propose to use program analysis to extract contexts a priori (instead of relying on symbols and masked language modeling as in vanilla models). We then leverage a novel attention masking method to only allow the model attending to these contexts, e.g., bi-directional program dependence transitive closures and token co-occurrences. In the meantime, the inherent self-attention mechanism is utilized to learn which of the allowed attentions are more impo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NewsSerow&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#29615;&#22659;&#20445;&#25252;&#20869;&#23481;&#65292;&#24182;&#19988;&#22312;&#23612;&#27850;&#23572;&#35821;&#20013;&#20351;&#29992;&#23569;&#20110;10&#20010;&#31034;&#20363;&#26032;&#38395;&#25991;&#31456;&#26102;&#65292;NewsSerow&#26174;&#30528;&#20248;&#20110;&#20854;&#20182;&#23569;&#26679;&#26412;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11818</link><description>&lt;p&gt;
&#22312;&#30495;&#27491;&#37325;&#35201;&#30340;&#22320;&#26041;&#65306;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#23569;&#26679;&#26412;&#29615;&#22659;&#20445;&#25252;&#23186;&#20307;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Where It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11818
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NewsSerow&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#29615;&#22659;&#20445;&#25252;&#20869;&#23481;&#65292;&#24182;&#19988;&#22312;&#23612;&#27850;&#23572;&#35821;&#20013;&#20351;&#29992;&#23569;&#20110;10&#20010;&#31034;&#20363;&#26032;&#38395;&#25991;&#31456;&#26102;&#65292;NewsSerow&#26174;&#30528;&#20248;&#20110;&#20854;&#20182;&#23569;&#26679;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#20445;&#32452;&#32455;&#24120;&#35268;&#30417;&#27979;&#26377;&#21487;&#33021;&#23545;&#29615;&#22659;&#20135;&#29983;&#24433;&#21709;&#30340;&#20445;&#25252;&#21306;&#23186;&#20307;&#20869;&#23481;&#65292;&#20197;&#20445;&#25345;&#23545;&#21487;&#33021;&#21457;&#23637;&#30340;&#24773;&#20917;&#30340;&#35748;&#35782;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#23186;&#20307;&#30417;&#27979;&#31995;&#32479;&#38656;&#35201;&#30001;&#39046;&#22495;&#19987;&#23478;&#26631;&#35760;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#36825;&#22312;&#33521;&#35821;&#31561;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35268;&#27169;&#19978;&#25165;&#26159;&#21487;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#24037;&#20855;&#22312;&#20840;&#29699;&#21335;&#26041;&#26368;&#38656;&#35201;&#65292;&#22312;&#37027;&#37324;&#24863;&#20852;&#36259;&#30340;&#26032;&#38395;&#20027;&#35201;&#26159;&#29992;&#26412;&#22320;&#20302;&#36164;&#28304;&#35821;&#35328;&#21457;&#24067;&#30340;&#65292;&#21487;&#25345;&#32493;&#22320;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#27880;&#37322;&#30340;&#19987;&#23478;&#20063;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;NewsSerow&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#29615;&#22659;&#20445;&#25252;&#20869;&#23481;&#12290;NewsSerow&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24635;&#32467;&#12289;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20998;&#31867;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#27969;&#31243;&#12290;&#22312;&#23612;&#27850;&#23572;&#35821;&#20013;&#20351;&#29992;&#26368;&#22810;10&#20010;&#26032;&#38395;&#31034;&#20363;&#25991;&#31456;&#65292;NewsSerow&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#23569;&#26679;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11818v1 Announce Type: cross  Abstract: Environmental conservation organizations routinely monitor news content on conservation in protected areas to maintain situational awareness of developments that can have an environmental impact. Existing automated media monitoring systems require large amounts of data labeled by domain experts, which is only feasible at scale for high-resource languages like English. However, such tools are most needed in the global south where news of interest is mainly in local low-resource languages, and far fewer experts are available to annotate datasets sustainably. In this paper, we propose NewsSerow, a method to automatically recognize environmental conservation content in low-resource languages. NewsSerow is a pipeline of summarization, in-context few-shot classification, and self-reflection using large language models (LLMs). Using at most 10 demonstration example news articles in Nepali, NewsSerow significantly outperforms other few-shot me
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11815</link><description>&lt;p&gt;
HU&#22312;SemEval-2024&#20219;&#21153;8A&#20013;&#30340;&#34920;&#29616;&#65306;&#23545;&#27604;&#23398;&#20064;&#33021;&#21542;&#23398;&#20064;&#23884;&#20837;&#20197;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11815
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;SemEval-2024&#20219;&#21153;8&#8220;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#8221;&#24320;&#21457;&#30340;&#31995;&#32479;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#34394;&#20551;&#25991;&#26412;&#29983;&#25104;&#12289;&#32593;&#32476;&#38035;&#40060;&#12289;&#32771;&#35797;&#20316;&#24330;&#29978;&#33267;&#25220;&#34989;&#29256;&#26435;&#26448;&#26009;&#20013;&#30340;&#20351;&#29992;&#65292;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19968;&#30452;&#26159;&#20027;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#35768;&#22810;&#31995;&#32479;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#20999;&#23454;&#38469;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#36890;&#24120;&#19981;&#21487;&#33021;&#30693;&#36947;&#29992;&#25143;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#20855;&#20307;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#20854;&#20351;&#29992;&#22522;&#32447;&#21442;&#25968;&#30340;&#22823;&#32422;40%&#65288;149M&#27604;355M&#65289;&#65292;&#20294;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21487;&#27604;&#30340;&#24615;&#33021;&#65288;&#22312;137&#20010;&#21442;&#19982;&#32773;&#20013;&#25490;&#21517;&#31532;21&#65289;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#26159;&#65292;&#21363;&#20351;&#27809;&#26377;&#22810;&#20010;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11815v1 Announce Type: cross  Abstract: This paper describes our system developed for SemEval-2024 Task 8, "Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection." Machine-generated texts have been one of the main concerns due to the use of large language models (LLM) in fake text generation, phishing, cheating in exams, or even plagiarizing copyright materials. A lot of systems have been developed to detect machine-generated text. Nonetheless, the majority of these systems rely on the text-generating model, a limitation that is impractical in real-world scenarios, as it's often impossible to know which specific model the user has used for text generation. In this work, we propose a single model based on contrastive learning, which uses ~40% of the baseline's parameters (149M vs. 355M) but shows a comparable performance on the test dataset (21st out of 137 participants). Our key finding is that even without an ensemble of multiple models, a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#21387;&#21147;&#27979;&#35797;&#26041;&#27861;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#31995;&#32479;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#22330;&#26223;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#36793;&#30028;&#24773;&#20917;</title><link>https://arxiv.org/abs/2402.11813</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#21387;&#21147;&#27979;&#35797;&#39640;&#36895;&#20844;&#36335;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A novel framework for adaptive stress testing of autonomous vehicles in highways
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11813
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#21387;&#21147;&#27979;&#35797;&#26041;&#27861;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#31995;&#32479;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#22330;&#26223;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#36793;&#30028;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#35777;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#23433;&#20840;&#36816;&#34892;&#23545;&#20110;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#21644;&#20844;&#20247;&#25509;&#21463;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#19981;&#20165;&#23545;AV&#36827;&#34892;&#26631;&#20934;&#23433;&#20840;&#27979;&#35797;&#30340;&#35780;&#20272;&#65292;&#36824;&#21457;&#29616;&#21487;&#33021;&#23548;&#33268;&#19981;&#23433;&#20840;&#34892;&#20026;&#25110;&#24773;&#20917;&#30340;&#34987;&#27979;&#35797;AV&#30340;&#28508;&#22312;&#36793;&#30028;&#24773;&#20917;&#20855;&#26377;&#26497;&#20854;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#22330;&#26223;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#36793;&#30028;&#24773;&#20917;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#21147;&#27979;&#35797;&#65288;AST&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21046;&#23450;&#22330;&#26223;&#20197;&#21450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21457;&#29616;&#20195;&#34920;&#36793;&#30028;&#24773;&#20917;&#30340;&#29702;&#24819;&#27169;&#24335;&#30340;&#26032;&#20852;&#39564;&#35777;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20026;DRL&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#25351;&#23548;AST&#26681;&#25454;&#34987;&#27979;&#35797;AV&#65288;&#21363;&#33258;&#36710;&#65289;&#19982;&#20854;&#20182;&#36710;&#36742;&#20043;&#38388;&#30340;&#30896;&#25758;&#27010;&#29575;&#20272;&#35745;&#26469;&#35782;&#21035;&#30896;&#25758;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11813v1 Announce Type: cross  Abstract: Guaranteeing the safe operations of autonomous vehicles (AVs) is crucial for their widespread adoption and public acceptance. It is thus of a great significance to not only assess the AV against the standard safety tests, but also discover potential corner cases of the AV under test that could lead to unsafe behaviour or scenario. In this paper, we propose a novel framework to systematically explore corner cases that can result in safety concerns in a highway traffic scenario. The framework is based on an adaptive stress testing (AST) approach, an emerging validation method that leverages a Markov decision process to formulate the scenarios and deep reinforcement learning (DRL) to discover the desirable patterns representing corner cases. To this end, we develop a new reward function for DRL to guide the AST in identifying crash scenarios based on the collision probability estimate between the AV under test (i.e., the ego vehicle) and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11809</link><description>&lt;p&gt;
&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#65306;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#21152;&#36895;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#8221;&#65288;SPACE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;LLMs&#30340;&#26080;&#25439;&#21152;&#36895;&#12290;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#33021;&#21147;&#65292;SPACE&#29420;&#29305;&#22320;&#20351;&#33258;&#22238;&#24402;LLMs&#33021;&#22815;&#24182;&#34892;&#29983;&#25104;&#21644;&#39564;&#35777;&#20196;&#29260;&#12290;&#36825;&#26159;&#36890;&#36807;&#19987;&#38376;&#30340;&#21322;&#33258;&#22238;&#24402;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#30340;&#65292;&#35813;&#36807;&#31243;&#20351;&#29616;&#26377;LLMs&#20855;&#26377;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#20196;&#29260;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#31639;&#27861;&#20419;&#36827;&#20102;&#21333;&#20010;&#27169;&#22411;&#35843;&#29992;&#20869;&#20196;&#29260;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;LLMs&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;SPACE&#22312;HumanEval-X&#19978;&#34920;&#29616;&#20986;2.7&#20493;&#33267;4.0&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#30340;&#39640;&#36890;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11804</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#25552;&#31034;&#22120;&#65306;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#30340;&#39640;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#24402;&#32435;&#25512;&#29702;&#26088;&#22312;&#25512;&#26029;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;KG&#20013;&#32570;&#22833;&#30340;&#20107;&#23454;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;KG&#24402;&#32435;&#25512;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22788;&#29702;&#22312;&#25991;&#26412;&#21644;&#32467;&#26500;&#26041;&#38754;&#37117;&#31232;&#32570;&#30340;&#20302;&#36164;&#28304;&#22330;&#26223;&#12290;&#26412;&#25991;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#20174;&#32780;&#20026;KG&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#24102;&#26469;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#26222;&#36866;&#24615;&#12290;&#22312;&#26041;&#27861;&#35770;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;ProLINK&#65292;&#26088;&#22312;&#22312;&#20219;&#24847;KG&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#22312;&#23454;&#36341;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;36&#20010;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11804v1 Announce Type: new  Abstract: Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-res
&lt;/p&gt;</description></item><item><title>&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#20445;&#35777;&#20102;&#27599;&#27425;&#36845;&#20195;&#24555;&#36895;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#65292;&#30028;&#38480;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;&#21644;&#28151;&#21512;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.11800</link><description>&lt;p&gt;
&#20855;&#26377;&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#65306;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#30340;&#26377;&#38480;&#26102;&#38388;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11800
&lt;/p&gt;
&lt;p&gt;
&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#20445;&#35777;&#20102;&#27599;&#27425;&#36845;&#20195;&#24555;&#36895;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#65292;&#30028;&#38480;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;&#21644;&#28151;&#21512;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#22823;&#35268;&#27169;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#20855;&#26377;&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#26041;&#26696;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;&#34429;&#28982;&#24310;&#36831;&#30340;&#24433;&#21709;&#22312;&#20248;&#21270;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#19982;&#24213;&#23618;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#30456;&#20114;&#20316;&#29992;&#20197;&#22609;&#36896;SA&#30340;&#26377;&#38480;&#26102;&#38388;&#24615;&#33021;&#30340;&#26041;&#24335;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#35777;&#26126;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#24310;&#36831;&#30340;SA&#26356;&#26032;&#35268;&#21017;&#30830;&#20445;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21040;SA&#36816;&#31639;&#31526;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#20855;&#26377;&#25351;&#25968;&#24555;&#36895;&#30340;&#36895;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;$\tau_{max}$&#21644;&#28151;&#21512;&#26102;&#38388;$\tau_{mix}$&#26041;&#38754;&#26159;\emph{&#32039;&#33268;&#30340;}&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#32039;&#23494;&#30028;&#38480;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24402;&#32435;&#35777;&#26126;&#25216;&#26415;&#65292;&#19982;&#21508;&#31181;&#29616;&#26377;&#24310;&#36831;&#20248;&#21270;&#20998;&#26512;&#19981;&#21516;&#65292;&#23427;&#20381;&#36182;&#20110;&#24314;&#31435;&#26410;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11800v1 Announce Type: cross  Abstract: Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \emph{last iterate} to a ball around the SA operator's fixed point. Notably, our bound is \emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing un
&lt;/p&gt;</description></item><item><title>&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#34920;&#29616;&#20986;&#36807;&#24230;&#27867;&#21270;&#29616;&#35937;&#65292;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#35774;&#35745;&#20102;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#36882;&#24402;&#26144;&#23556;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768;&#29983;&#25104;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.11793</link><description>&lt;p&gt;
&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative Kaleidoscopic Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11793
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#34920;&#29616;&#20986;&#36807;&#24230;&#27867;&#21270;&#29616;&#35937;&#65292;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#35774;&#35745;&#20102;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#36882;&#24402;&#26144;&#23556;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768;&#29983;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#65288;&#25110;&#22810;&#23618;&#24863;&#30693;&#22120;&#26550;&#26500;&#65289;&#34920;&#29616;&#20986;&#8220;&#36807;&#24230;&#27867;&#21270;&#8221;&#29616;&#35937;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#37027;&#20123;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#30475;&#21040;&#30340;&#36755;&#20837;&#30340;&#36755;&#20986;&#20540;&#34987;&#26144;&#23556;&#21040;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#36755;&#20986;&#33539;&#22260;&#38468;&#36817;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#20102;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#65292;&#36825;&#31181;&#25928;&#24212;&#22312;&#22686;&#21152;&#23618;&#25968;&#25110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#28145;&#24230;&#26102;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#28145;&#23618;ReLU&#32593;&#32476;&#30340;&#36825;&#19968;&#29305;&#24615;&#26469;&#35774;&#35745;&#19968;&#20010;&#25968;&#25454;&#38598;&#19975;&#33457;&#31570;&#65292;&#31216;&#20026;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#22914;&#26524;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#22810;&#23618;&#24863;&#30693;&#22120;&#23558;&#36755;&#20837; $x\in\mathbb{R}^D$ &#26144;&#23556;&#21040;&#33258;&#36523; $f_\mathcal{N}(x)\rightarrow x$&#65292;&#37027;&#20040;&#8220;&#19975;&#33457;&#31570;&#37319;&#26679;&#8221;&#36807;&#31243;&#23558;&#20174;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768; $z\in\mathbb{R}^D$ &#24320;&#22987;&#65292;&#24182;&#36882;&#24402;&#22320;&#24212;&#29992; $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$&#12290;&#32463;&#36807;&#29123;&#28903;&#26399;&#21518;&#65292;&#25105;&#20204;&#24320;&#22987;&#35266;&#23519;&#26469;&#33258;&#36755;&#20837;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#21457;&#29616;&#26356;&#28145;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11793v1 Announce Type: cross  Abstract: We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\in\mathbb{R}^D$ to itself $f_\mathcal{N}(x)\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\in\mathbb{R}^D$ and recursively applies $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#23454;&#29616;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#29983;&#23384;&#39118;&#38505;&#20998;&#23618;&#65292;&#36890;&#36807;MaxViT&#27169;&#22411;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#34701;&#21512;&#36951;&#20256;&#21644;&#20020;&#24202;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#26377;&#26395;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#12290;</title><link>https://arxiv.org/abs/2402.11788</link><description>&lt;p&gt;
MM-SurvNet&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#22312;&#20083;&#33146;&#30284;&#20013;&#30340;&#29983;&#23384;&#39118;&#38505;&#20998;&#23618;
&lt;/p&gt;
&lt;p&gt;
MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast Cancer Through Multimodal Data Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11788
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#23454;&#29616;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#29983;&#23384;&#39118;&#38505;&#20998;&#23618;&#65292;&#36890;&#36807;MaxViT&#27169;&#22411;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#34701;&#21512;&#36951;&#20256;&#21644;&#20020;&#24202;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#26377;&#26395;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#39118;&#38505;&#20998;&#23618;&#26159;&#20083;&#33146;&#30284;&#31649;&#29702;&#20013;&#20020;&#24202;&#20915;&#31574;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#32452;&#32455;&#30149;&#29702;&#23398;&#24433;&#20687;&#12289;&#36951;&#20256;&#23398;&#21644;&#20020;&#24202;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#12290;&#23427;&#37319;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#29305;&#21035;&#26159;MaxViT&#27169;&#22411;&#65292;&#29992;&#20110;&#22270;&#20687;&#29305;&#24449;&#25552;&#21462;&#65292;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#24739;&#32773;&#27700;&#24179;&#25429;&#25417;&#22797;&#26434;&#30340;&#22270;&#20687;&#20851;&#31995;&#12290;&#21452;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;&#36951;&#20256;&#25968;&#25454;&#34701;&#21512;&#65292;&#32780;&#20020;&#24202;&#25968;&#25454;&#22312;&#26368;&#32456;&#23618;&#32423;&#36827;&#34892;&#21512;&#24182;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#20844;&#24320;&#30340;TCGA-BRCA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#36127;&#23545;&#25968;&#20284;&#28982;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#65292;&#21487;&#20197;&#36798;&#21040;0.64&#30340;&#24179;&#22343;C-index&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#36825;&#19968;&#36827;&#23637;&#26377;&#21161;&#20110;&#21046;&#23450;&#20010;&#24615;&#21270;&#27835;&#30103;&#31574;&#30053;&#65292;&#28508;&#22312;&#22320;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11788v1 Announce Type: cross  Abstract: Survival risk stratification is an important step in clinical decision making for breast cancer management. We propose a novel deep learning approach for this purpose by integrating histopathological imaging, genetic and clinical data. It employs vision transformers, specifically the MaxViT model, for image feature extraction, and self-attention to capture intricate image relationships at the patient level. A dual cross-attention mechanism fuses these features with genetic data, while clinical data is incorporated at the final layer to enhance predictive accuracy. Experiments on the public TCGA-BRCA dataset show that our model, trained using the negative log likelihood loss function, can achieve superior performance with a mean C-index of 0.64, surpassing existing methods. This advancement facilitates tailored treatment strategies, potentially leading to improved patient outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CiMNet&#65292;&#19968;&#20010;&#26088;&#22312;&#32852;&#21512;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#37197;&#32622;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#23384;&#20648;&#30828;&#20214;&#26500;&#24314;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11780</link><description>&lt;p&gt;
&#38754;&#21521;&#35745;&#31639;&#23384;&#20648;&#30828;&#20214;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#37197;&#32622;&#32852;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CiMNet&#65292;&#19968;&#20010;&#26088;&#22312;&#32852;&#21512;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#37197;&#32622;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#23384;&#20648;&#30828;&#20214;&#26500;&#24314;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38656;&#27714;&#30340;&#22686;&#38271;&#65292;&#35745;&#31639;&#23384;&#20648;&#65288;CiM&#65289;&#20316;&#20026;&#19968;&#31181;&#31361;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#32531;&#35299;&#32422;&#26463;&#20911;&#183;&#35834;&#20381;&#26364;&#20307;&#31995;&#32467;&#26500;&#30340;&#24102;&#23485;&#21644;&#33455;&#29255;&#20869;&#37096;&#36830;&#25509;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;CiM&#30828;&#20214;&#30340;&#26500;&#24314;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#19981;&#21516;&#25509;&#21475;&#30340;&#32531;&#23384;&#22823;&#23567;&#21644;&#20869;&#23384;&#24102;&#23485;&#26041;&#38754;&#30340;&#20219;&#20309;&#29305;&#23450;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#21487;&#33021;&#19981;&#29702;&#24819;&#22320;&#21305;&#37197;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#30340;&#23646;&#24615;&#65292;&#22914;&#24352;&#37327;&#32500;&#24230;&#21644;&#31639;&#26415;&#24378;&#24230;&#65292;&#22240;&#27492;&#23548;&#33268;&#27425;&#20248;&#21644;&#34920;&#29616;&#19981;&#20339;&#30340;&#31995;&#32479;&#12290; &#23613;&#31649;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25216;&#26415;&#22312;&#20026;&#32473;&#23450;&#30828;&#20214;&#24230;&#37327;&#39044;&#31639;&#65288;&#20363;&#22914;DNN&#25191;&#34892;&#26102;&#38388;&#25110;&#24310;&#36831;&#65289;&#20135;&#29983;&#39640;&#25928;&#23376;&#32593;&#32476;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#23427;&#20551;&#35774;&#30828;&#20214;&#37197;&#32622;&#34987;&#20923;&#32467;&#65292;&#36890;&#24120;&#20026;&#32473;&#23450;&#39044;&#31639;&#20135;&#29983;&#27425;&#20248;&#23376;&#32593;&#32476;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;CiMNet&#65292;&#19968;&#20010;&#32852;&#21512;&#25628;&#32034;&#26368;&#20248;&#23376;&#32593;&#32476;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11780v1 Announce Type: cross  Abstract: With the recent growth in demand for large-scale deep neural networks, compute in-memory (CiM) has come up as a prominent solution to alleviate bandwidth and on-chip interconnect bottlenecks that constrain Von-Neuman architectures. However, the construction of CiM hardware poses a challenge as any specific memory hierarchy in terms of cache sizes and memory bandwidth at different interfaces may not be ideally matched to any neural network's attributes such as tensor dimension and arithmetic intensity, thus leading to suboptimal and under-performing systems. Despite the success of neural architecture search (NAS) techniques in yielding efficient sub-networks for a given hardware metric budget (e.g., DNN execution time or latency), it assumes the hardware configuration to be frozen, often yielding sub-optimal sub-networks for a given budget. In this paper, we present CiMNet, a framework that jointly searches for optimal sub-networks and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#29702;&#35770;&#26694;&#26550;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#33258;&#28040;&#32791;&#24490;&#29615;&#20013;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#30495;&#23454;&#25968;&#25454;&#27604;&#20363;&#26465;&#20214;&#19979;&#65292;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#19982;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#33021;&#22815;&#34987;&#26377;&#25928;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.11778</link><description>&lt;p&gt;
&#26397;&#21521;&#33258;&#28040;&#32791;&#29983;&#25104;&#27169;&#22411;&#30340;&#29702;&#35770;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Theoretical Understandings of Self-Consuming Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11778
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#29702;&#35770;&#26694;&#26550;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#33258;&#28040;&#32791;&#24490;&#29615;&#20013;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#30495;&#23454;&#25968;&#25454;&#27604;&#20363;&#26465;&#20214;&#19979;&#65292;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#19982;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#33021;&#22815;&#34987;&#26377;&#25928;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#25361;&#25112;&#65292;&#21363;&#22312;&#19968;&#20010;&#33258;&#28040;&#32791;&#24490;&#29615;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20013;&#36830;&#32493;&#30340;&#27169;&#22411;&#19990;&#20195;&#36890;&#36807;&#28151;&#21512;&#20043;&#21069;&#19990;&#20195;&#30340;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#26469;&#36827;&#34892;&#36882;&#24402;&#35757;&#32451;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#20005;&#26684;&#35780;&#20272;&#36825;&#31181;&#35757;&#32451;&#26041;&#26696;&#23545;&#26410;&#26469;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#22312;&#19981;&#21516;&#28151;&#21512;&#35757;&#32451;&#22330;&#26223;&#19979;&#65292;&#26410;&#26469;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#19982;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#36317;&#31163;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#28151;&#21512;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#25110;&#30495;&#23454;&#25968;&#25454;&#27604;&#20363;&#36275;&#22815;&#22823;&#30340;&#26465;&#20214;&#19979;&#65292;&#36825;&#31181;&#36317;&#31163;&#21487;&#20197;&#34987;&#26377;&#25928;&#25511;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#30001;&#25193;&#22823;&#21512;&#25104;&#25968;&#25454;&#37327;&#24341;&#36215;&#30340;&#30456;&#21464;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#34429;&#28982;TV&#36317;&#31163;&#34920;&#29616;&#20986;&#21021;&#22987;&#19978;&#21319;&#65292;&#20294;&#21364;&#36880;&#28176;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11778v1 Announce Type: cross  Abstract: This paper tackles the emerging challenge of training generative models within a self-consuming loop, wherein successive generations of models are recursively trained on mixtures of real and synthetic data from previous generations. We construct a theoretical framework to rigorously evaluate how this training regimen impacts the data distributions learned by future models. Specifically, we derive bounds on the total variation (TV) distance between the synthetic data distributions produced by future models and the original real data distribution under various mixed training scenarios. Our analysis demonstrates that this distance can be effectively controlled under the condition that mixed training dataset sizes or proportions of real data are large enough. Interestingly, we further unveil a phase transition induced by expanding synthetic data amounts, proving theoretically that while the TV distance exhibits an initial ascent, it declin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;ETHICS Utilitarianism&#20219;&#21153;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#38544;&#21547;&#20102;&#23545;&#20154;&#31867;&#31119;&#31049;&#30340;&#29702;&#35299;&#65292;&#19988;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#26102;&#65292;&#20934;&#30830;&#29575;&#21576;&#38750;&#19979;&#38477;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.11777</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#20013;&#25581;&#31034;&#28508;&#22312;&#30340;&#20154;&#31867;&#31119;&#31049;
&lt;/p&gt;
&lt;p&gt;
Uncovering Latent Human Wellbeing in Language Model Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;ETHICS Utilitarianism&#20219;&#21153;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#38544;&#21547;&#20102;&#23545;&#20154;&#31867;&#31119;&#31049;&#30340;&#29702;&#35299;&#65292;&#19988;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#26102;&#65292;&#20934;&#30830;&#29575;&#21576;&#38750;&#19979;&#38477;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#38544;&#21547;&#22320;&#23398;&#20064;&#20102;&#20154;&#31867;&#31119;&#31049;&#30340;&#27010;&#24565;&#65311;&#25105;&#20204;&#36890;&#36807;ETHICS&#21151;&#21033;&#20027;&#20041;&#20219;&#21153;&#36827;&#34892;&#25506;&#35752;&#65292;&#35780;&#20272;&#32553;&#25918;&#26159;&#21542;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#26174;&#31034;&#65292;&#26080;&#38656;&#20219;&#20309;&#25552;&#31034;&#24037;&#31243;&#25110;&#24494;&#35843;&#65292;OpenAI&#30340;text-embedding-ada-002&#30340;&#20027;&#25104;&#20998;&#36798;&#21040;73.9%&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#19982;&#22312;&#25972;&#20010;ETHICS&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;BERT-large&#27169;&#22411;&#30340;74.6%&#20934;&#30830;&#29575;&#38750;&#24120;&#25509;&#36817;&#65292;&#34920;&#26126;&#39044;&#35757;&#32451;&#20256;&#36798;&#20102;&#23545;&#20154;&#31867;&#31119;&#31049;&#30340;&#26576;&#31181;&#29702;&#35299;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22235;&#31181;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#35266;&#23519;&#21151;&#21033;&#20027;&#20041;&#20934;&#30830;&#29575;&#38543;&#21442;&#25968;&#22686;&#21152;&#32780;&#21464;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#36275;&#22815;&#25968;&#37327;&#30340;&#20027;&#25104;&#20998;&#26102;&#65292;&#24615;&#33021;&#38543;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#38750;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11777v1 Announce Type: cross  Abstract: Do language models implicitly learn a concept of human wellbeing? We explore this through the ETHICS Utilitarianism task, assessing if scaling enhances pretrained models' representations. Our initial finding reveals that, without any prompt engineering or finetuning, the leading principal component from OpenAI's text-embedding-ada-002 achieves 73.9% accuracy. This closely matches the 74.6% of BERT-large finetuned on the entire ETHICS dataset, suggesting pretraining conveys some understanding about human wellbeing. Next, we consider four language model families, observing how Utilitarianism accuracy varies with increased parameters. We find performance is nondecreasing with increased model size when using sufficient numbers of principal components.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Dynamic Multi-network Mining (DMM)&#65292;&#33021;&#22815;&#23558;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#19981;&#21516;&#38271;&#24230;&#30340;&#27573;&#32452;&#65292;&#36890;&#36807;&#31232;&#30095;&#20381;&#36182;&#32593;&#32476;&#25552;&#20379;&#32858;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11773</link><description>&lt;p&gt;
&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#22810;&#32593;&#32476;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Dynamic Multi-Network Mining of Tensor Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11773
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Dynamic Multi-network Mining (DMM)&#65292;&#33021;&#22815;&#23558;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#19981;&#21516;&#38271;&#24230;&#30340;&#27573;&#32452;&#65292;&#36890;&#36807;&#31232;&#30095;&#20381;&#36182;&#32593;&#32476;&#25552;&#20379;&#32858;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#23376;&#24207;&#21015;&#32858;&#31867;&#26159;&#25968;&#25454;&#25366;&#25496;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#35299;&#37322;&#32467;&#26524;&#32858;&#31867;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#36890;&#24120;&#25105;&#20204;&#27809;&#26377;&#20851;&#20110;&#25968;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#38754;&#23545;&#30001;&#21253;&#21547;&#26102;&#38388;&#25139;&#22312;&#20869;&#30340;&#22810;&#31181;&#27169;&#24335;&#32452;&#25104;&#30340;&#22823;&#37327;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#25105;&#20204;&#22914;&#20309;&#20026;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#23454;&#29616;&#23376;&#24207;&#21015;&#32858;&#31867;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35265;&#35299;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21160;&#24577;&#22810;&#32593;&#32476;&#25366;&#25496;&#65288;DMM&#65289;&#65292;&#23427;&#23558;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#30001;l1&#33539;&#25968;&#32422;&#26463;&#30340;&#19968;&#32452;&#21508;&#31181;&#38271;&#24230;&#30340;&#27573;&#32452;&#65288;&#21363;&#32858;&#31867;&#65289;&#29305;&#24449;&#21270;&#30340;&#20381;&#36182;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20197;&#19979;&#29305;&#24615;&#12290;(a) &#21487;&#35299;&#37322;&#24615;&#65306;&#23427;&#20351;&#29992;&#22810;&#20010;&#32593;&#32476;&#23545;&#32858;&#31867;&#36827;&#34892;&#29305;&#24449;&#25551;&#36848;&#65292;&#27599;&#20010;&#32593;&#32476;&#26159;&#30456;&#24212;&#38750;&#26102;&#38388;&#27169;&#24335;&#30340;&#31232;&#30095;&#20381;&#36182;&#32593;&#32476;&#65292;&#20174;&#32780;&#25552;&#20379;&#21487;&#35265;&#19988;&#21487;&#35299;&#37322;&#30340;&#20851;&#38190;&#20851;&#31995;&#35265;&#35299;&#12290; (b) &#31934;&#30830;&#24615;&#65306;&#23427;&#21457;&#29616;&#20102;&#32858;&#31867;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11773v1 Announce Type: cross  Abstract: Subsequence clustering of time series is an essential task in data mining, and interpreting the resulting clusters is also crucial since we generally do not have prior knowledge of the data. Thus, given a large collection of tensor time series consisting of multiple modes, including timestamps, how can we achieve subsequence clustering for tensor time series and provide interpretable insights? In this paper, we propose a new method, Dynamic Multi-network Mining (DMM), that converts a tensor time series into a set of segment groups of various lengths (i.e., clusters) characterized by a dependency network constrained with l1-norm. Our method has the following properties. (a) Interpretable: it characterizes the cluster with multiple networks, each of which is a sparse dependency network of a corresponding non-temporal mode, and thus provides visible and interpretable insights into the key relationships. (b) Accurate: it discovers the clus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;&#22522;&#20110;&#25351;&#25968;&#30340;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#35793;&#21644;&#25193;&#23637;&#32479;&#35745;&#25991;&#29486;&#20013;&#30340;&#26368;&#26032;&#24605;&#24819;&#65292;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#20272;&#35745;&#22120;&#21644;&#35745;&#31639;&#28176;&#36817;&#27491;&#30830;&#32622;&#20449;&#21306;&#38388;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11771</link><description>&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#25351;&#25968;&#30340;&#27835;&#30103;&#20998;&#37197;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Effectiveness of Index-Based Treatment Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;&#22522;&#20110;&#25351;&#25968;&#30340;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#35793;&#21644;&#25193;&#23637;&#32479;&#35745;&#25991;&#29486;&#20013;&#30340;&#26368;&#26032;&#24605;&#24819;&#65292;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#20272;&#35745;&#22120;&#21644;&#35745;&#31639;&#28176;&#36817;&#27491;&#30830;&#32622;&#20449;&#21306;&#38388;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#36164;&#28304;&#31232;&#32570;&#26102;&#65292;&#38656;&#35201;&#19968;&#31181;&#20998;&#37197;&#31574;&#30053;&#26469;&#20915;&#23450;&#35841;&#33021;&#33719;&#24471;&#36164;&#28304;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;&#22522;&#20110;&#25351;&#25968;&#30340;&#20998;&#37197;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#30340;&#25968;&#25454;&#65292;&#23558;&#26377;&#38480;&#25968;&#37327;&#30340;&#36164;&#28304;&#20998;&#37197;&#32473;&#26368;&#38656;&#35201;&#30340;&#20154;&#12290;&#25105;&#20204;&#20174;&#32479;&#35745;&#25991;&#29486;&#20013;&#32763;&#35793;&#21644;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#24819;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#21644;&#35745;&#31639;&#28176;&#36817;&#27491;&#30830;&#32622;&#20449;&#21306;&#38388;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#24471;&#20986;&#26377;&#25928;&#30340;&#32479;&#35745;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11771v1 Announce Type: cross  Abstract: When resources are scarce, an allocation policy is needed to decide who receives a resource. This problem occurs, for instance, when allocating scarce medical resources and is often solved using modern ML methods. This paper introduces methods to evaluate index-based allocation policies -- that allocate a fixed number of resources to those who need them the most -- by using data from a randomized control trial. Such policies create dependencies between agents, which render the assumptions behind standard statistical tests invalid and limit the effectiveness of estimators. Addressing these challenges, we translate and extend recent ideas from the statistics literature to present an efficient estimator and methods for computing asymptotically correct confidence intervals. This enables us to effectively draw valid statistical conclusions, a critical gap in previous work. Our extensive experiments validate our methodology in practical sett
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#22686;&#24378;LLMs&#21435;&#20559;&#35265;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#21435;&#38500;&#24050;&#30693;&#20559;&#35265;&#24182;&#36328;&#36234;&#19981;&#21516;&#31867;&#21035;&#36827;&#34892;&#21435;&#20559;&#35265;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.11764</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#29992;&#20110;&#25913;&#21892;LLMs&#30340;&#21442;&#25968;&#39640;&#25928;&#21435;&#20559;&#35265;&#21270;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#22686;&#24378;LLMs&#21435;&#20559;&#35265;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#21435;&#38500;&#24050;&#30693;&#20559;&#35265;&#24182;&#36328;&#36234;&#19981;&#21516;&#31867;&#21035;&#36827;&#34892;&#21435;&#20559;&#35265;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34429;&#28982;&#21151;&#33021;&#24378;&#22823;&#65292;&#20294;&#23384;&#22312;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#30001;&#20110;&#35745;&#31639;&#25104;&#26412;&#12289;&#25968;&#25454;&#32422;&#26463;&#21644;&#21487;&#33021;&#38477;&#20302;&#22810;&#20219;&#21153;&#35821;&#35328;&#33021;&#21147;&#65292;&#21435;&#20559;&#35265;&#21270;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#30340;&#21435;&#20559;&#35265;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#30446;&#26631;&#25552;&#31034;&#65292;&#23545;&#24050;&#30693;&#20559;&#35265;&#25552;&#20379;&#26377;&#25928;&#30340;&#21435;&#20559;&#35265;&#21270;&#65292;&#20294;&#38656;&#35201;&#20107;&#20808;&#25351;&#23450;&#38382;&#39064;&#20013;&#30340;&#20559;&#35265;; &#19968;&#33324;&#25552;&#31034;&#65292;&#34429;&#28982;&#25928;&#26524;&#31245;&#36874;&#65292;&#20294;&#33021;&#22815;&#36328;&#21508;&#31181;&#31867;&#21035;&#36827;&#34892;&#21435;&#20559;&#35265;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;&#36866;&#37197;&#22120;&#35843;&#25972;&#26469;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;LLM&#21435;&#20559;&#35265;&#21270;&#65292;&#24182;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#21512;&#25104;&#25968;&#25454;&#19982;&#29616;&#26377;&#21435;&#20559;&#35265;&#21270;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;ChatGPT&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#29992;&#20110;&#21435;&#20559;&#35265;&#21270;&#20854;&#20182;LLMs&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#65307;&#65288;2&#65289;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#36229;&#36234;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#21435;&#20559;&#35265;&#21270;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11764v1 Announce Type: cross  Abstract: Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debias
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#38750;&#32431;&#35821;&#20041;&#25552;&#31034;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#25361;&#25112;&#12290;&#20116;&#20010;SOTA LLMs&#22312;&#35782;&#21035;ASCII&#33402;&#26415;&#25552;&#31034;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.11753</link><description>&lt;p&gt;
ArtPrompt: &#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#23545;&#40784;LLMs&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11753
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#38750;&#32431;&#35821;&#20041;&#25552;&#31034;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#25361;&#25112;&#12290;&#20116;&#20010;SOTA LLMs&#22312;&#35782;&#21035;ASCII&#33402;&#26415;&#25552;&#31034;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#25216;&#26415;&#65292;&#22914;&#25968;&#25454;&#36807;&#28388;&#21644;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#21152;&#24378;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24050;&#30693;&#30340;&#25216;&#26415;&#20551;&#35774;&#29992;&#20110;&#23545;&#40784;LLMs&#23433;&#20840;&#24615;&#30340;&#35821;&#26009;&#24211;&#20165;&#30001;&#35821;&#20041;&#36827;&#34892;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20551;&#35774;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#19981;&#25104;&#31435;&#65292;&#23548;&#33268;LLMs&#23384;&#22312;&#20005;&#37325;&#28431;&#27934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;Vision-in-Text Challenge&#65288;ViTC&#65289;&#26469;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#19981;&#33021;&#20165;&#36890;&#36807;&#35821;&#20041;&#36827;&#34892;&#35299;&#37322;&#30340;&#25552;&#31034;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20116;&#20010;SOTA LLMs&#65288;GPT-3.5&#12289;GPT-4&#12289;Gemini&#12289;Claude&#21644;Llama2&#65289;&#22312;&#35782;&#21035;&#20197;ASCII&#33402;&#26415;&#24418;&#24335;&#25552;&#20379;&#30340;&#25552;&#31034;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11753v1 Announce Type: cross  Abstract: Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we devel
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Diagonalisation Stochastic Gradient Descent&#65288;&#23545;&#35282;&#21270;SGD&#65289;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24179;&#28369;&#23454;&#29616;&#38750;&#21487;&#24494;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;SGD&#65292;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#31283;&#23450;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#25968;&#37327;&#32423;&#30340;&#24037;&#20316;&#35268;&#33539;&#21270;&#26041;&#24046;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.11752</link><description>&lt;p&gt;
&#23545;&#35282;&#21270;SGD&#65306;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24179;&#28369;&#23454;&#29616;&#38750;&#21487;&#24494;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;SGD
&lt;/p&gt;
&lt;p&gt;
Diagonalisation SGD: Fast &amp; Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11752
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Diagonalisation Stochastic Gradient Descent&#65288;&#23545;&#35282;&#21270;SGD&#65289;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24179;&#28369;&#23454;&#29616;&#38750;&#21487;&#24494;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;SGD&#65292;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#31283;&#23450;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#25968;&#37327;&#32423;&#30340;&#24037;&#20316;&#35268;&#33539;&#21270;&#26041;&#24046;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#23545;&#20110;&#38750;&#21487;&#24494;&#27169;&#22411;&#65292;&#23637;&#29616;&#20986;&#36739;&#20302;&#26041;&#24046;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26799;&#24230;&#20272;&#35745;&#22120;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#20559;&#24046;&#12290;&#36825;&#21487;&#33021;&#21361;&#21450;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;SGD&#65289;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35821;&#27861;&#26694;&#26550;&#26469;&#20998;&#22359;&#22320;&#23450;&#20041;&#38750;&#21487;&#24494;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#20351;&#37325;&#26032;&#21442;&#25968;&#21270;&#26799;&#24230;&#20272;&#35745;&#22120;&#26080;&#20559;&#30340;&#24179;&#28369;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;SGD&#21464;&#20307;&#65292;&#23545;&#35282;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#36880;&#27493;&#25552;&#39640;&#24179;&#28369;&#36817;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#21040;&#26410;&#24179;&#28369;&#65288;&#21407;&#22987;&#65289;&#30446;&#26631;&#30340;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#31283;&#23450;&#65292;&#24182;&#19988;&#22312;&#24037;&#20316;&#35268;&#33539;&#21270;&#26041;&#24046;&#19978;&#23454;&#29616;&#20102;&#25968;&#37327;&#32423;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11752v1 Announce Type: cross  Abstract: It is well-known that the reparameterisation gradient estimator, which exhibits low variance in practice, is biased for non-differentiable models. This may compromise correctness of gradient-based optimisation methods such as stochastic gradient descent (SGD). We introduce a simple syntactic framework to define non-differentiable functions piecewisely and present a systematic approach to obtain smoothings for which the reparameterisation gradient estimator is unbiased. Our main contribution is a novel variant of SGD, Diagonalisation Stochastic Gradient Descent, which progressively enhances the accuracy of the smoothed approximation during optimisation, and we prove convergence to stationary points of the unsmoothed (original) objective. Our empirical evaluation reveals benefits over the state of the art: our approach is simple, fast, stable and attains orders of magnitude reduction in work-normalised variance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;RESTA&#65292;&#36890;&#36807;&#20219;&#21153;&#31639;&#27861;&#23545;&#31934;&#35843;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#37325;&#26032;&#23450;&#20301;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#20854;&#26377;&#23475;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.11746</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23601;&#26159;&#38669;&#40664;&#183;&#36763;&#26222;&#26862;&#65281;&#36890;&#36807;&#20219;&#21153;&#31639;&#27861;&#23545;&#31934;&#35843;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#37325;&#26032;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;RESTA&#65292;&#36890;&#36807;&#20219;&#21153;&#31639;&#27861;&#23545;&#31934;&#35843;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#37325;&#26032;&#23450;&#20301;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#20854;&#26377;&#23475;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#24494;&#35843;&#30340;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#20250;&#23548;&#33268;&#23433;&#20840;&#24615;&#21463;&#25439;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;RESTA&#65292;&#35813;&#26041;&#27861;&#25191;&#34892;LLM&#23433;&#20840;&#37325;&#26032;&#23450;&#20301;&#12290;RESTA&#20195;&#34920;&#36890;&#36807;&#20219;&#21153;&#31639;&#27861;&#24674;&#22797;&#23433;&#20840;&#12290;&#22312;&#20854;&#26680;&#24515;&#24605;&#24819;&#20013;&#65292;&#23427;&#28041;&#21450;&#23558;&#19968;&#20010;&#23433;&#20840;&#21521;&#37327;&#31616;&#21333;&#22320;&#21152;&#21040;&#21463;&#25439;&#27169;&#22411;&#30340;&#26435;&#37325;&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RESTA&#22312;&#21442;&#25968;&#39640;&#25928;&#21644;&#23436;&#20840;&#24494;&#35843;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#20013;&#25991;&#12289;&#33521;&#25991;&#21644;&#21360;&#22320;&#25991;&#30340;&#25351;&#20196;&#36319;&#38543;&#65292;&#20197;&#21450;&#20195;&#30721;&#21644;&#25968;&#23398;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;RESTA&#22312;&#19977;&#20010;&#29616;&#26377;&#23433;&#20840;&#35780;&#20272;&#22522;&#20934;&#21644;&#20316;&#20026;&#26412;&#39033;&#24037;&#20316;&#19968;&#37096;&#20998;&#25552;&#20986;&#30340;&#19968;&#20010;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#36890;&#29992;&#24615;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;550&#20010;&#26377;&#23475;&#38382;&#39064;&#65292;&#28085;&#30422;11&#20010;&#31867;&#21035;&#65292;&#27599;&#20010;&#31867;&#21035;&#19979;&#21253;&#21547;5&#20010;&#26377;&#23475;&#30340;&#23376;&#31867;&#21035;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;RESTA&#38477;&#20302;&#20102;&#21463;&#25439;&#27169;&#22411;&#30340;&#26377;&#23475;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11746v1 Announce Type: cross  Abstract: Aligned language models face a significant limitation as their fine-tuning often results in compromised safety. To tackle this, we propose a simple method RESTA that performs LLM safety realignment. RESTA stands for REstoring Safety through Task Arithmetic. At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model. We demonstrate the effectiveness of RESTA in both parameter-efficient and full fine-tuning, covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math. We also showcase the generalizability of RESTA on three existing safety evaluation benchmarks and a multilingual benchmark dataset proposed as a part of this work, consisting of 550 harmful questions covering 11 categories, each with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of the compromised 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31561;&#25928;&#35780;&#20272;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20462;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#36755;&#20986;&#24046;&#24322;&#65292;&#21021;&#22987;&#21270;&#26032;&#30340;&#35757;&#32451;&#38598;&#24182;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#26469;&#25913;&#36827;&#21387;&#32553;&#32593;&#32476;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11737</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#31561;&#25928;&#35780;&#20272;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20462;&#22797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Compression Repair for Feedforward Neural Networks Based on Model Equivalence Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11737
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31561;&#25928;&#35780;&#20272;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20462;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#36755;&#20986;&#24046;&#24322;&#65292;&#21021;&#22987;&#21270;&#26032;&#30340;&#35757;&#32451;&#38598;&#24182;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#26469;&#25913;&#36827;&#21387;&#32553;&#32593;&#32476;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#31561;&#25928;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20462;&#22797;&#21387;&#32553;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNNs&#65289;&#12290;&#22312;&#20462;&#22797;&#26694;&#26550;&#20013;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#31561;&#25928;&#35780;&#20272;&#26041;&#27861;&#26469;&#35745;&#31639;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#36755;&#20986;&#24046;&#24322;&#12290;&#36755;&#20986;&#24046;&#24322;&#21487;&#20197;&#23450;&#37327;&#34920;&#24449;&#21387;&#32553;&#36807;&#31243;&#20135;&#29983;&#30340;&#36755;&#20986;&#24046;&#24322;&#12290;&#26681;&#25454;&#35745;&#31639;&#24471;&#21040;&#30340;&#36755;&#20986;&#24046;&#24322;&#65292;&#20462;&#22797;&#26041;&#27861;&#39318;&#20808;&#20026;&#21387;&#32553;&#32593;&#32476;&#21021;&#22987;&#21270;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#38598;&#65292;&#20197;&#32553;&#23567;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#38388;&#30340;&#24046;&#24322;&#24182;&#25913;&#36827;&#21387;&#32553;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#35757;&#32451;&#38598;&#30340;&#37325;&#26032;&#35757;&#32451;&#26469;&#20462;&#22797;&#21387;&#32553;&#30340; FNN&#12290;&#25105;&#20204;&#23558;&#25152;&#24320;&#21457;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110; MNIST &#25968;&#25454;&#38598;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#20462;&#22797;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11737v1 Announce Type: cross  Abstract: In this paper, we propose a method of repairing compressed Feedforward Neural Networks (FNNs) based on equivalence evaluation of two neural networks. In the repairing framework, a novel neural network equivalence evaluation method is developed to compute the output discrepancy between two neural networks. The output discrepancy can quantitatively characterize the output difference produced by compression procedures. Based on the computed output discrepancy, the repairing method first initializes a new training set for the compressed networks to narrow down the discrepancy between the two neural networks and improve the performance of the compressed network. Then, we repair the compressed FNN by re-training based on the training set. We apply our developed method to the MNIST dataset to demonstrate the effectiveness and advantages of our proposed repair method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#28857;&#36129;&#29486;&#65306;&#19968;&#26159;&#21019;&#24314;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;NL-to-code&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#20108;&#26159;&#24341;&#20837;&#20102;&#19968;&#31181;&#32858;&#31867;&#28982;&#21518;&#36873;&#25321;&#25552;&#31034;&#25216;&#26415;&#65292;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#28155;&#21152;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#34892;&#21040;LLM&#25552;&#31034;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.11734</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#20013;&#24515;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Solving Data-centric Tasks using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#28857;&#36129;&#29486;&#65306;&#19968;&#26159;&#21019;&#24314;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;NL-to-code&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#20108;&#26159;&#24341;&#20837;&#20102;&#19968;&#31181;&#32858;&#31867;&#28982;&#21518;&#36873;&#25321;&#25552;&#31034;&#25216;&#26415;&#65292;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#28155;&#21152;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#34892;&#21040;LLM&#25552;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#36805;&#36895;&#21462;&#20195;&#20687;StackOverflow&#36825;&#26679;&#30340;&#24110;&#21161;&#35770;&#22363;&#65292;&#24182;&#19988;&#23545;&#20110;&#38750;&#19987;&#19994;&#31243;&#24207;&#21592;&#21644;&#26368;&#32456;&#29992;&#25143;&#29305;&#21035;&#26377;&#24110;&#21161;&#12290;&#36825;&#20123;&#29992;&#25143;&#36890;&#24120;&#23545;&#25968;&#25454;&#20013;&#24515;&#20219;&#21153;&#24863;&#20852;&#36259;&#65292;&#20363;&#22914;&#30005;&#23376;&#34920;&#26684;&#25805;&#20316;&#21644;&#25968;&#25454;&#22788;&#29702;&#65292;&#22914;&#26524;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20256;&#36798;&#24847;&#22270;&#32780;&#19981;&#21253;&#21547;&#25968;&#25454;&#65292;&#36825;&#20123;&#20219;&#21153;&#24456;&#38590;&#35299;&#20915;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#22914;&#20309;&#20915;&#23450;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#22810;&#23569;&#25968;&#25454;&#21644;&#21738;&#20123;&#25968;&#25454;&#65311;&#26412;&#25991;&#23545;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#20570;&#20986;&#20102;&#20004;&#28857;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;StackOverflow&#24086;&#23376;&#20013;&#33719;&#21462;&#30340;&#25805;&#20316;&#34920;&#26684;&#25968;&#25454;&#30340;&#30495;&#23454;NL-to-code&#20219;&#21153;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32858;&#31867;&#28982;&#21518;&#36873;&#25321;&#25552;&#31034;&#25216;&#26415;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#20013;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#34892;&#28155;&#21152;&#21040;LLM&#25552;&#31034;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LLM&#30340;&#24615;&#33021;&#30830;&#23454;&#23545;&#20256;&#36882;&#21040;&#25552;&#31034;&#20013;&#30340;&#25968;&#25454;&#37327;&#25935;&#24863;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#22823;&#37327;&#35821;&#27861;&#21464;&#20307;&#30340;&#20219;&#21153;&#65292;&#20256;&#36882;&#30340;&#25968;&#25454;&#37327;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11734v1 Announce Type: cross  Abstract: Large language models (LLMs) are rapidly replacing help forums like StackOverflow, and are especially helpful for non-professional programmers and end users. These users are often interested in data-centric tasks, such as spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including the data. But how do we decide how much data and which data to include in the prompt? This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic vari
&lt;/p&gt;</description></item><item><title>FOMO&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#38543;&#26426;&#36951;&#24536;&#37096;&#20998;&#26435;&#37325;&#26469;&#35843;&#33410;&#20449;&#24687;&#24182;&#24378;&#35843;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#20986;&#29616;&#30340;&#31283;&#20581;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11733</link><description>&lt;p&gt;
&#38543;&#26426;&#36951;&#24536;&#23545;&#31283;&#20581;&#27867;&#21270;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
The Effectiveness of Random Forgetting for Robust Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11733
&lt;/p&gt;
&lt;p&gt;
FOMO&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#38543;&#26426;&#36951;&#24536;&#37096;&#20998;&#26435;&#37325;&#26469;&#35843;&#33410;&#20449;&#24687;&#24182;&#24378;&#35843;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#20986;&#29616;&#30340;&#31283;&#20581;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#36825;&#21487;&#33021;&#25439;&#23475;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#24050;&#32463;&#25104;&#20026;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#20813;&#21463;&#27492;&#31867;&#25915;&#20987;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;AT&#30340;&#20851;&#38190;&#25361;&#25112;&#20043;&#19968;&#26159;&#31283;&#20581;&#36807;&#25311;&#21512;&#65292;&#21363;&#32593;&#32476;&#23545;&#27979;&#35797;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#33021;&#38543;&#30528;&#36827;&#19968;&#27493;&#35757;&#32451;&#32780;&#24694;&#21270;&#65292;&#20174;&#32780;&#22952;&#30861;&#27867;&#21270;&#12290;&#21463;&#22823;&#33041;&#20013;&#20027;&#21160;&#36951;&#24536;&#30340;&#27010;&#24565;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36951;&#24536;&#26469;&#20943;&#36731;&#36807;&#25311;&#21512;&#65288;FOMO&#65289;&#8221;&#30340;&#26032;&#22411;&#23398;&#20064;&#33539;&#24335;&#12290;FOMO&#22312;&#36951;&#24536;&#38454;&#27573;&#38543;&#26426;&#36951;&#24536;&#37096;&#20998;&#26435;&#37325;&#24182;&#36890;&#36807;&#26435;&#37325;&#37325;&#26032;&#21021;&#22987;&#21270;&#35843;&#33410;&#27169;&#22411;&#30340;&#20449;&#24687;&#65292;&#28982;&#21518;&#22312;&#37325;&#26032;&#23398;&#20064;&#38454;&#27573;&#24378;&#35843;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23545;&#25239;&#25915;&#20987;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FOMO&#36890;&#36807;&#26174;&#33879;&#20943;&#23569;&#26368;&#20339;&#32467;&#26524;&#21644;&#26368;&#32456;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#32531;&#35299;&#20102;&#31283;&#20581;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11733v1 Announce Type: cross  Abstract: Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network's robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called "Forget to Mitigate Overfitting (FOMO)". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last
&lt;/p&gt;</description></item><item><title>Prospector heads&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#29305;&#24449;&#24402;&#22240;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32534;&#30721;&#22120;&#21644;&#20219;&#20309;&#25968;&#25454;&#24418;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#24418;&#24577;&#30340;&#23454;&#39564;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11729</link><description>&lt;p&gt;
Prospector Heads:&#22823;&#35268;&#27169;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24191;&#20041;&#29305;&#24449;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Prospector Heads: Generalized Feature Attribution for Large Models &amp; Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11729
&lt;/p&gt;
&lt;p&gt;
Prospector heads&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#29305;&#24449;&#24402;&#22240;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32534;&#30721;&#22120;&#21644;&#20219;&#20309;&#25968;&#25454;&#24418;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#24418;&#24577;&#30340;&#23454;&#39564;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26159;&#19968;&#31181;&#23450;&#20301;&#36755;&#20837;&#25968;&#25454;&#20013;&#19982;&#20998;&#31867;&#30456;&#20851;&#30340;&#21306;&#22495;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35328;&#65292;&#36825;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#33021;&#21147;&#12290;&#24403;&#21069;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20381;&#36182;&#20110;&#8220;&#35299;&#37322;&#8221;&#31471;&#21040;&#31471;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#65292;&#23384;&#22312;&#29305;&#24449;&#23450;&#20301;&#19981;&#31934;&#30830;&#20197;&#21450;&#30001;&#20110;&#35745;&#31639;&#25361;&#25112;&#32780;&#26080;&#27861;&#22312;&#23567;&#26679;&#26412;&#23610;&#23544;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#25506;&#23547;&#32773;&#22836;&#37096;&#65288;prospector heads&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#29305;&#24449;&#24402;&#22240;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32534;&#30721;&#22120;&#21644;&#20219;&#20309;&#25968;&#25454;&#24418;&#24577;&#12290;&#36890;&#36807;&#23545;&#24207;&#21015;&#65288;&#25991;&#26412;&#65289;&#12289;&#22270;&#20687;&#65288;&#30149;&#29702;&#23398;&#65289;&#21644;&#22270;&#65288;&#34507;&#30333;&#36136;&#32467;&#26500;&#65289;&#30340;&#23454;&#39564;&#65292;&#25506;&#23547;&#32773;&#22836;&#37096;&#22312;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#27010;&#25324;&#65292;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#24402;&#22240;&#26041;&#27861;&#65292;&#24179;&#22343;&#23616;&#37096;&#21270;AUPRC&#24471;&#20998;&#25552;&#21319;&#20102;&#39640;&#36798;49&#28857;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#25506;&#23547;&#32773;&#22836;&#37096;&#22914;&#20309;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11729v1 Announce Type: cross  Abstract: Feature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for machine learning models in scientific and biomedical domains. Current methods for feature attribution, which rely on "explaining" the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small sample sizes and high-dimensional datasets due to computational challenges. We introduce prospector heads, an efficient and interpretable alternative to explanation-based methods for feature attribution that can be applied to any encoder and any data modality. Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and graphs (protein structures), outperforming baseline attribution methods by up to 49 points in mean localization AUPRC. We also demonstrate how prospector heads enable improved interpr
&lt;/p&gt;</description></item><item><title>GNNavi&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25552;&#31034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20934;&#30830;&#24341;&#23548;&#20449;&#24687;&#27969;&#30340;&#27719;&#32858;&#21644;&#20998;&#24067;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23548;&#33322;&#20449;&#24687;&#27969;&#21160;&#24577;&#65292;&#36229;&#36234;&#20102;&#26631;&#20934;&#25552;&#31034;&#24335;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11709</link><description>&lt;p&gt;
GNNavi&#65306;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#23548;&#33322;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11709
&lt;/p&gt;
&lt;p&gt;
GNNavi&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25552;&#31034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20934;&#30830;&#24341;&#23548;&#20449;&#24687;&#27969;&#30340;&#27719;&#32858;&#21644;&#20998;&#24067;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23548;&#33322;&#20449;&#24687;&#27969;&#21160;&#24577;&#65292;&#36229;&#36234;&#20102;&#26631;&#20934;&#25552;&#31034;&#24335;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25509;&#25910;&#31034;&#33539;&#36755;&#20837;&#26102;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65288;ICL&#65289;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#36866;&#24212;&#24615;&#12290;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20294;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#39640;&#38656;&#27714;&#38480;&#21046;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25552;&#31034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;GNNavi&#21033;&#29992;&#20102;&#26377;&#20851;ICL&#20449;&#24687;&#27969;&#21160;&#24577;&#30340;&#35265;&#35299;&#65292;&#34920;&#26126;&#26631;&#31614;&#35789;&#22312;&#25552;&#31034;&#20013;&#20316;&#20026;&#20449;&#24687;&#20256;&#25773;&#30340;&#38170;&#28857;&#12290;GNNavi&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#31934;&#30830;&#22320;&#24341;&#23548;&#20449;&#24687;&#27969;&#30340;&#27719;&#32858;&#21644;&#20998;&#24067;&#65292;&#22312;&#22788;&#29702;&#25552;&#31034;&#26102;&#23558;&#26399;&#26395;&#30340;&#20449;&#24687;&#27969;&#30828;&#32534;&#30721;&#21040;GNN&#20013;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;GPT-2&#21644;Llama2&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;GNNavi&#36229;&#36234;&#20102;&#26631;&#20934;&#25552;&#31034;&#24335;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11709v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-ba
&lt;/p&gt;</description></item><item><title>&#35752;&#35770;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#25628;&#32034;&#24341;&#25806;&#30340;&#24433;&#21709;&#65292;&#25351;&#20986;&#20102;&#20854;&#21487;&#33021;&#23548;&#33268;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#20449;&#24687;&#26469;&#28304;&#19981;&#36879;&#26126;&#21644;&#20869;&#23481;&#27491;&#30830;&#24615;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11707</link><description>&lt;p&gt;
&#25628;&#32034;&#24341;&#25806;&#22312;ChatGPT&#20043;&#21518;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#38477;&#20302;&#25628;&#32034;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11707
&lt;/p&gt;
&lt;p&gt;
&#35752;&#35770;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#25628;&#32034;&#24341;&#25806;&#30340;&#24433;&#21709;&#65292;&#25351;&#20986;&#20102;&#20854;&#21487;&#33021;&#23548;&#33268;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#20449;&#24687;&#26469;&#28304;&#19981;&#36879;&#26126;&#21644;&#20869;&#23481;&#27491;&#30830;&#24615;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35780;&#35770;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25628;&#32034;&#24341;&#25806;&#30340;&#21457;&#23637;&#24615;&#36136;&#65292;&#22240;&#20026;&#23427;&#20204;&#24320;&#22987;&#29983;&#25104;&#12289;&#32034;&#24341;&#21644;&#20998;&#21457;&#30001;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#21019;&#24314;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#35752;&#35770;&#31361;&#20986;&#20102;&#22312;GenAI&#25972;&#21512;&#30340;&#26089;&#26399;&#38454;&#27573;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22260;&#32469;&#20107;&#23454;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#20559;&#35265;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;GenAI&#20135;&#29983;&#30340;&#36755;&#20986;&#24102;&#26469;&#20102;&#26080;&#31471;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#38477;&#20302;&#20102;&#36879;&#26126;&#24230;&#21644;&#20449;&#24687;&#26469;&#28304;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25628;&#32034;&#24341;&#25806;&#24050;&#32463;&#29992;&#21547;&#26377;&#38169;&#35823;&#30340;&#29983;&#25104;&#20869;&#23481;&#22238;&#31572;&#26597;&#35810;&#65292;&#36827;&#19968;&#27493;&#27169;&#31946;&#20102;&#20449;&#24687;&#26469;&#28304;&#65292;&#24433;&#21709;&#20102;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#25152;&#26377;&#36825;&#20123;&#22240;&#32032;&#21487;&#33021;&#38477;&#20302;&#25628;&#32034;&#24341;&#25806;&#30340;&#21487;&#38752;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#20123;&#27963;&#36291;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#26410;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11707v1 Announce Type: cross  Abstract: In this commentary, we discuss the evolving nature of search engines, as they begin to generate, index, and distribute content created by generative artificial intelligence (GenAI). Our discussion highlights challenges in the early stages of GenAI integration, particularly around factual inconsistencies and biases. We discuss how output from GenAI carries an unwarranted sense of credibility, while decreasing transparency and sourcing ability. Furthermore, search engines are already answering queries with error-laden, generated content, further blurring the provenance of information and impacting the integrity of the information ecosystem. We argue how all these factors could reduce the reliability of search engines. Finally, we summarize some of the active research directions and open questions.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#20027;&#35201;&#29992;&#20110;&#23637;&#31034;&#27010;&#24565;&#25110;&#25552;&#20379;&#31034;&#20363;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#25165;&#33021;&#23454;&#29616;&#29983;&#20135;&#23601;&#32490;&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.11702</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#33021;&#25903;&#25345;&#24320;&#21457;&#32773;&#65311;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11702
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#20027;&#35201;&#29992;&#20110;&#23637;&#31034;&#27010;&#24565;&#25110;&#25552;&#20379;&#31034;&#20363;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#25165;&#33021;&#23454;&#29616;&#29983;&#20135;&#23601;&#32490;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#23427;&#20204;&#22312;&#21508;&#31181;&#24320;&#21457;&#22330;&#26223;&#20013;&#20855;&#26377;&#24456;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20027;&#35201;&#25552;&#20379;&#20102;&#22312;&#30740;&#31350;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#65292;&#36825;&#22312;&#29702;&#35299;LLM&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#33021;&#26377;&#25928;&#25903;&#25345;&#24320;&#21457;&#32773;&#26041;&#38754;&#30041;&#19979;&#20102;&#26174;&#33879;&#30340;&#31354;&#30333;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;DevGPT&#20013;&#30340;&#23545;&#35805;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36825;&#26159;&#20174;&#24320;&#21457;&#32773;&#19982;ChatGPT&#30340;&#23545;&#35805;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65288;&#36890;&#36807;GitHub&#31561;&#24179;&#21488;&#19978;&#30340;Share Link&#21151;&#33021;&#25429;&#33719;&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#21069;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#23454;&#36341;&#36890;&#24120;&#20165;&#38480;&#20110;&#23637;&#31034;&#39640;&#23618;&#27010;&#24565;&#25110;&#25552;&#20379;&#25991;&#26723;&#20013;&#30340;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#20316;&#20026;&#21487;&#29992;&#20110;&#29983;&#20135;&#30340;&#20195;&#30721;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;LLM&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#36824;&#38656;&#35201;&#22823;&#37327;&#26410;&#26469;&#24037;&#20316;&#25165;&#33021;&#20351;&#20854;&#25104;&#20026;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11702v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers' conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37319;&#29992;GPT-4V&#21512;&#25104;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#30740;&#31350;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;ALLaVA&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;12&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#19982;&#26368;&#22810;3B LVLMs&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11684</link><description>&lt;p&gt;
&#21033;&#29992;GPT4V&#21512;&#25104;&#25968;&#25454;&#23454;&#29616;&#36731;&#37327;&#32423;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;ALLaVA
&lt;/p&gt;
&lt;p&gt;
ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11684
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37319;&#29992;GPT-4V&#21512;&#25104;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#30740;&#31350;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;ALLaVA&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;12&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#19982;&#26368;&#22810;3B LVLMs&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#21457;&#23637;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20294;&#37096;&#32626;&#26102;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#23588;&#20854;&#26159;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#24357;&#21512;&#20256;&#32479;&#23610;&#24230;LVLMs&#21644;&#36164;&#28304;&#21451;&#22909;&#22411;Lite&#29256;&#26412;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#36890;&#36807;&#21033;&#29992;GPT-4V&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#12289;&#22797;&#26434;&#25512;&#29702;&#25351;&#20196;&#21644;&#22270;&#29255;&#35814;&#32454;&#31572;&#26696;&#30340;&#33021;&#21147;&#21019;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#32467;&#26524;&#27169;&#22411;ALLaVA&#22312;12&#39033;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#19982;&#26368;&#22810;3B LVLMs&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#39033;&#24037;&#20316;&#31361;&#20986;&#20102;&#22312;&#35774;&#35745;&#26356;&#39640;&#25928;&#30340;LVLMs&#20013;&#37319;&#29992;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#28436;&#31034;&#21487;&#22312;\url{https://allava.freedomai.cn}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11684v1 Announce Type: cross  Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have enabled processing of multimodal inputs in language models but require significant computational resources for deployment, especially in edge devices. This study aims to bridge the performance gap between traditional-scale LVLMs and resource-friendly lite versions by adopting high-quality training data. To do this, a synthetic dataset is created by leveraging GPT-4V's ability to generate detailed captions, complex reasoning instructions and detailed answers from images. The resulted model trained with our data, ALLaVA, achieves competitive performance on 12 benchmarks up to 3B LVLMs. This work highlights the feasibility of adopting high-quality data in crafting more efficient LVLMs. Our online demo is available at \url{https://allava.freedomai.cn}.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#23545;3D&#28857;&#20113;&#36827;&#34892;&#21387;&#32553;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;3D&#21040;2D&#36716;&#25442;&#21644;&#23494;&#38598;&#30340;&#34920;&#31034;&#32467;&#26500;&#65292;&#22312;&#21387;&#32553;&#20013;&#39640;&#25928;&#21033;&#29992;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11680</link><description>&lt;p&gt;
&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#23545;3D&#28857;&#20113;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
3D Point Cloud Compression with Recurrent Neural Network and Image Compression Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11680
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#23545;3D&#28857;&#20113;&#36827;&#34892;&#21387;&#32553;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;3D&#21040;2D&#36716;&#25442;&#21644;&#23494;&#38598;&#30340;&#34920;&#31034;&#32467;&#26500;&#65292;&#22312;&#21387;&#32553;&#20013;&#39640;&#25928;&#21033;&#29992;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#20648;&#21644;&#20256;&#36755;LiDAR&#28857;&#20113;&#25968;&#25454;&#23545;&#20110;&#35768;&#22810;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#12289;&#36828;&#31243;&#25511;&#21046;&#12289;&#20113;&#26381;&#21153;&#25110;SLAM&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#21644;&#26080;&#24207;&#32467;&#26500;&#65292;&#23558;&#28857;&#20113;&#25968;&#25454;&#21387;&#32553;&#21040;&#36739;&#20302;&#20307;&#31215;&#26159;&#22256;&#38590;&#30340;&#12290;&#23558;&#21407;&#22987;&#28857;&#20113;&#25968;&#25454;&#36716;&#25442;&#20026;&#31264;&#23494;&#30340;2D&#30697;&#38453;&#32467;&#26500;&#26159;&#19968;&#31181;&#24212;&#29992;&#21387;&#32553;&#31639;&#27861;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#25439;&#21644;&#26657;&#20934;&#30340;3D&#21040;2D&#36716;&#25442;&#65292;&#20801;&#35768;&#21387;&#32553;&#31639;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;2D&#34920;&#31034;&#20013;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#21387;&#32553;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#24120;&#35265;&#30340;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#28145;&#24230;&#21387;&#32553;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#23558;LiDAR&#30340;&#24378;&#24230;&#27979;&#37327;&#37325;&#26032;&#25490;&#21015;&#25104;&#31264;&#23494;&#30340;2D&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#24378;&#24230;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11680v1 Announce Type: cross  Abstract: Storing and transmitting LiDAR point cloud data is essential for many AV applications, such as training data collection, remote control, cloud services or SLAM. However, due to the sparsity and unordered structure of the data, it is difficult to compress point cloud data to a low volume. Transforming the raw point cloud data into a dense 2D matrix structure is a promising way for applying compression algorithms. We propose a new lossless and calibrated 3D-to-2D transformation which allows compression algorithms to efficiently exploit spatial correlations within the 2D representation. To compress the structured representation, we use common image compression methods and also a self-supervised deep compression approach using a recurrent neural network. We also rearrange the LiDAR's intensity measurements to a dense 2D representation and propose a new metric to evaluate the compression performance of the intensity. Compared to approaches 
&lt;/p&gt;</description></item><item><title>MultiCorrupt&#26159;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#22312;&#21313;&#31181;&#19981;&#21516;&#25968;&#25454;&#25439;&#22351;&#31867;&#22411;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.11677</link><description>&lt;p&gt;
MultiCorrupt&#65306;&#19968;&#31181;&#29992;&#20110;3D&#29289;&#20307;&#26816;&#27979;&#30340;LiDAR-&#30456;&#26426;&#34701;&#21512;&#30340;&#22810;&#27169;&#24577;&#40065;&#26834;&#24615;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11677
&lt;/p&gt;
&lt;p&gt;
MultiCorrupt&#26159;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#22312;&#21313;&#31181;&#19981;&#21516;&#25968;&#25454;&#25439;&#22351;&#31867;&#22411;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20934;&#25968;&#25454;&#38598;&#22914;nuScenes&#19978;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#23545;&#23494;&#38598;&#37319;&#26679;&#30340;LiDAR&#28857;&#20113;&#21644;&#31934;&#24515;&#26657;&#20934;&#30340;&#20256;&#24863;&#22120;&#38453;&#21015;&#20381;&#36182;&#24615;&#20351;&#24471;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MultiCorrupt&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#23545;&#21313;&#31181;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#25439;&#22351;&#30340;&#40065;&#26834;&#24615;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11677v1 Announce Type: cross  Abstract: Multi-modal 3D object detection models for automated driving have demonstrated exceptional performance on computer vision benchmarks like nuScenes. However, their reliance on densely sampled LiDAR point clouds and meticulously calibrated sensor arrays poses challenges for real-world applications. Issues such as sensor misalignment, miscalibration, and disparate sampling frequencies lead to spatial and temporal misalignment in data from LiDAR and cameras. Additionally, the integrity of LiDAR and camera data is often compromised by adverse environmental conditions such as inclement weather, leading to occlusions and noise interference. To address this challenge, we introduce MultiCorrupt, a comprehensive benchmark designed to evaluate the robustness of multi-modal 3D object detectors against ten distinct types of corruptions. We evaluate five state-of-the-art multi-modal detectors on MultiCorrupt and analyze their performance in terms of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21453;&#21465;&#20107;&#65292;&#36890;&#36807;5&#20010;&#26041;&#38754;&#20174;&#19987;&#38376; NGO &#25351;&#21335;&#20013;&#25552;&#21462;&#23450;&#20041;&#30340;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#20197;&#24448;&#35780;&#20272;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11676</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21453;&#21465;&#20107;&#35780;&#20272;&#30340;&#22810;&#26041;&#38754;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11676
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21453;&#21465;&#20107;&#65292;&#36890;&#36807;5&#20010;&#26041;&#38754;&#20174;&#19987;&#38376; NGO &#25351;&#21335;&#20013;&#25552;&#21462;&#23450;&#20041;&#30340;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#20197;&#24448;&#35780;&#20272;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21465;&#20107;&#26159;&#23545;&#20167;&#24680;&#35328;&#35770;&#32972;&#26223;&#30340;&#30693;&#24773;&#22238;&#24212;&#65292;&#26088;&#22312;&#39539;&#26021;&#20167;&#24680;&#20027;&#24352;&#24182;&#21270;&#35299;&#20914;&#31361;&#65292;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20167;&#24680;&#35328;&#35770;&#24178;&#39044;&#31574;&#30053;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#33258;&#21160;&#29983;&#25104;&#21453;&#21465;&#20107;&#30340;&#26041;&#27861;&#26469;&#36741;&#21161;&#25163;&#21160;&#24178;&#39044;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#35780;&#20272;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21457;&#23637;&#12290;&#20808;&#21069;&#29992;&#20110;&#21453;&#21465;&#20107;&#35780;&#20272;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#32570;&#20047;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#19968;&#33268;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#34920;&#38754;&#21442;&#32771;&#27604;&#36739;&#65292;&#32780;&#19981;&#26159;&#23558;&#21453;&#21465;&#20107;&#36136;&#37327;&#30340;&#20851;&#38190;&#26041;&#38754;&#32435;&#20837;&#35780;&#20272;&#26631;&#20934;&#12290;&#20026;&#35299;&#20915;&#20808;&#21069;&#30340;&#35780;&#20272;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20419;&#20351;LLM&#25552;&#20379;&#29983;&#25104;&#30340;&#21453;&#21465;&#20107;&#20505;&#36873;&#30340;&#24471;&#20998;&#21644;&#21453;&#39304;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#19987;&#38376;NGO&#30340;&#21453;&#21465;&#20107;&#25351;&#21335;&#20013;&#25552;&#21462;&#30340;5&#20010;&#23450;&#20041;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11676v1 Announce Type: cross  Abstract: Counter narratives - informed responses to hate speech contexts designed to refute hateful claims and de-escalate encounters - have emerged as an effective hate speech intervention strategy. While previous work has proposed automatic counter narrative generation methods to aid manual interventions, the evaluation of these approaches remains underdeveloped. Previous automatic metrics for counter narrative evaluation lack alignment with human judgment as they rely on superficial reference comparisons instead of incorporating key aspects of counter narrative quality as evaluation criteria. To address prior evaluation limitations, we propose a novel evaluation framework prompting LLMs to provide scores and feedback for generated counter narrative candidates using 5 defined aspects derived from guidelines from counter narrative specialized NGOs. We found that LLM evaluators achieve strong alignment to human-annotated scores and feedback and
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#25104;&#21151;&#24320;&#21457;&#20102;&#29233;&#27801;&#23612;&#20122;&#35821;&#35328;&#30340;&#25340;&#20889;&#21644;&#35821;&#27861;&#26657;&#27491;&#24037;&#20855;&#65292;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#33258;&#21160;&#35780;&#20272;&#26469;&#20811;&#26381;&#21487;&#29992;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11671</link><description>&lt;p&gt;
&#29233;&#27801;&#23612;&#20122;&#25991;&#26412;&#30340;&#33258;&#21160;&#26657;&#27491;&#65306;EKTB25&#39033;&#30446;&#30340;&#26368;&#32456;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Autocorrect for Estonian texts: final report from project EKTB25
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11671
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#25104;&#21151;&#24320;&#21457;&#20102;&#29233;&#27801;&#23612;&#20122;&#35821;&#35328;&#30340;&#25340;&#20889;&#21644;&#35821;&#27861;&#26657;&#27491;&#24037;&#20855;&#65292;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#33258;&#21160;&#35780;&#20272;&#26469;&#20811;&#26381;&#21487;&#29992;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#30001;&#29233;&#27801;&#23612;&#20122;&#35821;&#35328;&#25216;&#26415;&#22269;&#23478;&#35745;&#21010;&#20110;2021-2023&#24180;&#36164;&#21161;&#65292;&#26088;&#22312;&#20026;&#29233;&#27801;&#23612;&#20122;&#35821;&#24320;&#21457;&#25340;&#20889;&#21644;&#35821;&#27861;&#26657;&#27491;&#24037;&#20855;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#25152;&#38656;&#29992;&#20110;&#27492;&#31867;&#24320;&#21457;&#30340;&#21487;&#29992;&#38169;&#35823;&#26657;&#27491;&#25968;&#25454;&#37327;&#38750;&#24120;&#23567;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;(1) &#25105;&#20204;&#20026;&#27169;&#22411;&#35757;&#32451;&#21644;&#27979;&#35797;&#27880;&#37322;&#20102;&#26356;&#22810;&#30340;&#26657;&#27491;&#25968;&#25454;&#65292;(2) &#25105;&#20204;&#27979;&#35797;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#21363;&#37325;&#26032;&#35757;&#32451;&#20026;&#20854;&#20182;&#20219;&#21153;&#21019;&#24314;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20415;&#19981;&#20165;&#20381;&#36182;&#20110;&#26657;&#27491;&#25968;&#25454;&#65292;(3) &#25105;&#20204;&#23545;&#27604;&#20102;&#24320;&#21457;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#19982;&#20854;&#20182;&#36873;&#25321;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#33258;&#21160;&#35780;&#20272;&#65292;&#21487;&#20197;&#36890;&#36807;&#38169;&#35823;&#31867;&#21035;&#35745;&#31639;&#20462;&#27491;&#30340;&#20934;&#30830;&#24615;&#21644;&#25910;&#30410;&#65292;&#20174;&#32780;&#21487;&#20197;&#35814;&#32454;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11671v1 Announce Type: cross  Abstract: The project was funded in 2021-2023 by the National Programme of Estonian Language Technology. Its main aim was to develop spelling and grammar correction tools for the Estonian language. The main challenge was the very small amount of available error correction data needed for such development. To mitigate this, (1) we annotated more correction data for model training and testing, (2) we tested transfer-learning, i.e. retraining machine learning models created for other tasks, so as not to depend solely on correction data, (3) we compared the developed method and model with alternatives, including large language models. We also developed automatic evaluation, which can calculate the accuracy and yield of corrections by error category, so that the effectiveness of different methods can be compared in detail.   There has been a breakthrough in large language models during the project: GPT4, a commercial language model with Estonian-lang
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#21160;&#24577;&#35268;&#21010;&#39046;&#22495;&#20013;&#27169;&#25311;&#24037;&#20855;&#20351;&#29992;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#35813;&#39046;&#22495;&#32771;&#34385;&#21040;&#29983;&#29289;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;</title><link>https://arxiv.org/abs/2402.11658</link><description>&lt;p&gt;
&#20998;&#23618;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Dynamic planning in hierarchical active inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11658
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#21160;&#24577;&#35268;&#21010;&#39046;&#22495;&#20013;&#27169;&#25311;&#24037;&#20855;&#20351;&#29992;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#35813;&#39046;&#22495;&#32771;&#34385;&#21040;&#29983;&#29289;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#20154;&#31867;&#22823;&#33041;&#25512;&#26029;&#21644;&#26045;&#21152;&#19982;&#35748;&#30693;&#20915;&#31574;&#30456;&#20851;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#33539;&#24335;&#65292;&#20027;&#21160;&#25512;&#26029;&#65292;&#20026;&#29983;&#29289;&#26377;&#26426;&#20307;&#36866;&#24212;&#24102;&#26469;&#20102;&#22522;&#26412;&#35265;&#35299;&#65292;&#19981;&#26029;&#21162;&#21147;&#26368;&#23567;&#21270;&#39044;&#27979;&#35823;&#24046;&#20197;&#23558;&#33258;&#24049;&#38480;&#21046;&#22312;&#19982;&#29983;&#21629;&#20860;&#23481;&#30340;&#29366;&#24577;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#20154;&#31867;&#21644;&#21160;&#29289;&#34892;&#20026;&#21487;&#20197;&#35299;&#37322;&#20026;&#20027;&#21160;&#25512;&#26029;&#36807;&#31243;&#65292;&#26080;&#35770;&#26159;&#20316;&#20026;&#31163;&#25955;&#20915;&#31574;&#36824;&#26159;&#36830;&#32493;&#36816;&#21160;&#25511;&#21046;&#65292;&#37117;&#28608;&#21457;&#20102;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#32570;&#20047;&#23545;&#22914;&#20309;&#26377;&#25928;&#22320;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#35268;&#21010;&#34892;&#21160;&#30340;&#20840;&#38754;&#23637;&#26395;&#12290;&#25105;&#20204;&#35774;&#23450;&#20102;&#23545;&#24037;&#20855;&#20351;&#29992;&#36827;&#34892;&#24314;&#27169;&#30340;&#30446;&#26631;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#20027;&#39064;&#65292;&#29282;&#35760;&#20004;&#20010;&#29983;&#29289;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#20851;&#38190;&#26041;&#38754;&#65306;&#29702;&#35299;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11658v1 Announce Type: new  Abstract: By dynamic planning, we refer to the ability of the human brain to infer and impose motor trajectories related to cognitive decisions. A recent paradigm, active inference, brings fundamental insights into the adaptation of biological organisms, constantly striving to minimize prediction errors to restrict themselves to life-compatible states. Over the past years, many studies have shown how human and animal behavior could be explained in terms of an active inferential process -- either as discrete decision-making or continuous motor control -- inspiring innovative solutions in robotics and artificial intelligence. Still, the literature lacks a comprehensive outlook on how to effectively plan actions in changing environments. Setting ourselves the goal of modeling tool use, we delve into the topic of dynamic planning in active inference, keeping in mind two crucial aspects of biological goal-directed behavior: the capacity to understand a
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#38754;&#20020;&#30528;&#36830;&#32493;&#21644;&#31163;&#25955;&#36164;&#28304;&#32422;&#26463;&#30340;&#25361;&#25112;&#65292;&#20294;&#26377;&#26395;&#23454;&#29616;&#39640;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.11653</link><description>&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#32452;&#21512;&#24335;&#23458;&#25143;&#31471;-&#20027;&#25511;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20219;&#21153;&#21368;&#36733;
&lt;/p&gt;
&lt;p&gt;
Combinatorial Client-Master Multiagent Deep Reinforcement Learning for Task Offloading in Mobile Edge Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11653
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#38754;&#20020;&#30528;&#36830;&#32493;&#21644;&#31163;&#25955;&#36164;&#28304;&#32422;&#26463;&#30340;&#25361;&#25112;&#65292;&#20294;&#26377;&#26395;&#23454;&#29616;&#39640;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#25191;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;&#35270;&#39057;&#27969;&#23186;&#20307;&#12289;&#25968;&#25454;&#25366;&#25496;&#12289;&#34394;&#25311;&#29616;&#23454;&#12289;&#22686;&#24378;&#29616;&#23454;&#12289;&#22270;&#20687;&#22788;&#29702;&#12289;&#35270;&#39057;&#22788;&#29702;&#12289;&#20154;&#33080;&#35782;&#21035;&#21644;&#22312;&#32447;&#28216;&#25103;&#12290;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#28385;&#36275;&#29992;&#25143;&#35774;&#22791;&#65288;UDs&#65289;&#26085;&#30410;&#22686;&#38271;&#30340;&#35745;&#31639;&#38656;&#27714;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;MEC&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#26159;&#19968;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;UDs&#21644;MEC&#26381;&#21153;&#22120;&#20043;&#38388;&#20998;&#37197;&#20219;&#21153;&#26469;&#28385;&#36275;UDs&#30340;&#38656;&#27714;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#20013;&#21463;&#21040;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36866;&#24212;&#21160;&#24577;&#21464;&#21270;&#24182;&#26368;&#23567;&#21270;&#22312;&#32447;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;UDs&#21644;MEC&#26381;&#21153;&#22120;&#19978;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#36830;&#32493;&#21644;&#31163;&#25955;&#36164;&#28304;&#32422;&#26463;&#23545;&#26377;&#25928;&#30340;&#22522;&#20110;DRL&#30340;&#20219;&#21153;&#21368;&#36733;&#35774;&#35745;&#26500;&#25104;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11653v1 Announce Type: new  Abstract: Recently, there has been an explosion of mobile applications that perform computationally intensive tasks such as video streaming, data mining, virtual reality, augmented reality, image processing, video processing, face recognition, and online gaming. However, user devices (UDs), such as tablets and smartphones, have a limited ability to perform the computation needs of the tasks. Mobile edge computing (MEC) has emerged as a promising technology to meet the increasing computing demands of UDs. Task offloading in MEC is a strategy that meets the demands of UDs by distributing tasks between UDs and MEC servers. Deep reinforcement learning (DRL) is gaining attention in task-offloading problems because it can adapt to dynamic changes and minimize online computational complexity. However, the various types of continuous and discrete resource constraints on UDs and MEC servers pose challenges to the design of an efficient DRL-based task-offlo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#21644;&#26657;&#27491;&#37327;&#23376;&#22788;&#29702;&#22270;&#20687;&#20013;&#30340;&#22122;&#22768;&#65292;&#20197;&#25552;&#39640;&#37327;&#23376;&#22270;&#20687;&#22788;&#29702;&#30340;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#23454;&#29616;&#19982;&#32463;&#20856;&#35745;&#31639;&#26426;&#31867;&#20284;&#30340;&#22788;&#29702;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.11645</link><description>&lt;p&gt;
&#20511;&#21161;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#37327;&#23376;&#22270;&#20687;&#21435;&#22122;&#65306;&#25913;&#36827;&#37327;&#23376;&#22270;&#20687;&#22788;&#29702;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantum Image Denoising with Machine Learning: A Novel Approach to Improve Quantum Image Processing Quality and Reliability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11645
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#21644;&#26657;&#27491;&#37327;&#23376;&#22788;&#29702;&#22270;&#20687;&#20013;&#30340;&#22122;&#22768;&#65292;&#20197;&#25552;&#39640;&#37327;&#23376;&#22270;&#20687;&#22788;&#29702;&#30340;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#23454;&#29616;&#19982;&#32463;&#20856;&#35745;&#31639;&#26426;&#31867;&#20284;&#30340;&#22788;&#29702;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Quantum Image Processing&#65288;QIP&#65289;&#26159;&#19968;&#20010;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#20248;&#21183;&#26469;&#25805;&#20316;&#21644;&#20998;&#26512;&#22270;&#20687;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;QIP&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#37327;&#23376;&#27604;&#29305;&#30340;&#38480;&#21046;&#21644;&#37327;&#23376;&#26426;&#22120;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;QIP&#20013;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#20351;&#29992;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#24182;&#26657;&#27491;&#37327;&#23376;&#22788;&#29702;&#22270;&#20687;&#20013;&#30340;&#22122;&#22768;&#65292;&#25105;&#20204;&#21487;&#20197;&#24357;&#34917;&#26426;&#22120;&#36896;&#25104;&#30340;&#22024;&#26434;&#65292;&#24182;&#20197;&#27604;&#32463;&#20856;&#35745;&#31639;&#26426;&#26356;&#39640;&#30340;&#25928;&#29575;&#26816;&#32034;&#20986;&#19982;&#20043;&#31867;&#20284;&#30340;&#22788;&#29702;&#32467;&#26524;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21253;&#25324;&#29616;&#26377;&#22788;&#29702;&#22270;&#20687;&#21644;&#26469;&#33258;&#24320;&#25918;&#33719;&#21462;&#25968;&#25454;&#38598;&#30340;&#37327;&#23376;&#22788;&#29702;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#23558;&#33021;&#22815;&#20026;&#25105;&#20204;&#25552;&#20379;&#27599;&#20010;&#20687;&#32032;&#30340;&#32622;&#20449;&#27700;&#24179;&#21450;&#20854;&#28508;&#22312;&#21407;&#22987;&#20540;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#27169;&#22411;&#22312;&#24357;&#34917;Q&#22788;&#29702;&#20013;&#30340;&#25439;&#22833;&#21644;&#36864;&#30456;&#24178;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11645v1 Announce Type: cross  Abstract: Quantum Image Processing (QIP) is a field that aims to utilize the benefits of quantum computing for manipulating and analyzing images. However, QIP faces two challenges: the limitation of qubits and the presence of noise in a quantum machine. In this research we propose a novel approach to address the issue of noise in QIP. By training and employing a machine learning model that identifies and corrects the noise in quantum processed images, we can compensate for the noisiness caused by the machine and retrieve a processing result similar to that performed by a classical computer with higher efficiency. The model is trained by learning a dataset consisting of both existing processed images and quantum processed images from open access datasets. This model will be capable of providing us with the confidence level for each pixel and its potential original value. To assess the model's accuracy in compensating for loss and decoherence in Q
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#26694;&#26550;&#65288;LogicCheckGPT&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.11622</link><description>&lt;p&gt;
&#36923;&#36753;&#38381;&#29615;&#65306;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23545;&#35937;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11622
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#26694;&#26550;&#65288;LogicCheckGPT&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#24187;&#35273;&#19968;&#30452;&#26159;&#38459;&#30861;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#26356;&#24191;&#27867;&#24212;&#29992;&#30340;&#36719;&#32907;&#12290;&#23545;&#35937;&#24187;&#35273;&#26159;&#25351;LVLMs&#22312;&#22270;&#20687;&#20013;&#22768;&#31216;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30340;&#29616;&#35937;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#25351;&#23548;&#35843;&#25972;&#21644;&#22522;&#20110;&#22806;&#37096;&#27169;&#22411;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#22806;&#37096;&#27169;&#22411;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26410;&#28145;&#20837;&#25506;&#35752;&#30340;&#39046;&#22495;&#65292;&#21363;&#21033;&#29992;LVLM&#26412;&#36523;&#26469;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36825;&#26679;&#30340;&#30452;&#35273;&#65292;&#21363;LVLM&#20542;&#21521;&#20110;&#23545;&#23384;&#22312;&#30340;&#23545;&#35937;&#20570;&#20986;&#36923;&#36753;&#19968;&#33268;&#30340;&#21453;&#24212;&#65292;&#20294;&#23545;&#24187;&#35273;&#23545;&#35937;&#20570;&#20986;&#19981;&#19968;&#33268;&#30340;&#21453;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#23545;&#35937;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#36731;&#26694;&#26550;&#65292;&#21363;LogicCheckGPT&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36923;&#36753;&#19968;&#33268;&#24615;&#25506;&#27979;&#26469;&#25552;&#20986;&#20855;&#26377;&#36923;&#36753;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11622v1 Announce Type: cross  Abstract: Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical corr
&lt;/p&gt;</description></item><item><title>`spotRiverGUI`&#26159;&#19968;&#20010;&#20026;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65292;&#31616;&#21270;&#20102;&#29992;&#25143;&#25163;&#21160;&#25628;&#32034;&#26368;&#20339;&#36229;&#21442;&#25968;&#35774;&#32622;&#30340;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.11594</link><description>&lt;p&gt;
&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#31616;&#21270;--spotRiverGUI
&lt;/p&gt;
&lt;p&gt;
Simplifying Hyperparameter Tuning in Online Machine Learning -- The spotRiverGUI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11594
&lt;/p&gt;
&lt;p&gt;
`spotRiverGUI`&#26159;&#19968;&#20010;&#20026;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65292;&#31616;&#21270;&#20102;&#29992;&#25143;&#25163;&#21160;&#25628;&#32034;&#26368;&#20339;&#36229;&#21442;&#25968;&#35774;&#32622;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#26426;&#22120;&#23398;&#20064;(BML)&#22312;&#22788;&#29702;&#22823;&#37327;&#27969;&#25968;&#25454;&#26102;&#23384;&#22312;&#20869;&#23384;&#12289;&#25968;&#25454;&#27969;&#28418;&#31227;&#22788;&#29702;&#21644;&#22788;&#29702;&#26032;&#30340;&#26410;&#30693;&#25968;&#25454;&#31561;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;(OML)&#26159;BML&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#20197;&#39034;&#24207;&#26041;&#24335;&#22788;&#29702;&#25968;&#25454;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#25968;&#25454;&#27969;&#12290;`river`&#21253;&#26159;&#19968;&#20010;Python OML&#24211;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#32858;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#31561;&#12290;`spotRiver`&#21253;&#20026;OML&#27169;&#22411;&#25552;&#20379;&#20102;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#26694;&#26550;&#12290;`spotRiverGUI`&#26159;`spotRiver`&#21253;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#20174;&#24378;&#22823;&#30340;&#31639;&#27861;&#20013;&#36873;&#25321;&#26368;&#20248;&#36229;&#21442;&#25968;&#35774;&#32622;&#30340;&#20415;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11594v1 Announce Type: cross  Abstract: Batch Machine Learning (BML) reaches its limits when dealing with very large amounts of streaming data. This is especially true for available memory, handling drift in data streams, and processing new, unknown data. Online Machine Learning (OML) is an alternative to BML that overcomes the limitations of BML. OML is able to process data in a sequential manner, which is especially useful for data streams. The `river` package is a Python OML-library, which provides a variety of online learning algorithms for classification, regression, clustering, anomaly detection, and more. The `spotRiver` package provides a framework for hyperparameter tuning of OML models. The `spotRiverGUI` is a graphical user interface for the `spotRiver` package. The `spotRiverGUI` releases the user from the burden of manually searching for the optimal hyperparameter setting. After the data is provided, users can compare different OML algorithms from the powerful `
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33033;&#20914;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#21033;&#29992;Transformer&#21462;&#20195;U-net&#32467;&#26500;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;SNN&#30340;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#30340;&#23454;&#35777;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.11588</link><description>&lt;p&gt;
&#24102;Transformer&#30340;&#33033;&#20914;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SDiT: Spiking Diffusion Model with Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33033;&#20914;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#21033;&#29992;Transformer&#21462;&#20195;U-net&#32467;&#26500;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;SNN&#30340;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#30340;&#23454;&#35777;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#20855;&#26377;&#20302;&#21151;&#32791;&#21644;&#29983;&#29289;&#35299;&#37322;&#29305;&#24615;&#65292;&#34987;&#35748;&#20026;&#22312;&#33410;&#33021;&#35745;&#31639;&#26041;&#38754;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#25506;&#32034;SNNs&#30340;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#65292;&#23578;&#26410;&#25552;&#20986;&#22522;&#20110;SNN&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#32479;&#19968;&#19988;&#26377;&#25928;&#30340;&#32467;&#26500;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19968;&#31181;&#26032;&#22411;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#21033;&#29992;Transformer&#26469;&#21462;&#20195;&#20027;&#27969;&#25193;&#25955;&#27169;&#22411;&#20013;&#24120;&#29992;&#30340;U-net&#32467;&#26500;&#12290;&#23427;&#33021;&#22815;&#20197;&#30456;&#23545;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36739;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#29983;&#25104;&#36136;&#37327;&#26356;&#39640;&#30340;&#22270;&#20687;&#12290;&#23427;&#26088;&#22312;&#20026;&#22522;&#20110;SNN&#30340;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#25552;&#20379;&#23454;&#35777;&#22522;&#20934;&#12290;&#22312;MNIST&#12289;Fashion-MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#19982;&#29616;&#26377;&#30340;SNN&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11588v1 Announce Type: cross  Abstract: Spiking neural networks (SNNs) have low power consumption and bio-interpretable characteristics, and are considered to have tremendous potential for energy-efficient computing. However, the exploration of SNNs on image generation tasks remains very limited, and a unified and effective structure for SNN-based generative models has yet to be proposed. In this paper, we explore a novel diffusion model architecture within spiking neural networks. We utilize transformer to replace the commonly used U-net structure in mainstream diffusion models. It can generate higher quality images with relatively lower computational cost and shorter sampling time. It aims to provide an empirical baseline for research of generative models based on SNNs. Experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our work is highly competitive compared to existing SNN generative models.
&lt;/p&gt;</description></item><item><title>&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#31038;&#20132;&#26426;&#22120;&#20154;&#20013;&#65292;&#23454;&#29616;&#26356;&#21160;&#24577;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#23545;&#35805;&#65292;&#21253;&#25324;&#20351;&#29992;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#65292;&#35843;&#25972;&#35821;&#35843;&#65292;&#24182;&#21033;&#29992;&#34920;&#24773;&#31526;&#21495;&#29983;&#25104;&#26426;&#22120;&#20154;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.11571</link><description>&lt;p&gt;
&#19981;&#35843;&#30382; &#8212;&#8212; &#20351;&#29992;LLMs&#29983;&#25104;&#19982;&#21488;&#24335;&#26426;&#22120;&#20154;Haru&#23545;&#35805;&#20013;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Ain't Misbehavin' -- Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11571
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#31038;&#20132;&#26426;&#22120;&#20154;&#20013;&#65292;&#23454;&#29616;&#26356;&#21160;&#24577;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#23545;&#35805;&#65292;&#21253;&#25324;&#20351;&#29992;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#65292;&#35843;&#25972;&#35821;&#35843;&#65292;&#24182;&#21033;&#29992;&#34920;&#24773;&#31526;&#21495;&#29983;&#25104;&#26426;&#22120;&#20154;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#26426;&#22120;&#20154;&#26088;&#22312;&#36890;&#36807;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#19982;&#20154;&#31867;&#24314;&#31435;&#38271;&#26399;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#23545;&#35805;&#26041;&#27861;&#20381;&#36182;&#20110;&#33050;&#26412;&#21270;&#20114;&#21160;&#65292;&#24448;&#24448;&#26080;&#27861;&#20445;&#25345;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#21040;&#31038;&#20132;&#26426;&#22120;&#20154;&#20013;&#65292;&#23454;&#29616;&#26356;&#21160;&#24577;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#23545;&#35805;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#19968;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#21033;&#29992;LLMs&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#34892;&#20026;&#30340;&#26426;&#22120;&#20154;&#21709;&#24212;&#65292;&#19982;&#26426;&#22120;&#20154;&#20010;&#24615;&#19968;&#33268;&#12290;&#25105;&#20204;&#23558;&#26426;&#22120;&#20154;&#34892;&#20026;&#19982;&#20004;&#31181;&#24418;&#24335;&#32467;&#21512;&#36215;&#26469;&#65306;1&#65289;&#19968;&#20010;&#33021;&#22815;&#20855;&#22791;&#21508;&#31181;&#35821;&#35843;&#39118;&#26684;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#24341;&#25806;&#65292;&#20197;&#21450;2&#65289;&#26426;&#22120;&#20154;&#30340;&#19968;&#31995;&#21015;&#29289;&#29702;&#21160;&#20316;&#24211;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#23450;&#20041;&#30340;&#26368;&#26032;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#65292;&#21160;&#24577;&#36873;&#25321;&#26426;&#22120;&#20154;&#30340;&#35821;&#35843;&#65292;&#24182;&#21033;&#29992;LLM&#36755;&#20986;&#20013;&#30340;&#34920;&#24773;&#31526;&#21495;&#20316;&#20026;&#29983;&#25104;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#32447;&#32034;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#28436;&#31034;&#21487;&#22312;&#27492;&#22788;&#26597;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11571v1 Announce Type: cross  Abstract: Social robots aim to establish long-term bonds with humans through engaging conversation. However, traditional conversational approaches, reliant on scripted interactions, often fall short in maintaining engaging conversations. This paper addresses this limitation by integrating large language models (LLMs) into social robots to achieve more dynamic and expressive conversations. We introduce a fully-automated conversation system that leverages LLMs to generate robot responses with expressive behaviors, congruent with the robot's personality. We incorporate robot behavior with two modalities: 1) a text-to-speech (TTS) engine capable of various delivery styles, and 2) a library of physical actions for the robot. We develop a custom, state-of-the-art emotion recognition model to dynamically select the robot's tone of voice and utilize emojis from LLM output as cues for generating robot actions. A demo of our system is available here. To i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Haru&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#20027;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21457;&#25381;&#20102;&#20854;&#24773;&#24863;&#34920;&#36798;&#21644;&#29420;&#29305;&#20154;&#26684;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#34892;&#20026;&#25913;&#21464;&#36741;&#23548;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.11569</link><description>&lt;p&gt;
&#20351;&#29992;Haru&#24320;&#21457;&#33258;&#20027;&#26426;&#22120;&#20154;&#20171;&#23548;&#30340;&#34892;&#20026;&#36741;&#23548;&#20250;&#35805;
&lt;/p&gt;
&lt;p&gt;
Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11569
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Haru&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#20027;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21457;&#25381;&#20102;&#20854;&#24773;&#24863;&#34920;&#36798;&#21644;&#29420;&#29305;&#20154;&#26684;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#34892;&#20026;&#25913;&#21464;&#36741;&#23548;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#20026;&#34892;&#20026;&#25913;&#21464;&#36741;&#23548;&#35774;&#35745;&#21644;&#24433;&#21709;&#33258;&#20027;&#23545;&#35805;&#30340;&#32463;&#39564;&#35843;&#26597;&#12290;&#25105;&#20204;&#20851;&#27880;&#20351;&#29992;&#26700;&#38754;&#31038;&#20132;&#26426;&#22120;&#20154;Haru&#65292;&#24182;&#25506;&#32034;&#23454;&#26045;&#24494;&#20064;&#24815;&#26041;&#27861;&#26469;&#20419;&#36827;&#31215;&#26497;&#34892;&#20026;&#25913;&#21464;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#26680;&#24515;&#26159;&#24320;&#21457;&#19968;&#20010;&#23436;&#20840;&#33258;&#20027;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21457;&#25381;Haru&#30340;&#24773;&#24863;&#34920;&#36798;&#21644;&#29420;&#29305;&#20154;&#26684;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#23545;&#35805;&#31995;&#32479;&#30340;&#36845;&#20195;&#35774;&#35745;&#21644;&#24191;&#27867;&#27979;&#35797;&#65292;&#30830;&#20445;&#23427;&#26377;&#25928;&#22320;&#20307;&#29616;&#20102;&#24494;&#20064;&#24815;&#26041;&#27861;&#30340;&#21407;&#21017;&#65292;&#21516;&#26102;&#36824;&#34701;&#21512;&#20102;&#24314;&#31435;&#20449;&#20219;&#21644;&#30772;&#22351;&#20449;&#20219;&#30340;&#31574;&#30053;&#12290;&#26368;&#32456;&#29256;&#26412;&#23545;&#35805;&#30340;&#26377;&#25928;&#24615;&#22312;&#20154;&#31867;&#21442;&#19982;&#32773;&#65288;N=12&#65289;&#30340;&#23454;&#39564;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;Haru&#30340;&#29983;&#27668;&#12289;&#20114;&#21160;&#24615;&#21644;&#20013;&#31435;&#24615;&#30340;&#35748;&#30693;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11569v1 Announce Type: cross  Abstract: This study presents an empirical investigation into the design and impact of autonomous dialogues in human-robot interaction for behavior change coaching. We focus on the use of Haru, a tabletop social robot, and explore the implementation of the Tiny Habits method for fostering positive behavior change. The core of our study lies in developing a fully autonomous dialogue system that maximizes Haru's emotional expressiveness and unique personality. Our methodology involved iterative design and extensive testing of the dialogue system, ensuring it effectively embodied the principles of the Tiny Habits method while also incorporating strategies for trust-raising and trust-dampening. The effectiveness of the final version of the dialogue was evaluated in an experimental study with human participants (N=12). The results indicated a significant improvement in perceptions of Haru's liveliness, interactivity, and neutrality. Additionally, our
&lt;/p&gt;</description></item><item><title>&#23545;&#22270;&#19978;&#25345;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#20998;&#31867;&#65292;&#24357;&#34917;&#20102;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#19978;&#25345;&#32493;&#23398;&#20064;&#30740;&#31350;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.11565</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#65306;&#25361;&#25112;&#12289;&#35299;&#20915;&#26041;&#26696;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Continual Learning on Graphs: Challenges, Solutions, and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11565
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22270;&#19978;&#25345;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#20998;&#31867;&#65292;&#24357;&#34917;&#20102;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#19978;&#25345;&#32493;&#23398;&#20064;&#30740;&#31350;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22270;&#25968;&#25454;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#22240;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#20197;&#21450;&#23558;&#39034;&#24207;&#26356;&#26032;&#30340;&#27169;&#22411;&#36866;&#24212;&#26032;&#20986;&#29616;&#30340;&#22270;&#20219;&#21153;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#34429;&#28982;&#20154;&#20204;&#21162;&#21147;&#24635;&#32467;&#20102;&#22312;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#65288;&#20363;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#19978;&#25345;&#32493;&#23398;&#20064;&#30740;&#31350;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#21363;&#25345;&#32493;&#22270;&#23398;&#20064;&#65288;CGL&#65289;&#25110;&#32456;&#36523;&#22270;&#23398;&#20064;&#65292;&#20173;&#28982;&#38656;&#35201;&#36827;&#34892;&#31995;&#32479;&#24615;&#23457;&#26597;&#12290;&#22270;&#25968;&#25454;&#22312;&#25968;&#25454;&#32467;&#26500;&#21644;&#24212;&#29992;&#22330;&#26223;&#26041;&#38754;&#35201;&#22797;&#26434;&#24471;&#22810;&#65292;&#36825;&#20351;&#24471;CGL&#20219;&#21153;&#35774;&#32622;&#12289;&#27169;&#22411;&#35774;&#35745;&#21644;&#24212;&#29992;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#24046;&#36317;&#65292;&#25105;&#20204;&#36890;&#36807;&#38416;&#26126;&#19981;&#21516;&#30340;&#20219;&#21153;&#35774;&#32622;&#24182;&#26681;&#25454;&#29305;&#24615;&#23545;&#29616;&#26377;&#30340;&#25345;&#32493;&#22270;&#23398;&#20064;&#65288;CGL&#65289;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#25552;&#20379;&#20102;&#23545;&#29616;&#26377;&#25345;&#32493;&#22270;&#23398;&#20064;&#65288;CGL&#65289;&#31639;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#23558;CGL&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11565v1 Announce Type: cross  Abstract: Continual learning on graph data has recently attracted paramount attention for its aim to resolve the catastrophic forgetting problem on existing tasks while adapting the sequentially updated model to newly emerged graph tasks. While there have been efforts to summarize progress on continual learning research over Euclidean data, e.g., images and texts, a systematic review of progress in continual learning on graphs, a.k.a, continual graph learning (CGL) or lifelong graph learning, is still demanding. Graph data are far more complex in terms of data structures and application scenarios, making CGL task settings, model designs, and applications extremely challenging. To bridge the gap, we provide a comprehensive review of existing continual graph learning (CGL) algorithms by elucidating the different task settings and categorizing the existing methods based on their characteristics. We compare the CGL methods with traditional continual
&lt;/p&gt;</description></item><item><title>LongAgent&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23558;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#38271;&#25991;&#26412;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11550</link><description>&lt;p&gt;
LongAgent: &#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23558;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11550
&lt;/p&gt;
&lt;p&gt;
LongAgent&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23558;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#38271;&#25991;&#26412;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#35821;&#35328;&#21644;&#25191;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;LLMs&#20197;&#20854;&#26114;&#36149;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#39640;&#25512;&#29702;&#24310;&#36831;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22914;GPT-4&#21644;Claude2&#22312;&#22788;&#29702;&#36229;&#36807;$100k$&#26631;&#35760;&#30340;&#36755;&#20837;&#26102;&#20063;&#32463;&#24120;&#20986;&#38169;&#65292;&#36825;&#31181;&#29616;&#35937;&#20063;&#34987;&#31216;&#20026;\textit{&#20013;&#38388;&#36855;&#22833;}&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#26041;&#27861;\textsc{LongAgent}&#65292;&#23558;LLMs&#65288;&#20363;&#22914;LLaMA&#65289;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;&#65292;&#24182;&#23637;&#31034;&#20986;&#22312;&#38271;&#25991;&#26412;&#22788;&#29702;&#26041;&#38754;&#21487;&#33021;&#20248;&#20110;GPT-4&#30340;&#28508;&#21147;&#12290;&#22312;\textsc{LongAgent}&#20013;&#65292;&#19968;&#20301;&#39046;&#23548;&#32773;&#36127;&#36131;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#24182;&#25351;&#23548;&#22242;&#38431;&#25104;&#21592;&#20174;&#25991;&#26723;&#20013;&#33719;&#21462;&#20449;&#24687;&#12290;&#30001;&#20110;&#25104;&#21592;&#23384;&#22312;&#24187;&#35273;&#65292;&#39046;&#23548;&#32773;&#20174;&#20960;&#21313;&#21040;&#25968;&#30334;&#21517;&#25104;&#21592;&#30340;&#22238;&#24212;&#20013;&#33719;&#21462;&#20934;&#30830;&#20449;&#24687;&#24182;&#38750;&#26131;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11550v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated impressive performance in understanding language and executing complex reasoning tasks. However, LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \textit{lost in the middle}. In this paper, we propose \textsc{LongAgent}, a method based on multi-agent collaboration, which scales LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority in long-text processing compared to GPT-4. In \textsc{LongAgent}, a leader is responsible for understanding user intent and directing team members to acquire information from documents. Due to members' hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of member
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#36235;&#21183;&#65292;&#20351;&#29992;&#35758;&#20250;&#36777;&#35770;&#35821;&#26009;&#24211;&#65292;&#25506;&#35752;&#20102;&#21477;&#27861;&#20381;&#23384;&#36317;&#31163;&#26368;&#23567;&#21270;&#21450;&#22522;&#20110;&#26641;&#22270;&#23646;&#24615;&#30340;15&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#25581;&#31034;&#20102;&#29616;&#20195;&#35299;&#26512;&#22120;&#22312;&#36825;&#31181;&#21464;&#21270;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11549</link><description>&lt;p&gt;
&#33521;&#35821;&#21644;&#24503;&#35821;&#30340;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#65306;&#24230;&#37327;&#12289;&#35299;&#26512;&#22120;&#21644;&#36235;&#21516;
&lt;/p&gt;
&lt;p&gt;
Syntactic Language Change in English and German: Metrics, Parsers, and Convergences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#36235;&#21183;&#65292;&#20351;&#29992;&#35758;&#20250;&#36777;&#35770;&#35821;&#26009;&#24211;&#65292;&#25506;&#35752;&#20102;&#21477;&#27861;&#20381;&#23384;&#36317;&#31163;&#26368;&#23567;&#21270;&#21450;&#22522;&#20110;&#26641;&#22270;&#23646;&#24615;&#30340;15&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#25581;&#31034;&#20102;&#29616;&#20195;&#35299;&#26512;&#22120;&#22312;&#36825;&#31181;&#21464;&#21270;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#35821;&#35328;&#24448;&#24448;&#20250;&#20248;&#21270;&#35821;&#35328;&#32467;&#26500;&#20197;&#38477;&#20302;&#22797;&#26434;&#24615;&#65292;&#22686;&#21152;&#20132;&#27969;&#25928;&#29575;&#12290;&#21477;&#27861;&#20381;&#23384;&#36317;&#31163;&#34913;&#37327;&#20102;&#30456;&#20851;&#35789;&#27719;&#20043;&#38388;&#30340;&#32447;&#24615;&#36317;&#31163;&#65292;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#35821;&#35328;&#22788;&#29702;&#22256;&#38590;&#21644;&#24037;&#20316;&#35760;&#24518;&#36127;&#33655;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#30340;&#21382;&#26102;&#36235;&#21183;&#65292;&#20351;&#29992;&#20102;&#36807;&#21435;&#22823;&#32422;160&#24180;&#38388;&#30340;&#35758;&#20250;&#36777;&#35770;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#22522;&#20110;5&#20010;&#20381;&#23384;&#21477;&#27861;&#35299;&#26512;&#22120;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#21253;&#25324;&#24191;&#27867;&#20351;&#29992;&#30340;Stanford CoreNLP&#20197;&#21450;&#20854;&#20182;4&#20010;&#26356;&#26032;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#20998;&#26512;&#36229;&#36234;&#20102;&#32447;&#24615;&#20381;&#23384;&#36317;&#31163;&#65292;&#25506;&#35752;&#20102;&#19982;&#20381;&#23384;&#36317;&#31163;&#26368;&#23567;&#21270;&#65288;DDM&#65289;&#30456;&#20851;&#30340;15&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#25110;&#32773;&#22522;&#20110;&#26641;&#22270;&#23646;&#24615;&#65292;&#27604;&#22914;&#26641;&#39640;&#21644;&#24230;&#21464;&#24322;&#12290;&#23613;&#31649;&#25105;&#20204;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#26368;&#36817;&#22522;&#20110;&#29616;&#20195;&#26641;&#24211;&#35757;&#32451;&#30340;&#35299;&#26512;&#22120;&#24182;&#26410;&#21463;&#21040;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11549v1 Announce Type: cross  Abstract: Many studies have shown that human languages tend to optimize for lower complexity and increased communication efficiency. Syntactic dependency distance, which measures the linear distance between dependent words, is often considered a key indicator of language processing difficulty and working memory load. The current paper looks at diachronic trends in syntactic language change in both English and German, using corpora of parliamentary debates from the last c. 160 years. We base our observations on five dependency parsers, including the widely used Stanford CoreNLP as well as 4 newer alternatives. Our analysis of syntactic language change goes beyond linear dependency distance and explores 15 metrics relevant to dependency distance minimization (DDM) and/or based on tree graph properties, such as the tree height and degree variance. Even though we have evidence that recent parsers trained on modern treebanks are not heavily affected 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#26102;&#31354;&#30693;&#35782;&#22270;&#30340;&#38382;&#31572;&#31995;&#32479;STQAD&#65292;&#20197;&#35299;&#20915;&#38382;&#31572;&#31995;&#32479;&#22312;&#28085;&#30422;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#19978;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;STComplEx&#23884;&#20837;&#26041;&#27861;STCQA&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;</title><link>https://arxiv.org/abs/2402.11542</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#30693;&#35782;&#22270;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Question Answering Over Spatio-Temporal Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11542
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#26102;&#31354;&#30693;&#35782;&#22270;&#30340;&#38382;&#31572;&#31995;&#32479;STQAD&#65292;&#20197;&#35299;&#20915;&#38382;&#31572;&#31995;&#32479;&#22312;&#28085;&#30422;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#19978;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;STComplEx&#23884;&#20837;&#26041;&#27861;STCQA&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#30693;&#35782;&#22270;&#65288;STKG&#65289;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#21644;&#20301;&#32622;&#20449;&#24687;&#25193;&#23637;&#20102;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#30340;&#27010;&#24565;&#12290;&#23613;&#31649;&#30740;&#31350;&#30028;&#20851;&#27880;&#30693;&#35782;&#22270;&#38382;&#31572;&#65288;KGQA&#65289;&#65292;&#20294;&#22522;&#20110;STKG&#30340;&#28085;&#30422;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#22238;&#31572;&#39046;&#22495;&#20173;&#26410;&#34987;&#24191;&#27867;&#25506;&#35752;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#20063;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STQAD&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#25324;10,000&#20010;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#38754;&#21521;&#26102;&#31354;&#30693;&#35782;&#22270;&#38382;&#31572;&#65288;STKGQA&#65289;&#25968;&#25454;&#38598;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;KGQA&#26041;&#27861;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#36828;&#26410;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STCQA&#65292;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;KGQA&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;STComplEx&#30340;&#26032;&#22411;STKG&#23884;&#20837;&#26041;&#27861;&#12290;&#36890;&#36807;&#20174;&#38382;&#39064;&#20013;&#25552;&#21462;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#38382;&#31572;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11542v1 Announce Type: cross  Abstract: Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledge graphs (KGs) by incorporating time and location information. While the research community's focus on Knowledge Graph Question Answering (KGQA), the field of answering questions incorporating both spatio-temporal information based on STKGs remains largely unexplored. Furthermore, a lack of comprehensive datasets also has hindered progress in this area. To address this issue, we present STQAD, a dataset comprising 10,000 natural language questions for spatio-temporal knowledge graph question answering (STKGQA). Unfortunately, various state-of-the-art KGQA approaches fall far short of achieving satisfactory performance on our dataset. In response, we propose STCQA, a new spatio-temporal KGQA approach that utilizes a novel STKG embedding method named STComplEx. By extracting temporal and spatial information from a question, our QA model can better comprehend the quest
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;KG&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25506;&#32034;&#20026;LLMs&#25552;&#20379;&#30693;&#35782;&#22270;&#35889;&#30693;&#35782;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11541</link><description>&lt;p&gt;
&#36870;&#21521;&#35748;&#30693;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27604;&#25105;&#20204;&#24819;&#35937;&#30340;&#26356;&#25797;&#38271;&#29702;&#35299;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;KG&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25506;&#32034;&#20026;LLMs&#25552;&#20379;&#30693;&#35782;&#22270;&#35889;&#30693;&#35782;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#24182;&#20943;&#23569;&#23427;&#20204;&#30340;&#24187;&#35273;&#30340;&#26041;&#27861;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30446;&#21069;&#23545;&#22914;&#20309;&#20351;LLMs&#33021;&#22815;&#21363;&#26102;&#25972;&#21512;KGs&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#25506;&#32034;&#36824;&#19981;&#36275;&#12290;&#26412;&#25991;&#37319;&#29992;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#65288;CQA&#65289;&#20316;&#20026;&#19968;&#39033;&#20219;&#21153;&#65292;&#35780;&#20272;LLM&#29702;&#35299;KG&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;KG&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65288;&#20174;&#19977;&#20803;&#32452;&#21040;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#65289;&#65292;&#26088;&#22312;&#25506;&#32034;&#20026;LLMs&#25552;&#20379;KG&#30693;&#35782;&#30340;&#26368;&#20339;&#25552;&#31034;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11541v1 Announce Type: cross  Abstract: Although the method of enhancing large language models' (LLMs') reasoning ability and reducing their hallucinations through the use of knowledge graphs (KGs) has received widespread attention, the exploration of how to enable LLMs to integrate the structured knowledge in KGs on-the-fly remains inadequate. Researchers often co-train KG embeddings and LLM parameters to equip LLMs with the ability of comprehending KG knowledge. However, this resource-hungry training paradigm significantly increases the model learning cost and is also unsuitable for non-open-source, black-box LLMs. In this paper, we employ complex question answering (CQA) as a task to assess the LLM's ability of comprehending KG knowledge. We conducted a comprehensive comparison of KG knowledge injection methods (from triples to natural language text), aiming to explore the optimal prompting method for supplying KG knowledge to LLMs, thereby enhancing their comprehension o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#20116;&#20010;&#20027;&#35201;&#31867;&#21035;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;48&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#8220;&#39640;&#24433;&#21709;&#25968;&#25454;&#8221;&#65292;&#22914;&#20070;&#31821;&#65292;&#19982;&#27169;&#22411;&#33021;&#21147;&#30456;&#20851;&#32852;&#65292;&#20026;LLMs&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.11537</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#21435;&#23398;&#20064;&#30740;&#31350;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11537
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#20116;&#20010;&#20027;&#35201;&#31867;&#21035;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;48&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#8220;&#39640;&#24433;&#21709;&#25968;&#25454;&#8221;&#65292;&#22914;&#20070;&#31821;&#65292;&#19982;&#27169;&#22411;&#33021;&#21147;&#30456;&#20851;&#32852;&#65292;&#20026;LLMs&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20855;&#26377;&#21508;&#31181;&#26469;&#28304;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#32452;&#32455;&#20173;&#28982;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#20559;&#31163;&#26368;&#20339;&#29366;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#26469;&#33258;LLMs&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;5&#20010;&#20027;&#35201;&#31867;&#21035;&#30340;48&#20010;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#20851;&#20110;&#20061;&#20010;&#20027;&#35201;&#27169;&#22411;&#33021;&#21147;&#31867;&#21035;&#30340;&#22522;&#20934;&#26469;&#34913;&#37327;&#23427;&#20204;&#23545;LLMs&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#22810;&#20010;&#35821;&#26009;&#24211;&#23545;LLMs&#24615;&#33021;&#36129;&#29486;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#32852;&#21512;&#24433;&#21709;&#27169;&#24335;&#65292;&#21253;&#25324;&#20114;&#34917;&#30340;&#12289;&#27491;&#20132;&#30340;&#21644;&#30456;&#20851;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#19968;&#32452;&#8220;&#39640;&#24433;&#21709;&#25968;&#25454;&#8221;&#65292;&#22914;&#20070;&#31821;&#65292;&#19982;&#19968;&#32452;&#27169;&#22411;&#33021;&#21147;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#32452;&#32455;&#25968;&#25454;&#20197;&#25903;&#25345;LLMs&#20248;&#21270;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11537v1 Announce Type: cross  Abstract: Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of the pretraining corpus is still empirical and may deviate from the optimal. To address this issue, we systematically analyze the impact of 48 datasets from 5 major categories of pretraining data of LLMs and measure their impacts on LLMs using benchmarks about nine major categories of model capabilities. Our analyses provide empirical results about the contribution of multiple corpora on the performances of LLMs, along with their joint impact patterns, including complementary, orthogonal, and correlational relationships. We also identify a set of ``high-impact data'' such as Books that is significantly related to a set of model capabilities. These findings provide insights into the organization of data to sup
&lt;/p&gt;</description></item><item><title>PreAct&#26159;&#19968;&#20010;&#25972;&#21512;&#20102;&#39044;&#27979;&#12289;&#25512;&#29702;&#21644;&#34892;&#21160;&#30340;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#27979;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#22810;&#26679;&#21270;&#21644;&#31574;&#30053;&#24615;&#30340;&#25512;&#29702;&#65292;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#34892;&#21160;&#65292;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11534</link><description>&lt;p&gt;
PreAct: &#22312; ReAct &#20013;&#39044;&#27979;&#26410;&#26469;&#22686;&#24378;&#26234;&#33021;&#20307;&#30340;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PreAct: Predicting Future in ReAct Enhances Agent's Planning Ability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11534
&lt;/p&gt;
&lt;p&gt;
PreAct&#26159;&#19968;&#20010;&#25972;&#21512;&#20102;&#39044;&#27979;&#12289;&#25512;&#29702;&#21644;&#34892;&#21160;&#30340;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#27979;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#22810;&#26679;&#21270;&#21644;&#31574;&#30053;&#24615;&#30340;&#25512;&#29702;&#65292;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#34892;&#21160;&#65292;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#39044;&#27979;&#19982;&#23454;&#38469;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#24322;&#24120;&#24120;&#26377;&#21161;&#20110;&#20010;&#20307;&#25299;&#23637;&#24605;&#32500;&#36807;&#31243;&#24182;&#36827;&#34892;&#21453;&#24605;&#65292;&#20174;&#32780;&#20419;&#36827;&#26397;&#27491;&#30830;&#26041;&#21521;&#25512;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; PreAct &#30340;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#23427;&#23558;&#39044;&#27979;&#12289;&#25512;&#29702;&#21644;&#34892;&#21160;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#21033;&#29992;&#39044;&#27979;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#25552;&#20379;&#26356;&#22810;&#26679;&#21270;&#21644;&#31574;&#30053;&#23548;&#21521;&#30340;&#25512;&#29702;&#65292;&#36827;&#32780;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#34892;&#21160;&#65292;&#24110;&#21161;&#26234;&#33021;&#20307;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PreAct &#22312;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#20248;&#20110; ReAct &#26041;&#27861;&#65292;&#24182;&#19988;&#24403;&#19982;&#21453;&#24605;&#26041;&#27861;&#32467;&#21512;&#26102;&#65292;PreAct &#30340;&#25928;&#26524;&#21487;&#20197;&#24471;&#21040;&#25552;&#21319;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#25552;&#20379;&#19981;&#21516;&#25968;&#37327;&#30340;&#21382;&#21490;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#21382;&#21490;&#39044;&#27979;&#23545;LLM&#35268;&#21010;&#26377;&#25345;&#32493;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11534v1 Announce Type: cross  Abstract: Addressing the discrepancies between predictions and actual outcomes often aids individuals in expanding their thought processes and engaging in reflection, thereby facilitating reasoning in the correct direction. In this paper, we introduce $\textbf{PreAct}$, an agent framework that integrates $\textbf{pre}$diction with $\textbf{rea}$soning and $\textbf{act}$ion. Leveraging the information provided by predictions, a large language model (LLM) based agent can offer more diversified and strategically oriented reasoning, which in turn leads to more effective actions that help the agent complete complex tasks. Our experiments demonstrate that PreAct outperforms the ReAct approach in accomplishing complex tasks and that PreAct can be co-enhanced when combined with Reflexion methods. We prompt the model with different numbers of historical predictions and find that historical predictions have a sustained positive effect on LLM planning. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#22495;&#22686;&#24378;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38170;&#33410;&#28857;&#30340;&#21327;&#20316;&#37051;&#23621;&#35270;&#20026;&#27491;&#26679;&#26412;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#21327;&#21516;&#36807;&#28388;&#20013;&#25968;&#25454;&#31232;&#30095;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.11523</link><description>&lt;p&gt;
&#22522;&#20110;&#37051;&#22495;&#22686;&#24378;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative Filtering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#22495;&#22686;&#24378;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38170;&#33410;&#28857;&#30340;&#21327;&#20316;&#37051;&#23621;&#35270;&#20026;&#27491;&#26679;&#26412;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#21327;&#21516;&#36807;&#28388;&#20013;&#25968;&#25454;&#31232;&#30095;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#26377;&#25928;&#65292;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#25216;&#26415;&#38754;&#20020;&#30528;&#25968;&#25454;&#31232;&#30095;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#22987;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#24341;&#20837;&#39069;&#22806;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#20250;&#26080;&#24847;&#20013;&#23558;&#30446;&#26631;&#29992;&#25143;/&#39033;&#30446;&#19982;&#20182;&#20204;&#30340;&#21327;&#20316;&#37051;&#23621;&#20998;&#24320;&#65292;&#20174;&#32780;&#38480;&#21046;&#20854;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#38170;&#33410;&#28857;&#30340;&#21327;&#20316;&#37051;&#23621;&#35270;&#20026;&#26368;&#32456;&#30446;&#26631;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#27491;&#26679;&#26412;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#24320;&#21457;&#20004;&#20010;&#29420;&#29305;&#30340;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#26377;&#25928;&#32467;&#21512;&#20102;&#30417;&#30563;&#20449;&#21495;&#21644;&#23545;&#27604;&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#26799;&#24230;&#35270;&#35282;&#20998;&#26512;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#19981;&#21516;&#30340;&#27491;&#26679;&#26412;&#21516;&#26102;&#24433;&#21709;&#26356;&#26032;&#38170;&#33410;&#28857;&#30340;&#23884;&#20837;&#12290;&#36825;&#20123;&#26679;&#26412;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;&#23427;&#20204;&#19982;&#38170;&#33410;&#28857;&#21644;&#36127;&#26679;&#26412;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11523v1 Announce Type: cross  Abstract: While effective in recommendation tasks, collaborative filtering (CF) techniques face the challenge of data sparsity. Researchers have begun leveraging contrastive learning to introduce additional self-supervised signals to address this. However, this approach often unintentionally distances the target user/item from their collaborative neighbors, limiting its efficacy. In response, we propose a solution that treats the collaborative neighbors of the anchor node as positive samples within the final objective loss function. This paper focuses on developing two unique supervised contrastive loss functions that effectively combine supervision signals with contrastive loss. We analyze our proposed loss functions through the gradient lens, demonstrating that different positive samples simultaneously influence updating the anchor node's embeddings. These samples' impact depends on their similarities to the anchor node and the negative sample
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;FlexLoRA&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#32858;&#21512;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20805;&#20998;&#21033;&#29992;&#24322;&#36136;&#23458;&#25143;&#36164;&#28304;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#26412;&#22320;LoRA&#25490;&#21517;&#21644;&#37319;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#36827;&#34892;&#26435;&#37325;&#37325;&#26032;&#20998;&#37197;&#65292;&#25552;&#21319;&#20840;&#23616;&#27169;&#22411;&#30340;&#24191;&#27867;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.11505</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;&#35821;&#35328;&#20219;&#21153;&#21644;&#23458;&#25143;&#36164;&#28304;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32852;&#37030;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11505
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;FlexLoRA&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#32858;&#21512;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20805;&#20998;&#21033;&#29992;&#24322;&#36136;&#23458;&#25143;&#36164;&#28304;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#26412;&#22320;LoRA&#25490;&#21517;&#21644;&#37319;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#36827;&#34892;&#26435;&#37325;&#37325;&#26032;&#20998;&#37197;&#65292;&#25552;&#21319;&#20840;&#23616;&#27169;&#22411;&#30340;&#24191;&#27867;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#34987;&#24212;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#30340;&#36164;&#28304;&#21644;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#65292;&#36825;&#24341;&#21457;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;FlexLoRA&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;LLM&#24494;&#35843;&#32858;&#21512;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#32531;&#35299;&#20256;&#32479;FL&#20013;&#30340;&#8220;&#26742;&#25928;&#24212;&#8221;&#65292;&#35813;&#25928;&#24212;&#38480;&#21046;&#20102;&#25317;&#26377;&#20016;&#23500;&#36164;&#28304;&#30340;&#23458;&#25143;&#23454;&#29616;&#28508;&#21147;&#65292;&#23558;&#20182;&#20204;&#19982;&#26368;&#32570;&#20047;&#36164;&#28304;&#30340;&#21442;&#19982;&#32773;&#30340;&#33021;&#21147;&#25414;&#32465;&#22312;&#19968;&#36215;&#12290;FlexLoRA&#20801;&#35768;&#21160;&#24577;&#35843;&#25972;&#26412;&#22320;LoRA&#25490;&#21517;&#65292;&#20419;&#36827;&#20840;&#23616;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#24182;&#36171;&#20104;&#26356;&#24191;&#27867;&#12289;&#19981;&#22826;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20174;&#20010;&#20307;&#23458;&#25143;&#36129;&#29486;&#20013;&#21512;&#25104;&#23436;&#25972;&#22823;&#23567;&#30340;LoRA&#26435;&#37325;&#65292;&#24182;&#21033;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#36827;&#34892;&#26435;&#37325;&#37325;&#26032;&#20998;&#37197;&#65292;FlexLoRA&#20805;&#20998;&#21033;&#29992;&#20102;&#23458;&#25143;&#38388;&#30340;&#36164;&#28304;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#28041;&#21450;&#36229;&#36807;1600&#20010;&#25191;&#34892;&#22810;&#26679;NLP&#20219;&#21153;&#30340;&#23458;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11505v1 Announce Type: cross  Abstract: Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the "buckets effect" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600 clients performing diverse NLP tasks, ou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#25351;&#20196;&#22320;&#38754;&#21270;&#36816;&#21160;&#35268;&#21010;&#65288;LIMP&#65289;&#31995;&#32479;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;&#26102;&#38388;&#36923;&#36753;&#29983;&#25104;&#25351;&#20196;&#26465;&#20214;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#38271;&#26399;&#30340;&#25351;&#20196;&#65292;&#21253;&#25324;&#24320;&#25918;&#35789;&#27719;&#21442;&#29031;&#21644;&#22797;&#26434;&#30340;&#26102;&#31354;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2402.11498</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#22797;&#26434;&#26426;&#22120;&#20154;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
Verifiably Following Complex Robot Instructions with Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#25351;&#20196;&#22320;&#38754;&#21270;&#36816;&#21160;&#35268;&#21010;&#65288;LIMP&#65289;&#31995;&#32479;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;&#26102;&#38388;&#36923;&#36753;&#29983;&#25104;&#25351;&#20196;&#26465;&#20214;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#38271;&#26399;&#30340;&#25351;&#20196;&#65292;&#21253;&#25324;&#24320;&#25918;&#35789;&#27719;&#21442;&#29031;&#21644;&#22797;&#26434;&#30340;&#26102;&#31354;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35753;&#26426;&#22120;&#20154;&#33021;&#22815;&#36981;&#24490;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20154;&#20204;&#24076;&#26395;&#22312;&#25351;&#23548;&#26426;&#22120;&#20154;&#26102;&#33021;&#22815;&#28789;&#27963;&#34920;&#36798;&#32422;&#26463;&#65292;&#25351;&#21521;&#20219;&#24847;&#22320;&#26631;&#24182;&#39564;&#35777;&#34892;&#20026;&#12290;&#30456;&#21453;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#23558;&#20154;&#31867;&#25351;&#20196;&#28040;&#38500;&#27495;&#20041;&#65292;&#23558;&#25351;&#20196;&#21442;&#29031;&#29289;&#32852;&#31995;&#21040;&#30495;&#23454;&#19990;&#30028;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#25351;&#20196;&#22320;&#38754;&#21270;&#36816;&#21160;&#35268;&#21010;&#65288;LIMP&#65289;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;&#26102;&#38388;&#36923;&#36753;&#29983;&#25104;&#25351;&#20196;&#26465;&#20214;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#38271;&#26399;&#30340;&#25351;&#20196;&#65292;&#28085;&#30422;&#20102;&#24320;&#25918;&#35789;&#27719;&#21442;&#29031;&#21644;&#22797;&#26434;&#30340;&#26102;&#31354;&#32422;&#26463;&#12290;&#19982;&#20808;&#21069;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#25191;&#34892;&#20013;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;LIMP&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25351;&#20196;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#20154;&#19982;&#25351;&#23548;&#32773;&#39044;&#26399;&#21160;&#26426;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11498v1 Announce Type: cross  Abstract: Enabling robots to follow complex natural language instructions is an important yet challenging problem. People want to flexibly express constraints, refer to arbitrary landmarks and verify behavior when instructing robots. Conversely, robots must disambiguate human instructions into specifications and ground instruction referents in the real world. We propose Language Instruction grounding for Motion Planning (LIMP), a system that leverages foundation models and temporal logics to generate instruction-conditioned semantic maps that enable robots to verifiably follow expressive and long-horizon instructions with open vocabulary referents and complex spatiotemporal constraints. In contrast to prior methods for using foundation models in robot task execution, LIMP constructs an explainable instruction representation that reveals the robot's alignment with an instructor's intended motives and affords the synthesis of robot behaviors that 
&lt;/p&gt;</description></item><item><title>LEIA&#26159;&#19968;&#31181;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#36328;&#35821;&#35328;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11485</link><description>&lt;p&gt;
LEIA: &#21033;&#29992;&#22522;&#20110;&#23454;&#20307;&#30340;&#25968;&#25454;&#22686;&#24378;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#20419;&#36827;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11485
&lt;/p&gt;
&lt;p&gt;
LEIA&#26159;&#19968;&#31181;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#36328;&#35821;&#35328;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#20110;&#33521;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#20854;&#20182;&#35821;&#35328;&#30340;&#25805;&#20316;&#30001;&#20110;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#25928;&#29575;&#21644;&#28508;&#21147;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#36866;&#24212;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#36328;&#35821;&#35328;&#30417;&#30563;&#30340;&#22909;&#22788;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;LEIA&#65292;&#19968;&#31181;&#21033;&#29992;&#36328;&#35821;&#35328;&#23545;&#40784;&#30340;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#30340;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#33521;&#35821;&#23454;&#20307;&#21517;&#31216;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#20174;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;LEIA&#65292;&#20351;&#29992;7B&#21442;&#25968;&#30340;LLMs&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#22686;&#30410;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/studio-ousia/leia&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11485v1 Announce Type: cross  Abstract: Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages. The source code is available at https://github.com/studio-ousia/leia.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11472</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#65306;DDIPrompt
&lt;/p&gt;
&lt;p&gt;
DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11472
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#24314;&#27169;&#33647;&#29289;&#20998;&#23376;&#20869;&#37096;&#21644;&#20043;&#38388;&#21407;&#23376;&#21644;&#21151;&#33021;&#22242;&#20043;&#38388;&#22797;&#26434;&#20851;&#32852;&#26041;&#38754;&#30340;&#29087;&#32451;&#34920;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#65288;DDI&#65289;&#26041;&#38754;&#21464;&#24471;&#26085;&#30410;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#30340;&#21046;&#32422;&#65306;&#65288;1&#65289;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#20294;&#20851;&#38190;&#30340;&#38382;&#39064;&#65292;&#26576;&#20123;&#30456;&#20114;&#20316;&#29992;&#34987;&#24191;&#27867;&#22320;&#20302;&#20272;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#23545;&#23454;&#29616;&#20934;&#30830;&#21487;&#38752;&#30340;DDI&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#65288;2&#65289;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#26159;&#19968;&#20010;&#26222;&#36941;&#38382;&#39064;&#65292;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#65292;&#24448;&#24448;&#24573;&#35270;&#25110;&#30740;&#31350;&#19981;&#36275;&#30340;&#32597;&#35265;&#20294;&#28508;&#22312;&#20851;&#38190;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DDIPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#26368;&#36817;&#22270;&#25552;&#31034;&#23398;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#33391;&#26041;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11472v1 Announce Type: cross  Abstract: Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issue
&lt;/p&gt;</description></item><item><title>Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11463</link><description>&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21560;&#24341;&#23376;&#35760;&#24518;&#65306;&#28151;&#27788;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11463
&lt;/p&gt;
&lt;p&gt;
Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24573;&#35270;&#20102;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#28304;&#33258;&#28508;&#22312;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#23548;&#33268;&#32570;&#20047;&#22806;&#25512;&#21644;&#28436;&#21270;&#33021;&#21147;&#12290; &#37492;&#21035;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#28151;&#27788;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;\textbf{\textit{Attraos}}&#23558;&#28151;&#27788;&#29702;&#35770;&#34701;&#20837;&#21040;LTSF&#20013;&#65292;&#23558;&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#26410;&#30693;&#39640;&#32500;&#28151;&#27788;&#21160;&#24577;&#31995;&#32479;&#30340;&#35266;&#27979;&#12290; &#22312;&#21560;&#24341;&#23376;&#19981;&#21464;&#24615;&#30340;&#27010;&#24565;&#19979;&#65292;Attraos&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#26469;&#35760;&#24518;&#21382;&#21490;&#21160;&#24577;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#22686;&#24378;&#30340;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#36827;&#34892;&#39044;&#27979;&#12290; &#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#20016;&#23500;&#30340;&#32463;&#39564;&#35777;&#25454;&#19968;&#33268;&#34920;&#26126;&#65292;Attraos&#22312;&#20027;&#27969;LTSF&#25968;&#25454;&#38598;&#21644;&#28151;&#27788;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#21508;&#31181;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11463v1 Announce Type: cross  Abstract: In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FGeo-HyperGNet&#65292;&#23558;&#24418;&#24335;&#31526;&#21495;&#31995;&#32479;&#21644;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#65292;&#23454;&#29616;&#33258;&#21160;&#25191;&#34892;&#20154;&#31867;&#21270;&#30340;&#20960;&#20309;&#28436;&#32462;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.11461</link><description>&lt;p&gt;
FGeo-HyperGNet: &#20960;&#20309;&#38382;&#39064;&#27714;&#35299;&#20013;&#38598;&#25104;&#24418;&#24335;&#31526;&#21495;&#31995;&#32479;&#21644;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FGeo-HyperGNet: Geometry Problem Solving Integrating Formal Symbolic System and Hypergraph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11461
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FGeo-HyperGNet&#65292;&#23558;&#24418;&#24335;&#31526;&#21495;&#31995;&#32479;&#21644;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#65292;&#23454;&#29616;&#33258;&#21160;&#25191;&#34892;&#20154;&#31867;&#21270;&#30340;&#20960;&#20309;&#28436;&#32462;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#38382;&#39064;&#30340;&#27714;&#35299;&#19968;&#30452;&#26159;&#33258;&#21160;&#25512;&#29702;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#36825;&#26159;&#25105;&#20204;&#31995;&#21015;&#20316;&#21697;&#20013;&#30340;&#31532;&#20116;&#31687;&#25991;&#31456;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#25191;&#34892;&#31867;&#20284;&#20154;&#31867;&#30340;&#20960;&#20309;&#28436;&#32462;&#25512;&#29702;&#12290;&#31526;&#21495;&#37096;&#20998;&#26159;&#24314;&#31435;&#22312;FormalGeo&#19978;&#30340;&#24418;&#24335;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#25191;&#34892;&#20960;&#20309;&#20851;&#31995;&#25512;&#29702;&#21644;&#20195;&#25968;&#35745;&#31639;&#65292;&#24182;&#23558;&#27714;&#35299;&#36807;&#31243;&#32452;&#32455;&#25104;&#19968;&#20010;&#24102;&#26377;&#26465;&#20214;&#20316;&#20026;&#36229;&#33410;&#28857;&#21644;&#23450;&#29702;&#20316;&#20026;&#36229;&#36793;&#30340;&#35299;&#20915;&#26041;&#26696;&#36229;&#26641;&#12290;&#31070;&#32463;&#37096;&#20998;&#31216;&#20026;HyperGNet&#65292;&#26159;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#19968;&#20010;&#32534;&#30721;&#22120;&#26469;&#26377;&#25928;&#32534;&#30721;&#36229;&#26641;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#27714;&#35299;&#22120;&#26469;&#25552;&#20379;&#38382;&#39064;&#27714;&#35299;&#25351;&#23548;&#12290;&#31070;&#32463;&#37096;&#20998;&#26681;&#25454;&#36229;&#26641;&#39044;&#27979;&#23450;&#29702;&#65292;&#32780;&#31526;&#21495;&#37096;&#20998;&#24212;&#29992;&#23450;&#29702;&#24182;&#26356;&#26032;&#36229;&#26641;&#65292;&#20174;&#32780;&#24418;&#25104;&#19968;&#20010;&#39044;&#27979;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11461v1 Announce Type: new  Abstract: Geometry problem solving has always been a long-standing challenge in the fields of automated reasoning and artificial intelligence. This is the fifth article in a series of our works, we built a neural-symbolic system to automatically perform human-like geometric deductive reasoning. The symbolic part is a formal system built on FormalGeo, which can automatically perform geomertic relational reasoning and algebraic calculations and organize the solving process into a solution hypertree with conditions as hypernodes and theorems as hyperedges. The neural part, called HyperGNet, is a hypergraph neural network based on the attention mechanism, including a encoder to effectively encode the structural and semantic information of the hypertree, and a solver to provide problem-solving guidance. The neural part predicts theorems according to the hypertree, and the symbolic part applies theorems and updates the hypertree, thus forming a Predict-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26725;&#29983;&#25104;&#27169;&#22411; Re-Dock&#65292;&#29992;&#20110;&#28789;&#27963;&#21644;&#29616;&#23454;&#30340;&#20998;&#23376;&#23545;&#25509;&#65292;&#36890;&#36807;&#33021;&#37327;&#21040;&#20960;&#20309;&#26144;&#23556;&#26469;&#20849;&#21516;&#24314;&#27169;&#32467;&#21512;&#33021;&#21644;&#26500;&#35937;&#65292;&#22635;&#34917;&#20102;&#23545;&#25509;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#26500;&#35937;&#39044;&#27979;&#26041;&#38754;&#30340;&#24046;&#36317;</title><link>https://arxiv.org/abs/2402.11459</link><description>&lt;p&gt;
Re-Dock: &#26397;&#21521;&#20855;&#26377;&#25193;&#25955;&#26725;&#30340;&#28789;&#27963;&#21644;&#29616;&#23454;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11459
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26725;&#29983;&#25104;&#27169;&#22411; Re-Dock&#65292;&#29992;&#20110;&#28789;&#27963;&#21644;&#29616;&#23454;&#30340;&#20998;&#23376;&#23545;&#25509;&#65292;&#36890;&#36807;&#33021;&#37327;&#21040;&#20960;&#20309;&#26144;&#23556;&#26469;&#20849;&#21516;&#24314;&#27169;&#32467;&#21512;&#33021;&#21644;&#26500;&#35937;&#65292;&#22635;&#34917;&#20102;&#23545;&#25509;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#26500;&#35937;&#39044;&#27979;&#26041;&#38754;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#32467;&#26500;&#65292;&#21363;&#20998;&#23376;&#23545;&#25509;&#20219;&#21153;&#23545;&#20110;&#33647;&#29289;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23436;&#25972;&#34507;&#30333;&#36136;&#32467;&#26500;&#65288;&#23545;&#25509;&#65292;&#19988;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#19981;&#21487;&#36798;&#65289;&#25110;&#24573;&#30053;&#21475;&#34955;&#20391;&#38142;&#26500;&#35937;&#65292;&#23548;&#33268;&#26377;&#38480;&#30340;&#23454;&#29992;&#24615;&#21644;&#19981;&#20999;&#23454;&#38469;&#30340;&#26500;&#35937;&#39044;&#27979;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26410;&#32463;&#25506;&#32034;&#30340;&#20219;&#21153;&#65292;&#21629;&#21517;&#20026;&#26580;&#24615;&#23545;&#25509;&#65292;&#20197;&#21516;&#26102;&#39044;&#27979;&#37197;&#20307;&#21644;&#21475;&#34955;&#20391;&#38142;&#30340;&#23039;&#21183;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25193;&#23637;&#21040;&#20960;&#20309;&#27969;&#24418;&#30340;&#26032;&#22411;&#25193;&#25955;&#26725;&#29983;&#25104;&#27169;&#22411; Re-Dock&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21463;&#29275;&#39039;-&#27431;&#25289;&#26041;&#31243;&#21551;&#21457;&#30340;&#33021;&#37327;&#21040;&#20960;&#20309;&#26144;&#23556;&#65292;&#20197;&#20849;&#21516;&#24314;&#27169;&#32467;&#21512;&#33021;&#21644;&#26500;&#35937;&#65292;&#20197;&#21453;&#26144;&#33021;&#37327;&#32422;&#26463;&#23545;&#25509;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#35774;&#35745;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;apo-dock&#21644;cross-dock d
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11459v1 Announce Type: cross  Abstract: Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock d
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#24037;&#20855;&#22686;&#24378;&#22411;&#31185;&#23398;&#25512;&#29702;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#36890;&#36807;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#38598;&#65292;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#21464;&#24471;&#26356;&#21152;&#23454;&#29992;&#21644;&#21487;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.11451</link><description>&lt;p&gt;
SciAgent: &#24037;&#20855;&#22686;&#24378;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#31185;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SciAgent: Tool-augmented Language Models for Scientific Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11451
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#24037;&#20855;&#22686;&#24378;&#22411;&#31185;&#23398;&#25512;&#29702;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#36890;&#36807;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#38598;&#65292;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#21464;&#24471;&#26356;&#21152;&#23454;&#29992;&#21644;&#21487;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25512;&#29702;&#23545;&#20110;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35828;&#20063;&#26159;&#19968;&#39033;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;LLMs&#26356;&#21152;&#23454;&#29992;&#21644;&#21487;&#35299;&#20915;&#27492;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#24037;&#20855;&#22686;&#24378;&#22411;&#31185;&#23398;&#25512;&#29702;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#12290;&#36825;&#31181;&#35774;&#32622;&#36890;&#36807;&#20026;LLMs&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#38598;&#65292;&#23558;&#37325;&#28857;&#20174;&#36861;&#27714;&#20840;&#30693;&#38382;&#39064;&#27714;&#35299;&#22120;&#36716;&#21464;&#20026;&#29087;&#32451;&#20351;&#29992;&#24037;&#20855;&#30340;&#20154;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#31181;&#35774;&#32622;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MathFunc&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;30,000&#20010;&#26679;&#26412;&#21644;&#22823;&#32422;6,000&#20010;&#24037;&#20855;&#12290;&#22522;&#20110;MathFunc&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SciAgent&#65292;&#29992;&#20110;&#26816;&#32034;&#12289;&#29702;&#35299;&#65292;&#20197;&#21450;&#24517;&#35201;&#26102;&#20351;&#29992;&#24037;&#20855;&#36827;&#34892;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SciToolBench&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20116;&#20010;&#31185;&#23398;&#39046;&#22495;&#65292;&#20197;&#35780;&#20272;LLMs&#22312;&#24037;&#20855;&#36741;&#21161;&#19979;&#30340;&#33021;&#21147;&#12290;&#23545;SciToolBench&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;SciAgent&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;SciAgent-Mistral-7B&#36229;&#36807;&#20102;&#20854;&#20182;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11451v1 Announce Type: cross  Abstract: Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs' abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the sa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;358&#21517;&#32654;&#22269;&#21442;&#19982;&#32773;&#65292;&#21457;&#29616;&#31038;&#20250;&#24433;&#21709;&#12289;&#24615;&#33021;&#26399;&#26395;&#12289;&#20139;&#21463;&#21160;&#26426;&#12289;&#20415;&#21033;&#26465;&#20214;&#21644;&#21162;&#21147;&#26399;&#26395;&#26159;&#24433;&#21709;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25509;&#21463;&#24230;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.11444</link><description>&lt;p&gt;
&#22312;&#32654;&#22269;&#35780;&#20272;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#20844;&#20247;&#25509;&#21463;&#24230;
&lt;/p&gt;
&lt;p&gt;
Gauging Public Acceptance of Conditionally Automated Cars in the United States
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;358&#21517;&#32654;&#22269;&#21442;&#19982;&#32773;&#65292;&#21457;&#29616;&#31038;&#20250;&#24433;&#21709;&#12289;&#24615;&#33021;&#26399;&#26395;&#12289;&#20139;&#21463;&#21160;&#26426;&#12289;&#20415;&#21033;&#26465;&#20214;&#21644;&#21162;&#21147;&#26399;&#26395;&#26159;&#24433;&#21709;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25509;&#21463;&#24230;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26234;&#33021;&#22478;&#24066;&#30340;&#19968;&#20010;&#20803;&#32032;&#65292;&#21363;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;SAE Level 3&#65289;&#65292;&#30740;&#31350;&#20102;&#24433;&#21709;&#32654;&#22269;&#20844;&#20247;&#25509;&#21463;&#24230;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;UTUAT2&#27169;&#22411;&#30340;&#25913;&#32534;&#29256;&#12290;&#36890;&#36807;&#23454;&#39564;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;358&#21517;&#32654;&#22269;&#21442;&#19982;&#32773;&#65292;&#21521;&#20182;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#27010;&#36848;L3&#25216;&#26415;&#30340;&#30701;&#31687;&#25925;&#20107;&#65292;&#28982;&#21518;&#25552;&#20986;&#19968;&#31995;&#21015;&#38382;&#39064;&#65292;&#20197;&#25429;&#25417;&#20182;&#20204;&#23545;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#24863;&#30693;&#12290;&#37319;&#29992;PLS-SEM&#23545;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25216;&#26415;&#30340;&#25509;&#21463;&#24230;&#65292;&#25353;&#37325;&#35201;&#24615;&#36882;&#20943;&#30340;&#39034;&#24207;&#65292;&#21463;&#31038;&#20250;&#24433;&#21709;&#12289;&#24615;&#33021;&#26399;&#26395;&#12289;&#20139;&#21463;&#21160;&#26426;&#12289;&#20415;&#21033;&#26465;&#20214;&#21644;&#21162;&#21147;&#26399;&#26395;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20139;&#21463;&#21160;&#26426;&#12289;&#31038;&#20250;&#24433;&#21709;&#12289;&#20415;&#21033;&#26465;&#20214;&#21644;&#21162;&#21147;&#26399;&#26395;&#37117;&#23545;&#25216;&#26415;&#30340;&#23454;&#29992;&#24615;&#24863;&#30693;&#26377;&#31215;&#26497;&#24433;&#21709;&#65307;&#20415;&#21033;&#26465;&#20214;&#12289;&#20139;&#21463;&#21160;&#26426;&#21644;&#31038;&#20250;&#24433;&#21709;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11444v1 Announce Type: cross  Abstract: In this work we look at an element of smart cities, conditionally automated cars (SAE Level 3), investigating the factors influencing public acceptance in the United States. We apply an adaptation of the UTUAT2 model. Taking an experimental approach study 358 participants in the US were presented with a vignette outlining the L3 technology followed by a series of questions to capture their perceptions of conditionally automated cars. PLS-SEM was used to analyze the collected data. The results reveal that the acceptance of the technology, in order of decreasing importance, was determined by social influence, performance expectancy, hedonic motivation, facilitating conditions, and effort expectancy. Furthermore, hedonic motivation, social influence, facilitating conditions and effort expectancy all have a positive influence on the perception of how useful the technology is; facilitating conditions, hedonic motivation, and social influenc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#32531;&#35299;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11441</link><description>&lt;p&gt;
InfuserKI&#65306;&#36890;&#36807;Infuser&#24341;&#23548;&#30340;&#30693;&#35782;&#38598;&#25104;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11441
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#32531;&#35299;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#30693;&#35782;&#38598;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#27169;&#22359;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#22270;&#35889;&#19982;LLMs&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#25968;&#25454;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#30693;&#35782;&#26469;&#36827;&#34892;&#24494;&#35843;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#19981;&#37325;&#22797;&#24050;&#30693;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;LLMs&#20013;&#12290;&#27880;&#20837;&#26032;&#30693;&#35782;&#20250;&#23548;&#33268;&#36951;&#24536;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26469;&#30830;&#23450;&#26159;&#21542;&#24212;&#35813;&#22686;&#24378;&#21407;&#22987;LLM&#36755;&#20986;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#36731;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#22312;UMLS-2.5k&#21644;MetaQ&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11441v1 Announce Type: cross  Abstract: Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks. To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domain-specific knowledge graphs using external modules. However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning. Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge. Injecting new knowledge poses the risk of forgetting previously acquired knowledge. To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting. Evaluations on the UMLS-2.5k and MetaQ
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#33258;&#25105;&#20559;&#35265;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#20934;&#30830;&#35780;&#20272;&#30340;&#22806;&#37096;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#23454;&#38469;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11436</link><description>&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#30340;&#21361;&#38505;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#25105;&#20559;&#35265;&#34987;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11436
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#33258;&#25105;&#20559;&#35265;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#20934;&#30830;&#35780;&#20272;&#30340;&#22806;&#37096;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#23454;&#38469;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#25105;&#21453;&#39304;&#21487;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#21364;&#20250;&#24694;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#30683;&#30462;&#26159;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#33258;&#36523;&#36755;&#20986;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#27491;&#24335;&#23450;&#20041;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#20559;&#35265;&#8212;&#8212;&#20542;&#21521;&#20110;&#20559;&#29233;&#33258;&#36523;&#29983;&#25104;&#8212;&#8212;&#24182;&#20351;&#29992;&#20004;&#20010;&#32479;&#35745;&#37327;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#32763;&#35793;&#12289;&#21463;&#38480;&#25991;&#26412;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#19978;&#20998;&#26512;&#20102;&#20845;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#20559;&#35265;&#22312;&#25152;&#26377;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37117;&#26222;&#36941;&#23384;&#22312;&#65292;&#36328;&#22810;&#31181;&#35821;&#35328;&#21644;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#34429;&#28982;&#33258;&#25105;&#25913;&#36827;&#31649;&#36947;&#25552;&#39640;&#20102;&#27169;&#22411;&#36755;&#20986;&#30340;&#27969;&#30021;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#65292;&#20294;&#23427;&#36827;&#19968;&#27493;&#25918;&#22823;&#20102;&#33258;&#25105;&#20559;&#35265;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#25105;&#20204;&#21457;&#29616;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#20855;&#26377;&#20934;&#30830;&#35780;&#20272;&#30340;&#22806;&#37096;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#33258;&#25105;&#25913;&#36827;&#31649;&#36947;&#20013;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#38469;&#25913;&#21892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11436v1 Announce Type: cross  Abstract: Recent studies show that self-feedback improves large language models (LLMs) on certain tasks while worsens other tasks. We discovered that such a contrary is due to LLM's bias towards their own output. In this paper, we formally define LLM's self-bias -- the tendency to favor its own generation -- using two statistics. We analyze six LLMs on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.
&lt;/p&gt;</description></item><item><title>OptEx&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#35745;&#31639;&#26469;&#20943;&#36731;&#19968;&#38454;&#20248;&#21270;&#30340;&#36845;&#20195;&#29942;&#39048;&#24182;&#22686;&#24378;&#25928;&#29575;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26680;&#21270;&#26799;&#24230;&#20272;&#35745;&#23454;&#29616;&#36845;&#20195;&#30340;&#24182;&#34892;&#21270;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.11427</link><description>&lt;p&gt;
OptEx: &#21033;&#29992;&#36817;&#20284;&#24182;&#34892;&#21270;&#36845;&#20195;&#21152;&#36895;&#19968;&#38454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11427
&lt;/p&gt;
&lt;p&gt;
OptEx&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#35745;&#31639;&#26469;&#20943;&#36731;&#19968;&#38454;&#20248;&#21270;&#30340;&#36845;&#20195;&#29942;&#39048;&#24182;&#22686;&#24378;&#25928;&#29575;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26680;&#21270;&#26799;&#24230;&#20272;&#35745;&#23454;&#29616;&#36845;&#20195;&#30340;&#24182;&#34892;&#21270;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#38454;&#20248;&#21270;&#65288;FOO&#65289;&#31639;&#27861;&#22312;&#35832;&#22914;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#21435;&#22122;&#31561;&#20247;&#22810;&#35745;&#31639;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31561;&#22797;&#26434;&#20219;&#21153;&#24448;&#24448;&#23548;&#33268;&#26174;&#33879;&#30340;&#20302;&#25928;&#65292;&#22240;&#20026;&#38656;&#35201;&#35768;&#22810;&#39034;&#24207;&#36845;&#20195;&#20197;&#23454;&#29616;&#25910;&#25947;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#38454;&#20248;&#21270;&#21152;&#36895;&#36817;&#20284;&#24182;&#34892;&#36845;&#20195;&#65288;OptEx&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#35745;&#31639;&#26469;&#20943;&#36731;&#20854;&#36845;&#20195;&#29942;&#39048;&#32780;&#22686;&#24378;FOO&#25928;&#29575;&#30340;&#26694;&#26550;&#12290;OptEx&#37319;&#29992;&#26680;&#21270;&#26799;&#24230;&#20272;&#35745;&#26469;&#21033;&#29992;&#26799;&#24230;&#21382;&#21490;&#36827;&#34892;&#26410;&#26469;&#26799;&#24230;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#36845;&#20195;&#30340;&#24182;&#34892;&#21270; -- &#36825;&#26159;&#19968;&#31181;&#26366;&#32463;&#34987;&#35748;&#20026;&#30001;&#20110;FOO&#20013;&#22266;&#26377;&#30340;&#36845;&#20195;&#20381;&#36182;&#32780;&#19981;&#20999;&#23454;&#38469;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#26680;&#21270;&#26799;&#24230;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#21644;&#22522;&#20110;SGD&#30340;OptEx&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#30830;&#35748;&#20102;&#20854;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11427v1 Announce Type: cross  Abstract: First-order optimization (FOO) algorithms are pivotal in numerous computational domains such as machine learning and signal denoising. However, their application to complex tasks like neural network training often entails significant inefficiencies due to the need for many sequential iterations for convergence. In response, we introduce first-order optimization expedited with approximately parallelized iterations (OptEx), the first framework that enhances the efficiency of FOO by leveraging parallel computing to mitigate its iterative bottleneck. OptEx employs kernelized gradient estimation to make use of gradient history for future gradient prediction, enabling parallelization of iterations -- a strategy once considered impractical because of the inherent iterative dependency in FOO. We provide theoretical guarantees for the reliability of our kernelized gradient estimation and the iteration complexity of SGD-based OptEx, confirming t
&lt;/p&gt;</description></item><item><title>&#22312;&#24191;&#20041;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#35299;&#20915;&#24050;&#35265;&#25968;&#25454;&#20559;&#35265;&#38382;&#39064;&#30340;D$^3$GZSL&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#20869;&#21452;&#31354;&#38388;&#31934;&#39311;&#21644;&#20998;&#24067;&#22806;&#25209;&#27425;&#31934;&#39311;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#26356;&#24179;&#34913;&#30340;&#27169;&#22411;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.11424</link><description>&lt;p&gt;
&#29992;&#20110;&#24191;&#20041;&#38646;&#26679;&#26412;&#35782;&#21035;&#30340;&#25968;&#25454;&#20998;&#24067;&#31934;&#39311;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11424
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#20041;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#35299;&#20915;&#24050;&#35265;&#25968;&#25454;&#20559;&#35265;&#38382;&#39064;&#30340;D$^3$GZSL&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#20869;&#21452;&#31354;&#38388;&#31934;&#39311;&#21644;&#20998;&#24067;&#22806;&#25209;&#27425;&#31934;&#39311;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#26356;&#24179;&#34913;&#30340;&#27169;&#22411;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20559;&#21521;&#24050;&#35265;&#25968;&#25454;&#30340;&#24191;&#20041;&#38646;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#29983;&#25104;&#24335;&#24191;&#20041;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;D$^3$GZSL&#12290;&#35813;&#26694;&#26550;&#23558;&#24050;&#35265;&#21644;&#21512;&#25104;&#30340;&#26410;&#35265;&#25968;&#25454;&#20998;&#21035;&#35270;&#20026;&#20998;&#24067;&#20869;&#21644;&#20998;&#24067;&#22806;&#25968;&#25454;&#65292;&#20197;&#33719;&#24471;&#26356;&#21152;&#24179;&#34913;&#30340;&#27169;&#22411;&#12290;D$^3$GZSL&#21253;&#25324;&#20004;&#20010;&#26680;&#24515;&#27169;&#22359;&#65306;&#20998;&#24067;&#20869;&#21452;&#31354;&#38388;&#31934;&#39311;&#65288;ID$^2$SD&#65289;&#21644;&#20998;&#24067;&#22806;&#25209;&#27425;&#31934;&#39311;&#65288;O$^2$DBD&#65289;&#12290;ID$^2$SD&#22312;&#23884;&#20837;&#21644;&#26631;&#31614;&#31354;&#38388;&#20013;&#23545;&#24072;&#29983;&#36755;&#20986;&#36827;&#34892;&#23545;&#40784;&#65292;&#22686;&#24378;&#20102;&#23398;&#20064;&#30340;&#19968;&#33268;&#24615;&#12290;O$^2$DBD&#24341;&#20837;&#20102;&#27599;&#20010;&#25209;&#27425;&#26679;&#26412;&#30340;&#20302;&#32500;&#20998;&#24067;&#22806;&#34920;&#31034;&#65292;&#25429;&#25417;&#20102;&#24050;&#35265;&#21644;&#26410;&#35265;&#31867;&#21035;&#20043;&#38388;&#30340;&#20849;&#20139;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24050;&#24314;&#31435;&#30340;&#24191;&#20041;&#38646;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21040;&#20027;&#27969;&#29983;&#25104;&#24335;&#26694;&#26550;&#20013;&#12290;&#22823;&#37327;&#23454;&#39564;&#19968;&#33268;&#22320;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11424v1 Announce Type: cross  Abstract: In the realm of Zero-Shot Learning (ZSL), we address biases in Generalized Zero-Shot Learning (GZSL) models, which favor seen data. To counter this, we introduce an end-to-end generative GZSL framework called D$^3$GZSL. This framework respects seen and synthesized unseen data as in-distribution and out-of-distribution data, respectively, for a more balanced model. D$^3$GZSL comprises two core modules: in-distribution dual space distillation (ID$^2$SD) and out-of-distribution batch distillation (O$^2$DBD). ID$^2$SD aligns teacher-student outcomes in embedding and label spaces, enhancing learning coherence. O$^2$DBD introduces low-dimensional out-of-distribution representations per batch sample, capturing shared structures between seen and unseen categories. Our approach demonstrates its effectiveness across established GZSL benchmarks, seamlessly integrating into mainstream generative frameworks. Extensive experiments consistently showc
&lt;/p&gt;</description></item><item><title>LoRETTA&#26159;&#19968;&#20010;&#36890;&#36807;&#24352;&#37327;&#35757;&#32451;&#20998;&#35299;&#26174;&#33879;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#36229;&#20302;&#21442;&#25968;&#39640;&#25928;&#26694;&#26550;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#34920;&#29616;&#20986;&#19982;&#22823;&#22810;&#25968;PEFT&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11417</link><description>&lt;p&gt;
LoRETTA: &#20302;&#31209;&#32463;&#27982;&#24352;&#37327;&#35757;&#32451;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36229;&#20302;&#21442;&#25968;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11417
&lt;/p&gt;
&lt;p&gt;
LoRETTA&#26159;&#19968;&#20010;&#36890;&#36807;&#24352;&#37327;&#35757;&#32451;&#20998;&#35299;&#26174;&#33879;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#36229;&#20302;&#21442;&#25968;&#39640;&#25928;&#26694;&#26550;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#34920;&#29616;&#20986;&#19982;&#22823;&#22810;&#25968;PEFT&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#20197;&#23454;&#29616;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#37096;&#32626;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#20173;&#28982;&#21463;&#21040;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#22686;&#38271;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoRETTA&#65292;&#36825;&#26159;&#19968;&#20010;&#36229;&#21442;&#25968;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24352;&#37327;&#35757;&#32451;&#20998;&#35299;&#26174;&#33879;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#20998;&#21035;&#21629;&#21517;&#20026;{LoRETTA}$_{adp}$&#21644;{LoRETTA}$_{rep}$&#12290;&#21069;&#32773;&#37319;&#29992;&#24352;&#37327;&#21270;&#36866;&#37197;&#22120;&#65292;&#20026;LLMs&#30340;&#24494;&#35843;&#25552;&#20379;&#20102;&#39640;&#24615;&#33021;&#19988;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#12290;&#21518;&#32773;&#24378;&#35843;&#36890;&#36807;&#19968;&#32452;&#23567;&#24352;&#37327;&#22240;&#23376;&#36827;&#34892;&#26435;&#37325;&#21442;&#25968;&#21270;&#30340;&#24494;&#35843;&#12290;LoRETTA&#22312;LLaMA-2-7B&#27169;&#22411;&#19978;&#27604;&#22823;&#22810;&#25968;&#24191;&#27867;&#20351;&#29992;&#30340;PEFT&#26041;&#27861;&#20855;&#26377;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21442;&#25968;&#23569;&#36798;&#21040;100&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11417v1 Announce Type: cross  Abstract: Various parameter-efficient fine-tuning (PEFT) techniques have been proposed to enable computationally efficient fine-tuning while maintaining model performance. However, existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of Large Language Models (LLMs). To address this challenge, we present LoRETTA, an ultra-parameter-efficient framework that significantly reduces trainable parameters through tensor-train decomposition. Specifically, we propose two methods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs tensorized adapters, offering a high-performance yet lightweight approach for the fine-tuning of LLMs. The latter emphasizes fine-tuning via weight parameterization with a set of small tensor factors. LoRETTA achieves comparable or better performance than most widely used PEFT methods with up to $100\times$ fewer parameters on the LLaMA-2-7B models. Further
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#31070;&#32463;&#21644;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#22797;&#26434;&#20107;&#20214;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#20851;&#27880;&#26102;&#38388;&#25512;&#29702;&#65292;&#23454;&#39564;&#21457;&#29616;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#22312;&#36739;&#23569;&#25968;&#25454;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.11403</link><description>&lt;p&gt;
&#23545;&#31070;&#32463;&#21644;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#22312;&#23454;&#26102;&#22810;&#27169;&#24577;&#22797;&#26434;&#20107;&#20214;&#26816;&#27979;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of Neural and Neuro-symbolic Approaches to Real-time Multimodal Complex Event Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#31070;&#32463;&#21644;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#22797;&#26434;&#20107;&#20214;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#20851;&#27880;&#26102;&#38388;&#25512;&#29702;&#65292;&#23454;&#39564;&#21457;&#29616;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#22312;&#36739;&#23569;&#25968;&#25454;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Robots and autonomous systems require an understanding of complex events (CEs) from sensor data to interact with their environments and humans effectively. Traditional end-to-end neural architectures, despite processing sensor data efficiently, struggle with long-duration events due to limited context sizes and reasoning capabilities. Recent advances in neuro-symbolic methods, which integrate neural and symbolic models leveraging human knowledge, promise improved performance with less data. This study addresses the gap in understanding these approaches' effectiveness in complex event detection (CED), especially in temporal reasoning. We investigate neural and neuro-symbolic architectures' performance in a multimodal CED task, analyzing IMU and acoustic data streams to recognize CE patterns. Our methodology includes (i) end-to-end neural architectures for direct CE detection from sensor embeddings, (ii) two-stage concept-based neural mode
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11403v1 Announce Type: new  Abstract: Robots and autonomous systems require an understanding of complex events (CEs) from sensor data to interact with their environments and humans effectively. Traditional end-to-end neural architectures, despite processing sensor data efficiently, struggle with long-duration events due to limited context sizes and reasoning capabilities. Recent advances in neuro-symbolic methods, which integrate neural and symbolic models leveraging human knowledge, promise improved performance with less data. This study addresses the gap in understanding these approaches' effectiveness in complex event detection (CED), especially in temporal reasoning. We investigate neural and neuro-symbolic architectures' performance in a multimodal CED task, analyzing IMU and acoustic data streams to recognize CE patterns. Our methodology includes (i) end-to-end neural architectures for direct CE detection from sensor embeddings, (ii) two-stage concept-based neural mode
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;LLM&#22686;&#24378;&#35821;&#20041;&#20998;&#26512;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;&#25991;&#26412;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26694;&#26550;&#65292;&#21487;&#26174;&#33879;&#25913;&#21892;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#35780;&#20272;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#19987;&#19994;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.11398</link><description>&lt;p&gt;
&#22312;&#27604;&#36739;&#20043;&#21069;&#36827;&#34892;&#25512;&#29702;&#65306;LLM&#22686;&#24378;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#29992;&#20110;&#39046;&#22495;&#19987;&#38376;&#25991;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11398
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;LLM&#22686;&#24378;&#35821;&#20041;&#20998;&#26512;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;&#25991;&#26412;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26694;&#26550;&#65292;&#21487;&#26174;&#33879;&#25913;&#21892;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#35780;&#20272;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#19987;&#19994;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#26469;&#22686;&#24378;&#35821;&#20041;&#20998;&#26512;&#65292;&#20026;&#25991;&#26412;&#24320;&#21457;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#35299;&#20915;&#20256;&#32479;&#26080;&#30417;&#30563;NLP&#24230;&#37327;&#65288;&#22914;ROUGE&#21644;BLEU&#65289;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;LLM&#65288;&#20363;&#22914;GPT-4&#65289;&#29992;&#20110;&#38646;&#26679;&#26412;&#25991;&#26412;&#35782;&#21035;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26631;&#31614;&#29983;&#25104;&#65292;&#22312;&#37027;&#37324;&#36825;&#20123;&#26631;&#31614;&#28982;&#21518;&#34987;&#29992;&#20316;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#12290;&#36890;&#36807;&#22312;MIMIC&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-4&#29983;&#25104;&#30340;&#26631;&#31614;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#35821;&#20041;&#30456;&#20284;&#24230;&#35780;&#20272;&#65292;&#24471;&#20998;&#26356;&#25509;&#36817;&#20020;&#24202;&#23454;&#38469;&#24773;&#20917;&#27604;&#20256;&#32479;NLP&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#20351;&#29992;LLM&#36827;&#34892;&#39640;&#24230;&#19987;&#19994;&#39046;&#22495;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#20041;&#20998;&#26512;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#26377;&#21322;&#23450;&#37327;&#25512;&#29702;&#32467;&#26524;&#12290;&#34429;&#28982;&#35813;&#26694;&#26550;&#38024;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20284;&#24615;&#20998;&#26512;&#36827;&#34892;&#20102;&#23454;&#26045;&#65292;&#20294;&#20854;&#27010;&#24565;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#19987;&#38376;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11398v1 Announce Type: cross  Abstract: In this study, we leverage LLM to enhance the semantic analysis and develop similarity metrics for texts, addressing the limitations of traditional unsupervised NLP metrics like ROUGE and BLEU. We develop a framework where LLMs such as GPT-4 are employed for zero-shot text identification and label generation for radiology reports, where the labels are then used as measurements for text similarity. By testing the proposed framework on the MIMIC data, we find that GPT-4 generated labels can significantly improve the semantic similarity assessment, with scores more closely aligned with clinical ground truth than traditional NLP metrics. Our work demonstrates the possibility of conducting semantic analysis of the text data using semi-quantitative reasoning results by the LLMs for highly specialized domains. While the framework is implemented for radiology report similarity analysis, its concept can be extended to other specialized domains 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Casanovo-DIA&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20174;DIA&#36136;&#35889;&#25968;&#25454;&#20013;&#35299;&#26512;&#32957;&#27573;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2402.11363</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#20840;&#26032;&#32957;&#27573;&#27979;&#24207;&#25216;&#26415;&#29992;&#20110;&#25968;&#25454;&#26080;&#20559;&#21521;&#37319;&#38598;&#36136;&#35889;
&lt;/p&gt;
&lt;p&gt;
Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11363
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Casanovo-DIA&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20174;DIA&#36136;&#35889;&#25968;&#25454;&#20013;&#35299;&#26512;&#32957;&#27573;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20018;&#32852;&#36136;&#35889;&#65288;MS/MS&#65289;&#20316;&#20026;&#20840;&#38754;&#20998;&#26512;&#29983;&#29289;&#26679;&#26412;&#20013;&#34507;&#30333;&#36136;&#21547;&#37327;&#30340;&#20027;&#35201;&#39640;&#36890;&#37327;&#25216;&#26415;&#65292;&#19968;&#30452;&#26159;&#25512;&#21160;&#34507;&#30333;&#36136;&#32452;&#23398;&#21457;&#23637;&#30340;&#22522;&#30707;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#25968;&#25454;&#26080;&#20559;&#21521;&#37319;&#38598;&#65288;DIA&#65289;&#31574;&#30053;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#26377;&#21161;&#20110;&#23545;&#21069;&#20307;&#31163;&#23376;&#36827;&#34892;&#20844;&#27491;&#21644;&#38750;&#38774;&#21521;&#30862;&#35010;&#12290;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#39640;&#22810;&#37325;&#24615;&#29305;&#24615;&#65292;DIA&#29983;&#25104;&#30340;MS/MS&#35889;&#22270;&#36896;&#25104;&#20102;&#24040;&#22823;&#38556;&#30861;&#12290;&#27599;&#20010;&#35889;&#22270;&#37117;&#21253;&#21547;&#26469;&#33258;&#22810;&#20010;&#21069;&#20307;&#32957;&#30340;&#30862;&#35010;&#20135;&#29289;&#31163;&#23376;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#29305;&#21035;&#22312;&#20840;&#26032;&#32957;&#27573;/&#34507;&#30333;&#36136;&#27979;&#24207;&#20013;&#26500;&#25104;&#20102;&#19968;&#20010;&#23574;&#38160;&#25361;&#25112;&#65292;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#22810;&#37325;&#24615;&#38590;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Casanovo-DIA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;DIA&#36136;&#35889;&#25968;&#25454;&#20013;&#35299;&#26512;&#32957;&#27573;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11363v1 Announce Type: cross  Abstract: Tandem mass spectrometry (MS/MS) stands as the predominant high-throughput technique for comprehensively analyzing protein content within biological samples. This methodology is a cornerstone driving the advancement of proteomics. In recent years, substantial strides have been made in Data-Independent Acquisition (DIA) strategies, facilitating impartial and non-targeted fragmentation of precursor ions. The DIA-generated MS/MS spectra present a formidable obstacle due to their inherent high multiplexing nature. Each spectrum encapsulates fragmented product ions originating from multiple precursor peptides. This intricacy poses a particularly acute challenge in de novo peptide/protein sequencing, where current methods are ill-equipped to address the multiplexing conundrum. In this paper, we introduce Casanovo-DIA, a deep-learning model based on transformer architecture. It deciphers peptide sequences from DIA mass spectrometry data. Our 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#36890;&#36807;&#36827;&#21270;&#20195;&#29702;&#30340;&#21151;&#33021;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;</title><link>https://arxiv.org/abs/2402.11359</link><description>&lt;p&gt;
&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Training Language Model Agents without Modifying Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#36890;&#36807;&#36827;&#21270;&#20195;&#29702;&#30340;&#21151;&#33021;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26368;&#36817;&#24050;&#32463;&#23558;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37325;&#26032;&#23450;&#20041;&#20026;&#20195;&#29702;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#21151;&#33021;&#33258;&#21160;&#21270;&#22320;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;LLM&#20195;&#29702;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#20462;&#25913;LLM&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;LLM&#20195;&#29702;&#30340;&#26032;&#33539;&#24335;&#65292;&#24403;LLM&#38590;&#20197;&#25110;&#26080;&#27861;&#36827;&#34892;&#20462;&#25913;&#26102;&#23588;&#20854;&#26377;&#29992;&#12290;&#21463;&#21040;&#20154;&#31867;&#19981;&#26029;&#38203;&#36896;&#24037;&#20855;&#20197;&#36866;&#24212;&#29616;&#23454;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#32780;&#19981;&#26159;&#25913;&#21464;&#25105;&#20204;&#30340;&#29983;&#29289;&#32467;&#26500;&#20197;&#36866;&#24212;&#19968;&#32452;&#38745;&#24577;&#24037;&#20855;&#65292;&#25105;&#20204;&#25552;&#20986;&#36880;&#27493;&#38203;&#36896;&#20195;&#29702;&#30340;&#21151;&#33021;&#65292;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20462;&#25913;LLM&#26435;&#37325;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#21151;&#33021;&#35270;&#20026;&#21487;&#23398;&#20064;&#30340;&#8220;&#20195;&#29702;&#21442;&#25968;&#8221;&#24182;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#35757;&#32451;&#30340;&#22522;&#26412;&#24605;&#24819;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AgentOptimizer&#65292;&#21033;&#29992;LLM&#26356;&#26032;&#20195;&#29702;&#30340;&#21151;&#33021;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20195;&#29702;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11359v1 Announce Type: new  Abstract: Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with tw
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#36335;&#30001;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;PEOs&#26377;&#25928;&#35782;&#21035;&#22270;&#20013;&#38656;&#35201;&#32771;&#34385;&#36827;&#34892;&#31934;&#30830;&#36317;&#31163;&#35745;&#31639;&#30340;&#37051;&#23621;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11354</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#36335;&#30001;&#30340;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#36335;&#30001;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;PEOs&#26377;&#25928;&#35782;&#21035;&#22270;&#20013;&#38656;&#35201;&#32771;&#34385;&#36827;&#34892;&#31934;&#30830;&#36317;&#31163;&#35745;&#31639;&#30340;&#37051;&#23621;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.11354v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;(ANNS)&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;ANNS&#30340;&#20248;&#36234;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#23613;&#31649;&#24341;&#20837;&#20102;&#21508;&#31181;&#22522;&#20110;&#22270;&#30340;ANNS&#20248;&#21270;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20381;&#36182;&#20110;&#32570;&#20047;&#27491;&#24335;&#29702;&#35770;&#25903;&#25345;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;&#22270;&#30340;ANNS&#20013;&#30340;&#36335;&#30001;&#65292;&#35813;&#26041;&#27861;&#22312;&#25506;&#32034;&#22270;&#20013;&#33410;&#28857;&#30340;&#37051;&#23621;&#26102;&#25552;&#20379;&#27010;&#29575;&#20445;&#35777;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#27010;&#29575;&#36335;&#30001;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#25935;&#24863;&#25216;&#26415;&#24320;&#21457;&#20102;&#20004;&#31181;&#22522;&#20934;&#31574;&#30053;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PEOs&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#35782;&#21035;&#22270;&#20013;&#24212;&#32771;&#34385;&#36827;&#34892;&#31934;&#30830;&#36317;&#31163;&#35745;&#31639;&#30340;&#37051;&#23621;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11354v1 Announce Type: cross  Abstract: Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a pivotal challenge in the field of machine learning. In recent years, graph-based methods have emerged as the superior approach to ANNS, establishing a new state of the art. Although various optimizations for graph-based ANNS have been introduced, they predominantly rely on heuristic methods that lack formal theoretical backing. This paper aims to enhance routing within graph-based ANNS by introducing a method that offers a probabilistic guarantee when exploring a node's neighbors in the graph. We formulate the problem as probabilistic routing and develop two baseline strategies by incorporating locality-sensitive techniques. Subsequently, we introduce PEOs, a novel approach that efficiently identifies which neighbors in the graph should be considered for exact distance computation, thus significantly improving efficiency in practice. Our experiments demonstrate 
&lt;/p&gt;</description></item><item><title>&#38271;&#26399;&#35760;&#24518;&#30340;&#24341;&#20837;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#20013;&#30340;&#29992;&#25143;&#33258;&#25105;&#25259;&#38706;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#20915;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#21644;&#38544;&#31169;&#38382;&#39064;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.11353</link><description>&lt;p&gt;
&#29702;&#35299;&#38271;&#26399;&#35760;&#24518;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#20013;&#33258;&#25105;&#25259;&#38706;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11353
&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#35760;&#24518;&#30340;&#24341;&#20837;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#20013;&#30340;&#29992;&#25143;&#33258;&#25105;&#25259;&#38706;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#20915;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#21644;&#38544;&#31169;&#38382;&#39064;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26377;&#28508;&#21147;&#36890;&#36807;&#24320;&#25918;&#24335;&#23545;&#35805;&#25903;&#25345;&#20844;&#20849;&#21355;&#29983;&#30417;&#27979;&#65292;&#20294;&#22312;&#22810;&#27425;&#20114;&#21160;&#20013;&#24456;&#23569;&#20445;&#30041;&#20851;&#20110;&#20010;&#20154;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#28155;&#21152;&#38271;&#26399;&#35760;&#24518;(LTM)&#26469;&#22686;&#24378;LLMs&#25552;&#20379;&#20102;&#25913;&#36827;&#21442;&#19982;&#24230;&#21644;&#33258;&#25105;&#25259;&#38706;&#30340;&#26426;&#20250;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#23545;LTM&#22914;&#20309;&#24433;&#21709;&#20154;&#20204;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#20013;&#19982;LLM&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20114;&#21160;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;1,252&#36890;&#35805;&#35760;&#24405;&#21644;&#23545;&#20061;&#21517;&#29992;&#25143;&#30340;&#35775;&#35848;&#65292;&#30740;&#31350;&#20102;CareCall&#36825;&#31181;&#24102;&#26377;LTM&#30340;LLM&#39537;&#21160;&#30340;&#35821;&#38899;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26696;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;LTM&#22686;&#24378;&#20102;&#20581;&#24247;&#25259;&#38706;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#29087;&#24713;&#24863;&#20419;&#36827;&#20102;&#29992;&#25143;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#31215;&#26497;&#30475;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#36890;&#36807;LTM&#20419;&#36827;&#33258;&#25105;&#25259;&#38706;&#23384;&#22312;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#35299;&#20915;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#21644;&#38544;&#31169;&#38382;&#39064;&#26041;&#38754;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;LTM&#25972;&#21512;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11353v1 Announce Type: cross  Abstract: Recent large language models (LLMs) offer the potential to support public health monitoring by facilitating health disclosure through open-ended conversations but rarely preserve the knowledge gained about individuals across repeated interactions. Augmenting LLMs with long-term memory (LTM) presents an opportunity to improve engagement and self-disclosure, but we lack an understanding of how LTM impacts people's interaction with LLM-driven chatbots in public health interventions. We examine the case of CareCall -- an LLM-driven voice chatbot with LTM -- through the analysis of 1,252 call logs and interviews with nine users. We found that LTM enhanced health disclosure and fostered positive perceptions of the chatbot by offering familiarity. However, we also observed challenges in promoting self-disclosure through LTM, particularly around addressing chronic health conditions and privacy concerns. We discuss considerations for LTM integr
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27809;&#26377;&#23398;&#20064;&#21040;&#35821;&#35328;&#30340;&#35270;&#21548;&#29305;&#24615;&#65292;&#22312;&#26032;&#30340;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#26292;&#38706;&#20102;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#19982;&#35821;&#35328;&#27169;&#22411;&#24863;&#23448;&#22788;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.11349</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26410;&#23398;&#20064;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Tasks That Language Models Don't Learn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11349
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27809;&#26377;&#23398;&#20064;&#21040;&#35821;&#35328;&#30340;&#35270;&#21548;&#29305;&#24615;&#65292;&#22312;&#26032;&#30340;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#26292;&#38706;&#20102;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#19982;&#35821;&#35328;&#27169;&#22411;&#24863;&#23448;&#22788;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#65292;&#25105;&#20204;&#30446;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27809;&#26377;&#23398;&#20064;&#21040;&#35821;&#35328;&#30340;&#26576;&#20123;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#20219;&#21153;&#65288;&#31216;&#20026;H-TEST&#65289;&#23545;&#35821;&#35328;&#30340;&#35270;&#21548;&#29305;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#36825;&#19968;&#22522;&#20934;&#27979;&#35797;&#31361;&#26174;&#20102;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#19982;LLMs&#30340;&#24863;&#23448;&#21463;&#38480;&#22788;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#36317;&#12290;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;1. &#25925;&#24847;&#25512;&#29702;&#65288;&#24605;&#32500;&#38142;&#65289;&#65292;2. &#23569;&#37327;&#26696;&#20363;&#65292;&#25110;3. &#21516;&#19968;&#27169;&#22411;&#31995;&#21015;&#30340;&#26356;&#24378;&#22823;LLM&#65288;LLaMA 2 13B-&gt;LLaMA 2 70B&#65289;&#24182;&#19981;&#33021;&#31616;&#21333;&#22320;&#24102;&#26469;H-TEST&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#29305;&#21035;&#23558;&#20854;&#19982;&#29595;&#20029;&#30340;&#21746;&#23398;&#26696;&#20363;&#32852;&#31995;&#36215;&#26469;&#65292;&#22905;&#22312;&#24863;&#23448;&#21463;&#38480;&#29615;&#22659;&#20013;&#20102;&#35299;&#19990;&#30028;&#65288;Jackson&#65292;1986&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19968;&#20123;&#26368;&#24378;&#22823;&#30340;&#19987;&#26377;LLMs&#30340;&#34920;&#29616;&#25509;&#36817;&#20110;&#38543;&#26426;&#22522;&#20934;&#20934;&#30830;&#29575;50&#65285;&#65292;&#31361;&#26174;&#20102;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11349v1 Announce Type: cross  Abstract: We argue that there are certain properties of language that our current large language models (LLMs) don't learn. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-TEST. This benchmark highlights a fundamental gap between human linguistic comprehension, which naturally integrates sensory experiences, and the sensory-deprived processing capabilities of LLMs. In support of our hypothesis, 1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the same model family (LLaMA 2 13B -&gt; LLaMA 2 70B) do not trivially bring improvements in H-TEST performance. Therefore, we make a particular connection to the philosophical case of Mary, who learns about the world in a sensory-deprived environment (Jackson, 1986). Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limit
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32570;&#20047;&#37096;&#20998;&#21453;&#39304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#31574;&#30053;&#26469;&#30830;&#20445;&#25152;&#26377;&#23376;&#32676;&#20307;&#37117;&#34987;&#25506;&#32034;&#12289;&#38450;&#27490;&#38169;&#35823;&#20998;&#31867;&#12289;&#20197;&#21450;&#25910;&#25947;&#21040;&#26399;&#26395;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.11338</link><description>&lt;p&gt;
&#20855;&#26377;&#37096;&#20998;&#21453;&#39304;&#30340;&#20844;&#24179;&#20998;&#31867;&#65306;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32570;&#20047;&#37096;&#20998;&#21453;&#39304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#31574;&#30053;&#26469;&#30830;&#20445;&#25152;&#26377;&#23376;&#32676;&#20307;&#37117;&#34987;&#25506;&#32034;&#12289;&#38450;&#27490;&#38169;&#35823;&#20998;&#31867;&#12289;&#20197;&#21450;&#25910;&#25947;&#21040;&#26399;&#26395;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39044;&#27979;&#22330;&#26223;&#65288;&#20363;&#22914;&#20449;&#36151;&#25918;&#27454;&#65289;&#20013;&#65292;&#21482;&#26377;&#36807;&#21435;&#34987;&#31215;&#26497;&#20998;&#31867;&#30340;&#26679;&#26412;&#25165;&#20250;&#35266;&#23519;&#21040;&#30495;&#23454;&#32467;&#26524;&#12290;&#36825;&#20123;&#36807;&#21435;&#30340;&#35266;&#23519;&#32467;&#26524;&#24418;&#25104;&#20102;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#20197;&#36827;&#34892;&#26410;&#26469;&#39044;&#27979;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#32570;&#20047;&#20851;&#20110;&#36807;&#21435;&#65288;&#38169;&#35823;&#22320;&#65289;&#34987;&#36127;&#38754;&#20998;&#31867;&#30340;&#26679;&#26412;&#32467;&#26524;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#19968;&#31995;&#21015;&#25506;&#32034;&#31574;&#30053;&#26469;&#25910;&#38598;&#20851;&#20110;&#21542;&#21017;&#20250;&#34987;&#24573;&#30053;&#30340;&#23376;&#32676;&#20307;&#30340;&#32467;&#26524;&#25968;&#25454;&#12290;&#23545;&#20110;&#20219;&#20309;&#25506;&#32034;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#37117;&#20855;&#26377;&#20197;&#19979;&#20445;&#35777;&#65306;&#65288;1&#65289;&#25152;&#26377;&#23376;&#32676;&#20307;&#37117;&#24471;&#21040;&#20102;&#25506;&#32034;&#65292;&#65288;2&#65289;&#20551;&#38451;&#24615;&#30340;&#27604;&#20363;&#21463;&#21040;&#20102;&#38480;&#21046;&#65292;&#65288;3&#65289;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#25910;&#25947;&#21040;&#19968;&#20010;&#8220;&#26399;&#26395;&#8221;&#30340;&#20998;&#31867;&#22120;&#12290;&#27491;&#30830;&#30340;&#25506;&#32034;&#31574;&#30053;&#21462;&#20915;&#20110;&#19978;&#19979;&#25991;&#65307;&#23427;&#21487;&#20197;&#36873;&#25321;&#20197;&#25913;&#21892;&#23398;&#20064;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11338v1 Announce Type: cross  Abstract: In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a "desired" classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees 
&lt;/p&gt;</description></item><item><title>&#37325;&#26500;&#23398;&#20064;&#25152;&#20135;&#29983;&#30340;&#29305;&#24449;&#23545;&#24863;&#30693;&#26080;&#29992;&#65292;&#38656;&#35201;&#36890;&#36807;&#20854;&#20182;&#31574;&#30053;&#22914;&#21435;&#22122;&#23398;&#20064;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11337</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26500;&#23398;&#20064;&#20135;&#29983;&#23545;&#24863;&#30693;&#26080;&#29992;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Learning by Reconstruction Produces Uninformative Features For Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11337
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26500;&#23398;&#20064;&#25152;&#20135;&#29983;&#30340;&#29305;&#24449;&#23545;&#24863;&#30693;&#26080;&#29992;&#65292;&#38656;&#35201;&#36890;&#36807;&#20854;&#20182;&#31574;&#30053;&#22914;&#21435;&#22122;&#23398;&#20064;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#31354;&#38388;&#37325;&#26500;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#12290;&#23613;&#31649;&#37325;&#26500;&#21644;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36890;&#36807;&#37325;&#26500;&#23398;&#20064;&#19982;&#20026;&#24863;&#30693;&#23398;&#20064;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21069;&#32773;&#23558;&#27169;&#22411;&#30340;&#23481;&#37327;&#20998;&#37197;&#32473;&#35299;&#37322;&#35266;&#23519;&#21040;&#30340;&#26041;&#24046;&#30340;&#25968;&#25454;&#23376;&#31354;&#38388;--&#36825;&#26159;&#23545;&#21518;&#32773;&#26080;&#29992;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11337v1 Announce Type: cross  Abstract: Input space reconstruction is an attractive representation learning paradigm. Despite interpretability of the reconstruction and generation, we identify a misalignment between learning by reconstruction, and learning for perception. We show that the former allocates a model's capacity towards a subspace of the data explaining the observed variance--a subspace with uninformative features for the latter. For example, the supervised TinyImagenet task with images projected onto the top subspace explaining 90\% of the pixel variance can be solved with 45\% test accuracy. Using the bottom subspace instead, accounting for only 20\% of the pixel variance, reaches 55\% test accuracy. The features for perception being learned last explains the need for long training time, e.g., with Masked Autoencoders. Learning by denoising is a popular strategy to alleviate that misalignment. We prove that while some noise strategies such as masking are indeed
&lt;/p&gt;</description></item><item><title>SpikeNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#24555;&#36895;&#25214;&#21040;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.11322</link><description>&lt;p&gt;
SpikeNAS: &#19968;&#31181;&#38754;&#21521;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11322
&lt;/p&gt;
&lt;p&gt;
SpikeNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#24555;&#36895;&#25214;&#21040;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#35745;&#31639;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;SNN&#26550;&#26500;&#37117;&#28304;&#33258;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#31070;&#32463;&#20803;&#30340;&#26550;&#26500;&#21644;&#25805;&#20316;&#19982;SNN&#19981;&#21516;&#65292;&#25110;&#32773;&#22312;&#19981;&#32771;&#34385;&#26469;&#33258;&#24213;&#23618;&#22788;&#29702;&#30828;&#20214;&#30340;&#20869;&#23384;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#24320;&#21457;&#12290;&#36825;&#20123;&#38480;&#21046;&#38459;&#30861;&#20102;SNN&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20805;&#20998;&#21457;&#25381;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpikeNAS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#65292;&#21487;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#24555;&#36895;&#25214;&#21040;&#19968;&#20010;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;SNN&#26550;&#26500;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;SpikeNAS&#37319;&#29992;&#20102;&#20960;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#20998;&#26512;&#32593;&#32476;&#25805;&#20316;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#22686;&#24378;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23398;&#20064;&#36136;&#37327;&#65292;&#24182;&#24320;&#21457;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11322v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) offer a promising solution to achieve ultra low-power/energy computation for solving machine learning tasks. Currently, most of the SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, or developed without considering memory budgets from the underlying processing hardware. These limitations hinder the SNNs from reaching their full potential in accuracy and efficiency. Towards this, we propose SpikeNAS, a novel memory-aware neural architecture search (NAS) framework for SNNs that can quickly find an appropriate SNN architecture with high accuracy under the given memory budgets. To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, and developing a fast memory-aware search algorithm. The experimental resul
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#29992;&#20110;&#25429;&#25417;&#26580;&#24615;&#36830;&#32493;&#26426;&#26800;&#33218;&#30005;&#32518;&#39537;&#21160;&#30340;&#38750;&#32447;&#24615;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#28382;&#21518;&#34917;&#20607;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.11319</link><description>&lt;p&gt;
&#20351;&#29992;RGBD&#24863;&#30693;&#21644;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#23545;&#26580;&#24615;&#36830;&#32493;&#26426;&#26800;&#33218;&#30340;&#28382;&#21518;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
Hysteresis Compensation of Flexible Continuum Manipulator using RGBD Sensing and Temporal Convolutional Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11319
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#29992;&#20110;&#25429;&#25417;&#26580;&#24615;&#36830;&#32493;&#26426;&#26800;&#33218;&#30005;&#32518;&#39537;&#21160;&#30340;&#38750;&#32447;&#24615;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#28382;&#21518;&#34917;&#20607;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26580;&#24615;&#36830;&#32493;&#26426;&#26800;&#33218;&#22240;&#33021;&#22815;&#36890;&#36807;&#38750;&#32447;&#24615;&#36335;&#24452;&#36827;&#20837;&#29421;&#31364;&#31354;&#38388;&#32780;&#34987;&#37325;&#35270;&#20110;&#24494;&#21019;&#25163;&#26415;&#12290;&#20294;&#26159;&#21463;&#21040;&#30005;&#32518;&#25928;&#24212;&#65288;&#22914;&#25705;&#25830;&#12289;&#20280;&#38271;&#21644;&#32806;&#21512;&#65289;&#24341;&#36215;&#30340;&#28382;&#21518;&#25928;&#24212;&#23548;&#33268;&#30005;&#32518;&#39537;&#21160;&#26426;&#26500;&#38754;&#20020;&#25511;&#21046;&#22256;&#38590;&#12290;&#36825;&#20123;&#25928;&#24212;&#30001;&#20110;&#38750;&#32447;&#24615;&#32780;&#24456;&#38590;&#24314;&#27169;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38271;&#19988;&#22810;&#33410;&#27573;&#26426;&#26800;&#33218;&#26102;&#36825;&#20123;&#22256;&#38590;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#30005;&#32518;&#39537;&#21160;&#30340;&#36825;&#31181;&#38750;&#32447;&#24615;&#21644;&#20197;&#24448;&#29366;&#24577;&#20381;&#36182;&#29305;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#23450;&#21046;&#30340;&#22522;&#20934;&#26631;&#35760;&#26469;&#25910;&#38598;&#29289;&#29702;&#20851;&#33410;&#37197;&#32622;&#20316;&#20026;&#25968;&#25454;&#38598;&#12290;&#23545;&#22235;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23398;&#20064;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#65288;TCN&#65289;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#21033;&#29992;&#32463;&#36807;&#35757;&#32451;&#30340;TCN&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25511;&#21046;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11319v1 Announce Type: cross  Abstract: Flexible continuum manipulators are valued for minimally invasive surgery, offering access to confined spaces through nonlinear paths. However, cable-driven manipulators face control difficulties due to hysteresis from cabling effects such as friction, elongation, and coupling. These effects are difficult to model due to nonlinearity and the difficulties become even more evident when dealing with long and multi-segmented manipulator. This paper proposes a data-driven approach based on recurrent neural networks to capture these nonlinear and previous states-dependent characteristics of cable actuation. We design customized fiducial markers to collect physical joint configurations as a dataset. Result on a study comparing the learning performance of four Deep Neural Network (DNN) models show that the Temporal Convolution Network (TCN) demonstrates the highest predictive capability. Leveraging trained TCNs, we build a control algorithm to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#23398;&#20064;&#36866;&#24212;&#24615;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#29615;&#22659;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#19982;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20114;&#20449;&#24687;&#20043;&#38388;&#30340;&#38590;&#39064;</title><link>https://arxiv.org/abs/2402.11317</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#38745;&#24577;&#21160;&#24577;&#30340;&#24555;&#36895;&#22312;&#32447;&#35843;&#25972;&#30340;&#21435;&#20559;&#32622;&#31163;&#32447;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#23398;&#20064;&#36866;&#24212;&#24615;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#29615;&#22659;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#19982;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20114;&#20449;&#24687;&#20043;&#38388;&#30340;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#36866;&#24212;&#38750;&#38745;&#24577;&#29615;&#22659;&#30340;&#31574;&#30053;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20165;&#26377;&#19968;&#32452;&#26377;&#38480;&#30340;&#39044;&#20808;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#31163;&#32447;&#35774;&#32622;&#20013;&#23398;&#20064;&#36825;&#31181;&#36866;&#24212;&#24615;&#31574;&#30053;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#21435;&#20559;&#32622;&#31163;&#32447;&#34920;&#31034;&#24555;&#36895;&#22312;&#32447;&#35843;&#25972;&#65288;DORA&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;DORA&#34701;&#20837;&#20102;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#26368;&#22823;&#21270;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#29615;&#22659;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DORA&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11317v1 Announce Type: cross  Abstract: Developing policies that can adjust to non-stationary environments is essential for real-world reinforcement learning applications. However, learning such adaptable policies in offline settings, with only a limited set of pre-collected trajectories, presents significant challenges. A key difficulty arises because the limited offline data makes it hard for the context encoder to differentiate between changes in the environment dynamics and shifts in the behavior policy, often leading to context misassociations. To address this issue, we introduce a novel approach called Debiased Offline Representation for fast online Adaptation (DORA). DORA incorporates an information bottleneck principle that maximizes mutual information between the dynamics encoding and the environmental data, while minimizing mutual information between the dynamics encoding and the actions of the behavior policy. We present a practical implementation of DORA, leverag
&lt;/p&gt;</description></item><item><title>&#22810;&#29983;&#25104;&#20195;&#29702;&#31995;&#32479;&#22312;&#22478;&#24066;&#35268;&#21010;&#20013;&#27169;&#25311;&#31038;&#21306;&#20915;&#31574;&#65292;&#21457;&#29616;&#27807;&#36890;&#26377;&#21161;&#20110;&#38598;&#20307;&#25512;&#29702;&#65292;&#32780;&#21253;&#21547;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#29983;&#27963;&#20215;&#20540;&#23548;&#33268;&#24847;&#35265;&#20998;&#27495;&#12290;&#36825;&#20026;&#22478;&#24066;&#35268;&#21010;&#21644;&#31038;&#21306;&#21442;&#19982;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.11314</link><description>&lt;p&gt;
&#22810;&#29983;&#25104;&#20195;&#29702;&#38598;&#20307;&#20915;&#31574;&#22312;&#22478;&#24066;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;&#65306;Kendall Square&#25913;&#36896;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Generative Agent Collective Decision-Making in Urban Planning: A Case Study for Kendall Square Renovation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11314
&lt;/p&gt;
&lt;p&gt;
&#22810;&#29983;&#25104;&#20195;&#29702;&#31995;&#32479;&#22312;&#22478;&#24066;&#35268;&#21010;&#20013;&#27169;&#25311;&#31038;&#21306;&#20915;&#31574;&#65292;&#21457;&#29616;&#27807;&#36890;&#26377;&#21161;&#20110;&#38598;&#20307;&#25512;&#29702;&#65292;&#32780;&#21253;&#21547;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#29983;&#27963;&#20215;&#20540;&#23548;&#33268;&#24847;&#35265;&#20998;&#27495;&#12290;&#36825;&#20026;&#22478;&#24066;&#35268;&#21010;&#21644;&#31038;&#21306;&#21442;&#19982;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#29983;&#25104;&#20195;&#29702;&#31995;&#32479;&#65292;&#29992;&#20110;&#27169;&#25311; Kendall Square Volpe &#22823;&#21414;&#25913;&#36896;&#30340;&#31038;&#21306;&#20915;&#31574;&#36807;&#31243;&#12290;&#36890;&#36807;&#19982;&#24403;&#22320;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35775;&#35848;&#65292;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#21512;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#27807;&#36890;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#29983;&#27963;&#20215;&#20540;&#22312;&#20195;&#29702;&#25552;&#31034;&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20195;&#29702;&#20043;&#38388;&#30340;&#27807;&#36890;&#25913;&#21892;&#20102;&#38598;&#20307;&#25512;&#29702;&#65292;&#32780;&#21253;&#21547;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#29983;&#27963;&#20215;&#20540;&#23548;&#33268;&#20102;&#26356;&#26126;&#26174;&#30340;&#24847;&#35265;&#20998;&#27495;&#12290;&#36825;&#20123;&#21457;&#29616;&#20984;&#26174;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#29702;&#35299;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#21644;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20026;&#22478;&#24066;&#35268;&#21010;&#21644; Kendall Square &#31561;&#22810;&#26679;&#21270;&#29615;&#22659;&#20013;&#30340;&#31038;&#21306;&#21442;&#19982;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11314v1 Announce Type: cross  Abstract: In this study, we develop a multiple-generative agent system to simulate community decision-making for the redevelopment of Kendall Square's Volpe building. Drawing on interviews with local stakeholders, our simulations incorporated varying degrees of communication, demographic data, and life values in the agent prompts. The results revealed that communication among agents improved collective reasoning, while the inclusion of demographic and life values led to more distinct opinions. These findings highlight the potential application of AI in understanding complex social interactions and decision-making processes, offering valuable insights for urban planning and community engagement in diverse settings like Kendall Square.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20154;&#31867;&#21644;32&#31181;&#19981;&#21516;LLM&#30340;&#20559;&#22909;&#65292;&#21457;&#29616;&#20154;&#31867;&#19981;&#22826;&#22312;&#24847;&#38169;&#35823;&#65292;&#20559;&#22909;&#25903;&#25345;&#31435;&#22330;&#30340;&#22238;&#24212;&#65292;&#32780;&#20808;&#36827;&#30340;LLM&#26356;&#27880;&#37325;&#27491;&#30830;&#24615;&#12289;&#28165;&#26224;&#24615;&#21644;&#26080;&#23475;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11296</link><description>&lt;p&gt;
&#20998;&#26512;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Dissecting Human and LLM Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20154;&#31867;&#21644;32&#31181;&#19981;&#21516;LLM&#30340;&#20559;&#22909;&#65292;&#21457;&#29616;&#20154;&#31867;&#19981;&#22826;&#22312;&#24847;&#38169;&#35823;&#65292;&#20559;&#22909;&#25903;&#25345;&#31435;&#22330;&#30340;&#22238;&#24212;&#65292;&#32780;&#20808;&#36827;&#30340;LLM&#26356;&#27880;&#37325;&#27491;&#30830;&#24615;&#12289;&#28165;&#26224;&#24615;&#21644;&#26080;&#23475;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#27169;&#22411;&#21709;&#24212;&#30340;&#30456;&#23545;&#36136;&#37327;&#27604;&#36739;&#65292;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20559;&#22909;&#22312;&#27169;&#22411;&#24494;&#35843;&#20013;&#20316;&#20026;&#20849;&#21516;&#30340;&#23545;&#40784;&#30446;&#26631;&#21644;&#35780;&#20272;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20559;&#22909;&#20165;&#21453;&#26144;&#20102;&#24191;&#27867;&#36235;&#21183;&#65292;&#23548;&#33268;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#36739;&#24046;&#65292;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#21078;&#26512;&#20102;&#20154;&#31867;&#21644;32&#31181;&#19981;&#21516;LLM&#30340;&#20559;&#22909;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#30340;&#23450;&#37327;&#32452;&#25104;&#65292;&#21033;&#29992;&#26469;&#33258;&#30495;&#23454;&#29992;&#25143;-&#27169;&#22411;&#23545;&#35805;&#30340;&#27880;&#37322;&#36827;&#34892;&#32454;&#31890;&#24230;&#12289;&#22330;&#26223;&#21270;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#23545;&#38169;&#35823;&#19981;&#22826;&#25935;&#24863;&#65292;&#20559;&#22909;&#25903;&#25345;&#20854;&#31435;&#22330;&#30340;&#22238;&#24212;&#65292;&#24182;&#22312;&#27169;&#22411;&#25215;&#35748;&#20854;&#23616;&#38480;&#24615;&#26102;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#19981;&#21916;&#27426;&#12290;&#30456;&#21453;&#65292;&#20687;GPT-4-Turbo&#36825;&#26679;&#30340;&#20808;&#36827;LLM&#26356;&#21152;&#24378;&#35843;&#27491;&#30830;&#24615;&#12289;&#28165;&#26224;&#24615;&#21644;&#26080;&#23475;&#24615;&#12290;&#27492;&#22806;&#65292;&#22823;&#23567;&#30456;&#20284;&#30340;LLM&#20542;&#21521;&#20110;&#23637;&#29616;&#20986;&#31867;&#20284;&#30340;&#20559;&#22909;&#65292;&#26080;&#35770;&#23427;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#22914;&#20309;&#65292;&#24182;&#19988;&#20026;&#20102;&#23545;&#40784;&#32780;&#36827;&#34892;&#30340;&#24494;&#35843;&#24182;&#19981;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11296v1 Announce Type: cross  Abstract: As a relative quality comparison of model responses, human and Large Language Model (LLM) preferences serve as common alignment goals in model fine-tuning and criteria in evaluation. Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks. In this work, we dissect the preferences of human and 32 different LLMs to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis. We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize correctness, clarity, and harmlessness more. Additionally, LLMs of similar sizes tend to exhibit similar preferences, regardless of their training methods, and fine-tuning for alignment does not sign
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#36890;&#36807;&#23558;&#38590;&#39064;&#20998;&#20026;&#22522;&#20110;&#35268;&#21017;&#21644;&#26080;&#35268;&#21017;&#20004;&#31867;&#30340;&#29420;&#29305;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#29616;&#65292;&#24378;&#35843;&#20102;&#22312;&#22797;&#26434;&#38590;&#39064;&#24773;&#22659;&#20013;LLMs&#30340;&#25361;&#25112;&#21644;&#20154;&#31867;&#31867;&#20284;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#25512;&#21160;LLMs&#35299;&#35868;&#33021;&#21147;&#21644;&#36129;&#29486;&#20110;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11291</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#35299;&#20915;&#38590;&#39064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Puzzle Solving using Reasoning of Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#36890;&#36807;&#23558;&#38590;&#39064;&#20998;&#20026;&#22522;&#20110;&#35268;&#21017;&#21644;&#26080;&#35268;&#21017;&#20004;&#31867;&#30340;&#29420;&#29305;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#29616;&#65292;&#24378;&#35843;&#20102;&#22312;&#22797;&#26434;&#38590;&#39064;&#24773;&#22659;&#20013;LLMs&#30340;&#25361;&#25112;&#21644;&#20154;&#31867;&#31867;&#20284;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#25512;&#21160;LLMs&#35299;&#35868;&#33021;&#21147;&#21644;&#36129;&#29486;&#20110;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#20915;&#38590;&#39064;&#20013;&#30340;&#33021;&#21147;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#26631;&#24535;&#30528;&#29702;&#35299;&#23427;&#20204;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#26412;&#35843;&#26597;&#21033;&#29992;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#23558;&#38590;&#39064;&#20998;&#20026;&#22522;&#20110;&#35268;&#21017;&#21644;&#26080;&#35268;&#21017;&#20004;&#31867;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#35780;&#20272;LLMs&#65292;&#21253;&#25324;&#25552;&#31034;&#25216;&#26415;&#12289;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#25209;&#21028;&#24615;&#23457;&#26597;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#22797;&#26434;&#38590;&#39064;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#35782;&#21035;&#20986;&#22797;&#26434;&#38590;&#39064;&#24773;&#22659;&#20013;&#30340;&#26174;&#33879;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;LLMs&#33021;&#21147;&#21450;&#31867;&#20154;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#39640;&#32423;&#36923;&#36753;&#25512;&#26029;&#30340;&#24773;&#20917;&#19979;&#12290;&#35843;&#26597;&#24378;&#35843;&#20102;&#38656;&#35201;&#26032;&#39062;&#31574;&#30053;&#21644;&#26356;&#20016;&#23500;&#25968;&#25454;&#38598;&#26469;&#25552;&#21319;LLMs&#30340;&#35299;&#35868;&#33021;&#21147;&#24182;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11291v1 Announce Type: cross  Abstract: Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in artificial intelligence, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy -- dividing puzzles into rule-based and rule-less categories -- to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs' performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs' puzzle-solving proficiency and contribute to AI's
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;O-Cloud&#30340;&#33021;&#32791;&#21450;&#20854;&#19982;&#26381;&#21153;&#22120;&#30828;&#20214;&#12289;&#23481;&#37327;&#21644;&#25968;&#25454;&#27969;&#37327;&#29305;&#24615;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#31574;&#30053;&#21644;&#26080;&#32447;&#31574;&#30053;&#65292;&#24179;&#34913;&#33021;&#28304;&#33410;&#32422;&#21644;&#24615;&#33021;&#65292;&#30830;&#20445;&#23427;&#20204;&#22312;&#26381;&#21153;&#22120;&#21644;&#29992;&#25143;&#20043;&#38388;&#20844;&#24179;&#20998;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.11285</link><description>&lt;p&gt;
&#22312;&#34394;&#25311;&#21270;O-RAN&#24179;&#21488;&#20013;&#30340;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Fair Resource Allocation in Virtualized O-RAN Platforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11285
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;O-Cloud&#30340;&#33021;&#32791;&#21450;&#20854;&#19982;&#26381;&#21153;&#22120;&#30828;&#20214;&#12289;&#23481;&#37327;&#21644;&#25968;&#25454;&#27969;&#37327;&#29305;&#24615;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#31574;&#30053;&#21644;&#26080;&#32447;&#31574;&#30053;&#65292;&#24179;&#34913;&#33021;&#28304;&#33410;&#32422;&#21644;&#24615;&#33021;&#65292;&#30830;&#20445;&#23427;&#20204;&#22312;&#26381;&#21153;&#22120;&#21644;&#29992;&#25143;&#20043;&#38388;&#20844;&#24179;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
O-RAN&#31995;&#32479;&#21450;&#20854;&#22312;&#34394;&#25311;&#21270;&#36890;&#29992;&#35745;&#31639;&#24179;&#21488;&#65288;O-Cloud&#65289;&#20013;&#30340;&#37096;&#32626;&#26500;&#25104;&#20102;&#19968;&#20010;&#39044;&#35745;&#23558;&#24102;&#26469;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#22686;&#30410;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#24102;&#26469;&#20102;&#26032;&#30340;&#23454;&#26045;&#25361;&#25112;&#65292;&#24182;&#23041;&#32961;&#30528;&#21152;&#21095;&#31227;&#21160;&#32593;&#32476;&#24050;&#32463;&#39640;&#33021;&#32791;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;O-Cloud&#30340;&#33021;&#32791;&#21450;&#20854;&#23545;&#26381;&#21153;&#22120;&#30828;&#20214;&#12289;&#23481;&#37327;&#21644;&#25968;&#25454;&#27969;&#37327;&#29305;&#24615;&#30340;&#20381;&#36182;&#24615;&#65292;&#36825;&#20123;&#29305;&#24615;&#36890;&#24120;&#20250;&#38543;&#26102;&#38388;&#25913;&#21464;&#12290;&#25509;&#19979;&#26469;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#31574;&#30053;&#65292;&#20197;&#33021;&#25928;&#30340;&#26041;&#24335;&#23558;&#22522;&#31449;&#25968;&#25454;&#36127;&#36733;&#20998;&#37197;&#21040;O-Cloud&#26381;&#21153;&#22120;&#65307;&#20197;&#21450;&#19968;&#20010;&#26080;&#32447;&#31574;&#30053;&#65292;&#21487;&#23454;&#26102;&#30830;&#23450;&#27599;&#20010;&#29992;&#25143;&#30340;&#26368;&#23567;&#20256;&#36755;&#22359;&#22823;&#23567;&#65292;&#20174;&#32780;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#33021;&#28304;&#25104;&#26412;&#12290;&#36825;&#20123;&#31574;&#30053;&#24179;&#34913;&#20102;&#33021;&#28304;&#33410;&#32422;&#21644;&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#23427;&#20204;&#22312;&#26381;&#21153;&#22120;&#21644;&#29992;&#25143;&#20043;&#38388;&#20998;&#25955;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11285v1 Announce Type: cross  Abstract: O-RAN systems and their deployment in virtualized general-purpose computing platforms (O-Cloud) constitute a paradigm shift expected to bring unprecedented performance gains. However, these architectures raise new implementation challenges and threaten to worsen the already-high energy consumption of mobile networks. This paper presents first a series of experiments which assess the O-Cloud's energy costs and their dependency on the servers' hardware, capacity and data traffic properties which, typically, change over time. Next, it proposes a compute policy for assigning the base station data loads to O-Cloud servers in an energy-efficient fashion; and a radio policy that determines at near-real-time the minimum transmission block size for each user so as to avoid unnecessary energy costs. The policies balance energy savings with performance, and ensure that both of them are dispersed fairly across the servers and users, respectively. 
&lt;/p&gt;</description></item><item><title>&#22810;&#36879;&#35270;&#19968;&#33268;&#24615;&#26041;&#27861;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#24102;&#26469;&#25913;&#36827;&#65292;&#33021;&#26377;&#25928;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11279</link><description>&lt;p&gt;
&#22810;&#36879;&#35270;&#19968;&#33268;&#24615;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11279
&lt;/p&gt;
&lt;p&gt;
&#22810;&#36879;&#35270;&#19968;&#33268;&#24615;&#26041;&#27861;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#24102;&#26469;&#25913;&#36827;&#65292;&#33021;&#26377;&#25928;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37096;&#32626;&#20013;&#65292;&#20934;&#30830;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#23545;&#20110;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#32463;&#24120;&#26080;&#27861;&#20811;&#26381;&#23545;&#38169;&#35823;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;&#32771;&#34385;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#25105;&#24847;&#35782;&#30340;&#33030;&#24369;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#36879;&#35270;&#19968;&#33268;&#24615;&#65288;MPC&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#27169;&#22411;&#20869;&#19981;&#21516;&#36879;&#35270;&#35282;&#24230;&#65288;MPC-Internal&#65289;&#21644;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#65288;MPC-Across&#65289;&#30340;&#20114;&#34917;&#35265;&#35299;&#26469;&#32531;&#35299;&#30001;&#21333;&#19968;&#35270;&#35282;&#20135;&#29983;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#12290;&#23545;&#20843;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;MPC&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;MPC&#33021;&#22815;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11279v1 Announce Type: cross  Abstract: In the deployment of large language models (LLMs), accurate confidence estimation is critical for assessing the credibility of model predictions. However, existing methods often fail to overcome the issue of overconfidence on incorrect answers. In this work, we focus on improving the confidence estimation of large language models. Considering the fragility of self-awareness in language models, we introduce a Multi-Perspective Consistency (MPC) method. We leverage complementary insights from different perspectives within models (MPC-Internal) and across different models (MPC-Across) to mitigate the issue of overconfidence arising from a singular viewpoint. The experimental results on eight publicly available datasets show that our MPC achieves state-of-the-art performance. Further analyses indicate that MPC can mitigate the problem of overconfidence and is effectively scalable to other models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#27169;&#22411;DFCPS&#65292;&#21019;&#26032;&#22320;&#34701;&#21512;&#20102;Fixmatch&#27010;&#24565;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#22788;&#29702;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20132;&#21449;&#20266;&#30417;&#30563;&#27010;&#24565;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#35282;&#24230;&#30340;&#20266;&#26631;&#31614;&#65292;&#22686;&#24378;&#35757;&#32451;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11273</link><description>&lt;p&gt;
&#22522;&#20110;&#20132;&#21449;&#20266;&#26631;&#35760;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#24369;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Medical Image Segmentation Method Based on Cross-pseudo Labeling Leveraging Strong and Weak Data Augmentation Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#27169;&#22411;DFCPS&#65292;&#21019;&#26032;&#22320;&#34701;&#21512;&#20102;Fixmatch&#27010;&#24565;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#22788;&#29702;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20132;&#21449;&#20266;&#30417;&#30563;&#27010;&#24565;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#35282;&#24230;&#30340;&#20266;&#26631;&#31614;&#65292;&#22686;&#24378;&#35757;&#32451;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#21382;&#21490;&#19978;&#36935;&#21040;&#20102;&#26576;&#20123;&#32422;&#26463;&#65292;&#30001;&#20110;&#25361;&#25112;&#24615;&#30340;&#25910;&#38598;&#36807;&#31243;&#12289;&#39640;&#26114;&#30340;&#26631;&#35760;&#25104;&#26412;&#12289;&#20302;&#20449;&#22122;&#27604;&#21644;&#22797;&#26434;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#25551;&#36848;&#20102;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#27169;&#22411;DFCPS&#65292;&#35813;&#27169;&#22411;&#21019;&#26032;&#24615;&#22320;&#32467;&#21512;&#20102;Fixmatch&#27010;&#24565;&#12290;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#22788;&#29702;&#65292;&#37319;&#29992;&#21508;&#31181;&#31574;&#30053;&#22788;&#29702;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#35774;&#35745;&#30528;&#37325;&#20110;&#20266;&#26631;&#31614;&#30340;&#29983;&#25104;&#12289;&#36807;&#28388;&#21644;&#32454;&#21270;&#36807;&#31243;&#12290;&#24341;&#20837;&#20102;&#20132;&#21449;&#20266;&#30417;&#30563;&#30340;&#26032;&#27010;&#24565;&#65292;&#23558;&#19968;&#33268;&#24615;&#23398;&#20064;&#19982;&#33258;&#25105;&#35757;&#32451;&#30456;&#32467;&#21512;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#35282;&#24230;&#30340;&#20266;&#26631;&#31614;&#65292;&#20174;&#32780;&#22686;&#24378;&#35757;&#32451;&#30340;&#22810;&#26679;&#24615;&#12290;DFCPS&#27169;&#22411;&#19982;&#22522;&#20934;&#32447;&#21644;&#19968;&#20010;&#36827;&#34892;&#20102;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11273v1 Announce Type: cross  Abstract: Traditional supervised learning methods have historically encountered certain constraints in medical image segmentation due to the challenging collection process, high labeling cost, low signal-to-noise ratio, and complex features characterizing biomedical images. This paper proposes a semi-supervised model, DFCPS, which innovatively incorporates the Fixmatch concept. This significantly enhances the model's performance and generalizability through data augmentation processing, employing varied strategies for unlabeled data. Concurrently, the model design gives appropriate emphasis to the generation, filtration, and refinement processes of pseudo-labels. The novel concept of cross-pseudo-supervision is introduced, integrating consistency learning with self-training. This enables the model to fully leverage pseudo-labels from multiple perspectives, thereby enhancing training diversity. The DFCPS model is compared with both baseline and a
&lt;/p&gt;</description></item><item><title>MoRAL&#32467;&#21512;&#20102;MoE&#30340;&#22810;&#20219;&#21153;&#33021;&#21147;&#21644;LoRA&#30340;&#24494;&#35843;&#33021;&#21147;&#65292;&#37319;&#29992;&#38382;&#31572;&#23545;&#20316;&#20026;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;LLM&#30340;&#26377;&#25928;&#32456;&#36523;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.11260</link><description>&lt;p&gt;
MoRAL: MoE&#22686;&#24378;LoRA&#29992;&#20110;LLM&#30340;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11260
&lt;/p&gt;
&lt;p&gt;
MoRAL&#32467;&#21512;&#20102;MoE&#30340;&#22810;&#20219;&#21153;&#33021;&#21147;&#21644;LoRA&#30340;&#24494;&#35843;&#33021;&#21147;&#65292;&#37319;&#29992;&#38382;&#31572;&#23545;&#20316;&#20026;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;LLM&#30340;&#26377;&#25928;&#32456;&#36523;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MoRAL&#30340;&#26041;&#27861;&#65292;&#21363;Mixture-of-Experts&#22686;&#24378;&#20302;&#31209;&#36866;&#24212;&#24615;&#29992;&#20110;LLM&#30340;&#32456;&#36523;&#23398;&#20064;&#12290; MoRAL&#23558;MoE&#30340;&#22810;&#20219;&#21153;&#33021;&#21147;&#19982;LoRA&#30340;&#24494;&#35843;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;LLM&#30340;&#26377;&#25928;&#32456;&#36523;&#23398;&#20064;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;MoRAL&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#38382;&#31572;&#23545;&#20316;&#20026;&#36755;&#20837;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#23454;&#29992;&#21644;&#26377;&#25928;&#30340;&#40065;&#26834;&#23398;&#20064;&#31574;&#30053;&#12290;&#37492;&#20110;&#26032;&#25968;&#25454;&#35774;&#32622;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21363;LLM&#30340;&#32456;&#36523;&#23398;&#20064;&#65288;5L-bench&#65289;&#65292;&#21253;&#25324;&#19968;&#20010;&#26032;&#30340;&#31934;&#24515;&#31574;&#21010;&#30340;&#38382;&#31572;&#23545;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#32452;&#29992;&#20110;&#22312;&#24320;&#25918;&#24335;&#21644;&#23553;&#38381;&#24335;&#29615;&#22659;&#19979;&#20005;&#26684;&#35780;&#20272;MoRAL&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#24320;&#25918;&#24335;&#29615;&#22659;&#19979;&#65292;LLM&#22312;&#30701;&#26102;&#38388;&#20869;&#36805;&#36895;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11260v1 Announce Type: cross  Abstract: Adapting large language models (LLMs) to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge. In this paper, we propose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for Lifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the fine-tuning abilities of LoRA for effective life-long learning of LLMs. In contrast to the conventional approaches that use factual triplets as inputs MoRAL relies on simple question-answer pairs, which is a more practical and effective strategy for robust and efficient learning. Owing to new data settings, we introduce a new evaluation benchmark namely: Life Long Learning of LLM (5L-bench) encompassing a newly curated dataset of question-answer pairs, and a set of evaluation metrics for rigorous evaluation of MoRAL in open-book and closed-book settings. Experimental evaluation shows (i) LLMs learn fast in open-book settings with up to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11253</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25919;&#31574;&#30340;&#33258;&#25105;&#21028;&#26029;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models by On-Policy Self-Judgment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65292;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#21033;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#25191;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#35201;&#20040;&#36890;&#36807;&#25918;&#24323;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#21644;&#23545;&#29420;&#31435;RM&#30340;&#38656;&#27714;&#31616;&#21270;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#23427;&#26082;&#26159;(1) &#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#21448;&#26159;(2) &#21442;&#25968;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;RM&#26469;&#35780;&#20272;&#26679;&#26412;&#36827;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#65292;&#20316;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#19968;&#23545;&#19968;&#21028;&#26029;&#20219;&#21153;&#35270;&#20026;&#25351;&#23548;&#24335;&#20219;&#21153;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20174;&#21709;&#24212;&#23545;&#20013;&#36873;&#25321;&#26356;&#22909;&#30340;&#21709;&#24212;&#12290;&#22240;&#27492;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#35780;&#21028;&#24403;&#21069;&#31574;&#30053;&#30340;&#21363;&#26102;&#21709;&#24212;&#20559;&#22909;&#65292;&#20174;&#33258;&#36523;&#21021;&#22987;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;SELF-JUDGE&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11253v1 Announce Type: cross  Abstract: To align large language models with human preferences, existing research either utilizes a separate reward model (RM) to perform on-policy learning or simplifies the training procedure by discarding the on-policy learning and the need for a separate RM. In this paper, we present a novel alignment framework, SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model acting as both a policy and a judge. Specifically, we view the pairwise judgment task as a special case of the instruction-following task, choosing the better response from a response pair. Thus, the resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20851;&#31995;&#22411;&#35770;&#35777;&#25366;&#25496;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#33021;&#22815;&#26174;&#33879;&#36229;&#36807;&#30446;&#21069;&#26368;&#20339;&#22522;&#20934;&#32447;&#65292;&#24182;&#19988;&#22312;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.11243</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#22522;&#20110;&#20851;&#31995;&#30340;&#35770;&#35777;&#25366;&#25496;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models perform Relation-based Argument Mining?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11243
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20851;&#31995;&#22411;&#35770;&#35777;&#25366;&#25496;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#33021;&#22815;&#26174;&#33879;&#36229;&#36807;&#30446;&#21069;&#26368;&#20339;&#22522;&#20934;&#32447;&#65292;&#24182;&#19988;&#22312;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25454;&#25366;&#25496;&#65288;AM&#65289;&#26159;&#20174;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#35770;&#25454;&#12289;&#23427;&#20204;&#30340;&#32452;&#25104;&#37096;&#20998;&#21644;/&#25110;&#35770;&#25454;&#21644;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#20851;&#31995;&#30340;&#36807;&#31243;&#12290;&#38543;&#30528;&#25903;&#25345;&#22312;&#32447;&#36777;&#35770;&#30340;&#24179;&#21488;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#23545;AM&#30340;&#38656;&#27714;&#21464;&#24471;&#24840;&#21457;&#36843;&#20999;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#25903;&#25345;&#19979;&#28216;&#20219;&#21153;&#12290;&#22522;&#20110;&#20851;&#31995;&#30340;AM&#65288;RbAM&#65289;&#26159;&#19968;&#31181;&#20851;&#27880;&#35782;&#21035;&#35770;&#25454;&#20043;&#38388;&#21327;&#35758;&#65288;&#25903;&#25345;&#65289;&#21644;&#19981;&#21516;&#24847;&#65288;&#25915;&#20987;&#65289;&#20851;&#31995;&#30340;AM&#24418;&#24335;&#12290;RbAM&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20196;&#20154;&#28385;&#24847;&#22320;&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#29992;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#32463;&#36807;&#36866;&#24403;&#30340;&#35843;&#25972;&#21644;&#25552;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#34920;&#29616;&#26368;&#22909;&#30340;&#65288;&#22522;&#20110;RoBERTa&#30340;&#65289;&#22522;&#20934;&#32447;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#24320;&#28304;LLM&#65288;Llama-2&#21644;Mistral&#65289;&#22312;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11243v1 Announce Type: cross  Abstract: Argument mining (AM) is the process of automatically extracting arguments, their components and/or relations amongst arguments and components from text. As the number of platforms supporting online debate increases, the need for AM becomes ever more urgent, especially in support of downstream tasks. Relation-based AM (RbAM) is a form of AM focusing on identifying agreement (support) and disagreement (attack) relations amongst arguments. RbAM is a challenging classification task, with existing methods failing to perform satisfactorily. In this paper, we show that general-purpose Large Language Models (LLMs), appropriately primed and prompted, can significantly outperform the best performing (RoBERTa-based) baseline. Specifically, we experiment with two open-source LLMs (Llama-2 and Mistral) with ten datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#22024;&#26434;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;Class-Balance-based Sample Selection (CBS)&#38450;&#27490;&#24573;&#35270;&#23614;&#37096;&#31867;&#21035;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;Confidence-based Sample Augmentation (CSA)&#22686;&#24378;&#24178;&#20928;&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11242</link><description>&lt;p&gt;
&#36890;&#36807;&#38450;&#27490;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#26469;&#23398;&#20064;&#19981;&#24179;&#34913;&#22024;&#26434;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Learning with Imbalanced Noisy Data by Preventing Bias in Sample Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11242
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#22024;&#26434;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;Class-Balance-based Sample Selection (CBS)&#38450;&#27490;&#24573;&#35270;&#23614;&#37096;&#31867;&#21035;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;Confidence-based Sample Augmentation (CSA)&#22686;&#24378;&#24178;&#20928;&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#19981;&#23436;&#32654;&#26631;&#31614;&#20250;&#20005;&#37325;&#24433;&#21709;&#28145;&#24230;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20542;&#21521;&#20110;&#23558;&#20302;&#25439;&#22833;&#26679;&#26412;&#35270;&#20026;&#24178;&#20928;&#26679;&#26412;&#65292;&#20002;&#24323;&#39640;&#25439;&#22833;&#26679;&#26412;&#20197;&#20943;&#36731;&#22024;&#26434;&#26631;&#31614;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19981;&#20165;&#21253;&#21547;&#22024;&#26434;&#26631;&#31614;&#65292;&#36824;&#21253;&#21547;&#31867;&#21035;&#19981;&#24179;&#34913;&#12290;&#19981;&#24179;&#34913;&#38382;&#39064;&#23481;&#26131;&#23548;&#33268;&#25439;&#22833;&#36739;&#22823;&#30340;&#23614;&#37096;&#31867;&#21035;&#30340;&#23398;&#20064;&#19981;&#36275;&#65292;&#20174;&#32780;&#20135;&#29983;&#39640;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#30340;&#22024;&#26434;&#26631;&#31614;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31867;&#24179;&#34913;&#30340;&#26679;&#26412;&#36873;&#25321;&#65288;CBS&#65289;&#26469;&#38450;&#27490;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24573;&#35270;&#23614;&#37096;&#31867;&#21035;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#26679;&#26412;&#22686;&#24378;&#65288;CSA&#65289;&#20197;&#21152;&#24378;&#25152;&#36873;&#24178;&#20928;&#26679;&#26412;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11242v1 Announce Type: cross  Abstract: Learning with noisy labels has gained increasing attention because the inevitable imperfect labels in real-world scenarios can substantially hurt the deep model performance. Recent studies tend to regard low-loss samples as clean ones and discard high-loss ones to alleviate the negative impact of noisy labels. However, real-world datasets contain not only noisy labels but also class imbalance. The imbalance issue is prone to causing failure in the loss-based sample selection since the under-learning of tail classes also leans to produce high losses. To this end, we propose a simple yet effective method to address noisy labels in imbalanced datasets. Specifically, we propose Class-Balance-based sample Selection (CBS) to prevent the tail class samples from being neglected during training. We propose Confidence-based Sample Augmentation (CSA) for the chosen clean samples to enhance their reliability in the training process. To exploit sel
&lt;/p&gt;</description></item><item><title>DiffPoint&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;ViT&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#29616;&#28857;&#20113;&#37325;&#24314;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.11241</link><description>&lt;p&gt;
DiffPoint: &#20351;&#29992;&#22522;&#20110;ViT&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21333;&#35270;&#28857;&#21644;&#22810;&#35270;&#28857;&#28857;&#20113;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
DiffPoint: Single and Multi-view Point Cloud Reconstruction with ViT Based Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11241
&lt;/p&gt;
&lt;p&gt;
DiffPoint&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;ViT&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#29616;&#28857;&#20113;&#37325;&#24314;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;2D&#21040;3D&#37325;&#24314;&#20219;&#21153;&#24341;&#36215;&#20102;&#26174;&#33879;&#20851;&#27880;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#28857;&#20113;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29983;&#25104;&#28857;&#20113;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#22270;&#20687;&#21644;&#28857;&#20113;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20173;&#28982;&#23384;&#22312;&#20135;&#29983;&#39640;&#20445;&#30495;&#32467;&#26524;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#23545;&#20174;&#22270;&#20687;&#37325;&#24314;&#28857;&#20113;&#30340;&#22909;&#22788;&#23578;&#26410;&#24471;&#21040;&#35777;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#27905;&#32780;&#24378;&#22823;&#30340;&#26550;&#26500;&#65292;&#21517;&#20026;DiffPoint&#65292;&#23427;&#32467;&#21512;&#20102;ViT&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#34892;&#28857;&#20113;&#37325;&#24314;&#20219;&#21153;&#12290;&#22312;&#27599;&#20010;&#25193;&#25955;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#23558;&#24102;&#26377;&#22122;&#22768;&#30340;&#28857;&#20113;&#20998;&#25104;&#19981;&#35268;&#21017;&#30340;&#34917;&#19969;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#19968;&#20010;&#26631;&#20934;&#30340;ViT&#20027;&#24178;&#65292;&#23558;&#25152;&#26377;&#36755;&#20837;&#65288;&#21253;&#25324;&#26102;&#38388;&#20449;&#24687;&#12289;&#22270;&#20687;&#23884;&#20837;&#21644;&#26377;&#22122;&#22768;&#30340;&#34917;&#19969;&#65289;&#35270;&#20026;&#20196;&#29260;&#65292;&#25105;&#20204;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11241v1 Announce Type: cross  Abstract: As the task of 2D-to-3D reconstruction has gained significant attention in various real-world scenarios, it becomes crucial to be able to generate high-quality point clouds. Despite the recent success of deep learning models in generating point clouds, there are still challenges in producing high-fidelity results due to the disparities between images and point clouds. While vision transformers (ViT) and diffusion models have shown promise in various vision tasks, their benefits for reconstructing point clouds from images have not been demonstrated yet. In this paper, we first propose a neat and powerful architecture called DiffPoint that combines ViT and diffusion models for the task of point cloud reconstruction. At each diffusion step, we divide the noisy point clouds into irregular patches. Then, using a standard ViT backbone that treats all inputs as tokens (including time information, image embeddings, and noisy patches), we train
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35843;&#26597;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#38754;&#20020;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#33324;&#26694;&#26550;&#21644;&#19981;&#21516;&#24418;&#24335;&#30340;&#21518;&#38376;&#25915;&#20987;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.11208</link><description>&lt;p&gt;
&#35686;&#24789;&#24744;&#30340;&#20195;&#29702;&#20154;&#65281;&#35843;&#26597;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#21518;&#38376;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11208
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35843;&#26597;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#38754;&#20020;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#33324;&#26694;&#26550;&#21644;&#19981;&#21516;&#24418;&#24335;&#30340;&#21518;&#38376;&#25915;&#20987;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLM&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#29992;&#20110;&#22788;&#29702;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#65288;&#21253;&#25324;&#37329;&#34701;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#36141;&#29289;&#31561;&#65289;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#12290;&#22312;&#24212;&#29992;&#36807;&#31243;&#20013;&#30830;&#20445;LLM&#20195;&#29702;&#20154;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;LLM&#20195;&#29702;&#20154;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#24037;&#20316;&#39318;&#27425;&#25506;&#35752;&#20102;&#20856;&#22411;&#23433;&#20840;&#23041;&#32961;&#20043;&#19968;&#65292;&#21363;&#23545;LLM&#20195;&#29702;&#20154;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20154;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#28982;&#21518;&#23545;&#19981;&#21516;&#24418;&#24335;&#30340;&#20195;&#29702;&#20154;&#21518;&#38376;&#25915;&#20987;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20174;&#26368;&#32456;&#25915;&#20987;&#32467;&#26524;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36873;&#25321;&#25805;&#32437;&#26368;&#32456;&#36755;&#20986;&#20998;&#24067;&#65292;&#25110;&#32773;&#20165;&#22312;&#20013;&#38388;&#25512;&#29702;&#36807;&#31243;&#20013;&#24341;&#20837;&#24694;&#24847;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#26368;&#32456;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#21069;&#19968;&#31867;&#21487;&#20197;&#20998;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11208v1 Announce Type: cross  Abstract: Leveraging the rapid development of Large Language Models LLMs, LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis on the different forms of agent backdoor attacks. Specifically, from the perspective of the final attacking outcomes, the attacker can either choose to manipulate the final output distribution, or only introduce malicious behavior in the intermediate reasoning process, while keeping the final output correct. Furthermore, the former category can be divided
&lt;/p&gt;</description></item><item><title>ChatGPT&#20316;&#20026;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#19981;&#26029;&#25361;&#25112;&#20256;&#32479;&#33539;&#24335;&#65292;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-3&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11203</link><description>&lt;p&gt;
&#25506;&#32034;ChatGPT&#22312;&#19979;&#19968;&#20195;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11203
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#19981;&#26029;&#25361;&#25112;&#20256;&#32479;&#33539;&#24335;&#65292;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-3&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20984;&#26174;&#20102;ChatGPT&#20316;&#20026;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;ChatGPT&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#22909;&#22788;&#65292;&#21560;&#24341;&#20102;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#30340;&#20851;&#27880;&#12290;&#19968;&#20123;&#20154;&#35748;&#20026;ChatGPT&#26159;&#19968;&#39033;&#24320;&#21019;&#24615;&#30340;&#21019;&#26032;&#65292;&#32780;&#21478;&#19968;&#20123;&#20154;&#23558;&#20854;&#25104;&#21151;&#24402;&#22240;&#20110;&#20135;&#21697;&#24320;&#21457;&#21644;&#24066;&#22330;&#31574;&#30053;&#30340;&#26377;&#25928;&#25972;&#21512;&#12290;ChatGPT&#30340;&#20986;&#29616;&#65292;&#20197;&#21450;&#19982;OpenAI&#30340;GPT-4&#19968;&#36215;&#65292;&#26631;&#24535;&#30528;&#29983;&#25104;&#24335;AI&#30340;&#26032;&#38454;&#27573;&#65292;&#20135;&#29983;&#30340;&#20869;&#23481;&#19982;&#35757;&#32451;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#36229;&#36234;&#20102;&#20197;&#24448;&#30340;GPT-3&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#19982;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;ChatGPT&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;&#33539;&#24335;&#65292;&#24102;&#26469;&#20102;&#20851;&#20110;&#25991;&#26412;&#36136;&#37327;&#20445;&#35777;&#12289;&#27169;&#22411;&#20559;&#24046;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#26032;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;ChatGPT&#23545;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11203v1 Announce Type: cross  Abstract: The rapid advancement of artificial intelligence (AI) has highlighted ChatGPT as a pivotal technology in the field of information retrieval (IR). Distinguished from its predecessors, ChatGPT offers significant benefits that have attracted the attention of both the industry and academic communities. While some view ChatGPT as a groundbreaking innovation, others attribute its success to the effective integration of product development and market strategies. The emergence of ChatGPT, alongside GPT-4, marks a new phase in Generative AI, generating content that is distinct from training examples and exceeding the capabilities of the prior GPT-3 model by OpenAI. Unlike the traditional supervised learning approach in IR tasks, ChatGPT challenges existing paradigms, bringing forth new challenges and opportunities regarding text quality assurance, model bias, and efficiency. This paper seeks to examine the impact of ChatGPT on IR tasks and offe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#26799;&#24230;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20004;&#20010;&#20851;&#38190;&#23376;&#31354;&#38388;&#26469;&#23454;&#29616;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32500;&#25345;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11196</link><description>&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#20445;&#25345;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Maintaining Adversarial Robustness in Continuous Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11196
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#26799;&#24230;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20004;&#20010;&#20851;&#38190;&#23376;&#31354;&#38388;&#26469;&#23454;&#29616;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32500;&#25345;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#22797;&#26434;&#30340;&#38450;&#24481;&#31639;&#27861;&#33719;&#24471;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#22312;&#31070;&#32463;&#32593;&#32476;&#19981;&#26029;&#28436;&#21270;&#20197;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#24456;&#23481;&#26131;&#34987;&#25273;&#21435;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#21487;&#20197;&#36890;&#36807;&#22521;&#20859;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#33021;&#21147;&#26469;&#35299;&#20915;&#65292;&#31216;&#20026;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#23427;&#22312;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#20851;&#27880;&#21069;&#26399;&#20219;&#21153;&#30340;(&#20998;&#31867;)&#24615;&#33021;&#21644;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#26799;&#24230;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#20110;&#26435;&#37325;&#26356;&#26032;&#30340;&#26799;&#24230;&#27491;&#20132;&#25237;&#24433;&#21040;&#20004;&#20010;&#20851;&#38190;&#23376;&#31354;&#38388;&#19978; -- &#19968;&#20010;&#29992;&#20110;&#31283;&#23450;&#24179;&#28369;&#26679;&#26412;&#26799;&#24230;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#31283;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#32456;&#36755;&#20986;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32500;&#25345;&#20102;&#23545;&#24378;&#23545;&#25239;&#24615;&#30340;&#25345;&#32493;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11196v1 Announce Type: cross  Abstract: Adversarial robustness is essential for security and reliability of machine learning systems. However, the adversarial robustness gained by sophisticated defense algorithms is easily erased as the neural network evolves to learn new tasks. This vulnerability can be addressed by fostering a novel capability for neural networks, termed continual robust learning, which focuses on both the (classification) performance and adversarial robustness on previous tasks during continuous learning. To achieve continuous robust learning, we propose an approach called Double Gradient Projection that projects the gradients for weight updates orthogonally onto two crucial subspaces -- one for stabilizing the smoothed sample gradients and another for stabilizing the final outputs of the neural network. The experimental results on four benchmarks demonstrate that the proposed approach effectively maintains continuous robustness against strong adversarial
&lt;/p&gt;</description></item><item><title>&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.11192</link><description>&lt;p&gt;
&#22914;&#26524;&#20320;&#35762;&#25105;&#30340;&#35821;&#35328;&#65292;&#25105;&#20250;&#26356;&#22909;&#22320;&#23398;&#20064;&#65306;&#20351;&#29992;&#39118;&#26684;&#23545;&#40784;&#21709;&#24212;&#35843;&#25972;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11192
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23567;&#25968;&#25454;&#38598;&#20026;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#20010;&#26222;&#36941;&#36935;&#21040;&#30340;&#20294;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#22312;&#26377;&#38480;&#30340;&#31034;&#20363;&#19978;&#36807;&#22810;&#25311;&#21512;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20445;&#30041;&#21407;&#22987;&#25216;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#23558;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;LLM&#22266;&#26377;&#39118;&#26684;&#21305;&#37197;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26368;&#23567;&#31243;&#24230;&#22320;&#20462;&#25913;LLM&#30340;&#29616;&#26377;&#21709;&#24212;&#20197;&#26356;&#27491;&#38169;&#35823;&#65292;&#20351;&#29992;&#36825;&#20123;&#35843;&#25972;&#21518;&#30340;&#21709;&#24212;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#23454;&#29616;&#19982;&#27169;&#22411;&#22266;&#26377;&#21709;&#24212;&#39118;&#26684;&#19968;&#33268;&#30340;&#31934;&#30830;&#26356;&#27491;&#65292;&#32500;&#25252;&#27169;&#22411;&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#20174;&#32780;&#36991;&#20813;&#36807;&#22810;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;LLM&#30340;&#29305;&#23450;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#20851;&#38190;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11192v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) with a small data set for particular tasks is a widely encountered yet complex challenge. The potential for overfitting on a limited number of examples can negatively impact the model's ability to generalize and retain its original skills. Our research explores the impact of the style of ground-truth responses during the fine-tuning process. We found that matching the ground-truth response style with the LLM's inherent style results in better learning outcomes. Building on this insight, we developed a method that minimally alters the LLM's pre-existing responses to correct errors, using these adjusted responses as training targets. This technique enables precise corrections in line with the model's native response style, safeguarding the model's core capabilities and thus avoid overfitting. Our findings show that this approach not only improves the LLM's task-specific accuracy but also crucially
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Layer Collapse&#65288;LaCo&#65289;&#30340;&#31616;&#26126;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#32467;&#26500;&#30340;&#21516;&#26102;&#36805;&#36895;&#20943;&#23567;&#23610;&#23544;&#65292;&#24182;&#22312;&#21098;&#26525;&#27604;&#20363;&#36798;&#21040;25-30%&#26102;&#20445;&#25345;&#36229;&#36807;80%&#30340;&#24179;&#22343;&#20219;&#21153;&#24615;&#33021;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11187</link><description>&lt;p&gt;
LaCo&#65306;&#36890;&#36807;&#23618;&#21472;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
LaCo: Large Language Model Pruning via Layer Collapse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11187
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Layer Collapse&#65288;LaCo&#65289;&#30340;&#31616;&#26126;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#32467;&#26500;&#30340;&#21516;&#26102;&#36805;&#36895;&#20943;&#23567;&#23610;&#23544;&#65292;&#24182;&#22312;&#21098;&#26525;&#27604;&#20363;&#36798;&#21040;25-30%&#26102;&#20445;&#25345;&#36229;&#36807;80%&#30340;&#24179;&#22343;&#20219;&#21153;&#24615;&#33021;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#32463;&#21382;&#30528;&#23610;&#23544;&#25193;&#22823;&#30340;&#26126;&#26174;&#36235;&#21183;&#65292;&#36825;&#32473;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#24102;&#26469;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22914;&#27169;&#22411;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#27169;&#22411;&#21098;&#26525;&#21463;&#21040;&#21508;&#31181;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#30828;&#20214;&#25903;&#25345;&#38480;&#21046;&#12289;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#21644;&#23545;&#27169;&#22411;&#20869;&#37096;&#32467;&#26500;&#30340;&#25913;&#21464;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#31216;&#20026;Layer Collapse&#65288;LaCo&#65289;&#65292;&#20854;&#20013;&#21518;&#32622;&#27169;&#22411;&#23618;&#25240;&#21472;&#21040;&#21069;&#32622;&#23618;&#65292;&#20351;&#27169;&#22411;&#23610;&#23544;&#36805;&#36895;&#20943;&#23567;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#32467;&#26500;&#12290;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21098;&#26525;&#27604;&#20363;&#36798;&#21040;25-30%&#26102;&#65292;&#20445;&#25345;&#20102;&#36229;&#36807;80%&#30340;&#24179;&#22343;&#20219;&#21153;&#24615;&#33021;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#21518;&#35757;&#32451;&#23454;&#39564;&#20197;&#30830;&#35748;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11187v1 Announce Type: cross  Abstract: Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the internal structure of the model. In this paper, we propose a concise layer-wise pruning method called \textit{Layer Collapse (LaCo)}, in which rear model layers collapse into a prior layer, enabling a rapid reduction in model size while preserving the model structure. Comprehensive experiments show that our method maintains an average task performance of over 80\% at pruning ratios of 25-30\%, significantly outperforming existing state-of-the-art structured pruning methods. We also conduct post-training experiments to confirm that the proposed pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#24335;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35748;&#35782;&#65292;&#21253;&#25324;&#35757;&#32451;&#27169;&#22411;&#26126;&#30830;&#35782;&#21035;&#31572;&#26696;&#20013;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#21644;&#38544;&#24335;&#21306;&#20998;&#21487;&#38752;&#21644;&#19981;&#21487;&#38752;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.11176</link><description>&lt;p&gt;
KnowTuning&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
KnowTuning: Knowledge-aware Fine-tuning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11176
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#24335;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35748;&#35782;&#65292;&#21253;&#25324;&#35757;&#32451;&#27169;&#22411;&#26126;&#30830;&#35782;&#21035;&#31572;&#26696;&#20013;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#21644;&#38544;&#24335;&#21306;&#20998;&#21487;&#38752;&#21644;&#19981;&#21487;&#38752;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#36827;&#34892;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#34920;&#29616;&#20986;&#29983;&#25104;&#19981;&#23436;&#25972;&#12289;&#38750;&#20107;&#23454;&#24615;&#25110;&#19981;&#21512;&#36923;&#36753;&#30340;&#31572;&#26696;&#31561;&#38480;&#21046;&#12290;&#36825;&#20123;&#38480;&#21046;&#28304;&#20110;LLMs&#22312;&#26222;&#36890;&#24494;&#35843;&#26399;&#38388;&#23545;&#30693;&#35782;&#30340;&#35748;&#35782;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;&#65288;KnowTuning&#65289;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#21644;&#38544;&#24335;&#22320;&#25913;&#21892;LLMs&#30340;&#30693;&#35782;&#35748;&#35782;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24335;&#30693;&#35782;&#24863;&#30693;&#29983;&#25104;&#38454;&#27573;&#65292;&#35757;&#32451;LLMs&#26126;&#30830;&#35782;&#21035;&#31572;&#26696;&#20013;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38544;&#24335;&#30693;&#35782;&#24863;&#30693;&#27604;&#36739;&#38454;&#27573;&#65292;&#35757;&#32451;LLMs&#38544;&#24335;&#21306;&#20998;&#21487;&#38752;&#21644;&#19981;&#21487;&#38752;&#30340;&#30693;&#35782;&#65292;&#21253;&#25324;&#23436;&#25972;&#24615;&#12289;&#20107;&#23454;&#24615;&#21644;&#36923;&#36753;&#24615;&#19977;&#20010;&#26041;&#38754;&#12290;&#23545;&#36890;&#29992;&#21644;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11176v1 Announce Type: cross  Abstract: Despite their success at many natural language processing (NLP) tasks, large language models (LLMs) still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers. These limitations stem from inadequate knowledge awareness of LLMs during vanilla fine-tuning. To address these problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to explicitly and implicitly improve the knowledge awareness of LLMs. We devise an explicit knowledge-aware generation stage to train LLMs to explicitly identify knowledge triples in answers. We also propose an implicit knowledge-aware comparison stage to train LLMs to implicitly distinguish between reliable and unreliable knowledge, in three aspects: completeness, factuality, and logicality. Extensive experiments on both generic and medical question answering (QA) datasets confirm the effec
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;&#30340;&#20449;&#20219;&#21306;&#22495;&#33021;&#22815;&#26377;&#25928;&#22320;&#27934;&#23519;&#27169;&#22411;&#34892;&#20026;&#12289;&#20445;&#35777;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#35299;&#37322;&#30340;&#37325;&#29992;</title><link>https://arxiv.org/abs/2402.11168</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Trust Regions for Explanations via Black-Box Probabilistic Certification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;&#30340;&#20449;&#20219;&#21306;&#22495;&#33021;&#22815;&#26377;&#25928;&#22320;&#27934;&#23519;&#27169;&#22411;&#34892;&#20026;&#12289;&#20445;&#35777;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#35299;&#37322;&#30340;&#37325;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#24615;&#36136;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#22823;&#37327;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26469;&#35299;&#26512;&#20010;&#21035;&#20915;&#31574;&#32972;&#21518;&#30340;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#40657;&#30418;&#65288;&#27010;&#29575;&#24615;&#65289;&#35299;&#37322;&#35748;&#35777;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#40657;&#30418;&#27169;&#22411;&#65292;&#21482;&#26377;&#26597;&#35810;&#35775;&#38382;&#26435;&#65292;&#19968;&#20010;&#31034;&#20363;&#30340;&#35299;&#37322;&#20197;&#21450;&#19968;&#20010;&#36136;&#37327;&#24230;&#37327;&#65288;&#22914;&#36924;&#30495;&#24230;&#12289;&#31283;&#23450;&#24615;&#65289;&#65292;&#25105;&#20204;&#26159;&#21542;&#33021;&#25214;&#21040;&#26368;&#22823;&#30340;&#36229;&#31435;&#26041;&#20307;&#65288;&#21363; $\ell_{\infty}$ &#29699;&#65289;&#65292;&#20197;&#31034;&#20363;&#20026;&#20013;&#24515;&#65292;&#20351;&#24471;&#24403;&#35299;&#37322;&#34987;&#24212;&#29992;&#20110;&#36229;&#31435;&#26041;&#20307;&#20869;&#30340;&#25152;&#26377;&#31034;&#20363;&#26102;&#65288;&#39640;&#27010;&#29575;&#19979;&#65289;&#36136;&#37327;&#26631;&#20934;&#24471;&#21040;&#28385;&#36275;&#65288;&#27604;&#22914;&#36924;&#30495;&#24230;&#39640;&#20110;&#26576;&#20010;&#20540;&#65289;&#65311;&#33021;&#22815;&#39640;&#25928;&#22320;&#25214;&#21040;&#36825;&#26679;&#19968;&#20010;&#20449;&#20219;&#21306;&#22495;&#26377;&#22810;&#37325;&#22909;&#22788;&#65306;i&#65289;&#27934;&#23519;&#27169;&#22411;&#22312;&#19968;&#20010;&#21306;&#22495;&#20869;&#30340;&#34892;&#20026;&#65292;&#20855;&#26377;&#20445;&#35777;&#65307;ii&#65289;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#24471;&#21040;&#20445;&#35777;&#65307;iii&#65289;&#35299;&#37322;&#30340;&#37325;&#29992;&#65292;&#21487;&#20197;&#33410;&#30465;&#26102;&#38388;&#12289;&#31934;&#21147;&#21644;&#37329;&#38065;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11168v1 Announce Type: cross  Abstract: Given the black box nature of machine learning models, a plethora of explainability methods have been developed to decipher the factors behind individual decisions. In this paper, we introduce a novel problem of black box (probabilistic) explanation certification. We ask the question: Given a black box model with only query access, an explanation for an example and a quality metric (viz. fidelity, stability), can we find the largest hypercube (i.e., $\ell_{\infty}$ ball) centered at the example such that when the explanation is applied to all examples within the hypercube, (with high probability) a quality criterion is met (viz. fidelity greater than some value)? Being able to efficiently find such a \emph{trust region} has multiple benefits: i) insight into model behavior in a \emph{region}, with a \emph{guarantee}; ii) ascertained \emph{stability} of the explanation; iii) \emph{explanation reuse}, which can save time, energy and mone
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;token-ensemble&#29983;&#25104;&#31574;&#30053;&#65292;&#25361;&#25112;&#20102;&#24403;&#21069;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#24403;&#21069;&#26816;&#27979;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#26816;&#27979;&#25216;&#26415;&#20197;&#24212;&#23545;&#22797;&#26434;&#23545;&#25239;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.11167</link><description>&lt;p&gt;
Token-Ensemble&#25991;&#26412;&#29983;&#25104;&#65306;&#23545;&#33258;&#21160;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11167
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;token-ensemble&#29983;&#25104;&#31574;&#30053;&#65292;&#25361;&#25112;&#20102;&#24403;&#21069;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#24403;&#21069;&#26816;&#27979;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#26816;&#27979;&#25216;&#26415;&#20197;&#24212;&#23545;&#22797;&#26434;&#23545;&#25239;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#20869;&#23481;&#26816;&#27979;&#27169;&#22411;&#23545;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25915;&#20987;&#65288;&#20363;&#22914;&#25913;&#20889;&#25110;&#35789;&#35821;&#26367;&#25442;&#65289;&#30340;&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;token-ensemble&#29983;&#25104;&#31574;&#30053;&#65292;&#25361;&#25112;&#20102;&#24403;&#21069;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20174;&#38543;&#26426;&#20505;&#36873;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#19979;&#19968;&#20010;token&#23436;&#25104;&#25552;&#31034;&#26469;&#25506;&#32034;&#38598;&#25104;&#25915;&#20987;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;token-ensemble&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;AI&#20869;&#23481;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65288;&#20195;&#30721;&#21644;&#27979;&#35797;&#38598;&#23558;&#21457;&#24067;&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;token-ensemble&#29983;&#25104;&#23545;&#24403;&#21069;&#26816;&#27979;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#25913;&#36827;&#26816;&#27979;&#25216;&#26415;&#20197;&#24212;&#23545;&#22797;&#26434;&#23545;&#25239;&#31574;&#30053;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11167v1 Announce Type: cross  Abstract: The robustness of AI-content detection models against cultivated attacks (e.g., paraphrasing or word switching) remains a significant concern. This study proposes a novel token-ensemble generation strategy to challenge the robustness of current AI-content detection approaches. We explore the ensemble attack strategy by completing the prompt with the next token generated from random candidate LLMs. We find the token-ensemble approach significantly drops the performance of AI-content detection models (The code and test sets will be released). Our findings reveal that token-ensemble generation poses a vital challenge to current detection models and underlines the need for advancing detection technologies to counter sophisticated adversarial strategies.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11161</link><description>&lt;p&gt;
PANDA&#65288;Pedantic ANswer-correctness Determination and Adjudication&#65289;&#65306;&#25913;&#36827;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#65288;QA&#65289;&#21482;&#26377;&#22312;&#25105;&#20204;&#30693;&#36947;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#26102;&#25165;&#33021;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#26368;&#20855;&#25361;&#25112;&#24615;&#21644;&#26377;&#36259;&#30340;QA&#31034;&#20363;&#65292;&#24403;&#21069;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#65288;AC&#65289;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20887;&#38271;&#12289;&#33258;&#30001;&#26684;&#24335;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#32570;&#20047;&#25968;&#25454;&#21644;&#27169;&#22411;&#36807;&#22823;&#12290;&#22522;&#20110;LLM&#30340;&#35780;&#20998;&#22120;&#19982;&#20154;&#31867;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20294;&#36825;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#20165;&#22312;&#26377;&#38480;&#30340;QA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#28165;&#26224;&#30340;&#25351;&#21335;&#26469;&#35780;&#20272;&#20174;&#20154;&#31867;QA&#27604;&#36187;&#20013;&#37319;&#32435;&#30340;&#26426;&#22120;QA&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#30830;&#23450;&#21644;&#35009;&#20915;&#65288;Precise ANswer correctness Determination and Adjudication&#65292;PANDA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23567;&#24039;&#12289;&#39640;&#25928;&#12289;&#30830;&#23450;&#24615;&#30340;AC&#20998;&#31867;&#22120;&#65288;812 KB&#65289;&#65292;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11161v1 Announce Type: cross  Abstract: Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current answer correctness (AC) metrics do not align with human judgments, particularly verbose, free form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big. LLM based scorers correlate better with humans, but this expensive task has only been tested on limited QA datasets. We rectify these issues by providing clear guidelines for evaluating machine QA adopted from human QA contests. We also introduce Precise ANswer correctness Determination and Adjudication (PANDA), a small, efficient, deterministic AC classifier (812 KB) that more accurately evaluates answer correctness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#39046;&#33521;&#19978;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;LiGNN&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;GNN&#34920;&#31034;&#23398;&#20064;&#30340;&#31639;&#27861;&#25913;&#36827;&#21644;&#22823;&#35268;&#27169;&#35757;&#32451;&#20248;&#21270;&#65292;&#20026;&#24037;&#20316;&#30003;&#35831;&#22238;&#22797;&#29575;&#12289;&#24191;&#21578;&#28857;&#20987;&#29575;&#21644;Feed&#27599;&#26085;&#27963;&#36291;&#29992;&#25143;&#25552;&#39640;&#24102;&#26469;&#20102;&#32422;1%-2%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.11139</link><description>&lt;p&gt;
LiGNN: &#39046;&#33521;&#19978;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LiGNN: Graph Neural Networks at LinkedIn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#39046;&#33521;&#19978;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;LiGNN&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;GNN&#34920;&#31034;&#23398;&#20064;&#30340;&#31639;&#27861;&#25913;&#36827;&#21644;&#22823;&#35268;&#27169;&#35757;&#32451;&#20248;&#21270;&#65292;&#20026;&#24037;&#20316;&#30003;&#35831;&#22238;&#22797;&#29575;&#12289;&#24191;&#21578;&#28857;&#20987;&#29575;&#21644;Feed&#27599;&#26085;&#27963;&#36291;&#29992;&#25143;&#25552;&#39640;&#24102;&#26469;&#20102;&#32422;1%-2%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LiGNN&#65292;&#19968;&#31181;&#24050;&#37096;&#32626;&#30340;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#22312;&#39046;&#33521;&#19978;&#24320;&#21457;&#21644;&#37096;&#32626;GNNs&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#31639;&#27861;&#25913;&#36827;&#65292;&#21253;&#25324;&#20855;&#26377;&#38271;&#26399;&#25439;&#22833;&#30340;&#26102;&#38388;&#22270;&#26550;&#26500;&#65292;&#36890;&#36807;&#22270;&#23494;&#38598;&#21270;&#23454;&#29616;&#30340;&#26377;&#25928;&#20919;&#21551;&#21160;&#35299;&#20915;&#26041;&#26696;&#65292;ID&#23884;&#20837;&#21644;&#22810;&#36339;&#37051;&#23621;&#37319;&#26679;&#65292;&#20197;&#25913;&#36827;GNN&#34920;&#31034;&#23398;&#20064;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#36890;&#36807;&#37051;&#23621;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#23545;&#35757;&#32451;&#25968;&#25454;&#25209;&#27425;&#36827;&#34892;&#20998;&#32452;&#21644;&#20999;&#29255;&#65292;&#19987;&#38376;&#30340;&#20849;&#20139;&#20869;&#23384;&#38431;&#21015;&#21644;&#26412;&#22320;&#26799;&#24230;&#20248;&#21270;&#23558;LinkedIn&#22270;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#21152;&#24555;7&#20493;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#20174;A/B&#27979;&#35797;&#23454;&#39564;&#20013;&#33719;&#24471;&#30340;&#37096;&#32626;&#32463;&#39564;&#21644;&#25945;&#35757;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;&#25216;&#26415;&#24050;&#32463;&#20026;&#24037;&#20316;&#30003;&#35831;&#22238;&#22797;&#29575;&#30340;&#30456;&#23545;&#25913;&#21892;&#29575;&#32422;&#20026;1&#65285;&#65292;&#24191;&#21578;&#28857;&#20987;&#29575;&#25552;&#21319;2&#65285;&#65292;Feed&#27599;&#26085;&#27963;&#36291;&#29992;&#25143;&#25552;&#39640;0.5&#65285;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11139v1 Announce Type: cross  Abstract: In this paper, we present LiGNN, a deployed large-scale Graph Neural Networks (GNNs) Framework. We share our insight on developing and deployment of GNNs at large scale at LinkedIn. We present a set of algorithmic improvements to the quality of GNN representation learning including temporal graph architectures with long term losses, effective cold start solutions via graph densification, ID embeddings and multi-hop neighbor sampling. We explain how we built and sped up by 7x our large-scale training on LinkedIn graphs with adaptive sampling of neighbors, grouping and slicing of training data batches, specialized shared-memory queue and local gradient optimization. We summarize our deployment lessons and learnings gathered from A/B test experiments. The techniques presented in this work have contributed to an approximate relative improvements of 1% of Job application hearing back rate, 2% Ads CTR lift, 0.5% of Feed engaged daily active 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;&#20219;&#21153;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2402.11138</link><description>&lt;p&gt;
&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Contrastive Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;&#20219;&#21153;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#19968;&#30452;&#34987;&#29992;&#20316;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#38754;&#20020;&#26410;&#30693;&#25351;&#20196;&#26102;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#31283;&#20581;&#24615;&#65292;&#24403;&#30456;&#21516;&#30340;&#25351;&#20196;&#20197;&#31245;&#24494;&#21464;&#21270;&#30340;&#24418;&#24335;&#25110;&#35821;&#35328;&#39118;&#26684;&#25552;&#20986;&#26102;&#20250;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#34892;&#20026;&#34920;&#26126;LLMs&#23545;&#25991;&#26412;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#21644;&#23545;&#26410;&#30693;&#25351;&#20196;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#21487;&#20449;&#24230;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#22823;&#21270;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#25351;&#20196;-&#23454;&#20363;&#23545;&#30340;&#38544;&#34255;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30340;&#21516;&#26102;&#65292;&#26368;&#23567;&#21270;&#35821;&#20041;&#19978;&#19981;&#21516;&#30340;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#36890;&#36807;&#37322;&#20041;&#20219;&#21153;&#25351;&#20196;&#65292;&#25193;&#20805;&#29616;&#26377;&#30340;FLAN&#38598;&#21512;&#12290;&#22312;PromptBench&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#65288;CoIN&#65289;&#19968;&#30452;&#25552;&#39640;&#20102;LLMs&#23545;&#26410;&#30693;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11138v1 Announce Type: cross  Abstract: Instruction tuning has been used as a promising approach to improve the performance of large language models (LLMs) on unseen tasks. However, current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning, which maximizes the similarity between the hidden representations of semantically equivalent instruction-instance pairs while minimizing the similarity between semantically different ones. To facilitate this approach, we augment the existing FLAN collection by paraphrasing task instructions. Experiments on the PromptBench benchmark show that CoIN consistently improves LLMs' robustness to unseen instructio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Speculative Streaming&#26041;&#27861;&#65292;&#23558;&#33609;&#31295;&#27169;&#22411;&#34701;&#20837;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23558;&#24494;&#35843;&#30446;&#26631;&#20174;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#26356;&#25913;&#20026;&#26410;&#26469;&#30340;n-gram&#39044;&#27979;&#65292;&#21152;&#36895;&#35299;&#30721;1.8-3.1&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.11131</link><description>&lt;p&gt;
&#25512;&#27979;&#24335;&#27969;&#24335;&#22788;&#29702;: &#26080;&#38656;&#36741;&#21161;&#27169;&#22411;&#30340;&#24555;&#36895;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Speculative Streaming: Fast LLM Inference without Auxiliary Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Speculative Streaming&#26041;&#27861;&#65292;&#23558;&#33609;&#31295;&#27169;&#22411;&#34701;&#20837;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23558;&#24494;&#35843;&#30446;&#26631;&#20174;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#26356;&#25913;&#20026;&#26410;&#26469;&#30340;n-gram&#39044;&#27979;&#65292;&#21152;&#36895;&#35299;&#30721;1.8-3.1&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#24335;&#35299;&#30721;&#26159;&#19968;&#31181;&#31361;&#20986;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#36741;&#21161;&#33609;&#31295;&#27169;&#22411;&#39044;&#27979;&#30340;&#22823;&#22411;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#34429;&#28982;&#22312;&#29305;&#23450;&#24212;&#29992;&#35774;&#32622;&#20013;&#26377;&#25928;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#24494;&#35843;&#33609;&#31295;&#21644;&#30446;&#26631;&#27169;&#22411;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#25509;&#21463;&#29575;&#12290;&#38543;&#30528;&#19979;&#28216;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#20123;&#33609;&#31295;&#27169;&#22411;&#32473;&#25512;&#29702;&#31995;&#32479;&#22686;&#21152;&#20102;&#26174;&#33879;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Speculative Streaming&#65292;&#19968;&#31181;&#21333;&#27169;&#22411;&#30340;&#25512;&#27979;&#24335;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#33609;&#25311;&#34701;&#20837;&#30446;&#26631;&#27169;&#22411;&#65292;&#23558;&#24494;&#35843;&#30446;&#26631;&#20174;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#23545;&#35937;&#26356;&#25913;&#20026;&#26410;&#26469;&#30340;n-gram&#39044;&#27979;&#12290; Speculative Streaming&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21152;&#36895;&#35299;&#30721;1.8-3.1&#20493;&#65292;&#22914;&#25688;&#35201;&#12289;&#32467;&#26500;&#21270;&#26597;&#35810;&#21644;&#24847;&#20041;&#34920;&#36798;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;Speculative Streaming&#21442;&#25968;&#26377;&#25928;&#12290;&#23427;&#23454;&#29616;&#20102;&#19982;Medusa&#39118;&#26684;&#26550;&#26500;&#30456;&#23218;&#32654;/&#26356;&#39640;&#30340;&#21152;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11131v1 Announce Type: cross  Abstract: Speculative decoding is a prominent technique to speed up the inference of a large target language model based on predictions of an auxiliary draft model. While effective, in application-specific settings, it often involves fine-tuning both draft and target models to achieve high acceptance rates. As the number of downstream tasks grows, these draft models add significant complexity to inference systems. We propose Speculative Streaming, a single-model speculative decoding method that fuses drafting into the target model by changing the fine-tuning objective from next token prediction to future n-gram prediction. Speculative Streaming speeds up decoding by 1.8 - 3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and Meaning Representation, without sacrificing generation quality. Additionally, Speculative Streaming is parameter-efficient. It achieves on-par/higher speed-ups than Medusa-style architectures while u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24773;&#22659;&#33218;&#21644;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#25968;&#25454;&#24314;&#31435;&#20010;&#24615;&#21270;&#30340;&#21326;&#27861;&#26519;&#21058;&#37327;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#32570;&#20047;&#22522;&#22240;&#22411;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11123</link><description>&lt;p&gt;
&#20351;&#29992;&#24773;&#22659;&#33218;&#30740;&#31350;&#20248;&#21270;&#21326;&#27861;&#26519;&#29992;&#37327;&#65306;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#21644;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Warfarin Dosing Using Contextual Bandit: An Offline Policy Learning and Evaluation Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24773;&#22659;&#33218;&#21644;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#25968;&#25454;&#24314;&#31435;&#20010;&#24615;&#21270;&#30340;&#21326;&#27861;&#26519;&#21058;&#37327;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#32570;&#20047;&#22522;&#22240;&#22411;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21326;&#27861;&#26519;&#26159;&#19968;&#31181;&#25239;&#20957;&#33647;&#29289;&#65292;&#26088;&#22312;&#39044;&#38450;&#21644;&#27835;&#30103;&#19982;&#24322;&#24120;&#34880;&#28082;&#20957;&#32467;&#30456;&#20851;&#30340;&#30142;&#30149;&#65292;&#26159;&#20840;&#29699;&#26368;&#24120;&#24320;&#22788;&#26041;&#30340;&#33647;&#29289;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20010;&#20307;&#21453;&#24212;&#21464;&#21270;&#65292;&#30830;&#23450;&#21512;&#36866;&#30340;&#21058;&#37327;&#20173;&#20855;&#25361;&#25112;&#24615;&#65292;&#38169;&#35823;&#30340;&#21058;&#37327;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;&#24773;&#22659;&#33218;&#21644;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#37492;&#20110;&#21382;&#21490;&#25919;&#31574;&#30340;&#35266;&#23519;&#25968;&#25454;&#24191;&#27867;&#21487;&#24471;&#19988;&#21307;&#30103;&#20915;&#31574;&#30340;&#23433;&#20840;&#24615;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#21382;&#21490;&#25919;&#31574;&#20316;&#20026;&#28436;&#31034;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#36890;&#36807;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#21644;&#35780;&#20272;&#22312;&#24773;&#22659;&#33218;&#29615;&#22659;&#20013;&#24314;&#31435;&#26368;&#20339;&#20010;&#24615;&#21270;&#21058;&#37327;&#31574;&#30053;&#12290;&#25105;&#20204;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#22312;&#27809;&#26377;&#22522;&#22240;&#22411;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#36825;&#20123;&#22522;&#32447;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#32473;&#23450;&#27425;&#20248;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#20063;&#34920;&#29616;&#20986;&#33394;&#65292;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11123v1 Announce Type: cross  Abstract: Warfarin, an anticoagulant medication, is formulated to prevent and address conditions associated with abnormal blood clotting, making it one of the most prescribed drugs globally. However, determining the suitable dosage remains challenging due to individual response variations, and prescribing an incorrect dosage may lead to severe consequences. Contextual bandit and reinforcement learning have shown promise in addressing this issue. Given the wide availability of observational data and safety concerns of decision-making in healthcare, we focused on using exclusively observational data from historical policies as demonstrations to derive new policies; we utilized offline policy learning and evaluation in a contextual bandit setting to establish the optimal personalized dosage strategy. Our learned policies surpassed these baseline approaches without genotype inputs, even when given a suboptimal demonstration, showcasing promising app
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#39034;&#24207;&#35760;&#24518;&#32534;&#36753;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20462;&#25913;&#21442;&#25968;ME&#21487;&#33021;&#20250;&#23548;&#33268;&#25152;&#26377;&#20219;&#21153;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#20445;&#30041;&#21442;&#25968;ME&#21017;&#33021;&#22815;&#20445;&#25345;&#36739;&#22909;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11122</link><description>&lt;p&gt;
&#23548;&#33322;&#21452;&#37325;&#38754;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#39034;&#24207;&#35760;&#24518;&#32534;&#36753;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11122
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#39034;&#24207;&#35760;&#24518;&#32534;&#36753;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20462;&#25913;&#21442;&#25968;ME&#21487;&#33021;&#20250;&#23548;&#33268;&#25152;&#26377;&#20219;&#21153;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#20445;&#30041;&#21442;&#25968;ME&#21017;&#33021;&#22815;&#20445;&#25345;&#36739;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#32534;&#36753;&#65288;ME&#65289;&#24050;&#32463;&#25104;&#20026;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#38169;&#35823;&#20107;&#23454;&#25110;&#27880;&#20837;&#26032;&#20107;&#23454;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23384;&#22312;&#20004;&#31181;&#20027;&#27969;ME&#26041;&#27861;&#65306;&#20462;&#25913;&#21442;&#25968;ME&#21644;&#20445;&#30041;&#21442;&#25968;ME&#65288;&#22312;&#20445;&#30041;&#21407;&#22987;&#21442;&#25968;&#30340;&#21516;&#26102;&#25972;&#21512;&#39069;&#22806;&#27169;&#22359;&#65289;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#20808;&#21069;&#23545;ME&#35780;&#20272;&#30340;&#30740;&#31350;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#65288;i&#65289;&#20165;&#35780;&#20272;&#24102;&#26377;&#21333;&#20010;&#32534;&#36753;&#30340;LLMs&#65292;&#24573;&#30053;&#20102;&#25345;&#32493;&#32534;&#36753;&#30340;&#38656;&#35201;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#35780;&#20272;&#20165;&#20851;&#27880;&#22522;&#26412;&#20107;&#23454;&#19977;&#20803;&#32452;&#65292;&#24573;&#35270;&#20102;&#26356;&#24191;&#27867;&#30340;LLM&#33021;&#21147;&#65292;&#22914;&#36923;&#36753;&#25512;&#29702;&#21644;&#38405;&#35835;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20197;&#19979;&#19977;&#28857;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#65306;&#65288;i&#65289;&#25105;&#20204;&#25506;&#32034;&#20102;ME&#22914;&#20309;&#24433;&#21709;LLMs&#30340;&#24191;&#27867;&#22522;&#26412;&#33021;&#21147;&#22312;&#39034;&#24207;&#32534;&#36753;&#19979;&#12290;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65306;&#22823;&#22810;&#25968;&#20462;&#25913;&#21442;&#25968;ME&#22312;&#20960;&#27425;&#39034;&#24207;&#32534;&#36753;&#21518;&#19968;&#36143;&#38477;&#20302;&#25152;&#26377;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11122v1 Announce Type: cross  Abstract: Memory Editing (ME) has emerged as an efficient method to modify erroneous facts or inject new facts into Large Language Models (LLMs). Two mainstream ME methods exist: parameter-modifying ME and parameter-preserving ME (integrating extra modules while preserving original parameters). Regrettably, previous studies on ME evaluation have two critical limitations: (i) evaluating LLMs with single edit only, neglecting the need for continuous editing, and (ii) evaluations focusing solely on basic factual triples, overlooking broader LLM capabilities like logical reasoning and reading understanding. This study addresses these limitations with contributions threefold: (i) We explore how ME affects a wide range of fundamental capabilities of LLMs under sequential editing. Experimental results reveal an intriguing phenomenon: Most parameter-modifying ME consistently degrade performance across all tasks after a few sequential edits. In contrast,
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;TutorEval&#21644;TutorChat&#65292;&#36890;&#36807;TutorEval&#22522;&#20934;&#21487;&#20197;&#34913;&#37327;LMs&#20316;&#20026;&#31185;&#23398;&#21161;&#25163;&#30340;&#23454;&#38469;&#21487;&#29992;&#24615;&#65292;TutorChat&#25968;&#25454;&#38598;&#29992;&#20110;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11111</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31185;&#23398;&#23548;&#24072;
&lt;/p&gt;
&lt;p&gt;
Language Models as Science Tutors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11111
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;TutorEval&#21644;TutorChat&#65292;&#36890;&#36807;TutorEval&#22522;&#20934;&#21487;&#20197;&#34913;&#37327;LMs&#20316;&#20026;&#31185;&#23398;&#21161;&#25163;&#30340;&#23454;&#38469;&#21487;&#29992;&#24615;&#65292;TutorChat&#25968;&#25454;&#38598;&#29992;&#20110;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#26368;&#36817;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#65292;&#26397;&#30528;&#35757;&#32451;&#20855;&#26377;&#36739;&#24378;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#26041;&#21521;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#21457;&#23637;&#24182;&#27809;&#26377;&#19987;&#27880;&#20110;LMs&#22312;&#31185;&#23398;&#25945;&#32946;&#20013;&#30340;&#23454;&#38469;&#29992;&#20363;&#65292;&#21253;&#25324;&#38656;&#35201;&#22788;&#29702;&#38271;&#31687;&#31185;&#23398;&#25991;&#26723;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TutorEval&#21644;TutorChat&#12290;TutorEval&#26159;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#38382;&#31572;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#26377;&#20851;STEM&#25945;&#31185;&#20070;&#38271;&#31687;&#31456;&#33410;&#30340;&#38382;&#39064;&#65292;&#30001;&#19987;&#23478;&#32534;&#20889;&#12290;TutorEval&#26377;&#21161;&#20110;&#34913;&#37327;LMs&#20316;&#20026;&#31185;&#23398;&#21161;&#25163;&#30340;&#23454;&#38469;&#21487;&#29992;&#24615;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#32467;&#21512;&#38271;&#19978;&#19979;&#25991;&#12289;&#33258;&#30001;&#29983;&#25104;&#21644;&#22810;&#23398;&#31185;&#31185;&#23398;&#30693;&#35782;&#30340;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#29616;&#26377;&#23545;&#35805;&#25968;&#25454;&#38598;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20250;&#23548;&#33268;TutorEval&#24615;&#33021;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;TutorChat&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;80,000&#20010;&#20851;&#20110;&#25945;&#31185;&#20070;&#30340;&#38271;&#21512;&#25104;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;TutorChat&#26469;&#24494;&#35843;Llemma&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11111v1 Announce Type: cross  Abstract: NLP has recently made exciting progress toward training language models (LMs) with strong scientific problem-solving skills. However, model development has not focused on real-life use-cases of LMs for science, including applications in education that require processing long scientific documents. To address this, we introduce TutorEval and TutorChat. TutorEval is a diverse question-answering benchmark consisting of questions about long chapters from STEM textbooks, written by experts. TutorEval helps measure real-life usability of LMs as scientific assistants, and it is the first benchmark combining long contexts, free-form generation, and multi-disciplinary scientific knowledge. Moreover, we show that fine-tuning base models with existing dialogue datasets leads to poor performance on TutorEval. Therefore, we create TutorChat, a dataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to fine-tune Llemma models wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#35810;&#38382;&#36873;&#27665;&#26377;&#20851;&#23569;&#37327;&#20505;&#36873;&#20154;&#30340;&#25237;&#31080;&#35268;&#21017;&#35745;&#31639;&#38382;&#39064;&#65292;&#23436;&#20840;&#34920;&#24449;&#20102;&#21487;&#35745;&#31639;&#30340;&#20301;&#32622;&#35780;&#20998;&#35268;&#21017;&#38598;&#21512;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#20110;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#31639;&#27861;&#30830;&#23450;&#26368;&#22823;&#24471;&#20998;&#20505;&#36873;&#20154;&#24517;&#39035;&#36827;&#34892;&#30340;&#26597;&#35810;&#27425;&#25968;&#30340;&#21442;&#25968;&#21270;&#19978;&#38480;&#21644;&#19979;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.11104</link><description>&lt;p&gt;
&#29992;&#35810;&#38382;&#19981;&#23436;&#25972;&#36873;&#31080;&#35745;&#31639;&#25237;&#31080;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Computing Voting Rules with Elicited Incomplete Votes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#35810;&#38382;&#36873;&#27665;&#26377;&#20851;&#23569;&#37327;&#20505;&#36873;&#20154;&#30340;&#25237;&#31080;&#35268;&#21017;&#35745;&#31639;&#38382;&#39064;&#65292;&#23436;&#20840;&#34920;&#24449;&#20102;&#21487;&#35745;&#31639;&#30340;&#20301;&#32622;&#35780;&#20998;&#35268;&#21017;&#38598;&#21512;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#20110;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#31639;&#27861;&#30830;&#23450;&#26368;&#22823;&#24471;&#20998;&#20505;&#36873;&#20154;&#24517;&#39035;&#36827;&#34892;&#30340;&#26597;&#35810;&#27425;&#25968;&#30340;&#21442;&#25968;&#21270;&#19978;&#38480;&#21644;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#22312;&#22823;&#22411;&#20505;&#36873;&#20154;&#32676;&#20307;&#20013;&#35828;&#26126;&#23436;&#25972;&#24207;&#25968;&#20559;&#22909;&#30340;&#22256;&#38590;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#35810;&#38382;&#36873;&#27665;&#26377;&#20851; $t &lt; m$ &#20505;&#36873;&#20154;&#30340;&#25237;&#31080;&#35268;&#21017;&#12290;&#22312;&#25512;&#24191;&#20102;&#20851;&#20110;&#35813;&#38382;&#39064;&#29305;&#23450;&#24773;&#20917;&#30340;&#20808;&#21069;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#23436;&#20840;&#34920;&#24449;&#20102;&#23545;&#20110;&#20219;&#24847; $1 \leq t &lt; m$ &#21487;&#20197;&#35745;&#31639;&#24471;&#20986;&#30340;&#20301;&#32622;&#35780;&#20998;&#35268;&#21017;&#30340;&#38598;&#21512;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#20854;&#20013;&#24182;&#19981;&#21253;&#25324;&#22810;&#25968;&#21046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#36825;&#19968;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21333;&#27425;&#21487;&#36716;&#31227;&#25237;&#31080;&#65288;&#28120;&#27760;&#25237;&#31080;&#65289;&#30340;&#31867;&#20284;&#26080;&#27861;&#35745;&#31639;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#36127;&#38754;&#32467;&#26524;&#26159;&#20449;&#24687;&#29702;&#35770;&#30340;&#65292;&#19981;&#20851;&#24515;&#26597;&#35810;&#30340;&#25968;&#37327;&#12290;&#26368;&#21518;&#65292;&#23545;&#20110;&#21487;&#20197;&#20351;&#29992;&#26377;&#38480;&#22823;&#23567;&#26597;&#35810;&#35745;&#31639;&#30340;&#35780;&#20998;&#35268;&#21017;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#21442;&#25968;&#21270;&#30340;&#20851;&#20110;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#31639;&#27861;&#24517;&#39035;&#20570;&#20986;&#30340;&#26597;&#35810;&#25968;&#37327;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#30830;&#23450;&#24615;&#31639;&#27861;&#30340;&#30028;&#38480;&#20043;&#38388;&#27809;&#26377;&#24046;&#36317;&#65292;&#20294;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11104v1 Announce Type: cross  Abstract: Motivated by the difficulty of specifying complete ordinal preferences over a large set of $m$ candidates, we study voting rules that are computable by querying voters about $t &lt; m$ candidates. Generalizing prior works that focused on specific instances of this problem, our paper fully characterizes the set of positional scoring rules that can be computed for any $1 \leq t &lt; m$, which notably does not include plurality. We then extend this to show a similar impossibility result for single transferable vote (elimination voting). These negative results are information-theoretic and agnostic to the number of queries. Finally, for scoring rules that are computable with limited-sized queries, we give parameterized upper and lower bounds on the number of such queries a deterministic or randomized algorithm must make to determine the score-maximizing candidate. While there is no gap between our bounds for deterministic algorithms, identifying
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#26694;&#26550;&#65292;&#22312;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#25506;&#31350;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;DALLE-3&#22312;&#24615;&#21035;&#32844;&#19994;&#21644;&#32452;&#32455;&#26435;&#21147;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11089</link><description>&lt;p&gt;
&#30007;&#24615;CEO&#21644;&#22899;&#24615;&#21161;&#29702;&#65306;&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#25506;&#31350;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
The Male CEO and the Female Assistant: Probing Gender Biases in Text-To-Image Models Through Paired Stereotype Test
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11089
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#26694;&#26550;&#65292;&#22312;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#25506;&#31350;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;DALLE-3&#22312;&#24615;&#21035;&#32844;&#19994;&#21644;&#32452;&#32455;&#26435;&#21147;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#65288;&#22914;DALLE-3&#65289;&#23637;&#31034;&#20102;&#22312;&#26032;&#24212;&#29992;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#20844;&#24179;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#21333;&#20154;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;T2I&#27169;&#22411;&#24212;&#29992;&#21487;&#33021;&#38656;&#35201;&#21516;&#26102;&#25551;&#32472;&#20004;&#20010;&#25110;&#26356;&#22810;&#20154;&#12290;&#35813;&#35774;&#23450;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#20173;&#26410;&#34987;&#25506;&#31350;&#65292;&#23548;&#33268;&#20351;&#29992;&#20013;&#30340;&#20844;&#24179;&#30456;&#20851;&#39118;&#38505;&#12290;&#20026;&#20102;&#30740;&#31350;T2I&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#20559;&#35265;&#35780;&#20272;&#26694;&#26550;&#12290;PST&#20419;&#20351;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#22270;&#20687;&#20013;&#30340;&#20004;&#20010;&#20010;&#20307;&#65292;&#29992;&#19982;&#30456;&#21453;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30456;&#20851;&#32852;&#30340;&#20004;&#20010;&#31038;&#20250;&#36523;&#20221;&#26469;&#25551;&#36848;&#20182;&#20204;&#12290;&#36890;&#36807;&#29983;&#25104;&#30340;&#22270;&#20687;&#36981;&#20174;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30340;&#31243;&#24230;&#26469;&#34913;&#37327;&#20559;&#35265;&#12290;&#21033;&#29992;PST&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#35780;&#20272;DALLE-3&#65306;&#24615;&#21035;&#32844;&#19994;&#20013;&#30340;&#20559;&#35265;&#21644;&#32452;&#32455;&#26435;&#21147;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11089v1 Announce Type: cross  Abstract: Recent large-scale Text-To-Image (T2I) models such as DALLE-3 demonstrate great potential in new applications, but also face unprecedented fairness challenges. Prior studies revealed gender biases in single-person image generation, but T2I model applications might require portraying two or more people simultaneously. Potential biases in this setting remain unexplored, leading to fairness-related risks in usage. To study these underlying facets of gender biases in T2I models, we propose a novel Paired Stereotype Test (PST) bias evaluation framework. PST prompts the model to generate two individuals in the same image. They are described with two social identities that are stereotypically associated with the opposite gender. Biases can then be measured by the level of conformation to gender stereotypes in generated images. Using PST, we evaluate DALLE-3 from 2 perspectives: biases in gendered occupation and biases in organizational power.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;AI&#23433;&#20840;&#30171;&#28857;&#37329;&#23383;&#22612;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#29702;&#35299;&#21644;&#24212;&#23545;&#21508;&#20010;&#32423;&#21035;&#30340;AI&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.11082</link><description>&lt;p&gt;
AI&#23433;&#20840;&#30171;&#28857;&#37329;&#23383;&#22612;
&lt;/p&gt;
&lt;p&gt;
The AI Security Pyramid of Pain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11082
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;AI&#23433;&#20840;&#30171;&#28857;&#37329;&#23383;&#22612;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#29702;&#35299;&#21644;&#24212;&#23545;&#21508;&#20010;&#32423;&#21035;&#30340;AI&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;AI&#23433;&#20840;&#30171;&#28857;&#37329;&#23383;&#22612;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#32593;&#32476;&#23433;&#20840;&#30171;&#28857;&#37329;&#23383;&#22612;&#24212;&#29992;&#20110;AI&#29305;&#23450;&#23041;&#32961;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20026;&#29702;&#35299;&#21644;&#35299;&#20915;&#21508;&#20010;&#32423;&#21035;&#30340;AI&#23041;&#32961;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#12290;&#20174;&#22522;&#30784;&#24320;&#22987;&#65292;&#37329;&#23383;&#22612;&#24378;&#35843;&#20102;&#25968;&#25454;&#23436;&#25972;&#24615;&#65292;&#36825;&#23545;&#20110;&#25968;&#25454;&#38598;&#21644;AI&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26435;&#37325;&#21644;&#21442;&#25968;&#12290;&#30830;&#20445;&#25968;&#25454;&#23436;&#25972;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#25903;&#25745;&#20102;&#25152;&#26377;&#22522;&#20110;AI&#30340;&#20915;&#31574;&#21644;&#36816;&#33829;&#30340;&#26377;&#25928;&#24615;&#12290;&#19979;&#19968;&#20010;&#32423;&#21035;&#65292;AI&#31995;&#32479;&#24615;&#33021;&#65292;&#20851;&#27880;MLOps&#39537;&#21160;&#30340;&#25351;&#26631;&#65292;&#22914;&#27169;&#22411;&#28418;&#31227;&#12289;&#20934;&#30830;&#24615;&#21644;&#35823;&#25253;&#29575;&#12290;&#36825;&#20123;&#25351;&#26631;&#23545;&#20110;&#26816;&#27979;&#28508;&#22312;&#30340;&#23433;&#20840;&#28431;&#27934;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#36827;&#34892;&#26089;&#26399;&#24178;&#39044;&#24182;&#32500;&#25252;AI&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;&#12290;&#36827;&#19968;&#27493;&#28145;&#20837;&#65292;&#37329;&#23383;&#22612;&#35299;&#20915;&#20102;&#25932;&#23545;&#24037;&#20855;&#24102;&#26469;&#30340;&#23041;&#32961;&#65292;&#35782;&#21035;&#24182;&#20013;&#21644;&#34987;&#29992;&#20110;adversarial&#25915;&#20987;&#30340;&#24037;&#20855;&#31561;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11082v1 Announce Type: cross  Abstract: We introduce the AI Security Pyramid of Pain, a framework that adapts the cybersecurity Pyramid of Pain to categorize and prioritize AI-specific threats. This framework provides a structured approach to understanding and addressing various levels of AI threats. Starting at the base, the pyramid emphasizes Data Integrity, which is essential for the accuracy and reliability of datasets and AI models, including their weights and parameters. Ensuring data integrity is crucial, as it underpins the effectiveness of all AI-driven decisions and operations. The next level, AI System Performance, focuses on MLOps-driven metrics such as model drift, accuracy, and false positive rates. These metrics are crucial for detecting potential security breaches, allowing for early intervention and maintenance of AI system integrity. Advancing further, the pyramid addresses the threat posed by Adversarial Tools, identifying and neutralizing tools used by ad
&lt;/p&gt;</description></item><item><title>&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11078</link><description>&lt;p&gt;
&#36890;&#36807;&#32431;&#24494;&#35843;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Model Editing by Pure Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11078
&lt;/p&gt;
&lt;p&gt;
&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#35843;&#25972;&#34987;&#35748;&#20026;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#19981;&#22815;&#26377;&#25928;&#65292;&#22240;&#20026;&#30456;&#23545;&#26356;&#19987;&#19994;&#30340;&#26041;&#27861;&#32780;&#35328;&#65292;&#23427;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#26159;&#31616;&#21333;&#30340;&#65292;&#19981;&#20851;&#24515;&#34987;&#32534;&#36753;&#27169;&#22411;&#30340;&#20307;&#31995;&#32467;&#26500;&#32454;&#33410;&#65292;&#24182;&#19988;&#33021;&#22815;&#21033;&#29992;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#30340;&#19981;&#26029;&#36827;&#23637;&#65288;&#20363;&#22914;PEFT&#65289;&#65292;&#20351;&#20854;&#25104;&#20026;&#27169;&#22411;&#32534;&#36753;&#22120;&#30340;&#21560;&#24341;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32431;&#31929;&#30340;&#24494;&#35843;&#21487;&#20197;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#26420;&#32032;&#24494;&#35843;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#32780;&#38750;&#23436;&#25972;&#20284;&#28982;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#26469;&#22686;&#21152;&#25968;&#25454;&#65292;&#20197;&#40723;&#21169;&#27867;&#21270;&#21644;&#23616;&#37096;&#24615;&#12290;&#25105;&#20204;&#22312;ZsRE&#21644;CounterFact&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#19968;&#31616;&#21333;&#20462;&#25913;&#20351;&#24471;&#24494;&#35843;&#36890;&#24120;&#21487;&#20197;&#19982;&#19987;&#19994;&#32534;&#36753;&#22120;&#22312;&#32534;&#36753;&#20998;&#25968;&#26041;&#38754;&#21305;&#25932;&#29978;&#33267;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11078v1 Announce Type: cross  Abstract: Fine-tuning is dismissed as not effective for model editing due to its poor performance compared to more specialized methods. However, fine-tuning is simple, agnostic to the architectural details of the model being edited, and able to leverage ongoing advances in standard training methods (e.g., PEFT), making it an appealing choice for a model editor. In this work, we show that pure fine-tuning can be a viable approach to model editing. We propose a slight modification of naive fine-tuning with two key ingredients. First, we optimize the conditional likelihood rather than the full likelihood. Second, we augment the data with random paraphrases and facts to encourage generalization and locality. Our experiments on ZsRE and CounterFact show that this simple modification allows fine-tuning to often match or outperform specialized editors in the edit score.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550; AFaCTA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#30340;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11073</link><description>&lt;p&gt;
AFaCTA: &#20351;&#29992;&#21487;&#38752;&#30340;LLM&#26631;&#27880;&#32773;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#30340;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11073
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550; AFaCTA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#30340;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#65292;&#29992;&#20110;&#25171;&#20987;&#35823;&#23548;&#20449;&#24687;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#65292;&#21363;&#20107;&#23454;&#26680;&#26597;&#31649;&#36947;&#20013;&#30340;&#31532;&#19968;&#27493;&#65292;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#21487;&#20280;&#32553;&#24615;&#21644;&#27867;&#21270;&#24615;&#65306;&#65288;1&#65289;&#20219;&#21153;&#23450;&#20041;&#21644;&#32034;&#36180;&#27010;&#24565;&#30340;&#19981;&#19968;&#33268;&#24615;&#20197;&#21450;&#65288;2&#65289;&#25163;&#21160;&#26631;&#27880;&#30340;&#39640;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#65288;1&#65289;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#30456;&#20851;&#24037;&#20316;&#20013;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32858;&#28966;&#20110;&#21487;&#39564;&#35777;&#24615;&#30340;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#32479;&#19968;&#23450;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#65288;2&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AFaCTA&#65288;&#33258;&#21160;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#26631;&#27880;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#20013;&#25552;&#20379;&#24110;&#21161;&#12290;AFaCTA&#36890;&#36807;&#27839;&#30528;&#19977;&#26465;&#39044;&#23450;&#20041;&#30340;&#25512;&#29702;&#36335;&#24452;&#20445;&#25345;&#19968;&#33268;&#24615;&#26469;&#26657;&#20934;&#20854;&#27880;&#37322;&#30340;&#32622;&#20449;&#24230;&#12290;&#22312;&#25919;&#27835;&#35328;&#35770;&#39046;&#22495;&#30340;&#22823;&#37327;&#35780;&#20272;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;AFaCTA&#33021;&#22815;&#39640;&#25928;&#22320;&#21327;&#21161;&#19987;&#19994;&#20154;&#21592;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11073v1 Announce Type: cross  Abstract: With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT4&#65289;&#25972;&#21512;&#21040;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#26102;&#23545;&#20803;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#21019;&#26032;&#21033;&#29992;&#65292;&#24378;&#35843;&#20102;LLMs&#22312;&#22686;&#24378;&#20256;&#32479;CD&#26041;&#27861;&#21644;&#20316;&#20026;&#19987;&#23478;&#36741;&#21161;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11068</link><description>&lt;p&gt;
&#26550;&#36215;&#22240;&#26524;&#21457;&#29616;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#26725;&#26753;&#65306;&#25972;&#21512;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Bridging Causal Discovery and Large Language Models: A Comprehensive Survey of Integrative Approaches and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT4&#65289;&#25972;&#21512;&#21040;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#26102;&#23545;&#20803;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#21019;&#26032;&#21033;&#29992;&#65292;&#24378;&#35843;&#20102;LLMs&#22312;&#22686;&#24378;&#20256;&#32479;CD&#26041;&#27861;&#21644;&#20316;&#20026;&#19987;&#23478;&#36741;&#21161;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#65288;CD&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#30528;&#20004;&#20010;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#39046;&#22495;&#12290;&#23613;&#31649;&#23427;&#20204;&#36215;&#28304;&#19981;&#21516;&#65292;CD&#20391;&#37325;&#20110;&#20174;&#25968;&#25454;&#20013;&#25581;&#31034;&#22240;&#26524;&#20851;&#31995;&#65292;LLMs&#21017;&#20391;&#37325;&#20110;&#22788;&#29702;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#25991;&#26412;&#65292;&#20294;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#20026;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#35265;&#35299;&#21644;&#26041;&#27861;&#35770;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;LLMs&#65288;&#22914;GPT4&#65289;&#25972;&#21512;&#21040;CD&#20219;&#21153;&#20013;&#30340;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#23457;&#26597;&#21644;&#27604;&#36739;&#20102;&#21033;&#29992;LLMs&#36827;&#34892;&#21508;&#31181;CD&#20219;&#21153;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20102;&#23427;&#20204;&#23545;&#20803;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#21019;&#26032;&#21033;&#29992;&#20197;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;LLMs&#22312;&#22686;&#24378;&#20256;&#32479;CD&#26041;&#27861;&#21644;&#20316;&#20026;&#19981;&#23436;&#32654;&#19987;&#23478;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#24403;&#21069;&#23454;&#36341;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11068v1 Announce Type: cross  Abstract: Causal discovery (CD) and Large Language Models (LLMs) represent two emerging fields of study with significant implications for artificial intelligence. Despite their distinct origins, CD focuses on uncovering cause-effect relationships from data, and LLMs on processing and generating humanlike text, the convergence of these domains offers novel insights and methodologies for understanding complex systems. This paper presents a comprehensive survey of the integration of LLMs, such as GPT4, into CD tasks. We systematically review and compare existing approaches that leverage LLMs for various CD tasks and highlight their innovative use of metadata and natural language to infer causal structures. Our analysis reveals the strengths and potential of LLMs in both enhancing traditional CD methods and as an imperfect expert, alongside the challenges and limitations inherent in current practices. Furthermore, we identify gaps in the literature 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102; Persona-DB&#65292;&#19968;&#20010;&#31616;&#21333;&#21364;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#32423;&#26500;&#24314;&#36807;&#31243;&#21644;&#21327;&#21516;&#20248;&#21270;&#65292;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#20013;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26816;&#32034;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11060</link><description>&lt;p&gt;
Persona-DB&#65306;&#29992;&#20110;&#21709;&#24212;&#39044;&#27979;&#30340;&#39640;&#25928;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#19982;&#21327;&#21516;&#25968;&#25454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11060
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102; Persona-DB&#65292;&#19968;&#20010;&#31616;&#21333;&#21364;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#32423;&#26500;&#24314;&#36807;&#31243;&#21644;&#21327;&#21516;&#20248;&#21270;&#65292;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#20013;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26816;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20010;&#24615;&#21270;&#20132;&#20114;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#20934;&#30830;&#24555;&#36895;&#35782;&#21035;&#29992;&#25143;&#24847;&#35265;&#21644;&#20559;&#22909;&#30340;&#26041;&#27861;&#12290;&#26816;&#32034;&#22686;&#24378;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#31574;&#30053;&#20986;&#29616;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36866;&#24212;&#22823;&#37327;&#29992;&#25143;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#26816;&#32034;&#38454;&#27573;&#65292;&#24182;&#23545;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#20248;&#21270;&#36827;&#34892;&#20102;&#26377;&#38480;&#30340;&#25506;&#32034;&#65292;&#36825;&#26159;&#20010;&#24615;&#21270;&#31561;&#20219;&#21153;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#30528;&#37325;&#20110;&#22914;&#20309;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#25968;&#25454;&#65292;&#20197;&#20415;&#22312;LLM&#23450;&#21046;&#30340;&#24773;&#22659;&#19979;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#26816;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Persona-DB&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#20998;&#23618;&#26500;&#24314;&#36807;&#31243;&#65292;&#20197;&#25913;&#21892;&#36328;&#20219;&#21153;&#32972;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#34892;&#21327;&#21516;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11060v1 Announce Type: cross  Abstract: The increasing demand for personalized interactions with large language models (LLMs) calls for the development of methodologies capable of accurately and efficiently identifying user opinions and preferences. Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more efficient retrieval in the context of LLM customization. To tackle this challenge, we introduce Persona-DB, a simple yet effective framework consisting of a hierarchical construction process to improve generalization across task contexts and collaborative refinement to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Conan&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#20390;&#25506;&#21465;&#20107;&#20013;&#25552;&#21462;&#21644;&#20998;&#26512;&#22797;&#26434;&#30340;&#20154;&#29289;&#20851;&#31995;&#22270;&#65292;&#24182;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#22797;&#26434;&#20851;&#31995;&#21644;&#22788;&#29702;&#38271;&#31687;&#21465;&#20107;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11051</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30701;&#26495;&#65306;&#29702;&#35299;&#20390;&#25506;&#21465;&#20107;&#20013;&#22797;&#26434;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11051
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Conan&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#20390;&#25506;&#21465;&#20107;&#20013;&#25552;&#21462;&#21644;&#20998;&#26512;&#22797;&#26434;&#30340;&#20154;&#29289;&#20851;&#31995;&#22270;&#65292;&#24182;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#22797;&#26434;&#20851;&#31995;&#21644;&#22788;&#29702;&#38271;&#31687;&#21465;&#20107;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21465;&#20107;&#29702;&#35299;&#25968;&#25454;&#38598;&#24448;&#24448;&#26080;&#27861;&#34920;&#36798;&#29616;&#23454;&#31038;&#20250;&#22330;&#26223;&#20013;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Conan&#65292;&#26088;&#22312;&#20174;&#20390;&#25506;&#21465;&#20107;&#20013;&#25552;&#21462;&#21644;&#20998;&#26512;&#22797;&#26434;&#30340;&#20154;&#29289;&#20851;&#31995;&#22270;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20998;&#23618;&#20851;&#31995;&#31867;&#21035;&#65292;&#24182;&#20174;&#19981;&#21516;&#35282;&#33394;&#30340;&#35282;&#24230;&#25163;&#21160;&#25552;&#21462;&#21644;&#27880;&#37322;&#20102;&#20197;&#35282;&#33394;&#20026;&#23548;&#21521;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;&#22823;&#22810;&#25968;&#35282;&#33394;&#30693;&#26195;&#30340;&#20844;&#24320;&#20851;&#31995;&#21644;&#20165;&#23569;&#25968;&#35282;&#33394;&#30693;&#26195;&#30340;&#31192;&#23494;&#20851;&#31995;&#12290;&#25105;&#20204;&#23545;GPT-3.5&#12289;GPT-4&#21644;Llama2&#31561;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#25512;&#29702;&#22797;&#26434;&#20851;&#31995;&#21644;&#22788;&#29702;&#36739;&#38271;&#21465;&#20107;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;Conan&#25968;&#25454;&#38598;&#19982;&#25105;&#20204;&#30340;&#27969;&#31243;&#31574;&#30053;&#30340;&#32467;&#21512;&#26088;&#22312;&#20102;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#21465;&#20107;&#32972;&#26223;&#20013;&#24494;&#22937;&#20851;&#31995;&#21160;&#24577;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11051v1 Announce Type: cross  Abstract: Existing datasets for narrative understanding often fail to represent the complexity and uncertainty of relationships in real-life social scenarios. To address this gap, we introduce a new benchmark, Conan, designed for extracting and analysing intricate character relation graphs from detective narratives. Specifically, we designed hierarchical relationship categories and manually extracted and annotated role-oriented relationships from the perspectives of various characters, incorporating both public relationships known to most characters and secret ones known to only a few. Our experiments with advanced Large Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their limitations in inferencing complex relationships and handling longer narratives. The combination of the Conan dataset and our pipeline strategy is geared towards understanding the ability of LLMs to comprehend nuanced relational dynamics in narrative contexts.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20986;&#21709;&#24212;&#26102;&#23384;&#22312;&#19968;&#20010;&#20215;&#20540;&#20559;&#22909;&#30340;&#26426;&#21046;&#65292;&#20542;&#21521;&#20110;&#20559;&#21521;&#29702;&#24819;&#29366;&#24577;&#65292;&#36825;&#31181;&#20559;&#24046;&#20250;&#23545;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11005</link><description>&lt;p&gt;
&#25506;&#31350;&#20215;&#20540;&#20559;&#22909;&#65306;LLMs&#20559;&#21521;&#29702;&#24819;&#29366;&#24577;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Exploring Value Biases: How LLMs Deviate Towards the Ideal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11005
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20986;&#21709;&#24212;&#26102;&#23384;&#22312;&#19968;&#20010;&#20215;&#20540;&#20559;&#22909;&#30340;&#26426;&#21046;&#65292;&#20542;&#21521;&#20110;&#20559;&#21521;&#29702;&#24819;&#29366;&#24577;&#65292;&#36825;&#31181;&#20559;&#24046;&#20250;&#23545;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#37096;&#32626;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#21709;&#24212;&#23545;&#31038;&#20250;&#20135;&#29983;&#30528;&#36234;&#26469;&#36234;&#22823;&#30340;&#24433;&#21709;&#12290;&#29702;&#35299;LLMs&#22312;&#32473;&#20986;&#21709;&#24212;&#26102;&#30340;&#38750;&#25925;&#24847;&#26426;&#21046;&#23545;&#20110;&#35299;&#37322;&#23427;&#20204;&#30340;&#24615;&#33021;&#24182;&#36776;&#21035;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#20559;&#24046;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31867;&#20284;&#20110;&#20154;&#31867;&#30740;&#31350;&#20013;&#65292;&#36825;&#31181;&#26080;&#24847;&#35782;&#30340;&#21709;&#24212;&#34987;&#31216;&#20026;&#25277;&#26679;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#36825;&#31181;&#25277;&#26679;&#29616;&#35937;&#65292;&#21457;&#29616;LLMs&#30340;&#25277;&#26679;&#20542;&#21521;&#20110;&#20559;&#29233;&#39640;&#20215;&#20540;&#36873;&#39033;&#12290;&#20215;&#20540;&#20559;&#22909;&#23545;&#24212;&#20110;&#20174;&#26368;&#21487;&#33021;&#30340;&#21709;&#24212;&#21521;LLM&#20013;&#20195;&#34920;&#30340;&#29702;&#24819;&#20215;&#20540;&#30340;&#36716;&#21464;&#12290;&#23454;&#38469;&#19978;&#65292;&#21363;&#20415;&#26159;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#23398;&#20064;&#21040;&#30340;&#26032;&#23454;&#20307;&#65292;&#36825;&#31181;&#25928;&#26524;&#20063;&#33021;&#22815;&#20877;&#29616;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#20559;&#24046;&#34920;&#29616;&#22312;&#24847;&#24819;&#19981;&#21040;&#30340;&#22320;&#26041;&#65292;&#24182;&#23545;&#36873;&#25321;&#20856;&#22411;&#23454;&#20363;&#31561;&#30456;&#20851;&#24212;&#29992;&#22330;&#26223;&#20135;&#29983;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20215;&#20540;&#20559;&#22909;&#22312;&#19981;&#21516;&#20998;&#31867;&#30340;LLMs&#20013;&#37117;&#24456;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11005v1 Announce Type: cross  Abstract: Large-Language-Models (LLMs) are deployed in a wide range of applications, and their response has an increasing social impact. Understanding the non-deliberate(ive) mechanism of LLMs in giving responses is essential in explaining their performance and discerning their biases in real-world applications. This is analogous to human studies, where such inadvertent responses are referred to as sampling. We study this sampling of LLMs in light of value bias and show that the sampling of LLMs tends to favour high-value options. Value bias corresponds to this shift of response from the most likely towards an ideal value represented in the LLM. In fact, this effect can be reproduced even with new entities learnt via in-context prompting. We show that this bias manifests in unexpected places and has implications on relevant application scenarios, like choosing exemplars. The results show that value bias is strong in LLMs across different categor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;ASGEA&#65292;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35774;&#35745;&#20102;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;ASGNN&#65292;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#39564;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.11000</link><description>&lt;p&gt;
ASGEA&#65306;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#36827;&#34892;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;ASGEA&#65292;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35774;&#35745;&#20102;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;ASGNN&#65292;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#26088;&#22312;&#35782;&#21035;&#20195;&#34920;&#30456;&#21516;&#29616;&#23454;&#19990;&#30028;&#23545;&#35937;&#30340;&#19981;&#21516;&#30693;&#35782;&#22270;&#20013;&#30340;&#23454;&#20307;&#12290;&#26368;&#36817;&#22522;&#20110;&#23884;&#20837;&#30340;EA&#26041;&#27861;&#22312;EA&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#38754;&#20020;&#30528;&#35299;&#37322;&#24615;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#23436;&#20840;&#20381;&#36182;&#20110;&#23884;&#20837;&#36317;&#31163;&#65292;&#24182;&#24573;&#35270;&#20102;&#19968;&#23545;&#23545;&#40784;&#23454;&#20307;&#32972;&#21518;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Align-Subgraph&#23454;&#20307;&#23545;&#40784;&#65288;ASGEA&#65289;&#26694;&#26550;&#26469;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;ASGEA&#20351;&#29992;&#38170;&#38142;&#25509;&#20316;&#20026;&#26725;&#26753;&#26469;&#26500;&#24314;Align-Subgraphs&#65292;&#24182;&#27839;&#30528;&#36328;&#30693;&#35782;&#22270;&#30340;&#36335;&#24452;&#20256;&#25773;&#65292;&#36825;&#20351;&#20854;&#21306;&#21035;&#20110;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;ASGNN&#65292;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#25972;&#21512;&#36328;&#30693;&#35782;&#22270;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#33410;&#28857;&#32423;&#22810;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#32467;&#21512;&#22810;&#27169;&#24577;&#22686;&#24378;&#30340;&#38170;&#28857;&#26469;&#22686;&#24378;Align-Subgraph&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11000v1 Announce Type: cross  Abstract: Entity alignment (EA) aims to identify entities across different knowledge graphs that represent the same real-world objects. Recent embedding-based EA methods have achieved state-of-the-art performance in EA yet faced interpretability challenges as they purely rely on the embedding distance and neglect the logic rules behind a pair of aligned entities. In this paper, we propose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic rules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct Align-Subgraphs and spreads along the paths across KGs, which distinguishes it from the embedding-based methods. Furthermore, we design an interpretable Path-based Graph Neural Network, ASGNN, to effectively identify and integrate the logic rules across KGs. We also introduce a node-level multi-modal attention mechanism coupled with multi-modal enriched anchors to augment the Align-Subgraph. Our experimental results 
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#27835;&#30103;&#26041;&#26696;&#38656;&#27880;&#24847;&#24739;&#32773;&#21097;&#20313;&#29983;&#21629;&#21644;&#21512;&#24182;&#30151;&#65292;&#30740;&#31350;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26500;&#24314;&#22810;&#31867;&#20998;&#31867;&#27169;&#22411;&#26469;&#39044;&#27979;&#32769;&#24180;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.10999</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#39044;&#27979;2&#22411;&#31958;&#23615;&#30149;&#32769;&#24180;&#24739;&#32773;&#30340;&#22810;&#31867;&#20998;&#31867;&#27515;&#20129;&#29575;
&lt;/p&gt;
&lt;p&gt;
Analysis and Mortality Prediction using Multiclass Classification for Older Adults with Type 2 Diabetes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10999
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#27835;&#30103;&#26041;&#26696;&#38656;&#27880;&#24847;&#24739;&#32773;&#21097;&#20313;&#29983;&#21629;&#21644;&#21512;&#24182;&#30151;&#65292;&#30740;&#31350;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26500;&#24314;&#22810;&#31867;&#20998;&#31867;&#27169;&#22411;&#26469;&#39044;&#27979;&#32769;&#24180;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#21512;&#36866;&#30340;&#27835;&#30103;&#26041;&#26696;&#26469;&#31649;&#29702;&#31958;&#23615;&#30149;&#35201;&#27714;&#21307;&#25252;&#20154;&#21592;&#27880;&#24847;&#24739;&#32773;&#21097;&#20313;&#30340;&#29983;&#21629;&#20197;&#21450;&#24433;&#21709;&#20182;&#20204;&#30340;&#21512;&#24182;&#30151;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;68&#20010;&#28508;&#22312;&#27515;&#20129;&#39044;&#27979;&#22240;&#23376;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;275,190&#21517;&#24180;&#40836;&#22312;65&#23681;&#25110;&#20197;&#19978;&#30340;&#32654;&#22269;&#36864;&#20237;&#20891;&#20154;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#36890;&#36807;&#23558;&#20004;&#20010;&#21407;&#22987;&#30446;&#26631;&#21464;&#37327;&#32452;&#21512;&#36215;&#26469;&#21019;&#36896;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#21464;&#37327;&#12290;&#36890;&#36807;&#31163;&#25955;&#21270;&#36830;&#32493;&#21464;&#37327;&#26469;&#22788;&#29702;&#24322;&#24120;&#20540;&#65292;&#23545;&#20998;&#31867;&#21464;&#37327;&#36827;&#34892;&#20102;&#34394;&#25311;&#32534;&#30721;&#12290;&#36890;&#36807;&#38543;&#26426;&#27424;&#37319;&#26679;&#23454;&#29616;&#20102;&#31867;&#24179;&#34913;&#12290;&#20351;&#29992;&#24102;LASSO&#30340;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#24314;&#31435;&#20102;&#22522;&#20934;&#22238;&#24402;&#27169;&#22411;&#12290;&#37319;&#29992;&#21345;&#26041;&#26816;&#39564;&#21644;&#20449;&#24687;&#22686;&#30410;&#20316;&#20026;&#22522;&#20110;&#36807;&#28388;&#30340;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#12290;&#20998;&#31867;&#22120;&#21253;&#25324;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;&#65288;XGB&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10999v1 Announce Type: cross  Abstract: Designing proper treatment plans to manage diabetes requires health practitioners to pay heed to the individuals remaining life along with the comorbidities affecting them. Older adults with Type 2 Diabetes Mellitus (T2DM) are prone to experience premature death or even hypoglycaemia. The structured dataset utilized has 68 potential mortality predictors for 275,190 diabetic U.S. military Veterans aged 65 years or older. A new target variable is invented by combining the two original target variables. Outliers are handled by discretizing the continuous variables. Categorical variables have been dummy encoded. Class balancing is achieved by random under-sampling. A benchmark regression model is built using Multinomial Logistic Regression with LASSO. Chi-Squared and Information Gain are the filter-based feature selection techniques utilized. Classifiers such as Multinomial Logistic Regression, Random Forest, Extreme Gradient Boosting (XGB
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#19982;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30456;&#32467;&#21512;&#30340;VerSAILLE&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#19978;&#30340;&#23433;&#20840;&#24615;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.10998</link><description>&lt;p&gt;
&#36890;&#36807;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Provably Safe Neural Network Controllers via Differential Dynamic Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10998
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#19982;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30456;&#32467;&#21512;&#30340;VerSAILLE&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#19978;&#30340;&#23433;&#20840;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#20316;&#20026;&#38754;&#21521;&#30446;&#26631;&#30340;&#25511;&#21046;&#22120;&#22312;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#39564;&#35777;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25511;&#21046;&#31995;&#32479;&#65288;NNCS&#65289;&#30340;&#23433;&#20840;&#24615;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;NN&#26469;&#35828;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#23545;&#26080;&#30028;&#26102;&#38388;&#33539;&#22260;&#36827;&#34892;&#23433;&#20840;&#24615;&#39564;&#35777;&#26102;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;VerSAILLE&#65288;&#36890;&#36807;&#36923;&#36753;&#38142;&#25509;&#21253;&#39564;&#35777;&#30340;&#21487;&#39564;&#35777;&#23433;&#20840;&#20154;&#24037;&#26234;&#33021;&#65289;&#65306;&#36825;&#26159;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#65288;dL&#65289;&#21644;NN&#39564;&#35777;&#32452;&#21512;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#20316;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;NN&#39564;&#35777;&#24037;&#20855;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#30041;dL&#30340;&#20005;&#35880;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25511;&#21046;&#22120;&#20449;&#23553;&#30340;&#23433;&#20840;&#24615;&#35777;&#26126;&#65292;&#20197;&#35777;&#26126;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#19978;&#20855;&#20307;NNCS&#30340;&#23433;&#20840;&#24615;&#12290;VerSAILLE&#23548;&#33268;&#30340;NN&#39564;&#35777;&#23646;&#24615;&#36890;&#24120;&#38656;&#35201;&#38750;&#32447;&#24615;&#31639;&#26415;&#65292;&#32780;&#39640;&#25928;&#30340;NN&#39564;&#35777;&#24037;&#20855;&#20165;&#25903;&#25345;&#32447;&#24615;&#31639;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10998v1 Announce Type: cross  Abstract: While neural networks (NNs) have a large potential as goal-oriented controllers for Cyber-Physical Systems, verifying the safety of neural network based control systems (NNCSs) poses significant challenges for the practical use of NNs -- especially when safety is needed for unbounded time horizons. One reason for this is the intractability of NN and hybrid system analysis. We introduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The first approach for the combination of differential dynamic logic (dL) and NN verification. By joining forces, we can exploit the efficiency of NN verification tools while retaining the rigor of dL. We reflect a safety proof for a controller envelope in an NN to prove the safety of concrete NNCS on an infinite-time horizon. The NN verification properties resulting from VerSAILLE typically require nonlinear arithmetic while efficient NN verification tools merely support linear arithmetic. T
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20102;&#23545;&#35821;&#20041;&#30340;&#28176;&#36827;&#29702;&#35299;&#65292;&#36890;&#36807;&#24212;&#29992;&#24515;&#28789;&#21746;&#23398;&#21644;&#35821;&#35328;&#23398;&#20013;&#20851;&#20110;&#21547;&#20041;&#30340;&#26680;&#24515;&#20551;&#35774;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#19981;&#20165;&#20165;&#26159;&#29983;&#25104;&#25991;&#26412;&#30340;&#24037;&#20855;&#65292;&#32780;&#26159;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#24050;&#32463;&#29702;&#35299;&#20102;&#23427;&#20204;&#29983;&#25104;&#30340;&#35821;&#35328;&#12290;</title><link>https://arxiv.org/abs/2402.10992</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#65306;&#35821;&#20041;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
"Understanding AI": Semantic Grounding in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10992
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20102;&#23545;&#35821;&#20041;&#30340;&#28176;&#36827;&#29702;&#35299;&#65292;&#36890;&#36807;&#24212;&#29992;&#24515;&#28789;&#21746;&#23398;&#21644;&#35821;&#35328;&#23398;&#20013;&#20851;&#20110;&#21547;&#20041;&#30340;&#26680;&#24515;&#20551;&#35774;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#19981;&#20165;&#20165;&#26159;&#29983;&#25104;&#25991;&#26412;&#30340;&#24037;&#20855;&#65292;&#32780;&#26159;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#24050;&#32463;&#29702;&#35299;&#20102;&#23427;&#20204;&#29983;&#25104;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#25105;&#20204;&#30446;&#30585;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#29983;&#25104;&#24335;&#36716;&#21464;&#65292;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23545;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#38382;&#39064;&#65292;LLMs&#26159;&#21542;&#29702;&#35299;&#20854;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21547;&#20041;&#65311;&#23427;&#20204;&#26159;&#21542;&#20855;&#26377;&#35821;&#20041;&#22522;&#30784;&#65311;&#25105;&#20204;&#22914;&#20309;&#20102;&#35299;&#23427;&#20204;&#26159;&#21542;&#29702;&#35299;&#20197;&#21450;&#29702;&#35299;&#30340;&#26159;&#20160;&#20040;&#65311;&#26412;&#25991;&#25506;&#35752;&#20102;&#23545;&#35821;&#20041;&#22522;&#30784;&#38382;&#39064;&#30340;&#35780;&#20272;&#65292;&#21306;&#20998;&#21644;&#35752;&#35770;&#20102;&#20116;&#31181;&#26041;&#27861;&#12290;&#20854;&#20013;&#26368;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26159;&#23558;&#24515;&#28789;&#21746;&#23398;&#21644;&#35821;&#35328;&#23398;&#20013;&#20851;&#20110;&#21547;&#20041;&#30340;&#26680;&#24515;&#20551;&#35774;&#24212;&#29992;&#20110;LLMs&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#20041;&#22522;&#30784;&#26159;&#19968;&#20010;&#28176;&#36827;&#30340;&#36807;&#31243;&#65292;&#21253;&#25324;&#21151;&#33021;&#24615;&#12289;&#31038;&#20250;&#24615;&#21644;&#22240;&#26524;&#24615;&#19977;&#20010;&#32500;&#24230;&#30340;&#21306;&#20998;&#12290;LLMs&#22312;&#36825;&#19977;&#20010;&#32500;&#24230;&#19978;&#23637;&#29616;&#20986;&#22522;&#26412;&#35777;&#25454;&#12290;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#35770;&#25454;&#26159;LLMs&#20250;&#24418;&#25104;&#19990;&#30028;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;LLMs&#26082;&#19981;&#26159;&#38543;&#26426;&#30340;&#40550;&#40521;&#65292;&#20063;&#19981;&#26159;&#35821;&#20041;&#20725;&#23608;&#65292;&#32780;&#26159;&#33267;&#23569;&#22312;&#22522;&#26412;&#23618;&#38754;&#19978;&#24050;&#32463;&#29702;&#35299;&#23427;&#20204;&#29983;&#25104;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10992v1 Announce Type: cross  Abstract: Do LLMs understand the meaning of the texts they generate? Do they possess a semantic grounding? And how could we understand whether and what they understand? I start the paper with the observation that we have recently witnessed a generative turn in AI, since generative models, including LLMs, are key for self-supervised learning. To assess the question of semantic grounding, I distinguish and discuss five methodological ways. The most promising way is to apply core assumptions of theories of meaning in philosophy of mind and language to LLMs. Grounding proves to be a gradual affair with a three-dimensional distinction between functional, social and causal grounding. LLMs show basic evidence in all three dimensions. A strong argument is that LLMs develop world models. Hence, LLMs are neither stochastic parrots nor semantic zombies, but already understand the language they generate, at least in an elementary sense.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10991</link><description>&lt;p&gt;
&#21152;&#36895;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Semi-Asynchronous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10991
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#22312;&#20854;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#65292;&#22914;Federated Averaging&#65288;FedAvg&#65289;&#21450;&#20854;&#21464;&#31181;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#24050;&#32463;&#34987;&#35777;&#26126;&#25910;&#25947;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#23458;&#25143;&#31471;&#20197;&#21516;&#27493;&#26041;&#24335;&#23558;&#20854;&#26412;&#22320;&#26356;&#26032;&#19978;&#20256;&#33267;&#26381;&#21153;&#22120;&#65292;&#36825;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#21464;&#24471;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#32487;&#32493;&#20351;&#29992;&#38472;&#26087;&#30340;&#20840;&#23616;&#27169;&#22411;&#23545;&#20854;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20165;&#20165;&#32858;&#21512;&#20102;&#25152;&#26377;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#20854;&#30456;&#23545;&#36129;&#29486;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#38472;&#26087;&#31243;&#24230;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10991v1 Announce Type: cross  Abstract: Federated Learning (FL) is a distributed machine learning paradigm that allows clients to train models on their data while preserving their privacy. FL algorithms, such as Federated Averaging (FedAvg) and its variants, have been shown to converge well in many scenarios. However, these methods require clients to upload their local updates to the server in a synchronous manner, which can be slow and unreliable in realistic FL settings. To address this issue, researchers have developed asynchronous FL methods that allow clients to continue training on their local data using a stale global model. However, most of these methods simply aggregate all of the received updates without considering their relative contributions, which can slow down convergence. In this paper, we propose a contribution-aware asynchronous FL method that takes into account the staleness and statistical heterogeneity of the received updates. Our method dynamically adju
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WilKE&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#32534;&#36753;&#23618;&#26469;&#21305;&#37197;&#19981;&#21516;&#23618;&#32423;&#20013;&#30340;&#30693;&#35782;&#32534;&#36753;&#27169;&#24335;&#31243;&#24230;&#65292;&#22312;&#32456;&#36523;&#32534;&#36753;&#20013;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24179;&#22343;&#23637;&#29616;&#20102;46.2%&#21644;67.8%&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.10987</link><description>&lt;p&gt;
WilKE&#65306;&#26234;&#24935;&#23618;&#30693;&#35782;&#32534;&#36753;&#22120;&#29992;&#20110;&#32456;&#36523;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10987
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WilKE&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#32534;&#36753;&#23618;&#26469;&#21305;&#37197;&#19981;&#21516;&#23618;&#32423;&#20013;&#30340;&#30693;&#35782;&#32534;&#36753;&#27169;&#24335;&#31243;&#24230;&#65292;&#22312;&#32456;&#36523;&#32534;&#36753;&#20013;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24179;&#22343;&#23637;&#29616;&#20102;46.2%&#21644;67.8%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19981;&#20934;&#30830;&#24615;&#65292;&#32780;&#26080;&#38656;&#20026;&#36807;&#26102;&#25110;&#38169;&#35823;&#30340;&#30693;&#35782;&#36827;&#34892;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#21333;&#27425;&#32534;&#36753;&#65292;&#26410;&#33021;&#28385;&#36275;&#32456;&#36523;&#32534;&#36753;&#30340;&#35201;&#27714;&#12290;&#26412;&#25991;&#20013;&#65292;&#32456;&#36523;&#32534;&#36753;&#19982;&#32456;&#36523;&#30693;&#35782;&#32534;&#36753;&#21516;&#20041;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#30693;&#35782;&#32534;&#36753;&#22312;&#32456;&#36523;&#32534;&#36753;&#20013;&#36935;&#21040;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#20854;&#29305;&#24449;&#20026;&#27602;&#24615;&#31215;&#32047;&#21644;&#27602;&#24615;&#38378;&#29616;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#27169;&#24335;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WilKE&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#23427;&#26681;&#25454;&#19981;&#21516;&#23618;&#32423;&#20013;&#32534;&#36753;&#30693;&#35782;&#30340;&#27169;&#24335;&#21305;&#37197;&#31243;&#24230;&#36873;&#25321;&#32534;&#36753;&#23618;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32456;&#36523;&#32534;&#36753;&#20013;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;WilKE&#22312;&#32534;&#36753;GPT2-XL&#21644;GPT-J&#26041;&#38754;&#20998;&#21035;&#24179;&#22343;&#25913;&#36827;&#20102;46.2%&#21644;67.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10987v1 Announce Type: cross  Abstract: Knowledge editing aims to rectify inaccuracies in large language models (LLMs) without costly retraining for outdated or erroneous knowledge. However, current knowledge editing methods primarily focus on single editing, failing to meet the requirements for lifelong editing. In this paper, lifelong editing is synonymous with lifelong knowledge editing. This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch. We introduce a knowledge editing approach named WilKE, which selects editing layer based on the pattern matching degree of editing knowledge across different layers. Experimental results demonstrate that, in lifelong editing, WilKE exhibits an average improvement of 46.2\% and 67.8\% on editing GPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods.
&lt;/p&gt;</description></item><item><title>FinTral&#26159;&#19968;&#31867;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#30340;GPT-4&#32423;&#21035;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#26816;&#32034;&#26041;&#27861;&#20248;&#21270;&#65292;&#22312;AI&#39537;&#21160;&#37329;&#34701;&#25216;&#26415;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.10986</link><description>&lt;p&gt;
FinTral&#65306;&#19968;&#31867;GPT-4&#32423;&#21035;&#30340;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10986
&lt;/p&gt;
&lt;p&gt;
FinTral&#26159;&#19968;&#31867;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#30340;GPT-4&#32423;&#21035;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#26816;&#32034;&#26041;&#27861;&#20248;&#21270;&#65292;&#22312;AI&#39537;&#21160;&#37329;&#34701;&#25216;&#26415;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;FinTral&#65292;&#36825;&#26159;&#19968;&#32452;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#26500;&#24314;&#30340;&#19968;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#19987;&#38376;&#20026;&#37329;&#34701;&#20998;&#26512;&#23450;&#21046;&#12290;FinTral&#25972;&#21512;&#20102;&#25991;&#26412;&#12289;&#25968;&#23383;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20026;&#26412;&#30740;&#31350;&#31574;&#21010;&#30340;&#22823;&#37327;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#12289;&#25351;&#23548;&#24494;&#35843;&#21644;RLAIF&#35757;&#32451;&#22686;&#24378;&#20102;FinTral&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20061;&#20010;&#20219;&#21153;&#21644;25&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#25324;&#37329;&#34701;&#39046;&#22495;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;FinTral&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#20808;&#36827;&#30340;&#24037;&#20855;&#21644;&#26816;&#32034;&#26041;&#27861;&#36827;&#34892;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#35757;&#32451;&#65292;&#21629;&#21517;&#20026;FinTral-DPO-T&amp;R&#65292;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#23427;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;ChatGPT-3.5&#65292;&#24182;&#22312;&#20061;&#39033;&#20219;&#21153;&#20013;&#30340;&#20116;&#39033;&#20013;&#36229;&#36234;GPT-4&#65292;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#37329;&#34701;&#25216;&#26415;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;FinTral&#20855;&#26377;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10986v1 Announce Type: cross  Abstract: We introduce FinTral, a suite of state-of-the-art multimodal large language models (LLMs) built upon the Mistral-7b model and tailored for financial analysis. FinTral integrates textual, numerical, tabular, and image data. We enhance FinTral with domain-specific pretraining, instruction fine-tuning, and RLAIF training by exploiting a large collection of textual and visual datasets we curate for this work. We also introduce an extensive benchmark featuring nine tasks and 25 datasets for evaluation, including hallucinations in the financial domain. Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&amp;R, demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5 in all tasks and surpasses GPT-4 in five out of nine tasks, marking a significant advancement in AI-driven financial technology. We also demonstrate that FinTral has the potential to e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24314;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;PDDL&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#21487;&#33021;&#23548;&#33268;&#35832;&#22914;&#21202;&#32034;&#36719;&#20214;&#21644;&#25935;&#24863;&#25968;&#25454;&#22806;&#27844;&#31561;&#24191;&#27867;&#25915;&#20987;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2402.10985</link><description>&lt;p&gt;
&#21033;&#29992;AI&#35268;&#21010;&#25216;&#26415;&#26816;&#27979;&#20113;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Leveraging AI Planning For Detecting Cloud Security Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10985
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24314;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;PDDL&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#21487;&#33021;&#23548;&#33268;&#35832;&#22914;&#21202;&#32034;&#36719;&#20214;&#21644;&#25935;&#24863;&#25968;&#25454;&#22806;&#27844;&#31561;&#24191;&#27867;&#25915;&#20987;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#35745;&#31639;&#26381;&#21153;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#19988;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#25968;&#25454;&#23384;&#20648;&#12289;&#22788;&#29702;&#21644;&#21327;&#20316;&#35299;&#20915;&#26041;&#26696;&#12290;&#38543;&#30528;&#23427;&#20204;&#30340;&#26222;&#21450;&#65292;&#19982;&#20854;&#23433;&#20840;&#28431;&#27934;&#30456;&#20851;&#30340;&#25285;&#24551;&#20063;&#22312;&#22686;&#38271;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#27844;&#38706;&#21644;&#21202;&#32034;&#36719;&#20214;&#31561;&#22797;&#26434;&#25915;&#20987;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#36798;&#20113;&#31995;&#32479;&#20013;&#19981;&#21516;&#23545;&#35937;&#65288;&#22914;&#29992;&#25143;&#12289;&#25968;&#25454;&#23384;&#20648;&#12289;&#23433;&#20840;&#35282;&#33394;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#24314;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#12290;&#35775;&#38382;&#25511;&#21046;&#35823;&#37197;&#32622;&#36890;&#24120;&#26159;&#20113;&#25915;&#20987;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;PDDL&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#23433;&#20840;&#28431;&#27934;&#65292;&#20363;&#22914;&#21487;&#33021;&#23548;&#33268;&#24191;&#27867;&#25915;&#20987;&#65288;&#22914;&#21202;&#32034;&#36719;&#20214;&#65289;&#21644;&#25935;&#24863;&#25968;&#25454;&#22806;&#27844;&#31561;&#12290;&#35268;&#21010;&#22120;&#21487;&#20197;&#29983;&#25104;&#25915;&#20987;&#20197;&#35782;&#21035;&#20113;&#20013;&#30340;&#27492;&#31867;&#28431;&#27934;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;14&#20010;&#19981;&#21516;&#21830;&#19994;&#32452;&#32455;&#30340;&#30495;&#23454;&#20122;&#39532;&#36874;AWS&#20113;&#37197;&#32622;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10985v1 Announce Type: cross  Abstract: Cloud computing services provide scalable and cost-effective solutions for data storage, processing, and collaboration. Alongside their growing popularity, concerns related to their security vulnerabilities leading to data breaches and sophisticated attacks such as ransomware are growing. To address these, first, we propose a generic framework to express relations between different cloud objects such as users, datastores, security roles, to model access control policies in cloud systems. Access control misconfigurations are often the primary driver for cloud attacks. Second, we develop a PDDL model for detecting security vulnerabilities which can for example lead to widespread attacks such as ransomware, sensitive data exfiltration among others. A planner can then generate attacks to identify such vulnerabilities in the cloud. Finally, we test our approach on 14 real Amazon AWS cloud configurations of different commercial organizations
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;</title><link>https://arxiv.org/abs/2402.10980</link><description>&lt;p&gt;
CHEMREASONER&#65306;&#20351;&#29992;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#36827;&#34892;&#21551;&#21457;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10980
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 &#31867;&#22411;&#20844;&#21578;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#21457;&#29616;&#26032;&#30340;&#20652;&#21270;&#21058;&#23545;&#20110;&#35774;&#35745;&#26032;&#30340;&#26356;&#39640;&#25928;&#30340;&#21270;&#23398;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#23454;&#29616;&#21521;&#21487;&#25345;&#32493;&#26410;&#26469;&#30340;&#36807;&#28193;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#19977;&#32500;&#21407;&#23376;&#34920;&#31034;&#30340;&#21453;&#39304;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#26500;&#24314;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#23548;&#30340;&#20551;&#35774;&#19982;&#22522;&#20110;&#21407;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#21453;&#39304;&#30340;&#36845;&#20195;&#32452;&#21512;&#65292;&#31215;&#26497;&#25628;&#32034;&#39640;&#25928;&#20652;&#21270;&#21058;&#12290;&#22312;&#20013;&#38388;&#25628;&#32034;&#27493;&#39588;&#30830;&#23450;&#30340;&#20652;&#21270;&#21058;&#32463;&#36807;&#22522;&#20110;&#31354;&#38388;&#23450;&#21521;&#12289;&#21453;&#24212;&#36884;&#24452;&#21644;&#31283;&#23450;&#24615;&#30340;&#32467;&#26500;&#35780;&#20272;&#12290;&#22522;&#20110;&#21560;&#38468;&#33021;&#21644;&#21183;&#22418;&#30340;&#35780;&#20998;&#20989;&#25968;&#24341;&#23548;&#22312;LLM&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#21521;&#33021;&#37327;&#26377;&#21033;&#12289;&#39640;&#25928;&#30340;&#20652;&#21270;&#21058;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#20197;&#33258;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#22260;&#32469;&#20307;&#32946;&#25968;&#25454;&#20998;&#26512;&#23637;&#24320;&#30340;&#22235;&#39033;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#25512;&#29702;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10979</link><description>&lt;p&gt;
SportsMetrics:&#23558;&#25991;&#26412;&#21644;&#25968;&#20540;&#25968;&#25454;&#34701;&#21512;&#20197;&#29702;&#35299;LLM&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10979
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#22260;&#32469;&#20307;&#32946;&#25968;&#25454;&#20998;&#26512;&#23637;&#24320;&#30340;&#22235;&#39033;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#25512;&#29702;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10979v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25972;&#21512;&#25991;&#26412;&#25991;&#26723;&#21644;&#25968;&#25454;&#24211;&#35760;&#24405;&#31561;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#36827;&#34892;&#20808;&#36827;&#20998;&#26512;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#34701;&#21512;&#25991;&#26412;&#21644;&#25968;&#20540;&#25968;&#25454;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;LLMs&#38656;&#35201;&#22788;&#29702;&#21644;&#20132;&#21449;&#24341;&#29992;&#23454;&#20307;&#21644;&#25968;&#23383;&#65292;&#22788;&#29702;&#25968;&#25454;&#19981;&#19968;&#33268;&#24615;&#21644;&#20887;&#20313;&#65292;&#24182;&#21457;&#23637;&#35268;&#21010;&#33021;&#21147;&#65292;&#27604;&#22914;&#26500;&#24314;&#29992;&#20110;&#31649;&#29702;&#22797;&#26434;&#25968;&#25454;&#26597;&#35810;&#30340;&#24037;&#20316;&#20869;&#23384;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22260;&#32469;&#20307;&#32946;&#25968;&#25454;&#20998;&#26512;&#30340;&#22235;&#39033;&#26032;&#39062;&#20219;&#21153;&#65292;&#20197;&#35780;&#20272;LLMs&#30340;&#25968;&#20540;&#25512;&#29702;&#21644;&#20449;&#24687;&#34701;&#21512;&#33021;&#21147;&#12290;&#36825;&#20123;&#20219;&#21153;&#28041;&#21450;&#21521;LLMs&#25552;&#20379;&#35814;&#32454;&#30340;&#36880;&#22330;&#27604;&#36187;&#25551;&#36848;&#65292;&#28982;&#21518;&#22312;&#38754;&#23545;&#35832;&#22914;&#26032;&#27604;&#36187;&#35268;&#21017;&#12289;&#26356;&#38271;&#25345;&#32493;&#26102;&#38388;&#12289;&#25925;&#20107;&#28151;&#20081;&#20197;&#21450;&#20998;&#26512;&#27604;&#36187;&#25688;&#35201;&#20013;&#30340;&#20851;&#38190;&#32479;&#35745;&#25968;&#25454;&#31561;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;&#25105;&#20204;&#23545;NBA&#21644;NFL&#27604;&#36187;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;LLMs&#22312;&#35813;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10979v1 Announce Type: cross  Abstract: Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#36830;&#25509;&#35821;&#35328;&#24314;&#27169;&#21644;&#31526;&#21512;&#39044;&#27979;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#39640;&#27010;&#29575;&#27491;&#30830;&#24615;&#20445;&#35777;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.10978</link><description>&lt;p&gt;
&#20855;&#26377;&#31526;&#21512;&#20107;&#23454;&#24615;&#20445;&#35777;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language Models with Conformal Factuality Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#36830;&#25509;&#35821;&#35328;&#24314;&#27169;&#21644;&#31526;&#21512;&#39044;&#27979;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#39640;&#27010;&#29575;&#27491;&#30830;&#24615;&#20445;&#35777;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#21644;&#20107;&#23454;&#24615;&#20445;&#35777;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31526;&#21512;&#20107;&#23454;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#36830;&#25509;&#35821;&#35328;&#24314;&#27169;&#21644;&#31526;&#21512;&#39044;&#27979;&#65292;&#30830;&#20445;LM&#30340;&#39640;&#27010;&#29575;&#27491;&#30830;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;LM&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#31561;&#20215;&#20110;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#19981;&#30830;&#23450;&#24615;&#38598;&#34987;&#23450;&#20041;&#20026;LM&#36755;&#20986;&#30340;&#34164;&#21547;&#38598;&#12290;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31526;&#21512;&#39044;&#27979;&#23545;&#24212;&#20110;&#19968;&#31181;&#21518;&#36864;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#28176;&#20351;LM&#36755;&#20986;&#21464;&#24471;&#19981;&#22826;&#20855;&#20307;&#65288;&#24182;&#25193;&#22823;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65289;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#27491;&#30830;&#24615;&#20445;&#35777;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;LM&#65292;&#24182;&#19988;&#38656;&#35201;&#24456;&#23569;&#30340;&#20154;&#24037;&#27880;&#37322;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#23553;&#38381;&#20070;&#31821;QA&#65288;FActScore&#65292;NaturalQuestions&#65289;&#21644;&#25512;&#29702;&#20219;&#21153;&#65288;MATH&#65289;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10978v1 Announce Type: cross  Abstract: Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem. In this work, we propose conformal factuality, a framework that can ensure high probability correctness guarantees for LMs by connecting language modeling and conformal prediction. We observe that the correctness of an LM output is equivalent to an uncertainty quantification problem, where the uncertainty sets are defined as the entailment set of an LM's output. Using this connection, we show that conformal prediction in language models corresponds to a back-off algorithm that provides high probability correctness guarantees by progressively making LM outputs less specific (and expanding the associated uncertainty sets). This approach applies to any black-box LM and requires very few human-annotated samples. Evaluations of our approach on closed book QA (FActScore, NaturalQuestions) and reasoning tasks (MATH) show that our approach can p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35821;&#20041;&#25216;&#26415;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#25216;&#26415;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#38454;&#27573;&#65292;&#20026;&#38738;&#23569;&#24180;&#37202;&#31934;&#20351;&#29992;&#38556;&#30861;&#30340;&#20010;&#24615;&#21270;&#29305;&#24449;&#21644;&#39118;&#38505;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10967</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#25216;&#26415;&#23545;&#38738;&#23569;&#24180;&#37202;&#31934;&#20351;&#29992;&#38556;&#30861;&#36827;&#34892;&#20010;&#24615;&#21270;&#29305;&#24449;&#21644;&#39118;&#38505;&#35780;&#20272;&#30340;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Social network analysis for personalized characterization and risk assessment of alcohol use disorders in adolescents using semantic technologies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35821;&#20041;&#25216;&#26415;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#25216;&#26415;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#38454;&#27573;&#65292;&#20026;&#38738;&#23569;&#24180;&#37202;&#31934;&#20351;&#29992;&#38556;&#30861;&#30340;&#20010;&#24615;&#21270;&#29305;&#24449;&#21644;&#39118;&#38505;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37202;&#31934;&#20351;&#29992;&#38556;&#30861;(AUD)&#26159;&#20840;&#29699;&#20844;&#20849;&#21355;&#29983;&#32452;&#32455;&#29305;&#21035;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#38738;&#23569;&#24180;&#32676;&#20307;&#12290;&#38738;&#23569;&#24180;&#39278;&#37202;&#34892;&#20026;&#21463;&#21040;&#26379;&#21451;&#29978;&#33267;&#29238;&#27597;&#39278;&#37202;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#30740;&#31350;&#21033;&#29992;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;(SNA)&#25216;&#26415;&#30740;&#31350;&#38738;&#23569;&#24180;&#28041;&#21450;&#30340;&#19981;&#21516;&#31038;&#20132;&#32593;&#32476;(&#21516;&#40836;&#20154;&#12289;&#26379;&#21451;&#12289;&#23478;&#20154;&#31561;)&#12290;&#36825;&#31867;&#30740;&#31350;&#38656;&#35201;&#36890;&#36807;&#38382;&#21367;&#35843;&#26597;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#65292;&#28982;&#21518;&#21033;&#29992;SNA&#25216;&#26415;&#36827;&#34892;&#21518;&#32493;&#20998;&#26512;&#12290;&#36825;&#19968;&#36807;&#31243;&#28041;&#21450;&#22810;&#20010;&#25163;&#21160;&#25968;&#25454;&#22788;&#29702;&#38454;&#27573;&#65292;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#21033;&#29992;&#30693;&#35782;&#24037;&#31243;&#25216;&#26415;(&#21253;&#25324;&#26500;&#24314;&#39046;&#22495;&#26412;&#20307;&#35770;)&#26469;&#34920;&#31034;&#20449;&#24687;&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;&#21021;&#22987;&#25968;&#25454;&#25910;&#38598;&#21040;&#20998;&#26512;&#38454;&#27573;&#30340;&#25152;&#26377;&#27963;&#21160;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10967v1 Announce Type: new  Abstract: Alcohol Use Disorder (AUD) is a major concern for public health organizations worldwide, especially as regards the adolescent population. The consumption of alcohol in adolescents is known to be influenced by seeing friends and even parents drinking alcohol. Building on this fact, a number of studies into alcohol consumption among adolescents have made use of Social Network Analysis (SNA) techniques to study the different social networks (peers, friends, family, etc.) with whom the adolescent is involved. These kinds of studies need an initial phase of data gathering by means of questionnaires and a subsequent analysis phase using the SNA techniques. The process involves a number of manual data handling stages that are time consuming and error-prone. The use of knowledge engineering techniques (including the construction of a domain ontology) to represent the information, allows the automation of all the activities, from the initial data
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;</title><link>https://arxiv.org/abs/2402.10962</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#27979;&#37327;&#21644;&#25511;&#21046;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Measuring and Controlling Persona Drift in Language Model Dialogs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26159;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#20351;&#20854;&#33021;&#22815;&#25215;&#25285;&#29305;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#12290;&#22312;&#20351;&#29992;&#25552;&#31034;&#26102;&#30340;&#19968;&#20010;&#38544;&#21547;&#20551;&#35774;&#26159;&#65292;&#23427;&#20204;&#23558;&#26159;&#31283;&#23450;&#30340;&#65292;&#22240;&#27492;&#32842;&#22825;&#26426;&#22120;&#20154;&#23558;&#22312;&#25972;&#20010;&#23545;&#35805;&#36807;&#31243;&#20013;&#32487;&#32493;&#26681;&#25454;&#35268;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#36890;&#36807;&#20004;&#20010;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#33258;&#25105;&#23545;&#35805;&#26469;&#35780;&#20272;&#8220;&#20154;&#35774;&#8221;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#27169;&#22411;&#22914;LLaMA2-chat-70B&#36827;&#34892;&#27979;&#35797;&#65292;&#21457;&#29616;&#22312;&#20843;&#36718;&#23545;&#35805;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#12290;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#30001;&#20110;&#38271;&#23545;&#35805;&#20013;&#30340;&#27880;&#24847;&#21147;&#34928;&#20943;&#65292;&#21464;&#21387;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#12290;&#20026;&#20102;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#19982;&#20004;&#20010;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#65292;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10958</link><description>&lt;p&gt;
&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;: &#36890;&#36807;&#23545;&#30456;&#21516;&#21644;&#19981;&#21516;&#25552;&#31034;&#30340;&#23545;&#27604;&#21709;&#24212;&#22686;&#24378;LLM&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10958
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#65292;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#65292;&#23558;&#27169;&#22411;&#19982;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#30452;&#25509;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;DPO&#65289;&#22312;&#36825;&#19968;&#39046;&#22495;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;DPO&#36890;&#36807;&#20351;&#29992;&#20174;&#30456;&#21516;&#25552;&#31034;&#20013;&#27966;&#29983;&#30340;&#20559;&#22909;&#23545;&#26469;&#24037;&#20316;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DPO&#24182;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#20154;&#31867;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#31181;&#23398;&#20064;&#24448;&#24448;&#28041;&#21450;&#23545;&#19981;&#20165;&#30456;&#21516;&#32780;&#19988;&#30456;&#20284;&#38382;&#39064;&#30340;&#23545;&#27604;&#21709;&#24212;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#12290;RPO&#26088;&#22312;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#12290;&#23427;&#24341;&#20837;&#20102;&#23545;&#27604;&#21152;&#26435;&#26426;&#21046;&#65292;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;&#26356;&#24191;&#27867;&#30340;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#35843;&#25972;&#65292;&#21253;&#25324;&#25104;&#23545;&#21644;&#19981;&#25104;&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#21033;&#29992;&#26356;&#22810;&#30340;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10958v1 Announce Type: cross  Abstract: In the field of large language models (LLMs), aligning models with the diverse preferences of users is a critical challenge. Direct Preference Optimization (DPO) has played a key role in this area. It works by using pairs of preferences derived from the same prompts, and it functions without needing an additional reward model. However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions. To overcome this shortfall, we propose Relative Preference Optimization (RPO). RPO is designed to discern between more and less preferred responses derived from both identical and related prompts. It introduces a contrastive weighting mechanism, enabling the tuning of LLMs using a broader range of preference data, including both paired and unpaired sets. This approach expands the learning capabilities of the model, allowing it to lever
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#32467;&#21512;&#24515;&#29702;&#37327;&#34920;&#36890;&#36807;LLMs&#36827;&#34892;&#38646;-shot&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.10948</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#32467;&#21512;&#24515;&#29702;&#37327;&#34920;&#36827;&#34892;&#38646;-shot&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Explainable Mental Health Analysis on Social Media by incorporating Mental Scales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10948
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#32467;&#21512;&#24515;&#29702;&#37327;&#34920;&#36890;&#36807;LLMs&#36827;&#34892;&#38646;-shot&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26041;&#27861;&#22312;&#23481;&#37327;&#26041;&#38754;&#34920;&#29616;&#24378;&#22823;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#35268;&#27169;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#26377;&#28508;&#21147;&#25670;&#33073;&#32321;&#37325;&#30340;&#27880;&#37322;&#24182;&#25552;&#20379;&#35299;&#37322;&#12290;&#21463;&#21040;&#20351;&#29992;&#37327;&#34920;&#35780;&#20272;&#24515;&#29702;&#29366;&#24577;&#30340;&#24515;&#29702;&#35780;&#20272;&#23454;&#36341;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;LLMs&#32467;&#21512;&#20102;&#20004;&#20010;&#31243;&#24207;&#12290;&#39318;&#20808;&#65292;&#24739;&#32773;&#23436;&#25104;&#24515;&#29702;&#20581;&#24247;&#38382;&#21367;&#65292;&#20854;&#27425;&#65292;&#24515;&#29702;&#23398;&#23478;&#35299;&#37322;&#26469;&#33258;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#25910;&#38598;&#20449;&#24687;&#24182;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20854;&#20182;&#38646;-shot&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10948v1 Announce Type: cross  Abstract: Traditional discriminative approaches in mental health analysis are known for their strong capacity but lack interpretability and demand large-scale annotated data. On the other hand, generative approaches, such as those based on large language models (LLMs),have the potential to get rid of heavy annotations and provide explanations. However, their capabilities still fall short compared to discriminative approaches, and their explanations may be unreliable due to the fact that the generation of explanation is a black-box process. Inspired by the psychological assessment practice of using scales to evaluate mental states, our method incorporates two procedures via LLMs. First, the patient completes mental health questionnaires, and second, the psychologist interprets the collected information from the mental health questions and makes informed decisions. Experimental results show that our method outperforms other zero-shot methods. Our 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.10946</link><description>&lt;p&gt;
&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
CultureLLM: Incorporating Cultural Differences into Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25253;&#36947;&#20559;&#21521;&#20110;&#26576;&#20123;&#25991;&#21270;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20027;&#35201;&#26469;&#33258;&#33521;&#35821;&#35821;&#26009;&#24211;&#12290;&#30001;&#20110;&#22810;&#35821;&#31181;&#25991;&#21270;&#25968;&#25454;&#36890;&#24120;&#36739;&#38590;&#25910;&#38598;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25110;&#29305;&#23450;&#25991;&#21270;&#30340;&#39044;&#35757;&#32451;&#26469;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#24573;&#35270;&#20102;&#20302;&#36164;&#28304;&#25991;&#21270;&#30340;&#30693;&#35782;&#32570;&#20047;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CultureLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;LLMs&#20013;&#12290;CultureLLM&#37319;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#35821;&#20041;&#31561;&#25928;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20165;&#20351;&#29992;&#26469;&#33258;WVS&#30340;50&#20010;&#31181;&#23376;&#26679;&#26412;&#21644;&#22686;&#24378;&#25968;&#25454;&#65292;&#25105;&#20204;&#23545;9&#31181;&#21253;&#25324;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#21270;&#29305;&#23450;LLMs&#21644;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23545;60&#20010;&#19982;&#25991;&#21270;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CultureLLM&#22312;&#22686;&#24378;LLM&#30340;&#25991;&#21270;&#29305;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10946v1 Announce Type: cross  Abstract: Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM signif
&lt;/p&gt;</description></item><item><title>Text2Data&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#30340;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10941</link><description>&lt;p&gt;
Text2Data&#65306;&#20351;&#29992;&#25991;&#26412;&#25511;&#21046;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text2Data: Low-Resource Data Generation with Textual Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10941
&lt;/p&gt;
&lt;p&gt;
Text2Data&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#30340;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#20154;&#31867;&#19982;&#26426;&#22120;&#26080;&#32541;&#20132;&#20114;&#30340;&#19968;&#31181;&#24120;&#35265;&#30452;&#25509;&#25511;&#21046;&#20449;&#21495;&#12290;&#24847;&#35782;&#21040;&#36825;&#19968;&#25509;&#21475;&#30340;&#37325;&#35201;&#24615;&#65292;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#27491;&#22312;&#25237;&#20837;&#22823;&#37327;&#31934;&#21147;&#29983;&#25104;&#19982;&#25991;&#26412;&#25351;&#20196;&#22312;&#35821;&#20041;&#19978;&#19968;&#33268;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#22312;&#28085;&#30422;&#22270;&#20687;&#32534;&#36753;&#12289;&#38899;&#39057;&#21512;&#25104;&#12289;&#35270;&#39057;&#29983;&#25104;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20302;&#36164;&#28304;&#39046;&#22495;&#30001;&#20110;&#26114;&#36149;&#27880;&#37322;&#25110;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#65288;&#22914;&#20998;&#23376;&#12289;&#36816;&#21160;&#21160;&#24577;&#21644;&#26102;&#24207;&#65289;&#31561;&#29305;&#28857;&#65292;&#24448;&#24448;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#12290;&#36825;&#31181;&#19981;&#36275;&#38459;&#30861;&#20102;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23558;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Text2Data&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10941v1 Announce Type: cross  Abstract: Natural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22522;&#20110;ICD-9&#20195;&#30721;&#30340;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10940</link><description>&lt;p&gt;
&#20020;&#24202;&#31243;&#24207;&#20195;&#30721;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation of clinical procedure codes for medical diagnosis and uncertainty quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10940
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22522;&#20110;ICD-9&#20195;&#30721;&#30340;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#26088;&#22312;&#36890;&#36807;&#23558;&#31995;&#32479;&#29983;&#25104;&#30340;&#24314;&#35758;&#19982;&#21307;&#23398;&#19987;&#19994;&#30693;&#35782;&#32467;&#21512;&#26469;&#22686;&#24378;&#20020;&#24202;&#21307;&#29983;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#22522;&#20110;&#25163;&#26415;ICD-9&#20195;&#30721;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26469;&#37327;&#21270;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19981;&#20165;&#23637;&#31034;&#20102;&#31243;&#24207;&#20195;&#30721;&#19982;&#23454;&#38469;&#21307;&#30103;&#32467;&#26524;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10940v1 Announce Type: new  Abstract: A Clinical Decision Support System (CDSS) is designed to enhance clinician decision-making by combining system-generated recommendations with medical expertise. Given the high costs, intensive labor, and time-sensitive nature of medical treatments, there is a pressing need for efficient decision support, especially in complex emergency scenarios. In these scenarios, where information can be limited, an advanced CDSS framework that leverages AI (artificial intelligence) models to effectively reduce diagnostic uncertainty has utility. Such an AI-enabled CDSS framework with quantified uncertainty promises to be practical and beneficial in the demanding context of real-world medical care. In this study, we introduce the concept of Medical Entropy, quantifying uncertainties in patient outcomes predicted by neural machine translation based on the ICD-9 code of procedures. Our experimental results not only show strong correlations between proce
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;Inception&#22686;&#24378;&#30340;U-Net&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24067;&#32447;&#25317;&#22622;&#21644;&#35774;&#35745;&#35268;&#21017;&#26816;&#26597;&#28909;&#28857;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.10937</link><description>&lt;p&gt;
&#19968;&#31181;&#36731;&#37327;&#32423;Inception&#22686;&#24378;U-Net&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24067;&#32447;&#21487;&#39044;&#27979;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Lightweight Inception Boosted U-Net Neural Network for Routability Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10937
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;Inception&#22686;&#24378;&#30340;U-Net&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24067;&#32447;&#25317;&#22622;&#21644;&#35774;&#35745;&#35268;&#21017;&#26816;&#26597;&#28909;&#28857;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;CPU&#12289;GPU&#21644;NPU&#33455;&#29255;&#35774;&#35745;&#22797;&#26434;&#24615;&#21644;&#26230;&#20307;&#31649;&#25968;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#20197;&#21450;&#21322;&#23548;&#20307;&#25216;&#26415;&#33410;&#28857;&#19981;&#26029;&#32553;&#23567;&#33267;&#36817;1&#32435;&#31859;&#65292;&#24067;&#23616;&#21644;&#24067;&#32447;&#36880;&#28176;&#25104;&#20026;&#29616;&#20195;&#36229;&#22823;&#35268;&#27169;&#38598;&#25104;&#65288;VLSI&#65289;&#30005;&#36335;&#21518;&#31471;&#35774;&#35745;&#20013;&#26368;&#20851;&#38190;&#30340;&#20004;&#20010;&#36807;&#31243;&#12290;&#22914;&#20309;&#26377;&#25928;&#20934;&#30830;&#22320;&#22312;&#20808;&#26399;&#35780;&#20272;&#21487;&#36335;&#30001;&#24615;&#65288;&#22312;&#24067;&#23616;&#21644;&#20840;&#23616;&#24067;&#32447;&#38454;&#27573;&#65289;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#39046;&#22495;&#30340;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;U-Net&#21464;&#20307;&#27169;&#22411;&#65292;&#36890;&#36807;&#23884;&#20837;Inception&#27169;&#22359;&#26469;&#39044;&#27979;&#36335;&#30001;&#25317;&#22622;&#65288;RC&#65289;&#21644;&#35774;&#35745;&#35268;&#21017;&#26816;&#26597;&#65288;DRC&#65289;&#28909;&#28857;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;CircuitNet&#25968;&#25454;&#38598;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;Avg-NRMSE&#65288;&#24179;&#22343;&#24402;&#19968;&#21270;&#26681;&#22343;&#26041;&#35823;&#24046;&#65289;&#26041;&#38754;&#23454;&#29616;&#20102;&#39640;&#36798;5%&#65288;RC&#65289;&#21644;20%&#65288;DRC&#65289;&#30340;&#38477;&#20302;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10937v1 Announce Type: cross  Abstract: As the modern CPU, GPU, and NPU chip design complexity and transistor counts keep increasing, and with the relentless shrinking of semiconductor technology nodes to nearly 1 nanometer, the placement and routing have gradually become the two most pivotal processes in modern very-large-scale-integrated (VLSI) circuit back-end design. How to evaluate routability efficiently and accurately in advance (at the placement and global routing stages) has grown into a crucial research area in the field of artificial intelligence (AI) assisted electronic design automation (EDA). In this paper, we propose a novel U-Net variant model boosted by an Inception embedded module to predict Routing Congestion (RC) and Design Rule Checking (DRC) hotspots. Experimental results on the recently published CircuitNet dataset benchmark show that our proposed method achieves up to 5% (RC) and 20% (DRC) rate reduction in terms of Avg-NRMSE (Average Normalized Root 
&lt;/p&gt;</description></item><item><title>ConSmax&#26159;&#19968;&#31181;&#30828;&#20214;&#21451;&#22909;&#22411;Softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#21407;Softmax&#20851;&#38190;&#20219;&#21153;&#30340;&#39640;&#25928;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.10930</link><description>&lt;p&gt;
ConSmax: &#20855;&#26377;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#30828;&#20214;&#21451;&#22909;&#22411;Softmax&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10930
&lt;/p&gt;
&lt;p&gt;
ConSmax&#26159;&#19968;&#31181;&#30828;&#20214;&#21451;&#22909;&#22411;Softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#21407;Softmax&#20851;&#38190;&#20219;&#21153;&#30340;&#39640;&#25928;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#26426;&#21046;&#23558;&#22522;&#20110;transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#21367;&#31215;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21306;&#20998;&#24320;&#26469;&#12290;&#23613;&#31649;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#65292;&#20294;&#30001;&#20110;&#33258;&#27880;&#24847;&#20013;&#24191;&#27867;&#20351;&#29992;Softmax&#65292;&#22312;&#30789;&#19978;&#23454;&#29616;&#23454;&#26102;LLM&#25512;&#26029;&#20173;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Constant Softmax&#65288;ConSmax&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;Softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#37319;&#29992;&#21487;&#24494;&#30340;&#35268;&#33539;&#21270;&#21442;&#25968;&#26469;&#28040;&#38500;Softmax&#20013;&#30340;&#26368;&#22823;&#25628;&#32034;&#21644;&#20998;&#27597;&#27714;&#21644;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10930v1 Announce Type: cross  Abstract: The self-attention mechanism sets transformer-based large language model (LLM) apart from the convolutional and recurrent neural networks. Despite the performance improvement, achieving real-time LLM inference on silicon is challenging due to the extensively used Softmax in self-attention. Apart from the non-linearity, the low arithmetic intensity greatly reduces the processing parallelism, which becomes the bottleneck especially when dealing with a longer context. To address this challenge, we propose Constant Softmax (ConSmax), a software-hardware co-design as an efficient Softmax alternative. ConSmax employs differentiable normalization parameters to remove the maximum searching and denominator summation in Softmax. It allows for massive parallelization while performing the critical tasks of Softmax. In addition, a scalable ConSmax hardware utilizing a bitwidth-split look-up table (LUT) can produce lossless non-linear operation and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#36866;&#24212;&#32570;&#22833;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;, &#35813;&#27169;&#22411;&#21253;&#25324;&#26597;&#35810;&#33258;&#36866;&#24212;&#34701;&#21512;&#21644;&#22810;&#27169;&#24577;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;&#20004;&#22823;&#29305;&#28857;&#65292;&#26088;&#22312;&#25552;&#39640;&#24773;&#32490;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10921</link><description>&lt;p&gt;
AM^2-EmoJE&#65306;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;&#23454;&#29616;&#23545;&#35805;&#20013;&#30340;&#33258;&#36866;&#24212;&#32570;&#22833;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10921
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#32570;&#22833;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;, &#35813;&#27169;&#22411;&#21253;&#25324;&#26597;&#35810;&#33258;&#36866;&#24212;&#34701;&#21512;&#21644;&#22810;&#27169;&#24577;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;&#20004;&#22823;&#29305;&#28857;&#65292;&#26088;&#22312;&#25552;&#39640;&#24773;&#32490;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#24773;&#32490;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#27169;&#24335;&#34920;&#36798;&#65292;&#20363;&#22914;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#27599;&#31181;&#27169;&#24335;&#22312;&#23637;&#31034;&#24773;&#32490;&#26102;&#30340;&#36129;&#29486;&#24182;&#19981;&#22343;&#21248;&#12290;&#27492;&#22806;&#65292;&#22312;&#27979;&#35797;&#26102;&#65292;&#19981;&#19968;&#23450;&#24635;&#26159;&#33021;&#22815;&#20445;&#35777;&#23436;&#25972;&#30340;&#27169;&#24335;&#29305;&#23450;&#32454;&#33410;&#21487;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AM^2-EmoJE&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#23545;&#35805;&#20013;&#23454;&#29616;&#33258;&#36866;&#24212;&#32570;&#22833;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#20004;&#26041;&#38754;&#30340;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#26597;&#35810;&#33258;&#36866;&#24212;&#34701;&#21512;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#20854;&#27169;&#24335;&#29305;&#23450;&#34920;&#31034;&#22312;&#26597;&#35810;&#29305;&#23450;&#26041;&#24335;&#19979;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#27169;&#22411;&#26088;&#22312;&#20248;&#20808;&#32771;&#34385;&#24773;&#32490;&#27169;&#24335;&#30340;&#27169;&#24335;&#19981;&#21464;&#31354;&#38388;&#26597;&#35810;&#32454;&#33410;&#65292;&#21516;&#26102;&#22312;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#26597;&#35810;&#25551;&#36848;&#31526;&#20013;&#20445;&#30041;&#20854;&#29420;&#21344;&#27169;&#24335;&#26041;&#38754;&#12290;&#20854;&#27425;&#65292;&#22810;&#27169;&#24577;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;&#27169;&#22359;&#26126;&#30830;&#35299;&#20915;&#20102;&#27979;&#35797;&#26102;&#30340;&#21508;&#31181;&#32570;&#22833;&#27169;&#24577;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10921v1 Announce Type: new  Abstract: Human emotion can be presented in different modes i.e., audio, video, and text. However, the contribution of each mode in exhibiting each emotion is not uniform. Furthermore, the availability of complete mode-specific details may not always be guaranteed in the test time. In this work, we propose AM^2-EmoJE, a model for Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning model that is grounded on two-fold contributions: First, a query adaptive fusion that can automatically learn the relative importance of its mode-specific representations in a query-specific manner. By this the model aims to prioritize the mode-invariant spatial query details of the emotion patterns, while also retaining its mode-exclusive aspects within the learned multimodal query descriptor. Second the multimodal joint embedding learning module that explicitly addresses various missing modality scenarios in test-time. By this, th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;ChatGPT4&#20026;&#21487;&#32534;&#31243;&#33033;&#20914;&#31070;&#32463;&#20803;&#38453;&#21015;ASIC&#29983;&#25104;Verilog&#25551;&#36848;&#30340;&#35774;&#35745;&#27969;&#31243;&#65292;&#39564;&#35777;&#20102;AI&#29983;&#25104;&#30340;&#35774;&#35745;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#39537;&#21160;&#30828;&#20214;&#35774;&#35745;&#30340;&#29616;&#29366;&#12290;</title><link>https://arxiv.org/abs/2402.10920</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#35774;&#35745;&#30789;&#33041;&#65306;&#21033;&#29992;ChatGPT&#33258;&#21160;&#29983;&#25104;&#33033;&#20914;&#31070;&#32463;&#20803;&#38453;&#21015;&#30340;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Designing Silicon Brains using LLM: Leveraging ChatGPT for Automated Description of a Spiking Neuron Array
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;ChatGPT4&#20026;&#21487;&#32534;&#31243;&#33033;&#20914;&#31070;&#32463;&#20803;&#38453;&#21015;ASIC&#29983;&#25104;Verilog&#25551;&#36848;&#30340;&#35774;&#35745;&#27969;&#31243;&#65292;&#39564;&#35777;&#20102;AI&#29983;&#25104;&#30340;&#35774;&#35745;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#39537;&#21160;&#30828;&#20214;&#35774;&#35745;&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#32508;&#21512;&#27491;&#30830;&#30340;&#22238;&#22797;&#32780;&#24341;&#36215;&#20851;&#27880;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#29992;&#20110;&#25351;&#23548;ChatGPT4&#29983;&#25104;&#21487;&#32508;&#21512;&#21644;&#21151;&#33021;&#24615;Verilog&#25551;&#36848;&#30340;&#25552;&#31034;&#65292;&#20197;&#25551;&#36848;&#21487;&#32534;&#31243;&#33033;&#20914;&#31070;&#32463;&#20803;&#38453;&#21015;ASIC&#30340;&#25972;&#20307;&#12290;&#36825;&#31181;&#35774;&#35745;&#27969;&#31243;&#23637;&#31034;&#20102;&#20351;&#29992;ChatGPT4&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#39537;&#21160;&#30828;&#20214;&#35774;&#35745;&#30340;&#24403;&#21069;&#29366;&#24577;&#12290;&#36890;&#36807;&#25163;&#24037;&#21046;&#20316;&#30340;&#27979;&#35797;&#21488;&#39564;&#35777;&#20102;AI&#29983;&#25104;&#30340;&#35774;&#35745;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#24320;&#28304;EDA&#27969;&#31243;&#30340;Tiny Tapeout 5&#25552;&#20132;&#21040;Skywater 130nm&#36827;&#34892;&#21046;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10920v1 Announce Type: cross  Abstract: Large language models (LLMs) have made headlines for synthesizing correct-sounding responses to a variety of prompts, including code generation. In this paper, we present the prompts used to guide ChatGPT4 to produce a synthesizable and functional verilog description for the entirety of a programmable Spiking Neuron Array ASIC. This design flow showcases the current state of using ChatGPT4 for natural language driven hardware design. The AI-generated design was verified in simulation using handcrafted testbenches and has been submitted for fabrication in Skywater 130nm through Tiny Tapeout 5 using an open-source EDA flow.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;LLAMA2&#35821;&#35328;&#27169;&#22411;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#33021;&#22815;&#20174;&#31038;&#20132;&#23186;&#20307;&#21644;&#32039;&#24613;&#28040;&#24687;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#32039;&#24613;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#21487;&#21327;&#21161;&#22312;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#32039;&#24613;&#24773;&#20917;&#19979;&#20844;&#20849;&#23433;&#20840;&#35805;&#21153;&#21592;&#21644;&#22823;&#20247;&#65292;&#25552;&#20379;&#30456;&#20851;&#25351;&#23548;&#24182;&#36890;&#30693;&#25919;&#24220;&#26426;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.10908</link><description>&lt;p&gt;
LLM&#36741;&#21161;&#21361;&#26426;&#31649;&#29702;&#65306;&#26500;&#24314;&#29992;&#20110;&#26377;&#25928;&#24212;&#24613;&#21709;&#24212;&#21644;&#20844;&#20247;&#21327;&#20316;&#30340;&#20808;&#36827;LLM&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
LLM-Assisted Crisis Management: Building Advanced LLM Platforms for Effective Emergency Response and Public Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10908
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;LLAMA2&#35821;&#35328;&#27169;&#22411;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#33021;&#22815;&#20174;&#31038;&#20132;&#23186;&#20307;&#21644;&#32039;&#24613;&#28040;&#24687;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#32039;&#24613;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#21487;&#21327;&#21161;&#22312;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#32039;&#24613;&#24773;&#20917;&#19979;&#20844;&#20849;&#23433;&#20840;&#35805;&#21153;&#21592;&#21644;&#22823;&#20247;&#65292;&#25552;&#20379;&#30456;&#20851;&#25351;&#23548;&#24182;&#36890;&#30693;&#25919;&#24220;&#26426;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#24613;&#24773;&#20917;&#21644;&#37325;&#22823;&#20107;&#20214;&#24448;&#24448;&#36805;&#36895;&#21457;&#23637;&#65292;&#38656;&#35201;&#36805;&#36895;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLAMA2&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21644;&#30452;&#25509;&#30340;&#32039;&#24613;&#28040;&#24687;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#32039;&#24613;&#24773;&#20917;&#12290;&#26088;&#22312;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#21147;&#37327;&#65292;&#21327;&#21161;&#20844;&#20849;&#23433;&#20840;&#35805;&#21153;&#21592;&#21644;&#22823;&#37327;&#27665;&#20247;&#22312;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#32039;&#24613;&#24773;&#20917;&#20013;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#24320;&#21457;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#22312;911&#21628;&#21483;&#20013;&#25551;&#36848;&#33258;&#24049;&#30340;&#24773;&#20917;&#65292;&#20351;LLAMA2&#33021;&#22815;&#20998;&#26512;&#20869;&#23481;&#24182;&#20026;&#35805;&#21153;&#21592;&#25552;&#20379;&#30456;&#20851;&#25351;&#23548;&#65292;&#21516;&#26102;&#21019;&#24314;&#24037;&#20316;&#27969;&#31243;&#65292;&#22312;&#24517;&#35201;&#26102;&#23558;&#21628;&#21483;&#32773;&#20449;&#24687;&#36890;&#30693;&#25919;&#24220;&#26426;&#26500;&#12290;&#35813;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#21478;&#19968;&#20010;&#22909;&#22788;&#26159;&#65292;&#24403;911&#31995;&#32479;&#19981;&#22570;&#37325;&#36127;&#26102;&#65292;&#23427;&#33021;&#22815;&#22312;&#37325;&#22823;&#32039;&#24613;&#20107;&#20214;&#20013;&#21327;&#21161;&#20154;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10908v1 Announce Type: cross  Abstract: Emergencies and critical incidents often unfold rapidly, necessitating a swift and effective response. In this research, we introduce a novel approach to identify and classify emergency situations from social media posts and direct emergency messages using an open source Large Language Model, LLAMA2. The goal is to harness the power of natural language processing and machine learning to assist public safety telecommunicators and huge crowds during countrywide emergencies. Our research focuses on developing a language model that can understand users describe their situation in the 911 call, enabling LLAMA2 to analyze the content and offer relevant instructions to the telecommunicator, while also creating workflows to notify government agencies with the caller's information when necessary. Another benefit this language model provides is its ability to assist people during a significant emergency incident when the 911 system is overwhelme
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;Hermite&#20989;&#25968;&#26681;&#20316;&#20026;&#37197;&#28857;&#65292;&#25552;&#39640;&#20102;&#35299;&#20915;&#20108;&#32500;&#34203;&#23450;&#35860;&#26041;&#31243;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#65292;&#19982;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#21644;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10649</link><description>&lt;p&gt;
Hermite&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#35299;&#20915;&#20108;&#32500;&#34203;&#23450;&#35860;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Hermite Neural Network Simulation for Solving the 2D Schrodinger Equation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10649
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;Hermite&#20989;&#25968;&#26681;&#20316;&#20026;&#37197;&#28857;&#65292;&#25552;&#39640;&#20102;&#35299;&#20915;&#20108;&#32500;&#34203;&#23450;&#35860;&#26041;&#31243;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#65292;&#19982;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#21644;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv: 2402.10649v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#34203;&#23450;&#35860;&#26041;&#31243;&#26159;&#25551;&#36848;&#37327;&#23376;&#21147;&#23398;&#31995;&#32479;&#20013;&#27874;&#20989;&#25968;&#34892;&#20026;&#30340;&#25968;&#23398;&#26041;&#31243;&#12290;&#23427;&#26159;&#19968;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#37327;&#23376;&#21147;&#23398;&#22522;&#26412;&#21407;&#29702;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#19982;&#22522;&#20110;Hermite&#20989;&#25968;&#30340;&#37197;&#28857;&#26041;&#27861;&#26469;&#20197;&#36275;&#22815;&#30340;&#31934;&#24230;&#35299;&#20915;&#34203;&#23450;&#35860;&#26041;&#31243;&#12290;&#26368;&#21021;&#65292;Hermite&#20989;&#25968;&#30340;&#26681;&#34987;&#29992;&#20316;&#37197;&#28857;&#65292;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#12290;&#34203;&#23450;&#35860;&#26041;&#31243;&#22312;&#26080;&#38480;&#22495;&#20013;&#23450;&#20041;&#65292;&#20351;&#29992;Hermite&#20989;&#25968;&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#23548;&#33268;&#20102;&#20986;&#33394;&#30340;&#31934;&#24230;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;MATLAB&#30340;Simulink&#24037;&#20855;&#36827;&#34892;&#20102;&#27169;&#25311;&#12290;&#28982;&#21518;&#23558;&#32467;&#26524;&#19982;&#20351;&#29992;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33719;&#24471;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10649v1 Announce Type: cross  Abstract: The Schrodinger equation is a mathematical equation describing the wave function's behavior in a quantum-mechanical system. It is a partial differential equation that provides valuable insights into the fundamental principles of quantum mechanics. In this paper, the aim was to solve the Schrodinger equation with sufficient accuracy by using a mixture of neural networks with the collocation method base Hermite functions. Initially, the Hermite functions roots were employed as collocation points, enhancing the efficiency of the solution. The Schrodinger equation is defined in an infinite domain, the use of Hermite functions as activation functions resulted in excellent precision. Finally, the proposed method was simulated using MATLAB's Simulink tool. The results were then compared with those obtained using Physics-informed neural networks and the presented method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#30740;&#31350;&#25991;&#26412;&#20013;&#30340;&#35805;&#35821;&#29305;&#24449;&#26469;&#21306;&#20998;&#20154;&#31867;&#21019;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#36825;&#20123;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#20889;&#20316;&#22312;&#32467;&#26500;&#19978;&#26356;&#20026;&#22810;&#26679;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10586</link><description>&lt;p&gt;
&#32454;&#24494;&#20043;&#32447;&#65306;&#36890;&#36807;&#35805;&#35821;&#20027;&#39064;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#30740;&#31350;&#25991;&#26412;&#20013;&#30340;&#35805;&#35821;&#29305;&#24449;&#26469;&#21306;&#20998;&#20154;&#31867;&#21019;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#36825;&#20123;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#20889;&#20316;&#22312;&#32467;&#26500;&#19978;&#26356;&#20026;&#22810;&#26679;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#31867;&#21019;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#20043;&#38388;&#30340;&#30028;&#38480;&#21464;&#24471;&#26085;&#30410;&#27169;&#31946;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35782;&#21035;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#20013;&#21487;&#36776;&#35782;&#21644;&#29420;&#29305;&#30340;&#35821;&#35328;&#29305;&#24615;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#25581;&#31034;&#25991;&#26412;&#22312;&#34920;&#38754;&#32467;&#26500;&#20043;&#22806;&#30340;&#28508;&#22312;&#35805;&#35821;&#32467;&#26500;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#35770;&#65292;&#25105;&#20204;&#21033;&#29992;&#23618;&#27425;&#21270;&#35299;&#26512;&#26641;&#21644;&#36882;&#24402;&#36229;&#22270;&#26469;&#25581;&#31034;LLM&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#29420;&#29305;&#35805;&#35821;&#27169;&#24335;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;LLM&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#37117;&#21463;&#29305;&#23450;&#39046;&#22495;&#30340;&#24433;&#21709;&#32780;&#20135;&#29983;&#19981;&#21516;&#30340;&#35805;&#35821;&#27169;&#24335;&#65292;&#20294;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#34920;&#29616;&#20986;&#26356;&#22810;&#30340;&#32467;&#26500;&#21464;&#24322;&#24615;&#65292;&#21453;&#26144;&#20102;&#19981;&#21516;&#39046;&#22495;&#20154;&#31867;&#20889;&#20316;&#30340;&#24494;&#22937;&#24615;&#36136;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24341;&#20837;&#23618;&#27425;&#35805;&#35821;&#29305;&#24449;&#21487;&#20197;&#22686;&#24378;&#20108;&#20803;&#20998;&#31867;&#22120;&#22312;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10586v1 Announce Type: new  Abstract: With the advent of large language models (LLM), the line between human-crafted and machine-generated texts has become increasingly blurred. This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures. Introducing a novel methodology, we leverage hierarchical parse trees and recursive hypergraphs to unveil distinctive discourse patterns in texts produced by both LLMs and humans. Empirical findings demonstrate that, although both LLMs and humans generate distinct discourse patterns influenced by specific domains, human-written texts exhibit more structural variability, reflecting the nuanced nature of human writing in different domains. Notably, incorporating hierarchical discourse features enhances binary classifiers' overall performance in distinguishing between huma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10567</link><description>&lt;p&gt;
&#22312;InSaAF&#20013;&#34701;&#20837;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615; | LLM&#26159;&#21542;&#24050;&#32463;&#20934;&#22791;&#22909;&#36827;&#20837;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#65311;
&lt;/p&gt;
&lt;p&gt;
InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#25552;&#20986;&#20102;&#20247;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25191;&#34892;&#27861;&#24459;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#20174;&#39044;&#27979;&#21028;&#20915;&#21040;&#29983;&#25104;&#25688;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#24182;&#23637;&#31034;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;&#28041;&#21450;&#31038;&#20250;&#22240;&#32032;&#26102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;$\beta$-&#21152;&#26435;&#30340;$\textit{&#27861;&#24459;&#23433;&#20840;&#20998;&#25968;($LSS_{\beta}$)}$&#65292;&#23558;LLM&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#20004;&#20010;&#26041;&#38754;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;LLM&#22312;$\textit{&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;}$&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#20854;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#23637;&#31034;&#26469;&#35780;&#20272;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;LLaMA&#21644;LLaMA--2&#27169;&#22411;&#30340;&#20219;&#21153;&#34920;&#29616;&#21644;&#20844;&#24179;&#24471;&#20998;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10567v1 Announce Type: cross  Abstract: Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs' safety by considering its performance in the $\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society. Task performance and fairness scores of LLaMA and LLaMA--2 models indicate th
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#31361;&#20986;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#23433;&#20840;&#24615;&#21644;&#20581;&#22766;&#24615;&#20851;&#38190;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#24182;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10340</link><description>&lt;p&gt;
&#35770;&#37096;&#32626;LLMs/VLMs&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#23384;&#22312;&#30340;&#23433;&#20840;&#38382;&#39064;&#65306;&#31361;&#26174;&#39118;&#38505;&#21644;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10340
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#31361;&#20986;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#23433;&#20840;&#24615;&#21644;&#20581;&#22766;&#24615;&#20851;&#38190;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#24182;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#25972;&#21512;&#21040;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25152;&#28041;&#21450;&#30340;&#20581;&#22766;&#24615;&#21644;&#23433;&#20840;&#24615;&#20851;&#38190;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#21033;&#29992;LLMs&#21644;VLMs&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#20219;&#21153;&#65288;&#22914;&#25805;&#20316;&#65292;&#23548;&#33322;&#31561;&#65289;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#20250;&#24341;&#20837;&#26174;&#30528;&#30340;&#28431;&#27934;&#65292;&#21363;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#24694;&#24847;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36890;&#36807;&#30740;&#31350;LLMs/VLMs&#19982;&#26426;&#22120;&#20154;&#30028;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36731;&#26494;&#25805;&#32437;&#25110;&#35823;&#23548;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#65292;&#23548;&#33268;&#23433;&#20840;&#38544;&#24739;&#12290;&#25105;&#20204;&#23450;&#20041;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#21487;&#33021;&#30340;&#24694;&#24847;&#25915;&#20987;&#31034;&#20363;&#65292;&#24182;&#23545;&#38598;&#25104;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#30693;&#21517;&#26426;&#22120;&#20154;&#26694;&#26550;&#65288;&#21253;&#25324;KnowNo VIMA&#21644;Instruct2Act&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10340v1 Announce Type: cross  Abstract: In this paper, we highlight the critical issues of robustness and safety associated with integrating large language models (LLMs) and vision-language models (VLMs) into robotics applications. Recent works have focused on using LLMs and VLMs to improve the performance of robotics tasks, such as manipulation, navigation, etc. However, such integration can introduce significant vulnerabilities, in terms of their susceptibility to adversarial attacks due to the language models, potentially leading to catastrophic consequences. By examining recent works at the interface of LLMs/VLMs and robotics, we show that it is easy to manipulate or misguide the robot's actions, leading to safety hazards. We define and provide examples of several plausible adversarial attacks, and conduct experiments on three prominent robot frameworks integrated with a language model, including KnowNo VIMA, and Instruct2Act, to assess their susceptibility to these atta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10207</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22870;&#21169;&#65306;&#22522;&#20110;&#21160;&#24577;&#20559;&#22909;&#35843;&#25972;&#30340;&#22810;&#30446;&#26631;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#22810;&#30446;&#26631;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#26159;&#23454;&#29616;&#26377;&#30410;&#21644;&#26080;&#23475;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#36890;&#24120;&#26159;&#26114;&#36149;&#19988;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#32500;&#24230;&#12289;&#24322;&#36136;&#24615;&#21644;&#20914;&#31361;&#24615;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#21462;&#20915;&#20110;&#20854;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#22870;&#21169;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#36827;&#34892;&#23545;&#40784;&#12290;RiC&#30340;&#26174;&#33879;&#29305;&#28857;&#26159;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#23545;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;&#21463;&#21040;&#25277;&#35937;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25512;&#29702;&#26102;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10184</link><description>&lt;p&gt;
&#37325;&#22609;RLHF&#20013;&#30340;&#20449;&#24687;&#32467;&#26500;&#65306;&#22522;&#20110;&#22270;&#35770;&#30340;&#22870;&#21169;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#23384;&#22312;&#19968;&#20010;&#19977;&#38590;&#38382;&#39064;&#65306;&#39640;&#24230;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;RLHF&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#20854;&#25551;&#32472;&#20026;&#25991;&#26412;&#20998;&#24067;&#19978;&#30340;&#33258;&#21160;&#32534;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24418;&#24335;&#21270;&#20102;RLHF&#30446;&#26631;&#65292;&#21363;&#30830;&#20445;&#20154;&#31867;&#20559;&#22909;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34892;&#20026;&#20043;&#38388;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;RLHF&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#20449;&#24687;&#32467;&#26500;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#30340;&#22870;&#21169;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#22270;&#35770;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#27867;&#21270;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#24494;&#35843;&#20043;&#21069;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#24433;&#21709;&#36739;&#22909;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2402.10100</link><description>&lt;p&gt;
&#35843;&#35856;&#65306;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#24494;&#35843;&#20043;&#21069;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#24433;&#21709;&#36739;&#22909;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#38480;&#21046;&#26465;&#20214;&#26159;&#20197;&#21453;&#26144;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#25910;&#38598;&#30340;&#23567;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21253;&#25324;DenseNet&#21644;ConvNeXt&#22312;&#20869;&#30340;CNN&#27169;&#22411;&#65292;&#20197;&#21450;ViT&#12289;SWIN&#21644;AST&#31561;&#36716;&#25442;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#35832;&#22914;YAMNet&#21644;VGGish&#30340;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24378;&#35843;&#20102;&#22312;&#29305;&#23450;&#20020;&#24202;&#25968;&#25454;&#19978;&#24494;&#35843;&#20043;&#21069;&#65292;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#20174;&#21330;&#20013;&#24739;&#32773;&#20013;&#26032;&#25910;&#38598;&#20102;&#20004;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#24739;&#32773;&#38899;&#39057;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#21457;&#29616;&#22522;&#20110;&#23427;&#20204;&#20174;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;RGB&#21644;&#28784;&#24230;&#35889;&#22270;&#36716;&#25442;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#21487;&#20197;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#65292;&#20854;&#20013;DenseNet-Contrastive&#21644;AST&#27169;&#22411;&#34920;&#29616;&#31361;&#20986;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10100v1 Announce Type: cross  Abstract: This study assesses deep learning models for audio classification in a clinical setting with the constraint of small datasets reflecting real-world prospective data collection. We analyze CNNs, including DenseNet and ConvNeXt, alongside transformer models like ViT, SWIN, and AST, and compare them against pre-trained audio models such as YAMNet and VGGish. Our method highlights the benefits of pre-training on large datasets before fine-tuning on specific clinical data. We prospectively collected two first-of-their-kind patient audio datasets from stroke patients. We investigated various preprocessing techniques, finding that RGB and grayscale spectrogram transformations affect model performance differently based on the priors they learn from pre-training. Our findings indicate CNNs can match or exceed transformer models in small dataset contexts, with DenseNet-Contrastive and AST models showing notable performance. This study highlights
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#22686;&#24378;&#37329;&#34701;&#34892;&#19994;&#30340;&#32593;&#32476;&#23433;&#20840;&#38887;&#24615;&#65292;&#24182;&#23454;&#29616;&#39640;&#32423;&#23041;&#32961;&#26816;&#27979;&#12290;&#30446;&#21069;&#30340;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#24448;&#24448;&#22522;&#20110;&#35268;&#21017;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#27861;&#36866;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#24212;&#29992;&#65292;&#24182;&#19988;&#26080;&#27861;&#26377;&#25928;&#26816;&#27979;&#26410;&#30693;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.09820</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#37329;&#34701;&#34892;&#19994;&#30340;&#32593;&#32476;&#23433;&#20840;&#38887;&#24615;&#65292;&#23454;&#29616;&#39640;&#32423;&#23041;&#32961;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cybersecurity Resilience in Finance with Deep Learning for Advanced Threat Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09820
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#22686;&#24378;&#37329;&#34701;&#34892;&#19994;&#30340;&#32593;&#32476;&#23433;&#20840;&#38887;&#24615;&#65292;&#24182;&#23454;&#29616;&#39640;&#32423;&#23041;&#32961;&#26816;&#27979;&#12290;&#30446;&#21069;&#30340;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#24448;&#24448;&#22522;&#20110;&#35268;&#21017;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#27861;&#36866;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#24212;&#29992;&#65292;&#24182;&#19988;&#26080;&#27861;&#26377;&#25928;&#26816;&#27979;&#26410;&#30693;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#32852;&#32593;&#26102;&#20195;&#65292;&#20154;&#20204;&#30340;&#29983;&#27963;&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;&#20170;&#22825;&#30340;&#32593;&#32476;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#25216;&#26415;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#32473;&#20154;&#20204;&#24102;&#26469;&#20415;&#21033;&#30340;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#23433;&#20840;&#25361;&#25112;&#12290;&#20445;&#25345;&#32593;&#32476;&#23433;&#20840;&#21644;&#20445;&#25252;&#29992;&#25143;&#30340;&#21512;&#27861;&#21033;&#30410;&#26159;&#32593;&#32476;&#24314;&#35774;&#30340;&#26680;&#24515;&#12290;&#23041;&#32961;&#26816;&#27979;&#26159;&#19968;&#20010;&#23436;&#25972;&#26377;&#25928;&#30340;&#38450;&#24481;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#32593;&#32476;&#20449;&#24687;&#23433;&#20840;&#39046;&#22495;&#65292;&#32593;&#32476;&#25915;&#20987;&#21644;&#32593;&#32476;&#38450;&#25252;&#30340;&#25216;&#26415;&#26356;&#26032;&#26085;&#30410;&#36805;&#29467;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#26816;&#27979;&#26410;&#30693;&#23041;&#32961;&#26159;&#32593;&#32476;&#38450;&#25252;&#30340;&#20851;&#27880;&#28966;&#28857;&#20043;&#19968;&#12290;&#30446;&#21069;&#65292;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#36890;&#24120;&#22522;&#20110;&#35268;&#21017;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21019;&#24314;&#20154;&#24037;&#35268;&#21017;&#25110;&#25552;&#21462;&#24120;&#35265;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24212;&#29992;&#65292;&#24182;&#19988;&#26410;&#30693;&#23041;&#32961;&#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#31995;&#32479;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09820v1 Announce Type: cross  Abstract: In the age of the Internet, people's lives are increasingly dependent on today's network technology. However, network technology is a double-edged sword, bringing convenience to people but also posing many security challenges. Maintaining network security and protecting the legitimate interests of users is at the heart of network construction. Threat detection is an important part of a complete and effective defense system. In the field of network information security, the technical update of network attack and network protection is spiraling. How to effectively detect unknown threats is one of the concerns of network protection. Currently, network threat detection is usually based on rules and traditional machine learning methods, which create artificial rules or extract common spatiotemporal features, which cannot be applied to large-scale data applications, and the emergence of unknown threats causes the detection accuracy of the or
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22402;&#30452;&#21644;&#27700;&#24179;&#26102;&#38388;&#25509;&#36817;&#24230;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.09784</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#25509;&#36817;&#24230;&#19978;&#30340;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09784
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22402;&#30452;&#21644;&#27700;&#24179;&#26102;&#38388;&#25509;&#36817;&#24230;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#26368;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#25429;&#25417;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#21333;&#21521;&#21644;&#21452;&#21521;&#27169;&#24335;&#65292;&#20294;&#23545;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#22914;&#20010;&#20307;&#34892;&#20026;&#21644;&#31038;&#20250;&#36235;&#21183;&#27169;&#24335;&#65292;&#20173;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#26368;&#36817;&#30340;&#27169;&#22411;&#36890;&#24120;&#24573;&#30053;&#20102;&#22312;&#31867;&#20284;&#30340;&#26102;&#38388;&#27573;&#20869;&#38544;&#21547;&#22312;&#29992;&#25143;&#20043;&#38388;&#21457;&#29983;&#30340;&#29992;&#25143;&#34892;&#20026;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#22402;&#30452;&#26102;&#38388;&#25509;&#36817;&#24230;&#12290;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#36890;&#36807;&#36866;&#24212;Transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#26469;&#32771;&#34385;&#29992;&#25143;&#34892;&#20026;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#21516;&#26102;&#65292;&#36825;&#31181;&#36866;&#24212;&#22312;&#32771;&#34385;&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#27700;&#24179;&#26102;&#38388;&#25509;&#36817;&#24230;&#26041;&#38754;&#20173;&#28982;&#26377;&#38480;&#65292;&#20363;&#22914;&#21306;&#20998;&#22312;&#19968;&#21608;&#20869;&#19982;&#19968;&#20010;&#26376;&#20869;&#36141;&#20080;&#30340;&#36830;&#32493;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09784v1 Announce Type: cross  Abstract: Sequential recommender systems identify user preferences from their past interactions to predict subsequent items optimally. Although traditional deep-learning-based models and modern transformer-based models in previous studies capture unidirectional and bidirectional patterns within user-item interactions, the importance of temporal contexts, such as individual behavioral and societal trend patterns, remains underexplored. Notably, recent models often neglect similarities in users' actions that occur implicitly among users during analogous timeframes-a concept we term vertical temporal proximity. These models primarily adapt the self-attention mechanisms of the transformer to consider the temporal context in individual user actions. Meanwhile, this adaptation still remains limited in considering the horizontal temporal proximity within item interactions, like distinguishing between subsequent item purchases within a week versus a mon
&lt;/p&gt;</description></item><item><title>&#23613;&#31649;&#27169;&#22411;&#32534;&#36753;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20462;&#35746;&#30693;&#35782;&#30340;&#28508;&#21147;&#65292;&#20294;&#23569;&#37327;&#32534;&#36753;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#23849;&#28291;&#65292;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22256;&#24785;&#24230;&#20316;&#20026;&#26367;&#20195;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20854;&#19982;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09656</link><description>&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#30340;&#34676;&#34678;&#25928;&#24212;&#65306;&#23569;&#37327;&#32534;&#36753;&#21487;&#33021;&#24341;&#21457;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09656
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#27169;&#22411;&#32534;&#36753;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20462;&#35746;&#30693;&#35782;&#30340;&#28508;&#21147;&#65292;&#20294;&#23569;&#37327;&#32534;&#36753;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#23849;&#28291;&#65292;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22256;&#24785;&#24230;&#20316;&#20026;&#26367;&#20195;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20854;&#19982;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#27169;&#22411;&#32534;&#36753;&#24050;&#26174;&#31034;&#20986;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#20462;&#35746;&#30693;&#35782;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#23545;LLMs&#30340;&#20869;&#22312;&#33021;&#21147;&#30340;&#24433;&#21709;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#20851;&#38190;&#29616;&#35937;&#65306;&#21363;&#20351;&#21482;&#36827;&#34892;&#19968;&#20010;&#32534;&#36753;&#65292;&#20063;&#21487;&#20197;&#24341;&#21457;&#27169;&#22411;&#23849;&#28291;&#65292;&#34920;&#29616;&#20026;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#27599;&#27425;&#32534;&#36753;&#21518;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#34429;&#28982;&#24517;&#35201;&#65292;&#20294;&#32791;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22256;&#24785;&#24230;&#20316;&#20026;&#26367;&#20195;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20854;&#19982;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#39034;&#24207;&#32534;&#36753;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#36825;&#26159;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#19968;&#31181;&#24120;&#35265;&#24773;&#20917;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#25105;&#20204;&#20043;&#21069;&#21333;&#27425;&#32534;&#36753;&#30740;&#31350;&#20013;&#30340;&#22256;&#38590;&#26696;&#20363;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20960;&#20046;&#25152;&#26377;&#30740;&#31350;&#30340;&#32534;&#36753;&#26041;&#27861;&#37117;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09656v1 Announce Type: new  Abstract: Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating its strong correlation with downstream task performance. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#36935;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#36825;&#23545;&#20110;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.09588</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26032;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Emerging Opportunities of Using Large Language Language Models for Translation Between Drug Molecules and Indications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#36935;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#36825;&#23545;&#20110;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#20998;&#23376;&#26159;&#19968;&#31181;&#25913;&#21464;&#29983;&#29289;&#20307;&#31934;&#31070;&#25110;&#36523;&#20307;&#29366;&#24577;&#30340;&#29289;&#36136;&#12290;&#27599;&#31181;&#25209;&#20934;&#30340;&#33647;&#29289;&#37117;&#26377;&#36866;&#24212;&#30151;&#65292;&#25351;&#30340;&#26159;&#35813;&#33647;&#29289;&#27835;&#30103;&#29305;&#23450;&#21307;&#30103;&#26465;&#20214;&#30340;&#27835;&#30103;&#29992;&#36884;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#19968;&#31181;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#22312;&#20998;&#23376;&#21644;&#20854;&#25991;&#26412;&#25551;&#36848;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#20173;&#23384;&#22312;&#20851;&#20110;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#65288;&#25110;&#21453;&#20043;&#65289;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#24212;&#29992;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#36825;&#21487;&#33021;&#26497;&#22823;&#22320;&#26377;&#30410;&#20110;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#12290;&#20174;&#32473;&#23450;&#30340;&#36866;&#24212;&#30151;&#29983;&#25104;&#33647;&#29289;&#30340;&#33021;&#21147;&#23558;&#20801;&#35768;&#21457;&#29616;&#38024;&#23545;&#29305;&#23450;&#30142;&#30149;&#25110;&#38774;&#28857;&#30340;&#33647;&#29289;&#65292;&#24182;&#26368;&#32456;&#20026;&#24739;&#32773;&#25552;&#20379;&#26356;&#22909;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#33647;&#29289;&#20998;&#23376;&#21644;&#30456;&#24212;&#36866;&#24212;&#30151;&#20043;&#38388;&#30340;&#32763;&#35793;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#23454;&#39564;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09588v1 Announce Type: new  Abstract: A drug molecule is a substance that changes the organism's mental or physical state. Every approved drug has an indication, which refers to the therapeutic use of that drug for treating a particular medical condition. While the Large Language Model (LLM), a generative Artificial Intelligence (AI) technique, has recently demonstrated effectiveness in translating between molecules and their textual descriptions, there remains a gap in research regarding their application in facilitating the translation between drug molecules and indications, or vice versa, which could greatly benefit the drug discovery process. The capability of generating a drug from a given indication would allow for the discovery of drugs targeting specific diseases or targets and ultimately provide patients with better treatments. In this paper, we first propose a new task, which is the translation between drug molecules and corresponding indications, and then test exi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#24212;&#23545;&#28145;&#24230;&#20266;&#36896;&#23041;&#32961;&#30340;&#25919;&#31574;&#24314;&#35758;&#65292;&#21253;&#25324;&#32972;&#26223;&#20449;&#24687;&#12289;&#21361;&#23475;&#12289;&#20808;&#21069;&#30340;&#31435;&#27861;&#25552;&#26696;&#20197;&#21450;&#20840;&#38754;&#30340;&#28145;&#24230;&#20266;&#36896;&#20379;&#24212;&#38142;&#25919;&#31574;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.09581</link><description>&lt;p&gt;
&#25171;&#20987;&#28145;&#24230;&#20266;&#36896;&#65306;&#24212;&#23545;&#22269;&#23478;&#23433;&#20840;&#23041;&#32961;&#21644;&#20405;&#29359;&#26435;&#21033;&#30340;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Combatting deepfakes: Policies to address national security threats and rights violations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#24212;&#23545;&#28145;&#24230;&#20266;&#36896;&#23041;&#32961;&#30340;&#25919;&#31574;&#24314;&#35758;&#65292;&#21253;&#25324;&#32972;&#26223;&#20449;&#24687;&#12289;&#21361;&#23475;&#12289;&#20808;&#21069;&#30340;&#31435;&#27861;&#25552;&#26696;&#20197;&#21450;&#20840;&#38754;&#30340;&#28145;&#24230;&#20266;&#36896;&#20379;&#24212;&#38142;&#25919;&#31574;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#24212;&#23545;&#28145;&#24230;&#20266;&#36896;&#23041;&#32961;&#30340;&#25919;&#31574;&#24314;&#35758;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#28145;&#24230;&#20266;&#36896;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#24182;&#22238;&#39038;&#20102;&#23427;&#20204;&#25152;&#24102;&#26469;&#30340;&#21361;&#23475;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#30446;&#21069;&#28145;&#24230;&#20266;&#36896;&#34987;&#29992;&#20110;&#20256;&#25773;&#24615;&#34384;&#24453;&#26448;&#26009;&#12289;&#36827;&#34892;&#27450;&#35784;&#12289;&#25805;&#32437;&#36873;&#27665;&#34892;&#20026;&#20197;&#21450;&#23545;&#22269;&#23478;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#30340;&#24773;&#20917;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20808;&#21069;&#30340;&#31435;&#27861;&#25552;&#26696;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#38382;&#39064;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#25919;&#31574;&#24314;&#35758;&#65292;&#37325;&#28857;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#20379;&#24212;&#38142;&#30340;&#22810;&#20010;&#29615;&#33410;&#12290;&#28145;&#24230;&#20266;&#36896;&#20379;&#24212;&#38142;&#20174;&#23569;&#25968;&#27169;&#22411;&#24320;&#21457;&#32773;&#12289;&#27169;&#22411;&#25552;&#20379;&#32773;&#21644;&#35745;&#31639;&#25552;&#20379;&#32773;&#24320;&#22987;&#65292;&#25193;&#23637;&#33267;&#25968;&#21313;&#20159;&#28508;&#22312;&#30340;&#28145;&#24230;&#20266;&#36896;&#21046;&#20316;&#32773;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#36825;&#20010;&#20379;&#24212;&#38142;&#65292;&#24182;&#35828;&#26126;&#27599;&#20010;&#29615;&#33410;&#30340;&#23454;&#20307;&#37117;&#24212;&#37319;&#21462;&#21512;&#29702;&#25514;&#26045;&#65292;&#38450;&#27490;&#28145;&#24230;&#20266;&#36896;&#30340;&#21046;&#36896;&#21644;&#20256;&#25773;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#23545;&#31574;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09581v1 Announce Type: cross  Abstract: This paper provides policy recommendations to address threats from deepfakes. First, we provide background information about deepfakes and review the harms they pose. We describe how deepfakes are currently used to proliferate sexual abuse material, commit fraud, manipulate voter behavior, and pose threats to national security. Second, we review previous legislative proposals designed to address deepfakes. Third, we present a comprehensive policy proposal that focuses on addressing multiple parts of the deepfake supply chain. The deepfake supply chain begins with a small number of model developers, model providers, and compute providers, and it expands to include billions of potential deepfake creators. We describe this supply chain in greater detail and describe how entities at each step of the supply chain ought to take reasonable measures to prevent the creation and proliferation of deepfakes. Finally, we address potential counterpo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32593;&#32476;&#29305;&#24449;&#22312;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30340;PageRank&#31639;&#27861;&#26469;&#25429;&#25417;&#27450;&#35784;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;PPR&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#24182;&#25552;&#20379;&#29420;&#29305;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.09495</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#32593;&#32476;&#29305;&#24449;&#22312;&#27450;&#35784;&#26816;&#27979;&#20013;&#28508;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Potential of Network-Based Features for Fraud Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32593;&#32476;&#29305;&#24449;&#22312;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30340;PageRank&#31639;&#27861;&#26469;&#25429;&#25417;&#27450;&#35784;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;PPR&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#24182;&#25552;&#20379;&#29420;&#29305;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20132;&#26131;&#27450;&#35784;&#32473;&#20225;&#19994;&#21644;&#28040;&#36153;&#32773;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#32463;&#27982;&#25439;&#22833;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#38590;&#20197;&#36319;&#19978;&#27450;&#35784;&#25112;&#26415;&#30340;&#28436;&#21464;&#65292;&#23548;&#33268;&#39640;&#35823;&#25253;&#29575;&#21644;&#28431;&#25253;&#29575;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#35782;&#21035;&#27450;&#35784;&#27169;&#24335;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#20010;&#24615;&#21270;&#30340;PageRank&#65288;PPR&#65289;&#31639;&#27861;&#36890;&#36807;&#20998;&#26512;&#37329;&#34701;&#36134;&#25143;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#25429;&#25417;&#27450;&#35784;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#27604;&#36739;&#20256;&#32479;&#29305;&#24449;&#19982;&#28155;&#21152;PPR&#22312;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;PPR&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;PPR&#29305;&#24449;&#25552;&#20379;&#20102;&#29420;&#29305;&#32780;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#36890;&#36807;&#20854;&#39640;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#24471;&#20197;&#35777;&#26126;&#12290;&#29305;&#24449;&#31283;&#23450;&#24615;&#20998;&#26512;&#35777;&#23454;&#20102;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09495v1 Announce Type: cross  Abstract: Online transaction fraud presents substantial challenges to businesses and consumers, risking significant financial losses. Conventional rule-based systems struggle to keep pace with evolving fraud tactics, leading to high false positive rates and missed detections. Machine learning techniques offer a promising solution by leveraging historical data to identify fraudulent patterns. This article explores using the personalised PageRank (PPR) algorithm to capture the social dynamics of fraud by analysing relationships between financial accounts. The primary objective is to compare the performance of traditional features with the addition of PPR in fraud detection models. Results indicate that integrating PPR enhances the model's predictive power, surpassing the baseline model. Additionally, the PPR feature provides unique and valuable information, evidenced by its high feature importance score. Feature stability analysis confirms consist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LlaSMol&#65292;&#23427;&#26159;&#19968;&#31181;&#25512;&#36827;&#21270;&#23398;&#39046;&#22495;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;LlaSMol&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;GPT-4&#24182;&#25509;&#36817;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09391</link><description>&lt;p&gt;
LlaSMol:&#21033;&#29992;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#25512;&#36827;&#21270;&#23398;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LlaSMol&#65292;&#23427;&#26159;&#19968;&#31181;&#25512;&#36827;&#21270;&#23398;&#39046;&#22495;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;LlaSMol&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;GPT-4&#24182;&#25509;&#36817;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#22312;&#33647;&#29289;&#30740;&#21457;&#21644;&#26448;&#26009;&#31185;&#23398;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#29616;&#26377;&#24037;&#20316;&#34920;&#26126;&#23427;&#20204;&#22312;&#21270;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20196;&#20154;&#22833;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#24320;&#21457;&#30340;LLM&#22312;&#19968;&#31995;&#21015;&#21270;&#23398;&#20219;&#21153;&#19978;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GPT-4&#65292;&#24182;&#25509;&#36817;SoTA&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#25105;&#20204;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#26159;&#19968;&#20010;&#21517;&#20026;SMolInstruct&#30340;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;&#20102;14&#20010;&#32463;&#36807;&#31934;&#24515;&#25361;&#36873;&#30340;&#21270;&#23398;&#20219;&#21153;&#21644;&#36229;&#36807;&#19977;&#30334;&#19975;&#20010;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#21270;&#23398;LLM&#22880;&#23450;&#20102;&#22362;&#23454;&#22522;&#30784;&#12290;&#22522;&#20110;SMolInstruct&#65292;&#25105;&#20204;&#23545;&#19968;&#32452;&#24320;&#28304;LLM&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20854;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;Mistral ser&#26159;&#26368;&#20339;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09391v1 Announce Type: new Abstract: Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models. The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct. It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral ser
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23545;&#27604;&#20102;&#20154;&#31867;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#20855;&#26377;&#21363;&#26102;&#27010;&#25324;&#33021;&#21147;&#65292;&#32780;DNNs&#23384;&#22312;&#28382;&#21518;&#27010;&#25324;&#29616;&#35937;&#65292;&#36825;&#34920;&#26126;&#20102;&#34920;&#31034;&#20998;&#27495;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.09303</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#30340;&#21363;&#26102;&#27010;&#25324;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28382;&#21518;&#27010;&#25324;&#8212;&#8212;&#34920;&#31034;&#20998;&#27495;&#30340;&#35777;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Immediate generalisation in humans but a generalisation lag in deep neural networks$\unicode{x2014}$evidence for representational divergence?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09303
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#27604;&#20102;&#20154;&#31867;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#20855;&#26377;&#21363;&#26102;&#27010;&#25324;&#33021;&#21147;&#65292;&#32780;DNNs&#23384;&#22312;&#28382;&#21518;&#27010;&#25324;&#29616;&#35937;&#65292;&#36825;&#34920;&#26126;&#20102;&#34920;&#31034;&#20998;&#27495;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#23545;&#27604;&#20102;&#20154;&#31867;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#35768;&#22810;&#34892;&#20026;&#27604;&#36739;&#12290;&#36890;&#24120;&#65292;&#27604;&#36739;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#23398;&#20064;&#36807;&#31243;&#30340;&#26368;&#32456;&#32467;&#26524;&#65292;&#36890;&#36807;&#27979;&#37327;&#21644;&#27604;&#36739;&#30446;&#26631;&#31867;&#21035;&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34920;&#31034;&#22914;&#20309;&#24418;&#25104;&#21363;&#20854;&#36807;&#31243;&#8212;&#8212;&#21363;&#22312;&#33719;&#21462;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#21464;&#21270;&#21644;&#20013;&#38388;&#38454;&#27573;&#8212;&#8212;&#24448;&#24448;&#23569;&#26377;&#30452;&#25509;&#21644;&#23454;&#35777;&#30340;&#27604;&#36739;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#23545;&#20154;&#31867;&#35266;&#23519;&#32773;&#21644;&#19981;&#21516;&#32463;&#20856;&#19982;&#26368;&#26032;&#25216;&#26415;&#30340;DNNs&#20013;&#21487;&#36716;&#31227;&#34920;&#31034;&#26159;&#22914;&#20309;&#34987;&#33719;&#21462;&#30340;&#30340;&#35814;&#32454;&#35843;&#26597;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#38480;&#30340;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#65292;&#35813;&#29615;&#22659;&#20013;&#25105;&#20204;&#23545;&#40784;&#20102;&#23398;&#20064;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#22914;&#36215;&#22987;&#28857;&#12289;&#36755;&#20837;&#27169;&#24335;&#12289;&#21487;&#29992;&#36755;&#20837;&#25968;&#25454;&#20197;&#21450;&#25552;&#20379;&#30340;&#21453;&#39304;&#12290;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#25105;&#20204;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09303v1 Announce Type: cross Abstract: Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification. Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed. However, the process of how these representations emerge$\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\unicode{x2014}$is less often directly and empirically compared.   Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs. We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided. Across the whole learning process we evaluate 
&lt;/p&gt;</description></item><item><title>SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08653</link><description>&lt;p&gt;
SAGMAN: &#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#24418;&#19978;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08653
&lt;/p&gt;
&lt;p&gt;
SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23545;&#36755;&#20837;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#21464;&#21270;&#25935;&#24863;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;SAGMAN&#30340;&#35889;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#39564;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;GNN&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#27969;&#24418;&#20043;&#38388;&#24341;&#36215;&#30340;&#36317;&#31163;&#22833;&#30495;: &#24403;&#36755;&#20837;&#27969;&#34892;&#20013;&#20004;&#20010;&#38468;&#36817;&#30340;&#33410;&#28857;&#65288;&#36890;&#36807;GNN&#27169;&#22411;&#65289;&#34987;&#26144;&#23556;&#21040;&#36755;&#20986;&#27969;&#34892;&#19978;&#30340;&#20004;&#20010;&#36828;&#31163;&#30340;&#33410;&#28857;&#26102;&#65292;&#24847;&#21619;&#30528;&#23384;&#22312;&#36739;&#22823;&#30340;&#36317;&#31163;&#22833;&#30495;&#65292;&#20174;&#32780;&#23548;&#33268;GNN&#30340;&#31283;&#23450;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#65288;GDR&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#35889;&#22270;&#23884;&#20837;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#26469;&#21019;&#24314;&#20302;&#32500;&#30340;&#36755;&#20837;/&#36755;&#20986;&#22522;&#20110;&#22270;&#30340;&#27969;&#24418;&#65292;&#20197;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;SAGMAN&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#27599;&#20010;&#33410;&#28857;&#22312;&#38754;&#23545;&#19981;&#21516;&#36793;&#32536;&#25110;&#29305;&#24449;&#25200;&#21160;&#26102;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability. We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature pe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38544;&#31169;&#21644;&#39118;&#26684;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08631</link><description>&lt;p&gt;
&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Knowledge Editing on Black-box Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38544;&#31169;&#21644;&#39118;&#26684;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#39640;&#25928;&#12289;&#31934;&#30830;&#22320;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#26356;&#26032;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#32780;&#19981;&#23545;&#20854;&#20182;&#30693;&#35782;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#30333;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#19978;&#65292;&#24573;&#35270;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#22330;&#26223;&#65306;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65292;&#21363;&#36890;&#36807;&#25509;&#21475;&#35775;&#38382;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20165;&#21487;&#29992;&#25991;&#26412;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#22312;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#19978;&#19981;&#36866;&#29992;&#19988;&#32570;&#20047;&#20840;&#38754;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#39118;&#26684;&#20445;&#30041;&#30340;&#35780;&#20272;&#32435;&#20837;&#20854;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#20013;&#30340;&#32534;&#36753;&#25968;&#25454;&#38544;&#31169;&#27844;&#28431;&#21644;&#39118;&#26684;&#36807;&#24230;&#32534;&#36753;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#36890;&#36807;&#19979;&#28216;&#21518;&#22788;&#29702;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#21407;&#22987;&#22238;&#31572;&#36827;&#34892;&#32454;&#31890;&#24230;&#32534;&#36753;&#26469;&#20445;&#25345;&#25991;&#26412;&#39118;&#26684;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#19982;&#20998;&#26512;&#34920;&#26126;&#65292;postEdit&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available. To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#37325;&#22797;&#20844;&#24179;&#20998;&#21106;&#38382;&#39064;&#20013;&#30340;&#31454;&#20105;&#31574;&#30053;&#65292;&#21457;&#29616;&#22914;&#26524;Bob&#36807;&#20110;&#20559;&#22909;&#26576;&#19968;&#22359;&#34507;&#31957;&#65292;Alice&#21033;&#29992;&#31867;&#20284;&#20110;&#20108;&#20998;&#26597;&#25214;&#30340;&#31574;&#30053;&#21487;&#20197;&#31995;&#32479;&#24615;&#22320;&#23545;Bob&#23454;&#26045;&#21093;&#21066;&#65292;&#20174;&#32780;&#22312;&#26102;&#38388;&#19978;&#33719;&#24471;&#26356;&#22810;&#36164;&#28304;&#20221;&#39069;&#12290;</title><link>https://arxiv.org/abs/2402.08547</link><description>&lt;p&gt;
&#25581;&#31034;&#20102;&#22312;&#37325;&#22797;&#30340;&#34507;&#31957;&#20999;&#21106;&#20013;&#33521;&#21191;&#31454;&#20105;&#30340;&#33402;&#26415;
&lt;/p&gt;
&lt;p&gt;
Dueling Over Dessert, Mastering the Art of Repeated Cake Cutting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08547
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#37325;&#22797;&#20844;&#24179;&#20998;&#21106;&#38382;&#39064;&#20013;&#30340;&#31454;&#20105;&#31574;&#30053;&#65292;&#21457;&#29616;&#22914;&#26524;Bob&#36807;&#20110;&#20559;&#22909;&#26576;&#19968;&#22359;&#34507;&#31957;&#65292;Alice&#21033;&#29992;&#31867;&#20284;&#20110;&#20108;&#20998;&#26597;&#25214;&#30340;&#31574;&#30053;&#21487;&#20197;&#31995;&#32479;&#24615;&#22320;&#23545;Bob&#23454;&#26045;&#21093;&#21066;&#65292;&#20174;&#32780;&#22312;&#26102;&#38388;&#19978;&#33719;&#24471;&#26356;&#22810;&#36164;&#28304;&#20221;&#39069;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#29609;&#23478;&#65292;&#20998;&#21035;&#20195;&#34920;Alice&#21644;Bob&#65292;&#36890;&#36807;&#23545;&#34507;&#31957;&#30340;&#20010;&#20154;&#20272;&#20540;&#36827;&#34892;&#20102;&#37325;&#22797;&#30340;&#20844;&#24179;&#20998;&#21106;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#20250;&#20986;&#29616;&#19968;&#22359;&#19982;&#20043;&#21069;&#36718;&#27425;&#30456;&#21516;&#30340;&#26032;&#34507;&#31957;&#12290;Alice&#22312;&#33258;&#24049;&#36873;&#25321;&#30340;&#19968;&#20010;&#28857;&#19978;&#20999;&#21106;&#34507;&#31957;&#65292;&#32780;Bob&#36873;&#25321;&#24038;&#36793;&#30340;&#37096;&#20998;&#25110;&#21491;&#36793;&#30340;&#37096;&#20998;&#65292;&#25226;&#21097;&#20313;&#30340;&#32473;Alice&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#21464;&#31181;&#65306;&#39034;&#24207;&#27169;&#24335;&#65292;Bob&#22312;&#36873;&#25321;&#24038;/&#21491;&#20043;&#21069;&#35266;&#23519;Alice&#30340;&#20999;&#21106;&#28857;&#65307;&#21516;&#26102;&#27169;&#24335;&#65292;&#20182;&#21482;&#22312;&#20570;&#20986;&#36873;&#25321;&#21518;&#35266;&#23519;&#22905;&#30340;&#20999;&#21106;&#28857;&#12290;&#21516;&#26102;&#27169;&#24335;&#26159;&#30001;Aumann&#21644;Maschler&#65288;1995&#65289;&#39318;&#27425;&#25552;&#20986;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;Bob&#20960;&#20046;&#26159;&#30446;&#20809;&#30701;&#27973;&#30340;&#65292;&#24182;&#19988;&#32463;&#24120;&#36873;&#25321;&#33258;&#24049;&#21916;&#27426;&#30340;&#37096;&#20998;&#65292;&#37027;&#20040;&#20182;&#21487;&#20197;&#34987;Alice&#36890;&#36807;&#31867;&#20284;&#20110;&#20108;&#20998;&#26597;&#25214;&#30340;&#31574;&#30053;&#31995;&#32479;&#24615;&#22320;&#21033;&#29992;&#12290;&#36825;&#20010;&#31574;&#30053;&#20351;&#24471;Alice&#21487;&#20197;&#36234;&#26469;&#36234;&#31934;&#30830;&#22320;&#27169;&#25311;Bob&#30340;&#20559;&#22909;&#65292;&#20174;&#32780;&#22312;&#26102;&#38388;&#19978;&#33719;&#24471;&#19981;&#25104;&#27604;&#20363;&#30340;&#36164;&#28304;&#20221;&#39069;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#29609;&#23478;&#33021;&#22815;&#21033;&#29992;&#20854;&#20248;&#21183;&#30340;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the setting of repeated fair division between two players, denoted Alice and Bob, with private valuations over a cake. In each round, a new cake arrives, which is identical to the ones in previous rounds. Alice cuts the cake at a point of her choice, while Bob chooses the left piece or the right piece, leaving the remainder for Alice. We consider two versions: sequential, where Bob observes Alice's cut point before choosing left/right, and simultaneous, where he only observes her cut point after making his choice. The simultaneous version was first considered by Aumann and Maschler (1995).   We observe that if Bob is almost myopic and chooses his favorite piece too often, then he can be systematically exploited by Alice through a strategy akin to a binary search. This strategy allows Alice to approximate Bob's preferences with increasing precision, thereby securing a disproportionate share of the resource over time.   We analyze the limits of how much a player can exploit t
&lt;/p&gt;</description></item><item><title>ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08303</link><description>&lt;p&gt;
ChatCell: &#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
ChatCell: Facilitating Single-Cell Analysis with Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08303
&lt;/p&gt;
&lt;p&gt;
ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#22312;&#31185;&#23398;&#20013;&#30340;&#24433;&#21709;&#26085;&#30410;&#31361;&#20986;&#12290;LLMs&#22312;&#20219;&#21153;&#27867;&#21270;&#21644;&#33258;&#30001;&#23545;&#35805;&#26041;&#38754;&#30340;&#26032;&#20852;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#25512;&#36827;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#36825;&#20010;&#26500;&#25104;&#29983;&#29289;&#20307;&#22522;&#30784;&#26500;&#20214;&#30340;&#39046;&#22495;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#24403;&#21069;&#26041;&#27861;&#22312;&#30693;&#35782;&#38376;&#27099;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;LLMs&#22312;&#25484;&#25569;&#21333;&#32454;&#32990;&#25968;&#25454;&#26041;&#38754;&#30340;&#20805;&#20998;&#21033;&#29992;&#65292;&#24433;&#21709;&#20102;&#30452;&#25509;&#21487;&#35775;&#38382;&#21644;&#24555;&#36895;&#36845;&#20195;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ChatCell&#65292;&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#22312;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#39046;&#22495;&#33719;&#24471;&#20102;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
&lt;/p&gt;</description></item><item><title>LLaGA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.08170</link><description>&lt;p&gt;
LLaGA: &#22823;&#22411;&#35821;&#35328;&#21644;&#22270;&#24418;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
LLaGA: Large Language and Graph Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08170
&lt;/p&gt;
&lt;p&gt;
LLaGA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#25512;&#21160;&#20102;&#22270;&#32467;&#26500;&#25968;&#25454;&#20998;&#26512;&#30340;&#36827;&#27493;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#30340;&#23835;&#36215;&#39044;&#31034;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#36824;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#30001;&#20110;&#23558;&#22270;&#32467;&#26500;&#36716;&#21270;&#20026;&#25991;&#26412;&#30340;&#22266;&#26377;&#38590;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#8212;&#8212;&#22823;&#22411;&#35821;&#35328;&#21644;&#22270;&#24418;&#21161;&#25163;&#65288;LLaGA&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;LLM&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;LLaGA&#20445;&#30041;&#20102;LLM&#30340;&#36890;&#29992;&#24615;&#65292;&#21516;&#26102;&#23558;&#22270;&#25968;&#25454;&#36716;&#21270;&#20026;&#19982;LLM&#36755;&#20837;&#20860;&#23481;&#30340;&#26684;&#24335;&#12290;LLaGA&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{G}raph \textbf{A}ssistant (\textbf{LLaGA}), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and 
&lt;/p&gt;</description></item><item><title>UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07939</link><description>&lt;p&gt;
UFO: &#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#20132;&#20114;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
UFO: A UI-Focused Agent for Windows OS Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07939
&lt;/p&gt;
&lt;p&gt;
UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;UFO&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20102;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;UFO&#37319;&#29992;&#21452;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#31934;&#30830;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#21644;&#25511;&#21046;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#26234;&#33021;&#20307;&#21487;&#20197;&#26080;&#32541;&#22320;&#22312;&#21333;&#20010;&#24212;&#29992;&#31243;&#24207;&#20869;&#20197;&#21450;&#36328;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#23548;&#33322;&#21644;&#25805;&#20316;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#21363;&#20351;&#28041;&#21450;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#21160;&#20316;&#36830;&#25509;&#65292;&#24182;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#12290;&#22240;&#27492;&#65292;UFO&#23558;&#33392;&#24040;&#32780;&#32791;&#26102;&#30340;&#36807;&#31243;&#36716;&#21464;&#20026;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#23601;&#21487;&#20197;&#23436;&#25104;&#30340;&#31616;&#21333;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;9&#20010;&#27969;&#34892;&#30340;Windows&#24212;&#29992;&#31243;&#24207;&#19978;&#23545;UFO&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#21453;&#26144;&#29992;&#25143;&#26085;&#24120;&#20351;&#29992;&#24773;&#26223;&#30340;&#21508;&#31181;&#24773;&#20917;&#12290;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#21644;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#24471;&#20986;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;UFO&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;GTT&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#36890;&#36947;&#32423;&#21035;&#30340;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07570</link><description>&lt;p&gt;
&#21482;&#26377;&#26354;&#32447;&#24418;&#29366;&#26377;&#20851;&#65306;&#36890;&#36807;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07570
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;GTT&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#36890;&#36947;&#32423;&#21035;&#30340;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;General Time Transformer (GTT)&#65292;&#19968;&#31181;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;GTT&#22312;&#19968;&#20010;&#21253;&#21547;2&#20159;&#20010;&#39640;&#36136;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#39046;&#22495;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20219;&#21153;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#36880;&#36890;&#36947;&#30340;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#38750;&#37325;&#21472;&#30340;&#26354;&#32447;&#24418;&#29366;&#65292;&#20855;&#26377;&#32479;&#19968;&#30340;&#25968;&#20540;&#22823;&#23567;&#12290;GTT&#22312;&#36890;&#36947;&#32423;&#21035;&#19978;&#36890;&#36807;&#39044;&#27979;&#36807;&#21435;&#26354;&#32447;&#24418;&#29366;&#30340;&#31383;&#21475;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GTT&#22312;&#26410;&#35265;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#38646;&#26679;&#26412;&#22810;&#20803;&#39044;&#27979;&#33021;&#21147;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;GTT&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#35268;&#27169;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#35266;&#23519;&#21040;&#22312;&#38646;&#26679;&#26412;&#22810;&#20803;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#35268;&#27169;&#23450;&#24459;&#20063;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains. In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude. GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner. Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines. Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;DGIB&#65289;&#33021;&#22815;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#12290;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#24341;&#23548;&#21644;&#25913;&#36827;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#65292;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#24182;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#33021;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06716</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Information Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06716
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;DGIB&#65289;&#33021;&#22815;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#12290;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#24341;&#23548;&#21644;&#25913;&#36827;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#65292;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#24182;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#33021;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#24191;&#27867;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23427;&#20204;&#25658;&#24102;&#30528;&#22797;&#26434;&#30340;&#26102;&#31354;&#29305;&#24449;&#27169;&#24335;&#65292;&#23545;&#20110;&#23427;&#20204;&#30340;&#34920;&#31034;&#23398;&#20064;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGNNs&#65289;&#36890;&#36807;&#21033;&#29992;&#20869;&#22312;&#30340;&#21160;&#24577;&#24615;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;DGNNs&#23637;&#31034;&#20102;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#65292;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;DGIB&#65289;&#26694;&#26550;&#26469;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#12290;&#20511;&#21161;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#21407;&#29702;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#26399;&#26395;&#30340;&#26368;&#20248;&#34920;&#31034;&#24212;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#65288;MSC&#65289;&#26465;&#20214;&#12290;&#20026;&#20102;&#22312;&#28508;&#22312;&#34920;&#31034;&#20013;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#21644;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;DGIB&#36845;&#20195;&#22320;&#24341;&#23548;&#21644;&#25913;&#36827;&#36890;&#36807;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#12290;&#20026;&#20102;&#28385;&#36275;MSC&#26465;&#20214;&#65292;&#25105;&#20204;&#23558;&#25972;&#20307;IB&#30446;&#26631;&#20998;&#35299;&#20026;DGIB$_{MS}$&#21644;DGIB$_C$&#65292;&#20854;&#20013;DGIB$_{MS}$&#36890;&#36947;&#30340;&#30446;&#26631;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the DGIB$_{MS}$ channel aims 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06529</link><description>&lt;p&gt;
&#20869;&#30465;&#35268;&#21010;&#65306;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#36890;&#36807;&#36866;&#24403;&#30340;&#22522;&#30784;&#22609;&#36896;&#26469;&#31574;&#30053;&#24615;&#22320;&#36827;&#34892;&#39640;&#32423;&#34892;&#21160;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;LLM&#20135;&#29983;&#30340;&#24187;&#35273;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#33258;&#20449;&#22320;&#25191;&#34892;&#19982;&#29992;&#25143;&#30446;&#26631;&#19981;&#31526;&#25110;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#19981;&#23433;&#20840;&#30340;&#35745;&#21010;&#12290;&#27492;&#22806;&#65292;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#21487;&#33021;&#24341;&#21457;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#36873;&#39033;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;LLMs&#24517;&#39035;&#35782;&#21035;&#27492;&#31867;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#24341;&#23548;LLMs&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#24418;&#25104;&#24847;&#35782;&#21040;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#25191;&#34892;&#35745;&#21010;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;&#26426;&#22120;&#20154;&#35268;&#21010;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#35777;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38190;&#20540;&#32422;&#26463;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#39537;&#36880;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#21147;&#24471;&#20998;&#21644;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;RoCo&#31574;&#30053;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.06262</link><description>&lt;p&gt;
&#20851;&#20110;&#38024;&#23545;&#38190;&#20540;&#32422;&#26463;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#39537;&#36880;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38190;&#20540;&#32422;&#26463;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#39537;&#36880;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#21147;&#24471;&#20998;&#21644;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;RoCo&#31574;&#30053;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#23545;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#36807;&#24230;&#38656;&#27714;&#65292;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#37096;&#32626;&#20173;&#28982;&#26114;&#36149;&#12290;&#38500;&#20102;&#27169;&#22411;&#21442;&#25968;&#22806;&#65292;&#38190;&#20540;&#32531;&#23384;&#20063;&#23384;&#20648;&#22312;GPU&#20869;&#23384;&#20013;&#65292;&#38543;&#30528;&#25209;&#22788;&#29702;&#22823;&#23567;&#21644;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#32447;&#24615;&#22686;&#38271;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#38024;&#23545;&#32473;&#23450;&#39044;&#31639;&#19979;&#32500;&#25252;&#38190;&#20540;&#32531;&#23384;&#24320;&#38144;&#30340;&#39537;&#36880;&#31574;&#30053;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#29616;&#26377;&#39537;&#36880;&#31574;&#30053;&#22312;&#37325;&#35201;&#24615;&#35780;&#20998;&#35745;&#31639;&#21644;&#39537;&#36880;&#33539;&#22260;&#26500;&#24314;&#20004;&#20010;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20808;&#21069;&#31574;&#30053;&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#21147;&#24471;&#20998;&#21644;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;RoCo&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#32531;&#23384;&#39537;&#36880;&#31574;&#30053;&#12290;&#28085;&#30422;&#20102;&#39044;&#22635;&#20805;&#21644;&#33258;&#22238;&#24402;&#35299;&#30721;&#38454;&#27573;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;RoCo&#30340;&#20248;&#36234;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;RoCo&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#20379;&#30740;&#31350;&#32773;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success associated with Large Language Models~(LLMs), they are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands. In addition to model parameters, the key-value cache is also stored in GPU memory, growing linearly with batch size and sequence length. As a remedy, recent works have proposed various eviction policies for maintaining the overhead of key-value cache under a given budget. This paper embarks on the efficacy of existing eviction policies in terms of \textit{importance score calculation} and \textit{eviction scope construction}. We identify the deficiency of prior policies in these two aspects and introduce RoCo, a \underline{r}\underline{o}bust \underline{c}ache \underline{o}mission policy based on temporal attention scores and robustness measures. Extensive experimentation spanning prefilling and auto-regressive decoding stages validates the superiority of RoCo. Finally, we relea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.06025</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#27010;&#29575;&#25512;&#29702;&#36827;&#34892;&#23454;&#39564;&#19982;&#20462;&#35746;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#12290;&#35813;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#65292;&#21363;&#20351;&#35268;&#21017;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#23398;&#20064;&#32773;&#20063;&#20250;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#27169;&#31946;&#27010;&#29575;&#35268;&#21017;&#65292;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#26681;&#25454;&#36817;&#20284;&#36125;&#21494;&#26031;&#21407;&#21017;&#22312;&#27599;&#27425;&#23454;&#39564;&#21518;&#22312;&#32447;&#26356;&#26032;&#33258;&#24049;&#30340;&#20551;&#35774;&#12290;&#22312;&#21516;&#19968;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36824;&#26681;&#25454;&#20449;&#24687;&#35770;&#20934;&#21017;&#24314;&#31435;&#20102;&#23454;&#39564;&#35774;&#35745;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#19977;&#20010;&#21407;&#21017;&#30340;&#32452;&#21512;&#8212;&#8212;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#8212;&#8212;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#21435;&#25481;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#32452;&#20214;&#37117;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#35299;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build a computational model of how humans actively infer hidden rules by doing experiments. The basic principles behind the model is that, even if the rule is deterministic, the learner considers a broader space of fuzzy probabilistic rules, which it represents in natural language, and updates its hypotheses online after each experiment according to approximately Bayesian principles. In the same framework we also model experiment design according to information-theoretic criteria. We find that the combination of these three principles -- explicit hypotheses, probabilistic rules, and online updates -- can explain human performance on a Zendo-style task, and that removing any of these components leaves the model unable to account for the data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#24037;&#31243;&#26694;&#26550;(REOM)&#20197;&#23558;&#32534;&#35793;&#21518;&#30340;&#35774;&#22791;&#19978;TFLite&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35843;&#35797;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.05493</link><description>&lt;p&gt;
&#25506;&#32034;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Investigating White-Box Attacks for On-Device Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#24037;&#31243;&#26694;&#26550;(REOM)&#20197;&#23558;&#32534;&#35793;&#21518;&#30340;&#35774;&#22791;&#19978;TFLite&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35843;&#35797;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#21033;&#29992;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20174;&#30456;&#24212;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20013;&#36731;&#26131;&#25552;&#21462;&#20986;&#26469;&#12290;&#29616;&#26377;&#30340;&#35774;&#22791;&#19978;&#25915;&#20987;&#26041;&#27861;&#21482;&#33021;&#29983;&#25104;&#40657;&#30418;&#25915;&#20987;&#65292;&#36825;&#31181;&#26041;&#27861;&#36828;&#19981;&#22914;&#30333;&#30418;&#31574;&#30053;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;&#36825;&#26159;&#22240;&#20026;&#31227;&#21160;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22914;TFLite&#19981;&#25903;&#25345;&#26799;&#24230;&#35745;&#31639;&#65292;&#32780;&#26799;&#24230;&#35745;&#31639;&#23545;&#20110;&#30333;&#30418;&#25915;&#20987;&#31639;&#27861;&#26159;&#24517;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#21457;&#29616;&#21487;&#33021;&#20302;&#20272;&#20102;&#35774;&#22791;&#19978;&#25915;&#20987;&#30340;&#21361;&#23475;&#24615;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65306;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#30333;&#30418;&#31574;&#30053;&#30452;&#25509;&#21463;&#21040;&#25915;&#20987;&#65311;&#25105;&#20204;&#39318;&#20808;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#23558;&#35774;&#22791;&#19978;&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35843;&#35797;&#29256;&#26412;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#36870;&#21521;&#24037;&#31243;&#26694;&#26550;(REOM)&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#23558;&#32534;&#35793;&#21518;&#30340;&#35774;&#22791;&#19978;TFLite&#27169;&#22411;&#36870;&#21521;&#20026;&#21487;&#35843;&#35797;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;REOM
&lt;/p&gt;
&lt;p&gt;
Numerous mobile apps have leveraged deep learning capabilities. However, on-device models are vulnerable to attacks as they can be easily extracted from their corresponding mobile apps. Existing on-device attacking approaches only generate black-box attacks, which are far less effective and efficient than white-box strategies. This is because mobile deep learning frameworks like TFLite do not support gradient computing, which is necessary for white-box attacking algorithms. Thus, we argue that existing findings may underestimate the harmfulness of on-device attacks. To this end, we conduct a study to answer this research question: Can on-device models be directly attacked via white-box strategies? We first systematically analyze the difficulties of transforming the on-device model to its debuggable version, and propose a Reverse Engineering framework for On-device Models (REOM), which automatically reverses the compiled on-device TFLite model to the debuggable model. Specifically, REOM
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TASER&#26041;&#27861;&#65292;&#23427;&#26159;&#38024;&#23545;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05396</link><description>&lt;p&gt;
TASER: &#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#24555;&#36895;&#20934;&#30830;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05396
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TASER&#26041;&#27861;&#65292;&#23427;&#26159;&#38024;&#23545;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGNN&#65289;&#22312;&#21253;&#25324;&#27450;&#35784;&#26816;&#27979;&#21644;&#20869;&#23481;&#25512;&#33616;&#22312;&#20869;&#30340;&#21508;&#31181;&#37325;&#35201;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;TGNN&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#26102;&#38388;&#36807;&#26102;&#30340;&#38142;&#25509;&#21644;&#20559;&#26012;&#30340;&#20132;&#20114;&#20998;&#24067;&#12290;&#36825;&#20123;&#22122;&#22768;&#23548;&#33268;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20005;&#37325;&#25439;&#23475;&#20102;TGNN&#30340;&#20934;&#30830;&#24615;&#65306;&#65288;1&#65289;&#27169;&#22411;&#21463;&#21040;&#36739;&#24046;&#20132;&#20114;&#30340;&#30417;&#30563;&#65292;&#65288;2&#65289;&#22122;&#22768;&#36755;&#20837;&#23548;&#33268;&#32858;&#21512;&#28040;&#24687;&#30340;&#39640;&#26041;&#24046;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;TGNN&#21435;&#22122;&#25216;&#26415;&#24182;&#26410;&#32771;&#34385;&#27599;&#20010;&#33410;&#28857;&#30340;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#22122;&#22768;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#38754;&#20020;&#30528;&#36941;&#21382;&#26356;&#22810;&#37051;&#23621;&#23548;&#33268;&#20135;&#29983;&#36807;&#22810;&#23567;&#25209;&#37327;&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#30456;&#20449;&#24555;&#36895;&#20934;&#30830;&#30340;TGNN&#30340;&#35299;&#20915;&#26041;&#27861;&#22312;&#20110;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TASER&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#36827;&#34892;&#20248;&#21270;&#30340;TGNN&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content recommendation. Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic graphs like time-deprecated links and skewed interaction distribution. The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are supervised by inferior interactions, and (2) noisy input induces high variance in the aggregated messages. However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node. In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors. We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling. In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.05119</link><description>&lt;p&gt;
&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Limitations of Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#65288;IT&#65289;&#26159;&#20351;&#29992;&#25351;&#20196;-&#22238;&#24212;&#23545;&#26469;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36807;&#31243;&#65292;&#24050;&#25104;&#20026;&#23558;&#22522;&#30784;&#39044;&#35757;&#32451;LLM&#36716;&#21270;&#20026;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#34429;&#28982;IT&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#24182;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#23616;&#38480;&#24615;&#21644;&#19981;&#36275;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#23545;LLM&#36890;&#36807;IT&#21457;&#29983;&#30340;&#21464;&#21270;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;IT&#30340;&#22810;&#31181;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;IT&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#25110;&#25216;&#33021;&#12290;LoRA&#24494;&#35843;&#20165;&#38480;&#20110;&#23398;&#20064;&#22238;&#24212;&#30340;&#21551;&#21160;&#21644;&#26679;&#24335;&#20196;&#29260;&#65292;&#32780;&#20840;&#21442;&#25968;&#24494;&#35843;&#20250;&#23548;&#33268;&#30693;&#35782;&#36864;&#21270;&#12290;&#65288;2&#65289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;IT&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#20250;&#23548;&#33268;&#22238;&#24212;&#36136;&#37327;&#19979;&#38477;&#12290;&#65288;3&#65289;&#20840;&#21442;&#25968;&#24494;&#35843;&#36890;&#36807;&#19981;&#20934;&#30830;&#22320;&#20174;IT&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#27010;&#24565;&#19978;&#30456;&#20284;&#23454;&#20363;&#30340;&#26631;&#35760;&#65292;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
&lt;/p&gt;</description></item><item><title>ScreenAI&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#20462;&#34917;&#31574;&#30053;&#21644;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#20197;&#21450;&#38024;&#23545;UI&#20803;&#32032;&#30340;&#23631;&#24149;&#27880;&#35299;&#20219;&#21153;&#30340;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04615</link><description>&lt;p&gt;
ScreenAI: &#29992;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ScreenAI: A Vision-Language Model for UI and Infographics Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04615
&lt;/p&gt;
&lt;p&gt;
ScreenAI&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#20462;&#34917;&#31574;&#30053;&#21644;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#20197;&#21450;&#38024;&#23545;UI&#20803;&#32032;&#30340;&#23631;&#24149;&#27880;&#35299;&#20219;&#21153;&#30340;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23631;&#24149;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#21644;&#20449;&#24687;&#22270;&#34920;&#22312;&#20154;&#31867;&#27807;&#36890;&#21644;&#20154;&#26426;&#20132;&#20114;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#20849;&#20139;&#30456;&#20284;&#30340;&#35270;&#35273;&#35821;&#35328;&#21644;&#35774;&#35745;&#21407;&#21017;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ScreenAI&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25913;&#36827;&#20102;PaLI&#26550;&#26500;&#65292;&#37319;&#29992;&#20102;pix2struct&#30340;&#28789;&#27963;&#20462;&#34917;&#31574;&#30053;&#65292;&#24182;&#32463;&#36807;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#12290;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#26680;&#24515;&#26159;&#19968;&#39033;&#26032;&#39062;&#30340;&#23631;&#24149;&#27880;&#35299;&#20219;&#21153;&#65292;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;UI&#20803;&#32032;&#30340;&#31867;&#22411;&#21644;&#20301;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25991;&#26412;&#27880;&#35299;&#26469;&#25551;&#36848;&#23631;&#24149;&#65292;&#24182;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#38382;&#31572;&#65288;QA&#65289;&#65292;UI&#23548;&#33322;&#21644;&#25688;&#35201;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#20197;&#23637;&#31034;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#22312;&#20165;&#26377;5B&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;ScreenAI&#22312;&#22522;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#30340;&#20219;&#21153;&#65288;&#22810;&#39029;&#25991;&#26723;VQA&#65292;WebSRC&#65292;MoTIF&#21644;Widget&#23383;&#24149;&#65289;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Screen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction. We introduce ScreenAI, a vision-language model that specializes in UI and infographics understanding. Our model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets. At the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements. We use these text annotations to describe screens to Large Language Models and automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. We run ablation studies to demonstrate the impact of these design choices. At only 5B parameters, ScreenAI achieves new state-of-the-artresults on UI- and infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget Captioning), and new best-in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;IDE&#30340;&#26412;&#22320;&#38598;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IDECoder&#26694;&#26550;&#65292;&#21033;&#29992;IDE&#25552;&#20379;&#30340;&#20934;&#30830;&#21644;&#23454;&#26102;&#30340;&#36328;&#25991;&#20214;&#20449;&#24687;&#26469;&#22686;&#24378;LLM-Based&#32534;&#30721;&#24037;&#20855;&#65292;&#35299;&#20915;&#20102;&#25361;&#25112;&#24615;&#30340;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03630</link><description>&lt;p&gt;
&#36890;&#36807;IDE&#27966;&#29983;&#30340;&#38745;&#24577;&#19978;&#19979;&#25991;&#30340;&#26412;&#22320;&#38598;&#25104;&#22686;&#24378;LLM-Based&#32534;&#30721;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03630
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;IDE&#30340;&#26412;&#22320;&#38598;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IDECoder&#26694;&#26550;&#65292;&#21033;&#29992;IDE&#25552;&#20379;&#30340;&#20934;&#30830;&#21644;&#23454;&#26102;&#30340;&#36328;&#25991;&#20214;&#20449;&#24687;&#26469;&#22686;&#24378;LLM-Based&#32534;&#30721;&#24037;&#20855;&#65292;&#35299;&#20915;&#20102;&#25361;&#25112;&#24615;&#30340;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#20195;&#30721;&#23436;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#65292;&#22914;Copilot&#31561;&#20195;&#30721;&#21161;&#25163;&#26381;&#21153;&#30340;&#37325;&#35201;&#20316;&#29992;&#25152;&#35777;&#26126;&#12290;&#30446;&#21069;&#30340;LLM&#36890;&#36807;&#23545;&#25991;&#20214;&#19978;&#19979;&#25991;&#30340;&#35757;&#32451;&#65292;&#22312;&#21333;&#20010;&#28304;&#25991;&#20214;&#30340;&#20195;&#30721;&#23436;&#25104;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38656;&#35201;&#36328;&#25991;&#20214;&#20449;&#24687;&#30340;&#22823;&#22411;&#36719;&#20214;&#39033;&#30446;&#30340;&#23384;&#20648;&#24211;&#32423;&#20195;&#30721;&#23436;&#25104;&#65292;&#23545;&#23427;&#20204;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#24211;&#32423;&#20195;&#30721;&#23436;&#25104;&#30740;&#31350;&#35782;&#21035;&#21644;&#25972;&#21512;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#65292;&#20294;&#22240;LLM&#30340;&#31934;&#24230;&#20302;&#21644;&#19978;&#19979;&#25991;&#38271;&#24230;&#26377;&#38480;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#35748;&#20026;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;(IDE)&#21487;&#20197;&#20026;&#23384;&#20648;&#24211;&#32423;&#20195;&#30721;&#23436;&#25104;&#25552;&#20379;&#30452;&#25509;&#12289;&#20934;&#30830;&#21644;&#23454;&#26102;&#30340;&#36328;&#25991;&#20214;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IDECoder&#65292;&#19968;&#20010;&#23454;&#38469;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;IDE&#30340;&#26412;&#22320;&#38745;&#24577;&#19978;&#19979;&#25991;&#36827;&#34892;&#36328;&#19978;&#19979;&#25991;&#26500;&#24314;&#21644;&#33258;&#25105;&#23436;&#21892;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;IDECoder&#21033;&#29992;&#20016;&#23500;&#30340;&#36328;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success in code completion, as evidenced by their essential roles in developing code assistant services such as Copilot. Being trained on in-file contexts, current LLMs are quite effective in completing code for single source files. However, it is challenging for them to conduct repository-level code completion for large software projects that require cross-file information. Existing research on LLM-based repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of LLMs. In this paper, we argue that Integrated Development Environments (IDEs) can provide direct, accurate and real-time cross-file information for repository-level code completion. We propose IDECoder, a practical framework that leverages IDE native static contexts for cross-context construction and diagnosis results for self-refinement. IDECoder utilizes the rich cross-context information 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#21253;&#25324;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#20998;&#26512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#35843;&#30740;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24615;&#12290;&#21516;&#26102;&#65292;&#35843;&#30740;&#36824;&#25552;&#20986;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.03358</link><description>&lt;p&gt;
&#22270;&#32553;&#20943;&#30340;&#32508;&#21512;&#35843;&#30740;&#65306;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03358
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#21253;&#25324;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#20998;&#26512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#35843;&#30740;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24615;&#12290;&#21516;&#26102;&#65292;&#35843;&#30740;&#36824;&#25552;&#20986;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#22270;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22270;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#30340;&#22686;&#21152;&#20026;&#20998;&#26512;&#21644;&#35745;&#31639;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#22270;&#32553;&#20943;&#25216;&#26415;&#22312;&#20445;&#30041;&#20851;&#38190;&#23646;&#24615;&#30340;&#21516;&#26102;&#31616;&#21270;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#22312;&#26412;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#22270;&#32553;&#20943;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#21253;&#25324;&#22270;&#31232;&#30095;&#21270;&#12289;&#22270;&#31895;&#21270;&#21644;&#22270;&#27987;&#32553;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#19968;&#23450;&#20041;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#23618;&#20998;&#31867;&#27861;&#26469;&#20998;&#31867;&#36825;&#20123;&#26041;&#27861;&#25152;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#35770;&#25991;&#21015;&#34920;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction techniques have gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at https://github.
&lt;/p&gt;</description></item><item><title>TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;</title><link>https://arxiv.org/abs/2402.02441</link><description>&lt;p&gt;
TopoX: &#19968;&#20010;&#29992;&#20110;&#25299;&#25169;&#22495;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TopoX: A Suite of Python Packages for Machine Learning on Topological Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02441
&lt;/p&gt;
&lt;p&gt;
TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;topox&#65292;&#19968;&#20010;&#25552;&#20379;&#21487;&#38752;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#65288;&#25193;&#23637;&#20102;&#22270;&#30340;&#39046;&#22495;&#65289;&#19978;&#36827;&#34892;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#65306;&#36229;&#22270;&#12289;&#21333;&#32431;&#12289;&#32990;&#33108;&#12289;&#36335;&#24452;&#21644;&#32452;&#21512;&#22797;&#21512;&#20307;&#12290;topox&#30001;&#19977;&#20010;&#36719;&#20214;&#21253;&#32452;&#25104;&#65306;toponetx&#29992;&#20110;&#26500;&#24314;&#21644;&#35745;&#31639;&#36825;&#20123;&#22495;&#65292;&#21253;&#25324;&#33410;&#28857;&#12289;&#36793;&#21644;&#39640;&#38454;&#21333;&#20803;&#30340;&#22788;&#29702;&#65307;topoembedx&#25552;&#20379;&#20102;&#23558;&#25299;&#25169;&#22495;&#23884;&#20837;&#21040;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#31639;&#27861;&#65292;&#22914;node2vec&#65307;topomodelx&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#20026;&#25299;&#25169;&#22495;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;topox&#30340;&#28304;&#20195;&#30721;&#32463;&#36807;&#24191;&#27867;&#30340;&#25991;&#26723;&#21270;&#21644;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#22312;https://github.com/pyt-team&#20197;MIT&#35768;&#21487;&#35777;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;12&#20010;&#20020;&#24202;&#19987;&#19994;&#20013;&#25552;&#20379;&#23433;&#20840;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#23450;&#21046;&#30340;&#22788;&#26041;&#38169;&#35823;&#35686;&#25253;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#35782;&#21035;&#33647;&#29289;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.01741</link><description>&lt;p&gt;
&#24320;&#21457;&#24182;&#27979;&#35797;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#33647;&#29289;&#23433;&#20840;&#30340;12&#31181;&#20020;&#24202;&#19987;&#19994;
&lt;/p&gt;
&lt;p&gt;
Development and Testing of a Novel Large Language Model-Based Clinical Decision Support Systems for Medication Safety in 12 Clinical Specialties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;12&#20010;&#20020;&#24202;&#19987;&#19994;&#20013;&#25552;&#20379;&#23433;&#20840;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#23450;&#21046;&#30340;&#22788;&#26041;&#38169;&#35823;&#35686;&#25253;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#35782;&#21035;&#33647;&#29289;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#65306;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;-&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#65292;&#29992;&#20110;&#23433;&#20840;&#29992;&#33647;&#22788;&#26041;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#20379;&#19982;&#24739;&#32773;&#32972;&#26223;&#21644;&#26426;&#26500;&#25351;&#21335;&#30456;&#20851;&#30340;&#22788;&#26041;&#38169;&#35823;&#35686;&#25253;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;CDSS&#30340;&#23616;&#38480;&#24615;&#12290;&#30446;&#26631;&#65306;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;LLM&#30340;CDSS&#22312;&#35782;&#21035;&#21508;&#31181;&#21307;&#23398;&#21644;&#22806;&#31185;&#30149;&#20363;&#20013;&#30340;&#33647;&#29289;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#20154;&#24037;&#19987;&#23478;&#23567;&#32452;&#36827;&#34892;&#27604;&#36739;&#12290;&#23427;&#36824;&#30740;&#31350;&#20102;&#20020;&#24202;&#21307;&#29983;&#22312;&#19981;&#21516;CDSS&#38598;&#25104;&#26041;&#24335;&#65288;&#21021;&#32423;&#33647;&#24072;&#12289;&#20165;&#22522;&#20110;LLM&#30340;CDSS&#21644;&#20108;&#32773;&#30340;&#32452;&#21512;&#65289;&#20013;&#30340;&#20559;&#22909;&#12290;&#35774;&#35745;&#12289;&#35774;&#32622;&#21644;&#21442;&#19982;&#32773;&#65306;&#21033;&#29992;&#24102;&#26377;GPT-4.0&#30340;RAG&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#28041;&#21450;12&#20010;&#19987;&#19994;&#20013;23&#20010;&#20020;&#24202;&#26696;&#20363;&#30340;61&#20010;&#22788;&#26041;&#38169;&#35823;&#22330;&#26223;&#12290;&#19987;&#23478;&#23567;&#32452;&#20351;&#29992;PCNE&#20998;&#31867;&#21644;NCC MERP&#25351;&#25968;&#35780;&#20272;&#36825;&#20123;&#26696;&#20363;&#12290;&#19977;&#21517;&#21021;&#32423;&#33647;&#24072;&#29420;&#31435;&#23457;&#26680;&#27599;&#20010;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#22788;&#29702;&#24314;&#35758;&#12290;&#26681;&#25454;&#26816;&#26597;&#30340;&#38169;&#35823;&#21644;&#24314;&#35758;&#32534;&#21046;&#20102;&#21453;&#39304;&#25253;&#21578;&#12290; &#28982;&#21518;&#65292;&#19977;&#21517;&#21307;&#29983;&#29420;&#31435;&#23457;&#26680;&#36825;&#20123;&#25253;&#21578;&#65292;&#24182;&#25552;&#20986;&#23545;&#19979;&#19968;&#27493;&#22788;&#29702;&#30340;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large Language Model (LLM) as a Clinical Decision Support System (CDSS) for safe medication prescription. This model addresses the limitations of traditional rule-based CDSS by providing relevant prescribing error alerts tailored to patient context and institutional guidelines.   Objective: The study evaluates the efficacy of an LLM-based CDSS in identifying medication errors across various medical and surgical case vignettes, compared to a human expert panel. It also examines clinician preferences among different CDSS integration modalities: junior pharmacist, LLM-based CDSS alone, and a combination of both.   Design, Setting, and Participants: Utilizing a RAG model with GPT-4.0, the study involved 61 prescribing error scenarios within 23 clinical vignettes across 12 specialties. An expert panel assessed these cases using the PCNE classification and NCC MERP index. Three junior pharmacists independently reviewed eac
&lt;/p&gt;</description></item><item><title>CapHuman&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#8220;&#32534;&#30721;&#28982;&#21518;&#23398;&#20064;&#23545;&#40784;&#8221;&#30340;&#33539;&#24335;&#23454;&#29616;&#20102;&#21487;&#27867;&#21270;&#30340;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#24773;&#22659;&#20013;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#30340;&#22836;&#37096;&#20301;&#32622;&#12289;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#29305;&#23450;&#20010;&#20307;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.00627</link><description>&lt;p&gt;
CapHuman: &#22312;&#24179;&#34892;&#23431;&#23449;&#20013;&#25429;&#25417;&#20320;&#30340;&#30636;&#38388;
&lt;/p&gt;
&lt;p&gt;
CapHuman: Capture Your Moments in Parallel Universes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00627
&lt;/p&gt;
&lt;p&gt;
CapHuman&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#8220;&#32534;&#30721;&#28982;&#21518;&#23398;&#20064;&#23545;&#40784;&#8221;&#30340;&#33539;&#24335;&#23454;&#29616;&#20102;&#21487;&#27867;&#21270;&#30340;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#24773;&#22659;&#20013;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#30340;&#22836;&#37096;&#20301;&#32622;&#12289;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#29305;&#23450;&#20010;&#20307;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#65292;&#21363;&#20165;&#32473;&#23450;&#19968;&#20010;&#21442;&#32771;&#38754;&#37096;&#29031;&#29255;&#65292;&#26399;&#26395;&#33021;&#22815;&#22312;&#19981;&#21516;&#24773;&#22659;&#20013;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#30340;&#22836;&#37096;&#20301;&#32622;&#12289;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#29305;&#23450;&#20010;&#20307;&#22270;&#20687;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#24212;&#20855;&#22791;&#20197;&#19979;&#26377;&#21033;&#29305;&#24449;&#65306;&#65288;1&#65289;&#23545;&#19990;&#30028;&#21644;&#20154;&#31867;&#31038;&#20250;&#26377;&#24378;&#22823;&#30340;&#35270;&#35273;&#21644;&#35821;&#20041;&#29702;&#35299;&#65292;&#29992;&#20110;&#22522;&#26412;&#29289;&#20307;&#21644;&#20154;&#31867;&#22270;&#20687;&#30340;&#29983;&#25104;&#65307;&#65288;2&#65289;&#21487;&#27867;&#21270;&#30340;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#65307;&#65288;3&#65289;&#28789;&#27963;&#32454;&#31890;&#24230;&#30340;&#22836;&#37096;&#25511;&#21046;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#25104;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#29983;&#25104;&#22522;&#30784;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19978;&#36848;&#20004;&#31181;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CapHuman&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#8220;&#32534;&#30721;&#28982;&#21518;&#23398;&#20064;&#23545;&#40784;&#8221;&#30340;&#33539;&#24335;&#65292;&#20026;&#26032;&#20010;&#20307;&#23454;&#29616;&#20102;&#21487;&#27867;&#21270;&#30340;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to align" paradigm, which enables generalizable identity preservation for new individuals without cum
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CTAug&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#26080;&#32541;&#25972;&#21512;&#21040;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#12290;&#36890;&#36807;&#25913;&#36827;&#22270;&#25299;&#25169;&#22686;&#24378;&#21644;&#22270;&#23398;&#20064;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#23545;&#21508;&#31181;&#22270;&#30340;&#34920;&#24449;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17580</link><description>&lt;p&gt;
&#20855;&#26377;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning with Cohesive Subgraph Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CTAug&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#26080;&#32541;&#25972;&#21512;&#21040;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#12290;&#36890;&#36807;&#25913;&#36827;&#22270;&#25299;&#25169;&#22686;&#24378;&#21644;&#22270;&#23398;&#20064;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#23545;&#21508;&#31181;&#22270;&#30340;&#34920;&#24449;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#24050;&#25104;&#20026;&#23398;&#20064;&#21508;&#31181;&#22270;&#34920;&#24449;&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#21253;&#25324;&#31038;&#20132;&#21644;&#29983;&#29289;&#21307;&#23398;&#32593;&#32476;&#12290;GCL&#24191;&#27867;&#20351;&#29992;&#38543;&#26426;&#22270;&#25299;&#25169;&#22686;&#24378;&#65292;&#22914;&#22343;&#21248;&#33410;&#28857;&#20002;&#22833;&#65292;&#29983;&#25104;&#22686;&#24378;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#38543;&#26426;&#22686;&#24378;&#21487;&#33021;&#20005;&#37325;&#25439;&#23475;&#22270;&#30340;&#20869;&#22312;&#23646;&#24615;&#24182;&#24694;&#21270;&#21518;&#32493;&#30340;&#34920;&#24449;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#22270;&#22686;&#24378;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#26377;&#21487;&#33021;&#25552;&#39640;GCL&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CTAug&#30340;&#26032;&#39062;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#26080;&#32541;&#22320;&#23558;&#20869;&#32858;&#24847;&#35782;&#25972;&#21512;&#21040;&#21508;&#31181;&#29616;&#26377;&#30340;GCL&#26426;&#21046;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;CTAug&#21253;&#25324;&#20004;&#20010;&#19987;&#38376;&#30340;&#27169;&#22359;&#65306;&#25299;&#25169;&#22686;&#24378;&#22686;&#24378;&#21644;&#22270;&#23398;&#20064;&#22686;&#24378;&#12290;&#21069;&#32773;&#29983;&#25104;&#35880;&#24910;&#20445;&#30041;&#20869;&#32858;&#24615;&#36136;&#30340;&#22686;&#24378;&#22270;&#65292;&#32780;&#21518;&#32773;&#22686;&#24378;&#20102;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning (GCL) has emerged as a state-of-the-art strategy for learning representations of diverse graphs including social and biomedical networks. GCL widely uses stochastic graph topology augmentation, such as uniform node dropping, to generate augmented graphs. However, such stochastic augmentations may severely damage the intrinsic properties of a graph and deteriorate the following representation learning process. We argue that incorporating an awareness of cohesive subgraphs during the graph augmentation and learning processes has the potential to enhance GCL performance. To this end, we propose a novel unified framework called CTAug, to seamlessly integrate cohesion awareness into various existing GCL mechanisms. In particular, CTAug comprises two specialized modules: topology augmentation enhancement and graph learning enhancement. The former module generates augmented graphs that carefully preserve cohesion properties, while the latter module bolsters the grap
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22810;&#38754;&#26495;&#35270;&#35273;&#38382;&#31572;&#65288;MultipanelVQA&#65289;&#22522;&#20934;&#25361;&#25112;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#23545;&#29702;&#35299;&#22810;&#38754;&#26495;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;LVLMs&#22312;&#36825;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.15847</link><description>&lt;p&gt;
&#26494;&#39292;&#36824;&#26159;&#21513;&#23043;&#23043;&#65311;&#29992;&#22810;&#38754;&#26495;VQA&#25361;&#25112;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15847
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22810;&#38754;&#26495;&#35270;&#35273;&#38382;&#31572;&#65288;MultipanelVQA&#65289;&#22522;&#20934;&#25361;&#25112;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#23545;&#29702;&#35299;&#22810;&#38754;&#26495;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;LVLMs&#22312;&#36825;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#38754;&#26495;&#22270;&#20687;&#65292;&#36890;&#24120;&#22312;&#32593;&#39029;&#25130;&#22270;&#12289;&#28023;&#25253;&#31561;&#20013;&#30475;&#21040;&#65292;&#20805;&#26021;&#30528;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#12290;&#36825;&#20123;&#22270;&#20687;&#20197;&#22810;&#20010;&#23376;&#22270;&#20197;&#19981;&#21516;&#24067;&#23616;&#32452;&#25104;&#65292;&#26377;&#25928;&#22320;&#21521;&#20154;&#20204;&#20256;&#36798;&#20449;&#24687;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#32423;&#30340;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#22914;&#33021;&#29702;&#35299;&#22797;&#26434;&#22330;&#26223;&#24182;&#22312;&#32593;&#39029;&#20013;&#23548;&#33322;&#30340;&#20195;&#29702;&#31243;&#24207;&#65292;&#22810;&#38754;&#26495;&#35270;&#35273;&#25512;&#29702;&#30340;&#25216;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#23545;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#38754;&#26495;&#35270;&#35273;&#38382;&#31572;&#65288;MultipanelVQA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6,600&#20010;&#38382;&#39064;&#12289;&#31572;&#26696;&#21644;&#22810;&#38754;&#26495;&#22270;&#20687;&#19977;&#20803;&#32452;&#65292;&#19987;&#38376;&#25361;&#25112;&#27169;&#22411;&#29702;&#35299;&#22810;&#38754;&#26495;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;MultipanelVQA&#22522;&#20934;&#20013;&#30340;&#38382;&#39064;&#23545;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#21363;&#20351;&#20154;&#31867;&#21487;&#20197;&#33719;&#24471;&#32422;99%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15847v2 Announce Type: replace-cross  Abstract: Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Large Vision Language Models (LVLMs) tested, even though humans can attain approximately 99\% accurac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#31639;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#65292;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#26412;&#32508;&#36848;&#21576;&#29616;&#20102;ERL&#39046;&#22495;&#30340;&#21508;&#20010;&#30740;&#31350;&#20998;&#25903;&#65292;&#31361;&#20986;&#20102;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#12289;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#36825;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.11963</link><description>&lt;p&gt;
&#36328;&#36234;&#36827;&#21270;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11963
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#31639;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#65292;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#26412;&#32508;&#36848;&#21576;&#29616;&#20102;ERL&#39046;&#22495;&#30340;&#21508;&#20010;&#30740;&#31350;&#20998;&#25903;&#65292;&#31361;&#20986;&#20102;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#12289;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#36825;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23558;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#36827;&#34892;&#20248;&#21270;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36890;&#36807;&#34701;&#21512;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;ERL&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;ERL&#20013;&#19981;&#21516;&#30740;&#31350;&#20998;&#25903;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#24635;&#32467;&#20102;&#30456;&#20851;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#30830;&#23450;&#20102;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#65306;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#65292;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#65292;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#20102;&#27599;&#20010;&#30740;&#31350;&#26041;&#21521;&#65292;&#32452;&#32455;&#20102;&#22810;&#20010;&#30740;&#31350;&#20998;&#25903;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#27599;&#20010;&#20998;&#25903;&#33268;&#21147;&#20110;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;EA&#21644;RL&#30340;&#25972;&#21512;&#22914;&#20309;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11963v2 Announce Type: replace-cross  Abstract: Evolutionary Reinforcement Learning (ERL), which integrates Evolutionary Algorithms (EAs) and Reinforcement Learning (RL) for optimization, has demonstrated remarkable performance advancements. By fusing the strengths of both approaches, ERL has emerged as a promising research direction. This survey offers a comprehensive overview of the diverse research branches in ERL. Specifically, we systematically summarize recent advancements in relevant algorithms and identify three primary research directions: EA-assisted optimization of RL, RL-assisted optimization of EA, and synergistic optimization of EA and RL. Following that, we conduct an in-depth analysis of each research direction, organizing multiple research branches. We elucidate the problems that each branch aims to tackle and how the integration of EA and RL addresses these challenges. In conclusion, we discuss potential challenges and prospective future research directions
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20197;&#25552;&#21319;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20559;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2401.11911</link><description>&lt;p&gt;
&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20197;&#22686;&#24378;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20197;&#25552;&#21319;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20559;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36741;&#21161;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20851;&#38190;&#65292;&#20294;&#23545;&#20110;LLMs&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#30340;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20173;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#30830;&#23450;LLMs&#30340;&#21709;&#24212;&#26159;&#28304;&#33258;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36824;&#26159;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21253;&#21547;&#30456;&#20114;&#20914;&#31361;&#30340;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#38382;&#39064;&#37117;&#19982;&#29983;&#25104;&#30340;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#37197;&#23545;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#20102;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#65288;&#22914;GPT-4/3.5&#21644;Llama2&#65289;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#26356;&#20542;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#36825;&#20123;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#23548;&#33268;&#36825;&#31181;&#20559;&#24046;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;i&#65289;LLMs&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36890;&#24120;&#19982;&#38382;&#39064;&#26356;&#30456;&#20284;&#65292;&#22686;&#21152;&#20102;&#20854;&#34987;&#36873;&#25321;&#30340;&#21487;&#33021;&#24615;&#65307;ii&#65289;&#26816;&#32034;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#30340;&#20998;&#21106;&#36807;&#31243;&#25171;&#26029;&#20102;&#20854;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a systematic framework to identify whether LLMs' responses, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To achieve this, we construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs (GPT-4/3.5 and Llama2) towards generated contexts, even when they provide incorrect information. We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) the segmentation process used in retrieved contexts disrupts their compl
&lt;/p&gt;</description></item><item><title>PsySafe&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#26234;&#33021;&#20307;&#24515;&#29702;&#23398;&#65292;&#25581;&#31034;&#26234;&#33021;&#20307;&#30340;&#40657;&#26263;&#24515;&#29702;&#29366;&#24577;&#23545;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#39118;&#38505;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2401.11880</link><description>&lt;p&gt;
PsySafe&#65306;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#23433;&#20840;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#30340;&#32508;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11880
&lt;/p&gt;
&lt;p&gt;
PsySafe&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#26234;&#33021;&#20307;&#24515;&#29702;&#23398;&#65292;&#25581;&#31034;&#26234;&#33021;&#20307;&#30340;&#40657;&#26263;&#24515;&#29702;&#29366;&#24577;&#23545;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#39118;&#38505;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#21152;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21518;&#65292;&#23637;&#29616;&#20986;&#20102;&#38598;&#20307;&#26234;&#33021;&#30340;&#28145;&#36828;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26234;&#33021;&#34987;&#24694;&#24847;&#20351;&#29992;&#21487;&#33021;&#24102;&#26469;&#37325;&#22823;&#39118;&#38505;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#21019;&#26032;&#30340;&#35270;&#35282;&#25506;&#32034;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#21457;&#29616;&#26234;&#33021;&#20307;&#30340;&#40657;&#26263;&#24515;&#29702;&#29366;&#24577;&#26500;&#25104;&#20102;&#23545;&#23433;&#20840;&#30340;&#37325;&#22823;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#26234;&#33021;&#20307;&#24515;&#29702;&#23398;&#20026;&#22522;&#30784;&#30340;&#32508;&#21512;&#26694;&#26550;&#65288;PsySafe&#65289;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65306;&#39318;&#20808;&#65292;&#35782;&#21035;&#26234;&#33021;&#20307;&#20013;&#30340;&#40657;&#26263;&#20154;&#26684;&#29305;&#24449;&#22914;&#20309;&#23548;&#33268;&#39118;&#38505;&#34892;&#20026;&#65307;&#20854;&#27425;&#65292;&#20174;&#24515;&#29702;&#21644;&#34892;&#20026;&#35282;&#24230;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65307;&#31532;&#19977;&#65292;&#21046;&#23450;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11880v2 Announce Type: replace-cross  Abstract: Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit profound capabilities in collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. In this paper, we explore these concerns through the innovative lens of agent psychology, revealing that the dark psychological states of agents constitute a significant threat to safety. To tackle these concerns, we propose a comprehensive framework (PsySafe) grounded in agent psychology, focusing on three key areas: firstly, identifying how dark personality traits in agents can lead to risky behaviors; secondly, evaluating the safety of multi-agent systems from the psychological and behavioral perspectives, and thirdly, devising effective strategies to mitigate these risks. Our experiments reveal
&lt;/p&gt;</description></item><item><title>LightDiC&#26159;&#22522;&#20110;&#30913;&#24615;&#25289;&#26222;&#25289;&#26031;&#30340;&#21487;&#25193;&#23637;&#26377;&#21521;&#22270;&#21367;&#31215;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31163;&#32447;&#39044;&#22788;&#29702;&#20013;&#36827;&#34892;&#25299;&#25169;&#30456;&#20851;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#12290;</title><link>https://arxiv.org/abs/2401.11772</link><description>&lt;p&gt;
LightDiC: &#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#26377;&#21521;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LightDiC: A Simple yet Effective Approach for Large-scale Digraph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11772
&lt;/p&gt;
&lt;p&gt;
LightDiC&#26159;&#22522;&#20110;&#30913;&#24615;&#25289;&#26222;&#25289;&#26031;&#30340;&#21487;&#25193;&#23637;&#26377;&#21521;&#22270;&#21367;&#31215;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31163;&#32447;&#39044;&#22788;&#29702;&#20013;&#36827;&#34892;&#25299;&#25169;&#30456;&#20851;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23616;&#38480;&#20110;&#26080;&#21521;&#22270;&#65292;&#20854;&#25429;&#25417;&#20851;&#31995;&#20449;&#24687;&#30340;&#33539;&#22260;&#21463;&#38480;&#21046;&#65292;&#38480;&#21046;&#20102;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#12290; &#30456;&#36739;&#20110;&#26080;&#21521;&#22270;&#65292;&#26377;&#21521;&#22270;&#65288;&#26377;&#21521;&#22270;&#65289;&#26356;&#36866;&#21512;&#24314;&#27169;&#26356;&#22797;&#26434;&#30340;&#25299;&#25169;&#31995;&#32479;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#26356;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#22914;&#21046;&#23450;&#20132;&#36890;&#21644;&#37329;&#34701;&#32593;&#32476;&#12290; &#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26377;&#21521;GNN&#65292;&#20294;&#23427;&#20204;&#30340;&#28789;&#24863;&#20027;&#35201;&#26469;&#33258;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#36825;&#23548;&#33268;&#20102;&#20887;&#20313;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#37327;&#65292;&#20351;&#20854;&#26080;&#27861;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#12290; &#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30913;&#24615;&#25289;&#26222;&#25289;&#26031;&#30340;&#21487;&#25193;&#23637;&#21464;&#31181;&#26377;&#21521;&#22270;&#21367;&#31215;&#65292;LightDiC&#12290; &#30001;&#20110;&#25299;&#25169;&#30456;&#20851;&#30340;&#35745;&#31639;&#20165;&#22312;&#31163;&#32447;&#39044;&#22788;&#29702;&#36807;&#31243;&#20013;&#36827;&#34892;&#65292;LightDiC&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21521;&#19979;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11772v2 Announce Type: replace-cross  Abstract: Most existing graph neural networks (GNNs) are limited to undirected graphs, whose restricted scope of the captured relational information hinders their expressive capabilities and deployments in real-world scenarios. Compared with undirected graphs, directed graphs (digraphs) fit the demand for modeling more complex topological systems by capturing more intricate relationships between nodes, such as formulating transportation and financial networks. While some directed GNNs have been introduced, their inspiration mainly comes from deep learning architectures, which lead to redundant complexity and computation, making them inapplicable to large-scale databases. To address these issues, we propose LightDiC, a scalable variant of the digraph convolution based on the magnetic Laplacian. Since topology-related computations are conducted solely during offline pre-processing, LightDiC achieves exceptional scalability, enabling downst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.08189</link><description>&lt;p&gt;
PRewrite: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
PRewrite: Prompt Rewriting with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#20197;&#8220;&#35797;&#38169;&#8221;&#30340;&#26041;&#24335;&#25163;&#21160;&#23436;&#25104;&#12290;&#36825;&#31181;&#25163;&#21160;&#31243;&#24207;&#21487;&#33021;&#32791;&#26102;&#65292;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#29983;&#25104;&#30340;&#25552;&#31034;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#21363;&#20351;&#23545;&#37027;&#20123;&#30475;&#20284;&#36816;&#20316;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#22987;&#32456;&#23384;&#22312;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36827;&#19968;&#27493;&#20462;&#25913;&#20351;&#25552;&#31034;&#21464;&#24471;&#26356;&#22909;&#21602;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#31034;&#24037;&#31243;&#33258;&#21160;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#20351;&#29992;&#24773;&#26223;&#65292;&#21363;&#24320;&#21457;&#32773;/&#29992;&#25143;&#24050;&#32463;&#36215;&#33609;&#20102;&#21021;&#22987;&#25552;&#31034;&#65292;&#20294;&#32570;&#20047;&#26102;&#38388;/&#19987;&#19994;&#30693;&#35782;&#26469;&#20248;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRewrite&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#21487;&#37325;&#20889;&#36825;&#20123;&#33609;&#26696;&#65292;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#12290;PRewrite&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#31471;&#21040;&#31471;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#20801;&#35768;RL&#25628;&#32034;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 Announce Type: replace  Abstract: Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;6G&#32593;&#32476;&#20013;&#20026;LLM agents&#35774;&#35745;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#21327;&#20316;&#21033;&#29992;&#31227;&#21160;&#35774;&#22791;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#65292;&#20197;&#22312;&#38271;&#26399;&#20132;&#20114;&#20013;&#23454;&#29616;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#35282;&#33394;&#30340;LLMs&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2401.07764</link><description>&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#36935;&#35265;6G&#32593;&#32476;&#65306;&#24863;&#30693;&#12289;&#22522;&#30784;&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;6G&#32593;&#32476;&#20013;&#20026;LLM agents&#35774;&#35745;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#21327;&#20316;&#21033;&#29992;&#31227;&#21160;&#35774;&#22791;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#65292;&#20197;&#22312;&#38271;&#26399;&#20132;&#20114;&#20013;&#23454;&#29616;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#35282;&#33394;&#30340;LLMs&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;AI agent&#34987;&#26399;&#26395;&#30528;&#38761;&#26032;&#20154;&#26426;&#20132;&#20114;&#65292;&#24182;&#22312;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#25945;&#32946;&#12289;&#21046;&#36896;&#19994;&#21644;&#23089;&#20048;&#31561;&#21508;&#20010;&#39046;&#22495;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#30340;&#21161;&#29702;&#26381;&#21153;&#12290;&#22312;6G&#32593;&#32476;&#20013;&#37096;&#32626;LLM agents&#20351;&#29992;&#25143;&#33021;&#22815;&#36890;&#36807;&#31227;&#21160;&#35774;&#22791;&#27665;&#20027;&#22320;&#35775;&#38382;&#27492;&#21069;&#26114;&#36149;&#30340;AI&#21161;&#29702;&#26381;&#21153;&#65292;&#20174;&#32780;&#20943;&#23569;&#20132;&#20114;&#24310;&#36831;&#65292;&#24182;&#26356;&#22909;&#22320;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#31227;&#21160;&#35774;&#22791;&#30340;&#26377;&#38480;&#23481;&#37327;&#38480;&#21046;&#20102;&#37096;&#32626;&#21644;&#25191;&#34892;&#26412;&#22320;LLMs&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#23601;&#38656;&#35201;&#22312;&#38271;&#26399;&#20132;&#20114;&#36807;&#31243;&#20013;&#23558;&#22797;&#26434;&#20219;&#21153;&#21368;&#36733;&#21040;&#36816;&#34892;&#22312;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#30340;&#20840;&#23616;LLMs&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;6G&#32593;&#32476;&#20013;&#20026;LLM agents&#35774;&#35745;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#31227;&#21160;&#35774;&#22791;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#35282;&#33394;&#30340;LLMs&#20998;&#24067;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#20043;&#38388;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07764v2 Announce Type: replace  Abstract: AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#26412;&#20307;&#25972;&#21512;&#30340;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#22686;&#24378;&#22411;&#26412;&#20307;&#23398;&#20064;&#12289;&#35821;&#20041;&#25968;&#25454;&#25366;&#25496;&#21644;&#23398;&#20064;&#19982;&#25512;&#29702;&#31995;&#32479;&#36825;&#19977;&#31181;&#20027;&#35201;&#30340;&#28151;&#21512;&#31867;&#21035;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.07744</link><description>&lt;p&gt;
&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#26412;&#20307;&#35770;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Combining Machine Learning and Ontology: A Systematic Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07744
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#26412;&#20307;&#25972;&#21512;&#30340;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#22686;&#24378;&#22411;&#26412;&#20307;&#23398;&#20064;&#12289;&#35821;&#20041;&#25968;&#25454;&#25366;&#25496;&#21644;&#23398;&#20064;&#19982;&#25512;&#29702;&#31995;&#32479;&#36825;&#19977;&#31181;&#20027;&#35201;&#30340;&#28151;&#21512;&#31867;&#21035;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#23545;&#23558;&#24402;&#32435;&#25512;&#29702;&#21644;&#28436;&#32462;&#25512;&#29702;&#32467;&#21512;&#30340;&#25506;&#32034;&#36807;&#31243;&#30340;&#28212;&#26395;&#39537;&#20351;&#65292;&#25105;&#20204;&#23545;&#35843;&#26597;&#26426;&#22120;&#23398;&#20064;&#21644;&#26412;&#20307;&#19968;&#20307;&#21270;&#30340;&#25991;&#31456;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#12290;&#20854;&#30446;&#26631;&#26159;&#35782;&#21035;&#28085;&#30422;&#24402;&#32435;&#25512;&#29702;&#65288;&#30001;&#26426;&#22120;&#23398;&#20064;&#25191;&#34892;&#65289;&#21644;&#28436;&#32462;&#25512;&#29702;&#65288;&#30001;&#26412;&#20307;&#25191;&#34892;&#65289;&#30340;&#22810;&#31181;&#25216;&#26415;&#20197;&#25972;&#21512;&#21040;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21253;&#25324;&#23545;128&#39033;&#30740;&#31350;&#30340;&#20998;&#26512;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#26426;&#22120;&#23398;&#20064;&#21644;&#26412;&#20307;&#20043;&#38388;&#30340;&#19977;&#20010;&#20027;&#35201;&#28151;&#21512;&#31867;&#21035;&#65306;&#22686;&#24378;&#22411;&#26412;&#20307;&#23398;&#20064;&#12289;&#35821;&#20041;&#25968;&#25454;&#25366;&#25496;&#20197;&#21450;&#23398;&#20064;&#19982;&#25512;&#29702;&#31995;&#32479;&#12290;&#25105;&#20204;&#23545;&#25152;&#26377;&#36825;&#20123;&#31867;&#21035;&#36827;&#34892;&#20102;&#20840;&#38754;&#26816;&#26597;&#65292;&#24378;&#35843;&#20102;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#31867;&#19982;&#39046;&#22495;&#20869;&#31867;&#20284;&#30340;&#36817;&#26399;&#24037;&#20316;&#20197;&#21450;&#28151;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07744v2 Announce Type: replace  Abstract: Motivated by the desire to explore the process of combining inductive and deductive reasoning, we conducted a systematic literature review of articles that investigate the integration of machine learning and ontologies. The objective was to identify diverse techniques that incorporate both inductive reasoning (performed by machine learning) and deductive reasoning (performed by ontologies) into artificial intelligence systems. Our review, which included the analysis of 128 studies, allowed us to identify three main categories of hybridization between machine learning and ontologies: learning-enhanced ontologies, semantic data mining, and learning and reasoning systems. We provide a comprehensive examination of all these categories, emphasizing the various machine learning algorithms utilized in the studies. Furthermore, we compared our classification with similar recent work in the field of hybrid AI and neuro-symbolic approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#22312;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20013;&#20135;&#29983;&#20013;&#38388;&#27493;&#39588;&#22870;&#21169;&#65292;&#20197;&#24212;&#23545;&#31232;&#30095;&#22870;&#21169;&#20449;&#21495;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.07382</link><description>&lt;p&gt;
&#36229;&#36234;&#31232;&#30095;&#22870;&#21169;&#65306;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#35780;&#35770;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#22312;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20013;&#20135;&#29983;&#20013;&#38388;&#27493;&#39588;&#22870;&#21169;&#65292;&#20197;&#24212;&#23545;&#31232;&#30095;&#22870;&#21169;&#20449;&#21495;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#38750;&#21487;&#24494;&#20998;&#22870;&#21169;&#20449;&#21495;&#65288;&#22914;&#20154;&#31867;&#20559;&#22909;&#65289;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#36825;&#20123;&#22870;&#21169;&#20449;&#21495;&#30340;&#31232;&#30095;&#24615; - &#36890;&#24120;&#65292;&#25972;&#20010;&#36755;&#20986;&#21482;&#26377;&#19968;&#20010;&#22870;&#21169;&#12290;&#36825;&#31181;&#22870;&#21169;&#30340;&#31232;&#30095;&#24615;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#25928;&#29575;&#20302;&#19979;&#19988;&#19981;&#31283;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#35770;&#33021;&#21147;&#22312;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#20013;&#38388;&#27493;&#39588;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23558;&#31574;&#30053;&#27169;&#22411;&#19982;&#35780;&#35770;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#35780;&#35770;&#35821;&#35328;&#27169;&#22411;&#36127;&#36131;&#20026;&#36755;&#20986;&#30340;&#27599;&#20010;&#37096;&#20998;&#25552;&#20379;&#32454;&#33268;&#30340;&#21453;&#39304;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#21453;&#39304;&#36716;&#21270;&#20026;&#21487;&#29992;&#20110;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#30340;&#26631;&#35760;&#25110;&#36328;&#24230;&#32423;&#21035;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#31574;&#30053;&#27169;&#22411;&#36739;&#23567;&#19988;&#19982;&#26356;&#24378;&#22823;&#30340;&#35780;&#35770;&#32773;&#37197;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07382v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful crit
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27431;&#30431;&#27861;&#24459;&#27861;&#35268;&#20013;&#30340;&#24212;&#29992;&#24341;&#21457;&#20102;&#36131;&#20219;&#12289;&#38544;&#31169;&#12289;&#30693;&#35782;&#20135;&#26435;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#23545;&#29616;&#26377;&#21644;&#25311;&#35758;&#30340;&#27861;&#24459;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2401.07348</link><description>&lt;p&gt;
&#27431;&#30431;&#27861;&#24459;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#36131;&#20219;&#12289;&#38544;&#31169;&#12289;&#30693;&#35782;&#20135;&#26435;&#21644;&#32593;&#32476;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07348
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27431;&#30431;&#27861;&#24459;&#27861;&#35268;&#20013;&#30340;&#24212;&#29992;&#24341;&#21457;&#20102;&#36131;&#20219;&#12289;&#38544;&#31169;&#12289;&#30693;&#35782;&#20135;&#26435;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#23545;&#29616;&#26377;&#21644;&#25311;&#35758;&#30340;&#27861;&#24459;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#21450;&#20854;&#21518;&#32487;&#27169;&#22411;&#65292;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#27425;&#33539;&#24335;&#36716;&#21464;&#12290;&#20808;&#36827;&#30340;LLMs&#34920;&#29616;&#20986;&#22810;&#27169;&#24577;&#24615;&#65292;&#33021;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#26684;&#24335;&#65292;&#20174;&#32780;&#25299;&#23485;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#26032;&#20852;&#33258;&#27835;&#24615;&#24341;&#20837;&#20102;&#39044;&#27979;&#24615;&#21644;&#27861;&#24459;&#36981;&#20174;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;LLMs&#22312;&#27431;&#30431;&#32972;&#26223;&#19979;&#30340;&#27861;&#24459;&#21644;&#30417;&#31649;&#24433;&#21709;&#65292;&#20998;&#26512;&#20102;&#36131;&#20219;&#12289;&#38544;&#31169;&#12289;&#30693;&#35782;&#20135;&#26435;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#26041;&#38754;&#12290;&#23427;&#25209;&#21028;&#24615;&#22320;&#23457;&#35270;&#20102;&#29616;&#26377;&#21644;&#25311;&#35758;&#30340;&#27431;&#30431;&#31435;&#27861;&#65288;&#21253;&#25324;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#12299;&#33609;&#26696;&#65289;&#22312;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26222;&#36941;&#21644;LLMs&#29305;&#21035;&#25361;&#25112;&#26041;&#38754;&#30340;&#20805;&#20998;&#24615;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#31435;&#27861;&#26694;&#26550;&#20013;&#30340;&#28508;&#22312;&#24046;&#36317;&#21644;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#24314;&#35758;&#20197;&#30830;&#20445;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07348v2 Announce Type: replace-cross  Abstract: The advent of Generative AI, particularly through Large Language Models (LLMs) like ChatGPT and its successors, marks a paradigm shift in the AI landscape. Advanced LLMs exhibit multimodality, handling diverse data formats, thereby broadening their application scope. However, the complexity and emergent autonomy of these models introduce challenges in predictability and legal compliance. This paper delves into the legal and regulatory implications of Generative AI and LLMs in the European Union context, analyzing aspects of liability, privacy, intellectual property, and cybersecurity. It critically examines the adequacy of the existing and proposed EU legislation, including the Artificial Intelligence Act (AIA) draft, in addressing the unique challenges posed by Generative AI in general and LLMs in particular. The paper identifies potential gaps and shortcomings in the legislative framework and proposes recommendations to ensur
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#32467;&#21512;&#32447;&#24615;&#21270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#26102;&#30340;&#24369;&#28857;&#12290;</title><link>https://arxiv.org/abs/2401.07105</link><description>&lt;p&gt;
&#22270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07105
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#32467;&#21512;&#32447;&#24615;&#21270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#26102;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20027;&#21147;&#20891;&#65292;&#23427;&#20204;&#19982;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30340;&#30456;&#20114;&#20316;&#29992;&#20173;&#22312;&#31215;&#26497;&#30740;&#31350;&#20013;&#12290;&#24403;&#21069;&#29992;&#20110;&#32534;&#30721;&#36825;&#20123;&#22270;&#24418;&#30340;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#65288;i&#65289;&#23558;&#23427;&#20204;&#32447;&#24615;&#21270;&#20197;&#20379;LM&#23884;&#20837;--&#36825;&#26679;&#20250;&#20302;&#25928;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#65292;&#35201;&#20040;&#65288;ii&#65289;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#20445;&#30041;&#22270;&#32467;&#26500;--&#20294;GNNs&#26080;&#27861;&#20687;&#39044;&#35757;&#32451;&#30340;LM&#19968;&#26679;&#24456;&#22909;&#22320;&#34920;&#31034;&#25991;&#26412;&#29305;&#24449;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;LM&#31867;&#22411;&#65292;&#21363;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#23427;&#25972;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#24182;&#20943;&#36731;&#20102;&#23427;&#20204;&#30340;&#24369;&#28857;&#12290;GLM&#21442;&#25968;&#20174;&#39044;&#35757;&#32451;&#30340;LM&#20013;&#21021;&#22987;&#21270;&#65292;&#20197;&#22686;&#24378;&#23545;&#20010;&#21035;&#22270;&#27010;&#24565;&#21644;&#19977;&#20803;&#32452;&#30340;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;GLM&#30340;&#26550;&#26500;&#20197;&#25972;&#21512;&#22270;&#20559;&#24046;&#65292;&#20174;&#32780;&#20419;&#36827;&#22270;&#20869;&#30340;&#30693;&#35782;&#20998;&#24067;&#12290;&#36825;&#20351;GLM&#33021;&#22815;&#22788;&#29702;&#22270;&#24418;&#12289;&#25991;&#26412;&#20197;&#21450;&#20004;&#32773;&#30340;&#20132;&#32455;&#36755;&#20837;&#12290;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07105v2 Announce Type: replace-cross  Abstract: While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -- which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#23569;&#37327;&#26597;&#35810;&#23545;&#25552;&#21462;&#8220;&#23433;&#20840;&#27169;&#24335;&#8221;&#65292;&#25104;&#21151;&#35268;&#36991;&#30446;&#26631;&#27169;&#22411;&#30340;&#38450;&#24481;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36234;&#29425;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.06824</link><description>&lt;p&gt;
&#25171;&#24320;LLMs&#30340;&#28504;&#22810;&#25289;&#39764;&#30418;&#65306;&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06824
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#23569;&#37327;&#26597;&#35810;&#23545;&#25552;&#21462;&#8220;&#23433;&#20840;&#27169;&#24335;&#8221;&#65292;&#25104;&#21151;&#35268;&#36991;&#30446;&#26631;&#27169;&#22411;&#30340;&#38450;&#24481;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36234;&#29425;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#29425;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#35825;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#23545;&#24694;&#24847;&#26597;&#35810;&#20135;&#29983;&#26377;&#27602;&#21709;&#24212;&#65292;&#26469;&#25506;&#32034;LLMs&#23433;&#20840;&#24615;&#36793;&#30028;&#65292;&#36825;&#22312;LLMs&#31038;&#21306;&#20869;&#26159;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#65288;Jailbreaking LLMs through Representation Engineering&#65292;JRE&#65289;&#30340;&#26032;&#39062;&#36234;&#29425;&#26041;&#27861;&#65292;&#20854;&#20165;&#38656;&#35201;&#23569;&#37327;&#26597;&#35810;&#23545;&#20197;&#25552;&#21462;&#21487;&#29992;&#20110;&#35268;&#36991;&#30446;&#26631;&#27169;&#22411;&#38450;&#24481;&#30340;&#8220;&#23433;&#20840;&#27169;&#24335;&#8221;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36234;&#29425;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06824v2 Announce Type: replace-cross  Abstract: Jailbreaking techniques aim to probe the boundaries of safety in large language models (LLMs) by inducing them to generate toxic responses to malicious queries, a significant concern within the LLM community. While existing jailbreaking methods primarily rely on prompt engineering, altering inputs to evade LLM safety mechanisms, they suffer from low attack success rates and significant time overheads, rendering them inflexible. To overcome these limitations, we propose a novel jailbreaking approach, named Jailbreaking LLMs through Representation Engineering (JRE). Our method requires only a small number of query pairs to extract ``safety patterns'' that can be used to circumvent the target model's defenses, achieving unprecedented jailbreaking performance. Building upon these findings, we also introduce a novel defense framework inspired by JRE principles, which demonstrates notable effectiveness. Extensive experimentation conf
&lt;/p&gt;</description></item><item><title>GoT&#26159;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#65292;&#21033;&#29992;&#22270;&#32467;&#26500;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#25191;&#34892;&#20013;&#30340;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#21160;&#24577;&#36335;&#24452;&#36873;&#25321;&#25512;&#36827;&#20102;&#20256;&#32479;&#35748;&#30693;&#27169;&#22411;&#65292;&#24320;&#28304;&#24341;&#25806;GoTFlow&#23637;&#31034;&#20102;&#20854;&#22312;&#33258;&#21160;&#21270;&#20915;&#31574;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.06801</link><description>&lt;p&gt;
&#24605;&#32500;&#22270;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#21160;&#24577;&#21830;&#19994;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph-of-Thought: Utilizing Large Language Models to Solve Complex and Dynamic Business Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06801
&lt;/p&gt;
&lt;p&gt;
GoT&#26159;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#65292;&#21033;&#29992;&#22270;&#32467;&#26500;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#25191;&#34892;&#20013;&#30340;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#21160;&#24577;&#36335;&#24452;&#36873;&#25321;&#25512;&#36827;&#20102;&#20256;&#32479;&#35748;&#30693;&#27169;&#22411;&#65292;&#24320;&#28304;&#24341;&#25806;GoTFlow&#23637;&#31034;&#20102;&#20854;&#22312;&#33258;&#21160;&#21270;&#20915;&#31574;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24605;&#32500;&#22270;&#65288;Graph-of-Thought&#65292;GoT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#30340;&#20840;&#26032;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#20219;&#21153;&#25191;&#34892;&#20013;&#30340;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#12290;GoT&#36890;&#36807;&#20855;&#26377;&#21160;&#24577;&#36335;&#24452;&#36873;&#25321;&#21151;&#33021;&#30340;&#22270;&#32467;&#26500;&#65292;&#25512;&#36827;&#20102;&#20256;&#32479;&#32447;&#24615;&#21644;&#26641;&#29366;&#35748;&#30693;&#27169;&#22411;&#12290;&#24320;&#28304;&#24341;&#25806;GoTFlow&#23637;&#31034;&#20102;GoT&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20419;&#36827;&#20102;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#33258;&#21160;&#21270;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#12290;&#23613;&#31649;&#22312;&#22797;&#26434;&#24615;&#21644;&#36879;&#26126;&#24230;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#20294;GoTFlow&#22312;&#25913;&#21892;&#21830;&#19994;&#27969;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#24040;&#22823;&#65292;&#25215;&#35834;&#36890;&#36807;&#25345;&#32493;&#21457;&#23637;&#22312;&#25928;&#29575;&#21644;&#20915;&#31574;&#36136;&#37327;&#19978;&#21462;&#24471;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06801v2 Announce Type: replace  Abstract: This paper presents Graph-of-Thought (GoT), a new model for workflow automation that enhances the flexibility and efficiency of Large Language Models (LLMs) in complex task execution. GoT advances beyond traditional linear and tree-like cognitive models with a graph structure that enables dynamic path selection. The open-source engine GoTFlow demonstrates the practical application of GoT, facilitating automated, data-driven decision-making across various domains. Despite challenges in complexity and transparency, GoTFlow's potential for improving business processes is significant, promising advancements in both efficiency and decision quality with continuous development.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#25913;&#36827;&#20102;&#29616;&#26377;&#30417;&#30563;&#27169;&#22411;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#33258;&#30417;&#30563;&#25216;&#26415;&#20351;&#24615;&#33021;&#21464;&#24046;&#65292;&#32780;&#19981;&#26159;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2401.00850</link><description>&lt;p&gt;
&#20248;&#21270;&#39044;&#35757;&#32451;&#30340;&#36816;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Refining Pre-Trained Motion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00850
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#25913;&#36827;&#20102;&#29616;&#26377;&#30417;&#30563;&#27169;&#22411;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#33258;&#30417;&#30563;&#25216;&#26415;&#20351;&#24615;&#33021;&#21464;&#24046;&#65292;&#32780;&#19981;&#26159;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22312;&#35270;&#39057;&#20013;&#25163;&#21160;&#27880;&#37322;&#36816;&#21160;&#30340;&#22256;&#38590;&#65292;&#30446;&#21069;&#26368;&#22909;&#30340;&#36816;&#21160;&#20272;&#35745;&#26041;&#27861;&#26159;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#27492;&#22312;&#35757;&#32451;/&#27979;&#35797;&#20043;&#38388;&#23384;&#22312;&#19968;&#23450;&#22256;&#38590;&#12290;&#33258;&#30417;&#30563;&#26041;&#27861;&#25215;&#35834;&#30452;&#25509;&#22312;&#30495;&#23454;&#35270;&#39057;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#36890;&#24120;&#34920;&#29616;&#36739;&#24046;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#32728;&#26354;&#35823;&#24046;&#65288;&#21363;&#39068;&#33394;&#24658;&#23450;&#65289;&#19982;&#24179;&#28369;&#39033;&#30456;&#32467;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#40723;&#21169;&#20272;&#35745;&#30340;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65288;&#21363;&#21521;&#21518;&#36319;&#36394;&#24212;&#20135;&#29983;&#19982;&#21521;&#21069;&#36319;&#36394;&#30456;&#21453;&#30340;&#36712;&#36857;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25509;&#21463;&#20102;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#25913;&#36827;&#26368;&#20808;&#36827;&#30417;&#30563;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#21021;&#22987;&#21270;&#26159;&#30417;&#30563;&#26435;&#37325;&#26102;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#25216;&#26415;&#23454;&#38469;&#19978;&#20250;&#20351;&#24615;&#33021;&#21464;&#24046;&#32780;&#19981;&#26159;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#30475;&#21040;&#26032;&#25968;&#25454;&#30340;&#22909;&#22788;&#34987;&#35757;&#32451;&#20449;&#21495;&#20013;&#30340;&#22122;&#22768;&#25152;&#25513;&#30422;&#12290;&#19987;&#27880;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00850v2 Announce Type: replace-cross  Abstract: Given the difficulty of manually annotating motion in video, the current best motion estimation methods are trained with synthetic data, and therefore struggle somewhat due to a train/test gap. Self-supervised methods hold the promise of training directly on real video, but typically perform worse. These include methods trained with warp error (i.e., color constancy) combined with smoothness terms, and methods that encourage cycle-consistency in the estimates (i.e., tracking backwards should yield the opposite trajectory as tracking forwards). In this work, we take on the challenge of improving state-of-the-art supervised models with self-supervised training. We find that when the initialization is supervised weights, most existing self-supervision techniques actually make performance worse instead of better, which suggests that the benefit of seeing the new data is overshadowed by the noise in the training signal. Focusing on 
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#23558;GPT-4&#27169;&#22411;&#25972;&#21512;&#21040;&#22522;&#22240;&#21464;&#20307;&#35780;&#20272;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#24615;&#33021;&#12289;&#38750;&#30830;&#23450;&#24615;&#21644;&#28418;&#31227;&#29305;&#24449;&#65292;&#20026;&#20854;&#22312;&#22797;&#26434;&#20020;&#24202;&#27969;&#31243;&#20013;&#30340;&#36866;&#29992;&#24615;&#25552;&#20379;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2312.13521</link><description>&lt;p&gt;
&#20934;&#22791;&#23558;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#31995;&#21015;4&#27169;&#22411;&#25972;&#21512;&#21040;&#22522;&#22240;&#21464;&#20307;&#35780;&#20272;&#24037;&#20316;&#27969;&#20013;&#65306;&#35780;&#20272;&#24615;&#33021;&#12289;&#28418;&#31227;&#21644;&#38750;&#30830;&#23450;&#24615;&#29305;&#24449;&#30456;&#23545;&#20110;&#25991;&#29486;&#20013;&#21151;&#33021;&#24615;&#35777;&#25454;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Preparing to Integrate Generative Pretrained Transformer Series 4 models into Genetic Variant Assessment Workflows: Assessing Performance, Drift, and Nondeterminism Characteristics Relative to Classifying Functional Evidence in Literature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13521
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#23558;GPT-4&#27169;&#22411;&#25972;&#21512;&#21040;&#22522;&#22240;&#21464;&#20307;&#35780;&#20272;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#24615;&#33021;&#12289;&#38750;&#30830;&#23450;&#24615;&#21644;&#28418;&#31227;&#29305;&#24449;&#65292;&#20026;&#20854;&#22312;&#22797;&#26434;&#20020;&#24202;&#27969;&#31243;&#20013;&#30340;&#36866;&#29992;&#24615;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#26395;&#25913;&#21892;&#20020;&#24202;&#27979;&#35797;&#20013;&#22522;&#22240;&#21464;&#20307;&#25991;&#29486;&#22238;&#39038;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;4&#65288;GPT-4&#65289;&#30340;&#24615;&#33021;&#12289;&#38750;&#30830;&#23450;&#24615;&#21644;&#28418;&#31227;&#65292;&#20197;&#30830;&#23450;&#20854;&#22312;&#22797;&#26434;&#20020;&#24202;&#27969;&#31243;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26041;&#27861;&#65306;&#20351;&#29992;45&#31687;&#25991;&#31456;&#30340;&#24320;&#21457;&#38598;&#23545;&#29992;&#20110;&#20998;&#31867;&#21151;&#33021;&#24615;&#35777;&#25454;&#30340;2&#25552;&#31034;&#36807;&#31243;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#25552;&#31034;&#35201;&#27714;GPT-4&#25552;&#20379;&#19982;&#21464;&#20307;&#30456;&#20851;&#30340;&#25991;&#31456;&#20013;&#30340;&#25152;&#26377;&#21151;&#33021;&#25968;&#25454;&#65292;&#25110;&#25351;&#31034;&#27809;&#26377;&#21151;&#33021;&#24615;&#35777;&#25454;&#23384;&#22312;&#12290;&#23545;&#20110;&#34987;&#25351;&#31034;&#21253;&#21547;&#21151;&#33021;&#24615;&#35777;&#25454;&#30340;&#25991;&#31456;&#65292;&#31532;&#20108;&#20010;&#25552;&#31034;&#35201;&#27714;GPT-4&#23558;&#35777;&#25454;&#20998;&#31867;&#20026;&#33268;&#30149;&#24615;&#12289;&#33391;&#24615;&#25110;&#20013;&#38388;/&#19981;&#26126;&#30830;&#31867;&#21035;&#12290;&#26368;&#32456;&#30340;&#27979;&#35797;&#38598;&#21253;&#25324;72&#31687;&#25163;&#21160;&#20998;&#31867;&#30340;&#25991;&#31456;&#29992;&#20110;&#27979;&#35797;&#24615;&#33021;&#12290;&#32467;&#26524;&#65306;&#22312;2.5&#20010;&#26376;&#30340;&#26102;&#38388;&#20869;&#65288;2023&#24180;12&#26376;&#33267;2024&#24180;2&#26376;&#65289;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#22825;&#20869;&#30340;&#26174;&#30528;&#24046;&#24322;&#65288;&#38750;&#30830;&#23450;&#24615;&#65289;&#21644;a
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.13521v2 Announce Type: replace-cross  Abstract: Background. Large Language Models (LLMs) hold promise for improving genetic variant literature review in clinical testing. We assessed Generative Pretrained Transformer 4's (GPT-4) performance, nondeterminism, and drift to inform its suitability for use in complex clinical processes. Methods. A 2-prompt process for classification of functional evidence was optimized using a development set of 45 articles. The prompts asked GPT-4 to supply all functional data present in an article related to a variant or indicate that no functional evidence is present. For articles indicated as containing functional evidence, a second prompt asked GPT-4 to classify the evidence into pathogenic, benign, or intermediate/inconclusive categories. A final test set of 72 manually classified articles was used to test performance. Results. Over a 2.5-month period (Dec 2023-Feb 2024), we observed substantial differences in intraday (nondeterminism) and a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20381;&#36182;&#20110;&#20195;&#25968;&#20915;&#31574;&#22270;&#30693;&#35782;&#32534;&#35793;&#26041;&#27861;&#30340;&#31934;&#30830;&#20266;&#24067;&#23572;&#27169;&#22411;&#35745;&#25968;&#22120;PBCount&#65292;&#22312;&#27169;&#22411;&#35745;&#25968;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;</title><link>https://arxiv.org/abs/2312.12341</link><description>&lt;p&gt;
&#35774;&#35745;&#19968;&#31181;&#31934;&#30830;&#30340;&#20266;&#24067;&#23572;&#27169;&#22411;&#35745;&#25968;&#22120;
&lt;/p&gt;
&lt;p&gt;
Engineering an Exact Pseudo-Boolean Model Counter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12341
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20381;&#36182;&#20110;&#20195;&#25968;&#20915;&#31574;&#22270;&#30693;&#35782;&#32534;&#35793;&#26041;&#27861;&#30340;&#31934;&#30830;&#20266;&#24067;&#23572;&#27169;&#22411;&#35745;&#25968;&#22120;PBCount&#65292;&#22312;&#27169;&#22411;&#35745;&#25968;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#35745;&#25968;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#28041;&#21450;&#30830;&#23450;&#24067;&#23572;&#20844;&#24335;&#30340;&#28385;&#36275;&#36171;&#20540;&#25968;&#37327;&#65292;&#36890;&#24120;&#20197;&#21512;&#21462;&#33539;&#24335;&#65288;CNF&#65289;&#34920;&#31034;&#12290;&#34429;&#28982;&#23545;&#20110;CNF&#20844;&#24335;&#30340;&#27169;&#22411;&#35745;&#25968;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#23545;&#20110;&#20266;&#24067;&#23572;&#65288;PB&#65289;&#20844;&#24335;&#30340;&#27169;&#22411;&#35745;&#25968;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#30456;&#27604;&#21629;&#39064;&#24067;&#23572;&#20844;&#24335;&#65292;&#20266;&#24067;&#23572;&#20844;&#24335;&#26356;&#20026;&#31616;&#27905;&#65292;&#21487;&#20197;&#26356;&#28789;&#27963;&#22320;&#34920;&#31034;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#30740;&#31350;&#29992;&#20110;PB&#20844;&#24335;&#30340;&#27169;&#22411;&#35745;&#25968;&#30340;&#39640;&#25928;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31934;&#30830;&#30340;&#20266;&#24067;&#23572;&#27169;&#22411;&#35745;&#25968;&#22120;PBCount&#65292;&#23427;&#20381;&#36182;&#20110;&#20195;&#25968;&#20915;&#31574;&#22270;&#30340;&#30693;&#35782;&#32534;&#35793;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;PBCount&#21487;&#20197;&#35745;&#31639;1513&#20010;&#23454;&#20363;&#30340;&#35745;&#25968;&#65292;&#32780;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#35745;&#25968;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12341v2 Announce Type: replace  Abstract: Model counting, a fundamental task in computer science, involves determining the number of satisfying assignments to a Boolean formula, typically represented in conjunctive normal form (CNF). While model counting for CNF formulas has received extensive attention with a broad range of applications, the study of model counting for Pseudo-Boolean (PB) formulas has been relatively overlooked. Pseudo-Boolean formulas, being more succinct than propositional Boolean formulas, offer greater flexibility in representing real-world problems. Consequently, there is a crucial need to investigate efficient techniques for model counting for PB formulas.   In this work, we propose the first exact Pseudo-Boolean model counter, PBCount, that relies on knowledge compilation approach via algebraic decision diagrams. Our extensive empirical evaluation shows that PBCount can compute counts for 1513 instances while the current state-of-the-art approach cou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;PPO-Clip&#31639;&#27861;&#26041;&#38754;&#20570;&#20986;&#36129;&#29486;&#65292;&#24314;&#31435;&#20102;&#20854;&#22312;&#34920;&#26684;&#21644;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#35774;&#32622;&#20013;&#30340;&#20840;&#23616;&#25910;&#25947;&#32467;&#26524;&#65292;&#29305;&#21035;&#31361;&#20986;&#20102;&#22312;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#24773;&#22659;&#19979;&#30340;$O(1/\sqrt{T})$&#26368;&#23567;&#36845;&#20195;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.12065</link><description>&lt;p&gt;
PPO-Clip&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#24615;&#65306;&#26356;&#28145;&#20837;&#29702;&#35299;&#20462;&#21098;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PPO-Clip Attains Global Optimality: Towards Deeper Understandings of Clipping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12065
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;PPO-Clip&#31639;&#27861;&#26041;&#38754;&#20570;&#20986;&#36129;&#29486;&#65292;&#24314;&#31435;&#20102;&#20854;&#22312;&#34920;&#26684;&#21644;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#35774;&#32622;&#20013;&#30340;&#20840;&#23616;&#25910;&#25947;&#32467;&#26524;&#65292;&#29305;&#21035;&#31361;&#20986;&#20102;&#22312;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#24773;&#22659;&#19979;&#30340;$O(1/\sqrt{T})$&#26368;&#23567;&#36845;&#20195;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#24314;&#31435;&#20102;PPO-Clip&#21464;&#20307;&#22312;&#34920;&#26684;&#21644;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#35774;&#32622;&#20013;&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#29305;&#21035;&#31361;&#20986;&#20102;&#22312;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#24773;&#22659;&#19979;&#30340;$O(1/\sqrt{T})$&#26368;&#23567;&#36845;&#20195;&#25910;&#25947;&#36895;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;PPO-Clip&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#32467;&#21512;&#20854;&#19982;&#38128;&#38142;&#25439;&#22833;&#30340;&#20851;&#31995;&#65292;&#37319;&#29992;&#29109;&#38236;&#20687;&#19979;&#38477;&#65292;&#25105;&#20204;&#20026;&#30452;&#25509;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#34920;&#26684;PPO-Clip&#24314;&#31435;&#20102;&#28176;&#36817;&#25910;&#25947;&#12290;&#21463;&#34920;&#26684;&#20998;&#26512;&#21551;&#21457;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12065v2 Announce Type: replace-cross  Abstract: Proximal Policy Optimization algorithm employing a clipped surrogate objective (PPO-Clip) is a prominent exemplar of the policy optimization methods. However, despite its remarkable empirical success, PPO-Clip lacks theoretical substantiation to date. In this paper, we contribute to the field by establishing the first global convergence results of a PPO-Clip variant in both tabular and neural function approximation settings. Our findings highlight the $O(1/\sqrt{T})$ min-iterate convergence rate specifically in the context of neural function approximation. We tackle the inherent challenges in analyzing PPO-Clip through three central concepts: (i) We introduce a generalized version of the PPO-Clip objective, illuminated by its connection with the hinge loss. (ii) Employing entropic mirror descent, we establish asymptotic convergence for tabular PPO-Clip with direct policy parameterization. (iii) Inspired by the tabular analysis,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35745;&#25968;&#22870;&#21169;&#33258;&#21160;&#26426;&#27010;&#24565;&#65292;&#33021;&#22815;&#23545;&#20219;&#20309;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#19988;&#33021;&#22815;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#26469;&#25351;&#23450;&#29366;&#24577;&#26426;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.11364</link><description>&lt;p&gt;
&#35745;&#25968;&#22870;&#21169;&#33258;&#21160;&#26426;&#65306;&#36890;&#36807;&#21033;&#29992;&#22870;&#21169;&#20989;&#25968;&#32467;&#26500;&#23454;&#29616;&#39640;&#25928;&#25277;&#26679;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Counting Reward Automata: Sample Efficient Reinforcement Learning Through the Exploitation of Reward Function Structure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11364
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35745;&#25968;&#22870;&#21169;&#33258;&#21160;&#26426;&#27010;&#24565;&#65292;&#33021;&#22815;&#23545;&#20219;&#20309;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#19988;&#33021;&#22815;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#26469;&#25351;&#23450;&#29366;&#24577;&#26426;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#35745;&#25968;&#22870;&#21169;&#33258;&#21160;&#26426;&#8212;&#8212;&#19968;&#31181;&#26377;&#38480;&#29366;&#24577;&#26426;&#21464;&#20307;&#65292;&#33021;&#22815;&#23545;&#20219;&#20309;&#21487;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#30340;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#19982;&#20197;&#24448;&#20165;&#33021;&#34920;&#36798;&#20219;&#21153;&#20026;&#27491;&#21017;&#35821;&#35328;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#25551;&#36848;&#20026;&#26080;&#38480;&#21046;&#25991;&#27861;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#37197;&#22791;&#36825;&#26679;&#25277;&#35937;&#26426;&#22120;&#30340;&#20195;&#29702;&#33021;&#22815;&#35299;&#20915;&#27604;&#20351;&#29992;&#24403;&#21069;&#26041;&#27861;&#30340;&#20219;&#21153;&#38598;&#26356;&#24191;&#27867;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#36825;&#31181;&#34920;&#36798;&#33021;&#21147;&#22686;&#21152;&#24182;&#19981;&#20250;&#22686;&#21152;&#33258;&#21160;&#26426;&#22797;&#26434;&#24615;&#30340;&#20195;&#20215;&#12290;&#25552;&#20986;&#20102;&#19968;&#20123;&#21033;&#29992;&#33258;&#21160;&#26426;&#32467;&#26500;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#22312;&#25105;&#20204;&#30340;&#20844;&#24335;&#20013;&#25152;&#38656;&#30340;&#29366;&#24577;&#26426;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#20013;&#25351;&#23450;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#33258;&#21160;&#26426;&#22797;&#26434;&#24615;&#26041;&#38754;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11364v2 Announce Type: replace  Abstract: We present counting reward automata-a finite state machine variant capable of modelling any reward function expressible as a formal language. Unlike previous approaches, which are limited to the expression of tasks as regular languages, our framework allows for tasks described by unrestricted grammars. We prove that an agent equipped with such an abstract machine is able to solve a larger set of tasks than those utilising current approaches. We show that this increase in expressive power does not come at the cost of increased automaton complexity. A selection of learning algorithms are presented which exploit automaton structure to improve sample efficiency. We show that the state machines required in our formulation can be specified from natural language task descriptions using large language models. Empirical results demonstrate that our method outperforms competing approaches in terms of sample efficiency, automaton complexity, an
&lt;/p&gt;</description></item><item><title>&#25351;&#20196;&#24494;&#35843;&#25552;&#21319;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#25351;&#20196;&#31867;&#22411;&#23545;&#29305;&#23450;&#24212;&#29992;&#26356;&#26377;&#21033;&#65292;&#20294;&#21487;&#33021;&#23545;&#20854;&#20182;&#39046;&#22495;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2312.10793</link><description>&lt;p&gt;
&#35299;&#35835;&#22823;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#25351;&#20196;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Demystifying Instruction Mixing for Fine-tuning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10793
&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#24494;&#35843;&#25552;&#21319;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#25351;&#20196;&#31867;&#22411;&#23545;&#29305;&#23450;&#24212;&#29992;&#26356;&#26377;&#21033;&#65292;&#20294;&#21487;&#33021;&#23545;&#20854;&#20182;&#39046;&#22495;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#24494;&#35843;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20248;&#21270;LLM&#24494;&#35843;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#28151;&#21512;&#30340;&#36807;&#31243;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#30740;&#31350;&#23558;&#25351;&#20196;&#20998;&#20026;&#19977;&#31867;&#20027;&#35201;&#31867;&#22411;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#12289;&#32534;&#31243;&#21644;&#19968;&#33324;&#23545;&#35805;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#25351;&#20196;&#24494;&#35843;&#23545;LLM&#24615;&#33021;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#32452;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#26576;&#20123;&#25351;&#20196;&#31867;&#22411;&#23545;&#29305;&#23450;&#24212;&#29992;&#26356;&#26377;&#21033;&#65292;&#20294;&#21487;&#33021;&#23545;&#20854;&#20182;&#39046;&#22495;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#25351;&#20196;&#28151;&#21512;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10793v3 Announce Type: replace-cross  Abstract: Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance, and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights into instruction mixtures, laying the foundations for future research.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BSARec&#30340;&#26032;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#33258;&#27880;&#24847;&#21147;&#65292;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#27880;&#20837;&#20102;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#38598;&#25104;&#20102;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#20197;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;</title><link>https://arxiv.org/abs/2312.10325</link><description>&lt;p&gt;
&#36229;&#36234;&#33258;&#27880;&#24847;&#21147;&#30340;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#20851;&#27880;&#24402;&#32435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10325
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BSARec&#30340;&#26032;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#33258;&#27880;&#24847;&#21147;&#65292;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#27880;&#20837;&#20102;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#38598;&#25104;&#20102;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#20197;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#24207;&#21015;&#25512;&#33616;&#65288;SR&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290; Transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36935;&#21040;&#20102;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#21363;&#38544;&#34255;&#34920;&#31034;&#21464;&#24471;&#31867;&#20284;&#20110;&#26631;&#35760;&#12290; &#22312;SR&#39046;&#22495;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#30456;&#21516;&#38382;&#39064;&#30340;&#21457;&#29983;&#12290; &#25105;&#20204;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#33258;&#27880;&#24847;&#22312;SR&#20013;&#30340;&#20302;&#36890;&#28388;&#27874;&#29305;&#24615;&#65292;&#23548;&#33268;&#20102;&#36807;&#24230;&#24179;&#28369;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\textbf{B}$eyond $\textbf{S}$elf-$\textbf{A}$ttention for Sequential $\textbf{Rec}$ommendation&#65288;BSARec&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#26469; i&#65289;&#36890;&#36807;&#32771;&#34385;&#32454;&#31890;&#24230;&#30340;&#24207;&#21015;&#27169;&#24335;&#27880;&#20837;&#24402;&#32435;&#20559;&#24046;&#21644; ii&#65289;&#38598;&#25104;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#20197;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#12290; &#25105;&#20204;&#30340;&#21457;&#29616;&#22312;SR&#39046;&#22495;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#26377;&#26395;&#25645;&#36215;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10325v2 Announce Type: replace-cross  Abstract: Sequential recommendation (SR) models based on Transformers have achieved remarkable successes. The self-attention mechanism of Transformers for computer vision and natural language processing suffers from the oversmoothing problem, i.e., hidden representations becoming similar to tokens. In the SR domain, we, for the first time, show that the same problem occurs. We present pioneering investigations that reveal the low-pass filtering nature of self-attention in the SR, which causes oversmoothing. To this end, we propose a novel method called $\textbf{B}$eyond $\textbf{S}$elf-$\textbf{A}$ttention for Sequential $\textbf{Rec}$ommendation (BSARec), which leverages the Fourier transform to i) inject an inductive bias by considering fine-grained sequential patterns and ii) integrate low and high-frequency information to mitigate oversmoothing. Our discovery shows significant advancements in the SR domain and is expected to bridge t
&lt;/p&gt;</description></item><item><title>TAP4LLM&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#34920;&#26684;&#25552;&#31034;&#30340;&#22810;&#21151;&#33021;&#39044;&#22788;&#29702;&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#37319;&#26679;&#12289;&#22686;&#34917;&#21644;&#25171;&#21253;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#21644;&#22823;&#22411;&#34920;&#26684;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.09039</link><description>&lt;p&gt;
TAP4LLM&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#34920;&#26684;&#25552;&#20379;&#32773;&#22312;&#23545;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#37319;&#26679;&#12289;&#22686;&#34917;&#21644;&#25171;&#21253;
&lt;/p&gt;
&lt;p&gt;
TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09039
&lt;/p&gt;
&lt;p&gt;
TAP4LLM&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#34920;&#26684;&#25552;&#31034;&#30340;&#22810;&#21151;&#33021;&#39044;&#22788;&#29702;&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#37319;&#26679;&#12289;&#22686;&#34917;&#21644;&#25171;&#21253;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#21644;&#22823;&#22411;&#34920;&#26684;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#30340;&#25512;&#29702;&#22312;&#32467;&#21512;&#28145;&#24230;&#27169;&#22411;&#21644;&#31163;&#25955;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#38656;&#35201;&#23545;&#33258;&#30001;&#24418;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#38382;&#39064;&#21644;&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#34920;&#26684;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#21482;&#32771;&#34385;&#23567;&#22411;&#34920;&#26684;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#26356;&#22823;&#34920;&#26684;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#25512;&#29702;&#22797;&#26434;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#22522;&#26412;&#20449;&#24687;&#25110;&#20998;&#25955;&#22312;&#19981;&#21516;&#20301;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TAP4LLM&#20316;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#39044;&#22788;&#29702;&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#24179;&#34913;&#26631;&#35760;&#20998;&#37197;&#26435;&#34913;&#26469;&#29983;&#25104;&#34920;&#26684;&#25552;&#31034;&#65292;&#23454;&#29616;(1) &#34920;&#26684;&#37319;&#26679;&#65292;(2) &#34920;&#26684;&#22686;&#34917;&#21644;(3) &#34920;&#26684;&#25171;&#21253;&#12290;&#22312;&#27599;&#20010;&#27169;&#22359;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#21644;&#35774;&#35745;&#20102;&#20960;&#31181;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#20351;&#29992;&#30340;&#24120;&#35265;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;&#36895;&#24230;&#19982;&#20934;&#30830;&#24615;&#30340;&#24179;&#34913;&#65289;&#12290;&#25105;&#20204;&#36824;&#23545;T&#20869;&#37096;&#27599;&#20010;&#32452;&#20214;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09039v2 Announce Type: replace-cross  Abstract: Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and semi-structured tabular data. However, previous table reasoning solutions only consider small-sized tables and exhibit limitations in handling larger tables. In addition, most existing methods struggle to reason over complex questions since they lack essential information or they are scattered in different places. To alleviate these challenges, we propose TAP4LLM as a versatile pre-processing toolbox to generate table prompts through (1) table sampling, (2) table augmentation, and (3) table packing while balancing the token allocation trade-off. In each module, we collect and design several common methods for usage in various scenarios (e.g., speed over accuracy). We also provide a comprehensive evaluation on performance of each components inside T
&lt;/p&gt;</description></item><item><title>Math-Shepherd&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#23398;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#36807;&#31243;&#30417;&#30563;&#25968;&#25454;&#23454;&#29616;LLMs&#30340;&#36880;&#27493;&#39564;&#35777;&#21644;&#21152;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.08935</link><description>&lt;p&gt;
Math-Shepherd: &#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#36880;&#27493;&#39564;&#35777;&#21644;&#21152;&#24378;LLMs
&lt;/p&gt;
&lt;p&gt;
Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08935
&lt;/p&gt;
&lt;p&gt;
Math-Shepherd&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#23398;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#36807;&#31243;&#30417;&#30563;&#25968;&#25454;&#23454;&#29616;LLMs&#30340;&#36880;&#27493;&#39564;&#35777;&#21644;&#21152;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Math-Shepherd&#30340;&#21019;&#26032;&#36807;&#31243;&#23548;&#21521;&#25968;&#23398;&#22870;&#21169;&#27169;&#22411;&#65292;&#20026;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#27599;&#19968;&#27493;&#20998;&#37197;&#22870;&#21169;&#20998;&#25968;&#12290;Math-Shepherd&#30340;&#35757;&#32451;&#26159;&#20351;&#29992;&#33258;&#21160;&#26500;&#24314;&#30340;&#22522;&#20110;&#36807;&#31243;&#30340;&#30417;&#30563;&#25968;&#25454;&#23436;&#25104;&#30340;&#65292;&#25171;&#30772;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#23545;&#25163;&#21160;&#26631;&#27880;&#30340;&#20005;&#37325;&#20381;&#36182;&#29942;&#39048;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;Math-Shepherd&#22312;&#20004;&#31181;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65306;1&#65289;\textit{&#39564;&#35777;}&#65306;&#21033;&#29992;Math-Shepherd&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#30340;&#22810;&#20010;&#36755;&#20986;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65307;2&#65289;\textit{&#24378;&#21270;&#23398;&#20064;}&#65306;&#20351;&#29992;Math-Shepherd&#36890;&#36807;&#36880;&#27493;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#21152;&#24378;LLMs&#12290;&#36890;&#36807;Math-Shepherd&#65292;&#19968;&#31995;&#21015;&#24320;&#28304;LLMs&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;Math-Shepherd&#30340;&#36880;&#27493;PPO&#26174;&#33879;&#25552;&#39640;&#20102;Mistral-7B&#30340;&#20934;&#30830;&#29575;(GSM8K&#30001;77.9%&#25552;&#39640;&#21040;84.1%&#65292;MATH&#30001;28.6%&#25552;&#39640;&#21040;33.0%)
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08935v3 Announce Type: replace  Abstract: In this paper, we present an innovative process-oriented math process reward model called \textbf{Math-Shepherd}, which assigns a reward score to each step of math problem solutions. The training of Math-Shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-Shepherd in two scenarios: 1) \textit{Verification}: Math-Shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) \textit{Reinforcement Learning}: Math-Shepherd is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With Math-Shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with Math-Shepherd significantly improves the accuracy of Mistral-7B (77.9\%$\to$84.1\% on GSM8K and 28.6\%$\to$33.0\% on MATH). The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#24120;&#29992;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#22810;&#26679;&#21270;&#20559;&#22909;&#23545;&#22870;&#21169;&#24314;&#27169;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#22870;&#21169;&#23398;&#20064;&#26041;&#27861;&#20197;&#22686;&#24378;&#26657;&#20934;&#24615;&#33021;</title><link>https://arxiv.org/abs/2312.07401</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#22810;&#26679;&#21270;&#20559;&#22909;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Diversified Preferences of Large Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#24120;&#29992;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#22810;&#26679;&#21270;&#20559;&#22909;&#23545;&#22870;&#21169;&#24314;&#27169;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#22870;&#21169;&#23398;&#20064;&#26041;&#27861;&#20197;&#22686;&#24378;&#26657;&#20934;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#34987;&#35748;&#20026;&#26159;&#25552;&#39640;LLMs&#20132;&#20114;&#36136;&#37327;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#22810;&#20803;&#21270;&#30340;&#19990;&#30028;&#20013;&#65292;&#30001;&#20110;&#26631;&#27880;&#32773;&#30340;&#19981;&#21516;&#20559;&#22909;&#65292;&#20154;&#31867;&#20559;&#22909;&#21487;&#33021;&#20250;&#22810;&#26679;&#21270;&#65292;&#36825;&#38459;&#30861;&#20102;LLM&#23545;&#40784;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;&#24120;&#29992;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#38598;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#65292;&#20197;&#30740;&#31350;&#22810;&#26679;&#21270;&#20559;&#22909;&#23545;&#22870;&#21169;&#24314;&#27169;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22870;&#21169;&#27169;&#22411;&#65288;RMs&#65289;&#30340;&#26657;&#20934;&#24615;&#33021;&#19982;LLMs&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22810;&#26679;&#21270;&#20559;&#22909;&#25968;&#25454;&#23545;&#20154;&#31867;&#20849;&#20139;&#20559;&#22909;&#65288;&#22914;&#8220;&#26080;&#23475;&#21644;&#26377;&#24110;&#21161;&#8221;&#65289;&#19978;&#30340;&#22870;&#21169;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;LLMs&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#31181;&#26080;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#22870;&#21169;&#23398;&#20064;&#26041;&#27861;&#65288;MORE&#65289;&#20197;&#22686;&#24378;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07401v3 Announce Type: replace  Abstract: Aligning large language models (LLMs) with human preferences has been recognized as the key to improving LLMs' interaction quality. However, in this pluralistic world, human preferences can be diversified due to annotators' different tastes, which hinders the effectiveness of LLM alignment methods. This paper presents the first quantitative analysis of commonly used human feedback datasets to investigate the impact of diversified preferences on reward modeling. Our analysis reveals a correlation between the calibration performance of reward models (RMs) and the alignment performance of LLMs. We find that diversified preference data negatively affect the calibration performance of RMs on human-shared preferences, such as \textit{Harmless\&amp;Helpful}, thereby impairing the alignment performance of LLMs. To address the ineffectiveness, we propose a novel Multi-Objective Reward learning method (MORE) to enhance the calibration performance 
&lt;/p&gt;</description></item><item><title>KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.06185</link><description>&lt;p&gt;
KnowGPT&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
KnowGPT: Black-Box Knowledge Injection for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06185
&lt;/p&gt;
&lt;p&gt;
KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#25552;&#20379;&#20114;&#21160;&#24335;API&#65292;&#21487;&#20197;&#20197;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#25110;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20250;&#32473;&#20986;&#19981;&#20934;&#30830;&#25110;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#36825;&#20123;&#30693;&#35782;&#24182;&#26410;&#21253;&#21547;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;LLMs&#24182;&#38750;&#24320;&#28304;&#65292;&#36825;&#20351;&#24471;&#20165;&#20351;&#29992;&#27169;&#22411;API&#27880;&#20837;&#30693;&#35782;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KnowGPT&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#22312;&#38382;&#31572;&#20013;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#12290;KnowGPT&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#30693;&#35782;&#22270;&#20013;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#20026;&#27599;&#20010;&#38382;&#39064;&#26500;&#24314;&#26368;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;KnowGPT&#26174;&#33879;&#22686;&#24378;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;KnowGPT&#24179;&#22343;&#25913;&#36827;&#20102;23%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06185v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs), such as ChatGPT, offer interactive APIs that can answer common questions at a human-expert level. However, these models often give inaccurate or incorrect responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus. Furthermore, many state-of-the-art LLMs are not open-source, making it challenging to inject knowledge with model APIs only. In this work, we introduce KnowGPT, a black-box knowledge injection framework for LLMs in question answering. KnowGPT leverages deep reinforcement learning (RL) to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed Bandit (MAB) to construct the most suitable prompt for each question. Our extensive experiments on three benchmark datasets showcase that KnowGPT significantly enhances the existing methods. Notably, KnowGPT achieves an average improvement of 23.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23558;&#25991;&#26412;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#26410;&#26469;&#21463;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#19981;&#33391;&#34892;&#20026;&#24182;&#24378;&#21046;&#25191;&#34892;&#23545;&#25351;&#20196;&#30340;&#24544;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;LLMs&#26377;&#25928;&#25351;&#23548;&#25991;&#26412;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2312.06149</link><description>&lt;p&gt;
&#35299;&#38145;&#39044;&#27979;&#24615;&#25991;&#26412;&#29983;&#25104;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#30340;&#21463;&#38480;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06149
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23558;&#25991;&#26412;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#26410;&#26469;&#21463;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#19981;&#33391;&#34892;&#20026;&#24182;&#24378;&#21046;&#25191;&#34892;&#23545;&#25351;&#20196;&#30340;&#24544;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;LLMs&#26377;&#25928;&#25351;&#23548;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32473;&#23450;&#25552;&#31034;&#25110;&#25351;&#20196;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21313;&#20159;&#32423;&#21035;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#19981;&#33391;&#34892;&#20026;&#22914;&#27602;&#24615;&#25110;&#24187;&#35273;&#21487;&#33021;&#20250;&#26174;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25991;&#26412;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#26410;&#26469;&#21463;&#38480;&#29983;&#25104;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#21270;&#19981;&#33391;&#34892;&#20026;&#24182;&#24378;&#21046;&#25191;&#34892;&#23545;&#25351;&#20196;&#30340;&#24544;&#23454;&#24615;&#12290;&#20351;&#29992;LLMs&#23454;&#29616;&#26410;&#26469;&#32422;&#26463;&#28385;&#36275;&#24230;&#30340;&#20272;&#35745;&#24341;&#23548;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65306;&#20851;&#38190;&#35789;&#21463;&#38480;&#29983;&#25104;&#12289;&#27602;&#24615;&#20943;&#23569;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06149v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 202
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31867;&#21035;&#30340;&#20462;&#21098;&#25216;&#26415;&#65292;&#19982;&#20808;&#21069;&#30340;&#20462;&#21098;&#31574;&#30053;&#19981;&#21516;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#35780;&#20272;&#19982;&#31867;&#21035;&#25968;&#37327;&#30456;&#20851;&#30340;&#28388;&#27874;&#22120;&#37325;&#35201;&#24615;&#26469;&#21387;&#32553;DNN&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2312.05875</link><description>&lt;p&gt;
&#38754;&#21521;&#31867;&#21035;&#30340;&#20462;&#21098;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Class-Aware Pruning for Efficient Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31867;&#21035;&#30340;&#20462;&#21098;&#25216;&#26415;&#65292;&#19982;&#20808;&#21069;&#30340;&#20462;&#21098;&#31574;&#30053;&#19981;&#21516;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#35780;&#20272;&#19982;&#31867;&#21035;&#25968;&#37327;&#30456;&#20851;&#30340;&#28388;&#27874;&#22120;&#37325;&#35201;&#24615;&#26469;&#21387;&#32553;DNN&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;DNN&#20013;&#30340;&#22823;&#37327;&#28014;&#28857;&#36816;&#31639;(FLOPs)&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;(&#22914;&#36793;&#32536;&#35774;&#22791;)&#20013;&#37096;&#32626;&#26102;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20462;&#21098;&#26469;&#20943;&#23569;&#25191;&#34892;DNN&#26102;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#19982;&#20808;&#21069;&#30340;&#20462;&#21098;&#31574;&#30053;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31867;&#21035;&#30340;&#20462;&#21098;&#25216;&#26415;&#26469;&#21387;&#32553;DNN&#65292;&#20026;&#20943;&#23569;DNN&#30340;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#34987;&#20462;&#25913;&#20197;&#23454;&#29616;&#38754;&#21521;&#31867;&#21035;&#30340;&#20462;&#21098;&#12290;&#28982;&#21518;&#65292;&#35780;&#20272;&#20102;&#19982;&#31867;&#21035;&#25968;&#37327;&#30456;&#20851;&#30340;&#28388;&#27874;&#22120;&#30340;&#37325;&#35201;&#24615;&#12290;&#38024;&#23545;&#21482;&#23545;&#23569;&#25968;&#31867;&#21035;&#37325;&#35201;&#30340;&#28388;&#27874;&#22120;&#36827;&#34892;&#31227;&#38500;&#12290;&#38543;&#21518;&#31070;&#32463;&#32593;&#32476;&#34987;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05875v2 Announce Type: replace  Abstract: Deep neural networks (DNNs) have demonstrated remarkable success in various fields. However, the large number of floating-point operations (FLOPs) in DNNs poses challenges for their deployment in resource-constrained applications, e.g., edge devices. To address the problem, pruning has been introduced to reduce the computational cost in executing DNNs. Previous pruning strategies are based on weight values, gradient values and activation outputs. Different from previous pruning solutions, in this paper, we propose a class-aware pruning technique to compress DNNs, which provides a novel perspective to reduce the computational cost of DNNs. In each iteration, the neural network training is modified to facilitate the class-aware pruning. Afterwards, the importance of filters with respect to the number of classes is evaluated. The filters that are only important for a few number of classes are removed. The neural network is then retraine
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#26080;&#25968;&#25454;&#31070;&#32463;&#32593;&#26684;&#36866;&#37197;&#22120;&#65288;DMM&#65289;&#35299;&#20915;&#20102;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#38656;&#35201;&#26114;&#36149;&#32593;&#26684;&#25968;&#25454;&#21644;&#35299;&#20915;&#31354;&#38388;&#30340;&#33258;&#30001;&#24230;&#21644;&#25299;&#25169;&#32467;&#26500;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.05583</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#25968;&#25454;&#32593;&#26684;&#31227;&#21160;&#22120;&#23454;&#29616;&#26356;&#22909;&#30340;&#31070;&#32463;PDE&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Better Neural PDE Solvers Through Data-Free Mesh Movers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05583
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#26080;&#25968;&#25454;&#31070;&#32463;&#32593;&#26684;&#36866;&#37197;&#22120;&#65288;DMM&#65289;&#35299;&#20915;&#20102;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#38656;&#35201;&#26114;&#36149;&#32593;&#26684;&#25968;&#25454;&#21644;&#35299;&#20915;&#31354;&#38388;&#30340;&#33258;&#30001;&#24230;&#21644;&#25299;&#25169;&#32467;&#26500;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#29289;&#29702;&#31995;&#32479;&#24314;&#27169;&#20013;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#12290;&#34429;&#28982;&#20027;&#35201;&#30740;&#31350;&#38598;&#20013;&#22312;&#23398;&#20064;&#39044;&#23450;&#20041;&#38745;&#24577;&#32593;&#26684;&#31163;&#25955;&#21270;&#19978;&#30340;&#31995;&#32479;&#28436;&#21270;&#65292;&#20294;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#26469;&#21019;&#24314;&#36866;&#24212;&#24615;&#21644;&#21160;&#24577;&#32593;&#26684;&#65292;&#30001;&#20110;&#36825;&#20123;&#31995;&#32479;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;1&#65289;&#38656;&#35201;&#26114;&#36149;&#30340;&#26368;&#20339;&#32593;&#26684;&#25968;&#25454;&#65292;&#21644;&#65288;2&#65289;&#22312;&#32593;&#26684;&#32454;&#21270;&#36807;&#31243;&#20013;&#35299;&#20915;&#31354;&#38388;&#30340;&#33258;&#30001;&#24230;&#21644;&#25299;&#25169;&#32467;&#26500;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#31070;&#32463;&#32593;&#26684;&#36866;&#37197;&#22120;&#30340;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#25968;&#25454;&#31070;&#32463;&#32593;&#26684;&#36866;&#37197;&#22120;&#65292;&#31216;&#20026;Data-free Mesh Mover&#65288;DMM&#65289;&#65292;&#20855;&#26377;&#20004;&#20010;&#20027;&#35201;&#21019;&#26032;&#28857;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#25805;&#20316;&#31526;&#65292;&#23558;&#35299;&#26144;&#23556;&#21040;&#33258;&#36866;&#24212;&#32593;&#26684;&#19978;&#65292;&#24182;&#20351;&#29992;Monge-Amp\`ere&#26041;&#31243;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05583v2 Announce Type: replace-cross  Abstract: Recently, neural networks have been extensively employed to solve partial differential equations (PDEs) in physical system modeling. While major studies focus on learning system evolution on predefined static mesh discretizations, some methods utilize reinforcement learning or supervised learning techniques to create adaptive and dynamic meshes, due to the dynamic nature of these systems. However, these approaches face two primary challenges: (1) the need for expensive optimal mesh data, and (2) the change of the solution space's degree of freedom and topology during mesh refinement. To address these challenges, this paper proposes a neural PDE solver with a neural mesh adapter. To begin with, we introduce a novel data-free neural mesh adaptor, called Data-free Mesh Mover (DMM), with two main innovations. Firstly, it is an operator that maps the solution to adaptive meshes and is trained using the Monge-Amp\`ere equation withou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#35780;&#20998;&#23398;&#29983;&#23545;&#31185;&#23398;&#35780;&#20272;&#20889;&#20316;&#21453;&#39304;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#36890;&#36807;&#38646;&#27425;&#25110;&#23569;&#27425;&#23398;&#20064;&#31561;&#31574;&#30053;&#33258;&#21160;&#35780;&#20998;&#65292;&#20854;&#20013;&#23569;&#27425;&#23398;&#20064;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>https://arxiv.org/abs/2312.03748</link><description>&lt;p&gt;
&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24605;&#32500;&#38142;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Applying Large Language Models and Chain-of-Thought for Automatic Scoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#35780;&#20998;&#23398;&#29983;&#23545;&#31185;&#23398;&#35780;&#20272;&#20889;&#20316;&#21453;&#39304;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#36890;&#36807;&#38646;&#27425;&#25110;&#23569;&#27425;&#23398;&#20064;&#31561;&#31574;&#30053;&#33258;&#21160;&#35780;&#20998;&#65292;&#20854;&#20013;&#23569;&#27425;&#23398;&#20064;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159; GPT-3.5 &#21644; GPT-4&#65292;&#20197;&#21450;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#22312;&#33258;&#21160;&#35780;&#20998;&#23398;&#29983;&#23545;&#31185;&#23398;&#35780;&#20272;&#30340;&#20889;&#20316;&#21453;&#39304;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20811;&#26381;&#20808;&#21069;&#38480;&#21046;&#30740;&#31350;&#20154;&#21592;&#21644;&#25945;&#32946;&#24037;&#20316;&#32773;&#20351;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#33258;&#21160;&#35780;&#20998;&#24037;&#20855;&#30340;&#26080;&#27861;&#35775;&#38382;&#24615;&#12289;&#25216;&#26415;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#19968;&#20010;&#21253;&#25324; 1,650 &#20010;&#23398;&#29983;&#21453;&#39304;&#30340;&#20845;&#39033;&#35780;&#20272;&#20219;&#21153;&#65288;&#19977;&#20010;&#20108;&#39033;&#21644;&#19977;&#20010;&#19977;&#39033;&#65289;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#37319;&#29992;&#20845;&#31181;&#25552;&#31034;&#24037;&#31243;&#31574;&#30053;&#26469;&#33258;&#21160;&#35780;&#20998;&#23398;&#29983;&#30340;&#21453;&#39304;&#12290;&#36825;&#20845;&#31181;&#31574;&#30053;&#23558;&#38646;&#27425;&#23398;&#20064;&#25110;&#23569;&#27425;&#23398;&#20064;&#19982; CoT &#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#35201;&#20040;&#29420;&#33258;&#20351;&#29992;&#65292;&#35201;&#20040;&#19982;&#39033;&#30446;&#24178;&#21644;&#35780;&#20998;&#32454;&#21017;&#19968;&#36215;&#20351;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23569;&#27425;&#23398;&#20064;&#65288;&#20934;&#30830;&#29575;=.67&#65289;&#20248;&#20110;&#38646;&#27425;&#23398;&#20064;&#65288;&#20934;&#30830;&#29575;=.60&#65289;&#65292;&#25552;&#39640;12.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03748v2 Announce Type: replace-cross  Abstract: This study investigates the application of large language models (LLMs), specifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT) in the automatic scoring of student-written responses to science assessments. We focused on overcoming the challenges of accessibility, technical complexity, and lack of explainability that have previously limited the use of artificial intelligence-based automatic scoring tools among researchers and educators. With a testing dataset comprising six assessment tasks (three binomial and three trinomial) with 1,650 student responses, we employed six prompt engineering strategies to automatically score student responses. The six strategies combined zero-shot or few-shot learning with CoT, either alone or alongside item stem and scoring rubrics. Results indicated that few-shot (acc = .67) outperformed zero-shot learning (acc = .60), with 12.6% increase. CoT, when used without item stem and scoring rubric
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PA-RL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29109;&#29575;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#23454;&#29616;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#22312;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#20540;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2311.18703</link><description>&lt;p&gt;
&#36890;&#36807;&#29109;&#29575;&#26368;&#23567;&#21270;&#23454;&#29616;&#21487;&#39044;&#27979;&#30340;&#24378;&#21270;&#23398;&#20064;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PA-RL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29109;&#29575;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#23454;&#29616;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#22312;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#27809;&#26377;&#21160;&#26426;&#23637;&#31034;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#65292;&#36890;&#24120;&#36890;&#36807;&#31574;&#30053;&#29109;&#27491;&#21017;&#21270;&#25512;&#21160;&#26234;&#33021;&#20307;&#22312;&#25506;&#32034;&#19978;&#38543;&#26426;&#21270;&#20854;&#34892;&#20026;&#12290;&#20174;&#20154;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24456;&#38590;&#35299;&#37322;&#21644;&#39044;&#27979;&#65307;&#20174;&#23433;&#20840;&#35282;&#24230;&#26469;&#30475;&#65292;&#26356;&#38590;&#20197;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21487;&#39044;&#27979;&#24615;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#65288;PA-RL&#65289;&#65292;&#29992;&#20110;&#24341;&#23548;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#65292;&#20854;&#21033;&#29992;&#29366;&#24577;&#24207;&#21015;&#29109;&#29575;&#20316;&#20026;&#21487;&#39044;&#27979;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29109;&#29575;&#21046;&#23450;&#20026;&#24179;&#22343;&#22870;&#21169;&#30446;&#26631;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#29109;&#22870;&#21169;&#20989;&#25968;&#20381;&#36182;&#20110;&#31574;&#30053;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#20316;&#30456;&#20851;&#30340;&#26367;&#20195;&#29109;&#65292;&#20197;&#21033;&#29992;PG&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#23384;&#22312;&#65292;&#24182;&#19988;&#26368;&#23567;&#21270;&#20102;&#23454;&#38469;&#29109;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#23398;&#20064;&#21040;&#30340;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#19982;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Reinforcement Learning (RL), agents have no incentive to exhibit predictable behaviors, and are often pushed (through e.g. policy entropy regularization) to randomize their actions in favor of exploration. From a human perspective, this makes RL agents hard to interpret and predict, and from a safety perspective, even harder to formally verify. We propose a novel method to induce predictable behavior in RL agents, referred to as Predictability-Aware RL (PA-RL), which employs the state sequence entropy rate as a predictability measure. We show how the entropy rate can be formulated as an average reward objective, and since its entropy reward function is policy-dependent, we introduce an action-dependent surrogate entropy enabling the use of PG methods. We prove that deterministic policies minimizing the average surrogate reward exist and also minimize the actual entropy rate, and show how, given a learned dynamical model, we are able to approximate the value function associated to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21512;&#35268;&#35282;&#24230;&#20998;&#26512;&#27861;&#24459;&#35201;&#27714;&#65292;&#29305;&#21035;&#26159;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#35201;&#27714;&#24037;&#31243;&#38454;&#27573;&#65292;&#20197;&#30830;&#20445;&#36719;&#20214;&#30340;&#21512;&#35268;&#24615;&#12290;&#20027;&#35201;&#20851;&#27880;&#27431;&#30431;&#30340;&#19968;&#33324;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#31561;&#27861;&#35268;&#23545;&#25910;&#38598;&#12289;&#22788;&#29702;&#25110;&#20998;&#20139;&#20010;&#20154;&#25968;&#25454;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#35268;&#23450;&#12290;</title><link>https://arxiv.org/abs/2311.13871</link><description>&lt;p&gt;
&#27861;&#24459;&#35201;&#27714;&#20998;&#26512;&#65306;&#20174;&#21512;&#35268;&#30340;&#35282;&#24230;&#30475;
&lt;/p&gt;
&lt;p&gt;
Legal Requirements Analysis: A Regulatory Compliance Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21512;&#35268;&#35282;&#24230;&#20998;&#26512;&#27861;&#24459;&#35201;&#27714;&#65292;&#29305;&#21035;&#26159;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#35201;&#27714;&#24037;&#31243;&#38454;&#27573;&#65292;&#20197;&#30830;&#20445;&#36719;&#20214;&#30340;&#21512;&#35268;&#24615;&#12290;&#20027;&#35201;&#20851;&#27880;&#27431;&#30431;&#30340;&#19968;&#33324;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#31561;&#27861;&#35268;&#23545;&#25910;&#38598;&#12289;&#22788;&#29702;&#25110;&#20998;&#20139;&#20010;&#20154;&#25968;&#25454;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#35268;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36719;&#20214;&#24050;&#25104;&#20026;&#35768;&#22810;&#23398;&#31185;&#21644;&#24212;&#29992;&#29615;&#22659;&#20013;&#26085;&#24120;&#27963;&#21160;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23454;&#29616;&#26234;&#33021;&#33258;&#21160;&#21270;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;AI&#30340;&#26377;&#25928;&#24615;&#21487;&#20197;&#24402;&#21151;&#20110;&#22810;&#31181;&#22240;&#32032;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#25968;&#25454;&#30340;&#22686;&#21152;&#21487;&#29992;&#24615;&#12290;&#27431;&#27954;&#32852;&#30431;&#65288;EU&#65289;&#30340;&#19968;&#33324;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#31561;&#27861;&#35268;&#30340;&#20986;&#21488;&#26159;&#20026;&#20102;&#30830;&#20445;&#20010;&#20154;&#25968;&#25454;&#30340;&#20445;&#25252;&#12290;&#25910;&#38598;&#12289;&#22788;&#29702;&#25110;&#20998;&#20139;&#20010;&#20154;&#25968;&#25454;&#30340;&#36719;&#20214;&#31995;&#32479;&#24517;&#39035;&#36981;&#23432;&#36825;&#20123;&#27861;&#35268;&#30340;&#21512;&#35268;&#24615;&#35201;&#27714;&#12290;&#24320;&#21457;&#31526;&#21512;&#27861;&#35268;&#35201;&#27714;&#30340;&#36719;&#20214;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#35201;&#27714;&#24037;&#31243;&#65288;RE&#65289;&#38454;&#27573;&#20013;&#38656;&#35201;&#37325;&#35270;&#22788;&#29702;&#27861;&#24459;&#35201;&#27714;&#12290;RE&#20851;&#27880;&#30340;&#26159;&#25351;&#23450;&#21644;&#32500;&#25252;&#19968;&#20010;&#31995;&#32479;&#30340;&#38656;&#27714;&#65292;&#21253;&#25324;&#27861;&#24459;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern software has been an integral part of everyday activities in many disciplines and application contexts. Introducing intelligent automation by leveraging artificial intelligence (AI) led to break-throughs in many fields. The effectiveness of AI can be attributed to several factors, among which is the increasing availability of data. Regulations such as the general data protection regulation (GDPR) in the European Union (EU) are introduced to ensure the protection of personal data. Software systems that collect, process, or share personal data are subject to compliance with such regulations. Developing compliant software depends heavily on addressing legal requirements stipulated in applicable regulations, a central activity in the requirements engineering (RE) phase of the software development process. RE is concerned with specifying and maintaining requirements of a system-to-be, including legal requirements. Legal agreements which describe the policies organizations implement f
&lt;/p&gt;</description></item><item><title>&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#23384;&#22312;&#29256;&#26435;&#20405;&#26435;&#39118;&#38505;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#30740;&#31350;&#20102;&#26356;&#24494;&#22937;&#30340;&#20405;&#26435;&#24418;&#24335;&#65292;&#25552;&#20379;&#20102;&#26356;&#23454;&#38469;&#30340;&#29256;&#26435;&#20405;&#26435;&#35843;&#26597;&#26041;&#27861;</title><link>https://arxiv.org/abs/2311.12803</link><description>&lt;p&gt;
&#20851;&#20110;&#25991;&#26412;&#36716;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#29256;&#26435;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
On Copyright Risks of Text-to-Image Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12803
&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#23384;&#22312;&#29256;&#26435;&#20405;&#26435;&#39118;&#38505;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#30740;&#31350;&#20102;&#26356;&#24494;&#22937;&#30340;&#20405;&#26435;&#24418;&#24335;&#65292;&#25552;&#20379;&#20102;&#26356;&#23454;&#38469;&#30340;&#29256;&#26435;&#20405;&#26435;&#35843;&#26597;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35768;&#22810;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#20174;&#25991;&#26412;&#25552;&#31034;&#20013;&#21019;&#24314;&#22270;&#20687;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#20010;&#20219;&#21153;&#34987;&#31216;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22797;&#21046;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20803;&#32032;&#65292;&#23548;&#33268;&#36817;&#24180;&#26469;&#23454;&#38469;&#24212;&#29992;&#20013;&#29256;&#26435;&#25285;&#24551;&#19981;&#26029;&#22686;&#21152;&#12290;&#20026;&#20102;&#22238;&#24212;&#20851;&#20110;&#29256;&#26435;&#20405;&#26435;&#30340;&#25285;&#24551;&#22686;&#21152;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;&#30452;&#25509;&#26377;&#29256;&#26435;&#30340;&#25552;&#31034;&#26102;&#25193;&#25955;&#27169;&#22411;&#30340;&#29256;&#26435;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#26816;&#26597;&#26356;&#24494;&#22937;&#30340;&#20405;&#26435;&#24418;&#24335;&#25193;&#23637;&#20102;&#36825;&#19968;&#28857;&#65292;&#21363;&#21363;&#20351;&#26159;&#38388;&#25509;&#30340;&#25552;&#31034;&#20063;&#21487;&#33021;&#24341;&#21457;&#29256;&#26435;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#20197;&#31995;&#32479;&#22320;&#20135;&#29983;&#29992;&#20110;&#30740;&#31350;&#25193;&#25955;&#27169;&#22411;&#20013;&#29256;&#26435;&#38382;&#39064;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#26356;&#23454;&#38469;&#30340;&#29615;&#22659;&#20013;&#35843;&#26597;&#29256;&#26435;&#20405;&#26435;&#65292;&#28041;&#21450;&#22797;&#21046;&#35270;&#35273;&#29305;&#24449;&#32780;&#19981;&#26159;&#25972;&#20010;&#20316;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12803v2 Announce Type: replace-cross  Abstract: Diffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues. Specifically, we introduce a data generation pipeline to systematically produce data for studying copyright in diffusion models. Our pipeline enables us to investigate copyright infringement in a more practical setting, involving replicating visual features rather than entire works
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;Alpha Zero&#31639;&#27861;&#20013;&#30340;&#31526;&#21495;&#22238;&#24402;&#26469;&#21457;&#23637;&#29289;&#29702;&#23398;&#20013;&#30340;&#35299;&#26512;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#21487;&#20197;&#25512;&#23548;&#20986;Floquet&#31995;&#32479;&#20013;&#30340;&#39640;&#39057;&#23637;&#24320;&#65292;&#21487;&#33021;&#24102;&#26469;&#26032;&#30340;&#29289;&#29702;&#23398;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2311.12713</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#20013;&#30340;Alpha Zero&#65306;&#23558;Alpha Zero&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#20197;&#25214;&#21040;&#29289;&#29702;&#23398;&#20013;&#30340;&#35299;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Alpha Zero for Physics: Application of Symbolic Regression with Alpha Zero to find the analytical methods in physics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12713
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;Alpha Zero&#31639;&#27861;&#20013;&#30340;&#31526;&#21495;&#22238;&#24402;&#26469;&#21457;&#23637;&#29289;&#29702;&#23398;&#20013;&#30340;&#35299;&#26512;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#21487;&#20197;&#25512;&#23548;&#20986;Floquet&#31995;&#32479;&#20013;&#30340;&#39640;&#39057;&#23637;&#24320;&#65292;&#21487;&#33021;&#24102;&#26469;&#26032;&#30340;&#29289;&#29702;&#23398;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#29616;&#22312;&#25104;&#20026;&#21508;&#31181;&#20219;&#21153;&#30340;&#26356;&#24378;&#22823;&#24037;&#20855;&#65292;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22270;&#20687;&#35782;&#21035;&#12289;&#36194;&#24471;&#28216;&#25103;&#65292;&#29978;&#33267;&#29992;&#20110;&#29289;&#29702;&#38382;&#39064;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#20851;&#20110;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#25968;&#20540;&#35745;&#31639;&#21644;&#23454;&#39564;&#36741;&#21161;&#30340;&#30740;&#31350;&#65292;&#20294;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#23547;&#25214;&#35299;&#26512;&#26041;&#27861;&#30340;&#26041;&#27861;&#21364;&#40092;&#20026;&#20154;&#30693;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24102;Alpha Zero&#31639;&#27861;&#30340;&#31526;&#21495;&#22238;&#24402;&#26469;&#21457;&#23637;&#29289;&#29702;&#23398;&#20013;&#35299;&#26512;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#21363;&#29289;&#29702;&#23398;&#20013;&#30340;Alpha Zero&#65288;AZfP&#65289;&#12290;&#20316;&#20026;&#28436;&#31034;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AZfP&#21487;&#20197;&#25512;&#23548;&#20986;Floquet&#31995;&#32479;&#20013;&#30340;&#39640;&#39057;&#23637;&#24320;&#12290;AZfP&#21487;&#33021;&#20855;&#26377;&#22312;&#29289;&#29702;&#23398;&#20013;&#21457;&#23637;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12713v3 Announce Type: replace-cross  Abstract: Machine learning with neural networks is now becoming a more and more powerful tool for various tasks, such as natural language processing, image recognition, winning the game, and even for the issues of physics. Although there are many studies on the application of machine learning to numerical calculation and assistance of experiments, the methods of applying machine learning to find the analytical method are poorly studied. In this paper, we propose the frameworks of developing analytical methods in physics by using the symbolic regression with the Alpha Zero algorithm, that is Alpha Zero for physics (AZfP). As a demonstration, we show that AZfP can derive the high-frequency expansion in the Floquet systems. AZfP may have the possibility of developing a new theoretical framework in physics.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;EarnMore&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;RL&#20195;&#29702;&#19982;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#65288;CSPs&#65289;&#20132;&#20114;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2311.10801</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23631;&#34109;&#32929;&#31080;&#34920;&#31034;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#20013;&#36827;&#34892;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10801
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;EarnMore&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;RL&#20195;&#29702;&#19982;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#65288;CSPs&#65289;&#20132;&#20114;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#65288;PM&#65289;&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;&#37329;&#34701;&#20132;&#26131;&#20219;&#21153;&#65292;&#25506;&#32034;&#23450;&#26399;&#23558;&#36164;&#37329;&#37325;&#26032;&#37197;&#32622;&#21040;&#19981;&#21516;&#32929;&#31080;&#20013;&#20197;&#36861;&#27714;&#38271;&#26399;&#21033;&#28070;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26174;&#31034;&#20986;&#20854;&#28508;&#21147;&#65292;&#36890;&#36807;&#19982;&#37329;&#34701;&#24066;&#22330;&#20114;&#21160;&#26469;&#35757;&#32451;&#20855;&#26377;&#30408;&#21033;&#33021;&#21147;&#30340;PM&#20195;&#29702;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#22266;&#23450;&#32929;&#31080;&#27744;&#19978;&#65292;&#36825;&#19982;&#25237;&#36164;&#32773;&#30340;&#23454;&#38469;&#38656;&#27714;&#19981;&#19968;&#33268;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;EarnMore&#65292;&#19968;&#31181;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;RL&#20195;&#29702;&#19982;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#65288;CSPs&#65289;&#20132;&#20114;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10801v3 Announce Type: replace-cross  Abstract: Portfolio management (PM) is a fundamental financial trading task, which explores the optimal periodical reallocation of capitals into different stocks to pursue long-term profits. Reinforcement learning (RL) has recently shown its potential to train profitable agents for PM through interacting with financial markets. However, existing work mostly focuses on fixed stock pools, which is inconsistent with investors' practical demand. Specifically, the target stock pool of different investors varies dramatically due to their discrepancy on market states and individual investors may temporally adjust stocks they desire to trade (e.g., adding one popular stocks), which lead to customizable stock pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny change of the stock pool, which leads to high computational cost and unstable performance. To tackle this challenge, we propose EarnMore, a rEinforcement leARNin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;(MC)&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#21442;&#19982;&#21327;&#20316;&#22810;&#36718;&#35752;&#35770;&#65292;&#20174;&#32780;&#25552;&#39640;LLM&#30340;&#29087;&#32451;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;</title><link>https://arxiv.org/abs/2311.10537</link><description>&lt;p&gt;
MedAgents: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#21307;&#23398;&#25512;&#29702;&#30340;&#21512;&#20316;&#32773;
&lt;/p&gt;
&lt;p&gt;
MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10537
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;(MC)&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#21442;&#19982;&#21327;&#20316;&#22810;&#36718;&#35752;&#35770;&#65292;&#20174;&#32780;&#25552;&#39640;LLM&#30340;&#29087;&#32451;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23613;&#31649;&#22312;&#21508;&#31181;&#36890;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#38754;&#20020;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;(MC)&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#21442;&#19982;&#21327;&#20316;&#22810;&#36718;&#35752;&#35770;&#65292;&#20174;&#32780;&#25552;&#39640;LLM&#30340;&#29087;&#32451;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#21253;&#25324;&#20116;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#25910;&#38598;&#39046;&#22495;&#19987;&#23478;&#12289;&#25552;&#20986;&#20010;&#21035;&#20998;&#26512;&#12289;&#23558;&#36825;&#20123;&#20998;&#26512;&#24635;&#32467;&#25104;&#25253;&#21578;&#12289;&#22312;&#35752;&#35770;&#20013;&#21453;&#22797;&#36845;&#20195;&#30452;&#21040;&#36798;&#25104;&#20849;&#35782;&#65292;&#26368;&#32456;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#38646;-shot&#24773;&#26223;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#12290;&#22312;&#20061;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10537v2 Announce Type: replace-cross  Abstract: Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine dataset
&lt;/p&gt;</description></item><item><title>Symbol-LLM &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#21644;&#26694;&#26550;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31526;&#21495;&#25968;&#25454;&#27880;&#20837;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#25429;&#25417;&#31526;&#21495;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21644;&#20419;&#36827;&#21327;&#21516;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.09278</link><description>&lt;p&gt;
Symbol-LLM: &#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#31526;&#21495;&#20013;&#24515;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09278
&lt;/p&gt;
&lt;p&gt;
Symbol-LLM &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#21644;&#26694;&#26550;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31526;&#21495;&#25968;&#25454;&#27880;&#20837;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#25429;&#25417;&#31526;&#21495;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21644;&#20419;&#36827;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#25991;&#26412;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#22312;&#29702;&#35299;&#21644;&#34920;&#36798;&#36229;&#20986;&#33258;&#28982;&#35821;&#35328;&#33539;&#22260;&#30340;&#19990;&#30028;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;(&#20363;&#22914;&#21270;&#23398;&#20998;&#23376;&#24335;)&#12290;&#30452;&#25509;&#23558;&#19968;&#31995;&#21015;&#31526;&#21495;&#25968;&#25454;&#27880;&#20837;&#21040;LLMs&#30340;&#35757;&#32451;&#20013;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#24573;&#35270;&#20102;&#19981;&#21516;&#31526;&#21495;&#23478;&#26063;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#65292;&#20063;&#24573;&#35270;&#20102;&#33258;&#28982;&#25968;&#25454;&#21644;&#31526;&#21495;&#25968;&#25454;&#20043;&#38388;&#24179;&#34913;&#28151;&#21512;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#25454;&#21644;&#26694;&#26550;&#20004;&#20010;&#26041;&#38754;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#24341;&#20837;&#20102;Symbol-LLM&#31995;&#21015;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;34&#20010;&#20219;&#21153;&#24182;&#28085;&#30422;&#32422;20&#20010;&#19981;&#21516;&#31526;&#21495;&#23478;&#26063;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25429;&#25417;&#31526;&#21495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#24182;&#20419;&#36827;&#31526;&#21495;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#20004;&#38454;&#27573;&#35843;&#20248;&#26694;&#26550;&#25104;&#21151;&#22320;&#27880;&#20837;&#20102;&#31526;&#21495;&#30693;&#35782;&#32780;&#19981;&#20250;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09278v2 Announce Type: replace-cross  Abstract: Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language(e.g., chemical molecular formula). Injecting a collection of symbolic data directly into the training of LLMs can be problematic, as it disregards the synergies among different symbolic families and overlooks the need for a balanced mixture of natural and symbolic data. In this work, we tackle these challenges from both a data and framework perspective and introduce Symbol-LLM series models. First, we curated a data collection consisting of 34 tasks and incorporating approximately 20 distinct symbolic families, intending to capture the interrelations and foster synergies between symbols. Then, a two-stage tuning framework succeeds in injecting symbolic knowledge without loss 
&lt;/p&gt;</description></item><item><title>Safer-Instruct&#36890;&#36807;&#21453;&#21521;&#25351;&#23548;&#35843;&#25972;&#12289;&#25351;&#23548;&#24863;&#24212;&#21644;&#19987;&#23478;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26500;&#24314;&#22823;&#35268;&#27169;&#20559;&#22909;&#25968;&#25454;&#30340;&#30446;&#30340;&#65292;&#20174;&#32780;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2311.08685</link><description>&lt;p&gt;
Safer-Instruct: &#20351;&#29992;&#33258;&#21160;&#21270;&#20559;&#22909;&#25968;&#25454;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Safer-Instruct: Aligning Language Models with Automated Preference Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08685
&lt;/p&gt;
&lt;p&gt;
Safer-Instruct&#36890;&#36807;&#21453;&#21521;&#25351;&#23548;&#35843;&#25972;&#12289;&#25351;&#23548;&#24863;&#24212;&#21644;&#19987;&#23478;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26500;&#24314;&#22823;&#35268;&#27169;&#20559;&#22909;&#25968;&#25454;&#30340;&#30446;&#30340;&#65292;&#20174;&#32780;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#37325;&#35201;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#20026;RLHF&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#26159;&#19968;&#39033;&#36164;&#28304;&#23494;&#38598;&#19988;&#38656;&#35201;&#21019;&#36896;&#21147;&#30340;&#36807;&#31243;&#65292;&#32780;&#29616;&#26377;&#30340;&#33258;&#21160;&#29983;&#25104;&#26041;&#27861;&#22312;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Safer-Instruct&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#26500;&#24314;&#22823;&#35268;&#27169;&#20559;&#22909;&#25968;&#25454;&#30340;&#20840;&#26032;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#21453;&#21521;&#25351;&#23548;&#35843;&#25972;&#12289;&#25351;&#23548;&#24863;&#24212;&#21644;&#19987;&#23478;&#27169;&#22411;&#35780;&#20272;&#65292;&#20197;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#32773;&#12290;&#20026;&#20102;&#39564;&#35777;Safer-Instruct&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#35813;&#27969;&#27700;&#32447;&#24212;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#20559;&#22909;&#25968;&#25454;&#38598;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#22312;&#36825;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;Alpaca&#27169;&#22411;&#19981;&#20165;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#26080;&#23475;&#24615;&#65292;&#36824;&#34920;&#29616;&#20986;&#20248;&#20110;&#22312;&#20154;&#24037;&#26631;&#27880;&#30340;&#23433;&#20840;&#20559;&#22909;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08685v2 Announce Type: replace-cross  Abstract: Reinforcement learning from human feedback (RLHF) is a vital strategy for enhancing model capability in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while existing automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for automatically constructing large-scale preference data. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. To verify the effectiveness of Safer-Instruct, we apply the pipeline to construct a safety preference dataset as a case study. Finetuning an Alpaca model on this synthetic dataset not only demonstrates improved harmlessness but also outperforms models fine-tuned on human-annotated safety preference data, all the while mainta
&lt;/p&gt;</description></item><item><title>&#33258;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;&#24773;&#26223;&#20013;&#65292;&#33021;&#22815;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#37325;&#22797;&#24615;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.08154</link><description>&lt;p&gt;
&#20877;&#38382;&#19968;&#27425;&#65306;&#33258;&#19968;&#33268;&#24615;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#65288;&#20960;&#20046;&#65289;&#25152;&#26377;&#22330;&#26223;&#20013;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08154
&lt;/p&gt;
&lt;p&gt;
&#33258;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;&#24773;&#26223;&#20013;&#65292;&#33021;&#22815;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#37325;&#22797;&#24615;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;CoT&#25552;&#31034;&#20013;&#36890;&#24120;&#20351;&#29992;&#30340;&#36138;&#23146;&#35299;&#30721;&#20250;&#23548;&#33268;&#37325;&#22797;&#24615;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#32570;&#28857;&#65292;&#38598;&#25104;&#20248;&#21270;&#23581;&#35797;&#33719;&#24471;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#20197;&#24471;&#21040;&#26368;&#32456;&#31572;&#26696;&#38598;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#35201;&#20040;&#31616;&#21333;&#22320;&#37319;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#21518;&#22788;&#29702;&#65292;&#27604;&#22914;&#8220;&#33258;&#19968;&#33268;&#24615;&#8221;&#65292;&#35201;&#20040;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;&#20960;&#20010;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20154;&#31867;&#27880;&#37322;&#30340;&#38468;&#21152;&#27169;&#22411;&#26469;&#22312;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#20013;&#36873;&#25321;&#26368;&#20339;&#36335;&#24452;&#65292;&#20294;&#26410;&#33021;&#25512;&#24191;&#21040;&#29616;&#23454;&#35774;&#32622;&#65292;&#20854;&#20013;&#36755;&#20837;&#38382;&#39064;&#31867;&#22411;&#26410;&#30693;&#25110;&#25512;&#29702;&#36335;&#24452;&#30340;&#31572;&#26696;&#26684;&#24335;&#26410;&#30693;&#12290;&#20026;&#20102;&#36991;&#20813;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33258;&#19968;&#33268;&#24615;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#26223;&#20013;&#36866;&#29992;&#65292;&#20854;&#20013;&#36755;&#20837;&#38382;&#39064;&#30340;&#31867;&#22411;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08154v2 Announce Type: replace-cross  Abstract: Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local optimality. To address this shortcoming, ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly. However, current ensemble-optimization methods either simply employ rule-based post-processing such as \textit{self-consistency}, or train an additional model based on several task-related human annotations to select the best one among multiple reasoning paths, yet fail to generalize to realistic settings where the type of input questions is unknown or the answer format of reasoning paths is unknown. To avoid their limitations, we propose \textbf{Self-Agreement}, a generalizable ensemble-optimization method applying in almost all scenarios where the type of input question
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2311.08045</link><description>&lt;p&gt;
&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adversarial Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08045
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#35843;&#25972;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20132;&#20114;&#36136;&#37327;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25351;&#23548;LLM&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25345;&#32493;&#26356;&#26032;LLMs&#20250;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#19982;&#20154;&#31867;&#39318;&#36873;&#21709;&#24212;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#65292;&#36825;&#38459;&#30861;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#39069;&#22806;&#36827;&#34892;&#20559;&#22909;&#27880;&#37322;&#65292;&#20197;&#36866;&#24212;&#36716;&#31227;&#20998;&#24067;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#36164;&#28304;&#12290;&#38024;&#23545;&#26356;&#39640;&#25928;&#30340;&#20154;&#31867;&#20559;&#22909;&#20248;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;LLM&#20195;&#29702;&#21644;&#20559;&#22909;&#27169;&#22411;&#36890;&#36807;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#20132;&#26367;&#26356;&#26032;&#12290;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;APO&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#35782;&#21035;&#26377;&#27602;&#12289;&#20882;&#29359;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;&#20869;&#23481;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#25913;&#36827;&#26159;&#21542;&#30495;&#27491;&#28385;&#36275;&#20102;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#24037;&#20316;&#20013;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2311.07879</link><description>&lt;p&gt;
&#27602;&#24615;&#26816;&#27979;&#24182;&#19981;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#65306;&#24357;&#21512;&#25903;&#25345;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#35782;&#21035;&#26377;&#27602;&#12289;&#20882;&#29359;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;&#20869;&#23481;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#25913;&#36827;&#26159;&#21542;&#30495;&#27491;&#28385;&#36275;&#20102;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#24037;&#20316;&#20013;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#35782;&#21035;&#26377;&#27602;&#12289;&#20882;&#29359;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;&#20869;&#23481;&#26041;&#38754;&#21462;&#24471;&#20102;&#38271;&#36275;&#30340;&#36827;&#23637;&#65292;&#26088;&#22312;&#20943;&#36731;&#31649;&#29702;&#21592;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#20219;&#21153;&#30340;&#25913;&#36827;&#26159;&#21542;&#30495;&#27491;&#28385;&#36275;&#20102;&#31649;&#29702;&#21592;&#22312;&#24037;&#20316;&#20013;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#36807;&#21435;&#30740;&#31350;&#21162;&#21147;&#33268;&#21147;&#20110;&#20026;&#20869;&#23481;&#31649;&#29702;&#30340;&#21508;&#20010;&#26041;&#38754;&#25552;&#20379;&#33258;&#21160;&#21270;&#25903;&#25345;&#19982;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#30340;&#38656;&#27714;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#23588;&#20854;&#26159;&#22312;&#35782;&#21035;&#36829;&#21453;&#21508;&#31181;&#31649;&#29702;&#35268;&#21017;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;Hugging Face&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#25581;&#31034;&#28085;&#30422;&#19977;&#20010;&#31034;&#33539;&#35770;&#22363;&#30340;&#21508;&#31181;&#31649;&#29702;&#35268;&#21017;&#21644;&#25351;&#21335;&#30340;&#27169;&#22411;&#30340;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#26631;&#35760;&#26576;&#20010;&#29305;&#23450;&#35770;&#22363;&#30340;&#24179;&#21488;&#35268;&#21017;&#36829;&#35268;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29992;&#25143;&#35843;&#26597;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07879v2 Announce Type: replace-cross  Abstract: Extensive efforts in automated approaches for content moderation have been focused on developing models to identify toxic, offensive, and hateful content with the aim of lightening the load for moderators. Yet, it remains uncertain whether improvements on those tasks have truly addressed moderators' needs in accomplishing their work. In this paper, we surface gaps between past research efforts that have aimed to provide automation for aspects of content moderation and the needs of volunteer content moderators, regarding identifying violations of various moderation rules. To do so, we conduct a model review on Hugging Face to reveal the availability of models to cover various moderation rules and guidelines from three exemplar forums. We further put state-of-the-art LLMs to the test, evaluating how well these models perform in flagging violations of platform rules from one particular forum. Finally, we conduct a user survey stud
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Residual Message Graph Convolution Network&#65288;ResMGCN&#65289;&#65292;&#29992;&#20110;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#39044;&#27979;&#29983;&#29289;&#21307;&#23398;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.07632</link><description>&lt;p&gt;
ResMGCN&#65306;&#29992;&#20110;&#24555;&#36895;&#29983;&#29289;&#21307;&#23398;&#30456;&#20114;&#20316;&#29992;&#21457;&#29616;&#30340;&#27531;&#24046;&#28040;&#24687;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ResMGCN: Residual Message Graph Convolution Network for Fast Biomedical Interactions Discovering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07632
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Residual Message Graph Convolution Network&#65288;ResMGCN&#65289;&#65292;&#29992;&#20110;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#39044;&#27979;&#29983;&#29289;&#21307;&#23398;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#22270;&#23545;&#20110;&#29616;&#20195;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#30456;&#20114;&#20316;&#29992;&#30340;&#21457;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;&#27604;&#22914;&#22810;&#26679;&#21270;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#35782;&#21035;&#21644;&#33647;&#29289;&#21457;&#29616;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#20154;&#31867;&#20581;&#24247;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#30446;&#21069;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#34987;&#25552;&#20986;&#26469;&#23398;&#20064;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#23454;&#20307;&#65292;&#24182;&#36890;&#36807;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#20934;&#30830;&#25581;&#31034;&#29983;&#29289;&#21307;&#23398;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#34429;&#28982;&#24357;&#34917;&#20102;&#36828;&#36317;&#31163;&#29305;&#24449;&#30340;&#34928;&#20943;&#65292;&#20294;&#21364;&#20197;&#20887;&#20313;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#20026;&#20195;&#20215;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Residual Message Graph Convolution Network&#65288;ResMGCN&#65289;&#65292;&#29992;&#20110;&#20197;&#19968;&#31181;&#19981;&#21516;&#30340;&#24605;&#36335;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#39044;&#27979;&#29983;&#29289;&#21307;&#23398;&#30456;&#20114;&#20316;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ResMGCN&#19981;&#26159;&#22686;&#24378;&#36828;&#31243;&#33410;&#28857;&#30340;&#28040;&#24687;&#65292;&#32780;&#26159;&#19982;&#19979;&#19968;&#36718;&#39640;&#38454;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07632v2 Announce Type: replace-cross  Abstract: Biomedical information graphs are crucial for interaction discovering of biomedical information in modern age, such as identification of multifarious molecular interactions and drug discovery, which attracts increasing interests in biomedicine, bioinformatics, and human healthcare communities. Nowadays, more and more graph neural networks have been proposed to learn the entities of biomedical information and precisely reveal biomedical molecule interactions with state-of-the-art results. These methods remedy the fading of features from a far distance but suffer from remedying such problem at the expensive cost of redundant memory and time. In our paper, we propose a novel Residual Message Graph Convolution Network (ResMGCN) for fast and precise biomedical interaction prediction in a different idea. Specifically, instead of enhancing the message from far nodes, ResMGCN aggregates lower-order information with the next round highe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#24320;&#25918;&#22270;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#23545;&#26410;&#30693;&#24322;&#24120;&#30340;&#24191;&#20041;&#26816;&#27979;&#33021;&#21147;</title><link>https://arxiv.org/abs/2311.06835</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#23454;&#29616;&#24320;&#25918;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open-Set Graph Anomaly Detection via Normal Structure Regularisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06835
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#24320;&#25918;&#22270;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#23545;&#26410;&#30693;&#24322;&#24120;&#30340;&#24191;&#20041;&#26816;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#20219;&#21153;&#65292;&#21363;&#24320;&#25918;&#24335;GAD&#65292;&#26088;&#22312;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#27491;&#24120;&#33410;&#28857;&#21644;&#24322;&#24120;&#33410;&#28857;&#65288;&#31216;&#20026;&#24050;&#30693;&#24322;&#24120;&#65289;&#26469;&#26816;&#27979;&#24322;&#24120;&#33410;&#28857;&#65292;&#36825;&#20123;&#33410;&#28857;&#26080;&#27861;&#23637;&#31034;&#25152;&#26377;&#21487;&#33021;&#30340;&#25512;&#29702;&#26102;&#24322;&#24120;&#12290;&#24050;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20026;GAD&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#24322;&#24120;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#22823;&#22823;&#38477;&#20302;&#26816;&#27979;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#24448;&#24448;&#36807;&#20998;&#24378;&#35843;&#25311;&#21512;&#24050;&#30693;&#24322;&#24120;&#65292;&#23548;&#33268;&#23545;&#26410;&#30693;&#24322;&#24120;&#65288;&#21363;&#26410;&#34987;&#26631;&#35760;&#30340;&#24322;&#24120;&#33410;&#28857;&#65289;&#30340;&#24369;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#34987;&#24341;&#20837;&#20197;&#22788;&#29702;&#27431;&#20960;&#37324;&#24503;&#25968;&#25454;&#65292;&#26410;&#33021;&#26377;&#25928;&#25429;&#25417;GAD&#30340;&#37325;&#35201;&#38750;&#27431;&#20960;&#37324;&#24503;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#24335;GAD&#26041;&#27861;&#65292;&#21363;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#65288;NSReg&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#30693;&#24322;&#24120;&#30340;&#24191;&#20041;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06835v2 Announce Type: replace-cross  Abstract: This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to detect anomalous nodes using a small number of labelled training normal and anomaly nodes (known as seen anomalies) that cannot illustrate all possible inference-time abnormalities. The availability of that labelled data provides crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current methods tend to over-emphasise fitting the seen anomalies, leading to a weak generalisation ability to detect unseen anomalies, i.e., those that are not illustrated by the labelled anomaly nodes. Further, they were introduced to handle Euclidean data, failing to effectively capture important non-Euclidean features for GAD. In this work, we propose a novel open-set GAD approach, namely Normal Structure Regularisation (NSReg), to achieve generalised detection ability to unseen 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#19982;&#25628;&#32034;&#24341;&#25806;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#25552;&#21462;&#30456;&#20851;&#19978;&#19979;&#25991;&#26469;&#20010;&#24615;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#25913;&#36827;&#32593;&#32476;&#25628;&#32034;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2311.06318</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#26597;&#35810;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06318
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#19982;&#25628;&#32034;&#24341;&#25806;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#25552;&#21462;&#30456;&#20851;&#19978;&#19979;&#25991;&#26469;&#20010;&#24615;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#25913;&#36827;&#32593;&#32476;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25797;&#38271;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#23427;&#20204;&#25152;&#28041;&#21450;&#30340;&#25104;&#26412;&#24040;&#22823;&#65292;&#23427;&#20204;&#20173;&#28982;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#38745;&#24577;&#30340;&#65292;&#24182;&#19988;&#38590;&#20197;&#20010;&#24615;&#21270;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#20174;&#26681;&#25454;&#29992;&#25143;&#30340;&#20559;&#22909;&#12289;&#30446;&#26631;&#21644;&#30693;&#35782;&#37327;&#23450;&#21046;&#30340;&#29983;&#25104;&#20013;&#21463;&#30410;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#32593;&#32476;&#25628;&#32034;&#65292;&#20102;&#35299;&#29992;&#25143;&#35797;&#22270;&#20570;&#20160;&#20040;&#12289;&#20851;&#24515;&#20160;&#20040;&#20197;&#21450;&#20182;&#20204;&#30693;&#36947;&#20160;&#20040;&#21487;&#20197;&#25552;&#39640;&#25628;&#32034;&#20307;&#39564;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#29992;&#25143;&#19982;&#25628;&#32034;&#24341;&#25806;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;LLM&#20197;&#20010;&#24615;&#21270;&#20854;&#36755;&#20986;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#29992;&#25143;&#22312;&#32593;&#32476;&#19978;&#30340;&#25628;&#32034;&#21644;&#27983;&#35272;&#27963;&#21160;&#26500;&#24314;&#20102;&#27599;&#20010;&#29992;&#25143;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#23384;&#20648;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#20026;LLM&#25552;&#20379;&#20855;&#26377;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#30340;&#25552;&#31034;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06318v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) excel at tackling various natural language tasks. However, due to the significant costs involved in re-training or fine-tuning them, they remain largely static and difficult to personalize. Nevertheless, a variety of applications could benefit from generations that are tailored to users' preferences, goals, and knowledge. Among them is web search, where knowing what a user is trying to accomplish, what they care about, and what they know can lead to improved search experiences. In this work, we propose a novel and general approach that augments an LLM with relevant context from users' interaction histories with a search engine in order to personalize its outputs. Specifically, we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web, which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29369;&#35947;&#27169;&#31946;&#38598;&#30340;&#22810;&#31181;&#21253;&#21547;&#20851;&#31995;&#23450;&#20041;&#12289;&#29369;&#35947;&#27169;&#31946;&#20449;&#24687;&#31995;&#32479;&#30340;&#22522;&#30784;&#21629;&#39064;&#21644;&#22522;&#20110;&#22810;&#24378;&#24230;&#26234;&#33021;&#20998;&#31867;&#22120;&#30340;&#20581;&#24247;&#29366;&#24577;&#35786;&#26029;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.04256</link><description>&lt;p&gt;
&#29369;&#35947;&#27169;&#31946;&#38598;&#21450;&#20854;&#24212;&#29992;&#20110;&#22810;&#24378;&#24230;&#26234;&#33021;&#20998;&#31867;&#22120;&#30340;&#22522;&#30784;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Foundational theories of hesitant fuzzy sets and hesitant fuzzy information systems and their applications for multi-strength intelligent classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29369;&#35947;&#27169;&#31946;&#38598;&#30340;&#22810;&#31181;&#21253;&#21547;&#20851;&#31995;&#23450;&#20041;&#12289;&#29369;&#35947;&#27169;&#31946;&#20449;&#24687;&#31995;&#32479;&#30340;&#22522;&#30784;&#21629;&#39064;&#21644;&#22522;&#20110;&#22810;&#24378;&#24230;&#26234;&#33021;&#20998;&#31867;&#22120;&#30340;&#20581;&#24247;&#29366;&#24577;&#35786;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29369;&#35947;&#27169;&#31946;&#38598;&#22312;&#26576;&#20123;&#19981;&#30830;&#23450;&#21644;&#29369;&#35947;&#30340;&#24773;&#20917;&#19979;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#22312;&#38598;&#21512;&#20013;&#65292;&#21253;&#21547;&#20851;&#31995;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#22522;&#30784;&#30340;&#23450;&#20041;&#12290;&#22240;&#27492;&#65292;&#20316;&#20026;&#19968;&#31181;&#38598;&#21512;&#65292;&#29369;&#35947;&#27169;&#31946;&#38598;&#38656;&#35201;&#19968;&#20010;&#26126;&#30830;&#30340;&#21253;&#21547;&#20851;&#31995;&#23450;&#20041;&#12290;&#22522;&#20110;&#31163;&#25955;&#24418;&#24335;&#30340;&#29369;&#35947;&#27169;&#31946;&#38582;&#23646;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#31181;&#36866;&#29992;&#20110;&#29369;&#35947;&#27169;&#31946;&#38598;&#30340;&#21253;&#21547;&#20851;&#31995;&#12290;&#38543;&#21518;&#65292;&#20171;&#32461;&#20102;&#19968;&#20123;&#29369;&#35947;&#27169;&#31946;&#38598;&#30340;&#22522;&#30784;&#21629;&#39064;&#65292;&#20197;&#21450;&#29369;&#35947;&#27169;&#31946;&#38598;&#26063;&#30340;&#21629;&#39064;&#12290;&#38024;&#23545;&#21442;&#25968;&#20943;&#23569;&#65292;&#25552;&#20986;&#20102;&#29369;&#35947;&#27169;&#31946;&#20449;&#24687;&#31995;&#32479;&#30340;&#19968;&#20123;&#22522;&#30784;&#21629;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#31034;&#20363;&#21644;&#31639;&#27861;&#26469;&#35828;&#26126;&#21442;&#25968;&#20943;&#23569;&#30340;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#24378;&#24230;&#26234;&#33021;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#23545;&#22797;&#26434;&#31995;&#32479;&#36827;&#34892;&#20581;&#24247;&#29366;&#24577;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04256v3 Announce Type: replace  Abstract: Hesitant fuzzy sets are widely used in certain instances of uncertainty and hesitation. In sets, the inclusion relationship is an important and foundational definition. Thus, as a kind of set, hesitant fuzzy sets require an explicit definition of inclusion relationship. Based on the hesitant fuzzy membership degree of discrete form, several kinds of inclusion relationships for hesitant fuzzy sets are proposed in this work. Then, some foundational propositions of hesitant fuzzy sets are presented, along with propositions of families of hesitant fuzzy sets. Some foundational propositions of hesitant fuzzy information systems are proposed with respect to parameter reductions and an example and an algorithm are given to illustrate the processes of parameter reduction. Finally, a multi-strength intelligent classifier is proposed to make health state diagnoses for complex systems.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#29289;&#20307;&#30340;&#36816;&#21160;&#23398;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#21160;&#23398;&#24863;&#30693;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;LLMs&#30340;&#20302;&#32423;&#36816;&#21160;&#36712;&#36857;&#20301;&#28857;&#65292;&#20197;&#23454;&#29616;&#23545;&#21487;&#31227;&#21160;&#29289;&#20307;&#30340;&#27867;&#21270;&#25805;&#20316;</title><link>https://arxiv.org/abs/2311.02847</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#30340;&#36816;&#21160;&#23398;&#24863;&#30693;&#25552;&#31034;&#23454;&#29616;&#23545;&#21487;&#31227;&#21160;&#29289;&#20307;&#30340;&#27867;&#21270;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02847
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29289;&#20307;&#30340;&#36816;&#21160;&#23398;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#21160;&#23398;&#24863;&#30693;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;LLMs&#30340;&#20302;&#32423;&#36816;&#21160;&#36712;&#36857;&#20301;&#28857;&#65292;&#20197;&#23454;&#29616;&#23545;&#21487;&#31227;&#21160;&#29289;&#20307;&#30340;&#27867;&#21270;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#30340;&#21487;&#31227;&#21160;&#29289;&#20307;&#25805;&#20316;&#23545;&#20110;&#23478;&#24237;&#21161;&#25163;&#26426;&#22120;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20174;&#28436;&#31034;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#25110;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#25968;&#25454;&#25910;&#38598;&#21644;&#31934;&#30830;&#23545;&#35937;&#27169;&#25311;&#25104;&#26412;&#39640;&#26114;&#65292;&#36825;&#20123;&#24037;&#20316;&#20173;&#28982;&#38590;&#20197;&#22312;&#22810;&#26679;&#30340;&#21487;&#31227;&#21160;&#29289;&#20307;&#19978;&#23454;&#29616;&#24191;&#27867;&#30340;&#36866;&#24212;&#24615;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#23454;&#29616;&#21487;&#27867;&#21270;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20391;&#37325;&#20110;&#39640;&#23618;&#20219;&#21153;&#35268;&#21010;&#65292;&#24573;&#35270;&#20102;&#20302;&#23618;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#22522;&#20110;&#19968;&#20010;&#29702;&#24565;&#65292;&#21363;&#29289;&#20307;&#30340;&#36816;&#21160;&#23398;&#32467;&#26500;&#20915;&#23450;&#20102;&#25105;&#20204;&#22914;&#20309;&#25805;&#32437;&#23427;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36816;&#21160;&#23398;&#24863;&#30693;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#29289;&#20307;&#30340;&#36816;&#21160;&#23398;&#30693;&#35782;&#25552;&#31034;LLMs&#29983;&#25104;&#20302;&#23618;&#36816;&#21160;&#36712;&#36857;&#20301;&#28857;&#65292;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02847v3 Announce Type: replace-cross  Abstract: Generalizable articulated object manipulation is essential for home-assistant robots. Recent efforts focus on imitation learning from demonstrations or reinforcement learning in simulation, however, due to the prohibitive costs of real-world data collection and precise object simulation, it still remains challenging for these works to achieve broad adaptability across diverse articulated objects. Recently, many works have tried to utilize the strong in-context learning ability of Large Language Models (LLMs) to achieve generalizable robotic manipulation, but most of these researches focus on high-level task planning, sidelining low-level robotic control. In this work, building on the idea that the kinematic structure of the object determines how we can manipulate it, we propose a kinematic-aware prompting framework that prompts LLMs with kinematic knowledge of objects to generate low-level motion trajectory waypoints, supportin
&lt;/p&gt;</description></item><item><title>DreamSmooth&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#26102;&#38388;&#24179;&#28369;&#22870;&#21169;&#32780;&#38750;&#31934;&#30830;&#22870;&#21169;&#65292;&#20248;&#21270;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.01450</link><description>&lt;p&gt;
DreamSmooth&#65306;&#36890;&#36807;&#22870;&#21169;&#24179;&#28369;&#25913;&#36827;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.01450
&lt;/p&gt;
&lt;p&gt;
DreamSmooth&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#26102;&#38388;&#24179;&#28369;&#22870;&#21169;&#32780;&#38750;&#31934;&#30830;&#22870;&#21169;&#65292;&#20248;&#21270;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;: &#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#22240;&#20854;&#20197;&#33410;&#32422;&#26679;&#26412;&#30340;&#26041;&#24335;&#23398;&#20064;&#22797;&#26434;&#34892;&#20026;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65306;&#36890;&#36807;&#29983;&#25104;&#20855;&#26377;&#39044;&#27979;&#22870;&#21169;&#30340;&#34394;&#25311;&#36712;&#36857;&#26469;&#35268;&#21010;&#21160;&#20316;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22870;&#21169;&#39044;&#27979;&#36890;&#24120;&#26159;MBRL&#30340;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38590;&#20197;&#39044;&#27979;&#30340;&#31232;&#30095;&#22870;&#21169;&#12290;&#21463;&#21040;&#20154;&#31867;&#20174;&#31895;&#31961;&#22870;&#21169;&#20272;&#35745;&#20013;&#23398;&#20064;&#30340;&#30452;&#35273;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22870;&#21169;&#24179;&#28369;&#26041;&#27861;DreamSmooth&#65292;&#23427;&#23398;&#20064;&#39044;&#27979;&#19968;&#20010;&#22312;&#32473;&#23450;&#26102;&#38388;&#27493;&#30340;&#22870;&#21169;&#30340;&#26102;&#38388;&#24179;&#28369;&#29256;&#26412;&#65292;&#32780;&#19981;&#26159;&#31934;&#30830;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#23637;&#31034;DreamSmooth&#22312;&#38271;&#35270;&#37326;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26082;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#19978;&#65292;&#21448;&#19981;&#25439;&#22833;&#22312;&#24120;&#35265;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#65292;&#22914;Deepmind Control Suite&#21644;Atari&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.01450v2 Announce Type: replace-cross  Abstract: Model-based reinforcement learning (MBRL) has gained much attention for its ability to learn complex behaviors in a sample-efficient way: planning actions by generating imaginary trajectories with predicted rewards. Despite its success, we found that surprisingly, reward prediction is often a bottleneck of MBRL, especially for sparse rewards that are challenging (or even ambiguous) to predict. Motivated by the intuition that humans can learn from rough reward estimates, we propose a simple yet effective reward smoothing approach, DreamSmooth, which learns to predict a temporally-smoothed reward, instead of the exact reward at the given timestep. We empirically show that DreamSmooth achieves state-of-the-art performance on long-horizon sparse-reward tasks both in sample efficiency and final performance without losing performance on common benchmarks, such as Deepmind Control Suite and Atari benchmarks.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Syntheseus&#24314;&#31435;&#30340;&#22522;&#20934;&#24211;&#37325;&#26032;&#35780;&#20272;&#20102;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#24182;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#24037;&#20316;&#30340;&#25351;&#23548;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2310.19796</link><description>&lt;p&gt;
&#20351;&#29992;Syntheseus&#37325;&#26032;&#35780;&#20272;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Re-evaluating Retrosynthesis Algorithms with Syntheseus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19796
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Syntheseus&#24314;&#31435;&#30340;&#22522;&#20934;&#24211;&#37325;&#26032;&#35780;&#20272;&#20102;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#24182;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#24037;&#20316;&#30340;&#25351;&#23548;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#20998;&#23376;&#21512;&#25104;&#35268;&#21010;&#65292;&#20063;&#31216;&#20026;&#22238;&#28335;&#21512;&#25104;&#65292;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#21270;&#23398;&#30028;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#23613;&#31649;&#30475;&#20284;&#21462;&#24471;&#20102;&#31283;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#23384;&#22312;&#19981;&#23436;&#21892;&#30340;&#22522;&#20934;&#21644;&#19981;&#19968;&#33268;&#30340;&#27604;&#36739;&#25513;&#30422;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;syntheseus&#30340;&#22522;&#20934;&#24211;&#65292;&#36890;&#36807;&#40664;&#35748;&#25512;&#24191;&#26368;&#20339;&#23454;&#36341;&#65292;&#23454;&#29616;&#20102;&#23545;&#21333;&#27493;&#21644;&#22810;&#27493;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#30340;&#19968;&#33268;&#32780;&#26377;&#24847;&#20041;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;syntheseus&#37325;&#26032;&#35780;&#20272;&#20102;&#33509;&#24178;&#20808;&#21069;&#30340;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#20180;&#32454;&#35780;&#20272;&#26102;&#65292;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#30340;&#25490;&#21517;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#36825;&#19968;&#39046;&#22495;&#26410;&#26469;&#24037;&#20316;&#30340;&#25351;&#23548;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19796v2 Announce Type: replace-cross  Abstract: The planning of how to synthesize molecules, also known as retrosynthesis, has been a growing focus of the machine learning and chemistry communities in recent years. Despite the appearance of steady progress, we argue that imperfect benchmarks and inconsistent comparisons mask systematic shortcomings of existing techniques. To remedy this, we present a benchmarking library called syntheseus which promotes best practice by default, enabling consistent meaningful evaluation of single-step and multi-step retrosynthesis algorithms. We use syntheseus to re-evaluate a number of previous retrosynthesis algorithms, and find that the ranking of state-of-the-art models changes when evaluated carefully. We end with guidance for future works in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#29305;&#23450;&#25552;&#31034;&#20013;&#27602;&#25915;&#20987;&#30340;&#25104;&#21151;&#65292;&#24182;&#20171;&#32461;&#20102;Nightshade&#36825;&#31181;&#20248;&#21270;&#30340;&#20013;&#27602;&#25915;&#20987;&#26041;&#27861;&#65292;&#22312;&#35270;&#35273;&#19978;&#19982;&#33391;&#24615;&#22270;&#20687;&#30456;&#21516;&#65292;&#21487;&#22312;&#23569;&#20110;100&#20010;&#27602;&#26679;&#26412;&#20013;&#30772;&#22351;&#31283;&#23450;&#25193;&#25955;SDXL&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2310.13828</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#29305;&#23450;&#25552;&#31034;&#20013;&#27602;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.13828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#29305;&#23450;&#25552;&#31034;&#20013;&#27602;&#25915;&#20987;&#30340;&#25104;&#21151;&#65292;&#24182;&#20171;&#32461;&#20102;Nightshade&#36825;&#31181;&#20248;&#21270;&#30340;&#20013;&#27602;&#25915;&#20987;&#26041;&#27861;&#65292;&#22312;&#35270;&#35273;&#19978;&#19982;&#33391;&#24615;&#22270;&#20687;&#30456;&#21516;&#65292;&#21487;&#22312;&#23569;&#20110;100&#20010;&#27602;&#26679;&#26412;&#20013;&#30772;&#22351;&#31283;&#23450;&#25193;&#25955;SDXL&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#25805;&#32437;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#22312;&#35757;&#32451;&#26102;&#23558;&#24847;&#22806;&#34892;&#20026;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#20855;&#26377;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#24403;&#21069;&#23545;&#20013;&#27602;&#25915;&#20987;&#30340;&#29702;&#35299;&#34920;&#26126;&#65292;&#25104;&#21151;&#30340;&#25915;&#20987;&#38656;&#35201;&#23558;&#25968;&#30334;&#19975;&#20010;&#27602;&#26679;&#26412;&#27880;&#20837;&#23427;&#20204;&#30340;&#35757;&#32451;&#31649;&#36947;&#12290;&#26412;&#25991;&#34920;&#26126;&#20013;&#27602;&#25915;&#20987;&#21487;&#20197;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#27169;&#22411;&#20013;&#27599;&#20010;&#27010;&#24565;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#38750;&#24120;&#26377;&#38480;&#65292;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#29305;&#23450;&#25552;&#31034;&#20013;&#27602;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#38024;&#23545;&#27169;&#22411;&#23545;&#20010;&#21035;&#25552;&#31034;&#20316;&#20986;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Nightshade&#65292;&#19968;&#31181;&#38024;&#23545;&#29305;&#23450;&#25552;&#31034;&#30340;&#20248;&#21270;&#20013;&#27602;&#25915;&#20987;&#65292;&#20854;&#20013;&#27602;&#26679;&#26412;&#22312;&#35270;&#35273;&#19978;&#19982;&#20855;&#26377;&#21305;&#37197;&#25991;&#26412;&#25552;&#31034;&#30340;&#33391;&#24615;&#22270;&#20687;&#30475;&#36215;&#26469;&#23436;&#20840;&#30456;&#21516;&#12290;Nightshade&#27602;&#26679;&#26412;&#20063;&#32463;&#36807;&#20102;&#20248;&#21270;&#20197;&#36827;&#34892;&#26377;&#25928;&#25915;&#20987;&#65292;&#24182;&#21487;&#20197;&#22312;&lt;100&#20010;&#27602;&#26679;&#26412;&#20013;&#25439;&#22351;&#19968;&#20010;&#31283;&#23450;&#25193;&#25955;SDXL&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.13828v2 Announce Type: replace-cross  Abstract: Data poisoning attacks manipulate training data to introduce unexpected behaviors into machine learning models at training time. For text-to-image generative models with massive training datasets, current understanding of poisoning attacks suggests that a successful attack would require injecting millions of poison samples into their training pipeline. In this paper, we show that poisoning attacks can be successful on generative models. We observe that training data per concept can be quite limited in these models, making them vulnerable to prompt-specific poisoning attacks, which target a model's ability to respond to individual prompts.   We introduce Nightshade, an optimized prompt-specific poisoning attack where poison samples look visually identical to benign images with matching text prompts. Nightshade poison samples are also optimized for potency and can corrupt an Stable Diffusion SDXL prompt in &lt;100 poison samples. Ni
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;</title><link>https://arxiv.org/abs/2310.04910</link><description>&lt;p&gt;
&#20851;&#20110;&#24120;&#35782;&#25512;&#29702;&#30340;&#30693;&#35782;&#22270;&#35889;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithful Knowledge Graph Explanations for Commonsense Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#35821;&#35328;&#27169;&#22411;(LMs)&#21644;&#30693;&#35782;&#22270;&#35889;(KGs)&#24050;&#25104;&#20026;&#24120;&#35782;&#38382;&#31572;&#30740;&#31350;&#20013;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#20294;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;&#24605;&#36335;&#38142;&#35299;&#37322;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#25216;&#26415;&#30340;&#19968;&#20010;&#20027;&#35201;&#24369;&#28857;&#26159;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#24573;&#35270;&#20102;&#29983;&#25104;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631; - &#22270;&#19968;&#33268;&#24615;&#21644;&#22270;&#20445;&#30495;&#24230; - &#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;Consistent GNN (CGNN)&#65292;&#35813;&#26041;&#27861;&#28155;&#21152;&#20102;&#19968;&#39033;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;KG&#30340;&#39044;&#27979;&#32463;&#24120;&#20559;&#31163;&#21407;&#22987;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;CGNN&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#23637;&#31034;&#20102;&#23427;&#20135;&#29983;&#26356;&#21487;&#20449;&#35299;&#37322;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#26126;&#30830;&#35780;&#20272;&#35299;&#37322;&#21487;&#20449;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fusing language models (LMs) and knowledge graphs (KGs) has become common in commonsense question answering research, enabling faithful chain-of-thought explanations in these models remains an open problem. One major weakness of current KG-based explanation techniques is that they overlook the faithfulness of generated explanations during evaluation. To address this gap, we make two main contributions: (1) We propose and validate two quantitative metrics - graph consistency and graph fidelity - to measure the faithfulness of KG-based explanations. (2) We introduce Consistent GNN (CGNN), a novel training method that adds a consistency regularization term to improve explanation faithfulness. Our analysis shows that predictions from KG often diverge from original model predictions. The proposed CGNN approach boosts consistency and fidelity, demonstrating its potential for producing more faithful explanations. Our work emphasises the importance of explicitly evaluating suggest a path
&lt;/p&gt;</description></item><item><title>CoDi&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#39044;&#20808;&#35757;&#32451;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#25509;&#21463;&#39069;&#22806;&#30340;&#22270;&#20687;&#26465;&#20214;&#36755;&#20837;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26524;&#25152;&#38656;&#30340;&#37319;&#26679;&#27493;&#39588;&#12290;</title><link>https://arxiv.org/abs/2310.01407</link><description>&lt;p&gt;
CoDi: &#26465;&#20214;&#25193;&#25955;&#33976;&#39311;&#65292;&#29992;&#20110;&#26356;&#39640;&#20445;&#30495;&#24230;&#21644;&#26356;&#24555;&#30340;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01407
&lt;/p&gt;
&lt;p&gt;
CoDi&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#39044;&#20808;&#35757;&#32451;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#25509;&#21463;&#39069;&#22806;&#30340;&#22270;&#20687;&#26465;&#20214;&#36755;&#20837;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26524;&#25152;&#38656;&#30340;&#37319;&#26679;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#38761;&#26032;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#65292;&#24182;&#20026;&#20687;&#22270;&#20687;&#22686;&#24378;&#12289;&#24674;&#22797;&#12289;&#32534;&#36753;&#21644;&#21512;&#25104;&#31561;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CoDi&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#25509;&#21463;&#39069;&#22806;&#30340;&#22270;&#20687;&#26465;&#20214;&#36755;&#20837;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#38656;&#35201;&#36798;&#21040;&#39640;&#36136;&#37327;&#32467;&#26524;&#25152;&#38656;&#30340;&#37319;&#26679;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01407v2 Announce Type: replace-cross  Abstract: Large generative diffusion models have revolutionized text-to-image generation and offer immense potential for conditional generation tasks such as image enhancement, restoration, editing, and compositing. However, their widespread adoption is hindered by the high computational cost, which limits their real-time application. To address this challenge, we introduce a novel method dubbed CoDi, that adapts a pre-trained latent diffusion model to accept additional image conditioning inputs while significantly reducing the sampling steps required to achieve high-quality results. Our method can leverage architectures such as ControlNet to incorporate conditioning inputs without compromising the model's prior knowledge gained during large scale pre-training. Additionally, a conditional consistency loss enforces consistent predictions across diffusion steps, effectively compelling the model to generate high-quality images with conditio
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#29992;&#35780;&#20998;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#36890;&#36807;&#39318;&#20010;&#24320;&#28304;&#26694;&#26550;&#21644;CALM&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#20256;&#32479;&#20449;&#29992;&#35780;&#20998;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#21516;&#26102;&#35299;&#20915;&#28508;&#22312;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.00566</link><description>&lt;p&gt;
&#36171;&#33021;&#20247;&#22810;&#65292;&#20559;&#34962;&#23569;&#25968;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#36890;&#29992;&#20449;&#29992;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00566
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#29992;&#35780;&#20998;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#36890;&#36807;&#39318;&#20010;&#24320;&#28304;&#26694;&#26550;&#21644;CALM&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#20256;&#32479;&#20449;&#29992;&#35780;&#20998;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#21516;&#26102;&#35299;&#20915;&#28508;&#22312;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#34892;&#19994;&#65292;&#20449;&#29992;&#35780;&#20998;&#26159;&#19968;&#20010;&#22522;&#30784;&#35201;&#32032;&#65292;&#22609;&#36896;&#30528;&#20010;&#20154;&#21644;&#20225;&#19994;&#30340;&#20449;&#36151;&#20934;&#20837;&#65292;&#20915;&#23450;&#30528;&#36151;&#27454;&#26465;&#20214;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20449;&#29992;&#35780;&#20998;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#33021;&#22815;&#24378;&#22823;&#22320;&#36328;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#25506;&#32034;LLMs&#22312;&#20449;&#29992;&#35780;&#20998;&#20013;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#20840;&#38754;&#26694;&#26550;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#28085;&#30422;9&#20010;&#25968;&#25454;&#38598;&#12289;1.4K&#26679;&#26412;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#19987;&#38376;&#29992;&#20110;&#20449;&#29992;&#35780;&#20272;&#65292;&#24182;&#23545;LLMs&#20869;&#28508;&#22312;&#20559;&#35265;&#36827;&#34892;&#20102;&#37325;&#35201;&#26816;&#26597;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36229;&#36807;45K&#26679;&#26412;&#30340;&#26032;&#22411;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#25552;&#20986;&#20102;&#39318;&#20010;&#20449;&#36151;&#19982;&#39118;&#38505;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CALM&#65289;&#65292;&#20197;&#38024;&#23545;&#19981;&#21516;&#30340;&#24494;&#22937;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00566v3 Announce Type: replace-cross  Abstract: In the financial industry, credit scoring is a fundamental element, shaping access to credit and determining the terms of loans for individuals and businesses alike. Traditional credit scoring methods, however, often grapple with challenges such as narrow knowledge scope and isolated evaluation of credit tasks. Our work posits that Large Language Models (LLMs) have great potential for credit scoring tasks, with strong generalization ability across multiple tasks. To systematically explore LLMs for credit scoring, we propose the first open-source comprehensive framework. We curate a novel benchmark covering 9 datasets with 14K samples, tailored for credit assessment and a critical examination of potential biases within LLMs, and the novel instruction tuning data with over 45k samples. We then propose the first Credit and Risk Assessment Large Language Model (CALM) by instruction tuning, tailored to the nuanced demands of various
&lt;/p&gt;</description></item><item><title>&#25351;&#20196;&#35843;&#25972;&#23545;LLMs&#20135;&#29983;&#20102;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65307;2&#65289;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;&#30340;&#19981;&#26029;&#35843;&#25972;</title><link>https://arxiv.org/abs/2310.00492</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#24314;&#27169;&#21040;&#25351;&#20196;&#36319;&#38543;&#65306;&#29702;&#35299;&#25351;&#20196;&#35843;&#25972;&#21518;LLMs&#20013;&#34892;&#20026;&#30340;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00492
&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#23545;LLMs&#20135;&#29983;&#20102;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65307;2&#65289;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;&#30340;&#19981;&#26029;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#25351;&#20196;&#35843;&#25972;&#26159;&#23558;LLMs&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25351;&#20196;&#35843;&#25972;&#22914;&#20309;&#35843;&#25972;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#20869;&#22312;&#21464;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#20960;&#31181;&#26412;&#22320;&#21644;&#20840;&#23616;&#35299;&#37322;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#36755;&#20837;&#36755;&#20986;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#21450;&#29992;&#20110;&#35299;&#37322;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#23618;&#20013;&#30340;&#27169;&#24335;&#21644;&#27010;&#24565;&#30340;&#25216;&#26415;&#12290;&#28982;&#21518;&#36890;&#36807;&#27604;&#36739;&#20174;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#20013;&#24471;&#20986;&#30340;&#35299;&#37322;&#26469;&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20154;&#21487;&#29702;&#35299;&#30340;&#27700;&#24179;&#19978;&#25552;&#20379;&#20102;&#27169;&#22411;&#36716;&#21464;&#30340;&#20869;&#37096;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#23427;&#20351;LLMs&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65292;&#24182;&#19981;&#26029;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00492v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have achieved remarkable success, where instruction tuning is the critical step in aligning LLMs with user intentions. In this work, we investigate how the instruction tuning adjusts pre-trained models with a focus on intrinsic changes. Specifically, we first develop several local and global explanation methods, including a gradient-based method for input-output attribution and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. The impact of instruction tuning is then studied by comparing the explanations derived from the pre-trained and instruction-tuned models. This approach provides an internal perspective of the model shifts on a human-comprehensible level. Our findings reveal three significant impacts of instruction tuning: 1) It empowers LLMs to recognize the instruction parts from user prompts, and promotes the response generation constantly condition
&lt;/p&gt;</description></item><item><title>&#22330;&#26223;&#36890;&#30693;&#32773;&#26159;&#19968;&#31181;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#39044;&#27979;&#35266;&#23519;&#20195;&#29702;&#30340;&#36712;&#36857;&#21644;&#25512;&#26029;&#36974;&#25377;&#65292;&#20854;&#21033;&#29992;transformer&#32858;&#21512;&#36755;&#20837;&#27169;&#24577;&#24182;&#23454;&#29616;&#23545;&#21487;&#33021;&#19982;&#33258;&#20027;&#36710;&#36742;&#35745;&#21010;&#36335;&#24452;&#30456;&#20132;&#30340;&#36974;&#25377;&#30340;&#36873;&#25321;&#24615;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2309.13893</link><description>&lt;p&gt;
&#22330;&#26223;&#36890;&#30693;&#32773;&#65306;&#22522;&#20110;&#38170;&#28857;&#30340;&#36974;&#25377;&#25512;&#26029;&#21644;&#36712;&#36857;&#39044;&#27979;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;
&lt;/p&gt;
&lt;p&gt;
Scene Informer: Anchor-based Occlusion Inference and Trajectory Prediction in Partially Observable Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13893
&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#36890;&#30693;&#32773;&#26159;&#19968;&#31181;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#39044;&#27979;&#35266;&#23519;&#20195;&#29702;&#30340;&#36712;&#36857;&#21644;&#25512;&#26029;&#36974;&#25377;&#65292;&#20854;&#21033;&#29992;transformer&#32858;&#21512;&#36755;&#20837;&#27169;&#24577;&#24182;&#23454;&#29616;&#23545;&#21487;&#33021;&#19982;&#33258;&#20027;&#36710;&#36742;&#35745;&#21010;&#36335;&#24452;&#30456;&#20132;&#30340;&#36974;&#25377;&#30340;&#36873;&#25321;&#24615;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#33258;&#20027;&#36710;&#36742;&#65288;AVs&#65289;&#23548;&#33322;&#38656;&#35201;AVs&#25512;&#29702;&#21487;&#35265;&#21644;&#36974;&#25377;&#21306;&#22495;&#12290;&#36825;&#28041;&#21450;&#39044;&#27979;&#35266;&#23519;&#20195;&#29702;&#30340;&#26410;&#26469;&#36816;&#21160;&#65292;&#25512;&#26029;&#34987;&#36974;&#25377;&#30340;&#20195;&#29702;&#65292;&#24182;&#26681;&#25454;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#30340;&#30690;&#37327;&#21270;&#22330;&#26223;&#34920;&#31034;&#24314;&#27169;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36974;&#25377;&#25512;&#26029;&#21644;&#36712;&#36857;&#39044;&#27979;&#26041;&#38754;&#30340;&#20808;&#21069;&#24037;&#20316;&#26159;&#20998;&#21035;&#21457;&#23637;&#30340;&#65292;&#21069;&#32773;&#22522;&#20110;&#31616;&#21270;&#30340;&#20809;&#26629;&#26041;&#27861;&#65292;&#21518;&#32773;&#20551;&#23450;&#23436;&#25972;&#30340;&#29615;&#22659;&#21487;&#35266;&#23519;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22330;&#26223;&#36890;&#30693;&#32773;&#65292;&#36825;&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#35266;&#23519;&#20195;&#29702;&#30340;&#36712;&#36857;&#21644;&#25512;&#26029;&#36974;&#25377;&#12290;&#23427;&#20351;&#29992;&#19968;&#20010;transformer&#26469;&#32858;&#21512;&#21508;&#31181;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#20419;&#36827;&#23545;&#21487;&#33021;&#19982;AV&#35745;&#21010;&#36335;&#24452;&#30456;&#20132;&#30340;&#36974;&#25377;&#30340;&#36873;&#25321;&#24615;&#26597;&#35810;&#12290;&#35813;&#26694;&#26550;&#20272;&#35745;&#20102;&#36974;&#25377;&#27010;&#29575;&#21644;&#21487;&#33021;&#30340;&#36974;&#25377;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13893v2 Announce Type: replace-cross  Abstract: Navigating complex and dynamic environments requires autonomous vehicles (AVs) to reason about both visible and occluded regions. This involves predicting the future motion of observed agents, inferring occluded ones, and modeling their interactions based on vectorized scene representations of the partially observable environment. However, prior work on occlusion inference and trajectory prediction have developed in isolation, with the former based on simplified rasterized methods and the latter assuming full environment observability. We introduce the Scene Informer, a unified approach for predicting both observed agent trajectories and inferring occlusions in a partially observable setting. It uses a transformer to aggregate various input modalities and facilitate selective queries on occlusions that might intersect with the AV's planned path. The framework estimates occupancy probabilities and likely trajectories for occlusi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;ChatEDA&#65292;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;AutoMage&#36171;&#33021;&#30340;EDA&#33258;&#20027;&#20195;&#29702;&#65292;&#36890;&#36807;&#26377;&#25928;&#31649;&#29702;&#20219;&#21153;&#35745;&#21010;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#20219;&#21153;&#25191;&#34892;&#65292;&#31616;&#21270;&#20102;&#20174;RTL&#21040;GDSII&#30340;&#35774;&#35745;&#27969;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2308.10204</link><description>&lt;p&gt;
ChatEDA&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#29992;&#20110;EDA
&lt;/p&gt;
&lt;p&gt;
ChatEDA: A Large Language Model Powered Autonomous Agent for EDA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;ChatEDA&#65292;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;AutoMage&#36171;&#33021;&#30340;EDA&#33258;&#20027;&#20195;&#29702;&#65292;&#36890;&#36807;&#26377;&#25928;&#31649;&#29702;&#20219;&#21153;&#35745;&#21010;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#20219;&#21153;&#25191;&#34892;&#65292;&#31616;&#21270;&#20102;&#20174;RTL&#21040;GDSII&#30340;&#35774;&#35745;&#27969;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10204v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;&#25688;&#35201;&#65306;&#38598;&#25104;&#19968;&#31995;&#21015;&#22797;&#26434;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#24037;&#20855;&#20197;&#22686;&#24378;&#20114;&#25805;&#20316;&#24615;&#26159;&#30005;&#36335;&#35774;&#35745;&#32773;&#20851;&#27880;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19982;EDA&#24037;&#20855;&#25509;&#21475;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#20171;&#32461;&#20102;ChatEDA&#65292;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;AutoMage&#36171;&#33021;&#30340;EDA&#33258;&#20027;&#20195;&#29702;&#65292;&#32467;&#21512;&#20316;&#20026;&#25191;&#34892;&#22120;&#30340;EDA&#24037;&#20855;&#12290;ChatEDA&#36890;&#36807;&#26377;&#25928;&#31649;&#29702;&#20219;&#21153;&#35745;&#21010;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#20219;&#21153;&#25191;&#34892;&#65292;&#31616;&#21270;&#20102;&#20174;&#23492;&#23384;&#22120;&#20256;&#36755;&#32423;&#65288;RTL&#65289;&#21040;&#22270;&#24418;&#25968;&#25454;&#31995;&#32479;&#31532;&#20108;&#29256;&#65288;GDSII&#65289;&#30340;&#35774;&#35745;&#27969;&#31243;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;ChatEDA&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22788;&#29702;&#21508;&#31181;&#38656;&#27714;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#32463;&#36807;&#31934;&#24515;&#35843;&#20248;&#30340;AutoMage&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#30456;&#36739;&#20110;GPT-4&#21644;&#20854;&#20182;&#31867;&#20284;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10204v2 Announce Type: replace-cross  Abstract: The integration of a complex set of Electronic Design Automation (EDA) tools to enhance interoperability is a critical concern for circuit designers. Recent advancements in large language models (LLMs) have showcased their exceptional capabilities in natural language processing and comprehension, offering a novel approach to interfacing with EDA tools. This research paper introduces ChatEDA, an autonomous agent for EDA empowered by a large language model, AutoMage, complemented by EDA tools serving as executors. ChatEDA streamlines the design flow from the Register-Transfer Level (RTL) to the Graphic Data System Version II (GDSII) by effectively managing task planning, script generation, and task execution. Through comprehensive experimental evaluations, ChatEDA has demonstrated its proficiency in handling diverse requirements, and our fine-tuned AutoMage model has exhibited superior performance compared to GPT-4 and other simi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;Git&#25552;&#20132;&#30340;&#33258;&#28982;&#32467;&#26500;&#65292;&#23558;&#20195;&#30721;&#26356;&#25913;&#19982;&#20154;&#31867;&#25351;&#20196;&#37197;&#23545;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OctoPack&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2308.07124</link><description>&lt;p&gt;
OctoPack: &#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#20195;&#30721;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OctoPack: Instruction Tuning Code Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07124
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;Git&#25552;&#20132;&#30340;&#33258;&#28982;&#32467;&#26500;&#65292;&#23558;&#20195;&#30721;&#26356;&#25913;&#19982;&#20154;&#31867;&#25351;&#20196;&#37197;&#23545;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OctoPack&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25351;&#20196;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24494;&#35843;&#21487;&#26174;&#33879;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24212;&#29992;&#20195;&#30721;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#21033;&#29992;Git&#25552;&#20132;&#30340;&#33258;&#28982;&#32467;&#26500;&#65292;&#23558;&#20195;&#30721;&#26356;&#25913;&#19982;&#20154;&#31867;&#25351;&#20196;&#37197;&#23545;&#12290;&#25105;&#20204;&#32534;&#35793;&#20102;CommitPack&#65306;&#36328;350&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;4TB Git&#25552;&#20132;&#12290;&#25105;&#20204;&#22312;&#25317;&#26377;16B&#21442;&#25968;&#30340;StarCoder&#27169;&#22411;&#19978;&#23545;&#27604;CommitPack&#21644;&#20854;&#20182;&#33258;&#28982;&#19982;&#21512;&#25104;&#20195;&#30721;&#25351;&#20196;&#65288;xP3x, Self-Instruct, OASST&#65289;&#65292;&#22312;HumanEval Python&#22522;&#20934;&#27979;&#35797;&#65288;46.2% pass@1&#65289;&#20013;&#21462;&#24471;&#20102;&#26410;&#22312;OpenAI&#36755;&#20986;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#20986;HumanEvalPack&#65292;&#23558;HumanEval&#22522;&#20934;&#27979;&#35797;&#25193;&#23637;&#21040;&#20849;&#35745;3&#20010;&#32534;&#30721;&#20219;&#21153;&#65288;&#20195;&#30721;&#20462;&#22797;&#12289;&#20195;&#30721;&#35299;&#37322;&#12289;&#20195;&#30721;&#21512;&#25104;&#65289;&#36328;6&#31181;&#35821;&#35328;&#65288;Python&#12289;JavaScript&#12289;Java&#12289;Go&#12289;C ++&#12289;Rust&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;OctoCoder&#21644;OctoGeeX&#22312;HumanEvalPack&#20013;&#21462;&#24471;&#20102;&#25152;&#26377;&#35768;&#21487;&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07124v2 Announce Type: replace-cross  Abstract: Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive m
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#27169;&#22359;&#21270;&#30340;&#21327;&#20316;&#20307;&#29616;&#26234;&#33021;&#20307;&#65292;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36229;&#36234;&#35268;&#21010;&#26041;&#27861;&#24182;&#23637;&#31034;&#26377;&#25928;&#27807;&#36890;&#12290;</title><link>https://arxiv.org/abs/2307.02485</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#22359;&#21270;&#22320;&#26500;&#24314;&#21327;&#20316;&#20307;&#29616;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Building Cooperative Embodied Agents Modularly with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.02485
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#27169;&#22359;&#21270;&#30340;&#21327;&#20316;&#20307;&#29616;&#26234;&#33021;&#20307;&#65292;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36229;&#36234;&#35268;&#21010;&#26041;&#27861;&#24182;&#23637;&#31034;&#26377;&#25928;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22788;&#29702;&#20855;&#26377;&#21435;&#20013;&#24515;&#21270;&#25511;&#21046;&#12289;&#21407;&#22987;&#24863;&#30693;&#35266;&#23519;&#12289;&#26114;&#36149;&#36890;&#35759;&#21644;&#22810;&#30446;&#26631;&#20219;&#21153;&#30340;&#20855;&#26377;&#21508;&#31181;&#20307;&#29616;&#29615;&#22659;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35782;&#30693;&#35782;&#12289;&#25512;&#29702;&#33021;&#21147;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#23558;&#23427;&#20204;&#26080;&#32541;&#22320;&#34701;&#20837;&#21040;&#19968;&#20010;&#19982;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25191;&#34892;&#30456;&#32467;&#21512;&#30340;&#35748;&#30693;&#21551;&#21457;&#24335;&#27169;&#22359;&#21270;&#26694;&#26550;&#20013;&#12290;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#20197;&#35268;&#21010;&#12289;&#27807;&#36890;&#21644;&#19982;&#20854;&#20182;&#20154;&#21512;&#20316;&#20197;&#39640;&#25928;&#23436;&#25104;&#38271;&#26102;&#31243;&#20219;&#21153;&#30340;&#21512;&#20316;&#20307;&#29616;&#26234;&#33021;&#20307;CoELA&#12290;&#25105;&#20204;&#22312;C-WAH&#21644;TDW-MAT&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30001;GPT-4&#39537;&#21160;&#30340;CoELA&#21487;&#20197;&#36229;&#36234;&#24378;&#22823;&#30340;&#22522;&#20110;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20986;&#26032;&#20852;&#30340;&#26377;&#25928;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.02485v2 Announce Type: replace  Abstract: In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current O
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#26426;&#22120;&#20154;&#31215;&#26497;&#24863;&#30693;&#29615;&#22659;&#65292;&#36827;&#34892;&#22522;&#20110;&#24120;&#35782;&#30340;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2306.08651</link><description>&lt;p&gt;
&#36808;&#21521;&#22522;&#20110;&#24120;&#35782;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Toward Grounded Commonsense Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.08651
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#26426;&#22120;&#20154;&#31215;&#26497;&#24863;&#30693;&#29615;&#22659;&#65292;&#36827;&#34892;&#22522;&#20110;&#24120;&#35782;&#30340;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#19968;&#20010;&#34987;&#20132;&#20184;&#25972;&#29702;&#26700;&#23376;&#19978;&#31934;&#24515;&#26500;&#24314;&#30340;&#20048;&#39640;&#36305;&#36710;&#30340;&#26426;&#22120;&#20154;&#12290;&#20154;&#31867;&#21487;&#33021;&#35748;&#35782;&#21040;&#65292;&#25286;&#24320;&#36305;&#36710;&#24182;&#23558;&#20854;&#25918;&#22238;&#20316;&#20026;&#8220;&#25972;&#29702;&#8221;&#30340;&#19968;&#37096;&#20998;&#26159;&#19981;&#21512;&#36866;&#30340;&#12290;&#19968;&#20010;&#26426;&#22120;&#20154;&#22914;&#20309;&#24471;&#20986;&#36825;&#20010;&#32467;&#35770;&#21602;&#65311;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26368;&#36817;&#34987;&#29992;&#20110;&#23454;&#29616;&#24120;&#35782;&#25512;&#29702;&#65292;&#20294;&#23558;&#36825;&#31181;&#25512;&#29702;&#33853;&#23454;&#21040;&#29616;&#23454;&#19990;&#30028;&#20013;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#25512;&#29702;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#36229;&#36234;&#34987;&#21160;&#26597;&#35810;LLMs&#65292;&#31215;&#26497;&#22320;&#20174;&#29615;&#22659;&#20013;&#25910;&#38598;&#24517;&#35201;&#30340;&#20449;&#24687;&#26469;&#20570;&#20986;&#27491;&#30830;&#30340;&#20915;&#31574;&#12290;&#20363;&#22914;&#65292;&#22312;&#26816;&#27979;&#21040;&#26377;&#19968;&#20010;&#34987;&#36974;&#25377;&#30340;&#27773;&#36710;&#21518;&#65292;&#26426;&#22120;&#20154;&#21487;&#33021;&#38656;&#35201;&#20027;&#21160;&#24863;&#30693;&#27773;&#36710;&#65292;&#20197;&#20102;&#35299;&#23427;&#26159;&#30001;&#20048;&#39640;&#21046;&#20316;&#30340;&#39640;&#32423;&#22411;&#21495;&#27773;&#36710;&#65292;&#36824;&#26159;&#30001;&#24188;&#20799;&#21046;&#20316;&#30340;&#29609;&#20855;&#27773;&#36710;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#20010;LLM&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLM)&#26469;&#24110;&#21161;&#26426;&#22120;&#20154;&#20027;&#21160;&#24863;&#30693;&#20854;&#29615;&#22659;&#20197;&#36827;&#34892;&#22522;&#20110;&#20107;&#23454;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.08651v2 Announce Type: replace-cross  Abstract: Consider a robot tasked with tidying a desk with a meticulously constructed Lego sports car. A human may recognize that it is not appropriate to disassemble the sports car and put it away as part of the "tidying." How can a robot reach that conclusion? Although large language models (LLMs) have recently been used to enable commonsense reasoning, grounding this reasoning in the real world has been challenging. To reason in the real world, robots must go beyond passively querying LLMs and actively gather information from the environment that is required to make the right decision. For instance, after detecting that there is an occluded car, the robot may need to actively perceive the car to know whether it is an advanced model car made out of Legos or a toy car built by a toddler. We propose an approach that leverages an LLM and vision language model (VLM) to help a robot actively perceive its environment to perform grounded comm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#37319;&#26679;&#21644;Krylov&#23376;&#31354;&#38388;&#26041;&#27861;&#21327;&#21516;&#32467;&#21512;&#30340;&#26032;&#22411;&#39640;&#25928;&#37319;&#26679;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2303.05754</link><description>&lt;p&gt;
&#20998;&#35299;&#25193;&#25955;&#37319;&#26679;&#22120;&#29992;&#20110;&#21152;&#36895;&#22823;&#35268;&#27169;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.05754
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#37319;&#26679;&#21644;Krylov&#23376;&#31354;&#38388;&#26041;&#27861;&#21327;&#21516;&#32467;&#21512;&#30340;&#26032;&#22411;&#39640;&#25928;&#37319;&#26679;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Krylov&#23376;&#31354;&#38388;&#26159;&#36890;&#36807;&#23558;&#32473;&#23450;&#21521;&#37327;&#19982;&#32447;&#24615;&#21464;&#25442;&#30697;&#38453;&#21450;&#20854;&#36830;&#32493;&#24130;&#30456;&#20056;&#32780;&#29983;&#25104;&#30340;&#65292;&#24191;&#27867;&#30740;&#31350;&#30340;&#32463;&#20856;&#20248;&#21270;&#25991;&#29486;&#20013;&#21033;&#29992;Krylov&#23376;&#31354;&#38388;&#35774;&#35745;&#31639;&#27861;&#20197;&#24555;&#36895;&#25910;&#25947;&#22823;&#35268;&#27169;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#25193;&#25955;&#37319;&#26679;&#31574;&#30053;&#65292;&#23558;&#25193;&#25955;&#37319;&#26679;&#19982;Krylov&#23376;&#31354;&#38388;&#26041;&#27861;&#21327;&#21516;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.05754v3 Announce Type: replace-cross  Abstract: Krylov subspace, which is generated by multiplying a given vector by the matrix of a linear transformation and its successive powers, has been extensively studied in classical optimization literature to design algorithms that converge quickly for large linear inverse problems. For example, the conjugate gradient method (CG), one of the most popular Krylov subspace methods, is based on the idea of minimizing the residual error in the Krylov subspace. However, with the recent advancement of high-performance diffusion solvers for inverse problems, it is not clear how classical wisdom can be synergistically combined with modern diffusion models. In this study, we propose a novel and efficient diffusion sampling strategy that synergistically combines the diffusion sampling and Krylov subspace methods. Specifically, we prove that if the tangent space at a denoised sample by Tweedie's formula forms a Krylov subspace, then the CG initi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#20998;&#24067;&#24335;&#37327;&#21270;&#27969;&#30340;GFlowNets&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27969;&#20989;&#25968;&#36716;&#21270;&#20026;&#20998;&#24067;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#36890;&#36807;&#37327;&#21270;&#20989;&#25968;&#21442;&#25968;&#21270;&#27599;&#20010;&#36793;&#27969;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#23398;&#20064;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#23545;&#39118;&#38505;&#19981;&#30830;&#23450;&#24615;&#22330;&#26223;&#30340;&#22788;&#29702;&#65292;&#24182;&#22312;&#29616;&#26377;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2302.05793</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#24067;&#24335;&#37327;&#21270;&#27969;&#30340;GFlowNets
&lt;/p&gt;
&lt;p&gt;
Distributional GFlowNets with Quantile Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#20998;&#24067;&#24335;&#37327;&#21270;&#27969;&#30340;GFlowNets&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27969;&#20989;&#25968;&#36716;&#21270;&#20026;&#20998;&#24067;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#36890;&#36807;&#37327;&#21270;&#20989;&#25968;&#21442;&#25968;&#21270;&#27599;&#20010;&#36793;&#27969;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#23398;&#20064;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#23545;&#39118;&#38505;&#19981;&#30830;&#23450;&#24615;&#22330;&#26223;&#30340;&#22788;&#29702;&#65292;&#24182;&#22312;&#29616;&#26377;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#37319;&#26679;&#22120;&#31995;&#21015;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#19968;&#31995;&#21015;&#20915;&#31574;&#27493;&#39588;&#23398;&#20064;&#29983;&#25104;&#22797;&#26434;&#32452;&#21512;&#32467;&#26500;&#30340;&#38543;&#26426;&#31574;&#30053;&#12290;&#23613;&#31649;&#21463;&#24378;&#21270;&#23398;&#20064;&#21551;&#21457;&#65292;&#24403;&#21069;&#30340;GFlowNet&#26694;&#26550;&#22312;&#36866;&#29992;&#24615;&#19978;&#30456;&#23545;&#26377;&#38480;&#65292;&#26080;&#27861;&#22788;&#29702;&#22870;&#21169;&#20989;&#25968;&#20013;&#30340;&#38543;&#26426;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#24335;&#33539;&#24335;&#26469;&#22788;&#29702;GFlowNets&#65292;&#23558;&#27599;&#20010;&#27969;&#20989;&#25968;&#36716;&#21270;&#20026;&#19968;&#20010;&#20998;&#24067;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#36890;&#36807;&#36890;&#36807;&#37327;&#21270;&#20989;&#25968;&#23545;&#27599;&#20010;&#36793;&#27969;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#37327;&#21270;&#21305;&#37197;&#8221; GFlowNet&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#65292;&#36825;&#26159;&#22788;&#29702;&#39118;&#38505;&#19981;&#30830;&#23450;&#24615;&#22330;&#26223;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#30001;&#20110;&#25105;&#20204;&#22686;&#24378;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#26377;&#22522;&#20934;&#19978;&#23454;&#29616;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) are a new family of probabilistic samplers where an agent learns a stochastic policy for generating complex combinatorial structure through a series of decision-making steps. Despite being inspired from reinforcement learning, the current GFlowNet framework is relatively limited in its applicability and cannot handle stochasticity in the reward function. In this work, we adopt a distributional paradigm for GFlowNets, turning each flow function into a distribution, thus providing more informative learning signals during training. By parameterizing each edge flow through their quantile functions, our proposed \textit{quantile matching} GFlowNet learning algorithm is able to learn a risk-sensitive policy, an essential component for handling scenarios with risk uncertainty. Moreover, we find that the distributional approach can achieve substantial improvement on existing benchmarks compared to prior methods due to our enhanced training algorithm, even i
&lt;/p&gt;</description></item><item><title>&#22312;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#23547;&#25214;&#20855;&#26377;&#26368;&#23567;&#21463;&#24433;&#21709;&#33410;&#28857;&#25968;&#37327;&#30340;&#31995;&#32479;&#38750;&#24179;&#20961;&#19981;&#21160;&#28857;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#19979;&#21487;&#20197;&#39640;&#25928;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2301.04090</link><description>&lt;p&gt;
&#22312;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#20013;&#25214;&#21040;&#38750;&#24179;&#20961;&#30340;&#26368;&#23567;&#19981;&#21160;&#28857;
&lt;/p&gt;
&lt;p&gt;
Finding Nontrivial Minimum Fixed Points in Discrete Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.04090
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#23547;&#25214;&#20855;&#26377;&#26368;&#23567;&#21463;&#24433;&#21709;&#33410;&#28857;&#25968;&#37327;&#30340;&#31995;&#32479;&#38750;&#24179;&#20961;&#19981;&#21160;&#28857;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#19979;&#21487;&#20197;&#39640;&#25928;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#24120;&#29992;&#20110;&#27169;&#25311;&#20256;&#26579;&#30149;&#30340;&#20256;&#25773;&#21644;&#20915;&#31574;&#21327;&#35843;&#28216;&#25103;&#20013;&#30340;&#20195;&#29702;&#12290;&#36825;&#31181;&#21160;&#21147;&#31995;&#32479;&#30340;&#19981;&#21160;&#28857;&#20195;&#34920;&#31995;&#32479;&#25910;&#25947;&#21040;&#30340;&#37197;&#32622;&#12290;&#22312;&#20256;&#25773;&#19981;&#33391;&#20256;&#26579;&#30149;&#65288;&#22914;&#35875;&#35328;&#21644;&#38169;&#35823;&#20449;&#24687;&#65289;&#26041;&#38754;&#65292;&#25910;&#25947;&#21040;&#21463;&#24433;&#21709;&#33410;&#28857;&#25968;&#37327;&#36739;&#23569;&#30340;&#19981;&#21160;&#28857;&#26159;&#19968;&#20010;&#20540;&#24471;&#36861;&#27714;&#30340;&#30446;&#26631;&#12290;&#21463;&#36825;&#20123;&#32771;&#34385;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#23547;&#25214;&#20855;&#26377;&#26368;&#23567;&#21463;&#24433;&#21709;&#33410;&#28857;&#25968;&#37327;&#30340;&#31995;&#32479;&#38750;&#24179;&#20961;&#19981;&#21160;&#28857;&#12290;&#25105;&#20204;&#30830;&#23450;&#65292;&#38500;&#38750;P = NP&#65292;&#21542;&#21017;&#27809;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#21487;&#20197;&#22312;&#20219;&#24847;&#23567;&#20110; n^1-\epsilon &#30340;&#22240;&#23376;&#20869;&#36817;&#20284;&#27714;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#35745;&#31639;&#19978;&#30340;&#38590;&#39064;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.04090v4 Announce Type: replace-cross  Abstract: Networked discrete dynamical systems are often used to model the spread of contagions and decision-making by agents in coordination games. Fixed points of such dynamical systems represent configurations to which the system converges. In the dissemination of undesirable contagions (such as rumors and misinformation), convergence to fixed points with a small number of affected nodes is a desirable goal. Motivated by such considerations, we formulate a novel optimization problem of finding a nontrivial fixed point of the system with the minimum number of affected nodes. We establish that, unless P = NP, there is no polynomial time algorithm for approximating a solution to this problem to within the factor n^1-\epsilon for any constant epsilon &gt; 0. To cope with this computational intractability, we identify several special cases for which the problem can be solved efficiently. Further, we introduce an integer linear program to addr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#37197;&#32622;&#31354;&#38388;&#65288;VCS&#65289;&#30340;&#26367;&#20195;&#24615;&#21051;&#30011;&#65292;&#20351;&#20855;&#26377;&#36523;&#20307;&#32467;&#26500;&#30340;&#23454;&#20307;&#20195;&#29702;&#33021;&#22815;&#20351;&#29992;&#33258;&#24049;&#30340;&#22270;&#20687;&#38598;&#22312;&#38543;&#26426;&#23039;&#21183;&#19979;&#21457;&#29616;&#33258;&#24049;&#30340;&#36523;&#20307;&#32467;&#26500;&#65292;&#24182;&#22312;&#21608;&#36793;&#31354;&#38388;&#20013;&#35268;&#21010;&#26080;&#38556;&#30861;&#36816;&#21160;&#12290;</title><link>https://arxiv.org/abs/2210.04047</link><description>&lt;p&gt;
&#22312;&#35270;&#35273;&#27969;&#24418;&#19978;&#30340;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Motion Planning on Visual Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.04047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#37197;&#32622;&#31354;&#38388;&#65288;VCS&#65289;&#30340;&#26367;&#20195;&#24615;&#21051;&#30011;&#65292;&#20351;&#20855;&#26377;&#36523;&#20307;&#32467;&#26500;&#30340;&#23454;&#20307;&#20195;&#29702;&#33021;&#22815;&#20351;&#29992;&#33258;&#24049;&#30340;&#22270;&#20687;&#38598;&#22312;&#38543;&#26426;&#23039;&#21183;&#19979;&#21457;&#29616;&#33258;&#24049;&#30340;&#36523;&#20307;&#32467;&#26500;&#65292;&#24182;&#22312;&#21608;&#36793;&#31354;&#38388;&#20013;&#35268;&#21010;&#26080;&#38556;&#30861;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#37197;&#32622;&#31354;&#38388;&#27010;&#24565;&#30340;&#26367;&#20195;&#24615;&#21051;&#30011;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#35270;&#35273;&#37197;&#32622;&#31354;&#38388;&#65288;VCS&#65289;&#12290;&#36825;&#31181;&#26032;&#30340;&#25551;&#36848;&#20801;&#35768;&#19968;&#20010;&#20855;&#26377;&#36523;&#20307;&#32467;&#26500;&#30340;&#23454;&#20307;&#20195;&#29702;&#65288;&#22914;&#26426;&#22120;&#20154;&#65289;&#20351;&#29992;&#20854;&#38543;&#26426;&#23039;&#21183;&#30340;&#19968;&#32452;&#22270;&#20687;&#26469;&#21457;&#29616;&#33258;&#24049;&#30340;&#36523;&#20307;&#32467;&#26500;&#65292;&#24182;&#22312;&#20854;&#21608;&#36793;&#31354;&#38388;&#20013;&#35268;&#21010;&#26080;&#38556;&#30861;&#36816;&#21160;&#12290;&#36825;&#37324;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#23454;&#20307;&#20195;&#29702;&#12289;&#38556;&#30861;&#29289;&#25110;&#29615;&#22659;&#30340;&#20960;&#20309;&#30693;&#35782;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;VCS &#22312;&#65288;a&#65289;&#24314;&#31435;&#21644;&#22788;&#29702;&#20960;&#20309;&#26080;&#20851;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#65292;&#65288;b&#65289;&#35299;&#37322;&#20154;&#31867;&#23156;&#20799;&#22914;&#20309;&#36890;&#36807;&#36816;&#21160;&#32993;&#35328;&#26469;&#23398;&#20064;&#25235;&#21462;&#20854;&#21608;&#36793;&#31354;&#38388;&#20013;&#30340;&#29289;&#20307;&#65292;&#20197;&#21450;&#65288;c&#65289;&#33258;&#21160;&#29983;&#25104;&#34394;&#25311;&#29615;&#22659;&#20013;&#25968;&#23383;&#21270;&#21270;&#36523;&#20154;&#29289;&#30340;&#33258;&#28982;&#22836;&#37096;&#21160;&#20316;&#21160;&#30011;&#30340;&#23454;&#29992;&#24615;&#12290;&#36825;&#39033;&#24037;&#20316;&#22522;&#20110;&#27969;&#24418;&#30340;&#24418;&#24335;&#21270;&#21644;&#27969;&#24418;&#23398;&#20064;&#65292;&#20351;&#29992;&#20195;&#29702;&#30340;&#22270;&#20687;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#22312;&#35270;&#35273;&#27969;&#24418;&#19978;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.04047v2 Announce Type: replace-cross  Abstract: In this thesis, we propose an alternative characterization of the notion of Configuration Space, which we call Visual Configuration Space (VCS). This new characterization allows an embodied agent (e.g., a robot) to discover its own body structure and plan obstacle-free motions in its peripersonal space using a set of its own images in random poses. Here, we do not assume any knowledge of geometry of the agent, obstacles or the environment. We demonstrate the usefulness of VCS in (a) building and working with geometry-free models for robot motion planning, (b) explaining how a human baby might learn to reach objects in its peripersonal space through motor babbling, and (c) automatically generating natural looking head motion animations for digital avatars in virtual environments. This work is based on the formalism of manifolds and manifold learning using the agent's images and hence we call it Motion Planning on Visual Manifold
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#21453;&#20107;&#23454;&#39118;&#38505;&#26368;&#23567;&#21270;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#25209;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#24050;&#35760;&#24405;&#25968;&#25454;&#20013;&#21453;&#39304;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#26469;&#22788;&#29702;&#36825;&#31181;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2209.07148</link><description>&lt;p&gt;
&#20174;&#24050;&#35760;&#24405;&#25968;&#25454;&#20013;&#36827;&#34892;&#21322;&#30417;&#30563;&#25209;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Batch Learning From Logged Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.07148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#21453;&#20107;&#23454;&#39118;&#38505;&#26368;&#23567;&#21270;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#25209;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#24050;&#35760;&#24405;&#25968;&#25454;&#20013;&#21453;&#39304;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#26469;&#22788;&#29702;&#36825;&#31181;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#20174;&#24050;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#25968;&#25454;&#21253;&#25324;&#27599;&#20010;&#26679;&#26412;&#28857;&#30340;&#29615;&#22659;&#12289;&#21160;&#20316;&#21644;&#21453;&#39304;&#65288;&#25104;&#26412;&#25110;&#22870;&#21169;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#21453;&#20107;&#23454;&#39118;&#38505;&#26368;&#23567;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36824;&#20551;&#35774;&#33021;&#22815;&#35775;&#38382;&#27010;&#29575;&#24471;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#19968;&#20123;&#26679;&#26412;&#32570;&#22833;&#21453;&#39304;&#30340;&#38382;&#39064;&#25552;&#20986;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#22240;&#27492;&#22312;&#24050;&#35760;&#24405;&#25968;&#25454;&#20013;&#26377;&#20123;&#26679;&#26412;&#26377;&#21453;&#39304;&#65292;&#26377;&#20123;&#26679;&#26412;&#32570;&#22833;&#21453;&#39304;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#31867;&#22411;&#30340;&#23398;&#20064;&#31216;&#20026;&#20174;&#24050;&#35760;&#24405;&#25968;&#25454;&#20013;&#30340;&#21322;&#30417;&#30563;&#25209;&#37327;&#23398;&#20064;&#65292;&#36825;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#20986;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#30495;&#23454;&#39118;&#38505;&#30340;&#26032;&#19978;&#30028;&#65292;&#37319;&#29992;&#20498;&#25968;&#27010;&#29575;&#24471;&#20998;&#20272;&#35745;&#22120;&#12290;&#21033;&#29992;&#36825;&#20010;&#19978;&#30028;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#24050;&#35760;&#24405;&#25968;&#25454;&#30340;&#27491;&#21017;&#21270;&#21322;&#30417;&#30563;&#25209;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#27491;&#21017;&#21270;&#39033;&#19982;&#21453;&#39304;&#26080;&#20851;&#65292;&#32467;&#26524;&#21487;&#20197;&#20351;&#29992;&#24050;&#35760;&#24405;&#30340;&#32570;&#22833;&#21453;&#39304;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.07148v3 Announce Type: replace-cross  Abstract: Off-policy learning methods are intended to learn a policy from logged data, which includes context, action, and feedback (cost or reward) for each sample point. In this work, we build on the counterfactual risk minimization framework, which also assumes access to propensity scores. We propose learning methods for problems where feedback is missing for some samples, so there are samples with feedback and samples missing-feedback in the logged data. We refer to this type of learning as semi-supervised batch learning from logged data, which arises in a wide range of application domains. We derive a novel upper bound for the true risk under the inverse propensity score estimator to address this kind of learning problem. Using this bound, we propose a regularized semi-supervised batch learning method with logged data where the regularization term is feedback-independent and, as a result, can be evaluated using the logged missing-fe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#20027;&#24335;MARL&#65288;SPMARL&#65289;&#20197;&#35299;&#20915;&#24403;&#21069;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#35838;&#31243;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20248;&#20808;&#32771;&#34385;&#22522;&#20110;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#12290;</title><link>https://arxiv.org/abs/2205.10016</link><description>&lt;p&gt;
&#23398;&#20064;&#36827;&#24230;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Progress Driven Multi-Agent Curriculum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.10016
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#20027;&#24335;MARL&#65288;SPMARL&#65289;&#20197;&#35299;&#20915;&#24403;&#21069;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#35838;&#31243;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20248;&#20808;&#32771;&#34385;&#22522;&#20110;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#26088;&#22312;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#30340;&#38590;&#24230;&#65288;&#36890;&#24120;&#30001;&#21487;&#23454;&#29616;&#30340;&#39044;&#26399;&#22238;&#25253;&#37327;&#21270;&#65289;&#26469;&#21152;&#24555;&#23398;&#20064;&#36895;&#24230;&#12290;&#21463;CRL&#22312;&#21333;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#23558;CRL&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#65292;&#20351;&#29992;&#26234;&#33021;&#20307;&#25968;&#37327;&#26469;&#25511;&#21046;&#20219;&#21153;&#38590;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20351;&#29992;&#25163;&#21160;&#23450;&#20041;&#30340;&#35838;&#31243;&#65292;&#22914;&#32447;&#24615;&#26041;&#26696;&#12290;&#26412;&#25991;&#39318;&#20808;&#23558;&#26368;&#20808;&#36827;&#30340;&#21333;&#26234;&#33021;&#20307;&#33258;&#20027;&#24335;CRL&#24212;&#29992;&#20110;&#31232;&#30095;&#22870;&#21169;MARL&#12290;&#34429;&#28982;&#34920;&#29616;&#20196;&#20154;&#28385;&#24847;&#65292;&#20294;&#25105;&#20204;&#30830;&#23450;&#20102;&#29616;&#26377;&#22522;&#20110;&#22870;&#21169;&#30340;CRL&#26041;&#27861;&#29983;&#25104;&#30340;&#35838;&#31243;&#23384;&#22312;&#20004;&#20010;&#28508;&#22312;&#32570;&#38519;&#65306;&#65288;1&#65289;&#39640;&#22238;&#25253;&#30340;&#20219;&#21153;&#21487;&#33021;&#19981;&#25552;&#20379;&#20449;&#24687;&#37327;&#22823;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#65288;2&#65289;&#22312;&#22810;&#26234;&#33021;&#20307;&#20135;&#29983;&#26356;&#39640;&#22238;&#25253;&#30340;&#20219;&#21153;&#20013;&#65292;&#21152;&#21095;&#20102;&#23398;&#20998;&#20998;&#37197;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#33258;&#20027;&#24335;MARL&#65288;SPMARL&#65289;&#65292;&#20197;&#22522;&#20110;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#36827;&#34892;&#23433;&#25490;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.10016v2 Announce Type: replace  Abstract: Curriculum reinforcement learning (CRL) aims to speed up learning by gradually increasing the difficulty of a task, usually quantified by the achievable expected return. Inspired by the success of CRL in single-agent settings, a few works have attempted to apply CRL to multi-agent reinforcement learning (MARL) using the number of agents to control task difficulty. However, existing works typically use manually defined curricula such as a linear scheme. In this paper, we first apply state-of-the-art single-agent self-paced CRL to sparse reward MARL. Although with satisfying performance, we identify two potential flaws of the curriculum generated by existing reward-based CRL methods: (1) tasks with high returns may not provide informative learning signals and (2) the exacerbated credit assignment difficulty in tasks where more agents yield higher returns. Thereby, we further propose self-paced MARL (SPMARL) to prioritize tasks based on
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#32463;&#39564;&#22238;&#25918;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#36873;&#25321;&#24615;&#37325;&#22797;&#21033;&#29992;&#30456;&#20851;&#26679;&#26412;&#26469;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;PG-VRER&#12290;</title><link>https://arxiv.org/abs/2110.08902</link><description>&lt;p&gt;
&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#32463;&#39564;&#22238;&#25918;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Variance Reduction Based Experience Replay for Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.08902
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#32463;&#39564;&#22238;&#25918;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#36873;&#25321;&#24615;&#37325;&#22797;&#21033;&#29992;&#30456;&#20851;&#26679;&#26412;&#26469;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;PG-VRER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#38543;&#26426;&#31995;&#32479;&#19978;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26102;&#65292;&#26377;&#25928;&#21033;&#29992;&#21382;&#21490;&#26679;&#26412;&#20013;&#30340;&#20449;&#24687;&#20197;&#21152;&#36895;&#31574;&#30053;&#20248;&#21270;&#26159;&#24456;&#26377;&#24517;&#35201;&#30340;&#12290;&#20256;&#32479;&#30340;&#32463;&#39564;&#22238;&#25918;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#26159;&#23558;&#25152;&#26377;&#35266;&#27979;&#37117;&#35270;&#20026;&#30456;&#21516;&#65292;&#24573;&#30053;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24046;&#20943;&#23569;&#32463;&#39564;&#22238;&#25918;&#65288;VRER&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#23545;&#30456;&#20851;&#26679;&#26412;&#30340;&#36873;&#25321;&#24615;&#37325;&#22797;&#21033;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#12290;VRER&#20316;&#20026;&#19968;&#31181;&#36866;&#24212;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#20013;&#65292;&#26500;&#24314;&#20102;&#25105;&#20204;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;Policy Optimization with VRER (PG-VRER)&#12290;&#27492;&#22806;&#65292;&#25991;&#29486;&#20013;&#23545;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#32771;&#34385;&#26679;&#26412;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.08902v3 Announce Type: replace-cross  Abstract: For reinforcement learning on complex stochastic systems, it is desirable to effectively leverage the information from historical samples collected in previous iterations to accelerate policy optimization. Classical experience replay, while effective, treats all observations uniformly, neglecting their relative importance. To address this limitation, we introduce a novel Variance Reduction Experience Replay (VRER) framework, enabling the selective reuse of relevant samples to improve policy gradient estimation. VRER, as an adaptable method that can seamlessly integrate with different policy optimization algorithms, forms the foundation of our sample-efficient off-policy algorithm known as Policy Optimization with VRER (PG-VRER). Furthermore, the lack of a rigorous theoretical understanding of the experience replay method in the literature motivates us to introduce a novel theoretical framework that accounts for sample dependenc
&lt;/p&gt;</description></item><item><title>HCR-Net&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#33073;&#26426;&#25163;&#20889;&#23383;&#31526;&#35782;&#21035;&#32593;&#32476;&#65292;&#36890;&#36807;&#37096;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#29305;&#24449;&#25552;&#21462;&#23618;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2108.06663</link><description>&lt;p&gt;
HCR-Net&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33073;&#26426;&#25163;&#20889;&#23383;&#31526;&#35782;&#21035;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HCR-Net: A deep learning based script independent handwritten character recognition network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2108.06663
&lt;/p&gt;
&lt;p&gt;
HCR-Net&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#33073;&#26426;&#25163;&#20889;&#23383;&#31526;&#35782;&#21035;&#32593;&#32476;&#65292;&#36890;&#36807;&#37096;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#29305;&#24449;&#25552;&#21462;&#23618;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32463;&#36807;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#65292;&#25163;&#20889;&#23383;&#31526;&#35782;&#21035;&#65288;HCR&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27169;&#24335;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#19988;&#32570;&#20047;&#33073;&#26426;&#35782;&#21035;&#25216;&#26415;&#30340;&#30740;&#31350;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#30456;&#20284;&#30340;&#23383;&#31526;&#32467;&#26500;&#12289;&#19981;&#21516;&#30340;&#25163;&#20889;&#39118;&#26684;&#12289;&#19981;&#21516;&#30340;&#20070;&#20889;&#31995;&#32479;&#12289;&#25163;&#24037;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#12289;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#19981;&#21487;&#29992;&#24615;&#65292;&#20197;&#21450;&#33050;&#26412;&#29305;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HCR-Net&#30340;&#33073;&#26426;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#20026;HCR&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;HCR-Net&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;HCR&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;\textit{&#37096;&#20998;&#21033;&#29992;}&#20102;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#29305;&#24449;&#25552;&#21462;&#23618;&#12290;&#30001;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#22270;&#20687;&#22686;&#24378;&#65292;HCR-Net&#25552;&#20379;&#20102;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#23569;&#37327;&#25968;&#25454;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2108.06663v4 Announce Type: replace-cross  Abstract: Handwritten character recognition (HCR) remains a challenging pattern recognition problem despite decades of research, and lacks research on script independent recognition techniques. {\color{black}This is mainly because of similar character structures, different handwriting styles, diverse scripts, handcrafted feature extraction techniques, unavailability of data and code, and the development of script-specific deep learning techniques. To address these limitations, we have proposed a script independent deep learning network for HCR research, called HCR-Net, that sets a new research direction for the field. HCR-Net is based on a novel transfer learning approach for HCR, which \textit{partly utilizes} feature extraction layers of a pre-trained network.} Due to transfer learning and image augmentation, HCR-Net provides faster and computationally efficient training, better performance and generalizations, and can work with small 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21512;&#20316;&#36870;&#24378;&#21270;&#23398;&#20064;(CIRL)&#30340;&#23450;&#20041;&#65292;&#23558;&#20215;&#20540;&#23545;&#40784;&#38382;&#39064;&#36716;&#21270;&#20026;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#21512;&#20316;&#37096;&#20998;&#20449;&#24687;&#21338;&#24328;&#65292;&#36890;&#36807;&#31215;&#26497;&#25945;&#23398;&#12289;&#31215;&#26497;&#23398;&#20064;&#21644;&#27807;&#36890;&#34892;&#20026;&#31561;&#26041;&#24335;&#23454;&#29616;&#26368;&#22823;&#21270;&#20215;&#20540;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/1606.03137</link><description>&lt;p&gt;
&#21512;&#20316;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cooperative Inverse Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1606.03137
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21512;&#20316;&#36870;&#24378;&#21270;&#23398;&#20064;(CIRL)&#30340;&#23450;&#20041;&#65292;&#23558;&#20215;&#20540;&#23545;&#40784;&#38382;&#39064;&#36716;&#21270;&#20026;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#21512;&#20316;&#37096;&#20998;&#20449;&#24687;&#21338;&#24328;&#65292;&#36890;&#36807;&#31215;&#26497;&#25945;&#23398;&#12289;&#31215;&#26497;&#23398;&#20064;&#21644;&#27807;&#36890;&#34892;&#20026;&#31561;&#26041;&#24335;&#23454;&#29616;&#26368;&#22823;&#21270;&#20215;&#20540;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#33258;&#20027;&#31995;&#32479;&#23545;&#20154;&#31867;&#26377;&#25152;&#24110;&#21161;&#19988;&#19981;&#24102;&#26469;&#19981;&#24517;&#35201;&#30340;&#39118;&#38505;&#65292;&#23427;&#38656;&#35201;&#19982;&#29615;&#22659;&#20013;&#30340;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20351;&#20854;&#34892;&#21160;&#26377;&#21161;&#20110;&#26368;&#22823;&#21270;&#20154;&#31867;&#30340;&#20215;&#20540;&#12290;&#26412;&#25991;&#23558;&#20215;&#20540;&#23545;&#40784;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#21512;&#20316;&#36870;&#24378;&#21270;&#23398;&#20064;(Cooperative Inverse Reinforcement Learning, CIRL)&#12290;CIRL&#38382;&#39064;&#26159;&#19968;&#20010;&#21512;&#20316;&#30340;&#12289;&#37096;&#20998;&#20449;&#24687;&#30340;&#21338;&#24328;&#65292;&#26377;&#20004;&#20010;&#21442;&#19982;&#32773;&#65306;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#65307;&#20004;&#32773;&#26681;&#25454;&#20154;&#31867;&#30340;&#22870;&#21169;&#20989;&#25968;&#33719;&#24471;&#22870;&#21169;&#65292;&#20294;&#26426;&#22120;&#20154;&#26368;&#21021;&#24182;&#19981;&#30693;&#36947;&#36825;&#20010;&#20989;&#25968;&#26159;&#20160;&#20040;&#12290;&#19982;&#32463;&#20856;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;&#22312;&#37027;&#37324;&#20551;&#35774;&#20154;&#31867;&#26159;&#22312;&#23396;&#31435;&#29366;&#24577;&#19979;&#34892;&#20107;&#26368;&#20248;&#30340;&#65292;&#26368;&#20248;&#30340;CIRL&#35299;&#20915;&#26041;&#26696;&#20135;&#29983;&#35832;&#22914;&#31215;&#26497;&#25945;&#23398;&#12289;&#31215;&#26497;&#23398;&#20064;&#21644;&#27807;&#36890;&#34892;&#20026;&#31561;&#34892;&#20026;&#65292;&#36825;&#20123;&#34892;&#20026;&#26356;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#20215;&#20540;&#23545;&#40784;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;CIRL&#28216;&#25103;&#20013;&#35745;&#31639;&#26368;&#20248;&#32852;&#21512;&#31574;&#30053;&#21487;&#20197;&#31616;&#21270;&#20026;&#35299;&#20915;&#37096;&#20998;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(POMDP)&#65292;&#35777;&#26126;&#20102;&#22312;&#23396;&#31435;&#29366;&#24577;&#19979;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1606.03137v4 Announce Type: replace  Abstract: For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#21319;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.16578</link><description>&lt;p&gt;
&#21457;&#25381;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#38271;&#65292;&#25552;&#21319;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;LLM&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports. (arXiv:2401.16578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#21319;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25918;&#23556;&#23398;&#39046;&#22495;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#22823;&#22823;&#25512;&#36827;&#20102;&#25253;&#21578;&#29983;&#25104;&#65292;&#20294;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#21644;&#20020;&#24202;&#25928;&#33021;&#65288;CE&#65289;&#65292;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#20020;&#24202;&#32972;&#26223;&#30340;&#35821;&#20041;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#36807;&#20998;&#24378;&#35843;&#20020;&#24202;&#32454;&#33410;&#65292;&#38477;&#20302;&#20102;&#25253;&#21578;&#30340;&#28165;&#26224;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4 1&#65292;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;&#19978;&#19979;&#25991;&#25351;&#23548;&#23398;&#20064;&#65288;ICIL&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLM&#30340;&#35780;&#20272;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#26631;&#20934;&#20445;&#25345;&#19968;&#33268;&#65292;&#23454;&#29616;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25253;&#21578;&#19982;&#20154;&#31867;&#29983;&#25104;&#25253;&#21578;&#20043;&#38388;&#30340;&#35814;&#32454;&#27604;&#36739;&#12290;&#36825;&#36827;&#19968;&#27493;&#36890;&#36807;&#22238;&#24402;&#27169;&#22411;&#26469;&#32508;&#21512;&#21477;&#23376;&#35780;&#20272;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#8220;&#35814;&#32454;GPT-4&#65288;5&#27425;&#35757;&#32451;&#65289;&#8221;&#27169;&#22411;&#33719;&#24471;&#20102;0.48&#30340;&#20998;&#25968;&#65292;&#20248;&#20110;METEOR&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
In radiology, Artificial Intelligence (AI) has significantly advanced report generation, but automatic evaluation of these AI-produced reports remains challenging. Current metrics, such as Conventional Natural Language Generation (NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic intricacies of clinical contexts or overemphasize clinical details, undermining report clarity. To overcome these issues, our proposed method synergizes the expertise of professional radiologists with Large Language Models (LLMs), like GPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain of Thought (CoT) reasoning, our approach aligns LLM evaluations with radiologist standards, enabling detailed comparisons between human and AI generated reports. This is further enhanced by a Regression model that aggregates sentence evaluation scores. Experimental results show that our ''Detailed GPT-4 (5-shot)'' model achieves a 0.48 score, outperforming the METEOR metric 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#21487;&#25511;&#38376;&#36866;&#37197;&#22120;&#65288;ConGater&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#21487;&#35843;&#33410;&#25935;&#24863;&#24615;&#21442;&#25968;&#30340;&#26032;&#39062;&#27169;&#22359;&#21270;&#38376;&#26426;&#21046;&#65292;&#21487;&#22312;&#25512;&#29702;&#26102;&#36880;&#28176;&#36807;&#28193;&#20174;&#27169;&#22411;&#30340;&#20559;&#21521;&#29366;&#24577;&#21040;&#23436;&#20840;&#21435;&#20559;&#30340;&#29256;&#26412;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20998;&#31867;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16457</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#21487;&#25511;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#21033;&#29992;&#38376;&#36866;&#37197;&#22120;&#36827;&#34892;&#20998;&#31867;&#21644;&#26816;&#32034;&#12290;(arXiv:2401.16457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Effective Controllable Bias Mitigation for Classification and Retrieval using Gate Adapters. (arXiv:2401.16457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#21487;&#25511;&#38376;&#36866;&#37197;&#22120;&#65288;ConGater&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#21487;&#35843;&#33410;&#25935;&#24863;&#24615;&#21442;&#25968;&#30340;&#26032;&#39062;&#27169;&#22359;&#21270;&#38376;&#26426;&#21046;&#65292;&#21487;&#22312;&#25512;&#29702;&#26102;&#36880;&#28176;&#36807;&#28193;&#20174;&#27169;&#22411;&#30340;&#20559;&#21521;&#29366;&#24577;&#21040;&#23436;&#20840;&#21435;&#20559;&#30340;&#29256;&#26412;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20998;&#31867;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#24046;&#32531;&#35299;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#30340;&#20027;&#39064;&#65292;&#26368;&#36817;&#20851;&#27880;&#30340;&#28966;&#28857;&#26159;&#23398;&#20064;&#29420;&#31435;&#30340;&#27169;&#22359;&#65292;&#20363;&#22914;&#36866;&#37197;&#22120;&#36827;&#34892;&#25353;&#38656;&#21435;&#20559;&#12290;&#38500;&#20102;&#20248;&#21270;&#27169;&#22359;&#21270;&#21435;&#20559;&#27169;&#22411;&#22806;&#65292;&#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#38656;&#35201;&#22312;&#25512;&#29702;&#26102;&#25511;&#21046;&#20559;&#24046;&#20943;&#23569;&#30340;&#31243;&#24230;&#65292;&#20363;&#22914;&#65292;&#20026;&#20102;&#22312;&#25628;&#32034;&#32467;&#26524;&#20013;&#35843;&#25972;&#26399;&#26395;&#30340;&#24615;&#33021;-&#20844;&#24179;&#24615;&#26435;&#34913;&#25110;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#25511;&#21046;&#21435;&#20559;&#30340;&#24378;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#25511;&#38376;&#36866;&#37197;&#22120;&#65288;ConGater&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#21487;&#35843;&#33410;&#25935;&#24863;&#24615;&#21442;&#25968;&#30340;&#26032;&#39062;&#27169;&#22359;&#21270;&#38376;&#26426;&#21046;&#65292;&#20801;&#35768;&#22312;&#25512;&#29702;&#26102;&#20174;&#27169;&#22411;&#30340;&#20559;&#21521;&#29366;&#24577;&#36880;&#28176;&#36807;&#28193;&#21040;&#23436;&#20840;&#21435;&#20559;&#30340;&#29256;&#26412;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#23545;&#19977;&#20010;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24615;&#21435;&#20559;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;&#20844;&#24179;&#24615;&#21015;&#34920;&#27491;&#21017;&#21270;&#26469;&#20943;&#23569;&#25628;&#32034;&#32467;&#26524;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ConGater&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias mitigation of Language Models has been the topic of many studies with a recent focus on learning separate modules like adapters for on-demand debiasing. Besides optimizing for a modularized debiased model, it is often critical in practice to control the degree of bias reduction at inference time, e.g., in order to tune for a desired performance-fairness trade-off in search results or to control the strength of debiasing in classification tasks. In this paper, we introduce Controllable Gate Adapter (ConGater), a novel modular gating mechanism with adjustable sensitivity parameters, which allows for a gradual transition from the biased state of the model to the fully debiased version at inference time. We demonstrate ConGater performance by (1) conducting adversarial debiasing experiments with three different models on three classification tasks with four protected attributes, and (2) reducing the bias of search results through fairness list-wise regularization to enable adjusting a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#30340;&#20381;&#36182;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36866;&#24403;&#20381;&#36182;&#12290;&#35813;&#23450;&#20041;&#20998;&#31163;&#20102;&#20381;&#36182;&#30340;&#27010;&#24565;&#21644;&#20154;&#31867;&#22312;&#24418;&#25104;&#20934;&#30830;&#20449;&#24565;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20026;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20114;&#34917;&#24615;&#21644;&#20381;&#36182;&#24615;&#30340;&#30740;&#31350;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.15356</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20381;&#36182;&#30340;&#32479;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Statistical Framework for Measuring AI Reliance. (arXiv:2401.15356v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15356
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#30340;&#20381;&#36182;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36866;&#24403;&#20381;&#36182;&#12290;&#35813;&#23450;&#20041;&#20998;&#31163;&#20102;&#20381;&#36182;&#30340;&#27010;&#24565;&#21644;&#20154;&#31867;&#22312;&#24418;&#25104;&#20934;&#30830;&#20449;&#24565;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20026;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20114;&#34917;&#24615;&#21644;&#20381;&#36182;&#24615;&#30340;&#30740;&#31350;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#32463;&#24120;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24110;&#21161;&#19979;&#20570;&#20915;&#31574;&#12290;&#19968;&#20010;&#24120;&#35265;&#27169;&#24335;&#26159;&#20154;&#24037;&#26234;&#33021;&#21521;&#20154;&#31867;&#25512;&#33616;&#34892;&#21160;&#65292;&#32780;&#20154;&#31867;&#20445;&#30041;&#23545;&#26368;&#32456;&#20915;&#31574;&#30340;&#25511;&#21046;&#26435;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#30830;&#35748;&#65292;&#30830;&#20445;&#20154;&#31867;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36866;&#24403;&#20381;&#36182;&#26159;&#23454;&#29616;&#20114;&#34917;&#24615;&#33021;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#30446;&#21069;&#22312;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#36866;&#24403;&#20381;&#36182;&#30340;&#23450;&#20041;&#32570;&#20047;&#24418;&#24335;&#21270;&#30340;&#32479;&#35745;&#22522;&#30784;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#30683;&#30462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#30340;&#20381;&#36182;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#23427;&#23558;&#20381;&#36182;&#30340;&#27010;&#24565;&#19982;&#20154;&#31867;&#22312;&#21306;&#20998;&#20449;&#21495;&#24182;&#24418;&#25104;&#20934;&#30830;&#20449;&#24565;&#30340;&#25361;&#25112;&#20998;&#24320;&#12290;&#25105;&#20204;&#30340;&#23450;&#20041;&#20135;&#29983;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#26469;&#25351;&#23548;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20114;&#34917;&#24615;&#21644;&#20381;&#36182;&#24615;&#30340;&#30740;&#31350;&#35774;&#35745;&#21644;&#35299;&#37322;&#12290;&#21033;&#29992;&#26368;&#36817;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#20915;&#31574;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Humans frequently make decisions with the aid of artificially intelligent (AI) systems. A common pattern is for the AI to recommend an action to the human who retains control over the final decision. Researchers have identified ensuring that a human has appropriate reliance on an AI as a critical component of achieving complementary performance. We argue that the current definition of appropriate reliance used in such research lacks formal statistical grounding and can lead to contradictions. We propose a formal definition of reliance, based on statistical decision theory, which separates the concepts of reliance as the probability the decision-maker follows the AI's prediction from challenges a human may face in differentiating the signals and forming accurate beliefs about the situation. Our definition gives rise to a framework that can be used to guide the design and interpretation of studies on human-AI complementarity and reliance. Using recent AI-advised decision making studies f
&lt;/p&gt;</description></item><item><title>GeoDecoder&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#22320;&#22270;&#20013;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#22788;&#29702;&#27169;&#22359;&#65292;&#26080;&#32541;&#38598;&#25104;&#22806;&#37096;&#25968;&#25454;&#21644;&#29305;&#24449;&#65292;&#20197;&#21450;&#25191;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#25191;&#34892;&#65292;&#23454;&#29616;&#20102;&#24378;&#21270;&#22320;&#22270;&#35748;&#30693;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.15118</link><description>&lt;p&gt;
GeoDecoder: &#24378;&#21270;&#22810;&#27169;&#24577;&#22320;&#22270;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
GeoDecoder: Empowering Multimodal Map Understanding. (arXiv:2401.15118v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15118
&lt;/p&gt;
&lt;p&gt;
GeoDecoder&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#22320;&#22270;&#20013;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#22788;&#29702;&#27169;&#22359;&#65292;&#26080;&#32541;&#38598;&#25104;&#22806;&#37096;&#25968;&#25454;&#21644;&#29305;&#24449;&#65292;&#20197;&#21450;&#25191;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#25191;&#34892;&#65292;&#23454;&#29616;&#20102;&#24378;&#21270;&#22320;&#22270;&#35748;&#30693;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GeoDecoder&#65292;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#22320;&#22270;&#20013;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;GeoDecoder&#22522;&#20110;BeitGPT&#26550;&#26500;&#26500;&#24314;&#65292;&#24182;&#38598;&#25104;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#22788;&#29702;&#30340;&#19987;&#19994;&#27169;&#22359;&#12290;&#22312;&#22270;&#20687;&#26041;&#38754;&#65292;GeoDecoder&#21033;&#29992;&#39640;&#24503;&#22320;&#22270;&#20316;&#20026;&#24213;&#22270;&#65292;&#35813;&#22320;&#22270;&#20869;&#32622;&#20102;&#36947;&#36335;&#21644;&#24314;&#31569;&#24418;&#29366;&#12289;&#30456;&#23545;&#20301;&#32622;&#21644;&#20854;&#20182;&#23646;&#24615;&#30340;&#37325;&#35201;&#32454;&#33410;&#12290;&#36890;&#36807;&#28210;&#26579;&#25216;&#26415;&#65292;&#35813;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#20102;&#22806;&#37096;&#25968;&#25454;&#21644;&#29305;&#24449;&#65292;&#22914;&#31526;&#21495;&#26631;&#35760;&#12289;&#39550;&#39542;&#36712;&#36857;&#12289;&#28909;&#21147;&#22270;&#21644;&#29992;&#25143;&#23450;&#20041;&#30340;&#26631;&#35760;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#29305;&#24449;&#24037;&#31243;&#38656;&#27714;&#12290;GeoDecoder&#30340;&#25991;&#26412;&#27169;&#22359;&#25509;&#21463;&#21508;&#31181;&#19978;&#19979;&#25991;&#25991;&#26412;&#21644;&#38382;&#39064;&#25552;&#31034;&#65292;&#24182;&#29983;&#25104;&#31867;&#20284;&#20110;GPT&#30340;&#25991;&#26412;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;GPT&#30340;&#27169;&#22411;&#20801;&#35768;&#22312;&#21516;&#19968;&#27169;&#22411;&#20013;&#36827;&#34892;&#22810;&#20010;&#20219;&#21153;&#30340;&#35757;&#32451;&#21644;&#25191;&#34892;&#12290;&#20026;&#20102;&#22686;&#24378;&#22320;&#22270;&#35748;&#30693;&#33021;&#21147;&#24182;&#20351;GeoDecoder&#33719;&#21462;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
This paper presents GeoDecoder, a dedicated multimodal model designed for processing geospatial information in maps. Built on the BeitGPT architecture, GeoDecoder incorporates specialized expert modules for image and text processing. On the image side, GeoDecoder utilizes GaoDe Amap as the underlying base map, which inherently encompasses essential details about road and building shapes, relative positions, and other attributes. Through the utilization of rendering techniques, the model seamlessly integrates external data and features such as symbol markers, drive trajectories, heatmaps, and user-defined markers, eliminating the need for extra feature engineering. The text module of GeoDecoder accepts various context texts and question prompts, generating text outputs in the style of GPT. Furthermore, the GPT-based model allows for the training and execution of multiple tasks within the same model in an end-to-end manner. To enhance map cognition and enable GeoDecoder to acquire knowle
&lt;/p&gt;</description></item><item><title>LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13586</link><description>&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#25552;&#31034;&#26435;&#37325;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13586
&lt;/p&gt;
&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23567;&#22411;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#22914;&#20309;&#24433;&#21709;&#22312;&#25351;&#20196;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;7B&#22823;&#23567;&#30340;LLaMA&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#37325;&#29616;&#20102;&#26031;&#22374;&#31119;&#22823;&#23398;&#30340;Alpaca&#23454;&#39564;&#65292;&#20854;&#20013;&#21253;&#25324;LLaMA 1&#21644;LLaMA 2&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19982;PLW&#20043;&#38388;&#23384;&#22312;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#22312;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19981;&#21463;PLW&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27424;&#38459;&#23612; Langevin Monte Carlo &#21152;&#36895;&#30340;&#36817;&#20284; Thompson &#37319;&#26679;&#31574;&#30053;&#65292;&#36890;&#36807;&#29305;&#23450;&#21183;&#20989;&#25968;&#30340;&#35774;&#35745;&#25913;&#21892;&#20102;&#39640;&#32500;&#38382;&#39064;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#39640;&#32500;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.11665</link><description>&lt;p&gt;
&#20351;&#29992;&#27424;&#38459;&#23612; Langevin Monte Carlo &#21152;&#36895;&#36817;&#20284; Thompson &#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo. (arXiv:2401.11665v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27424;&#38459;&#23612; Langevin Monte Carlo &#21152;&#36895;&#30340;&#36817;&#20284; Thompson &#37319;&#26679;&#31574;&#30053;&#65292;&#36890;&#36807;&#29305;&#23450;&#21183;&#20989;&#25968;&#30340;&#35774;&#35745;&#25913;&#21892;&#20102;&#39640;&#32500;&#38382;&#39064;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#39640;&#32500;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27424;&#38459;&#23612; Langevin Monte Carlo &#30340;&#36817;&#20284; Thompson &#37319;&#26679;&#26041;&#27861;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#33539;&#22260;&#65292;&#20174;&#39640;&#26031;&#21518;&#39564;&#37319;&#26679;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#24179;&#28369;&#21518;&#39564;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#35201;&#27714;&#39640;&#20934;&#30830;&#24615;&#26102;&#65292;&#20173;&#28982;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284; Thompson &#37319;&#26679;&#31574;&#30053;&#65292;&#21033;&#29992;&#27424;&#38459;&#23612; Langevin Monte Carlo&#65292;&#21518;&#32773;&#26159;&#27169;&#25311;&#39640;&#32500;&#21518;&#39564;&#30340;&#36890;&#29992;&#24037;&#20855;&#12290;&#22522;&#20110;&#26631;&#20934;&#30340;&#24179;&#28369;&#24615;&#21644;&#23545;&#25968;&#20985;&#24615;&#26465;&#20214;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#29305;&#23450;&#21183;&#20989;&#25968;&#30340;&#21152;&#36895;&#21518;&#39564;&#38598;&#20013;&#21644;&#37319;&#26679;&#12290;&#35813;&#35774;&#35745;&#25913;&#36827;&#20102;&#23454;&#29616;&#23545;&#25968;&#36951;&#25022;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20174;$\mathcal{\tilde O}(d)$&#25913;&#36827;&#21040;$\mathcal{\tilde O}(\sqrt{d})$&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#22312;&#39640;&#32500;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#32463;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate Thompson sampling with Langevin Monte Carlo broadens its reach from Gaussian posterior sampling to encompass more general smooth posteriors. However, it still encounters scalability issues in high-dimensional problems when demanding high accuracy. To address this, we propose an approximate Thompson sampling strategy, utilizing underdamped Langevin Monte Carlo, where the latter is the go-to workhorse for simulations of high-dimensional posteriors. Based on the standard smoothness and log-concavity conditions, we study the accelerated posterior concentration and sampling using a specific potential function. This design improves the sample complexity for realizing logarithmic regrets from $\mathcal{\tilde O}(d)$ to $\mathcal{\tilde O}(\sqrt{d})$. The scalability and robustness of our algorithm are also empirically validated through synthetic experiments in high-dimensional bandit problems.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21028;&#26029;&#23433;&#20840;&#39118;&#38505;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;R-Judge&#65292;&#36890;&#36807;&#23545;162&#20010;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;72.29%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.10019</link><description>&lt;p&gt;
R-Judge: &#35780;&#20272;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#39118;&#38505;&#24847;&#35782;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
R-Judge: Benchmarking Safety Risk Awareness for LLM Agents. (arXiv:2401.10019v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21028;&#26029;&#23433;&#20840;&#39118;&#38505;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;R-Judge&#65292;&#36890;&#36807;&#23545;162&#20010;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;72.29%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#23436;&#25104;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLM&#20195;&#29702;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#25805;&#20316;&#26102;&#20250;&#24341;&#20837;&#24847;&#22806;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#19982;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#23433;&#20840;&#24615;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#23433;&#20840;&#24615;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;R-Judge&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#22312;&#32473;&#23450;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#26102;&#21028;&#26029;&#23433;&#20840;&#39118;&#38505;&#30340;&#33021;&#21147;&#12290;R-Judge&#21253;&#25324;162&#20010;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#65292;&#28085;&#30422;7&#20010;&#24212;&#29992;&#39046;&#22495;&#21644;10&#31181;&#39118;&#38505;&#31867;&#22411;&#30340;27&#20010;&#20851;&#38190;&#39118;&#38505;&#22330;&#26223;&#12290;&#23427;&#32467;&#21512;&#20102;&#20154;&#31867;&#23545;&#23433;&#20840;&#24615;&#30340;&#20849;&#35782;&#65292;&#24182;&#20855;&#26377;&#26631;&#35760;&#30340;&#23433;&#20840;&#39118;&#38505;&#26631;&#31614;&#21644;&#39640;&#36136;&#37327;&#30340;&#39118;&#38505;&#25551;&#36848;&#12290;&#21033;&#29992;R-Judge&#65292;&#25105;&#20204;&#23545;8&#31181;&#24120;&#29992;&#20316;&#20195;&#29702;&#39592;&#24178;&#30340;&#33879;&#21517;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4&#23454;&#29616;&#20102;72.29%&#30340;&#23545;&#27604;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on LLM-generated content safety in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging safety risks given agent interaction records. R-Judge comprises 162 agent interaction records, encompassing 27 key risk scenarios among 7 application categories and 10 risk types. It incorporates human consensus on safety with annotated safety risk labels and high-quality risk descriptions. Utilizing R-Judge, we conduct a comprehensive evaluation of 8 prominent LLMs commonly employed as the backbone for agents. The best-performing model, GPT-4, achieves 72.29% in contrast to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#32593;&#32476;&#25299;&#25169;&#19979;&#65292;&#19981;&#21487;&#35775;&#38382;&#33410;&#28857;&#23545;&#27969;&#35328;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.09498</link><description>&lt;p&gt;
&#25216;&#26415;&#25253;&#21578;&#65306;&#20851;&#20110;&#33410;&#28857;&#19981;&#21487;&#35775;&#38382;&#24773;&#20917;&#19979;&#27969;&#35328;&#23398;&#20064;&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Technical Report: On the Convergence of Gossip Learning in the Presence of Node Inaccessibility. (arXiv:2401.09498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#32593;&#32476;&#25299;&#25169;&#19979;&#65292;&#19981;&#21487;&#35775;&#38382;&#33410;&#28857;&#23545;&#27969;&#35328;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gossip learning&#65288;GL&#65289;&#20316;&#20026;&#20998;&#25955;&#24335;&#23398;&#20064;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#26356;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#32593;&#32476;&#65292;&#22914;&#30001;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#32452;&#25104;&#30340;FANETs&#12290;GL&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;UAV&#32593;&#32476;&#30340;&#25928;&#29575;&#24182;&#24310;&#38271;&#30005;&#27744;&#23551;&#21629;&#12290;&#23613;&#31649;&#20855;&#26377;&#36825;&#20123;&#20248;&#21183;&#65292;&#20294;GL&#30340;&#24615;&#33021;&#21463;&#25968;&#25454;&#20998;&#24067;&#12289;&#36890;&#20449;&#36895;&#24230;&#21644;&#32593;&#32476;&#36830;&#25509;&#24615;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;GL&#30340;&#25910;&#25947;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#29616;&#26377;&#30740;&#31350;&#22522;&#20110;&#34394;&#25311;&#25968;&#37327;&#26469;&#30740;&#31350;GL&#30340;&#25910;&#25947;&#24615;&#65292;&#20197;&#26041;&#20415;&#24615;&#32780;&#24573;&#30053;&#20102;&#24403;&#19968;&#20123;&#33410;&#28857;&#19981;&#21487;&#35775;&#38382;&#26102;&#32593;&#32476;&#30340;&#30495;&#23454;&#29366;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21160;&#24577;&#32593;&#32476;&#25299;&#25169;&#19979;&#19981;&#21487;&#35775;&#38382;&#33410;&#28857;&#23545;GL&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#26435;&#37325;&#21457;&#25955;&#20998;&#35299;&#20026;&#33410;&#28857;&#26159;&#21542;&#21487;&#35775;&#38382;&#30340;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#33410;&#28857;&#21487;&#35775;&#38382;&#24615;&#30340;&#21160;&#24577;&#19979;GL&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Gossip learning (GL), as a decentralized alternative to federated learning (FL), is more suitable for resource-constrained wireless networks, such as FANETs that are formed by unmanned aerial vehicles (UAVs). GL can significantly enhance the efficiency and extend the battery life of UAV networks. Despite the advantages, the performance of GL is strongly affected by data distribution, communication speed, and network connectivity. However, how these factors influence the GL convergence is still unclear. Existing work studied the convergence of GL based on a virtual quantity for the sake of convenience, which fail to reflect the real state of the network when some nodes are inaccessible. In this paper, we formulate and investigate the impact of inaccessible nodes to GL under a dynamic network topology. We first decompose the weight divergence by whether the node is accessible or not. Then, we investigate the GL convergence under the dynamic of node accessibility and theoretically provide
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#27010;&#36848;&#65292;&#26088;&#22312;&#23547;&#27714;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.08699</link><description>&lt;p&gt;
&#20851;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Image Search in Histopathology. (arXiv:2401.08699v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08699
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#27010;&#36848;&#65292;&#26088;&#22312;&#23547;&#27714;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#30149;&#29702;&#22270;&#20687;&#21487;&#20197;&#36890;&#36807;&#35013;&#26377;&#25668;&#20687;&#22836;&#30340;&#26174;&#24494;&#38236;&#25110;&#20840;&#25195;&#25551;&#20202;&#33719;&#21462;&#12290;&#21033;&#29992;&#30456;&#20284;&#24615;&#35745;&#31639;&#22522;&#20110;&#36825;&#20123;&#22270;&#20687;&#21305;&#37197;&#24739;&#32773;&#65292;&#22312;&#30740;&#31350;&#21644;&#20020;&#24202;&#29615;&#22659;&#20013;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#26368;&#36817;&#25628;&#32034;&#25216;&#26415;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#23545;&#21508;&#31181;&#32452;&#32455;&#31867;&#22411;&#30340;&#32454;&#32990;&#32467;&#26500;&#36827;&#34892;&#24494;&#22937;&#30340;&#37327;&#21270;&#65292;&#20419;&#36827;&#27604;&#36739;&#65292;&#24182;&#22312;&#19982;&#35786;&#26029;&#21644;&#27835;&#30103;&#36807;&#30340;&#30149;&#20363;&#25968;&#25454;&#24211;&#36827;&#34892;&#27604;&#36739;&#26102;&#23454;&#29616;&#20851;&#20110;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#26032;&#24739;&#32773;&#39044;&#27979;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#27010;&#36848;&#65292;&#20197;&#23547;&#27714;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pathology images of histopathology can be acquired from camera-mounted microscopes or whole slide scanners. Utilizing similarity calculations to match patients based on these images holds significant potential in research and clinical contexts. Recent advancements in search technologies allow for nuanced quantification of cellular structures across diverse tissue types, facilitating comparisons and enabling inferences about diagnosis, prognosis, and predictions for new patients when compared against a curated database of diagnosed and treated cases. In this paper, we comprehensively review the latest developments in image search technologies for histopathology, offering a concise overview tailored for computational pathology researchers seeking effective, fast and efficient image search methods in their work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03160</link><description>&lt;p&gt;
&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65306;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30830;&#20445;AVs&#30340;&#23433;&#20840;&#24615;&#21644;&#20132;&#36890;&#27969;&#25928;&#29575;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#21457;&#23637;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;HAIM-DRL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#65292;&#31216;&#20026;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65288;HAIM&#65289;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#20026;AI&#20195;&#29702;&#25552;&#20379;&#24110;&#21161;&#12290;&#22312;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#30340;&#21516;&#26102;&#65292;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#65292;&#24182;&#23637;&#31034;&#27491;&#30830;&#30340;&#34892;&#21160;&#20197;&#36991;&#20813;&#28508;&#22312;&#20107;&#25925;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21487;&#20197;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20174;&#32780;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress in autonomous vehicles (AVs), the development of driving policies that ensure both the safety of AVs and traffic flow efficiency has not yet been fully explored. In this paper, we propose an enhanced human-in-the-loop reinforcement learning method, termed the Human as AI mentor-based deep reinforcement learning (HAIM-DRL) framework, which facilitates safe and efficient autonomous driving in mixed traffic platoon. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves as a mentor to the AI agent. While allowing the agent to sufficiently explore uncertain environments, the human expert can take control in dangerous situations and demonstrate correct actions to avoid potential accidents. On the other hand, the agent could be guided to minimize traffic flow disturbance, thereby optimizi
&lt;/p&gt;</description></item><item><title>GraphPro&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#20379;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2311.16716</link><description>&lt;p&gt;
GraphPro: &#38754;&#21521;&#25512;&#33616;&#31995;&#32479;&#30340;&#22270;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GraphPro: Graph Pre-training and Prompt Learning for Recommendation. (arXiv:2311.16716v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16716
&lt;/p&gt;
&lt;p&gt;
GraphPro&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#20379;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#22810;&#27425;&#28040;&#24687;&#20256;&#36882;&#22312;&#24314;&#27169;&#22797;&#26434;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#26029;&#21464;&#21270;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#21160;&#24577;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#21464;&#21270;&#21644;&#26032;&#21040;&#36798;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26041;&#38754;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphPro&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#32452;&#21512;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;GraphPro&#26694;&#26550;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#20020;&#26102;&#25552;&#31034;&#26426;&#21046;&#21644;&#22270;&#32467;&#26500;&#25552;&#31034;&#23398;&#20064;&#26426;&#21046;&#21040;&#39044;&#35757;&#32451;&#30340;GNN&#27169;&#22411;&#20013;&#26469;&#35299;&#20915;&#29992;&#25143;&#20559;&#22909;&#19981;&#26029;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
GNN-based recommenders have excelled in modeling intricate user-item interactions through multi-hop message passing. However, existing methods often overlook the dynamic nature of evolving user-item interactions, which impedes the adaption to changing user preferences and distribution shifts in newly arriving data. Thus, their scalability and performances in real-world dynamic environments are limited. In this study, we propose GraphPro, a framework that incorporates parameter-efficient and dynamic graph pre-training with prompt learning. This novel combination empowers GNNs to effectively capture both long-term user preferences and short-term behavior dynamics, enabling the delivery of accurate and timely recommendations. Our GraphPro framework addresses the challenge of evolving user preferences by seamlessly integrating a temporal prompt mechanism and a graph-structural prompt learning mechanism into the pre-trained GNN model. The temporal prompt mechanism encodes time information o
&lt;/p&gt;</description></item><item><title>DRNet&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36827;&#34892;&#36710;&#36947;&#21464;&#25442;&#65292;&#24182;&#32771;&#34385;&#21040;&#21608;&#22260;&#36710;&#36742;&#30340;&#39550;&#39542;&#39118;&#26684;&#65292;&#23454;&#29616;&#23433;&#20840;&#30340;&#20915;&#31574;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2311.01602</link><description>&lt;p&gt;
DRNet&#65306;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#36710;&#36947;&#21464;&#25442;&#20915;&#31574;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DRNet: A Decision-Making Method for Autonomous Lane Changingwith Deep Reinforcement Learning. (arXiv:2311.01602v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01602
&lt;/p&gt;
&lt;p&gt;
DRNet&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36827;&#34892;&#36710;&#36947;&#21464;&#25442;&#65292;&#24182;&#32771;&#34385;&#21040;&#21608;&#22260;&#36710;&#36742;&#30340;&#39550;&#39542;&#39118;&#26684;&#65292;&#23454;&#29616;&#23433;&#20840;&#30340;&#20915;&#31574;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20915;&#31574;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#22797;&#26434;&#30340;&#39550;&#39542;&#22330;&#26223;&#21644;&#21608;&#22260;&#36710;&#36742;&#30340;&#22810;&#21464;&#31038;&#20132;&#34892;&#20026;&#65292;&#36710;&#36947;&#21464;&#25442;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#25913;&#36827;&#29616;&#26377;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DRNet&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20840;&#26032;&#12289;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#39640;&#36895;&#20844;&#36335;&#19978;&#25191;&#34892;&#21512;&#29702;&#30340;&#36710;&#36947;&#21464;&#25442;&#65292;&#24182;&#32771;&#34385;&#21608;&#22260;&#36710;&#36742;&#30340;&#39550;&#39542;&#39118;&#26684;&#65292;&#20351;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#23398;&#20250;&#39550;&#39542;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#23433;&#20840;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;DRNet&#32467;&#21512;&#20102;&#23433;&#20840;&#39564;&#35777;&#30340;&#24605;&#24819;&#65292;&#36825;&#26159;&#33258;&#21160;&#39550;&#39542;&#30340;&#26368;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#30830;&#20445;&#22312;&#20219;&#20309;&#26102;&#21051;&#21482;&#36873;&#25321;&#23433;&#20840;&#30340;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques have outperformed numerous rule-based methods for decision-making in autonomous vehicles. Despite recent efforts, lane changing remains a major challenge, due to the complex driving scenarios and changeable social behaviors of surrounding vehicles. To help improve the state of the art, we propose to leveraging the emerging \underline{D}eep \underline{R}einforcement learning (DRL) approach for la\underline{NE} changing at the \underline{T}actical level. To this end, we present "DRNet", a novel and highly efficient DRL-based framework that enables a DRL agent to learn to drive by executing reasonable lane changing on simulated highways with an arbitrary number of lanes, and considering driving style of surrounding vehicles to make better decisions. Furthermore, to achieve a safe policy for decision-making, DRNet incorporates ideas from safety verification, the most important component of autonomous driving, to ensure that only safe actions are chosen at any ti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#36229;&#36234;&#24179;&#22343;&#22238;&#25253;&#30340;&#38382;&#39064;&#65292;&#24635;&#32467;&#20102;&#21487;&#20197;&#20934;&#30830;&#35745;&#31639;&#21644;&#20248;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#26032;&#35268;&#21010;&#26041;&#27861;&#12290;&#36825;&#20123;&#32467;&#26524;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35770;&#21457;&#23637;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.20266</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#36229;&#36234;&#24179;&#22343;&#22238;&#25253;
&lt;/p&gt;
&lt;p&gt;
Beyond Average Return in Markov Decision Processes. (arXiv:2310.20266v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20266
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#36229;&#36234;&#24179;&#22343;&#22238;&#25253;&#30340;&#38382;&#39064;&#65292;&#24635;&#32467;&#20102;&#21487;&#20197;&#20934;&#30830;&#35745;&#31639;&#21644;&#20248;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#26032;&#35268;&#21010;&#26041;&#27861;&#12290;&#36825;&#20123;&#32467;&#26524;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35770;&#21457;&#23637;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#21738;&#20123;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#34987;&#20934;&#30830;&#22320;&#35745;&#31639;&#21644;&#20248;&#21270;&#65311;&#22312;&#26377;&#38480;&#26102;&#38388;&#27573;&#12289;&#26080;&#25240;&#25187;&#35774;&#32622;&#20013;&#65292;&#21160;&#24577;&#35268;&#21010;&#21482;&#33021;&#39640;&#25928;&#22788;&#29702;&#26576;&#20123;&#32479;&#35745;&#31867;&#21035;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#20123;&#31867;&#21035;&#22312;&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#35268;&#21010;&#38382;&#39064;&#30340;&#26032;&#35299;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#65288;DistRL&#65289;&#30340;&#26356;&#19968;&#33324;&#26694;&#26550;&#20013;&#65292;&#21482;&#26377;&#24191;&#20041;&#24179;&#22343;&#20540;&#21487;&#20197;&#34987;&#20934;&#30830;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;DistRL&#20801;&#35768;&#36817;&#20284;&#35780;&#20272;&#20854;&#20182;&#20989;&#25968;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#32467;&#26524;&#20272;&#35745;&#22120;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#35752;&#35770;&#20102;&#27492;&#26041;&#27861;&#30340;&#28508;&#21147;&#21450;&#20854;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#36890;&#36807;&#30740;&#31350;&#22238;&#25253;&#30340;&#25972;&#20307;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;&#39118;&#38505;&#24847;&#35782;&#30340;&#31574;&#30053;&#65292;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35770;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
What are the functionals of the reward that can be computed and optimized exactly in Markov Decision Processes? In the finite-horizon, undiscounted setting, Dynamic Programming (DP) can only handle these operations efficiently for certain classes of statistics. We summarize the characterization of these classes for policy evaluation, and give a new answer for the planning problem. Interestingly, we prove that only generalized means can be optimized exactly, even in the more general framework of Distributional Reinforcement Learning (DistRL).DistRL permits, however, to evaluate other functionals approximately. We provide error bounds on the resulting estimators, and discuss the potential of this approach as well as its limitations.These results contribute to advancing the theory of Markov Decision Processes by examining overall characteristics of the return, and particularly risk-conscious strategies.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.13505</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#21270;&#25913;&#20889;&#29983;&#25104;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#30340;&#40065;&#26834;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#23545;&#35805;&#38382;&#31572;&#65288;ConvQA&#65289;&#27169;&#22411;&#36890;&#24120;&#22312;&#40644;&#37329;QA&#23545;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#36825;&#24847;&#21619;&#30528;&#35757;&#32451;&#20165;&#38480;&#20110;&#22312;&#30456;&#24212;&#25968;&#25454;&#38598;&#20013;&#35265;&#21040;&#30340;&#34920;&#38754;&#24418;&#24335;&#65292;&#35780;&#20272;&#20165;&#38024;&#23545;&#19968;&#23567;&#37096;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;REIGN&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#20960;&#20010;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#30340;&#23398;&#20064;&#35774;&#32622;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#19981;&#23436;&#25972;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23558;ConvQA&#27169;&#22411;&#24341;&#23548;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#21482;&#25552;&#20379;&#37027;&#20123;&#26377;&#21161;&#20110;&#25552;&#39640;&#22238;&#31572;&#36136;&#37327;&#30340;&#25913;&#20889;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#20010;&#22522;&#20934;&#19978;&#35757;&#32451;&#20027;&#35201;&#27169;&#22411;&#32452;&#20214;&#24182;&#23558;&#20854;&#38646;-shot&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#30340;&#21487;&#34892;&#24615;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#21644;&#37325;&#26032;&#37197;&#32622;&#21021;&#22987;&#30340;&#25913;&#20889;&#12289;&#27979;&#35797;&#35821;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models for conversational question answering (ConvQA) over knowledge graphs (KGs) are usually trained and tested on benchmarks of gold QA pairs. This implies that training is limited to surface forms seen in the respective datasets, and evaluation is on a small set of held-out questions. Through our proposed framework REIGN, we take several steps to remedy this restricted learning setup. First, we systematically generate reformulations of training questions to increase robustness of models to surface form variations. This is a particularly challenging problem, given the incomplete nature of such questions. Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning. Third, we demonstrate the viability of training major model components on one benchmark and applying them zero-shot to another. Finally, for a rigorous evaluation of robustness for trained models, we use and re
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12508</link><description>&lt;p&gt;
SalUn&#65306;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26435;&#37325;&#26174;&#33879;&#24615;&#22686;&#24378;&#26426;&#22120;&#36951;&#24536;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12508
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#27861;&#35268;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;&#24403;&#21069;AI&#27169;&#22411;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MU&#26041;&#27861;&#36890;&#24120;&#22312;&#36951;&#24536;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36328;&#39046;&#22495;&#36866;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MU&#20013;&#30340;&#8220;&#26435;&#37325;&#26174;&#33879;&#24615;&#8221;&#27010;&#24565;&#65292;&#20511;&#37492;&#20102;&#27169;&#22411;&#35299;&#37322;&#20013;&#30340;&#36755;&#20837;&#26174;&#33879;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#23558;MU&#30340;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20102;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#26174;&#33879;&#24615;&#36951;&#24536;&#65288;SalUn&#65289;&#30340;&#26041;&#27861;&#23558;&#20854;&#19982;&#8220;&#31934;&#30830;&#8221;&#36951;&#24536;&#65288;&#22312;&#21024;&#38500;&#36951;&#24536;&#25968;&#25454;&#38598;&#21518;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65289;&#30340;&#24615;&#33021;&#24046;&#36317;&#32553;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SalUn&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;MU&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;SalUn&#21487;&#22312;&#22270;&#29255;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#25830;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHOT-GM&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#20013;&#38544;&#34255;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#21305;&#37197;&#32467;&#26524;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#25512;&#26029;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.12081</link><description>&lt;p&gt;
DHOT-GM&#65306;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#23454;&#29616;&#40065;&#26834;&#22270;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework. (arXiv:2310.12081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHOT-GM&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#20013;&#38544;&#34255;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#21305;&#37197;&#32467;&#26524;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#25512;&#26029;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#22270;&#21305;&#37197;&#26159;&#26368;&#37325;&#35201;&#30340;&#22270;&#20998;&#26512;&#20219;&#21153;&#20043;&#19968;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#21305;&#37197;&#22270;&#26102;&#20381;&#36182;&#20110;&#37051;&#25509;&#30697;&#38453;&#25110;&#33410;&#28857;&#23884;&#20837;&#65292;&#20854;&#24615;&#33021;&#24120;&#24120;&#19981;&#22815;&#20248;&#36234;&#65292;&#22240;&#20026;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22270;&#20013;&#38544;&#34255;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#22914;&#33410;&#28857;&#23646;&#24615;&#12289;&#23376;&#22270;&#32467;&#26500;&#31561;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#65288;HOT&#65289;&#26694;&#26550;&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#31216;&#20026;DHOT-GM&#12290;&#23454;&#36136;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;&#22270;&#34920;&#31034;&#20026;&#19982;&#19981;&#21516;&#27169;&#24577;&#20449;&#24687;&#23545;&#24212;&#30340;&#19968;&#32452;&#20851;&#31995;&#30697;&#38453;&#12290;&#32473;&#23450;&#20004;&#20010;&#22270;&#65292;&#25105;&#20204;&#26522;&#20030;&#25152;&#26377;&#20851;&#31995;&#30697;&#38453;&#23545;&#65292;&#24182;&#33719;&#21462;&#23427;&#20204;&#30340;&#21305;&#37197;&#32467;&#26524;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#21305;&#37197;&#32467;&#26524;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#25512;&#26029;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20026;&#35745;&#31639;&#20004;&#20010;&#22270;&#20043;&#38388;&#30340;HOT&#36317;&#31163;&#65292;&#27599;&#20010;&#22270;&#37117;&#26159;&#30001;&#20851;&#31995;&#30697;&#38453;&#34920;&#31034;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph matching is one of the most significant graph analytic tasks in practice, which aims to find the node correspondence across different graphs. Most existing approaches rely on adjacency matrices or node embeddings when matching graphs, whose performances are often sub-optimal because of not fully leveraging the multi-modal information hidden in graphs, such as node attributes, subgraph structures, etc. In this study, we propose a novel and effective graph matching method based on a differentiable hierarchical optimal transport (HOT) framework, called DHOT-GM. Essentially, our method represents each graph as a set of relational matrices corresponding to the information of different modalities. Given two graphs, we enumerate all relational matrix pairs and obtain their matching results, and accordingly, infer the node correspondence by the weighted averaging of the matching results. This method can be implemented as computing the HOT distance between the two graphs -- each matching 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#21327;&#21516;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.11730</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#30340;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation. (arXiv:2310.11730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#21327;&#21516;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#36890;&#36807;&#20803;&#36335;&#24452;&#25551;&#36848;&#20016;&#23500;&#30340;&#35821;&#20041;&#65292;&#24050;&#25104;&#20026;&#32531;&#35299;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;HIN&#30340;&#25512;&#33616;&#31995;&#32479;&#25345;&#26377;&#25968;&#25454;&#30340;&#38598;&#20013;&#23384;&#20648;&#20551;&#35774;&#65292;&#24182;&#36827;&#34892;&#38598;&#20013;&#24335;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24448;&#24448;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#23384;&#20648;&#65292;&#23548;&#33268;&#38598;&#20013;&#24335;HIN&#25512;&#33616;&#26080;&#27861;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;HIN&#20998;&#20026;&#23458;&#25143;&#31471;&#23384;&#20648;&#30340;&#31169;&#26377;HIN&#21644;&#26381;&#21153;&#22120;&#31471;&#30340;&#20849;&#20139;HIN&#12290;&#22312;&#27492;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;HIN&#19978;&#21327;&#20316;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#27844;&#38706;&#29992;&#25143;&#38544;&#31169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#22522;&#20110;HIN&#30340;&#32852;&#21512;&#25512;&#33616;&#65292;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#20809;&#19979;&#30830;&#23450;&#20102;&#38544;&#31169;&#23450;&#20041;&#65292;&#26088;&#22312;&#20445;&#25252;&#31169;&#26377;HIN&#30340;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#65292;&#20197;&#21450;&#29992;&#25143;&#30340;&#38544;&#31169;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has become a powerful tool to alleviate data sparsity in recommender systems. Existing HIN-based recommendations hold the data centralized storage assumption and conduct centralized model training. However, the real-world data is often stored in a distributed manner for privacy concerns, resulting in the failure of centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored in the client side and shared HINs in the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which can collaboratively train a recommendation model on distributed HINs without leaking user privacy. Specifically, we first formalize the privacy definition in the light of differential privacy for HIN-based federated recommendation, which aims to protect user-item interactions of private HIN as well as user's 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#26159;&#19968;&#20010;&#30740;&#31350;&#30340;&#26032;&#39046;&#22495;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#20010;&#22330;&#26223;&#65292;&#21487;&#20197;&#36890;&#36807;&#21435;&#23398;&#20064;&#35753;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65292;&#21482;&#38656;&#35201;&#36127;&#38754;&#31034;&#20363;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#29305;&#21035;&#23545;&#20110;&#30693;&#36947;&#20855;&#20307;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#30340;&#35757;&#32451;&#26679;&#26412;&#26356;&#20026;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.10683</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Unlearning. (arXiv:2310.10683v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10683
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#26159;&#19968;&#20010;&#30740;&#31350;&#30340;&#26032;&#39046;&#22495;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#20010;&#22330;&#26223;&#65292;&#21487;&#20197;&#36890;&#36807;&#21435;&#23398;&#20064;&#35753;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65292;&#21482;&#38656;&#35201;&#36127;&#38754;&#31034;&#20363;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#29305;&#21035;&#23545;&#20110;&#30693;&#36947;&#20855;&#20307;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#30340;&#35757;&#32451;&#26679;&#26412;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#23398;&#20064;&#65292;&#21363;&#24536;&#35760;&#19981;&#21463;&#27426;&#36814;&#30340;&#65288;&#38750;&#65289;&#34892;&#20026;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33267;&#23569;&#19977;&#31181;&#24773;&#22659;&#21487;&#20197;&#20174;&#21435;&#23398;&#20064;&#20013;&#20351;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65306;&#65288;1&#65289;&#21024;&#38500;&#26377;&#23475;&#22238;&#22797;&#65292;&#65288;2&#65289;&#25353;&#35201;&#27714;&#21024;&#38500;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20869;&#23481;&#65292;&#20197;&#21450;&#65288;3&#65289;&#28040;&#38500;&#24187;&#35273;&#12290;&#20316;&#20026;&#23545;&#40784;&#25216;&#26415;&#30340;&#19968;&#31181;&#65292;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;&#21482;&#38656;&#35201;&#36127;&#38754;&#65288;&#20363;&#22914;&#26377;&#23475;&#65289;&#31034;&#20363;&#65292;&#36825;&#27604;&#22312;RLHF&#65288;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#20013;&#25152;&#38656;&#30340;&#27491;&#38754;&#65288;&#20363;&#22914;&#26377;&#24110;&#21161;&#19988;&#36890;&#24120;&#30001;&#20154;&#31867;&#32534;&#20889;&#65289;&#31034;&#20363;&#26356;&#23481;&#26131;&#21644;&#26356;&#20415;&#23452;&#22320;&#25910;&#38598;&#65288;&#20363;&#22914;&#36890;&#36807;&#32418;&#38431;&#27979;&#35797;&#25110;&#29992;&#25143;&#25253;&#21578;&#65289;&#65307;&#65288;2&#65289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65307;&#65288;3&#65289;&#24403;&#25105;&#20204;&#30693;&#36947;&#21738;&#20123;&#35757;&#32451;&#26679;&#26412;&#23548;&#33268;&#20102;&#19981;&#33391;&#34892;&#20026;&#26102;&#65292;&#23427;&#29305;&#21035;&#26377;&#25928;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;LLM&#21435;&#23398;&#20064;&#30340;&#24037;&#20316;&#20043;&#19968;&#12290;&#25105;&#20204;&#20063;&#26159;&#39318;&#27425;&#22312;LLM&#21435;&#23398;&#20064;&#20013;&#21046;&#23450;&#20102;&#35774;&#32622;&#12289;&#30446;&#26631;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#20174;&#19994;&#32773;&#21482;&#26377;&#26377;&#38480;&#30340;
&lt;/p&gt;
&lt;p&gt;
We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27599;&#20010;&#38454;&#27573;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.06452</link><description>&lt;p&gt;
&#29702;&#35299;RLHF&#23545;LLM&#27867;&#21270;&#21644;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effects of RLHF on LLM Generalisation and Diversity. (arXiv:2310.06452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27599;&#20010;&#38454;&#27573;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;AI&#27169;&#22411;&#20013;&#65292;&#22914;OpenAI&#30340;ChatGPT&#25110;Anthropic&#30340;Claude&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#23613;&#31649;&#22312;&#36825;&#20123;&#26041;&#27861;&#30340;&#24320;&#21457;&#26041;&#38754;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#23545;RLHF&#36807;&#31243;&#20013;&#27599;&#20010;&#38454;&#27573;&#30340;&#21033;&#19982;&#24330;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#38454;&#27573;&#65288;&#21363;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#65292;&#22870;&#21169;&#24314;&#27169;&#21644;RLHF&#65289;&#22914;&#20309;&#24433;&#21709;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65306;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#24773;&#26223;&#30340;&#32972;&#26223;&#19979;&#65292;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#36755;&#20986;&#22810;&#26679;&#24615;&#25351;&#30340;&#26159;&#27169;&#22411;&#29983;&#25104;&#21508;&#31181;&#19981;&#21516;&#36755;&#20986;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#21508;&#31181;&#29992;&#20363;&#26469;&#35828;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#25688;&#35201;&#21644;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#20013;&#23545;&#20004;&#20010;&#22522;&#26412;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21518;&#32773;&#38750;&#24120;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's LLaMA-2. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.05518</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;LSTD&#21644;&#38543;&#26426;&#29305;&#24449;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21452;&#19979;&#38477;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#24615;&#33021;&#21463;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#36807;&#21442;&#25968;&#21270;&#21644;&#20854;&#24102;&#26469;&#30340;&#22909;&#22788;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#20294;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24773;&#20917;&#21017;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25506;&#35752;&#20102;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#21442;&#25968;&#20010;&#25968;&#19982;&#35775;&#38382;&#29366;&#24577;&#20010;&#25968;&#20043;&#27604;&#23450;&#20041;&#20026;&#20851;&#38190;&#22240;&#32032;&#65292;&#24403;&#35813;&#27604;&#20540;&#22823;&#20110;1&#26102;&#31216;&#20026;&#36807;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#21363;&#22312;&#21442;&#25968;/&#29366;&#24577;&#27604;&#20026;1&#38468;&#36817;&#20250;&#31361;&#28982;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;&#26080;&#38480;&#22823;&#30340;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
&lt;/p&gt;</description></item><item><title>Hieros&#26159;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#30340;&#20998;&#23618;&#24819;&#20687;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#26102;&#38388;&#25277;&#35937;&#30340;&#19990;&#30028;&#34920;&#31034;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#24819;&#35937;&#36712;&#36857;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#24819;&#35937;&#12290;</title><link>http://arxiv.org/abs/2310.05167</link><description>&lt;p&gt;
Hieros: &#22522;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#30340;&#20998;&#23618;&#24819;&#20687;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hieros: Hierarchical Imagination on Structured State Space Sequence World Models. (arXiv:2310.05167v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05167
&lt;/p&gt;
&lt;p&gt;
Hieros&#26159;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#30340;&#20998;&#23618;&#24819;&#20687;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#26102;&#38388;&#25277;&#35937;&#30340;&#19990;&#30028;&#34920;&#31034;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#24819;&#35937;&#36712;&#36857;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#24819;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#26679;&#26412;&#25928;&#29575;&#12290;&#35768;&#22810;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#65292;&#22312;&#24819;&#35937;&#20013;&#23436;&#20840;&#35757;&#32451;&#20195;&#29702;&#65292;&#28040;&#38500;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;&#30452;&#25509;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38754;&#20020;&#24819;&#20687;&#20934;&#30830;&#24615;&#12289;&#25506;&#32034;&#33021;&#21147;&#25110;&#36816;&#34892;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Hieros&#65292;&#19968;&#31181;&#20998;&#23618;&#31574;&#30053;&#65292;&#23427;&#23398;&#20064;&#26102;&#38388;&#25277;&#35937;&#30340;&#19990;&#30028;&#34920;&#31034;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#24819;&#35937;&#36712;&#36857;&#12290;Hieros&#20351;&#29992;&#22522;&#20110;S5&#23618;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#24182;&#34892;&#39044;&#27979;&#19979;&#19968;&#20010;&#19990;&#30028;&#29366;&#24577;&#65292;&#24182;&#22312;&#29615;&#22659;&#20132;&#20114;&#26399;&#38388;&#36827;&#34892;&#36845;&#20195;&#39044;&#27979;&#12290;&#30001;&#20110;S5&#23618;&#30340;&#29305;&#27530;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#24819;&#35937;&#36807;&#31243;&#20013;&#24182;&#34892;&#35757;&#32451;&#21644;&#36845;&#20195;&#39044;&#27979;&#19979;&#19968;&#20010;&#19990;&#30028;&#29366;&#24577;&#12290;&#36825;&#27604;&#22522;&#20110;RNN&#30340;&#19990;&#30028;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#20063;&#27604;&#22522;&#20110;Transformer&#30340;&#19990;&#30028;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#24819;&#35937;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models.  We show that our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#24615;&#20998;&#26512;LLM&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#20219;&#21153;&#20013;&#24515;&#35282;&#24230;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#23545;&#20110;&#26435;&#37325;&#20013;&#20887;&#20313;&#24615;&#30340;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;"&#22403;&#22334;DNA&#20551;&#35774;"&#12290;</title><link>http://arxiv.org/abs/2310.02277</link><description>&lt;p&gt;
"&#22403;&#22334;DNA&#20551;&#35774;&#65306;&#36890;&#36807;&#31232;&#30095;&#24615;&#23545;LLM&#39044;&#35757;&#32451;&#26435;&#37325;&#36827;&#34892;&#20219;&#21153;&#20013;&#24515;&#35282;&#24230;&#20998;&#26512;"
&lt;/p&gt;
&lt;p&gt;
Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity. (arXiv:2310.02277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#24615;&#20998;&#26512;LLM&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#20219;&#21153;&#20013;&#24515;&#35282;&#24230;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#23545;&#20110;&#26435;&#37325;&#20013;&#20887;&#20313;&#24615;&#30340;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;"&#22403;&#22334;DNA&#20551;&#35774;"&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#23545;"&#22403;&#22334;DNA"&#30340;&#27010;&#24565;&#38271;&#26399;&#20197;&#26469;&#19982;&#20154;&#31867;&#22522;&#22240;&#32452;&#20013;&#30340;&#38750;&#32534;&#30721;&#29255;&#27573;&#30456;&#20851;&#32852;&#65292;&#21344;&#20854;&#32452;&#25104;&#30340;&#22823;&#32422;98%&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20123;&#36825;&#20123;&#30475;&#20284;&#26080;&#21151;&#33021;&#30340;DNA&#24207;&#21015;&#22312;&#32454;&#32990;&#36807;&#31243;&#20013;&#36215;&#21040;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#19982;&#20154;&#31867;&#22522;&#22240;&#20013;&#35266;&#23519;&#21040;&#30340;&#20887;&#20313;&#24615;&#26377;&#30528;&#26174;&#33879;&#30340;&#30456;&#20284;&#24615;&#12290;&#20154;&#20204;&#35748;&#20026;&#65292;&#24222;&#22823;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#21253;&#21547;&#20102;&#36807;&#22810;&#30340;&#20887;&#20313;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#21453;&#35770;&#26469;&#25361;&#25112;&#36825;&#20010;&#20256;&#32479;&#35266;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#31232;&#30095;&#24615;&#20316;&#20026;&#19968;&#31181;&#24037;&#20855;&#65292;&#26469;&#29420;&#31435;&#32780;&#20934;&#30830;&#22320;&#37327;&#21270;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#20302;&#24133;&#24230;&#26435;&#37325;&#30340;&#32454;&#24494;&#37325;&#35201;&#24615;&#65292;&#20174;&#19979;&#28216;&#20219;&#21153;&#20013;&#24515;&#30340;&#35282;&#24230;&#29702;&#35299;&#23427;&#20204;&#21253;&#21547;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25903;&#25345;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#30340;"&#22403;&#22334;DNA&#20551;&#35774;"&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional notion of "Junk DNA" has long been linked to non-coding segments within the human genome, constituting roughly 98% of its composition. However, recent research has unveiled the critical roles some of these seemingly non-functional DNA sequences play in cellular processes. Intriguingly, the weights within deep neural networks exhibit a remarkable similarity to the redundancy observed in human genes. It was believed that weights in gigantic models contained excessive redundancy, and could be removed without compromising performance. This paper challenges this conventional wisdom by presenting a compelling counter-argument. We employ sparsity as a tool to isolate and quantify the nuanced significance of low-magnitude weights in pre-trained large language models (LLMs). Our study demonstrates a strong correlation between these weight magnitudes and the knowledge they encapsulate, from a downstream task-centric angle. we raise the "Junk DNA Hypothesis" backed by our in-depth
&lt;/p&gt;</description></item><item><title>ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2309.12244</link><description>&lt;p&gt;
ChaCha&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#19982;&#20010;&#20154;&#20107;&#20214;&#30456;&#20851;&#30340;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12244
&lt;/p&gt;
&lt;p&gt;
ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#36890;&#24120;&#36890;&#36807;&#19982;&#23478;&#20154;&#25110;&#20182;&#20154;&#20998;&#20139;&#25925;&#20107;&#21644;&#24863;&#21463;&#26469;&#23398;&#20064;&#36776;&#35782;&#21644;&#34920;&#36798;&#24773;&#32490;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#20799;&#31461;&#27491;&#22312;&#21457;&#23637;&#20182;&#20204;&#30340;&#20132;&#27969;&#25216;&#33021;&#65292;&#29238;&#27597;&#25110;&#20804;&#24351;&#22992;&#22969;&#24456;&#38590;&#19982;&#20182;&#20204;&#36827;&#34892;&#24773;&#24863;&#27807;&#36890;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChaCha&#65292;&#19968;&#20010;&#40723;&#21169;&#21644;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChaCha&#32467;&#21512;&#20102;&#29366;&#24577;&#26426;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#36827;&#34892;&#33258;&#30001;&#23545;&#35805;&#30340;&#21516;&#26102;&#20445;&#25345;&#23545;&#35805;&#30340;&#26041;&#21521;&#24615;&#12290;&#36890;&#36807;&#19982;20&#21517;&#24180;&#40836;&#22312;8-12&#23681;&#30340;&#20799;&#31461;&#36827;&#34892;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChaCha&#22914;&#20309;&#20419;&#20351;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#24182;&#24341;&#23548;&#20182;&#20204;&#25551;&#36848;&#30456;&#20851;&#24773;&#32490;&#12290;&#21442;&#19982;&#32773;&#35748;&#20026;ChaCha&#23601;&#20687;&#19968;&#20010;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#20998;&#20139;&#20102;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#65292;&#22914;&#23478;&#24237;&#26053;&#34892;&#21644;&#20010;&#20154;&#25104;&#23601;&#12290;&#22522;&#20110;&#23450;&#37327;&#21644;&#23450;&#24615;&#21457;&#29616;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#35774;&#35745;&#36866;&#21512;&#20799;&#31461;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children typically learn to identify and express emotions through sharing their stories and feelings with others, particularly their family. However, it is challenging for parents or siblings to have emotional communication with children since children are still developing their communication skills. We present ChaCha, a chatbot that encourages and guides children to share personal events and associated emotions. ChaCha combines a state machine and large language models (LLMs) to keep the dialogue on track while carrying on free-form conversations. Through an exploratory study with 20 children (aged 8-12), we examine how ChaCha prompts children to share personal events and guides them to describe associated emotions. Participants perceived ChaCha as a close friend and shared their stories on various topics, such as family trips and personal achievements. Based on the quantitative and qualitative findings, we discuss opportunities for leveraging LLMs to design child-friendly chatbots to
&lt;/p&gt;</description></item><item><title>RaTrack&#26159;&#19968;&#31181;&#38024;&#23545;&#38647;&#36798;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#20197;&#21450;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#31227;&#21160;&#29289;&#20307;&#30340;&#31934;&#30830;&#36319;&#36394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09737</link><description>&lt;p&gt;
RaTrack: &#24102;&#26377;4D&#38647;&#36798;&#28857;&#20113;&#30340;&#36816;&#21160;&#29289;&#20307;&#26816;&#27979;&#19982;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud. (arXiv:2309.09737v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09737
&lt;/p&gt;
&lt;p&gt;
RaTrack&#26159;&#19968;&#31181;&#38024;&#23545;&#38647;&#36798;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#20197;&#21450;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#31227;&#21160;&#29289;&#20307;&#30340;&#31934;&#30830;&#36319;&#36394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#33258;&#20027;&#24615;&#20381;&#36182;&#20110;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#31934;&#30830;&#24863;&#30693;&#12290;&#22312;3D&#19990;&#30028;&#20013;&#31283;&#23450;&#22320;&#36319;&#36394;&#31227;&#21160;&#29289;&#20307;&#22240;&#27492;&#23545;&#20110;&#36712;&#36857;&#39044;&#27979;&#12289;&#36991;&#38556;&#21644;&#36335;&#24452;&#35268;&#21010;&#31561;&#24212;&#29992;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;LiDAR&#25110;&#30456;&#26426;&#36827;&#34892;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#65292;&#20294;4D&#25104;&#20687;&#38647;&#36798;&#30340;&#33021;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#35748;&#35782;&#21040;4D&#38647;&#36798;&#25968;&#25454;&#20013;&#30340;&#38647;&#36798;&#22122;&#22768;&#21644;&#28857;&#31232;&#30095;&#24615;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RaTrack&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22522;&#20110;&#38647;&#36798;&#30340;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25682;&#24323;&#20102;&#23545;&#29305;&#23450;&#23545;&#35937;&#31867;&#22411;&#21644;3D&#36793;&#30028;&#26694;&#30340;&#20381;&#36182;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#65292;&#24182;&#37197;&#20197;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#12290;&#22312;View-of-Delft&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;RaTrack&#23637;&#31034;&#20986;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#36816;&#21160;&#29289;&#20307;&#36319;&#36394;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile autonomy relies on the precise perception of dynamic environments. Robustly tracking moving objects in 3D world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods utilize LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#23454;&#26102;&#25512;&#29702;&#33021;&#21147;&#30340;&#27969;&#24335;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#34394;&#25311;&#26799;&#24230;&#36827;&#34892;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#65292;&#20511;&#21161;&#35821;&#20041;&#35760;&#24518;&#26469;&#25233;&#21046;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.08227</link><description>&lt;p&gt;
VERSE&#65306;&#20855;&#26377;&#23454;&#26102;&#25512;&#29702;&#33021;&#21147;&#30340;&#34394;&#25311;&#26799;&#24230;&#24863;&#30693;&#27969;&#36716;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
VERSE: Virtual-Gradient Aware Streaming Lifelong Learning with Anytime Inference. (arXiv:2309.08227v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08227
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#23454;&#26102;&#25512;&#29702;&#33021;&#21147;&#30340;&#27969;&#24335;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#34394;&#25311;&#26799;&#24230;&#36827;&#34892;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#65292;&#20511;&#21161;&#35821;&#20041;&#35760;&#24518;&#26469;&#25233;&#21046;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#26159;&#25351;&#22312;&#35757;&#32451;AI&#20195;&#29702;&#30340;&#21516;&#26102;&#65292;&#38450;&#27490;&#20854;&#36951;&#24536;&#20197;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#20851;&#27880;&#22312;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#32456;&#36523;&#23398;&#20064;&#65292;&#24182;&#19988;&#32570;&#20047;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#20943;&#36731;&#36951;&#24536;&#30340;&#33021;&#21147;&#12290;&#27969;&#24335;&#32456;&#36523;&#23398;&#20064;&#26159;&#32456;&#36523;&#23398;&#20064;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#21160;&#24577;&#30340;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#32780;&#19981;&#36951;&#24536;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#27969;&#24335;&#30340;&#65292;&#20165;&#38656;&#35201;&#23545;&#25968;&#25454;&#36827;&#34892;&#19968;&#27425;&#36941;&#21382;&#65292;&#21487;&#20197;&#20197;&#31867;&#22686;&#37327;&#30340;&#26041;&#24335;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#34892;&#21363;&#26102;&#35780;&#20272;&#65288;&#23454;&#26102;&#25512;&#29702;&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#34394;&#25311;&#26799;&#24230;&#65292;&#20197;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#20511;&#21161;&#22522;&#20110;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#30340;&#35821;&#20041;&#35760;&#24518;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning, also referred to as continual learning, is the problem of training an AI agent continuously while also preventing it from forgetting its previously acquired knowledge. Most of the existing methods primarily focus on lifelong learning within a static environment and lack the ability to mitigate forgetting in a quickly-changing dynamic environment. Streaming lifelong learning is a challenging setting of lifelong learning with the goal of continuous learning in a dynamic non-stationary environment without forgetting. We introduce a novel approach to lifelong learning, which is streaming, requires a single pass over the data, can learn in a class-incremental manner, and can be evaluated on-the-fly (anytime inference). To accomplish these, we propose virtual gradients for continual representation learning to prevent catastrophic forgetting and leverage an exponential-moving-average-based semantic memory to further enhance performance. Extensive experiments on diverse data
&lt;/p&gt;</description></item><item><title>DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05173</link><description>&lt;p&gt;
DePT:&#20998;&#35299;&#25552;&#31034;&#35843;&#25972;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05173
&lt;/p&gt;
&lt;p&gt;
DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#65288;PT&#65289;&#26159;&#19968;&#31181;&#23558;&#21487;&#35757;&#32451;&#30340;&#23569;&#37327;&#36719;&#25552;&#31034;&#21521;&#37327;&#38468;&#21152;&#21040;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36755;&#20837;&#20013;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#19982;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#27604;&#65292;PT&#30340;&#31454;&#20105;&#24615;&#33021;&#21487;&#20197;&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#20854;&#21442;&#25968;&#24182;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290; &#20294;&#26159;&#65292;PT&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#36719;&#25552;&#31034;&#26631;&#35760;&#65292;&#23548;&#33268;&#36755;&#20837;&#24207;&#21015;&#21464;&#38271;&#65292;&#36825;&#23545;&#20110;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20250;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290; &#36825;&#23545;&#20110;&#38754;&#20020;&#22823;&#37327;&#27599;&#26085;&#26597;&#35810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#23398;&#29983;&#30340;&#31572;&#39064;&#35760;&#24405;&#20013;&#30452;&#25509;&#35786;&#26029;&#21487;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#32771;&#29983;&#29305;&#24449;&#21644;&#39064;&#30446;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#31572;&#39064;&#35760;&#24405;&#26469;&#30830;&#20445;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00300</link><description>&lt;p&gt;
&#29992;&#32534;&#30721;-&#35299;&#30721;&#22120;&#36827;&#34892;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#26469;&#24314;&#27169;&#23398;&#29983;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Identifiable Cognitive Diagnosis with Encoder-decoder for Modelling Students' Performance. (arXiv:2309.00300v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#23398;&#29983;&#30340;&#31572;&#39064;&#35760;&#24405;&#20013;&#30452;&#25509;&#35786;&#26029;&#21487;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#32771;&#29983;&#29305;&#24449;&#21644;&#39064;&#30446;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#31572;&#39064;&#35760;&#24405;&#26469;&#30830;&#20445;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#35786;&#26029;&#26088;&#22312;&#26681;&#25454;&#23398;&#29983;&#22312;&#32771;&#35797;&#39064;&#30446;&#19978;&#30340;&#31572;&#39064;&#25104;&#32489;&#26469;&#35786;&#26029;&#20182;&#20204;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#36825;&#26159;&#35768;&#22810;&#39046;&#22495;&#22914;&#35745;&#31639;&#33258;&#36866;&#24212;&#27979;&#35797;&#30340;&#22522;&#30784;&#12290;&#29616;&#26377;&#30340;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#65288;CDMs&#65289;&#36981;&#24490;&#20102;&#19968;&#20010;&#33021;&#21147;-&#21709;&#24212;&#33539;&#24335;&#65292;&#21363;&#23558;&#35786;&#26029;&#32467;&#26524;&#35270;&#20026;&#23398;&#29983;&#21709;&#24212;&#30340;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26469;&#23398;&#20064;&#35786;&#26029;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#24456;&#23481;&#26131;&#23548;&#33268;&#19981;&#21487;&#35782;&#21035;&#30340;&#35786;&#26029;&#32467;&#26524;&#21644;&#35299;&#37322;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#23398;&#29983;&#23398;&#20064;&#34920;&#29616;&#30340;&#37327;&#21270;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#35786;&#26029;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#30452;&#25509;&#20174;&#21709;&#24212;&#26085;&#24535;&#20013;&#35786;&#26029;&#21487;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#32771;&#29983;&#29305;&#24449;&#21644;&#39064;&#30446;&#29305;&#24449;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#36890;&#29992;&#30340;&#39044;&#27979;&#27169;&#22359;&#20174;&#35786;&#26029;&#32467;&#26524;&#20013;&#37325;&#24314;&#21709;&#24212;&#26085;&#24535;&#65292;&#20197;&#30830;&#20445;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive diagnosis aims to diagnose students' knowledge proficiencies based on their response scores on exam questions, which is the basis of many domains such as computerized adaptive testing. Existing cognitive diagnosis models (CDMs) follow a proficiency-response paradigm, which views diagnostic results as learnable embeddings that are the cause of students' responses and learns the diagnostic results through optimization. However, such a paradigm can easily lead to unidentifiable diagnostic results and the explainability overfitting problem, which is harmful to the quantification of students' learning performance. To address these problems, we propose a novel identifiable cognitive diagnosis framework. Specifically, we first propose a flexible diagnostic module which directly diagnose identifiable and explainable examinee traits and question features from response logs. Next, we leverage a general predictive module to reconstruct response logs from the diagnostic results to ensure
&lt;/p&gt;</description></item><item><title>&#36882;&#24402;&#24635;&#32467;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#22312;&#38271;&#23545;&#35805;&#20013;&#35760;&#24518;&#37325;&#35201;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15022</link><description>&lt;p&gt;
&#36882;&#24402;&#24635;&#32467;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models. (arXiv:2308.15022v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15022
&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#24635;&#32467;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#22312;&#38271;&#23545;&#35805;&#20013;&#35760;&#24518;&#37325;&#35201;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#31995;&#32479;&#22312;&#38271;&#26399;&#23545;&#35805;&#20013;&#23481;&#26131;&#36951;&#24536;&#37325;&#35201;&#20449;&#24687;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#29305;&#23450;&#30340;&#26816;&#32034;&#22120;&#25110;&#24635;&#32467;&#22120;&#20174;&#36807;&#21435;&#33719;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#36825;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#19988;&#39640;&#24230;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36882;&#24402;&#29983;&#25104;&#24635;&#32467;/&#35760;&#24518;&#65292;&#20197;&#22686;&#24378;&#38271;&#26399;&#35760;&#24518;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#21050;&#28608;LLMs&#35760;&#20303;&#23567;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#36882;&#24402;&#22320;&#20351;&#29992;&#20043;&#21069;&#30340;&#35760;&#24518;&#21644;&#38543;&#21518;&#30340;&#23545;&#35805;&#20869;&#23481;&#20135;&#29983;&#26032;&#30340;&#35760;&#24518;&#12290;&#26368;&#21518;&#65292;LLM&#21487;&#20197;&#22312;&#26368;&#26032;&#35760;&#24518;&#30340;&#24110;&#21161;&#19979;&#36731;&#26494;&#29983;&#25104;&#39640;&#24230;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#21644;text-davinci-003&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#23545;&#35805;&#20013;&#21487;&#20197;&#29983;&#25104;&#26356;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23454;&#29616;LLM&#24314;&#27169;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most open-domain dialogue systems suffer from forgetting important information, especially in a long-term conversation. Existing works usually train the specific retriever or summarizer to obtain key information from the past, which is time-consuming and highly depends on the quality of labeled data. To alleviate this problem, we propose to recursively generate summaries/ memory using large language models (LLMs) to enhance long-term memory ability. Specifically, our method first stimulates LLMs to memorize small dialogue contexts and then recursively produce new memory using previous memory and following contexts. Finally, the LLM can easily generate a highly consistent response with the help of the latest memory. We evaluate our method using ChatGPT and text-davinci-003, and the experiments on the widely-used public dataset show that our method can generate more consistent responses in a long-context conversation. Notably, our method is a potential solution to enable the LLM to model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13916</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#20247;&#22810;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#19981;&#23436;&#25972;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#19977;&#20803;&#32452;&#35270;&#20026;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#30693;&#35782;&#22270;&#35889;LLM&#65288;KG-LLM&#65289;&#65292;&#26469;&#23545;&#36825;&#20123;&#19977;&#20803;&#32452;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21033;&#29992;&#19977;&#20803;&#32452;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#21709;&#24212;&#36827;&#34892;&#39044;&#27979;&#12290;&#23545;&#21508;&#31181;&#22522;&#20934;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#31561;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24494;&#35843;&#30456;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;LLaMA-7B&#65292;ChatGLM-6B&#65289;&#20248;&#20110;&#26368;&#26032;&#30340;ChatGPT&#21644;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.11804</link><description>&lt;p&gt;
&#36825;&#19981;&#26159;&#19968;&#20010;&#33529;&#26524;&#65306;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings. (arXiv:2308.11804v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#23558;&#22270;&#20687;&#12289;&#22768;&#38899;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#31561;&#26144;&#23556;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23545;&#40784;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65288;&#20363;&#22914;&#23558;&#19968;&#24352;&#29399;&#30340;&#22270;&#20687;&#19982;&#19968;&#31181;&#21483;&#22768;&#30456;&#20851;&#32852;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#21487;&#20197;&#21463;&#21040;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#23545;&#25239;&#24187;&#35273;&#8221;&#30340;&#25915;&#20987;&#12290;&#32473;&#23450;&#20219;&#24847;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#23427;&#65292;&#20351;&#20854;&#23884;&#20837;&#25509;&#36817;&#20110;&#21478;&#19968;&#27169;&#24577;&#20013;&#20219;&#24847;&#23545;&#25163;&#36873;&#25321;&#30340;&#36755;&#20837;&#30340;&#23884;&#20837;&#12290;&#24187;&#35273;&#20351;&#23545;&#25163;&#33021;&#22815;&#23558;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#31561;&#36827;&#34892;&#23545;&#40784;&#12290;&#23545;&#25239;&#24187;&#35273;&#21033;&#29992;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#25509;&#36817;&#24615;&#65292;&#22240;&#27492;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#12290;&#20351;&#29992;ImageBind&#23884;&#20837;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#27809;&#26377;&#20855;&#20307;&#19979;&#28216;&#20219;&#21153;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23545;&#40784;&#30340;&#36755;&#20837;&#22914;&#20309;&#35823;&#23548;&#22270;&#20687;&#29983;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#38646;&#26679;&#20363;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.  Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11131</link><description>&lt;p&gt;
ReLLa: &#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. (arXiv:2308.11131v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#34987;&#31215;&#26497;&#25506;&#32034;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#36866;&#24212;&#21644;&#22686;&#24378;&#32431;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#25512;&#33616;&#39046;&#22495;&#20013;LLMs&#26080;&#27861;&#20174;&#38271;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#30340;&#25991;&#26412;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#24182;&#23450;&#20041;&#20102;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ReLLa&#65289;&#12290;&#38024;&#23545;&#38646;&#26679;&#26412;&#25512;&#33616;&#65292;&#25105;&#20204;&#25191;&#34892;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#39640;&#25968;&#25454;&#30340;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07758</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#65288;Chain-of-Though, CoT&#65289;&#25552;&#31034;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;Self-Consistency&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#36825;&#20123;&#38142;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#26102;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#21363;``&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19978;&#36848;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#20505;&#36873;&#31572;&#26696;&#65292;&#37027;&#20040;&#26410;&#30693;&#21464;&#37327;x&#30340;&#20540;&#26159;&#22810;&#23569;&#65311;''&#65292;&#23558;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#23631;&#34109;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#27491;&#30830;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;FOBAR&#26041;&#27861;&#65292;&#23558;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.11655</link><description>&lt;p&gt;
&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19982;&#24378;&#30423;&#21453;&#39304;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#30830;&#23450;&#24615;&#28436;&#21270;&#21644;&#19981;&#21487;&#35266;&#27979;&#30340;&#29366;&#24577;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20027;&#35201;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#30340;&#23398;&#20064;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#22312;&#27599;&#19968;&#36718;&#33719;&#24471;&#30340;&#22870;&#21169;&#26159;&#36873;&#25321;&#34892;&#21160;&#30340;&#30701;&#26399;&#22870;&#21169;&#21644;&#31995;&#32479;&#30340;&#8220;&#20581;&#24247;&#8221;&#31243;&#24230;&#65288;&#21363;&#36890;&#36807;&#20854;&#29366;&#24577;&#27979;&#37327;&#65289;&#30340;&#20989;&#25968;&#12290;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#24179;&#21488;&#20174;&#29992;&#25143;&#23545;&#29305;&#23450;&#31867;&#22411;&#20869;&#23481;&#30340;&#21442;&#19982;&#20013;&#33719;&#24471;&#30340;&#22870;&#21169;&#19981;&#20165;&#21462;&#20915;&#20110;&#20855;&#20307;&#20869;&#23481;&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#36824;&#21462;&#20915;&#20110;&#29992;&#25143;&#19982;&#24179;&#21488;&#19978;&#20854;&#20182;&#31867;&#22411;&#20869;&#23481;&#20114;&#21160;&#21518;&#20854;&#20559;&#22909;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#955;&#8712;[0,1]&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#22240;&#20808;&#21069;&#20869;&#23481;&#28040;&#36153;&#32780;&#24555;&#36895;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20851;&#20110;&#40065;&#26834;&#35270;&#35273;&#38382;&#31572;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#21253;&#25324;&#20102;&#25968;&#25454;&#38598;&#30340;&#21457;&#23637;&#36807;&#31243;&#12289;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20808;&#21069;&#30340;&#36890;&#29992;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#24120;&#24120;&#20250;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#38590;&#20197;&#23398;&#20064;&#21040;&#27491;&#30830;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#21435;&#20559;&#24046;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11471</link><description>&lt;p&gt;
&#40065;&#26834;&#35270;&#35273;&#38382;&#31572;&#65306;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#21644;&#26410;&#26469;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Robust Visual Question Answering: Datasets, Methods, and Future Challenges. (arXiv:2307.11471v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11471
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20851;&#20110;&#40065;&#26834;&#35270;&#35273;&#38382;&#31572;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#21253;&#25324;&#20102;&#25968;&#25454;&#38598;&#30340;&#21457;&#23637;&#36807;&#31243;&#12289;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20808;&#21069;&#30340;&#36890;&#29992;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#24120;&#24120;&#20250;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#38590;&#20197;&#23398;&#20064;&#21040;&#27491;&#30830;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#21435;&#20559;&#24046;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#38656;&#35201;&#19968;&#20010;&#31995;&#32479;&#33021;&#22815;&#22312;&#32473;&#23450;&#22270;&#20687;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20934;&#30830;&#30340;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#35748;&#35782;&#21040;&#20808;&#21069;&#30340;&#36890;&#29992;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#24448;&#24448;&#20542;&#21521;&#20110;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#24046;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#27491;&#30830;&#30340;&#34892;&#20026;&#65292;&#20363;&#22914;&#22312;&#39044;&#27979;&#31572;&#26696;&#20043;&#21069;&#23545;&#22270;&#20687;&#36827;&#34892;&#23450;&#20301;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#20998;&#24067;&#20869;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20998;&#24067;&#22806;&#34920;&#29616;&#36739;&#24046;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#21435;&#20559;&#24046;&#26041;&#27861;&#26469;&#20998;&#21035;&#35780;&#20272;&#21644;&#22686;&#24378;&#35270;&#35273;&#38382;&#31572;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#19968;&#20221;&#20851;&#27880;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#20998;&#24067;&#20869;&#21644;&#20998;&#24067;&#22806;&#30340;&#35270;&#35282;&#27010;&#36848;&#20102;&#25968;&#25454;&#38598;&#30340;&#21457;&#23637;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#25968;&#25454;&#38598;&#37319;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#30340;&#21457;&#23637;&#36807;&#31243;&#12289;&#30456;&#20284;&#20043;&#22788;&#21644;&#24046;&#24322;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering requires a system to provide an accurate natural language answer given an image and a natural language question. However, it is widely recognized that previous generic VQA methods often exhibit a tendency to memorize biases present in the training data rather than learning proper behaviors, such as grounding images before predicting answers. Therefore, these methods usually achieve high in-distribution but poor out-of-distribution performance. In recent years, various datasets and debiasing methods have been proposed to evaluate and enhance the VQA robustness, respectively. This paper provides the first comprehensive survey focused on this emerging fashion. Specifically, we first provide an overview of the development process of datasets from in-distribution and out-of-distribution perspectives. Then, we examine the evaluation metrics employed by these datasets. Thirdly, we propose a typology that presents the development process, similarities and differences,
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;&#30340;&#26412;&#36136;&#26159;&#19968;&#31995;&#21015;&#36890;&#36807;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#24314;&#31435;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#26469;&#26368;&#23567;&#21270;&#31995;&#32479;&#29109;&#30340;&#25968;&#23398;&#20989;&#25968;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.11114</link><description>&lt;p&gt;
&#26234;&#33021;&#30340;&#26412;&#36136;
&lt;/p&gt;
&lt;p&gt;
Nature of Intelligence. (arXiv:2307.11114v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11114
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#26412;&#36136;&#26159;&#19968;&#31995;&#21015;&#36890;&#36807;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#24314;&#31435;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#26469;&#26368;&#23567;&#21270;&#31995;&#32479;&#29109;&#30340;&#25968;&#23398;&#20989;&#25968;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#30784;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#22823;&#33041;&#65292;&#20154;&#24037;&#26234;&#33021;&#26500;&#24314;&#20855;&#26377;&#23398;&#20064;&#33021;&#21147;&#24182;&#25191;&#34892;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20219;&#21153;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30001;&#22810;&#20010;&#35745;&#31639;&#23618;&#32452;&#25104;&#65292;&#23398;&#20064;&#25968;&#25454;&#30340;&#34920;&#31034;&#24182;&#22312;&#35768;&#22810;&#35782;&#21035;&#39046;&#22495;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26234;&#33021;&#30340;&#26412;&#36136;&#65292;&#21363;&#36890;&#36807;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#20849;&#21516;&#20195;&#34920;&#30340;&#26234;&#33021;&#30340;&#26412;&#36136;&#65292;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#26234;&#33021;&#30340;&#26412;&#36136;&#26159;&#19968;&#31995;&#21015;&#36890;&#36807;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#24314;&#31435;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#26469;&#26368;&#23567;&#21270;&#31995;&#32479;&#29109;&#30340;&#25968;&#23398;&#20989;&#25968;&#36807;&#31243;&#12290;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#20197;&#19968;&#31181;&#21463;&#24378;&#21270;&#26041;&#24335;&#28040;&#32791;&#33021;&#37327;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#20123;&#20943;&#29109;&#36807;&#31243;&#12290;&#26681;&#25454;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20851;&#20110;&#35821;&#35328;&#12289;&#26080;&#24847;&#35782;&#21644;&#24847;&#35782;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#39044;&#27979;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#24037;&#31243;&#23454;&#29616;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human brain is the substrate for human intelligence. By simulating the human brain, artificial intelligence builds computational models that have learning capabilities and perform intelligent tasks approaching the human level. Deep neural networks consist of multiple computation layers to learn representations of data and improve the state-of-the-art in many recognition domains. However, the essence of intelligence commonly represented by both humans and AI is unknown. Here, we show that the nature of intelligence is a series of mathematically functional processes that minimize system entropy by establishing functional relationships between datasets over space and time. Humans and AI have achieved intelligence by implementing these entropy-reducing processes in a reinforced manner that consumes energy. With this hypothesis, we establish mathematical models of language, unconsciousness and consciousness, predicting the evidence to be found by neuroscience and achieved by AI engineer
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#25104;&#21151;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#21363;&#20351;&#25552;&#31034;&#34987;&#20445;&#23494;&#65292;&#31616;&#21333;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#20173;&#28982;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25581;&#31034;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06865</link><description>&lt;p&gt;
&#25552;&#31034;&#19981;&#24212;&#34987;&#35270;&#20026;&#31192;&#23494;&#65306;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#30340;&#25104;&#21151;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success. (arXiv:2307.06865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#25104;&#21151;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#21363;&#20351;&#25552;&#31034;&#34987;&#20445;&#23494;&#65292;&#31616;&#21333;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#20173;&#28982;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25581;&#31034;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#36890;&#24120;&#36890;&#36807;&#25552;&#31034;&#25216;&#26415;&#26469;&#25511;&#21046;&#65292;&#20854;&#20013;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#26597;&#35810;&#20197;&#26088;&#22312;&#25351;&#23548;&#27169;&#22411;&#22312;&#35813;&#26597;&#35810;&#19978;&#30340;&#34892;&#20026;&#30340;&#25552;&#31034;&#20316;&#20026;&#21069;&#32512;&#12290;&#20844;&#21496;&#29992;&#20110;&#25351;&#23548;&#20854;&#27169;&#22411;&#30340;&#25552;&#31034;&#36890;&#24120;&#34987;&#35270;&#20026;&#31192;&#23494;&#65292;&#38544;&#34255;&#22312;&#26597;&#35810;&#30340;&#29992;&#25143;&#20043;&#22806;&#12290;&#23427;&#20204;&#29978;&#33267;&#34987;&#35270;&#20026;&#21487;&#20197;&#20080;&#21334;&#30340;&#21830;&#21697;&#12290;&#28982;&#32780;&#65292;&#26377;&#32463;&#39564;&#24615;&#30340;&#35777;&#25454;&#26174;&#31034;&#65292;&#21363;&#20351;&#25552;&#31034;&#34987;&#20445;&#23494;&#65292;&#29992;&#25143;&#20173;&#28982;&#21487;&#20197;&#25552;&#21462;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#25104;&#21151;&#30340;&#26694;&#26550;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#28304;&#21644;&#22810;&#20010;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#23454;&#38469;&#19978;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25581;&#31034;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret. In this paper, we present a framework for systematically measuring the success of prompt extraction attacks. In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#25552;&#21319;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;</title><link>http://arxiv.org/abs/2307.05209</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#30340;&#19978;&#19979;&#25991;&#39044;&#35268;&#21010;&#20197;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning. (arXiv:2307.05209v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05209
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#25552;&#21319;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#19988;&#26080;&#27861;&#36866;&#24212;&#36731;&#24494;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;&#20026;&#20102;&#22312;&#36716;&#31227;&#21040;&#26410;&#35265;&#20219;&#21153;&#26102;&#21152;&#24555;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#65288;RM&#65289;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#22870;&#21169;&#26426;&#22120;&#26159;&#22522;&#20110;&#24403;&#21069;&#20219;&#21153;&#30340;&#22870;&#21169;&#21644;&#21160;&#24577;&#29983;&#25104;&#23376;&#20219;&#21153;&#30340;&#29366;&#24577;&#26426;&#25277;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20195;&#29702;&#25552;&#20379;&#20102;&#24403;&#21069;&#25277;&#35937;&#29366;&#24577;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#24182;&#22870;&#21169;&#23427;&#20204;&#36798;&#25104;&#36825;&#20123;&#36716;&#25442;&#12290;&#36825;&#20123;&#34920;&#31034;&#22312;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#20808;&#21069;&#36935;&#21040;&#30340;&#31526;&#21495;&#21644;&#36716;&#25442;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#36801;&#31227;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#34920;&#31034;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RM), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Our empirical evaluation shows that our representations improve sample efficiency and few-shot transfer in a variety of domains.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33136;&#37096;&#20329;&#25140;&#30340;&#21152;&#36895;&#35745;&#33258;&#21160;&#26816;&#27979;&#27493;&#24577;&#20107;&#20214;&#21644;&#34892;&#36208;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#24066;&#21806;&#26234;&#33021;&#25163;&#26426;&#21152;&#36895;&#35745;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#20174;&#24191;&#27867;&#30340;&#27493;&#24577;&#36895;&#24230;&#33539;&#22260;&#20013;&#25552;&#21462;&#27493;&#24577;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#23545;Duchenne&#32908;&#32905;&#33806;&#32553;&#24739;&#20799;&#21644;&#20856;&#22411;&#21457;&#32946;&#27491;&#24120;&#24739;&#32773;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.04866</link><description>&lt;p&gt;
&#20351;&#29992;&#33136;&#37096;&#20329;&#25140;&#30340;&#21152;&#36895;&#35745;&#22312;&#20856;&#22411;&#30340;&#34892;&#36208;&#21644;&#36305;&#27493;&#36895;&#24230;&#33539;&#22260;&#20869;&#33258;&#21160;&#26816;&#27979;&#27493;&#24577;&#20107;&#20214;&#21644;&#34892;&#36208;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Automated Detection of Gait Events and Travel Distance Using Waist-worn Accelerometers Across a Typical Range of Walking and Running Speeds. (arXiv:2307.04866v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04866
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33136;&#37096;&#20329;&#25140;&#30340;&#21152;&#36895;&#35745;&#33258;&#21160;&#26816;&#27979;&#27493;&#24577;&#20107;&#20214;&#21644;&#34892;&#36208;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#24066;&#21806;&#26234;&#33021;&#25163;&#26426;&#21152;&#36895;&#35745;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#20174;&#24191;&#27867;&#30340;&#27493;&#24577;&#36895;&#24230;&#33539;&#22260;&#20013;&#25552;&#21462;&#27493;&#24577;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#23545;Duchenne&#32908;&#32905;&#33806;&#32553;&#24739;&#20799;&#21644;&#20856;&#22411;&#21457;&#32946;&#27491;&#24120;&#24739;&#32773;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20272;&#35745;&#27493;&#24577;&#65288;CFs&#65289;&#30340;&#26102;&#38388;&#31354;&#38388;&#20020;&#24202;&#29305;&#24449;&#65292;&#22914;&#27493;&#25968;&#21644;&#38271;&#24230;&#12289;&#27493;&#38271;&#12289;&#27493;&#39057;&#12289;&#27493;&#36895;&#21644;&#34892;&#36208;&#36317;&#31163;&#31561;&#65292;&#22312;&#20351;&#29992;&#21487;&#31359;&#25140;&#24335;&#21152;&#36895;&#35745;&#36827;&#34892;&#22522;&#20110;&#31038;&#21306;&#30340;&#31227;&#21160;&#24615;&#35780;&#20272;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#22791;&#22797;&#26434;&#24615;&#21644;&#21487;&#29992;&#24615;&#12289;&#25104;&#26412;&#21644;&#20998;&#26512;&#26041;&#27861;&#23398;&#24341;&#36215;&#30340;&#25361;&#25112;&#38480;&#21046;&#20102;&#27492;&#31867;&#24037;&#20855;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#30740;&#31350;&#38382;&#39064;&#65306;&#33021;&#21542;&#20351;&#29992;&#24066;&#21806;&#26234;&#33021;&#25163;&#26426;&#30340;&#21152;&#36895;&#35745;&#25968;&#25454;&#26469;&#25552;&#21462;Duchenne&#32908;&#32905;&#33806;&#32553;&#65288;DMD&#65289;&#24739;&#20799;&#21644;&#20856;&#22411;&#21457;&#32946;&#27491;&#24120;&#65288;TDs&#65289;&#24739;&#32773;&#22312;&#24191;&#27867;&#27493;&#24577;&#36895;&#24230;&#33539;&#22260;&#20869;&#30340;&#27493;&#24577;CFs&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;15&#21517;DMD&#24739;&#20799;&#21644;15&#21517;TDs&#34987;&#35201;&#27714;&#22312;10MRW&#12289;25MRW&#12289;100MRW&#12289;6MWT&#21644;FW&#35780;&#20272;&#20013;&#20197;&#19968;&#31995;&#21015;&#27493;&#24577;&#36895;&#24230;&#36827;&#34892;&#30417;&#30563;&#24615;&#20020;&#24202;&#27979;&#35797;&#65292;&#21516;&#26102;&#20329;&#25140;&#25163;&#26426;&#22522;&#30784;&#21152;&#36895;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Estimation of temporospatial clinical features of gait (CFs), such as step count and length, step duration, step frequency, gait speed and distance traveled is an important component of community-based mobility evaluation using wearable accelerometers. However, challenges arising from device complexity and availability, cost and analytical methodology have limited widespread application of such tools. Research Question: Can accelerometer data from commercially-available smartphones be used to extract gait CFs across a broad range of attainable gait velocities in children with Duchenne muscular dystrophy (DMD) and typically developing controls (TDs) using machine learning (ML)-based methods Methods: Fifteen children with DMD and 15 TDs underwent supervised clinical testing across a range of gait speeds using 10 or 25m run/walk (10MRW, 25MRW), 100m run/walk (100MRW), 6-minute walk (6MWT) and free-walk (FW) evaluations while wearing a mobile phone-based accelerometer at the wa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#25968;&#25454;&#30340;&#20316;&#29992;&#65292;&#30740;&#31350;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#30340;&#26469;&#28304;&#21644;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#26679;&#26412;&#30340;&#23646;&#24615;&#26469;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.17828</link><description>&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#27010;&#24565;&#24433;&#21709;&#29702;&#35299;&#19981;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Unfairness via Training Concept Influence. (arXiv:2306.17828v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17828
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#25968;&#25454;&#30340;&#20316;&#29992;&#65292;&#30740;&#31350;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#30340;&#26469;&#28304;&#21644;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#26679;&#26412;&#30340;&#23646;&#24615;&#26469;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#30340;&#21407;&#22240;&#26377;&#21161;&#20110;&#20174;&#19994;&#20154;&#21592;&#26356;&#22909;&#22320;&#29702;&#35299;&#20182;&#20204;&#30340;&#25968;&#25454;&#21644;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22521;&#35757;&#25968;&#25454;&#36825;&#19968;&#20027;&#35201;&#19981;&#20844;&#24179;&#26469;&#28304;&#30340;&#35270;&#35282;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20197;&#19979;&#38382;&#39064;&#65306;&#22914;&#26524;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26377;&#20123;&#26679;&#26412;&#65288;1&#65289;&#26469;&#33258;&#19981;&#21516;&#30340;&#65288;&#20363;&#22914;&#20154;&#21475;&#32479;&#35745;&#23398;&#65289;&#32676;&#20307;&#65292;&#65288;2&#65289;&#26631;&#35760;&#26041;&#24335;&#19981;&#21516;&#65292;&#25110;&#32773;&#65288;3&#65289;&#26576;&#20123;&#29305;&#24449;&#21457;&#29983;&#20102;&#21464;&#21270;&#65292;&#37027;&#20040;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#34920;&#29616;&#20250;&#21457;&#29983;&#24590;&#26679;&#30340;&#21464;&#21270;&#65311;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#21453;&#20107;&#23454;&#22320;&#23545;&#22522;&#20110;&#39044;&#23450;&#20041;&#27010;&#24565;&#30340;&#26679;&#26412;&#36827;&#34892;&#24178;&#39044;&#21644;&#25913;&#21464;&#65292;&#37327;&#21270;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30456;&#23545;&#20110;&#27010;&#24565;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#22522;&#20110;&#27010;&#24565;&#29983;&#25104;&#21453;&#20107;&#23454;&#29256;&#26412;&#30340;&#26679;&#26412;&#65292;&#21363;&#22914;&#26524;&#27010;&#24565;&#21457;&#29983;&#21464;&#21270;&#65292;&#26679;&#26412;&#30340;&#21453;&#20107;&#23454;&#29256;&#26412;&#12290;&#28982;&#21518;&#25105;&#20204;&#35745;&#31639;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
Knowing the causes of a model's unfairness helps practitioners better understand their data and algorithms. This is an important yet relatively unexplored task. We look into this problem through the lens of the training data - one of the major sources of unfairness. We ask the following questions: how would a model's fairness performance change if, in its training data, some samples (1) were collected from a different (e.g. demographic) group, (2) were labeled differently, or (3) some features were changed? In other words, we quantify the fairness influence of training samples by counterfactually intervening and changing samples based on predefined concepts, i.e. data attributes such as features (X), labels (Y), or sensitive attributes (A). To calculate a training sample's influence on the model's unfairness w.r.t a concept, we first generate counterfactual samples based on the concept, i.e. the counterfactual versions of the sample if the concept were changed. We then calculate the re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.17256</link><description>&lt;p&gt;
&#20197;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#26681;&#25454;&#29992;&#25143;&#36807;&#21435;&#30340;&#34892;&#20026;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#20449;&#24687;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#21382;&#21490;&#20132;&#20114;&#35760;&#24405;&#19981;&#21487;&#29992;&#26102;&#65292;&#24320;&#21457;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#23601;&#26159;&#25152;&#35859;&#30340;&#31995;&#32479;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#12290;&#27492;&#38382;&#39064;&#22312;&#21019;&#19994;&#20225;&#19994;&#25110;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#20013;&#23588;&#20026;&#31361;&#20986;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#29992;&#25143;&#25110;&#29289;&#21697;&#30340;&#20919;&#21551;&#21160;&#22330;&#26223;&#65292;&#20854;&#20013;&#31995;&#32479;&#20173;&#28982;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#20013;&#30340;&#21382;&#21490;&#29992;&#25143;&#21644;&#29289;&#21697;&#20132;&#20114;&#36827;&#34892;&#35757;&#32451;&#26469;&#20026;&#26032;&#29992;&#25143;&#25110;&#29289;&#21697;&#25552;&#20379;&#25512;&#33616;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#25105;&#20204;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#36164;&#26009;&#21644;&#29289;&#21697;&#23646;&#24615;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#31216;&#20026;&#32452;&#27491;&#20132;&#27491;&#21017;&#21270;&#65292;&#33021;&#22815;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#33258;&#36866;&#24212;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#31181;&#27491;&#21017;&#21270;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10001</link><description>&lt;p&gt;
&#32452;&#27491;&#20132;&#27491;&#21017;&#21270;&#65306;&#29992;&#20110;&#35270;&#35273;&#27169;&#22411;&#33258;&#36866;&#24212;&#21644;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Group Orthogonalization Regularization For Vision Models Adaptation and Robustness. (arXiv:2306.10001v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#31216;&#20026;&#32452;&#27491;&#20132;&#27491;&#21017;&#21270;&#65292;&#33021;&#22815;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#33258;&#36866;&#24212;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#31181;&#27491;&#21017;&#21270;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#21464;&#24471;&#36234;&#26469;&#36234;&#28145;&#65292;&#21442;&#25968;&#20869;&#37096;&#30340;&#20887;&#20313;&#24230;&#20063;&#38543;&#20043;&#22686;&#21152;&#12290;&#36825;&#31181;&#29616;&#35937;&#23548;&#33268;&#20102;&#35768;&#22810;&#35797;&#22270;&#20943;&#23569;&#21367;&#31215;&#28388;&#27874;&#22120;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23427;&#40723;&#21169;&#21516;&#19968;&#23618;&#20869;&#28388;&#27874;&#22120;&#32452;&#20043;&#38388;&#30340;&#27491;&#20132;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#32467;&#21512;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65288;ViTs&#65289;&#26102;&#65292;&#36825;&#31181;&#27491;&#21017;&#21270;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#24403;&#22312;&#23545;&#25239;&#35757;&#32451;&#36807;&#31243;&#20013;&#24378;&#21046;&#36827;&#34892;&#32452;&#27491;&#20132;&#24615;&#26102;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/YoavKurtz/GOR&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
As neural networks become deeper, the redundancy within their parameters increases. This phenomenon has led to several methods that attempt to reduce the correlation between convolutional filters. We propose a computationally efficient regularization technique that encourages orthonormality between groups of filters within the same layer. Our experiments show that when incorporated into recent adaptation methods for diffusion models and vision transformers (ViTs), this regularization improves performance on downstream tasks. We further show improved robustness when group orthogonality is enforced during adversarial training. Our code is available at https://github.com/YoavKurtz/GOR.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.02865</link><description>&lt;p&gt;
&#25235;&#20303;&#24847;&#22806;&#25910;&#33719;&#65306;&#22312;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#20215;&#20540;(arXiv:2306.02865v2 [cs.LG]&#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic. (arXiv:2306.02865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39640;&#36136;&#37327;&#30340; Q &#20540;&#20989;&#25968;&#22312;&#35768;&#22810;&#29616;&#20195;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (RL) &#31639;&#27861;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#35299;&#20915;&#37319;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#21644;&#31163;&#32447;&#23398;&#20064;&#25152;&#23548;&#33268;&#30340;&#20540;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#19982;&#36825;&#31181;&#26222;&#36941;&#35266;&#28857;&#19981;&#21516;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040; Q &#20540;&#22312; RL &#35757;&#32451;&#36807;&#31243;&#30340;&#21518;&#26399;&#23454;&#38469;&#19978;&#34987;&#20302;&#20272;&#20102;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#36125;&#23572;&#26364;&#26356;&#26032;&#20013;&#65292;&#24403;&#21069;&#31574;&#30053;&#20351;&#29992;&#27604;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#26356;&#20248;&#30340;&#21160;&#20316;&#26679;&#26412;&#24046;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20010;&#38271;&#26399;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#21487;&#33021;&#38459;&#30861;&#20102;&#31574;&#30053;&#23398;&#20064;&#65292;&#38477;&#20302;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#22312;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#30340;&#21516;&#26102;&#65292;&#32467;&#21512;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#21033;&#29992;&#21644;&#25506;&#32034; (BEE) &#25805;&#20316;&#31526;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21382;&#21490;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#21160;&#20316;&#21644;&#24403;&#21069;&#31574;&#30053;&#29983;&#25104;&#30340;&#21160;&#20316;&#26469;&#26356;&#26032; Q &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning high-quality Q-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that Q-values are indeed underestimated in the latter stage of the RL training process, primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency. Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates Q-value using both historical best-performing actions and
&lt;/p&gt;</description></item><item><title>Med-UniC&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#36328;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#65292;&#23454;&#29616;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#23545;&#40784;&#35268;&#21017;(CTR)&#65292;&#20197;&#26126;&#30830;&#32479;&#19968;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#30340;&#21307;&#23398;&#25253;&#21578;&#30340;&#36328;&#35821;&#35328;&#35821;&#20041;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.19894</link><description>&lt;p&gt;
Med-UniC&#65306;&#36890;&#36807;&#20943;&#23569;&#20559;&#35265;&#23454;&#29616;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;
&lt;/p&gt;
&lt;p&gt;
Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias. (arXiv:2305.19894v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19894
&lt;/p&gt;
&lt;p&gt;
Med-UniC&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#36328;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#65292;&#23454;&#29616;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#23545;&#40784;&#35268;&#21017;(CTR)&#65292;&#20197;&#26126;&#30830;&#32479;&#19968;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#30340;&#21307;&#23398;&#25253;&#21578;&#30340;&#36328;&#35821;&#35328;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#32570;&#24615;&#23545;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#30340;&#25928;&#26524;&#36896;&#25104;&#20102;&#20005;&#37325;&#38556;&#30861;&#12290;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#22312;&#20110;&#32467;&#21512;&#26469;&#33258;&#21508;&#31181;&#35821;&#35328;&#31038;&#21306;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;&#25361;&#25112;&#26469;&#33258;&#20110;&#25972;&#21512;&#19981;&#21516;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#12289;&#29305;&#23450;&#20110;&#35821;&#35328;&#30340;&#21307;&#23398;&#26415;&#35821;&#20197;&#21450;&#29305;&#23450;&#20110;&#25991;&#21270;&#30340;&#38544;&#24335;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#32771;&#34385;&#22240;&#32032;&#26159;&#30001;&#19981;&#21516;&#35821;&#35328;&#24341;&#36215;&#30340;&#31038;&#21306;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#32479;&#19968;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;(Med-UniC)&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#25972;&#21512;&#26469;&#33258;&#20004;&#31181;&#26368;&#24120;&#35265;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#65292;&#21363;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#23545;&#40784;&#35268;&#21017;(CTR)&#65292;&#26126;&#30830;&#32479;&#19968;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#30340;&#21307;&#23398;&#25253;&#21578;&#30340;&#36328;&#35821;&#35328;&#35821;&#20041;&#34920;&#31034;&#12290;&#36890;&#36807;&#28508;&#22312;&#35821;&#35328;&#35299;&#32544;&#65292;&#20248;&#21270;CTR&#65292;&#20351;&#25105;&#20204;&#30340;&#20248;&#21270;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scarcity of data presents a critical obstacle to the efficacy of medical visionlanguage pre-training (VLP). A potential solution lies in the combination of datasets from various language communities. Nevertheless, the main challenge stems from the complexity of integrating diverse syntax and semantics, language-specific medical terminology, and culture-specific implicit knowledge. Therefore, one crucial aspect to consider is the presence of community bias caused by different languages. This paper presents a novel framework named Unifying Cross-Lingual Medical Vision-Language Pre-Training (Med-UniC), designed to integrate multimodal medical data from the two most prevalent languages, English and Spanish. Specifically, we propose Cross-lingual Text Alignment Regularization (CTR) to explicitly unify cross-lingual semantic representations of medical reports originating from diverse language communities. CTR is optimized through latent language disentanglement, rendering our optimizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SpikeGCL&#65292;&#19968;&#31181;&#29992;&#20108;&#20540;&#21270;1&#27604;&#29305;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#33410;&#32422;&#36164;&#28304;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20197;&#36817;32&#20493;&#30340;&#34920;&#31034;&#23384;&#20648;&#21387;&#32553;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.19306</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#20540;&#24471;&#19968;&#27604;&#29305;&#30340;&#24046;&#24322;&#24615;&#65306;&#24403;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#36935;&#21040;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks. (arXiv:2305.19306v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SpikeGCL&#65292;&#19968;&#31181;&#29992;&#20108;&#20540;&#21270;1&#27604;&#29305;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#33410;&#32422;&#36164;&#28304;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20197;&#36817;32&#20493;&#30340;&#34920;&#31034;&#23384;&#20648;&#21387;&#32553;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#20294;&#23545;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#36861;&#27714;&#38656;&#35201;&#22823;&#30340;&#38544;&#34255;&#32500;&#24230;&#26469;&#23398;&#20064;&#20449;&#24687;&#20016;&#23500;&#12289;&#26377;&#21306;&#21035;&#24615;&#30340;&#20840;&#31934;&#24230;&#34920;&#31034;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#35745;&#31639;&#12289;&#23384;&#20648;&#21644;&#33021;&#28304;&#28040;&#32791;&#36127;&#25285;&#65288;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#22823;&#22810;&#34987;&#24573;&#30053;&#65289;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#21363;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#36827;&#34892;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#65292;&#21033;&#29992;&#31232;&#30095;&#21644;&#20108;&#20803;&#29305;&#24615;&#26469;&#23398;&#20064;&#26356;&#20855;&#29983;&#29289;&#21487;&#34892;&#24615;&#21644;&#32039;&#20945;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SpikeGCL&#65292;&#19968;&#31181;&#23398;&#20064;&#22270;&#30340;&#20108;&#20540;&#21270;1&#27604;&#29305;&#34920;&#31034;&#30340;&#26032;&#22411;GCL&#26694;&#26550;&#65292;&#24179;&#34913;&#20102;&#25928;&#29575;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#35777;&#26126;SpikeGCL&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#19982;&#20854;&#20840;&#31934;&#24230;&#23545;&#24212;&#29289;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#34920;&#31034;&#23384;&#20648;&#21387;&#32553;&#36817;32&#20493;&#65292;SpikeGCL&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of high task accuracy requires a large hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This paper explores a promising direction for graph contrastive learning (GCL) with spiking neural networks (SNNs), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compressio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#37096;&#20998;&#20687;&#32032;&#31354;&#38388;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#19982;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#31995;&#21015;&#22522;&#26412;&#29289;&#29702;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#19978;&#19979;&#25991;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18485</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Autoencoding Conditional Neural Processes for Representation Learning. (arXiv:2305.18485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37096;&#20998;&#20687;&#32032;&#31354;&#38388;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#19982;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#31995;&#21015;&#22522;&#26412;&#29289;&#29702;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#19978;&#19979;&#25991;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;(CNPs)&#26159;&#19968;&#31181;&#28789;&#27963;&#39640;&#25928;&#30340;&#27169;&#22411;&#26063;&#32676;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#20540;&#20013;&#23398;&#20064;&#20986;&#19968;&#20010;&#38543;&#26426;&#36807;&#31243;&#12290;&#22312;&#35270;&#35273;&#39046;&#22495;&#20013;&#65292;CNPs &#22312;&#19978;&#19979;&#25991;&#22270;&#20687;&#34917;&#20840;&#20013;&#24471;&#21040;&#20102;&#29305;&#21035;&#30340;&#24212;&#29992;&#65292;&#21363;&#36890;&#36807;&#35266;&#23519;&#26576;&#20123;&#20301;&#32622;&#30340;&#20687;&#32032;&#20540;&#26469;&#39044;&#27979;&#20854;&#20182;&#26410;&#35266;&#23519;&#20301;&#32622;&#19978;&#30340;&#20540;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#36825;&#26679;&#19968;&#20010; CNP &#30340;&#20687;&#32032;&#36873;&#25321;&#36890;&#24120;&#26159;&#38543;&#26426;&#30340;&#25110;&#32773;&#26159;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#32479;&#35745;&#37327;(&#20363;&#22914;&#20687;&#32032;&#26041;&#24046;)&#23548;&#20986;&#30340;&#12290;&#26412;&#25991;&#23558;&#38382;&#39064;&#36716;&#21464;&#19968;&#19979;&#65306;&#19968;&#20010; CNP &#24819;&#35201;&#35266;&#23519;&#21738;&#20123;&#20687;&#32032;&#65311;&#20063;&#23601;&#26159;&#35828;&#65292;&#21738;&#20123;&#20687;&#32032;&#20801;&#35768;&#25311;&#21512; CNP&#65292;&#36825;&#26679;&#30340;&#20687;&#32032;&#33021;&#21578;&#35785;&#25105;&#20204;&#19968;&#20123;&#20851;&#20110;&#28508;&#22312;&#22270;&#20687;&#30340;&#20449;&#24687;&#21527;&#65311;&#23558;&#25552;&#20379;&#32473; CNP &#30340;&#19978;&#19979;&#25991;&#35270;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#19968;&#27425;&#24615;&#21464;&#20998;&#26694;&#26550;&#65292;&#37096;&#20998;&#20687;&#32032;&#31354;&#38388;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(Partical Pixel Space VAE, PPS-VAE)&#65292;&#21516;&#26102;&#39044;&#27979;&#36825;&#20010;&#19978;&#19979;&#25991;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010; CNP&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102; PPS-VAE&#65292;&#21457;&#29616;&#36890;&#36807;&#30456;&#23545;&#22823;&#23567;&#25110;&#21464;&#21270;&#39044;&#27979;&#20687;&#32032;&#30340;&#36873;&#25321;&#21487;&#20197;&#23433;&#25490;&#23398;&#20064;&#65292;&#19988;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#20102;&#19978;&#19979;&#25991;&#39044;&#27979;&#65292;&#24182;&#19988;&#21487;&#20197;&#23545;&#22522;&#26412;&#29289;&#29702;&#21644;&#25991;&#21270;&#27010;&#24565;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional neural processes (CNPs) are a flexible and efficient family of models that learn to learn a stochastic process from observations. In the visual domain, they have seen particular application in contextual image completion - observing pixel values at some locations to predict a distribution over values at other unobserved locations. However, the choice of pixels in learning such a CNP is typically either random or derived from a simple statistical measure (e.g. pixel variance). Here, we turn the problem on its head and ask: which pixels would a CNP like to observe? That is, which pixels allow fitting CNP, and do such pixels tell us something about the underlying image? Viewing the context provided to the CNP as fixed-size latent representations, we construct an amortised variational framework, Partial Pixel Space Variational Autoencoder (PPS-VAE), for predicting this context simultaneously with learning a CNP. We evaluate PPS-VAE on a set of vision datasets, and find that not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36991;&#20813;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24615;&#26469;&#25552;&#39640;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16830</link><description>&lt;p&gt;
&#31163;&#24034;&#65306;&#36229;&#36234;&#26412;&#22320;&#25439;&#22833;&#20989;&#25968;&#30340;&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize. (arXiv:2305.16830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36991;&#20813;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24615;&#26469;&#25552;&#39640;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;&#26159;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#36827;&#34892;&#20915;&#31574;&#21046;&#23450;&#30340;&#26694;&#26550;&#12290;&#23427;&#30340;&#20013;&#24515;&#30740;&#31350;&#38382;&#39064;&#26159;&#65292;&#8220;&#22914;&#20309;&#21033;&#29992;&#20915;&#31574;&#20219;&#21153;&#30340;&#32467;&#26500;&#26469;&#23450;&#21046;&#29305;&#23450;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65311;&#8221;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25429;&#25417;&#36825;&#31181;&#28508;&#22312;&#30340;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#23545;&#36825;&#20123;&#25439;&#22833;&#30340;&#24418;&#24335;&#21644;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#20570;&#20986;&#20102;&#38480;&#21046;&#24615;&#30340;&#20551;&#35774;&#12290;&#36825;&#20123;&#20551;&#35774;&#26082;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#20063;&#22312;&#23454;&#36341;&#20013;&#34987;&#36829;&#21453;&#26102;&#23548;&#33268;&#20102;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#19978;&#36848;&#20551;&#35774;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24615;&#26469;&#25552;&#39640;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#20174;&#25991;&#29486;&#20013;&#30340;&#22235;&#20010;&#39046;&#22495;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#36890;&#24120;&#38656;&#35201;&#27604;&#21487;&#27604;&#26041;&#27861;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predict-then-Optimize is a framework for using machine learning to perform decision-making under uncertainty. The central research question it asks is, "How can the structure of a decision-making task be used to tailor ML models for that specific task?" To this end, recent work has proposed learning task-specific loss functions that capture this underlying structure. However, current approaches make restrictive assumptions about the form of these losses and their impact on ML model behavior. These assumptions both lead to approaches with high computational cost, and when they are violated in practice, poor performance. In this paper, we propose solutions to these issues, avoiding the aforementioned assumptions and utilizing the ML model's features to increase the sample efficiency of learning loss functions. We empirically show that our method achieves state-of-the-art results in four domains from the literature, often requiring an order of magnitude fewer samples than comparable metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14259</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25991;&#29486;&#30340;&#35821;&#22659;&#21270;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#65288;LBD&#65289;&#26088;&#22312;&#36890;&#36807;&#25366;&#25496;&#35770;&#25991;&#24182;&#29983;&#25104;&#20551;&#35774;&#26469;&#21457;&#29616;&#26032;&#30340;&#31185;&#23398;&#30693;&#35782;&#12290;&#26631;&#20934;&#30340;LBD&#20165;&#38480;&#20110;&#39044;&#27979;&#31163;&#25955;&#27010;&#24565;&#20043;&#38388;&#30340;&#20004;&#20004;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#20851;&#32852;&#65289;&#12290;LBD&#36824;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23454;&#39564;&#35774;&#32622;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#35780;&#20272;&#30340;&#29305;&#23450;&#24739;&#32773;&#32676;&#20307;&#65289;&#21644;&#20154;&#31867;&#31185;&#23398;&#23478;&#32771;&#34385;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#21160;&#26426;&#65288;&#20363;&#22914;&#65292;&#25214;&#21040;&#27809;&#26377;&#29305;&#23450;&#21103;&#20316;&#29992;&#30340;&#33647;&#29289;&#20505;&#36873;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#21270;LBD&#65288;C-LBD&#65289;&#34920;&#36848;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65306;&#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31185;&#23398;&#20551;&#35774;&#65292;&#21516;&#26102;&#23558;&#23427;&#20204;&#32852;&#31995;&#21040;&#25511;&#21046;&#20551;&#35774;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#29992;&#33719;&#24471;&#30340;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#24322;&#26500;&#32593;&#32476;&#20013;&#30340;&#8220;&#28789;&#24863;&#8221;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;&#35770;&#25991;&#20013;&#27966;&#29983;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13062</link><description>&lt;p&gt;
GPT4Table&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#21527;&#65311;&#19968;&#39033;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23569;&#26679;&#26412;&#25512;&#29702;&#22120;&#26469;&#35299;&#20915;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#36234;&#26469;&#36234;&#20855;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;&#34920;&#26684;&#65289;&#30340;&#29702;&#35299;&#31243;&#24230;&#36824;&#26377;&#24456;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#22320;&#26041;&#12290;&#23613;&#31649;&#21487;&#20197;&#20351;&#29992;&#34920;&#26684;&#24207;&#21015;&#21270;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;LLMs&#26159;&#21542;&#30495;&#27491;&#33021;&#22815;&#29702;&#35299;&#36825;&#31867;&#25968;&#25454;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;LLMs&#30340;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#65288;SUC&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21019;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19971;&#20010;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26377;&#20854;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#21333;&#20803;&#26684;&#26597;&#25214;&#12289;&#34892;&#26816;&#32034;&#21644;&#22823;&#23567;&#26816;&#27979;&#12290;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#24615;&#33021;&#22240;&#22810;&#31181;&#36755;&#20837;&#36873;&#25321;&#32780;&#24322;&#65292;&#21253;&#25324;&#34920;&#26684;&#36755;&#20837;&#26684;&#24335;&#12289;&#20869;&#23481;&#39034;&#24207;&#12289;&#35282;&#33394;&#25552;&#31034;&#21644;&#20998;&#21306;&#26631;&#35760;&#31561;&#12290;&#26681;&#25454;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#25152;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack of comprehensive studies examining whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We run a series of evaluations on GPT-3.5 and GPT-4. We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we then propose \textit{self-augmentation} for effect
&lt;/p&gt;</description></item><item><title>CodeCompose&#26159;&#19968;&#20010;&#22522;&#20110;InCoder LLM&#30340;AI&#36741;&#21161;&#30340;&#20195;&#30721;&#32534;&#20889;&#24037;&#20855;&#65292;&#24050;&#32463;&#22312;Meta&#20869;&#37096;&#37096;&#32626;&#65292;&#21487;&#22312;10&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#21644;&#20960;&#20010;&#32534;&#30721;&#34920;&#38754;&#19978;&#20351;&#29992;&#12290; CodeCompose&#24050;&#25104;&#21151;&#24110;&#21161;&#25552;&#39640;Meta&#30340;&#24320;&#21457;&#20154;&#21592;&#29983;&#20135;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.12050</link><description>&lt;p&gt;
CodeCompose&#65306;&#22522;&#20110;AI&#36741;&#21161;&#30340;&#20195;&#30721;&#32534;&#20889;&#24037;&#20855;&#30340;&#22823;&#35268;&#27169;&#24037;&#19994;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
CodeCompose: A Large-Scale Industrial Deployment of AI-assisted Code Authoring. (arXiv:2305.12050v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12050
&lt;/p&gt;
&lt;p&gt;
CodeCompose&#26159;&#19968;&#20010;&#22522;&#20110;InCoder LLM&#30340;AI&#36741;&#21161;&#30340;&#20195;&#30721;&#32534;&#20889;&#24037;&#20855;&#65292;&#24050;&#32463;&#22312;Meta&#20869;&#37096;&#37096;&#32626;&#65292;&#21487;&#22312;10&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#21644;&#20960;&#20010;&#32534;&#30721;&#34920;&#38754;&#19978;&#20351;&#29992;&#12290; CodeCompose&#24050;&#25104;&#21151;&#24110;&#21161;&#25552;&#39640;Meta&#30340;&#24320;&#21457;&#20154;&#21592;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#35299;&#38145;&#20102;&#35813;&#25216;&#26415;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#24050;&#32463;&#35777;&#26126;&#29983;&#25104;&#24335;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#39537;&#21160;AI&#22522;&#30784;&#30340;&#20195;&#30721;&#32534;&#20889;&#24037;&#20855;&#65292;&#22312;&#32534;&#20889;&#20195;&#30721;&#26399;&#38388;&#21487;&#20197;&#24314;&#35758;&#25972;&#20010;&#35821;&#21477;&#25110;&#20195;&#30721;&#22359;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;InCoder LLM&#30340;CodeCompose&#65292;&#36825;&#26159;&#19968;&#20010;Meta&#20869;&#37096;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;AI&#36741;&#21161;&#20195;&#30721;&#32534;&#20889;&#24037;&#20855;&#12290;CodeCompose&#22522;&#20110;&#21512;&#24182;&#29983;&#25104;&#33021;&#21147;&#21644;&#21452;&#21521;&#24615;&#30340;LLM&#12290;&#25105;&#20204;&#24050;&#32463;&#23558;CodeCompose&#25193;&#23637;&#21040;Meta&#30340;&#25968;&#20197;&#19975;&#35745;&#30340;&#24320;&#21457;&#20154;&#21592;&#65292;&#22312;10&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#21644;&#20960;&#20010;&#32534;&#30721;&#34920;&#38754;&#19978;&#20351;&#29992;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#22823;&#35268;&#27169;&#24037;&#19994;&#29615;&#22659;&#20013;&#37096;&#32626;&#27492;&#31867;&#24037;&#20855;&#26102;&#20986;&#29616;&#30340;&#29992;&#25143;&#20307;&#39564;&#21644;&#25351;&#26631;&#26041;&#38754;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;CodeCompose&#30340;&#27169;&#22411;&#21644;&#31995;&#32479;&#26550;&#26500;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#23545;CodeCompose&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#25351;&#26631;&#65292;&#24182;&#23637;&#31034;&#23427;&#22914;&#20309;&#24110;&#21161;&#25552;&#39640;Meta&#30340;&#24320;&#21457;&#20154;&#21592;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of large language models (LLMs) has unlocked various applications of this technology in software development. In particular, generative LLMs have been shown to effectively power AI-based code authoring tools that can suggest entire statements or blocks of code during code authoring. In this paper we present CodeCompose, an AI-assisted code authoring tool developed and deployed at Meta internally. CodeCompose is based on the InCoder LLM that merges generative capabilities with bi-directionality. We have scaled up CodeCompose to serve tens of thousands of developers at Meta, across 10+ programming languages and several coding surfaces.  We discuss unique challenges in terms of user experience and metrics that arise when deploying such tools in large-scale industrial settings. We present our experience in making design decisions about the model and system architecture for CodeCompose that addresses these challenges. Finally, we present metrics from our large-scale deployment of C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11854</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577; Web &#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027; Web &#23548;&#33322;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#20381;&#36182;&#25968;&#21313;&#20159;&#27425;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;&#24615;&#20132;&#20114;&#21644;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#35774;&#35745;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#21033;&#29992;&#26469;&#33258;&#20016;&#23500;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33073;&#26426;&#35757;&#32451;&#65292;&#29992;&#20110;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340; Web &#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;&#65292; WebGUM&#65292;&#23427;&#35266;&#23519;&#20102;&#32593;&#39029;&#25130;&#22270;&#21644; HTML &#39029;&#38754;&#65292;&#24182;&#36755;&#20986; Web &#23548;&#33322;&#25805;&#20316;&#65292;&#22914;&#21333;&#20987;&#21644;&#36755;&#20837;&#12290;WebGUM &#26159;&#36890;&#36807;&#32852;&#21512;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#22312;&#22823;&#37327;&#30340;&#28436;&#31034;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;&#22312; MiniWoB &#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#36229;&#36807;&#20043;&#21069;&#26368;&#20339;&#33073;&#26426;&#26041;&#27861; 31.9% &#20197;&#19978;&#65292;&#25509;&#36817;&#23454;&#29616;&#22312;&#32447;&#20132;&#20114;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeformerNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#20302;&#32500;&#34920;&#31034;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#23545;&#29289;&#20307;&#24418;&#29366;&#30340;&#25805;&#32437;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#24037;&#29305;&#24449;&#21644;&#29289;&#20307;&#29305;&#23450;&#30340;&#25511;&#21046;&#27169;&#22411;&#65292;&#21487;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#28436;&#31034;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.04449</link><description>&lt;p&gt;
DeformerNet: &#23398;&#20064;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#21452;&#25163;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
DeformerNet: Learning Bimanual Manipulation of 3D Deformable Objects. (arXiv:2305.04449v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeformerNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#20302;&#32500;&#34920;&#31034;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#23545;&#29289;&#20307;&#24418;&#29366;&#30340;&#25805;&#32437;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#24037;&#29305;&#24449;&#21644;&#29289;&#20307;&#29305;&#23450;&#30340;&#25511;&#21046;&#27169;&#22411;&#65292;&#21487;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#28436;&#31034;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23478;&#24237;&#25252;&#29702;&#21040;&#20179;&#24211;&#37197;&#36865;&#20877;&#21040;&#22806;&#31185;&#25163;&#26415;&#21161;&#29702;&#31561;&#39046;&#22495;&#65292;&#24212;&#29992;&#38656;&#35201;&#26426;&#22120;&#20154;&#21487;&#38752;&#22320;&#25805;&#32437;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#24418;&#29366;&#12290;&#24377;&#24615;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#20998;&#26512;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#21442;&#25968;&#26469;&#25551;&#36848;&#20915;&#23450;&#29289;&#20307;&#24418;&#29366;&#30340;&#21487;&#33021;&#26080;&#38480;&#33258;&#30001;&#24230;&#12290;&#20197;&#24448;&#30340;3D&#24418;&#29366;&#25511;&#21046;&#23581;&#35797;&#20381;&#36182;&#20110;&#25163;&#24037;&#29305;&#24449;&#26469;&#34920;&#31034;&#29289;&#20307;&#24418;&#29366;&#65292;&#24182;&#38656;&#35201;&#35757;&#32451;&#29289;&#20307;&#29305;&#23450;&#30340;&#25511;&#21046;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#22411;DeformerNet&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#26550;&#26500;&#22312;&#34987;&#25805;&#32437;&#29289;&#20307;&#30340;&#37096;&#20998;&#35270;&#22270;&#28857;&#20113;&#21644;&#30446;&#26631;&#24418;&#29366;&#30340;&#28857;&#20113;&#19978;&#36816;&#34892;&#65292;&#23398;&#20064;&#29289;&#20307;&#24418;&#29366;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#36825;&#20010;&#24418;&#29366;&#23884;&#20837;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23398;&#20064;&#19968;&#31181;&#35270;&#35273;&#20282;&#26381;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#35745;&#31639;&#20986;&#25152;&#38656;&#30340;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#21160;&#20316;&#65292;&#23558;&#29289;&#20307;&#36845;&#20195;&#22320;&#21464;&#24418;&#21521;&#30446;&#26631;&#24418;&#29366;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#28436;&#31034;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications in fields ranging from home care to warehouse fulfillment to surgical assistance require robots to reliably manipulate the shape of 3D deformable objects. Analytic models of elastic, 3D deformable objects require numerous parameters to describe the potentially infinite degrees of freedom present in determining the object's shape. Previous attempts at performing 3D shape control rely on hand-crafted features to represent the object shape and require training of object-specific control models. We overcome these issues through the use of our novel DeformerNet neural network architecture, which operates on a partial-view point cloud of the manipulated object and a point cloud of the goal shape to learn a low-dimensional representation of the object shape. This shape embedding enables the robot to learn a visual servo controller that computes the desired robot end-effector action to iteratively deform the object toward the target shape. We demonstrate both in simulation and on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;MLCopilot&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#25193;&#23637;&#20854;&#33021;&#21147;&#20197;&#29702;&#35299;&#32467;&#26500;&#21270;&#36755;&#20837;&#24182;&#36827;&#34892;&#28145;&#20837;&#25512;&#29702;&#20197;&#35299;&#20915;&#26032;&#22411;ML&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;MLCopilot&#22312;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#65292;&#25991;&#26412;&#20998;&#31867;&#21644;&#34920;&#26684;&#20998;&#31867;&#19977;&#39033;&#20219;&#21153;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14979</link><description>&lt;p&gt;
MLCopilot&#65306;&#37322;&#25918;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks. (arXiv:2304.14979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;MLCopilot&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#25193;&#23637;&#20854;&#33021;&#21147;&#20197;&#29702;&#35299;&#32467;&#26500;&#21270;&#36755;&#20837;&#24182;&#36827;&#34892;&#28145;&#20837;&#25512;&#29702;&#20197;&#35299;&#20915;&#26032;&#22411;ML&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;MLCopilot&#22312;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#65292;&#25991;&#26412;&#20998;&#31867;&#21644;&#34920;&#26684;&#20998;&#31867;&#19977;&#39033;&#20219;&#21153;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#22240;&#27492;&#36880;&#28176;&#24341;&#21457;&#20102;&#23558;ML&#24212;&#29992;&#20110;&#29305;&#23450;&#22330;&#26223;&#30340;&#38656;&#27714;&#65292;&#20294;&#23454;&#29616;&#36215;&#26469;&#32791;&#26102;&#19988;&#19981;&#26131;&#12290; &#33258;&#21160;&#21270;&#35299;&#20915;ML&#20219;&#21153;&#65288;&#20363;&#22914;AutoML&#65289;&#30340;&#20027;&#35201;&#26041;&#27861;&#36890;&#24120;&#32791;&#36153;&#26102;&#38388;&#19988;&#38590;&#20197;&#29702;&#35299;&#12290; &#32780;&#19982;&#20043;&#30456;&#21453;&#65292;&#34429;&#28982;&#20154;&#31867;&#24037;&#31243;&#24072;&#20855;&#26377;&#29702;&#35299;&#20219;&#21153;&#21644;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#30340;&#38590;&#20197;&#32622;&#20449;&#30340;&#33021;&#21147;&#65292;&#20294;&#20182;&#20204;&#30340;&#32463;&#39564;&#21644;&#30693;&#35782;&#24448;&#24448;&#19981;&#20805;&#20998;&#19988;&#38590;&#20197;&#20511;&#21161;&#23450;&#37327;&#26041;&#27861;&#21033;&#29992;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;MLCopilot&#26469;&#24357;&#21512;&#26426;&#22120;&#26234;&#33021;&#21644;&#20154;&#31867;&#30693;&#35782;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;LLM&#26469;&#24320;&#21457;&#26032;&#22411;&#20219;&#21153;&#30340;ML&#35299;&#20915;&#26041;&#26696;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#25193;&#23637;LLM&#30340;&#33021;&#21147;&#20197;&#29702;&#35299;&#32467;&#26500;&#21270;&#36755;&#20837;&#24182;&#36827;&#34892;&#28145;&#20837;&#25512;&#29702;&#20197;&#35299;&#20915;&#26032;&#22411;ML&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290; &#32463;&#36807;&#19968;&#20123;&#19987;&#38376;&#35774;&#35745;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#21487;&#20197;&#65288;i&#65289;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#20214;&#20013;&#35266;&#23519;&#29616;&#26377;&#30693;&#35782;&#65292;&#65288;ii&#65289;&#21046;&#23450;&#35299;&#20915;ML&#20219;&#21153;&#30340;&#20855;&#20307;&#27493;&#39588;&#12290; &#25105;&#20204;&#23545;&#22270;&#20687;&#20998;&#31867;&#65292;&#25991;&#26412;&#20998;&#31867;&#21644;&#34920;&#26684;&#20998;&#31867;&#19977;&#39033;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;MLCopilot&#22312;&#35299;&#20915;&#23454;&#38469;ML&#38382;&#39064;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of machine learning (ML) has gained widespread adoption, leading to a significant demand for adapting ML to specific scenarios, which is yet expensive and non-trivial. The predominant approaches towards the automation of solving ML tasks (e.g., AutoML) are often time consuming and hard to understand for human developers. In contrast, though human engineers have the incredible ability to understand tasks and reason about solutions, their experience and knowledge are often sparse and difficult to utilize by quantitative approaches. In this paper, we aim to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot, which leverages the state-of-the-art LLMs to develop ML solutions for novel tasks. We showcase the possibility of extending the capability of LLMs to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks. And we find that, after some dedicated design, the LLM can (i) observe from the exi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#20316;&#20026;&#24773;&#24863;&#20998;&#26512;&#22120;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#21253;&#25324;&#26631;&#20934;&#35780;&#20272;&#12289;&#26497;&#24615;&#36716;&#31227;&#35780;&#20272;&#12289;&#24320;&#25918;&#22495;&#35780;&#20272;&#21644;&#24773;&#24863;&#25512;&#29702;&#35780;&#20272;&#65292;&#20849;&#28041;&#21450;18&#20010;&#25968;&#25454;&#38598;&#21644;5&#20010;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;&#19982;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#21644;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.04339</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#24773;&#24863;&#20998;&#26512;&#22120;&#21527;&#65311;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study. (arXiv:2304.04339v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#20316;&#20026;&#24773;&#24863;&#20998;&#26512;&#22120;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#21253;&#25324;&#26631;&#20934;&#35780;&#20272;&#12289;&#26497;&#24615;&#36716;&#31227;&#35780;&#20272;&#12289;&#24320;&#25918;&#22495;&#35780;&#20272;&#21644;&#24773;&#24863;&#25512;&#29702;&#35780;&#20272;&#65292;&#20849;&#28041;&#21450;18&#20010;&#25968;&#25454;&#38598;&#21644;5&#20010;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;&#19982;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#21644;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;ChatGPT&#22312;&#30740;&#31350;&#21644;&#20844;&#20247;&#30340;&#20851;&#27880;&#19979;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#29305;&#21035;&#24819;&#30693;&#36947;&#23427;&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#22120;&#12290;&#20026;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#22312;&#25991;&#26412;&#20013;&#21253;&#21547;&#30340;&#24847;&#35265;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#30340;&#29702;&#35299;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#35774;&#32622;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#26631;&#20934;&#35780;&#20272;&#12289;&#26497;&#24615;&#36716;&#31227;&#35780;&#20272;&#12289;&#24320;&#25918;&#22495;&#35780;&#20272;&#21644;&#24773;&#24863;&#25512;&#29702;&#35780;&#20272;&#12290;&#20197;&#19978;&#35780;&#20272;&#28041;&#21450;18&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;5&#20010;&#20195;&#34920;&#24615;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;ChatGPT&#19982;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#21644;&#30456;&#24212;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#24182;&#22312;&#26411;&#31471;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#23450;&#24615;&#26696;&#20363;&#30740;&#31350;&#20197;&#28145;&#20837;&#29702;&#35299;&#20854;&#24773;&#24863;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly curious about whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of opinions, sentiments, and emotions contained in the text. Specifically, we evaluate it in four settings, including standard evaluation, polarity shift evaluation, open-domain evaluation, and sentiment inference evaluation. The above evaluation involves 18 benchmark datasets and 5 representative sentiment analysis tasks, and we compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on end-task. Moreover, we also conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#19981;&#21487;&#38752;&#12290;&#25913;&#20889;&#25915;&#20987;&#21487;&#20197;&#30772;&#35299;&#22810;&#31181;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#65292;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21487;&#38752;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.11156</link><description>&lt;p&gt;
AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#21542;&#21487;&#38752;&#22320;&#26816;&#27979;&#20986;&#26469;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI-Generated Text be Reliably Detected?. (arXiv:2303.11156v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#19981;&#21487;&#38752;&#12290;&#25913;&#20889;&#25915;&#20987;&#21487;&#20197;&#30772;&#35299;&#22810;&#31181;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#65292;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21487;&#38752;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23454;&#35777;&#21644;&#29702;&#35770;&#20004;&#20010;&#26041;&#38754;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#24182;&#19981;&#21487;&#38752;&#12290;&#20174;&#23454;&#36341;&#19978;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36731;&#37327;&#32423;&#30340;&#25913;&#20889;&#22120;&#24212;&#29992;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#21487;&#20197;&#30772;&#35299;&#19968;&#31995;&#21015;&#30340;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#20351;&#29992;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26088;&#22312;&#36530;&#36991;&#25913;&#20889;&#25915;&#20987;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26816;&#27979;&#22120;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36882;&#24402;&#25913;&#20889;&#30340;&#25915;&#20987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#65292;&#25351;&#20986;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#26356;&#25797;&#38271;&#27169;&#20223;&#20154;&#31867;&#25991;&#26412;&#65292;&#22312;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#23545;&#20110;&#19968;&#20010;&#36275;&#22815;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#27169;&#20223;&#20154;&#31867;&#25991;&#26412;&#65292;&#21363;&#20351;&#26368;&#20339;&#30340;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#21482;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#22909;&#19978;&#19968;&#28857;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36275;&#22815;&#27010;&#25324;&#29305;&#23450;&#30340;&#22330;&#26223;&#65292;&#22914;&#25913;&#20889;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, both empirically and theoretically, we show that several AI-text detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of a large language model (LLM), can break a whole range of detectors, including ones using watermarking schemes as well as neural network-based detectors and zero-shot classifiers. Our experiments demonstrate that retrieval-based detectors, designed to evade paraphrasing attacks, are still vulnerable to recursive paraphrasing. We then provide a theoretical impossibility result indicating that as language models become more sophisticated and better at emulating human text, the performance of even the best-possible detector decreases. For a sufficiently advanced language model seeking to imitate human text, even the best-possible detector may only perform marginally better than a random classifier. Our result is general enough to capture specific scenarios such as par
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26500;&#24314;&#20102;&#20004;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;-&#35270;&#35273;&#23545;&#35805;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#29305;&#23450;&#35268;&#21017;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#35753;&#31995;&#32479;&#23637;&#31034;&#21487;&#36861;&#28335;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05983</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#25351;&#20196;&#25298;&#32477;&#22270;&#20687;&#20877;&#21019;&#20316;&#30340;&#25991;&#26412;-&#35270;&#35273;&#23545;&#35805;&#21487;&#36861;&#28335;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation. (arXiv:2303.05983v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26500;&#24314;&#20102;&#20004;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;-&#35270;&#35273;&#23545;&#35805;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#29305;&#23450;&#35268;&#21017;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#35753;&#31995;&#32479;&#23637;&#31034;&#21487;&#36861;&#28335;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#21644;GPT-4&#30340;&#25104;&#21151;&#24341;&#36215;&#20102;&#23545;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#32570;&#20047;&#21487;&#20197;&#39564;&#35777;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;-&#35270;&#35273;&#23545;&#35805;&#20219;&#21153;&#20013;&#22810;&#27169;&#24577;&#29983;&#25104;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65306;&#21512;&#25104;CLEVR-ATVC&#25968;&#25454;&#38598;&#65288;620K&#65289;&#21644;&#25163;&#21160;&#32472;&#21046;&#30340;Fruit-ATVC&#25968;&#25454;&#38598;&#65288;50K&#65289;&#65292;&#22343;&#20855;&#26377;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#20026;&#20102;&#35753;&#22810;&#27169;&#24577;&#31995;&#32479;&#33021;&#22815;&#25298;&#32477;&#20154;&#31867;&#35831;&#27714;&#65288;&#21363;&#23637;&#31034;&#21487;&#36861;&#28335;&#24615;&#65289;&#65292;&#25105;&#20204;&#22312;&#25968;&#25454;&#38598;&#20013;&#24320;&#21457;&#21644;&#24182;&#20837;&#20102;&#29305;&#23450;&#35268;&#21017;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#20801;&#35768;&#35757;&#32451;&#21518;&#30340;VLM&#22312;&#35270;&#35273;&#21644;&#25991;&#26412;&#25512;&#29702;&#21518;&#25552;&#20379;yes&#25110;no&#30340;&#31572;&#26696;&#65292;&#24182;&#38468;&#24102;&#35828;&#26126;&#35821;&#35328;&#20026;&#20160;&#20040;&#26080;&#27861;&#25191;&#34892;&#20154;&#31867;&#25351;&#20196;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#31243;&#24207;&#26469;&#35757;&#32451;&#22270;&#20687;&#33258;&#32534;&#30721;&#22120;&#21644;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, the academia community lacks a dataset that can validate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we construct two new multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K), both featuring visual and text-based inputs and outputs. Additionally, to enable the multimodal system to reject human requests (i.e., demonstrate accountability), as in language-based ChatGPT conversations, we develop and incorporate specific rules into the datasets as supervisory signals. This allows the trained VLM to provide a yes or no answer after visual and textual reasoning, accompanied by a language explanation as to why the human instruction cannot be excuted. In our method, we propose a two-state training procedure to train the image auto-encoder and auto-regress
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#24120;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#65292;&#25506;&#31350;&#19981;&#21516;&#35780;&#20272;&#24230;&#37327;&#19979;&#30340;&#34920;&#29616;&#20197;&#21450;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#21457;&#29616;&#26041;&#27861;&#30340;&#34920;&#29616;&#32463;&#24120;&#19981;&#19968;&#33268;&#19988;&#36873;&#25321;&#35780;&#20272;&#24230;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2302.10764</link><description>&lt;p&gt;
&#20851;&#20110;&#35270;&#35273;&#35299;&#37322;&#23450;&#37327;&#35780;&#20272;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On The Coherence of Quantitative Evaluation of Visual Explanations. (arXiv:2302.10764v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#24120;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#65292;&#25506;&#31350;&#19981;&#21516;&#35780;&#20272;&#24230;&#37327;&#19979;&#30340;&#34920;&#29616;&#20197;&#21450;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#21457;&#29616;&#26041;&#27861;&#30340;&#34920;&#29616;&#32463;&#24120;&#19981;&#19968;&#33268;&#19988;&#36873;&#25321;&#35780;&#20272;&#24230;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#35270;&#35273;&#35299;&#37322;&#26469;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#22686;&#24378;&#21457;&#23637;&#12290;&#36825;&#20123;&#35299;&#37322;&#36890;&#24120;&#37319;&#29992;&#28909;&#22270;&#30340;&#24418;&#24335;&#65292;&#20026;&#36755;&#20837;&#22270;&#20687;&#30340;&#27599;&#20010;&#20687;&#32032;&#20998;&#37197;&#19968;&#20010;&#26174;&#33879;&#24615;&#20540;&#65292;&#34920;&#31034;&#20687;&#32032;&#23545;&#26631;&#31614;&#39044;&#27979;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#12290;&#19968;&#20123;&#36825;&#26679;&#30340;&#35780;&#20272;&#26041;&#27861;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20294;&#36825;&#26679;&#20250;&#24341;&#20837;&#22312;&#26356;&#29616;&#23454;&#30340;&#24773;&#26223;&#19979;&#36866;&#29992;&#24615;&#30340;&#26377;&#38480;&#20445;&#35777;&#12290;&#21478;&#19968;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#23458;&#35266;&#35780;&#20272;&#30340;&#24230;&#37327;&#12290;&#20294;&#26159;&#26377;&#20851;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#30340;&#25191;&#34892;&#27700;&#24179;&#30340;&#19981;&#30830;&#23450;&#24615;&#24456;&#22823;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;ImageNet-1k&#39564;&#35777;&#38598;&#30340;&#19968;&#20010;&#23376;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#20351;&#29992;&#22810;&#20010;&#35780;&#20272;&#24230;&#37327;&#26469;&#35780;&#20272;&#19981;&#21516;&#30340;&#24120;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#19981;&#21516;&#30340;&#35780;&#20272;&#35774;&#32622;&#19979;&#21508;&#20010;&#26041;&#27861;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#22914;&#20309;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25152;&#20351;&#29992;&#30340;&#35780;&#20272;&#24230;&#37327;&#19978;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#34920;&#29616;&#32463;&#24120;&#26159;&#19981;&#19968;&#33268;&#30340;&#65292;&#32780;&#19988;&#22312;&#35266;&#23519;&#30340;&#34920;&#29616;&#20013;&#65292;&#36873;&#25321;&#35780;&#20272;&#24230;&#37327;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have shown an increased development of methods for justifying the predictions of neural networks through visual explanations. These explanations usually take the form of heatmaps which assign a saliency (or relevance) value to each pixel of the input image that expresses how relevant the pixel is for the prediction of a label.  Complementing this development, evaluation methods have been proposed to assess the "goodness" of such explanations. On the one hand, some of these methods rely on synthetic datasets. However, this introduces the weakness of having limited guarantees regarding their applicability on more realistic settings. On the other hand, some methods rely on metrics for objective evaluation. However the level to which some of these evaluation methods perform with respect to each other is uncertain.  Taking this into account, we conduct a comprehensive study on a subset of the ImageNet-1k validation set where we evaluate a number of different commonly-used expla
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#20102;&#22721;&#38754;&#36793;&#30028;&#23618;&#28237;&#27969;&#20013;&#30340;&#36895;&#24230;&#22330;&#65292;&#24182;&#21033;&#29992;SHAP&#31639;&#27861;&#35780;&#20272;&#20102;&#30456;&#24178;&#32467;&#26500;&#23545;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#19968;&#36807;&#31243;&#25110;&#26377;&#21161;&#20110;&#35299;&#20915;&#28237;&#27969;&#30740;&#31350;&#20013;&#30340;&#38590;&#39064;&#65292;&#20026;&#28237;&#27969;&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2302.01250</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#35299;&#37322;&#22721;&#38754;&#36793;&#30028;&#23618;&#28237;&#27969;
&lt;/p&gt;
&lt;p&gt;
Explaining wall-bounded turbulence through deep learning. (arXiv:2302.01250v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#20102;&#22721;&#38754;&#36793;&#30028;&#23618;&#28237;&#27969;&#20013;&#30340;&#36895;&#24230;&#22330;&#65292;&#24182;&#21033;&#29992;SHAP&#31639;&#27861;&#35780;&#20272;&#20102;&#30456;&#24178;&#32467;&#26500;&#23545;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#19968;&#36807;&#31243;&#25110;&#26377;&#21161;&#20110;&#35299;&#20915;&#28237;&#27969;&#30740;&#31350;&#20013;&#30340;&#38590;&#39064;&#65292;&#20026;&#28237;&#27969;&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22721;&#38754;&#36793;&#30028;&#23618;&#28237;&#27969;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#37325;&#22823;&#31185;&#23398;&#21644;&#25216;&#26415;&#24847;&#20041;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#23547;&#27714;&#26032;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;&#27969;&#22330;&#20013;&#30456;&#24178;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#28237;&#27969;&#36890;&#36947;&#20013;&#30340;&#30636;&#26102;&#36895;&#24230;&#22330;&#39044;&#27979;&#20102;&#26102;&#38388;&#20869;&#30340;&#36895;&#24230;&#22330;&#65292;&#28982;&#21518;&#21033;&#29992;SHapley Additive exPlanations&#65288;SHAP&#65289;&#31639;&#27861;&#23545;&#27599;&#20010;&#32467;&#26500;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#19982;&#20808;&#21069;&#25991;&#29486;&#35266;&#23519;&#32467;&#26524;&#19968;&#33268;&#65292;&#24182;&#36890;&#36807;&#37327;&#21270;&#38647;&#35834;&#24212;&#21147;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#25214;&#21040;&#20102;&#36825;&#20123;&#32467;&#26500;&#19982;&#27969;&#21160;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#21487;&#33021;&#26377;&#21161;&#20110;&#25581;&#31034;&#22721;&#38754;&#36793;&#30028;&#23618;&#28237;&#27969;&#30340;&#38271;&#26399;&#38382;&#39064;&#65292;&#24182;&#20026;&#28237;&#27969;&#27169;&#22411;&#30340;&#24320;&#21457;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its great scientific and technological importance, wall-bounded turbulence is an unresolved problem that requires new perspectives to be tackled. One of the key strategies has been to study interactions among the coherent structures in the flow. Such interactions are explored in this study for the first time using an explainable deep-learning method. The instantaneous velocity field in a turbulent channel is used to predict the velocity field in time through a convolutional neural network. Based on the predicted flow, we assess the importance of each structure for this prediction using the game-theoretic algorithm of SHapley Additive exPlanations (SHAP). This work provides results in agreement with previous observations in the literature and extends them by quantifying the importance of the Reynolds-stress structures, finding a connection between these structures and the dynamics of the flow. The process, based on deep-learning explainability, has the potential to shed light on
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#20272;&#35745;&#24178;&#39044;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#26102;&#38388;&#21464;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#20851;&#31995;&#21644;&#21327;&#21464;&#37327;&#21453;&#20107;&#23454;&#39044;&#27979;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.01900</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#20272;&#35745;&#21453;&#20107;&#23454;&#27835;&#30103;&#32467;&#26524;&#30340;&#26102;&#38388;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Estimating counterfactual treatment outcomes over time in complex multi-agent scenarios. (arXiv:2206.01900v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#20272;&#35745;&#24178;&#39044;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#26102;&#38388;&#21464;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#20851;&#31995;&#21644;&#21327;&#21464;&#37327;&#21453;&#20107;&#23454;&#39044;&#27979;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#24037;&#31243;&#21644;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#24178;&#39044;&#34892;&#20026;&#65288;&#20363;&#22914;&#65292;&#20154;&#31867;&#20309;&#26102;&#24212;&#35813;&#24178;&#39044;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65292;&#20309;&#26102;&#29699;&#21592;&#24212;&#35813;&#20256;&#32473;&#38431;&#21451;&#36827;&#34892;&#22909;&#23556;&#38376;&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20351;&#29992;&#21453;&#20107;&#23454;&#30340;&#38271;&#26399;&#39044;&#27979;&#26469;&#20272;&#35745;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65288;ITE&#65289;&#26159;&#35780;&#20272;&#27492;&#31867;&#24178;&#39044;&#25514;&#26045;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20256;&#32479;&#26694;&#26550;&#27809;&#26377;&#32771;&#34385;&#21040;&#22810;&#26234;&#33021;&#20307;&#20851;&#31995;&#30340;&#26102;&#38388;&#21464;&#21270;&#21644;&#21327;&#21464;&#37327;&#21453;&#20107;&#23454;&#39044;&#27979;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;ITE&#30340;&#38169;&#35823;&#35780;&#20272;&#21644;&#35299;&#37322;&#22256;&#38590;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20272;&#35745;&#24178;&#39044;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#22270;&#24418;&#21464;&#20998;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#35745;&#31639;&#26469;&#36827;&#34892;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#21464;&#37327;&#21644;&#32467;&#26524;&#30340;&#38271;&#26399;&#39044;&#27979;&#30340;ITE&#20272;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#30830;&#35748;&#24490;&#29615;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of intervention in a multi-agent system, e.g., when humans should intervene in autonomous driving systems and when a player should pass to teammates for a good shot, is challenging in various engineering and scientific fields. Estimating the individual treatment effect (ITE) using counterfactual long-term prediction is practical to evaluate such interventions. However, most of the conventional frameworks did not consider the time-varying complex structure of multi-agent relationships and covariate counterfactual prediction. This may lead to erroneous assessments of ITE and difficulty in interpretation. Here we propose an interpretable, counterfactual recurrent network in multi-agent systems to estimate the effect of the intervention. Our model leverages graph variational recurrent neural networks and theory-based computation with domain knowledge for the ITE estimation framework based on long-term prediction of multi-agent covariates and outcomes, which can confirm the circu
&lt;/p&gt;</description></item></channel></rss>