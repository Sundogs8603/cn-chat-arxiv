<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#21333;&#35789;&#21024;&#38500;&#26469;&#32531;&#35299;&#22240;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#35265;&#32780;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.14794</link><description>&lt;p&gt;
&#30465;&#24515;&#23398;&#20064;&#21464;&#24471;&#39046;&#20808;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#31616;&#21333;&#31181;&#23376;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification. (arXiv:2305.14794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#21333;&#35789;&#21024;&#38500;&#26469;&#32531;&#35299;&#22240;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#35265;&#32780;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#22797;&#26434;&#30340;&#26041;&#27861;&#65292;&#23558;&#39640;&#23618;&#27425;&#30340;&#20154;&#31867;&#21551;&#21457;&#24335;&#26041;&#27861;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#29983;&#25104;&#20266;&#26631;&#31614;&#30340;&#26368;&#31616;&#21333;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#31181;&#23376;&#21305;&#37197;&#30340;&#26377;&#38480;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#31181;&#23376;&#21305;&#37197;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#24046;&#65292;&#36825;&#20250;&#38459;&#27490;&#20998;&#31867;&#22120;&#23398;&#20064;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#31616;&#21333;&#22320;&#21024;&#38500;&#21305;&#37197;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#31181;&#23376;&#35789;&#21487;&#20197;&#32531;&#35299;&#26631;&#31614;&#20559;&#24046;&#24182;&#24110;&#21161;&#23398;&#20064;&#26356;&#22909;&#30340;&#32622;&#20449;&#24230;&#12290;&#38543;&#21518;&#65292;&#31181;&#23376;&#21305;&#37197;&#30340;&#24615;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#65292;&#20351;&#23427;&#36798;&#21040;&#25110;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22788;&#29702;&#31181;&#23376;&#35789;&#19981;&#20026;&#20154;&#30693;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24314;&#35758;&#31616;&#21333;&#22320;&#21024;&#38500;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#21333;&#35789;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in weakly supervised text classification mostly focus on designing sophisticated methods to turn high-level human heuristics into quality pseudo-labels. In this paper, we revisit the seed matching-based method, which is arguably the simplest way to generate pseudo-labels, and show that its power was greatly underestimated. We show that the limited performance of seed matching is largely due to the label bias injected by the simple seed-match rule, which prevents the classifier from learning reliable confidence for selecting high-quality pseudo-labels. Interestingly, simply deleting the seed words present in the matched input texts can mitigate the label bias and help learn better confidence. Subsequently, the performance achieved by seed matching can be improved significantly, making it on par with or even better than the state-of-the-art. Furthermore, to handle the case when the seed words are not made known, we propose to simply delete the word tokens in the input tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#27573;&#33853;&#32423;&#20013;&#25991;&#20027;&#39064;&#32467;&#26500;&#34920;&#31034;&#65292;&#20351;&#29992;&#21477;&#23376;&#32780;&#19981;&#26159;&#20851;&#38190;&#35789;&#26469;&#34920;&#31034;&#23376;&#20027;&#39064;&#65292;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2305.14790</link><description>&lt;p&gt;
&#25552;&#21319;&#20013;&#25991;&#25991;&#26412;&#20027;&#39064;&#21010;&#20998;&#21644;&#32434;&#35201;&#29983;&#25104;&#65306;&#27573;&#33853;&#32423;&#20027;&#39064;&#34920;&#31034;&#65292;&#35821;&#26009;&#24211;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark. (arXiv:2305.14790v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#27573;&#33853;&#32423;&#20013;&#25991;&#20027;&#39064;&#32467;&#26500;&#34920;&#31034;&#65292;&#20351;&#29992;&#21477;&#23376;&#32780;&#19981;&#26159;&#20851;&#38190;&#35789;&#26469;&#34920;&#31034;&#23376;&#20027;&#39064;&#65292;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#21010;&#20998;&#21644;&#32434;&#35201;&#29983;&#25104;&#26088;&#22312;&#23558;&#19968;&#20010;&#25991;&#26723;&#20998;&#25104;&#36830;&#36143;&#30340;&#20027;&#39064;&#27573;&#33853;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#23376;&#26631;&#39064;&#12290;&#36825;&#20010;&#36807;&#31243;&#25581;&#31034;&#20102;&#19968;&#20010;&#25991;&#26723;&#30340;&#35805;&#39064;&#32467;&#26500;&#65292;&#26377;&#21161;&#20110;&#20174;&#26356;&#39640;&#30340;&#23618;&#27425;&#24555;&#36895;&#25226;&#25569;&#21644;&#29702;&#35299;&#25991;&#26723;&#30340;&#25972;&#20307;&#24773;&#22659;&#12290;&#28982;&#32780;&#65292;&#19982;&#33521;&#35821;&#39046;&#22495;&#21462;&#24471;&#30340;&#25104;&#21151;&#30456;&#27604;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#27573;&#33853;&#32423;&#20027;&#39064;&#34920;&#31034;&#21644;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#35821;&#26009;&#24211;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#34920;&#31034;&#65292;&#21253;&#25324;&#26631;&#39064;&#12289;&#23376;&#26631;&#39064;&#21644;&#27573;&#33853;&#65292;&#32508;&#21512;&#22320;&#27169;&#25311;&#20102;&#25991;&#26723;&#30340;&#35805;&#39064;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#21477;&#23376;&#32780;&#19981;&#26159;&#20851;&#38190;&#35789;&#26469;&#34920;&#31034;&#23376;&#20027;&#39064;&#65292;&#30830;&#20445;&#26356;&#20840;&#38754;&#22320;&#34920;&#31034;&#25991;&#26723;&#20869;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#26681;&#25454;&#36825;&#31181;&#34920;&#31034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#26368;&#22823;&#30340;&#20013;&#25991;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#35821;&#26009;&#24211;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic segmentation and outline generation strive to divide a document into coherent topic sections and generate corresponding subheadings. Such a process unveils the discourse topic structure of a document that benefits quickly grasping and understanding the overall context of the document from a higher level. However, research and applications in this field have been restrained due to the lack of proper paragraph-level topic representations and large-scale, high-quality corpora in Chinese compared to the success achieved in English. Addressing these issues, we introduce a hierarchical paragraph-level topic structure representation with title, subheading, and paragraph that comprehensively models the document discourse topic structure. In addition, we ensure a more holistic representation of topic distribution within the document by using sentences instead of keywords to represent sub-topics. Following this representation, we construct the largest Chinese Paragraph-level Topic Structur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20854;&#22312;&#31616;&#21333;&#35821;&#35328;&#25512;&#26029;&#20219;&#21153;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#20854;&#23545;&#22522;&#26412;&#35821;&#35328;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.14785</link><description>&lt;p&gt;
ChatGPT&#21644;&#31616;&#21333;&#30340;&#35821;&#35328;&#25512;&#26029;&#65306;&#30450;&#28857;&#21644;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds. (arXiv:2305.14785v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20854;&#22312;&#31616;&#21333;&#35821;&#35328;&#25512;&#26029;&#20219;&#21153;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#20854;&#23545;&#22522;&#26412;&#35821;&#35328;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#29702;&#35299;&#33021;&#21147;&#38480;&#21046;&#65292;&#38024;&#23545;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#36890;&#24120;&#31616;&#21333;&#30340;&#25512;&#26029;&#20219;&#21153;&#65292;&#20294;&#36825;&#20123;&#20284;&#20046;&#23545;&#35813;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#38024;&#23545;(i)&#35821;&#27861;&#35268;&#23450;&#30340;&#34164;&#21547;&#65292;(ii)&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#35777;&#25454;&#21103;&#35789;&#30340;&#21069;&#25552;&#65292;&#20197;&#21450;(iii)&#21333;&#35843;&#34164;&#21547;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#20026;&#36825;&#20123;&#25512;&#29702;&#31867;&#22411;&#25552;&#20379;&#20102;&#19987;&#23478;&#35774;&#35745;&#30340;&#35780;&#20272;&#38598;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#36825;&#20123;&#25512;&#29702;&#31867;&#22411;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23637;&#31034;&#20013;&#31561;&#21040;&#20302;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;ChatGPT&#22312;&#30452;&#25509;&#25552;&#31034;&#19979;&#34920;&#29616;&#20986;&#23545;&#24213;&#23618;&#35821;&#35328;&#27010;&#24565;&#30340;&#20102;&#35299;&#65292;&#20294;&#23427;&#32463;&#24120;&#19981;&#33021;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#20316;&#20986;&#27491;&#30830;&#30340;&#25512;&#26029;&#12290;&#26356;&#26377;&#36259;&#30340;&#26159;&#65292;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#21069;&#25552;&#23884;&#20837;&#21069;&#25552;&#26465;&#20214;&#35302;&#21457;&#25110;&#38750;&#23454;&#38469;&#24615;&#21160;&#35789;&#20250;&#23548;&#33268;&#27169;&#22411;&#26356;&#39057;&#32321;&#22320;&#39044;&#27979;&#34164;&#21547;&#65292;&#32780;&#19981;&#32771;&#34385;&#27491;&#30830;&#30340;&#35821;&#20041;&#26631;&#31614;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#32487;&#32493;&#25913;&#21892;&#23427;&#20204;&#23545;&#22522;&#26412;&#35821;&#35328;&#27010;&#24565;&#30340;&#29702;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper sheds light on the limitations of ChatGPT's understanding capabilities, focusing on simple inference tasks that are typically easy for humans but appear to be challenging for the model. Specifically, we target (i) grammatically-specified entailments, (ii) premises with evidential adverbs of uncertainty, and (iii) monotonicity entailments. We present expert-designed evaluation sets for these inference types and conduct experiments in a zero-shot setup. Our results show that the model struggles with these types of inferences, exhibiting moderate to low accuracy. Moreover, while ChatGPT demonstrates knowledge of the underlying linguistic concepts when prompted directly, it often fails to incorporate this knowledge to make correct inferences. Even more strikingly, further experiments show that embedding the premise under presupposition triggers or non-factive verbs causes the model to predict entailment more frequently {regardless} of the correct semantic label. Overall these re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;PLMs&#20013;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#21033;&#29992;&#65292;&#30740;&#31350;&#21457;&#29616;PLMs&#23384;&#22312;&#24050;&#33719;&#21462;&#30340;&#30693;&#35782;&#21644;&#21033;&#29992;&#30340;&#30693;&#35782;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#21487;&#20197;&#24357;&#34917;&#24050;&#33719;&#21462;&#30693;&#35782;&#30340;&#24046;&#36317;&#65292;&#20294;&#21033;&#29992;&#30693;&#35782;&#30340;&#24046;&#36317;&#20173;&#28982;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2305.14775</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#30693;&#35782;&#33719;&#21462;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models. (arXiv:2305.14775v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;PLMs&#20013;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#21033;&#29992;&#65292;&#30740;&#31350;&#21457;&#29616;PLMs&#23384;&#22312;&#24050;&#33719;&#21462;&#30340;&#30693;&#35782;&#21644;&#21033;&#29992;&#30340;&#30693;&#35782;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#21487;&#20197;&#24357;&#34917;&#24050;&#33719;&#21462;&#30693;&#35782;&#30340;&#24046;&#36317;&#65292;&#20294;&#21033;&#29992;&#30693;&#35782;&#30340;&#24046;&#36317;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#33719;&#21462;&#20102;&#22823;&#37327;&#30340;&#30693;&#35782;&#65292;&#20294;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#36825;&#20123;&#21442;&#25968;&#21270;&#30693;&#35782;&#20013;&#26377;&#22810;&#23569;&#23454;&#38469;&#21487;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;PLMs&#20013;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#21033;&#29992;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#20174;PLM&#21442;&#25968;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#38543;&#21518;&#22260;&#32469;&#36825;&#20123;&#25552;&#21462;&#30340;&#30693;&#35782;&#26500;&#24314;&#19979;&#28216;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#23436;&#20840;&#20381;&#36182;&#20110;&#21033;&#29992;&#27169;&#22411;&#25152;&#20855;&#22791;&#30340;&#30693;&#35782;&#65292;&#36991;&#20813;&#20102;&#19981;&#20805;&#20998;&#30340;&#20449;&#21495;&#31561;&#28151;&#28102;&#22240;&#32032;&#12290;&#20316;&#20026;&#19968;&#20010;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;PLMs&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#27979;&#37327;&#20102;125M&#21040;13B&#21442;&#25968;PLMs&#30340;&#21033;&#29992;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65306;&#65288;1&#65289;PLMs&#22312;&#24050;&#33719;&#21462;&#30340;&#30693;&#35782;&#21644;&#21033;&#29992;&#30340;&#30693;&#35782;&#20043;&#38388;&#23384;&#22312;&#20004;&#20010;&#24046;&#36317;&#65292;&#65288;2&#65289;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#65292;&#23427;&#20204;&#22312;&#21033;&#29992;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#65292;&#65288;3&#65289;&#36739;&#22823;&#30340;&#27169;&#22411;&#21487;&#20197;&#24357;&#34917;&#24050;&#33719;&#21462;&#30693;&#35782;&#30340;&#24046;&#36317;&#65292;&#20294;&#21033;&#29992;&#30693;&#35782;&#30340;&#24046;&#36317;&#20173;&#28982;&#23384;&#22312;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#24403;&#21069;PLMs&#22312;&#21033;&#29992;&#24050;&#33719;&#21462;&#30693;&#35782;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
While pre-trained language models (PLMs) have shown evidence of acquiring vast amounts of knowledge, it remains unclear how much of this parametric knowledge is actually usable in performing downstream tasks. We propose a systematic framework to measure parametric knowledge utilization in PLMs. Our framework first extracts knowledge from a PLM's parameters and subsequently constructs a downstream task around this extracted knowledge. Performance on this task thus depends exclusively on utilizing the model's possessed knowledge, avoiding confounding factors like insufficient signal. As an instantiation, we study factual knowledge of PLMs and measure utilization across 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps in acquired vs. utilized knowledge, (2) they show limited robustness in utilizing knowledge under distribution shifts, and (3) larger models close the acquired knowledge gap but the utilized knowledge gap remains. Overall, our study provides insights 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;few-shot prompting&#27604;&#36739;&#38750;&#19978;&#19979;&#25991;&#25913;&#20889;&#21644;&#19978;&#19979;&#25991;&#25913;&#20889;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#19981;&#19968;&#23450;&#33021;&#21453;&#26144;&#20986;&#20154;&#31867;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.14755</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#27169;&#22411;&#21450;&#35780;&#20272;&#22312;&#25991;&#20307;&#25913;&#20889;&#20013;&#30340;&#24517;&#35201;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting. (arXiv:2305.14755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;few-shot prompting&#27604;&#36739;&#38750;&#19978;&#19979;&#25991;&#25913;&#20889;&#21644;&#19978;&#19979;&#25991;&#25913;&#20889;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#19981;&#19968;&#23450;&#33021;&#21453;&#26144;&#20986;&#20154;&#31867;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25991;&#20307;&#25913;&#20889;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#25805;&#20316;&#65292;&#20294;&#26159;&#24573;&#35270;&#25991;&#26412;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#21487;&#20197;&#23548;&#33268;&#25913;&#20889;&#32467;&#26524;&#26159;&#19968;&#33324;&#21270;&#12289;&#27495;&#20041;&#21644;&#19981;&#36830;&#36143;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#21040;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#65292;&#37325;&#28857;&#20851;&#27880;&#24418;&#24335;&#12289;&#27602;&#24615;&#21644;&#24773;&#24863;&#36716;&#31227;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545; GPT-3.5 &#21644; GPT NeoX &#30340; few-shot &#25552;&#38382;&#27604;&#36739;&#37325;&#20889;&#30340;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#38750;&#19978;&#19979;&#25991;&#25913;&#20889;&#21644;&#19978;&#19979;&#25991;&#25913;&#20889;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20154;&#20204;&#36890;&#24120;&#26356;&#21916;&#27426;&#19978;&#19979;&#25991;&#25913;&#20889;&#65292;&#20294;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#65288;&#22914; BLEU&#65292;sBERT&#65289;&#19981;&#26159;&#36825;&#26679;&#30340;&#12290;&#20026;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#29992;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#30340;&#19978;&#19979;&#25991;&#34701;&#21512;&#29256;&#26412;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#26356;&#33021;&#21453;&#26144;&#20154;&#31867;&#20559;&#22909;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#25991;&#24378;&#35843;&#22312;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing stylistic text rewriting methods operate on a sentence level, but ignoring the broader context of the text can lead to generic, ambiguous, and incoherent rewrites. In this paper, we propose the integration of preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting, focusing on formality, toxicity, and sentiment transfer tasks. We conduct a comparative evaluation of rewriting through few-shot prompting of GPT-3.5 and GPT NeoX, comparing non-contextual rewrites to contextual rewrites. Our experiments show that humans often prefer contextual rewrites over non-contextual ones, but automatic metrics (e.g., BLEU, sBERT) do not. To bridge this gap, we propose context-infused versions of common automatic metrics, and show that these better reflect human preferences. Overall, our paper highlights the importance of integrating preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#39564;&#35777;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#39564;&#35777;&#21644;&#20462;&#22797;&#36719;&#20214;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;ESBMC-AI&#20570;&#20986;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.14752</link><description>&lt;p&gt;
&#36208;&#21521;&#36719;&#20214;&#33258;&#24840;&#65306;&#32467;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#39564;&#35777;&#35299;&#20915;&#36719;&#20214;&#23433;&#20840;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification. (arXiv:2305.14752v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#39564;&#35777;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#39564;&#35777;&#21644;&#20462;&#22797;&#36719;&#20214;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;ESBMC-AI&#20570;&#20986;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#21270;&#39564;&#35777;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#20351;&#24471;&#36719;&#20214;&#28431;&#27934;&#21487;&#20197;&#24471;&#21040;&#39564;&#35777;&#21644;&#33258;&#21160;&#20462;&#22797;&#12290;&#39318;&#20808;&#21033;&#29992;&#26377;&#38480;&#27169;&#22411;&#26816;&#26597;&#65288;BMC&#65289;&#23450;&#20301;&#36719;&#20214;&#28431;&#27934;&#21644;&#27966;&#29983;&#21453;&#20363;&#12290;&#28982;&#21518;&#65292;&#23558;&#21453;&#20363;&#21644;&#28304;&#20195;&#30721;&#25552;&#20379;&#32473;&#22823;&#35821;&#35328;&#27169;&#22411;&#24341;&#25806;&#36827;&#34892;&#20195;&#30721;&#35843;&#35797;&#21644;&#29983;&#25104;&#65292;&#20174;&#32780;&#25214;&#21040;&#28431;&#27934;&#30340;&#26681;&#26412;&#21407;&#22240;&#24182;&#20462;&#22797;&#20195;&#30721;&#12290;&#26368;&#21518;&#65292;&#21017;&#20351;&#29992;BMC&#39564;&#35777;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20462;&#27491;&#29256;&#26412;&#30340;&#20195;&#30721;&#12290; &#20316;&#20026;&#27010;&#24565;&#35777;&#26126;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ESBMC-AI&#65292;&#23427;&#22522;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;SMT&#30340;&#19978;&#19979;&#25991;&#26377;&#30028;&#27169;&#22411;&#26816;&#26597;&#22120;&#65288;ESBMC&#65289;&#21644;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;gpt-3.5-turbo&#26469;&#26816;&#27979;&#21644;&#20462;&#22797;C&#31243;&#24207;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a novel solution that combines the capabilities of Large Language Models (LLMs) with Formal Verification strategies to verify and automatically repair software vulnerabilities. Initially, we employ Bounded Model Checking (BMC) to locate the software vulnerability and derive a counterexample. The counterexample provides evidence that the system behaves incorrectly or contains a vulnerability. The counterexample that has been detected, along with the source code, are provided to the LLM engine. Our approach involves establishing a specialized prompt language for conducting code debugging and generation to understand the vulnerability's root cause and repair the code. Finally, we use BMC to verify the corrected version of the code generated by the LLM. As a proof of concept, we create ESBMC-AI based on the Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained Transformer model, specifically gpt-3.5-turbo, to detect and fix errors in C program
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DialogVCS&#65292;&#24314;&#31435;&#20102;4&#20010;&#25968;&#25454;&#38598;&#29992;&#20110;&#27979;&#35797;&#40065;&#26834;NLU&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#65292;&#26032;&#24847;&#22270;&#30340;&#20986;&#29616;&#21487;&#33021;&#20250;&#19982;&#29616;&#26377;&#24847;&#22270;&#22312;&#35821;&#20041;&#19978;&#23384;&#22312;&#20851;&#32852;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14751</link><description>&lt;p&gt;
DialogVCS&#65306;&#23545;&#35805;&#31995;&#32479;&#21319;&#32423;&#20013;&#30340;&#40065;&#26834;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade. (arXiv:2305.14751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DialogVCS&#65292;&#24314;&#31435;&#20102;4&#20010;&#25968;&#25454;&#38598;&#29992;&#20110;&#27979;&#35797;&#40065;&#26834;NLU&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#65292;&#26032;&#24847;&#22270;&#30340;&#20986;&#29616;&#21487;&#33021;&#20250;&#19982;&#29616;&#26377;&#24847;&#22270;&#22312;&#35821;&#20041;&#19978;&#23384;&#22312;&#20851;&#32852;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20135;&#21697;&#23545;&#35805;&#31995;&#32479;&#30340;&#19981;&#26029;&#26356;&#26032;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22411;&#65292;&#22240;&#20026;&#23454;&#38469;&#29992;&#25143;&#25968;&#25454;&#20250;&#19982;&#19978;&#19968;&#27425;&#26356;&#26032;&#32047;&#31215;&#30340;&#25968;&#25454;&#21512;&#24182;&#12290;&#26032;&#25968;&#25454;&#20013;&#20250;&#20986;&#29616;&#26032;&#30340;&#24847;&#22270;&#65292;&#24182;&#21487;&#33021;&#19982;&#29616;&#26377;&#24847;&#22270;&#22312;&#35821;&#20041;&#19978;&#23384;&#22312;&#32416;&#32544;&#65292;&#20363;&#22914;&#65292;&#35821;&#20041;&#36807;&#20110;&#29305;&#23450;&#25110;&#36890;&#29992;&#30340;&#26032;&#24847;&#22270;&#23454;&#38469;&#19978;&#26159;&#35821;&#20041;&#31354;&#38388;&#20013;&#26576;&#20123;&#29616;&#26377;&#24847;&#22270;&#30340;&#23376;&#38598;&#25110;&#36229;&#38598;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;NLU&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20316;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#65292;&#25105;&#20204;&#35774;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;4&#20010;&#23545;&#35805;&#29256;&#26412;&#25511;&#21046;&#25968;&#25454;&#38598;&#65288;DialogVCS&#65289;&#12290;&#25105;&#20204;&#23558;&#20855;&#26377;&#19981;&#23436;&#32654;&#25968;&#25454;&#30340;&#31995;&#32479;&#26356;&#26032;&#20013;&#30340;&#24847;&#22270;&#26816;&#27979;&#23450;&#20041;&#20026;&#20855;&#26377;&#27491;&#20294;&#26410;&#26631;&#35760;&#24847;&#22270;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#35782;&#21035;&#25152;&#26377;&#36866;&#24403;&#30340;&#24847;&#22270;&#65292;&#21253;&#25324;&#20855;&#26377;&#35821;&#20041;&#32416;&#32544;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27169;&#22411;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the constant updates of the product dialogue systems, we need to retrain the natural language understanding (NLU) model as new data from the real users would be merged into the existent data accumulated in the last updates. Within the newly added data, new intents would emerge and might have semantic entanglement with the existing intents, e.g. new intents that are semantically too specific or generic are actually subset or superset of some existing intents in the semantic space, thus impairing the robustness of the NLU model. As the first attempt to solve this problem, we setup a new benchmark consisting of 4 Dialogue Version Control dataSets (DialogVCS). We formulate the intent detection with imperfect data in the system update as a multi-label classification task with positive but unlabeled intents, which asks the models to recognize all the proper intents, including the ones with semantic entanglement, in the inference. We also propose comprehensive baseline models and conduct i
&lt;/p&gt;</description></item><item><title>ECHo&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#25512;&#29702;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;CoT&#33539;&#24335;&#23545;&#40784;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#35780;&#20272;&#24403;&#21069;AI&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14740</link><description>&lt;p&gt;
ECHo: &#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#25512;&#29702;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
ECHo: Event Causality Inference via Human-centric Reasoning. (arXiv:2305.14740v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14740
&lt;/p&gt;
&lt;p&gt;
ECHo&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#25512;&#29702;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;CoT&#33539;&#24335;&#23545;&#40784;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#35780;&#20272;&#24403;&#21069;AI&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; ECHo&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#31038;&#20132;&#24773;&#22659;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#35786;&#26029;&#25968;&#25454;&#38598;&#12290; ECHo&#21033;&#29992;&#20174;&#29359;&#32618;&#21095;&#20013;&#25910;&#38598;&#30340;&#30495;&#23454;&#20154;&#31867;&#20013;&#24515;&#28436;&#32462;&#20449;&#24687;&#65292;&#36890;&#36807;&#28608;&#21457;&#20013;&#38388;&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#26469;&#24357;&#21512;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#40511;&#27807;&#65292;&#20174;&#32780;&#25552;&#39640;&#31038;&#20132;&#26234;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;Chain-of-Thought&#65288;CoT&#65289;&#33539;&#24335;&#23545;&#40784;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#24403;&#21069;AI&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20010;ToM&#22686;&#24378;&#30340;CoT&#31649;&#36947;&#21487;&#20197;&#22312; &#38646;-shot&#35270;&#35273;&#21644;&#35821;&#35328;&#29702;&#35299;&#20013;&#21253;&#23481;&#21644;&#25972;&#21512;&#21508;&#31181;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#20114;&#34917;&#30340;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#30340;ECHo&#20219;&#21153;&#26469;&#23457;&#26597;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ECHo&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26292;&#38706;&#25512;&#29702;&#20013;&#30340;&#19981;&#23436;&#21892;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ECHo, a diagnostic dataset of event causality inference grounded in visual-and-linguistic social scenarios. ECHo employs real-world human-centric deductive information collected from crime drama, bridging the gap in multimodal reasoning towards higher social intelligence through the elicitation of intermediate Theory-of-Mind (ToM). We propose a unified framework aligned with the Chain-of-Thought (CoT) paradigm to assess the reasoning capability of current AI systems. This ToM-enhanced CoT pipeline can accommodate and integrate various large foundation models in zero-shot visual-and-linguistic understanding. With this framework, we scrutinize the advanced large language and multimodal models via three complementary human-centric ECHo tasks. Further analysis demonstrates ECHo as a challenging dataset to expose imperfections and inconsistencies in reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.14735</link><description>&lt;p&gt;
&#36793;&#32536;&#32858;&#28966;&#65306;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#25439;&#20154;&#32676;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#23545;&#36793;&#32536;&#31038;&#21306;&#24433;&#21709;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#30830;&#23450;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#38024;&#23545;&#24369;&#21183;&#32676;&#20307;&#30340;&#20260;&#23475;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20250;&#25513;&#30422;&#30001;&#20132;&#21449;&#23376;&#32676;&#25110;&#36328;&#20154;&#21475;&#32676;&#20307;&#20849;&#20139;&#30340;&#20260;&#23475;&#27169;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#8220;&#36793;&#32536;&#8221;&#23450;&#20041;&#20026;&#20855;&#26377;&#36828;&#31163;&#8220;&#24120;&#24577;&#8221; &#30340;&#20154;&#21475;&#23646;&#24615;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#24230;&#37327;&#38024;&#23545;&#36825;&#20123;&#24322;&#24120;&#20540;&#30340;&#20260;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#24615;&#33021;&#24046;&#24322;&#25351;&#25968;&#65288;GPDI&#65289;&#65292;&#20197;&#34913;&#37327;&#25968;&#25454;&#38598;&#32454;&#20998;&#20026;&#23376;&#32452;&#23545;&#38754;&#20020;&#22686;&#21152;&#30340;&#20260;&#23475;&#30340;&#35782;&#21035;&#31243;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26816;&#27979;&#27602;&#24615;&#26816;&#27979;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#38024;&#23545;&#24322;&#24120;&#20540;&#30340;&#25991;&#26412;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#27602;&#24615;&#26816;&#39564;&#20013;&#27602;&#24615;&#26356;&#39640;&#65292;&#39640;&#36798;28&#65285;&#33267;86&#65285;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23545;&#20110;&#20154;&#21475;&#23398;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#22987;&#32456;&#36739;&#24046;&#65292;&#24322;&#24120;&#20540;&#21644;&#38750;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#38169;&#35823;&#24046;&#36317;&#39640;&#36798;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
A standard method for measuring the impacts of AI on marginalized communities is to determine performance discrepancies between specified demographic groups. These approaches aim to address harms toward vulnerable groups, but they obscure harm patterns faced by intersectional subgroups or shared across demographic groups. We instead operationalize "the margins" as data points that are statistical outliers due to having demographic attributes distant from the "norm" and measure harms toward these outliers. We propose a Group-Based Performance Disparity Index (GPDI) that measures the extent to which a subdivision of a dataset into subgroups identifies those facing increased harms. We apply our approach to detecting disparities in toxicity detection and find that text targeting outliers is 28% to 86% more toxic for all types of toxicity examined. We also discover that model performance is consistently worse for demographic outliers, with disparities in error between outliers and non-outli
&lt;/p&gt;</description></item><item><title>AutoDepthNet&#33021;&#22815;&#21033;&#29992;&#28151;&#21512;&#25668;&#20687;&#26426;&#35774;&#32622;&#20013;&#30340;&#28145;&#24230;&#30456;&#26426;&#21644;&#24425;&#33394;&#30456;&#26426;&#36827;&#34892;&#28145;&#24230;&#22270;&#37325;&#24314;&#65292;&#22312;GPU&#19978;&#20855;&#26377;8ms&#30340;&#25512;&#26029;&#26102;&#38388;&#65292;&#33021;&#22815;&#23454;&#29616;&#27599;&#31186;200&#24103;&#30340;&#23454;&#26102;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.14731</link><description>&lt;p&gt;
AutoDepthNet&#65306;&#20351;&#29992;&#26222;&#36890;&#28145;&#24230;&#21644;RGB&#30456;&#26426;&#36827;&#34892;&#39640;&#24103;&#29575;&#28145;&#24230;&#22270;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
AutoDepthNet: High Frame Rate Depth Map Reconstruction using Commodity Depth and RGB Cameras. (arXiv:2305.14731v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14731
&lt;/p&gt;
&lt;p&gt;
AutoDepthNet&#33021;&#22815;&#21033;&#29992;&#28151;&#21512;&#25668;&#20687;&#26426;&#35774;&#32622;&#20013;&#30340;&#28145;&#24230;&#30456;&#26426;&#21644;&#24425;&#33394;&#30456;&#26426;&#36827;&#34892;&#28145;&#24230;&#22270;&#37325;&#24314;&#65292;&#22312;GPU&#19978;&#20855;&#26377;8ms&#30340;&#25512;&#26029;&#26102;&#38388;&#65292;&#33021;&#22815;&#23454;&#29616;&#27599;&#31186;200&#24103;&#30340;&#23454;&#26102;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#30456;&#26426;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#35270;&#39057;&#28216;&#25103;&#31561;&#39046;&#22495;&#26377;&#30528;&#21508;&#31181;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26222;&#36890;&#28145;&#24230;&#30456;&#26426;&#30340;&#39640;&#24310;&#36831;&#21644;&#20302;&#24103;&#29575;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#28145;&#24230;&#22270;&#37325;&#24314;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#24310;&#36831;&#24182;&#25552;&#39640;&#28145;&#24230;&#30456;&#26426;&#30340;&#24103;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#28151;&#21512;&#25668;&#20687;&#26426;&#35774;&#32622;&#20013;&#30340;&#26222;&#36890;&#28145;&#24230;&#30456;&#26426;&#21644;&#24425;&#33394;&#30456;&#26426;&#65307;&#25105;&#20204;&#30340;&#21407;&#22411;&#20351;&#29992;Kinect Azure&#28145;&#24230;&#30456;&#26426;&#20197;30 fps&#30340;&#36895;&#24230;&#21644;&#22312;iPhone 11 Pro&#19978;&#25429;&#33719;&#30340;&#39640;&#36895;RGB&#30456;&#26426;&#20197;240 fps&#30340;&#36895;&#24230;&#36827;&#34892;&#23454;&#29616;&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;AutoDepthNet&#26159;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#23427;&#25429;&#33719;&#26469;&#33258;&#39640;&#36895;RGB&#30456;&#26426;&#30340;&#24103;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#20808;&#21069;&#30340;&#28145;&#24230;&#24103;&#32467;&#21512;&#36215;&#26469;&#37325;&#24314;&#19968;&#31995;&#21015;&#39640;&#24103;&#29575;&#28145;&#24230;&#22270;&#12290;&#22312;GPU&#19978;&#65292;&#20351;&#29992;480 x 270&#36755;&#20986;&#20998;&#36776;&#29575;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#23454;&#29616;&#20102;8&#27627;&#31186;&#30340;&#25512;&#26029;&#26102;&#38388;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#22788;&#29702;&#22312;&#39640;&#36798;200 fps&#30340;&#23454;&#26102;&#20351;&#29992;&#12290;AutoDepthNet&#21487;&#20197;&#36880;&#23618;&#31934;&#30830;&#22320;&#20272;&#35745;&#28145;&#24230;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth cameras have found applications in diverse fields, such as computer vision, artificial intelligence, and video gaming. However, the high latency and low frame rate of existing commodity depth cameras impose limitations on their applications. We propose a fast and accurate depth map reconstruction technique to reduce latency and increase the frame rate in depth cameras. Our approach uses only a commodity depth camera and color camera in a hybrid camera setup; our prototype is implemented using a Kinect Azure depth camera at 30 fps and a high-speed RGB iPhone 11 Pro camera captured at 240 fps. The proposed network, AutoDepthNet, is an encoder-decoder model that captures frames from the high-speed RGB camera and combines them with previous depth frames to reconstruct a stream of high frame rate depth maps. On GPU, with a 480 x 270 output resolution, our system achieves an inference time of 8 ms, enabling real-time use at up to 200 fps with parallel processing. AutoDepthNet can estim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20174;&#35821;&#35328;&#38544;&#21947;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#65292;&#24182;&#19988;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#20849;&#21019;&#20986;&#20855;&#26377;&#35270;&#35273;&#20914;&#20987;&#21147;&#21644;&#35821;&#20041;&#21547;&#20041;&#30340;&#38544;&#21947;&#12290;</title><link>http://arxiv.org/abs/2305.14724</link><description>&lt;p&gt;
&#25105;&#23547;&#35269;&#19968;&#20010;&#38544;&#21947;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20849;&#21019;&#35270;&#35273;&#38544;&#21947;
&lt;/p&gt;
&lt;p&gt;
I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. (arXiv:2305.14724v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20174;&#35821;&#35328;&#38544;&#21947;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#65292;&#24182;&#19988;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#20849;&#21019;&#20986;&#20855;&#26377;&#35270;&#35273;&#20914;&#20987;&#21147;&#21644;&#35821;&#20041;&#21547;&#20041;&#30340;&#38544;&#21947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38544;&#21947;&#26159;&#36890;&#36807;&#22270;&#20687;&#26469;&#35828;&#26381;&#25110;&#20256;&#36798;&#21019;&#24847;&#24819;&#27861;&#30340;&#24378;&#22823;&#20462;&#36766;&#25163;&#27861;&#12290;&#19982;&#35821;&#35328;&#38544;&#21947;&#31867;&#20284;&#65292;&#23427;&#20204;&#36890;&#36807;&#31526;&#21495;&#20027;&#20041;&#21644;&#31526;&#21495;&#30340;&#24182;&#32622;&#38544;&#21547;&#22320;&#20256;&#36798;&#21547;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#35821;&#35328;&#38544;&#21947;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#30340;&#26032;&#20219;&#21153;&#12290;&#36825;&#23545;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65288;&#22914;DALL $\cdot$ E 2&#65289;&#26469;&#35828;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#27169;&#25311;&#38544;&#21547;&#21547;&#20041;&#21644;&#32452;&#21512;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#37319;&#29992;&#20197;&#8220;&#20018;&#32852;&#24605;&#32500;&#8221;&#20026;&#25552;&#31034;&#30340;Instruct GPT-3&#65288;davinci-002&#65289;&#29983;&#25104;&#20195;&#34920;&#35821;&#35328;&#38544;&#21947;&#30340;&#35270;&#35273;&#38416;&#36848;&#30340;&#25991;&#26412;&#65292;&#20854;&#20013;&#21253;&#21547;&#38544;&#21547;&#21547;&#20041;&#21644;&#30456;&#20851;&#23545;&#35937;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#26694;&#26550;&#65292;&#20154;&#20204;&#19982;LLM&#21644;&#34920;&#29616;&#26368;&#20339;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#65292;&#21019;&#24314;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#38544;&#21947;&#21644;&#23427;&#20204;&#30340;&#35270;&#35273;&#23545;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LLMs&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#21487;&#20197;&#20849;&#21516;&#21019;&#36896;&#20986;&#20855;&#26377;&#35270;&#35273;&#20914;&#20987;&#21147;&#21644;&#35821;&#20041;&#21547;&#20041;&#30340;&#38544;&#21947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL$\cdot$E 2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the task through the collaboration between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3 (davinci-002) with Chain-of-Thought prompting generates text that represents a visual elaboration of the linguistic metaphor containing the implicit meaning and relevant objects, which is then used as input to the diffusion-based text-to-image models.Using a human-AI collaboration framework, where humans interact both with the LLM and the top-performing diffusion model, we create a high-qu
&lt;/p&gt;</description></item><item><title>BLIP-Diffusion&#26159;&#19968;&#31181;&#26032;&#30340;&#20027;&#20307;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#20027;&#20307;&#34920;&#24449;&#23398;&#20064;&#20219;&#21153;&#23454;&#29616;&#20102;&#21487;&#25511;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#65292;&#24182;&#25903;&#25345;&#38646;&#26679;&#26412;&#29983;&#25104;&#21644;&#23450;&#21046;&#21270;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2305.14720</link><description>&lt;p&gt;
BLIP-Diffusion: &#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#39044;&#35757;&#32451;&#20027;&#20307;&#34920;&#24449;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing. (arXiv:2305.14720v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14720
&lt;/p&gt;
&lt;p&gt;
BLIP-Diffusion&#26159;&#19968;&#31181;&#26032;&#30340;&#20027;&#20307;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#20027;&#20307;&#34920;&#24449;&#23398;&#20064;&#20219;&#21153;&#23454;&#29616;&#20102;&#21487;&#25511;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#65292;&#24182;&#25903;&#25345;&#38646;&#26679;&#26412;&#29983;&#25104;&#21644;&#23450;&#21046;&#21270;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20027;&#20307;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#21019;&#24314;&#26032;&#39062;&#30340;&#20027;&#20307;&#22270;&#20687;&#12290;&#29616;&#26377;&#27169;&#22411;&#23384;&#22312;&#38271;&#26102;&#38388;&#24494;&#35843;&#21644;&#38590;&#20197;&#20445;&#25345;&#20027;&#20307;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BLIP-Diffusion&#65292;&#19968;&#31181;&#26032;&#30340;&#25903;&#25345;&#22810;&#27169;&#24577;&#25511;&#21046;&#30340;&#20027;&#20307;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#20027;&#20307;&#22270;&#20687;&#21644;&#25991;&#26412;&#25552;&#31034;&#20316;&#20026;&#36755;&#20837;&#12290;&#19982;&#20854;&#20182;&#20027;&#20307;&#39537;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;BLIP-Diffusion&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#23545;&#20027;&#20307;&#34920;&#24449;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#39318;&#20808;&#25353;&#29031;BLIP-2&#30340;&#26041;&#27861;&#39044;&#35757;&#32451;&#20102;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#20197;&#29983;&#25104;&#19982;&#25991;&#26412;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20027;&#20307;&#34920;&#24449;&#23398;&#20064;&#20219;&#21153;&#65292;&#20351;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#36825;&#31181;&#35270;&#35273;&#34920;&#24449;&#24182;&#29983;&#25104;&#26032;&#30340;&#20027;&#20307;&#22270;&#20687;&#12290;&#19982;DreamBooth&#31561;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25903;&#25345;&#38646;&#26679;&#26412;&#20027;&#20307;&#39537;&#21160;&#29983;&#25104;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#23450;&#21046;&#20027;&#20307;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#37325;&#23450;&#20041;&#24314;&#27169;&#65288;MDM&#65289;&#20219;&#21153;&#65292;&#23558;&#30446;&#26631;&#35789;&#30340;&#25152;&#26377;&#19978;&#19979;&#25991;&#21644;&#23450;&#20041;&#27719;&#38598;&#22312;&#19968;&#36215;&#65292;&#30456;&#27604;&#21333;&#20010;&#23450;&#20041;&#24314;&#27169;&#65288;SDM&#65289;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#22312;&#39044;&#35757;&#32451;&#20219;&#21153;&#20013;&#21487;&#20197;&#25552;&#39640;SDM&#30340;&#24615;&#33021;&#65292;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#26377;&#21487;&#27604;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14717</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#37325;&#23450;&#20041;&#24314;&#27169;&#26469;&#25366;&#25496;&#19978;&#19979;&#25991;&#21644;&#23450;&#20041;&#20043;&#38388;&#30340;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Exploiting Correlations Between Contexts and Definitions with Multiple Definition Modeling. (arXiv:2305.14717v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#37325;&#23450;&#20041;&#24314;&#27169;&#65288;MDM&#65289;&#20219;&#21153;&#65292;&#23558;&#30446;&#26631;&#35789;&#30340;&#25152;&#26377;&#19978;&#19979;&#25991;&#21644;&#23450;&#20041;&#27719;&#38598;&#22312;&#19968;&#36215;&#65292;&#30456;&#27604;&#21333;&#20010;&#23450;&#20041;&#24314;&#27169;&#65288;SDM&#65289;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#22312;&#39044;&#35757;&#32451;&#20219;&#21153;&#20013;&#21487;&#20197;&#25552;&#39640;SDM&#30340;&#24615;&#33021;&#65292;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#26377;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#20041;&#24314;&#27169;&#26159;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#24212;&#29992;&#31243;&#24207;&#65288;&#20363;&#22914;&#29702;&#35299;&#21644;&#23545;&#35805;&#65289;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#30446;&#21069;&#65292;&#36825;&#31181;&#27169;&#22411;&#20391;&#37325;&#20110;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#20026;&#30446;&#26631;&#35789;&#25110;&#30701;&#35821;&#29983;&#25104;&#19968;&#20010;&#23450;&#20041;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#21333;&#20010;&#23450;&#20041;&#24314;&#27169;&#65288;SDM&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#33021;&#20805;&#20998;&#22320;&#27169;&#25311;&#21333;&#35789;&#30340;&#19981;&#21516;&#19978;&#19979;&#25991;&#21644;&#23450;&#20041;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#20026;SDM&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#19987;&#19994;&#30693;&#35782;&#21644;&#21162;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#31216;&#20026;&#22810;&#37325;&#23450;&#20041;&#24314;&#27169;&#65288;MDM&#65289;&#30340;&#26032;&#20219;&#21153;&#65292;&#23558;&#30446;&#26631;&#35789;&#30340;&#25152;&#26377;&#19978;&#19979;&#25991;&#21644;&#23450;&#20041;&#27719;&#38598;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#21019;&#24314;&#27169;&#22411;&#20197;&#21450;&#22810;&#20010;&#35757;&#32451;&#38598;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#30340;&#20415;&#21033;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#24182;&#20998;&#26512;&#20102;MDM&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#20351;&#29992;MDM&#20316;&#20026;&#39044;&#35757;&#32451;&#20219;&#21153;&#20197;&#25552;&#39640;SDM&#30340;&#24615;&#33021;&#21450;&#20854;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Definition modeling is an important task in advanced natural language applications such as understanding and conversation. Since its introduction, it focus on generating one definition for a target word or phrase in a given context, which we refer to as Single Definition Modeling (SDM). However, this approach does not adequately model the correlations and patterns among different contexts and definitions of words. In addition, the creation of a training dataset for SDM requires significant human expertise and effort. In this paper, we carefully design a new task called Multiple Definition Modeling (MDM) that pool together all contexts and definition of target words. We demonstrate the ease of creating a model as well as multiple training sets automatically. % In the experiments, we demonstrate and analyze the benefits of MDM, including improving SDM's performance by using MDM as the pretraining task and its comparable performance in the zero-shot setting.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#26102;&#26816;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#21160;&#24577;&#21644;&#38745;&#24577;&#27969;&#21452;&#27969;&#24863;&#30693;&#27169;&#22359;&#65292;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#24182;&#20943;&#36731;&#26102;&#24310;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#40060;&#30524;&#25668;&#20687;&#22836;&#30446;&#26631;&#26816;&#27979;&#20013;&#26102;&#24310;&#24046;&#24322;&#24102;&#26469;&#30340;&#23433;&#20840;&#38544;&#24739;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14713</link><description>&lt;p&gt;
&#33258;&#21160;&#27850;&#36710;&#30340;&#40060;&#30524;&#25668;&#20687;&#22836;&#27969;&#24335;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Streaming Object Detection on Fisheye Cameras for Automatic Parking. (arXiv:2305.14713v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14713
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#26102;&#26816;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#21160;&#24577;&#21644;&#38745;&#24577;&#27969;&#21452;&#27969;&#24863;&#30693;&#27169;&#22359;&#65292;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#24182;&#20943;&#36731;&#26102;&#24310;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#40060;&#30524;&#25668;&#20687;&#22836;&#30446;&#26631;&#26816;&#27979;&#20013;&#26102;&#24310;&#24046;&#24322;&#24102;&#26469;&#30340;&#23433;&#20840;&#38544;&#24739;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40060;&#30524;&#25668;&#20687;&#22836;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#21160;&#27850;&#36710;&#65292;&#32780;&#20854;&#35270;&#39057;&#27969;&#30446;&#26631;&#26816;&#27979;&#26159;&#30830;&#20445;&#36710;&#36742;&#23433;&#20840;&#36816;&#34892;&#30340;&#22522;&#26412;&#24863;&#30693;&#21151;&#33021;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#19982;&#24403;&#21069;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#30340;&#26102;&#24310;&#24046;&#24322;&#65292;&#28982;&#32780;&#29615;&#22659;&#22312;&#26102;&#24310;&#26102;&#38388;&#20869;&#19981;&#21487;&#36991;&#20813;&#30340;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#21487;&#33021;&#36896;&#25104;&#28508;&#22312;&#30340;&#23433;&#20840;&#38544;&#24739;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#37197;&#22791;&#21160;&#24577;&#21644;&#38745;&#24577;&#27969;&#21452;&#27969;&#24863;&#30693;&#27169;&#22359;&#65292;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#24182;&#20943;&#36731;&#26102;&#24310;&#38382;&#39064;&#30340;&#23454;&#26102;&#26816;&#27979;&#26694;&#26550;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#35780;&#20272;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#12290;&#26631;&#20934;&#36793;&#30028;&#26694;&#23545;&#20110;&#40060;&#30524;&#25668;&#20687;&#22836;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#40060;&#30524;&#25668;&#20687;&#22836;&#30340;&#24378;&#28872;&#24452;&#21521;&#30072;&#21464;&#65292;&#32780;&#20572;&#36710;&#24863;&#30693;&#30340;&#20027;&#35201;&#26816;&#27979;&#23545;&#35937;&#26159;&#36710;&#36742;&#21644;&#34892;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fisheye cameras are widely employed in automatic parking, and the video stream object detection (VSOD) of the fisheye camera is a fundamental perception function to ensure the safe operation of vehicles. In past research work, the difference between the output of the deep learning model and the actual situation at the current moment due to the existence of delay of the perception system is generally ignored. But the environment will inevitably change within the delay time which may cause a potential safety hazard. In this paper, we propose a real-time detection framework equipped with a dual-flow perception module (dynamic and static flows) that can predict the future and alleviate the time-lag problem. Meanwhile, we use a new scheme to evaluate latency and accuracy. The standard bounding box is unsuitable for the object in fisheye camera images due to the strong radial distortion of the fisheye camera and the primary detection objects of parking perception are vehicles and pedestrians
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#20415;&#21487;&#27704;&#20037;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#19988;&#38590;&#20197;&#34987;&#20462;&#22797;&#65292;&#38656;&#35201;&#26356;&#21152;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.14710</link><description>&lt;p&gt;
&#35757;&#32451;&#25351;&#20196;&#20316;&#20026;&#21518;&#38376;: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#30340;&#21518;&#38376;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models. (arXiv:2305.14710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14710
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#20415;&#21487;&#27704;&#20037;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#19988;&#38590;&#20197;&#34987;&#20462;&#22797;&#65292;&#38656;&#35201;&#26356;&#21152;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20854;&#30446;&#30340;&#26159;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#35813;&#22521;&#35757;&#33539;&#20363;&#30456;&#20851;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#22312;&#25104;&#21315;&#19978;&#19975;&#30340;&#25968;&#25454;&#20013;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#65292;&#20415;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#27602;&#21270;&#26469;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#29978;&#33267;&#26080;&#38656;&#20462;&#25913;&#25968;&#25454;&#23454;&#20363;&#25110;&#26631;&#31614;&#26412;&#36523;&#12290;&#36890;&#36807;&#36825;&#31181;&#25351;&#20196;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#22235;&#20010;&#24120;&#29992;&#30340; NLP &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#36229;&#36807;90% &#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#24341;&#36215;&#26131;&#20110;&#36716;&#31227;&#21040; 15 &#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#25345;&#20037;&#21518;&#38376;&#12290;&#36825;&#31181;&#25915;&#20987;&#36824;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#26377;&#27602;&#25351;&#20196;&#12290;&#26368;&#21518;&#65292;&#35813;&#25915;&#20987;&#26174;&#31034;&#20986;&#23545;&#29616;&#26377;&#25512;&#29702;&#26102;&#38450;&#24481;&#30340;&#25269;&#25239;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#20984;&#26174;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#38656;&#35201;&#26356;&#20026;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. However, in this work we raise security concerns about this training paradigm. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. In this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. Moreover, the poisoned model cannot be cured by continual learning. Lastly, instruction attacks show resistance to existing inference-time defense. These findings highlight the need for more robust defens
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#19988;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#26657;&#27491;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.14707</link><description>&lt;p&gt;
&#23398;&#29983;&#36229;&#36234;&#20102;&#22823;&#24072;&#65306;&#22522;&#20110;GPT-3&#30340;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#26041;&#27861;&#30340;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
The student becomes the master: Matching GPT3 on Scientific Factual Error Correction. (arXiv:2305.14707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#19988;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#26657;&#27491;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21019;&#24314;&#38169;&#35823;&#26657;&#27491;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#26497;&#39640;&#65292;&#22823;&#22810;&#25968;&#20107;&#23454;&#20027;&#24352;&#26657;&#27491;&#26041;&#27861;&#20381;&#36182;&#20110;&#24378;&#22823;&#30340;&#39564;&#35777;&#27169;&#22411;&#26469;&#25351;&#23548;&#26657;&#27491;&#36807;&#31243;&#12290;&#36825;&#23548;&#33268;&#22312;&#31185;&#23398;&#20107;&#23454;&#26657;&#27491;&#31561;&#39046;&#22495;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#22909;&#30340;&#39564;&#35777;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#23384;&#22312;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#19988;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#20294;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021; - &#22312;SciFact&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;94&#65285;&#30340;&#20462;&#27491;&#20934;&#30830;&#24615;&#65292;&#22312;SciFact-Open&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;62.5&#65285;&#30340;&#20462;&#27491;&#20934;&#30830;&#24615;&#65292;&#20998;&#21035;&#27604;&#19979;&#19968;&#20010;&#26368;&#22909;&#30340;&#26041;&#27861;&#39640;&#20986;0.5&#65285;&#21644;1.50&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#20013;&#30340;&#25552;&#31034;&#21151;&#33021;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#21019;&#24314;&#19968;&#20010;&#20016;&#23500;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#23436;&#20840;&#30417;&#30563;&#30340;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#32416;&#27491;&#20027;&#24352;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29992;&#20110;&#21019;&#24314;&#25968;&#25454;&#38598;&#30340;LLM&#30456;&#31454;&#20105;&#65292;&#35777;&#26126;&#20102;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#35757;&#32451;&#25552;&#39640;&#31185;&#23398;&#20027;&#24352;&#26657;&#27491;&#20219;&#21153;&#24615;&#33021;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the prohibitively high cost of creating error correction datasets, most Factual Claim Correction methods rely on a powerful verification model to guide the correction process. This leads to a significant drop in performance in domains like Scientific Claim Correction, where good verification models do not always exist. In this work, we introduce a claim correction system that makes no domain assumptions and does not require a verifier but is able to outperform existing methods by an order of magnitude -- achieving 94% correction accuracy on the SciFact dataset, and 62.5% on the SciFact-Open dataset, compared to the next best methods 0.5% and 1.50% respectively. Our method leverages the power of prompting with LLMs during training to create a richly annotated dataset that can be used for fully supervised training and regularization. We additionally use a claim-aware decoding procedure to improve the quality of corrected claims. Our method is competitive with the very LLM that was
&lt;/p&gt;</description></item><item><title>PruMUX&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#25968;&#25454;&#22797;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;BERT-base&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#12290;Auto-PruMUX&#21487;&#20197;&#39044;&#27979;&#20462;&#21098;&#21644;&#22797;&#29992;&#30340;&#39640;&#24615;&#33021;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.14706</link><description>&lt;p&gt;
PruMUX&#65306;&#21033;&#29992;&#27169;&#22411;&#21387;&#32553;&#22686;&#24378;&#25968;&#25454;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
PruMUX: Augmenting Data Multiplexing with Model Compression. (arXiv:2305.14706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14706
&lt;/p&gt;
&lt;p&gt;
PruMUX&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#25968;&#25454;&#22797;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;BERT-base&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#12290;Auto-PruMUX&#21487;&#20197;&#39044;&#27979;&#20462;&#21098;&#21644;&#22797;&#29992;&#30340;&#39640;&#24615;&#33021;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#25193;&#22823;&#65292;&#25552;&#39640;&#20854;&#25512;&#29702;&#25928;&#29575;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290; &#20808;&#21069;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#27169;&#22411;&#20462;&#21098;&#65292;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22797;&#29992;&#31561;&#25216;&#26415;&#65292;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#25968;&#25454;&#22797;&#29992;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#20004;&#31181;&#26041;&#27861;&#33719;&#24471;&#30340;&#21152;&#36895;&#20248;&#21183;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;PruMUX&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#38408;&#20540;&#20026;80&#65285;&#21040;74&#65285;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;BERT-base&#27169;&#22411;&#30456;&#27604;&#65292;&#21487;&#33719;&#24471;7.5-29.5&#20493;&#30340;&#21534;&#21520;&#37327;&#25552;&#39640;&#12290; &#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20004;&#31181;&#25216;&#26415;&#20013;&#19981;&#21516;&#21442;&#25968;&#65288;&#20363;&#22914;&#31232;&#30095;&#24615;&#21644;&#22797;&#29992;&#22240;&#23376;&#65289;&#30340;&#21508;&#31181;&#32452;&#21512;&#65292;&#20197;&#25552;&#20379;&#26377;&#20851;&#20934;&#30830;&#24615;&#21644;&#21534;&#21520;&#37327;&#20043;&#38388;&#26435;&#34913;&#30340;&#32508;&#21512;&#20998;&#26512;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Auto-PruMUX&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#26399;&#26395;&#30340;&#31934;&#24230;&#25439;&#22833;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#20462;&#21098;&#21644;&#22797;&#29992;&#30340;&#39640;&#24615;&#33021;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models increase in size by the day, methods for efficient inference are critical to leveraging their capabilities for various applications. Prior work has investigated techniques like model pruning, knowledge distillation, and data multiplexing to increase model throughput without sacrificing accuracy. In this paper, we combine two such methods -structured pruning and data multiplexing -- to compound the speedup gains obtained by either method. Our approach, PruMUX, obtains up to 7.5-29.5X throughput improvement over BERT-base model with accuracy threshold from 80% to 74%. We further study various combinations of parameters (such as sparsity and multiplexing factor) in the two techniques to provide a comprehensive analysis of the tradeoff between accuracy and throughput in the resulting models. We then propose Auto-PruMUX, a meta-level model that can predict the high-performance parameters for pruning and multiplexing given a desired accuracy loss budget, providing a prac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#35265;&#27880;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#33258;&#28982;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#20855;&#26377;&#28789;&#27963;&#24615;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.14701</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#23558;&#36125;&#21494;&#26031;&#20808;&#39564;&#27880;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#24555;&#36895;&#35821;&#35328;&#23398;&#20064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling rapid language learning by distilling Bayesian priors into artificial neural networks. (arXiv:2305.14701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#35265;&#27880;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#33258;&#28982;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#20855;&#26377;&#28789;&#27963;&#24615;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#20174;&#26497;&#23569;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#35821;&#35328;&#12290;&#24320;&#21457;&#33021;&#22815;&#35299;&#37322;&#36825;&#31181;&#33021;&#21147;&#30340;&#35745;&#31639;&#27169;&#22411;&#19968;&#30452;&#26159;&#35748;&#30693;&#31185;&#23398;&#30340;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#36125;&#21494;&#26031;&#27169;&#22411;&#23558;&#25351;&#23548;&#27010;&#25324;&#30340;&#24378;&#28872;&#24402;&#32435;&#20559;&#35265;&#22240;&#32032;&#32467;&#21512;&#36215;&#26469;&#65292;&#25104;&#21151;&#22320;&#35299;&#37322;&#20102;&#20154;&#31867;&#22914;&#20309;&#20174;&#23569;&#25968;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20363;&#23376;&#20013;&#36827;&#34892;&#24402;&#32435;&#65292;&#20294;&#36890;&#24120;&#36807;&#20110;&#20005;&#26684;&#32780;&#26080;&#27861;&#24212;&#29992;&#20110;&#26356;&#33258;&#28982;&#30340;&#25968;&#25454;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#28789;&#27963;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#24456;&#22909;&#22320;&#20174;&#33258;&#28982;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#20294;&#38656;&#35201;&#27604;&#20154;&#31867;&#25509;&#25910;&#21040;&#30340;&#26356;&#22810;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20559;&#35265;&#27880;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20174;&#26377;&#38480;&#30340;&#33258;&#28982;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#26159;&#21487;&#33021;&#30340;&#12290;&#19982;&#36125;&#21494;&#26031;&#27169;&#22411;&#19968;&#26679;&#65292;&#32467;&#26524;&#31995;&#32479;&#21487;&#20197;&#20174;&#23569;&#37327;&#30340;&#31034;&#20363;&#20013;&#23398;&#20064;&#24418;&#24335;&#35821;&#35328;&#27169;&#24335;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#28789;&#27963;&#24615;&#65292;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can learn languages from remarkably little experience. Developing computational models that explain this ability has been a major challenge in cognitive science. Bayesian models that build in strong inductive biases factors that guide generalization - have been successful at explaining how humans might generalize from few examples in controlled settings but are usually too restrictive to be tractably applied to more naturalistic data. By contrast, neural networks have flexible representations that allow them to learn well from naturalistic data but require many more examples than humans receive. We show that learning from limited naturalistic data is possible with an approach that combines the strong inductive biases of a Bayesian model with the flexible representations of a neural network. This approach works by distilling a Bayesian model's biases into a neural network. Like a Bayesian model, the resulting system can learn formal linguistic patterns from a small number of ex
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#24182;&#25552;&#20379;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#35265;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20284;&#23454;&#20307;&#30340;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.14695</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65306;&#19968;&#31181;&#22240;&#26524;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Causal View of Entity Bias in (Large) Language Models. (arXiv:2305.14695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14695
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#24182;&#25552;&#20379;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#35265;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20284;&#23454;&#20307;&#30340;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#20559;&#35265;&#24191;&#27867;&#24433;&#21709;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23548;&#33268;&#23427;&#20204;&#36807;&#24230;&#20381;&#36182;&#65288;&#26377;&#20559;&#35265;&#30340;&#65289;&#21442;&#25968;&#21270;&#30693;&#35782;&#26469;&#36827;&#34892;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#23613;&#31649;&#22240;&#26524;&#30456;&#20851;&#30340;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#32531;&#35299;&#23454;&#20307;&#20559;&#35265;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#31934;&#30830;&#20272;&#35745;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#21442;&#25968;&#20173;&#28982;&#24456;&#22256;&#38590;&#65292;&#40657;&#30418;&#23376;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#26080;&#27861;&#35843;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#65292;&#20854;&#21442;&#25968;&#27604;&#36739;&#23481;&#26131;&#20272;&#35745;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#30333;&#30418;&#21644;&#40657;&#30418;&#35774;&#32622;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#12290;&#36825;&#31181;&#22240;&#26524;&#24178;&#39044;&#23558;&#21407;&#22987;&#23454;&#20307;&#19982;&#30456;&#37051;&#23454;&#20307;&#19968;&#36215;&#36827;&#34892;&#25200;&#21160;&#12290;&#36825;&#31181;&#24178;&#39044;&#20943;&#23569;&#20102;&#19982;&#21407;&#22987;&#23454;&#20307;&#30456;&#20851;&#30340;&#29305;&#23450;&#20559;&#21521;&#20449;&#24687;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;&#20102;&#26469;&#33258;&#31867;&#20284;&#23454;&#20307;&#30340;&#36275;&#22815;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity bias widely affects pretrained (large) language models, causing them to excessively rely on (biased) parametric knowledge to make unfaithful predictions. Although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying causal models in practice. The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits. To address these problems, we propose a specific structured causal model (SCM) whose parameters are comparatively easier to estimate. Building upon this SCM, we propose causal intervention techniques to mitigate entity bias for both white-box and black-box settings. The proposed causal intervention perturbs the original entity with neighboring entities. This intervention reduces specific biasing information pertaining to the original entity while still preserving sufficient common predictive information from similar entities. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#8220;&#19987;&#23478;&#25552;&#31034;&#8221;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26480;&#20986;&#30340;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#27599;&#20010;&#25351;&#20196;&#30340;&#35814;&#32454;&#21644;&#23450;&#21046;&#30340;&#19987;&#23478;&#36523;&#20221;&#25551;&#36848;&#65292;&#24182;&#35201;&#27714;&#27169;&#22411;&#26681;&#25454;&#36825;&#20123;&#25552;&#31034;&#25552;&#20379;&#31572;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#32842;&#22825;&#21161;&#25163;ExpertLLaMA&#65292;&#35813;&#21161;&#25163;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;96&#65285;&#30340;ChatGPT&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14688</link><description>&lt;p&gt;
&#19987;&#23478;&#25552;&#31034;&#65306;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26480;&#20986;&#30340;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. (arXiv:2305.14688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#8220;&#19987;&#23478;&#25552;&#31034;&#8221;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26480;&#20986;&#30340;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#27599;&#20010;&#25351;&#20196;&#30340;&#35814;&#32454;&#21644;&#23450;&#21046;&#30340;&#19987;&#23478;&#36523;&#20221;&#25551;&#36848;&#65292;&#24182;&#35201;&#27714;&#27169;&#22411;&#26681;&#25454;&#36825;&#20123;&#25552;&#31034;&#25552;&#20379;&#31572;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#32842;&#22825;&#21161;&#25163;ExpertLLaMA&#65292;&#35813;&#21161;&#25163;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;96&#65285;&#30340;ChatGPT&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#20197;&#36866;&#24403;&#30340;&#25552;&#31034;&#26041;&#24335;&#36827;&#34892;&#22788;&#29702;&#65292;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22238;&#31572;&#36136;&#37327;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19987;&#23478;&#25552;&#31034;&#65292;&#20197;&#24341;&#21457;LLMs&#20316;&#20026;&#26480;&#20986;&#19987;&#23478;&#22238;&#31572;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#27599;&#20010;&#29305;&#23450;&#25351;&#20196;&#30340;&#35814;&#32454;&#21644;&#23450;&#21046;&#30340;&#19987;&#23478;&#36523;&#20221;&#25551;&#36848;&#65292;&#28982;&#21518;&#35201;&#27714;LLMs&#26681;&#25454;&#36825;&#31181;&#20195;&#29702;&#20154;&#32972;&#26223;&#25552;&#20379;&#31572;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#22686;&#24378;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#29983;&#25104;&#20102;&#19968;&#32452;&#26032;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#31454;&#20105;&#24615;&#30340;&#24320;&#28304;&#32842;&#22825;&#21161;&#25163;ExpertLLaMA&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;GPT4&#30340;&#35780;&#20272;&#26174;&#31034;&#65306;1&#65289;&#19987;&#23478;&#25968;&#25454;&#30340;&#36136;&#37327;&#26174;&#33879;&#39640;&#20110;&#26222;&#36890;&#31572;&#26696;&#65292;2&#65289;ExpertLLaMA&#32988;&#36807;&#29616;&#26377;&#30340;&#24320;&#28304;&#23545;&#25163;&#65292;&#23454;&#29616;&#20102;ChatGPT&#33021;&#21147;&#30340;96&#65285;&#12290;&#25152;&#26377;&#25968;&#25454;&#21644;ExpertLLaMA&#27169;&#22411;&#23558;&#22312;\url{https://github.com/OFA-Sys/Exp}&#19978;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\% of the original ChatGPT's capability. All data and the ExpertLLaMA model will be made publicly available at \url{https://github.com/OFA-Sys/Exp
&lt;/p&gt;</description></item><item><title>RSRM&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#25581;&#31034;&#22797;&#26434;&#25968;&#23398;&#26041;&#31243;&#30340;&#27169;&#22411;&#65292;&#23427;&#21253;&#25324;Monte Carlo Tree Search&#20195;&#29702;&#12289;&#21452;Q&#23398;&#20064;&#22359;&#21644;&#24378;&#21270;&#23398;&#20064;&#22238;&#24402;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.14656</link><description>&lt;p&gt;
RSRM: &#24378;&#21270;&#31526;&#21495;&#22238;&#24402;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
RSRM: Reinforcement Symbolic Regression Machine. (arXiv:2305.14656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14656
&lt;/p&gt;
&lt;p&gt;
RSRM&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#25581;&#31034;&#22797;&#26434;&#25968;&#23398;&#26041;&#31243;&#30340;&#27169;&#22411;&#65292;&#23427;&#21253;&#25324;Monte Carlo Tree Search&#20195;&#29702;&#12289;&#21452;Q&#23398;&#20064;&#22359;&#21644;&#24378;&#21270;&#23398;&#20064;&#22238;&#24402;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#30028;&#20013;&#65292;&#35768;&#22810;&#22797;&#26434;&#31995;&#32479;&#30340;&#34892;&#20026;&#21487;&#20197;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#26041;&#31243;&#26469;&#25551;&#36848;&#12290;&#23558;&#36825;&#20123;&#26041;&#31243;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#33258;&#21160;&#25552;&#21462;&#20986;&#26469;&#34987;&#35270;&#20026;&#19968;&#20010;&#31526;&#21495;&#22238;&#24402;&#30340;&#36807;&#31243;&#65292;&#36825;&#22312;&#36807;&#21435;&#19968;&#30452;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#19978;&#20184;&#20986;&#20102;&#24040;&#22823;&#30340;&#21162;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#31526;&#21495;&#22238;&#24402;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#36824;&#23384;&#22312;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#24403;&#31163;&#25955;&#25628;&#32034;&#31354;&#38388;&#36235;&#21521;&#20110;&#26080;&#31351;&#22823;&#65292;&#24182;&#19988;&#22522;&#30784;&#25968;&#23398;&#20844;&#24335;&#22797;&#26434;&#26102;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#31526;&#21495;&#22238;&#24402;&#26426;&#22120; (RSRM)&#65292;&#20197;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#25581;&#31034;&#20986;&#22797;&#26434;&#30340;&#25968;&#23398;&#26041;&#31243;&#20026;&#30446;&#26631;&#12290;RSRM&#27169;&#22411;&#30001;&#19977;&#20010;&#20851;&#38190;&#27169;&#22359;&#32452;&#25104;&#65306;(1)&#19968;&#20010;Monte Carlo &#26641;&#25628;&#32034; (MCTS) &#20195;&#29702;&#65292;&#23427;&#25506;&#32034;&#30001;&#39044;&#23450;&#20041;&#30340;&#25968;&#23398;&#36816;&#31639;&#31526;&#21644;&#21464;&#37327;&#32452;&#25104;&#30340;&#26368;&#20339;&#25968;&#23398;&#34920;&#36798;&#24335;&#26641;&#65292;(2)&#19968;&#20010;&#21452;Q&#23398;&#20064;&#22359;&#65292;&#36890;&#36807;&#20248;&#21270;&#25628;&#32034;&#31354;&#38388;&#26469;&#24110;&#21161;&#20943;&#23569;MCTS&#30340;&#21487;&#34892;&#25628;&#32034;&#31354;&#38388;&#65292;&#21644;(3)&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#22238;&#24402;&#22120;&#65292;&#29992;&#20110;&#21457;&#29616;&#26368;&#32456;&#30340;&#25968;&#23398;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In nature, the behaviors of many complex systems can be described by parsimonious math equations. Automatically distilling these equations from limited data is cast as a symbolic regression process which hitherto remains a grand challenge. Keen efforts in recent years have been placed on tackling this issue and demonstrated success in symbolic regression. However, there still exist bottlenecks that current methods struggle to break when the discrete search space tends toward infinity and especially when the underlying math formula is intricate. To this end, we propose a novel Reinforcement Symbolic Regression Machine (RSRM) that masters the capability of uncovering complex math equations from only scarce data. The RSRM model is composed of three key modules: (1) a Monte Carlo tree search (MCTS) agent that explores optimal math expression trees consisting of pre-defined math operators and variables, (2) a Double Q-learning block that helps reduce the feasible search space of MCTS via pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Barkour&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#37327;&#21270;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#25935;&#25463;&#24615;&#12290;&#35813;&#27979;&#35797;&#27169;&#20223;&#29356;&#21482;&#25935;&#25463;&#31454;&#36187;&#65292;&#21253;&#21547;&#21508;&#31181;&#38556;&#30861;&#21644;&#22522;&#20110;&#26102;&#38388;&#30340;&#25171;&#20998;&#26426;&#21046;&#65292;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#33021;&#20197;&#21487;&#25511;&#21644;&#22810;&#29992;&#36884;&#30340;&#26041;&#24335;&#34892;&#21160;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#20415;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#24555;&#36895;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2305.14654</link><description>&lt;p&gt;
Barkour&#65306;&#29992;&#22235;&#36275;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#21160;&#29289;&#32423;&#21035;&#30340;&#28789;&#25935;&#24230;
&lt;/p&gt;
&lt;p&gt;
Barkour: Benchmarking Animal-level Agility with Quadruped Robots. (arXiv:2305.14654v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14654
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Barkour&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#37327;&#21270;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#25935;&#25463;&#24615;&#12290;&#35813;&#27979;&#35797;&#27169;&#20223;&#29356;&#21482;&#25935;&#25463;&#31454;&#36187;&#65292;&#21253;&#21547;&#21508;&#31181;&#38556;&#30861;&#21644;&#22522;&#20110;&#26102;&#38388;&#30340;&#25171;&#20998;&#26426;&#21046;&#65292;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#33021;&#20197;&#21487;&#25511;&#21644;&#22810;&#29992;&#36884;&#30340;&#26041;&#24335;&#34892;&#21160;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#20415;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#24555;&#36895;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#28436;&#21270;&#20986;&#21508;&#31181;&#25935;&#25463;&#30340;&#36816;&#21160;&#31574;&#30053;&#65292;&#20363;&#22914;&#22868;&#36305;&#12289;&#36339;&#36291;&#21644;&#36339;&#36291;&#12290;&#30446;&#21069;&#36234;&#26469;&#36234;&#26377;&#20852;&#36259;&#30740;&#21457;&#20687;&#29983;&#29289;&#21516;&#26679;&#31227;&#21160;&#24182;&#23637;&#31034;&#21508;&#31181;&#25935;&#25463;&#25216;&#33021;&#20197;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#24555;&#36895;&#23548;&#33322;&#30340;&#22235;&#33050;&#26426;&#22120;&#20154;&#12290;&#23613;&#31649;&#23384;&#22312;&#20852;&#36259;&#65292;&#20294;&#35813;&#39046;&#22495;&#32570;&#20047;&#31995;&#32479;&#22522;&#20934;&#26469;&#34913;&#37327;&#25511;&#21046;&#31574;&#30053;&#21644;&#30828;&#20214;&#22312;&#28789;&#27963;&#24615;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Barkour&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#22235;&#33050;&#26426;&#22120;&#20154;&#25935;&#25463;&#24615;&#30340;&#38556;&#30861;&#35838;&#31243;&#12290;&#21463;&#29356;&#21482;&#25935;&#25463;&#31454;&#36187;&#30340;&#21551;&#21457;&#65292;&#23427;&#30001;&#19981;&#21516;&#30340;&#38556;&#30861;&#29289;&#21644;&#22522;&#20110;&#26102;&#38388;&#30340;&#35745;&#20998;&#26426;&#21046;&#32452;&#25104;&#12290;&#36825;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#19981;&#20165;&#31227;&#21160;&#24555;&#36895;&#32780;&#19988;&#20197;&#21487;&#25511;&#21644;&#22810;&#29992;&#36884;&#30340;&#26041;&#24335;&#34892;&#21160;&#30340;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#24314;&#31435;&#24378;&#26377;&#21147;&#30340;&#22522;&#32447;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22788;&#29702;&#35813;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#19987;&#19994;&#30340;&#36816;&#21160;&#25216;&#33021;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#39640;&#32423;&#23548;&#33322;&#25511;&#21046;&#22120;&#30456;&#32467;&#21512;&#12290;&#22312;&#31532;&#20108;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#26174;&#31034;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26174;&#24335;&#22320;&#35757;&#32451;&#36153;&#29992;&#20989;&#25968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#25972;&#20010;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Animals have evolved various agile locomotion strategies, such as sprinting, leaping, and jumping. There is a growing interest in developing legged robots that move like their biological counterparts and show various agile skills to navigate complex environments quickly. Despite the interest, the field lacks systematic benchmarks to measure the performance of control policies and hardware in agility. We introduce the Barkour benchmark, an obstacle course to quantify agility for legged robots. Inspired by dog agility competitions, it consists of diverse obstacles and a time based scoring mechanism. This encourages researchers to develop controllers that not only move fast, but do so in a controllable and versatile way. To set strong baselines, we present two methods for tackling the benchmark. In the first approach, we train specialist locomotion skills using on-policy reinforcement learning methods and combine them with a high-level navigation controller. In the second approach, we dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;JTFT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25552;&#21462;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#20943;&#23567;&#35745;&#31639;&#38656;&#27714;&#65292;&#36866;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20302;&#31209;&#27880;&#24847;&#23618;&#20197;&#26377;&#25928;&#25429;&#33719;&#36328;&#32500;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14649</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32852;&#21512;&#26102;&#39057;&#22495;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Joint Time-frequency Domain Transformer for Multivariate Time Series Forecasting. (arXiv:2305.14649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;JTFT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25552;&#21462;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#20943;&#23567;&#35745;&#31639;&#38656;&#27714;&#65292;&#36866;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20302;&#31209;&#27880;&#24847;&#23618;&#20197;&#26377;&#25928;&#25429;&#33719;&#36328;&#32500;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#32852;&#21512;&#26102;&#39057;&#22495;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#35745;&#31639;&#38656;&#27714;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#39057;&#29575;&#30340;&#31232;&#30095;&#24615;&#65292;&#22312;&#39057;&#22495;&#26377;&#25928;&#22320;&#25552;&#21462;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#38500;&#20102;&#39057;&#29575;&#22495;&#34920;&#31034;&#22806;&#65292;&#26368;&#36817;&#30340;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#28857;&#20063;&#34987;&#30452;&#25509;&#32534;&#30721;&#22312;&#26102;&#38388;&#22495;&#20013;&#65292;&#20197;&#22686;&#24378;&#23398;&#20064;&#23616;&#37096;&#20851;&#31995;&#24182;&#20943;&#36731;&#38750;&#24179;&#31283;&#24615;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;JTFT&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#22240;&#20026;&#20869;&#37096;&#34920;&#31034;&#30340;&#38271;&#24230;&#20445;&#25345;&#29420;&#31435;&#20110;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20302;&#31209;&#27880;&#24847;&#23618;&#65292;&#20197;&#26377;&#25928;&#25429;&#33719;&#36328;&#32500;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#38450;&#27490;&#30001;&#20110;&#26102;&#38388;&#21644;&#36890;&#36947;&#24314;&#27169;&#30340;&#32416;&#32544;&#32780;&#23548;&#33268;&#24615;&#33021;&#38477;&#32423;&#12290; &#23545;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;JTFT&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;-of-the-art&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance predicting performance while minimizing computational demands, this paper introduces a joint time-frequency domain Transformer (JTFT) for multivariate forecasting. The method exploits the sparsity of time series in the frequency domain using a small number of learnable frequencies to extract temporal dependencies effectively. Alongside the frequency domain representation, a fixed number of the most recent data points are directly encoded in the time domain, bolstering the learning of local relationships and mitigating the adverse effects of non-stationarity. JTFT achieves linear complexity since the length of the internal representation remains independent of the input sequence length. Additionally, a low-rank attention layer is proposed to efficiently capture cross-dimensional dependencies and prevent performance degradation due to the entanglement of temporal and channel-wise modeling. Experiments conducted on six real-world datasets demonstrate that JTFT outperforms state
&lt;/p&gt;</description></item><item><title>CMOT&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#27169;&#24577;&#35821;&#38899;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#25214;&#21040;&#35821;&#38899;&#21644;&#25991;&#26412;&#24207;&#21015;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#28151;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14635</link><description>&lt;p&gt;
CMOT: &#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#36328;&#27169;&#24577;Mixup&#65292;&#29992;&#20110;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation. (arXiv:2305.14635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14635
&lt;/p&gt;
&lt;p&gt;
CMOT&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#27169;&#24577;&#35821;&#38899;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#25214;&#21040;&#35821;&#38899;&#21644;&#25991;&#26412;&#24207;&#21015;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#28151;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#26159;&#23558;&#28304;&#35821;&#35328;&#20013;&#30340;&#35821;&#38899;&#20449;&#21495;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#25991;&#26412;&#30340;&#20219;&#21153;&#12290;&#20316;&#20026;&#19968;&#39033;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#31471;&#21040;&#31471;ST&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#36827;&#34892;&#35757;&#32451;&#38750;&#24120;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#23581;&#35797;&#20174;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20013;&#36716;&#31227;&#30693;&#35782;&#65292;&#20294;&#20854;&#24615;&#33021;&#30001;&#20110;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Cross-modal Mixup via Optimal Transport&#65288;CMOT&#65289;&#26469;&#20811;&#26381;&#27169;&#24577;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#25214;&#21040;&#35821;&#38899;&#21644;&#25991;&#26412;&#24207;&#21015;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#28982;&#21518;&#20351;&#29992;&#23545;&#40784;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#28151;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#24207;&#21015;&#12290;&#22312;MuST-C ST&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CMOT&#22312;8&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;BLEU&#20540;&#20026;30.0&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;CMOT&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#25214;&#21040;&#27169;&#24577;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#26377;&#21161;&#20110;&#32531;&#35299;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110; https://github.com/ic
&lt;/p&gt;
&lt;p&gt;
End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but their performances are restricted by the modality gap between speech and text. In this paper, we propose Cross-modal Mixup via Optimal Transport CMOT to overcome the modality gap. We find the alignment between speech and text sequences via optimal transport and then mix up the sequences from different modalities at a token level using the alignment. Experiments on the MuST-C ST benchmark demonstrate that CMOT achieves an average BLEU of 30.0 in 8 translation directions, outperforming previous methods. Further analysis shows CMOT can adaptively find the alignment between modalities, which helps alleviate the modality gap between speech and text. Code is publicly available at https://github.com/ic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20114;&#26021;&#35299;&#37322;&#30340;&#35825;&#23548;&#24335;&#24120;&#35782;&#25512;&#29702;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31867;&#35825;&#23548;&#24335;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#25110;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20854;&#20182;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2305.14618</link><description>&lt;p&gt;
&#36890;&#36807;&#20114;&#26021;&#35299;&#37322;&#24320;&#23637;&#35825;&#23548;&#24335;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations. (arXiv:2305.14618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20114;&#26021;&#35299;&#37322;&#30340;&#35825;&#23548;&#24335;&#24120;&#35782;&#25512;&#29702;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31867;&#35825;&#23548;&#24335;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#25110;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20854;&#20182;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35825;&#23548;&#24335;&#25512;&#29702;&#26088;&#22312;&#23547;&#25214;&#19968;&#20010;&#20107;&#20214;&#30340;&#21487;&#33021;&#35299;&#37322;&#12290;&#22312;&#24120;&#35782;&#20219;&#21153;&#20013;&#65292;&#23384;&#22312;&#30528;&#22810;&#31181;&#21512;&#29702;&#30340;&#35299;&#37322;&#24418;&#24335;&#65292;&#22240;&#27492;&#35825;&#23548;&#24335;&#25512;&#29702;&#23545;&#20110;&#36825;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20851;&#20110;&#35825;&#23548;&#24335;&#25512;&#29702;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25163;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#20294;&#26159;&#36825;&#31181;&#26631;&#27880;&#24448;&#24448;&#24102;&#26377;&#20027;&#35266;&#24615;&#21644;&#20559;&#35265;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35825;&#23548;&#24335;&#24120;&#35782;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#23545;&#20110;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#65292;&#21482;&#26377;&#35299;&#37322;&#30340;&#19968;&#20010;&#23376;&#38598;&#26159;&#27491;&#30830;&#30340;&#12290;&#26412;&#26041;&#27861;&#21033;&#29992;&#21518;&#39564;&#35268;&#33539;&#21270;&#26469;&#26045;&#21152;&#20114;&#26021;&#32422;&#26463;&#65292;&#40723;&#21169;&#27169;&#22411;&#23398;&#20064;&#27969;&#30021;&#35299;&#37322;&#21644;&#21512;&#29702;&#30340;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#25105;&#20204;&#22312;&#21508;&#31867;&#35825;&#23548;&#24335;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#20248;&#20110;&#25110;&#19982;&#30452;&#25509;&#24212;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20854;&#20182;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abductive reasoning aims to find plausible explanations for an event. This style of reasoning is critical for commonsense tasks where there are often multiple plausible explanations. Existing approaches for abductive reasoning in natural language processing (NLP) often rely on manually generated annotations for supervision; however, such annotations can be subjective and biased. Instead of using direct supervision, this work proposes an approach for abductive commonsense reasoning that exploits the fact that only a subset of explanations is correct for a given context. The method uses posterior regularization to enforce a mutual exclusion constraint, encouraging the model to learn the distinction between fluent explanations and plausible ones. We evaluate our approach on a diverse set of abductive reasoning datasets; experimental results show that our approach outperforms or is comparable to directly applying pretrained language models in a zero-shot manner and other knowledge-augmente
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;COMET-M&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25512;&#29702;&#22797;&#26434;&#21477;&#23376;&#20013;&#22810;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#29983;&#25104;&#24120;&#35782;&#25512;&#26029;&#65292;&#24182;&#22312;35K&#20010;&#20154;&#31867;&#32534;&#20889;&#30340;&#25512;&#26029;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;COMET&#27169;&#22411;&#22312;&#29983;&#25104;&#22810;&#20107;&#20214;&#25512;&#26029;&#26041;&#38754;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.14617</link><description>&lt;p&gt;
COMET-M: &#22312;&#22797;&#26434;&#21477;&#23376;&#20013;&#25512;&#29702;&#22810;&#20010;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
COMET-M: Reasoning about Multiple Events in Complex Sentences. (arXiv:2305.14617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14617
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;COMET-M&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25512;&#29702;&#22797;&#26434;&#21477;&#23376;&#20013;&#22810;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#29983;&#25104;&#24120;&#35782;&#25512;&#26029;&#65292;&#24182;&#22312;35K&#20010;&#20154;&#31867;&#32534;&#20889;&#30340;&#25512;&#26029;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;COMET&#27169;&#22411;&#22312;&#29983;&#25104;&#22810;&#20107;&#20214;&#25512;&#26029;&#26041;&#38754;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#36890;&#24120;&#28041;&#21450;&#32472;&#21046;&#24120;&#35782;&#25512;&#26029;&#65292;&#20197;&#25512;&#29702;&#26410;&#26126;&#30830;&#38472;&#36848;&#30340;&#20869;&#23481;&#12290;&#22312;&#22810;&#20107;&#20214;&#21477;&#23376;&#20013;&#65292;&#38656;&#35201;&#22522;&#20110;&#19978;&#19979;&#25991;&#30693;&#35782;&#29702;&#35299;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;COMET-M&#65288;Multi-Event&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#24120;&#35782;&#27169;&#22411;&#65292;&#33021;&#22815;&#38024;&#23545;&#22797;&#26434;&#21477;&#23376;&#20869;&#30340;&#30446;&#26631;&#20107;&#20214;&#29983;&#25104;&#24120;&#35782;&#25512;&#26029;&#12290;COMET-M&#26159;&#22522;&#20110;COMET&#65288;Bosselut et al.&#65292;2019&#65289;&#21457;&#23637;&#32780;&#26469;&#30340;&#65292;&#21518;&#32773;&#25797;&#38271;&#20026;&#31616;&#21333;&#21477;&#23376;&#29983;&#25104;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#25512;&#26029;&#65292;&#20294;&#22312;&#33258;&#28982;&#25991;&#26412;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22810;&#20107;&#20214;&#21477;&#23376;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#21253;&#21547;35K&#20010;&#20154;&#31867;&#32534;&#20889;&#25512;&#26029;&#30340;&#22810;&#20107;&#20214;&#25512;&#26029;&#25968;&#25454;&#38598;&#12290; &#25105;&#20204;&#22312;&#20154;&#31867;&#32534;&#20889;&#30340;&#25512;&#26029;&#19978;&#35757;&#32451;&#20102;COMET-M&#65292;&#24182;&#21019;&#24314;&#20102;&#20351;&#29992;&#33258;&#21160;&#26631;&#35760;&#31034;&#20363;&#30340;&#22522;&#32447;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;COMET-M&#22312;&#29983;&#25104;&#22810;&#20107;&#20214;&#25512;&#26029;&#26041;&#38754;&#30456;&#23545;&#20110;COMET&#20855;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;COMET-M&#25104;&#21151;&#39044;&#27979;&#20102;&#27979;&#35797;&#38598;&#20013;60&#65285;&#30340;&#22797;&#26434;&#21477;&#23376;&#30446;&#26631;&#20107;&#20214;&#30340;&#24120;&#35782;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the speaker's intended meaning often involves drawing commonsense inferences to reason about what is not stated explicitly. In multi-event sentences, it requires understanding the relationships between events based on contextual knowledge. We propose COMET-M (Multi-Event), an event-centric commonsense model capable of generating commonsense inferences for a target event within a complex sentence. COMET-M builds upon COMET (Bosselut et al., 2019), which excels at generating event-centric inferences for simple sentences, but struggles with the complexity of multi-event sentences prevalent in natural text. To overcome this limitation, we curate a multi-event inference dataset of 35K human-written inferences. We trained COMET-M on the human-written inferences and also created baselines using automatically labeled examples. Experimental results demonstrate the significant performance improvement of COMET-M over COMET in generating multi-event inferences. Moreover, COMET-M succ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35299;&#20915;&#27169;&#31946;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#37325;&#22797;&#24615;&#65292;&#25214;&#20986;&#20102;&#22312;&#21547;&#31946;&#38382;&#39064;&#38598;&#20013;&#22238;&#31572;&#39640;&#31934;&#24230;&#23376;&#38598;&#38382;&#39064;&#30340;&#26368;&#21487;&#38752;&#26041;&#27861;&#12290;&#36825;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27495;&#20041;&#65292;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14613</link><description>&lt;p&gt;
&#23545;&#27169;&#31946;&#38382;&#39064;&#30340;&#26377;&#36873;&#25321;&#24615;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Selectively Answering Ambiguous Questions. (arXiv:2305.14613v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35299;&#20915;&#27169;&#31946;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#37325;&#22797;&#24615;&#65292;&#25214;&#20986;&#20102;&#22312;&#21547;&#31946;&#38382;&#39064;&#38598;&#20013;&#22238;&#31572;&#39640;&#31934;&#24230;&#23376;&#38598;&#38382;&#39064;&#30340;&#26368;&#21487;&#38752;&#26041;&#27861;&#12290;&#36825;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27495;&#20041;&#65292;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#22312;&#19981;&#30693;&#36947;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#25918;&#24323;&#22238;&#31572;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38382;&#38382;&#32773;&#24847;&#22270;&#25110;&#19978;&#19979;&#25991;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38382;&#39064;&#30340;&#31572;&#26696;&#20063;&#21487;&#33021;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#20174;&#36825;&#20010;&#35282;&#24230;&#35843;&#26597;&#20102;&#38382;&#39064;&#22238;&#31572;&#65292;&#19987;&#27880;&#20110;&#22312;&#20247;&#22810;&#26412;&#36136;&#19978;&#21547;&#31946;&#30340;&#38382;&#39064;&#38598;&#20013;&#22238;&#31572;&#39640;&#31934;&#24230;&#23376;&#38598;&#30340;&#38382;&#39064;&#12290;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23450;&#37327;&#27979;&#37327;&#19968;&#32452;&#37319;&#26679;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#37325;&#22797;&#24615;&#26159;&#26368;&#21487;&#38752;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#32780;&#38750;&#20808;&#21069;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#30340;&#27010;&#29575;&#25110;&#33258;&#25105;&#39564;&#35777;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#19981;&#21516;&#30340;&#27169;&#22411;&#35268;&#27169;&#65292;&#20197;&#21450;&#24102;&#25110;&#19981;&#24102;&#25351;&#23548;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27495;&#20041;&#65292;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustworthy language models should abstain from answering questions when they do not know the answer. However, the answer to a question can be unknown for a variety of reasons. Prior research has focused on the case in which the question is clear and the answer is unambiguous but possibly unknown. However, the answer to a question can also be unclear due to uncertainty of the questioner's intent or context. We investigate question answering from this perspective, focusing on answering a subset of questions with a high degree of accuracy, from a set of questions in which many are inherently ambiguous. In this setting, we find that the most reliable approach to calibration involves quantifying repetition within a set of sampled model outputs, rather than the model's likelihood or self-verification as used in prior work. % We find this to be the case across different types of uncertainty, varying model scales and both with or without instruction tuning. Our results suggest that sampling-b
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;AI&#23398;&#26415;&#30028;&#30340;78K&#30740;&#31350;&#20154;&#21592;&#30340;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#22899;&#24615;&#31532;&#19968;&#20316;&#32773;&#30340;&#35770;&#25991;&#20855;&#26377;&#19981;&#21516;&#30340;&#35821;&#35328;&#39118;&#26684;&#65292;&#20363;&#22914;&#26356;&#38271;&#30340;&#25991;&#26412;&#12289;&#26356;&#22810;&#30340;&#27491;&#38754;&#24773;&#24863;&#35789;&#27719;&#21644;&#26356;&#24341;&#20154;&#27880;&#30446;&#30340;&#26631;&#39064;&#65307;&#22312;AI&#35770;&#25991;&#30340;&#21512;&#33879;&#20013;&#23384;&#22312;&#24456;&#22823;&#30340;&#24615;&#21035;&#21516;&#36136;&#24615;&#12290;&#25105;&#20204;&#40723;&#21169;&#26410;&#26469;&#23454;&#29616;&#26356;&#22810;&#30340;&#24615;&#21035;&#24179;&#31561;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14597</link><description>&lt;p&gt;
&#22905;&#20204;&#30340;&#22768;&#38899;&#65306;&#20998;&#26512;&#20154;&#24037;&#26234;&#33021;&#20986;&#29256;&#39046;&#22495;&#30340;&#24615;&#21035;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Voices of Her: Analyzing Gender Differences in the AI Publication World. (arXiv:2305.14597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14597
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;AI&#23398;&#26415;&#30028;&#30340;78K&#30740;&#31350;&#20154;&#21592;&#30340;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#22899;&#24615;&#31532;&#19968;&#20316;&#32773;&#30340;&#35770;&#25991;&#20855;&#26377;&#19981;&#21516;&#30340;&#35821;&#35328;&#39118;&#26684;&#65292;&#20363;&#22914;&#26356;&#38271;&#30340;&#25991;&#26412;&#12289;&#26356;&#22810;&#30340;&#27491;&#38754;&#24773;&#24863;&#35789;&#27719;&#21644;&#26356;&#24341;&#20154;&#27880;&#30446;&#30340;&#26631;&#39064;&#65307;&#22312;AI&#35770;&#25991;&#30340;&#21512;&#33879;&#20013;&#23384;&#22312;&#24456;&#22823;&#30340;&#24615;&#21035;&#21516;&#36136;&#24615;&#12290;&#25105;&#20204;&#40723;&#21169;&#26410;&#26469;&#23454;&#29616;&#26356;&#22810;&#30340;&#24615;&#21035;&#24179;&#31561;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#20998;&#26512;&#20102;&#23398;&#26415;&#30028;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#26159;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#24615;&#21035;&#24046;&#24322;&#30340;&#20998;&#26512;&#65292;&#28085;&#30422;&#21508;&#31181;&#20027;&#39064;&#21644;&#19981;&#21516;&#30340;&#21457;&#23637;&#36235;&#21183;&#12290;&#25105;&#20204;&#20351;&#29992;AI Scholar&#25968;&#25454;&#38598;&#20013;&#30340;78K&#20301;AI&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#24615;&#21035;&#24046;&#24322;&#65306;&#65288;1&#65289;&#34429;&#28982;&#22899;&#24615;&#30740;&#31350;&#20154;&#21592;&#30340;&#24635;&#24341;&#29992;&#27425;&#25968;&#27604;&#30007;&#24615;&#23569;&#65292;&#20294;&#36825;&#31181;&#24341;&#29992;&#24046;&#24322;&#24182;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#23398;&#26415;&#24180;&#40836;&#32452;&#65307;&#65288;2&#65289;&#22312;AI&#35770;&#25991;&#30340;&#21512;&#33879;&#20013;&#23384;&#22312;&#24456;&#22823;&#30340;&#24615;&#21035;&#21516;&#36136;&#24615;&#65307;&#65288;3&#65289;&#22899;&#24615;&#31532;&#19968;&#20316;&#32773;&#30340;&#35770;&#25991;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#35821;&#35328;&#39118;&#26684;&#65292;&#20363;&#22914;&#26356;&#38271;&#30340;&#25991;&#26412;&#12289;&#26356;&#22810;&#30340;&#27491;&#38754;&#24773;&#24863;&#35789;&#27719;&#21644;&#26356;&#24341;&#20154;&#27880;&#30446;&#30340;&#26631;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#25105;&#20204;&#30340;AI&#31038;&#21306;&#29616;&#26377;&#30340;&#20154;&#21475;&#32479;&#35745;&#36235;&#21183;&#25552;&#20379;&#20102;&#19968;&#20010;&#31383;&#21475;&#65292;&#24182;&#40723;&#21169;&#22312;&#26410;&#26469;&#23454;&#29616;&#26356;&#22810;&#30340;&#24615;&#21035;&#24179;&#31561;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/causalNLP/ai-scholar-gender&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
While several previous studies have analyzed gender bias in research, we are still missing a comprehensive analysis of gender differences in the AI community, covering diverse topics and different development trends. Using the AI Scholar dataset of 78K researchers in the field of AI, we identify several gender differences: (1) Although female researchers tend to have fewer overall citations than males, this citation difference does not hold for all academic-age groups; (2) There exist large gender homophily in co-authorship on AI papers; (3) Female first-authored papers show distinct linguistic styles, such as longer text, more positive emotion words, and more catchy titles than male first-authored papers. Our analysis provides a window into the current demographic trends in our AI community, and encourages more gender equality and diversity in the future. Our code and data are at https://github.com/causalNLP/ai-scholar-gender.
&lt;/p&gt;</description></item><item><title>RE$^2$&#26041;&#27861;&#21033;&#29992;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#21306;&#22495;&#32423;&#31354;&#38388;&#32467;&#26500;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#20851;&#31995;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14590</link><description>&lt;p&gt;
RE$^2$: &#38754;&#21521;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#30340;&#21306;&#22495;&#24863;&#30693;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
RE$^2$: Region-Aware Relation Extraction from Visually Rich Documents. (arXiv:2305.14590v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14590
&lt;/p&gt;
&lt;p&gt;
RE$^2$&#26041;&#27861;&#21033;&#29992;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#21306;&#22495;&#32423;&#31354;&#38388;&#32467;&#26500;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#20851;&#31995;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#34920;&#21333;&#29702;&#35299;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#38656;&#35201;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24067;&#23616;&#32467;&#26500;&#65288;&#21363;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#65289;&#23545;&#20110;&#20851;&#31995;&#25277;&#21462;&#30340;&#37325;&#35201;&#24615;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; RE$^2$ &#30340;&#21306;&#22495;&#24863;&#30693;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#21306;&#22495;&#32423;&#31354;&#38388;&#32467;&#26500;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#20851;&#31995;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36793;&#32536;&#24863;&#30693;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#26469;&#23398;&#20064;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#21516;&#26102;&#32771;&#34385;&#23427;&#20204;&#30340;&#21306;&#22495;&#32423;&#34920;&#31034;&#25152;&#23450;&#20041;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#32422;&#26463;&#30446;&#26631;&#65292;&#26469;&#35268;&#33539;&#27169;&#22411;&#20197;&#31526;&#21512;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#22266;&#26377;&#32422;&#26463;&#26465;&#20214;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#35821;&#35328;&#21644;&#39046;&#22495;&#30340;&#24191;&#27867;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current research in form understanding predominantly relies on large pre-trained language models, necessitating extensive data for pre-training. However, the importance of layout structure (i.e., the spatial relationship between the entity blocks in the visually rich document) to relation extraction has been overlooked. In this paper, we propose REgion-Aware Relation Extraction (RE$^2$) that leverages region-level spatial structure among the entity blocks to improve their relation prediction. We design an edge-aware graph attention network to learn the interaction between entities while considering their spatial relationship defined by their region-level representations. We also introduce a constraint objective to regularize the model towards consistency with the inherent constraints of the relation extraction task. Extensive experiments across various datasets, languages and domains demonstrate the superiority of our proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#27169;&#22411;&#30340;&#20107;&#21518;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#31867;&#21035; - &#20869;&#22312;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#24120;&#35265;&#30340;&#35299;&#37322;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#20986;&#20102;&#26102;&#38388;&#24207;&#21015;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.14582</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#27169;&#22411;&#30340;&#35299;&#37322;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Interpretation of Time-Series Deep Models: A Survey. (arXiv:2305.14582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#27169;&#22411;&#30340;&#20107;&#21518;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#31867;&#21035; - &#20869;&#22312;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#24120;&#35265;&#30340;&#35299;&#37322;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#20986;&#20102;&#26102;&#38388;&#24207;&#21015;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#24320;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#19981;&#30452;&#35266;&#24615;&#65292;&#35299;&#37322;&#24615;&#38382;&#39064;&#8212;&#8212;&#25105;&#20204;&#22914;&#20309;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#37096;&#21407;&#29702;&#8212;&#8212;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#31867;&#20284;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#31867;&#20284;&#30740;&#31350;&#25512;&#21160;&#20102;&#35768;&#22810;&#20107;&#21518;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#21487;&#20197;&#38416;&#26126;&#22914;&#20309;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#12289;&#25200;&#21160;&#12289;&#36924;&#36817;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#24819;&#30528;&#37325;&#20171;&#32461;&#20869;&#22312;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#21487;&#20154;&#29702;&#35299;&#30340;&#20449;&#24687;&#35774;&#35745;&#21040;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#35299;&#37322;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#35299;&#37322;&#30340;&#24120;&#35265;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#26410;&#26469;&#30740;&#31350;&#30340;&#20960;&#20010;&#26041;&#21521;&#12290;&#20540;&#24471;&#19968;&#25552;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#20165;&#24635;&#32467;&#20102;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#27169;&#22411;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#20107;&#21518;&#26041;&#27861;&#65292;&#36824;&#20984;&#26174;&#20102;&#21457;&#23637;&#20869;&#22312;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26032;&#20852;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models developed for time-series associated tasks have become more widely researched nowadays. However, due to the unintuitive nature of time-series data, the interpretability problem -- where we understand what is under the hood of these models -- becomes crucial. The advancement of similar studies in computer vision has given rise to many post-hoc methods, which can also shed light on how to explain time-series models. In this paper, we present a wide range of post-hoc interpretation methods for time-series models based on backpropagation, perturbation, and approximation. We also want to bring focus onto inherently interpretable models, a novel category of interpretation where human-understandable information is designed within the models. Furthermore, we introduce some common evaluation metrics used for the explanations, and propose several directions of future researches on the time-series interpretability problem. As a highlight, our work summarizes not only the well
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23545;&#33889;&#33796;&#29273;&#35821;&#20013;&#30340;Whisper ASR&#36827;&#34892;&#20102;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#20026;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#22312;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.14580</link><description>&lt;p&gt;
&#35780;&#20272;OpenAI&#25552;&#20379;&#30340;Whisper ASR&#22312;Museum of the Person&#30340;&#29983;&#27963;&#21490;&#20013;&#36827;&#34892;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#21644;&#20027;&#39064;&#24314;&#27169;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Evaluating OpenAI's Whisper ASR for Punctuation Prediction and Topic Modeling of life histories of the Museum of the Person. (arXiv:2305.14580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;&#33889;&#33796;&#29273;&#35821;&#20013;&#30340;Whisper ASR&#36827;&#34892;&#20102;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#20026;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#22312;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#22312;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#21313;&#24180;&#20013;&#25552;&#20986;&#30340;&#33889;&#33796;&#29273;&#35821;ASR&#27169;&#22411;&#22312;&#27491;&#30830;&#35782;&#21035;&#33258;&#21160;&#36716;&#24405;&#20013;&#30340;&#26631;&#28857;&#31526;&#21495;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#36825;&#20351;&#24471;&#36825;&#20123;&#36716;&#24405;&#19981;&#33021;&#34987;&#20854;&#20182;&#31995;&#32479;&#12289;&#27169;&#22411;&#21644;&#29978;&#33267;&#26159;&#20154;&#31867;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;OpenAI&#25552;&#20986;&#20102;Whisper ASR&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#26377;&#26395;&#22788;&#29702;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#27425;&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#20013;Whisper&#30340;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#24615;&#33021;&#36827;&#34892;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#39564;&#35780;&#20272;&#26469;&#32771;&#34385;&#20851;&#20110;&#20572;&#39039;&#28857;&#65288;&#36887;&#21495;&#65289;&#21644;&#23436;&#25972;&#24605;&#24819;&#65288;&#24863;&#21497;&#12289;&#30097;&#38382;&#21644;&#21477;&#21495;&#65289;&#30340;&#29702;&#35770;&#26041;&#38754;&#65292;&#20197;&#21450;&#19982;&#22522;&#20110;&#36716;&#24405;&#30340;&#20027;&#39064;&#24314;&#27169;&#30456;&#20851;&#30340;&#23454;&#38469;&#26041;&#38754;&#65292;&#20351;&#29992;&#26631;&#28857;&#31526;&#21495;&#26469;&#25552;&#39640;&#24615;&#33021;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) systems play a key role in applications involving human-machine interactions. Despite their importance, ASR models for the Portuguese language proposed in the last decade have limitations in relation to the correct identification of punctuation marks in automatic transcriptions, which hinder the use of transcriptions by other systems, models, and even by humans. However, recently Whisper ASR was proposed by OpenAI, a general-purpose speech recognition model that has generated great expectations in dealing with such limitations. This chapter presents the first study on the performance of Whisper for punctuation prediction in the Portuguese language. We present an experimental evaluation considering both theoretical aspects involving pausing points (comma) and complete ideas (exclamation, question, and fullstop), as well as practical aspects involving transcript-based topic modeling - an application dependent on punctuation marks for promising performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2305.14561</link><description>&lt;p&gt;
&#36127;&#21453;&#39304;&#35757;&#32451;&#65306;&#25552;&#39640;NVCiM DNN&#21152;&#36895;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Negative Feedback Training: A Novel Concept to Improve Robustness of NVCiM DNN Accelerators. (arXiv:2305.14561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#25381;&#21457;&#24615;&#23384;&#20648;&#22120;(NVM)&#23454;&#29616;&#30340;&#20869;&#23384;&#35745;&#31639;(CiM)&#20026;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290; CiM&#21152;&#36895;&#22120;&#36890;&#36807;&#22312;&#21516;&#19968;&#30005;&#36335;&#26495;&#32467;&#26500;&#20013;&#23384;&#20648;&#32593;&#32476;&#26435;&#37325;&#21644;&#25191;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20197;&#26368;&#23567;&#30340;&#38754;&#31215;&#38656;&#27714;&#21644;&#24322;&#24120;&#30340;&#33021;&#25928;&#65292;&#25552;&#20379;DNN&#25512;&#29702;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;NVM&#35774;&#22791;&#30340;&#38543;&#26426;&#24615;&#21644;&#20869;&#22312;&#21464;&#21270;&#24448;&#24448;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#22914;&#19982;&#39044;&#26399;&#32467;&#26524;&#30456;&#27604;&#20943;&#23569;&#20998;&#31867;&#31934;&#24230;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#35774;&#22791;&#21464;&#24322;&#24182;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#25972;&#20307;&#35843;&#33410;&#24182;&#32570;&#20047;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#12290;&#21463;&#21040;&#36127;&#21453;&#39304;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20986;&#21475;&#26426;&#21046;&#20316;&#20026;&#36127;&#21453;&#39304;&#65292;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compute-in-Memory (CiM) utilizing non-volatile memory (NVM) devices presents a highly promising and efficient approach for accelerating deep neural networks (DNNs). By concurrently storing network weights and performing matrix operations within the same crossbar structure, CiM accelerators offer DNN inference acceleration with minimal area requirements and exceptional energy efficiency. However, the stochasticity and intrinsic variations of NVM devices often lead to performance degradation, such as reduced classification accuracy, compared to expected outcomes. Although several methods have been proposed to mitigate device variation and enhance robustness, most of them rely on overall modulation and lack constraints on the training process. Drawing inspiration from the negative feedback mechanism, we introduce a novel training approach that uses a multi-exit mechanism as negative feedback to enhance the performance of DNN models in the presence of device variation. Our negative feedbac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;ChatGPT&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#21644;&#27880;&#37322;&#30340;&#28508;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#29983;&#25104;&#30340;&#23545;&#35805;&#21644;&#27880;&#37322;&#36136;&#37327;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2305.14556</link><description>&lt;p&gt;
&#25581;&#31034;ChatGPT: AI-&#29983;&#25104;&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#21644;&#27880;&#37322;&#30340;&#20851;&#38190;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations. (arXiv:2305.14556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;ChatGPT&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#21644;&#27880;&#37322;&#30340;&#28508;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#29983;&#25104;&#30340;&#23545;&#35805;&#21644;&#27880;&#37322;&#36136;&#37327;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#31034;&#25216;&#26415;&#20026;&#25163;&#27573;&#20135;&#29983;&#39640;&#36136;&#37327;&#25991;&#26412;&#30340;&#33021;&#21147;&#21069;&#25152;&#26410;&#26377;&#12290;&#36825;&#19968;&#20107;&#23454;&#20026;&#25968;&#25454;&#25910;&#38598;&#21644;&#27880;&#37322;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#36825;&#31181;&#25968;&#25454;&#31232;&#32570;&#12289;&#38590;&#20197;&#25910;&#38598;&#12289;&#26114;&#36149;&#29978;&#33267;&#25935;&#24863;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#21644;&#27880;&#37322;&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#30340;&#28508;&#21147;&#65292;&#24182;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#20197;&#35780;&#20272;&#23427;&#20204;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#37319;&#29992;ChatGPT&#65292;&#24182;&#21253;&#25324;&#19977;&#31867;&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#65288;&#20219;&#21153;&#23548;&#21521;&#12289;&#21327;&#20316;&#21644;&#35828;&#26126;&#24615;&#65289;&#12289;&#20004;&#31181;&#29983;&#25104;&#27169;&#24335;&#65288;&#20132;&#20114;&#24335;&#21644;&#19968;&#27425;&#24615;&#65289;&#21644;&#20004;&#31181;&#35821;&#35328;&#65288;&#33521;&#35821;&#21644;&#24847;&#22823;&#21033;&#35821;&#65289;&#12290;&#22522;&#20110;&#24191;&#27867;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29983;&#25104;&#30340;&#23545;&#35805;&#21644;&#27880;&#37322;&#30340;&#36136;&#37327;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models have exhibited unprecedented capabilities in producing high-quality text via prompting techniques. This fact introduces new possibilities for data collection and annotation, particularly in situations where such data is scarce, complex to gather, expensive, or even sensitive. In this paper, we explore the potential of these models to generate and annotate goal-oriented dialogues, and conduct an in-depth analysis to evaluate their quality. Our experiments employ ChatGPT, and encompass three categories of goal-oriented dialogues (task-oriented, collaborative, and explanatory), two generation modes (interactive and one-shot), and two languages (English and Italian). Based on extensive human-based evaluations, we demonstrate that the quality of generated dialogues and annotations is on par with those generated by humans.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;Transformer&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21452;&#23556;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;BERT-INN&#65292;&#26469;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21452;&#23556;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.14555</link><description>&lt;p&gt;
&#25152;&#26377;&#36947;&#36335;&#36890;&#24448;&#32599;&#39532;&#65311;&#25506;&#31350;Transformer&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
All Roads Lead to Rome? Exploring the Invariance of Transformers' Representations. (arXiv:2305.14555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;Transformer&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21452;&#23556;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;BERT-INN&#65292;&#26469;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21452;&#23556;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#24102;&#26469;&#20102;&#26497;&#22823;&#30340;&#36827;&#23637;&#65292;&#22240;&#27492;&#24341;&#21457;&#20102;&#23545;&#27169;&#22411;&#23398;&#20064;&#30340;&#34920;&#31034;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#34920;&#31034;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#31350;Transformer&#26159;&#21542;&#23398;&#20064;&#21040;&#20102;&#26412;&#36136;&#19978;&#21516;&#26500;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#25110;&#32773;&#36825;&#20123;&#34920;&#31034;&#31354;&#38388;&#26159;&#21542;&#23545;&#20854;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#31181;&#23376;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#23556;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;BERT-INN&#65292;&#26469;&#27604;&#20854;&#20182;&#29616;&#26377;&#21452;&#23556;&#26041;&#27861;&#65288;&#22914;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#65289;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21452;&#23556;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;BERT-INN&#30340;&#20248;&#21183;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23545;&#40784;&#37325;&#29616;&#30340;BERT&#23884;&#20837;&#65292;&#20197;&#33719;&#24471;&#23545;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#38142;&#25509;&#22312;&#25991;&#31456;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models bring propelling advances in various NLP tasks, thus inducing lots of interpretability research on the learned representations of the models. However, we raise a fundamental question regarding the reliability of the representations. Specifically, we investigate whether transformers learn essentially isomorphic representation spaces, or those that are sensitive to the random seeds in their pretraining process. In this work, we formulate the Bijection Hypothesis, which suggests the use of bijective methods to align different models' representation spaces. We propose a model based on invertible neural networks, BERT-INN, to learn the bijection more effectively than other existing bijective methods such as the canonical correlation analysis (CCA). We show the advantage of BERT-INN both theoretically and through extensive experiments, and apply it to align the reproduced BERT embeddings to draw insights that are meaningful to the interpretability research. Our code is at 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#28431;&#27934;&#19982;&#20256;&#32479;&#36719;&#20214;&#28431;&#27934;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#25913;&#21892;AI&#28431;&#27934;&#31649;&#29702;&#21644;&#32531;&#35299;&#30340;&#28508;&#22312;&#39046;&#22495;&#65292;&#20197;&#21450;&#23545;AI&#31995;&#32479;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#27861;&#24459;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14553</link><description>&lt;p&gt;
&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#19982;&#32593;&#32476;&#23433;&#20840;&#65306;&#39118;&#38505;&#12289;&#25361;&#25112;&#21644;&#27861;&#24459;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Adversarial Machine Learning and Cybersecurity: Risks, Challenges, and Legal Implications. (arXiv:2305.14553v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#28431;&#27934;&#19982;&#20256;&#32479;&#36719;&#20214;&#28431;&#27934;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#25913;&#21892;AI&#28431;&#27934;&#31649;&#29702;&#21644;&#32531;&#35299;&#30340;&#28508;&#22312;&#39046;&#22495;&#65292;&#20197;&#21450;&#23545;AI&#31995;&#32479;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#27861;&#24459;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2022&#24180;7&#26376;&#65292;&#20052;&#27835;&#22478;&#22823;&#23398;&#23433;&#20840;&#19982;&#26032;&#20852;&#25216;&#26415;&#20013;&#24515;&#65288;CSET&#65289;&#21644;&#26031;&#22374;&#31119;&#32593;&#32476;&#25919;&#31574;&#20013;&#24515;&#30340;&#22320;&#32536;&#25919;&#27835;&#12289;&#25216;&#26415;&#21644;&#27835;&#29702;&#39033;&#30446;&#21484;&#38598;&#20102;&#19968;&#25209;&#19987;&#23478;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#28431;&#27934;&#19982;&#20256;&#32479;&#36719;&#20214;&#28431;&#27934;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35752;&#35770;&#30340;&#20027;&#39064;&#21253;&#25324;AI&#28431;&#27934;&#22312;&#26631;&#20934;&#32593;&#32476;&#23433;&#20840;&#27969;&#31243;&#20013;&#30340;&#22788;&#29702;&#31243;&#24230;&#65292;&#30446;&#21069;&#38459;&#27490;&#20851;&#20110;AI&#28431;&#27934;&#20934;&#30830;&#20849;&#20139;&#30340;&#38556;&#30861;&#65292;&#23545;AI&#31995;&#32479;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#27861;&#24459;&#38382;&#39064;&#65292;&#20197;&#21450;&#25919;&#24220;&#25903;&#25345;&#21487;&#33021;&#25913;&#21892;AI&#28431;&#27934;&#31649;&#29702;&#21644;&#32531;&#35299;&#30340;&#28508;&#22312;&#39046;&#22495;&#12290;&#26412;&#25253;&#21578;&#26088;&#22312;&#23436;&#25104;&#20004;&#20214;&#20107;&#12290;&#39318;&#20808;&#65292;&#25552;&#20379;&#20102;&#23545;AI&#28431;&#27934;&#30340;&#39640;&#23618;&#27425;&#35752;&#35770;&#65292;&#21253;&#25324;&#23427;&#20204;&#22914;&#20309;&#19981;&#21516;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#28431;&#27934;&#20197;&#21450;&#20851;&#20110;&#20449;&#24687;&#20849;&#20139;&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
In July 2022, the Center for Security and Emerging Technology (CSET) at Georgetown University and the Program on Geopolitics, Technology, and Governance at the Stanford Cyber Policy Center convened a workshop of experts to examine the relationship between vulnerabilities in artificial intelligence systems and more traditional types of software vulnerabilities. Topics discussed included the extent to which AI vulnerabilities can be handled under standard cybersecurity processes, the barriers currently preventing the accurate sharing of information about AI vulnerabilities, legal issues associated with adversarial attacks on AI systems, and potential areas where government support could improve AI vulnerability management and mitigation.  This report is meant to accomplish two things. First, it provides a high-level discussion of AI vulnerabilities, including the ways in which they are disanalogous to other types of vulnerabilities, and the current state of affairs regarding information 
&lt;/p&gt;</description></item><item><title>&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#26356;&#36866;&#21512;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#19979;&#30340;&#36873;&#25321;&#65292;&#22312;&#20219;&#21153;&#33539;&#22260;&#22686;&#21152;&#26102;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.14550</link><description>&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#31454;&#20105;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence Modeling is a Robust Contender for Offline Reinforcement Learning. (arXiv:2305.14550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14550
&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#26356;&#36866;&#21512;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#19979;&#30340;&#36873;&#25321;&#65292;&#22312;&#20219;&#21153;&#33539;&#22260;&#22686;&#21152;&#26102;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#26368;&#22823;&#21270;&#25910;&#30410;&#31574;&#30053;&#12290;&#31163;&#32447;RL&#30340;&#19977;&#22823;&#33539;&#24335;&#26159;Q-Learning&#12289;Imitation Learning&#21644;Sequence Modeling&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#26159;&#65306;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#65292;&#21738;&#31181;&#33539;&#24335;&#34987;&#20248;&#20808;&#36873;&#25321;&#65311;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034;&#20195;&#34920;&#24615;&#31639;&#27861;&#8212;&#8212;&#20445;&#23432;Q-Learning(CQL)&#12289;&#34892;&#20026;&#20811;&#38534; (BC)&#21644;&#20915;&#31574;Transformer (DT)&#8212;&#8212;&#22312;&#24120;&#29992;&#30340;D4RL&#21644;Robomimic&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#26469;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#23454;&#39564;&#26469;&#29702;&#35299;&#23427;&#20204;&#22312;&#25968;&#25454;&#23376;&#20248;&#24615;&#21644;&#20219;&#21153;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;(1)&#24207;&#21015;&#24314;&#27169;&#38656;&#35201;&#27604;Q-Learning&#26356;&#22810;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#31454;&#20105;&#24615;&#31574;&#30053;&#65292;&#20294;&#26356;&#21152;&#31283;&#20581;&#65307;(2)&#24207;&#21015;&#24314;&#27169;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#37117;&#35201;&#22909;&#24471;&#22810;&#65307;(3)&#38543;&#30528;&#20219;&#21153;&#33539;&#22260;&#30340;&#22686;&#21152;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) allows agents to learn effective, return-maximizing policies from a static dataset. Three major paradigms for offline RL are Q-Learning, Imitation Learning, and Sequence Modeling. A key open question is: which paradigm is preferred under what conditions? We study this question empirically by exploring the performance of representative algorithms -- Conservative Q-Learning (CQL), Behavior Cloning (BC), and Decision Transformer (DT) -- across the commonly used D4RL and Robomimic benchmarks. We design targeted experiments to understand their behavior concerning data suboptimality and task complexity. Our key findings are: (1) Sequence Modeling requires more data than Q-Learning to learn competitive policies but is more robust; (2) Sequence Modeling is a substantially better choice than both Q-Learning and Imitation Learning in sparse-reward and low-quality data settings; and (3) Sequence Modeling and Imitation Learning are preferable as task horizon inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32423;&#32852;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#26415;&#35821;&#32422;&#26463;&#65292;&#21487;&#20197;&#21363;&#25554;&#21363;&#29992;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21487;&#23558;&#30446;&#26631;&#26415;&#35821;&#30340;&#27010;&#29575;&#22686;&#21152;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#32593;&#26684;&#26463;&#25628;&#32034;&#30340;&#32423;&#32852;&#26463;&#35774;&#32622;&#65292;&#24615;&#33021;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.14538</link><description>&lt;p&gt;
&#32423;&#32852;&#26463;&#25628;&#32034;&#65306;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#21363;&#25554;&#21363;&#29992;&#26415;&#35821;&#32422;&#26463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cascaded Beam Search: Plug-and-Play Terminology-Forcing For Neural Machine Translation. (arXiv:2305.14538v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32423;&#32852;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#26415;&#35821;&#32422;&#26463;&#65292;&#21487;&#20197;&#21363;&#25554;&#21363;&#29992;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21487;&#23558;&#30446;&#26631;&#26415;&#35821;&#30340;&#27010;&#29575;&#22686;&#21152;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#32593;&#26684;&#26463;&#25628;&#32034;&#30340;&#32423;&#32852;&#26463;&#35774;&#32622;&#65292;&#24615;&#33021;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#26415;&#35821;&#32422;&#26463;&#30340;&#32763;&#35793;&#30340;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#12290;&#26415;&#35821;&#32422;&#26463;&#26159;&#35768;&#22810;&#29616;&#20195;&#32763;&#35793;&#27969;&#31243;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#22312;&#19987;&#19994;&#39046;&#22495;&#21644;&#26032;&#20852;&#39046;&#22495;&#65288;&#22914;COVID-19&#30123;&#24773;&#65289;&#20013;&#65292;&#20934;&#30830;&#32763;&#35793;&#19987;&#19994;&#26415;&#35821;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#27169;&#22411;&#20174;&#36755;&#20837;&#20013;&#22797;&#21046;&#26415;&#35821;&#24182;&#23558;&#20854;&#39304;&#36865;&#21040;&#30446;&#26631;&#35821;&#21477;&#20013;&#12290;&#20294;&#36825;&#38656;&#35201;&#26114;&#36149;&#30340;&#22521;&#35757;&#65292;&#27599;&#24403;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#21457;&#29983;&#21464;&#21270;&#25110;&#31995;&#32479;&#38656;&#35201;&#19987;&#38376;&#21270;&#21040;&#26032;&#39046;&#22495;&#26102;&#37117;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#32423;&#32852;&#26463;&#25628;&#32034;&#65292;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26415;&#35821;&#24378;&#21046;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#12290;&#36825;&#31181;&#32423;&#32852;&#26463;&#25628;&#32034;&#26377;&#20004;&#20010;&#37096;&#20998;&#65306;1&#65289;&#36923;&#36753;&#25805;&#20316;&#65292;&#22686;&#21152;&#30446;&#26631;&#26415;&#35821;&#30340;&#27010;&#29575;&#65307;2&#65289;&#22522;&#20110;&#32593;&#26684;&#26463;&#25628;&#32034;&#30340;&#32423;&#32852;&#26463;&#35774;&#32622;&#65292;&#20854;&#20013;&#26681;&#25454;&#23427;&#20204;&#21253;&#21547;&#30340;&#26415;&#35821;&#25968;&#23558;&#26463;&#32452;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#31454;&#20105;&#26041;&#24335;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a plug-and-play approach for translation with terminology constraints. Terminology constraints are an important aspect of many modern translation pipelines. In both specialized domains and newly emerging domains (such as the COVID-19 pandemic), accurate translation of technical terms is crucial. Recent approaches often train models to copy terminologies from the input into the output sentence by feeding the target terminology along with the input. But this requires expensive training whenever the underlying language model is changed or the system should specialize to a new domain. We propose Cascade Beam Search, a plug-and-play terminology-forcing approach that requires no training. Cascade Beam Search has two parts: 1) logit manipulation to increase the probability of target terminologies and 2) a cascading beam setup based on grid beam search, where beams are grouped by the number of terminologies they contain. We evaluate the performance of our approach by compet
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36991;&#20813;&#36807;&#28388;&#27668;&#27873;&#12289;&#26356;&#20844;&#24179;&#22320;&#20998;&#25285;&#22810;&#26679;&#21270;&#36127;&#25285;&#30340;&#31038;&#20132;&#32593;&#32476;&#20869;&#23481;&#31574;&#21010;&#21644;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14537</link><description>&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#26497;&#31471;&#21270;&#38450;&#33539;
&lt;/p&gt;
&lt;p&gt;
Disincentivizing Polarization in Social Networks. (arXiv:2305.14537v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36991;&#20813;&#36807;&#28388;&#27668;&#27873;&#12289;&#26356;&#20844;&#24179;&#22320;&#20998;&#25285;&#22810;&#26679;&#21270;&#36127;&#25285;&#30340;&#31038;&#20132;&#32593;&#32476;&#20869;&#23481;&#31574;&#21010;&#21644;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#65292;&#31639;&#27861;&#20010;&#24615;&#21270;&#23558;&#29992;&#25143;&#24102;&#20837;&#20102;&#36807;&#28388;&#27668;&#27873;&#20013;&#65292;&#24456;&#23569;&#30475;&#21040;&#20559;&#31163;&#20182;&#20204;&#20852;&#36259;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36991;&#20813;&#36807;&#28388;&#27668;&#27873;&#30340;&#20869;&#23481;&#31574;&#21010;&#21644;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#20197;&#21450;&#31639;&#27861;&#20445;&#35777;&#21644;&#20960;&#20046;&#21305;&#37197;&#30340;&#19979;&#38480;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#24179;&#21488;&#22312;$T$&#26102;&#38388;&#27493;&#38271;&#20869;&#19982;$n$&#29992;&#25143;&#36827;&#34892;&#20114;&#21160;&#65292;&#20174;$k$&#31867;&#21035;&#20026;&#27599;&#20010;&#29992;&#25143;&#36873;&#25321;&#20869;&#23481;&#12290;&#20687;&#22810;&#33218;&#36172;&#21338;&#26426;&#19968;&#26679;&#65292;&#24179;&#21488;&#25509;&#25910;&#38543;&#26426;&#22870;&#21169;&#12290;&#20026;&#20102;&#36991;&#20813;&#36807;&#28388;&#27668;&#27873;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#36825;&#26679;&#19968;&#20010;&#30452;&#35273;&#65306;&#22914;&#26524;&#26576;&#20123;&#29992;&#25143;&#30475;&#21040;&#26576;&#20123;&#31867;&#21035;&#30340;&#20869;&#23481;&#65292;&#21017;&#25152;&#26377;&#29992;&#25143;&#37117;&#24212;&#35813;&#33267;&#23569;&#30475;&#21040;&#19968;&#23567;&#37096;&#20998;&#35813;&#20869;&#23481;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#36825;&#31181;&#30452;&#35273;&#30340;&#19968;&#20010;&#22825;&#30495;&#30340;&#24418;&#24335;&#21270;&#65292;&#35777;&#26126;&#23427;&#20855;&#26377;&#24847;&#24819;&#19981;&#21040;&#30340;&#21518;&#26524;&#65306;&#23427;&#23548;&#33268;"&#22810;&#25968;&#20154;&#30340;&#26292;&#25919;"&#65292;&#20351;&#23569;&#25968;&#20852;&#36259;&#30340;&#20154;&#20998;&#25285;&#20102;&#22810;&#26679;&#21270;&#30340;&#36127;&#25285;&#12290;&#36825;&#23548;&#33268;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#26356;&#20844;&#24179;&#22320;&#20998;&#25285;&#20102;&#36825;&#31181;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
On social networks, algorithmic personalization drives users into filter bubbles where they rarely see content that deviates from their interests. We present a model for content curation and personalization that avoids filter bubbles, along with algorithmic guarantees and nearly matching lower bounds. In our model, the platform interacts with $n$ users over $T$ timesteps, choosing content for each user from $k$ categories. The platform receives stochastic rewards as in a multi-arm bandit. To avoid filter bubbles, we draw on the intuition that if some users are shown some category of content, then all users should see at least a small amount of that content. We first analyze a naive formalization of this intuition and show it has unintended consequences: it leads to ``tyranny of the majority'' with the burden of diversification borne disproportionately by those with minority interests. This leads us to our model which distributes this burden more equitably. We require that the probabili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#22312;&#28151;&#21512;&#35821;&#35328;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#25216;&#26415;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#20219;&#21153;&#65292;&#20316;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;1030&#20010;&#25991;&#26412;&#30340;&#33521;&#35821;&#21644;&#32599;&#39532;&#20044;&#23572;&#28151;&#21512;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.14534</link><description>&lt;p&gt;
&#22312;&#28151;&#21512;&#35821;&#35328;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Detecting Propaganda Techniques in Code-Switched Social Media Text. (arXiv:2305.14534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#22312;&#28151;&#21512;&#35821;&#35328;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#25216;&#26415;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#20219;&#21153;&#65292;&#20316;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;1030&#20010;&#25991;&#26412;&#30340;&#33521;&#35821;&#21644;&#32599;&#39532;&#20044;&#23572;&#28151;&#21512;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23459;&#20256;&#26159;&#19968;&#31181;&#26088;&#22312;&#24433;&#21709;&#20844;&#20247;&#33286;&#35770;&#21644;&#24515;&#24577;&#20197;&#25512;&#24191;&#29305;&#23450;&#35758;&#31243;&#30340;&#27807;&#36890;&#24418;&#24335;&#12290;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#23835;&#36215;&#65292;&#23459;&#20256;&#24050;&#32463;&#36805;&#36895;&#20256;&#25773;&#65292;&#24341;&#21457;&#20102;&#23545;&#33258;&#21160;&#23459;&#20256;&#26816;&#27979;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#22823;&#22810;&#25968;&#23459;&#20256;&#26816;&#27979;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#33521;&#35821;&#65289;&#19978;&#65292;&#20960;&#20046;&#27809;&#26377;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#26816;&#27979;&#23459;&#20256;&#20570;&#20986;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#20132;&#27969;&#20013;&#21457;&#29616;&#22810;&#31181;&#35821;&#35328;&#30340;&#28151;&#21512;&#29616;&#35937;&#26159;&#24456;&#24120;&#35265;&#30340;&#65292;&#36825;&#34987;&#31216;&#20026;&#30721;&#28151;&#12290;&#30721;&#28151;&#22312;&#21516;&#19968;&#25991;&#26412;&#20013;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#65292;&#36825;&#23545;&#20110;&#33258;&#21160;&#31995;&#32479;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#27492;&#25552;&#20986;&#20102;&#26816;&#27979;&#28151;&#21512;&#25991;&#26412;&#20013;&#23459;&#20256;&#25216;&#26415;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;1030&#20010;&#25991;&#26412;&#30340;&#35821;&#26009;&#24211;&#65292;&#36825;&#20123;&#25991;&#26412;&#22312;&#33521;&#35821;&#21644;&#32599;&#39532;&#20044;&#23572;&#37117;&#36827;&#34892;&#20102;&#28151;&#21512;&#65292;&#24182;&#29992;20&#31181;&#23459;&#20256;&#25216;&#24039;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#25105;&#20204;&#24050;&#32463;&#20844;&#24320;&#20102;&#36825;&#20010;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#23545;&#27604;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#29305;&#24449;&#38598;&#21512;&#22312;&#27492;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Propaganda is a form of communication intended to influence the opinions and the mindset of the public to promote a particular agenda. With the rise of social media, propaganda has spread rapidly, leading to the need for automatic propaganda detection systems. Most work on propaganda detection has focused on high-resource languages, such as English, and little effort has been made to detect propaganda for low-resource languages. Yet, it is common to find a mix of multiple languages in social media communication, a phenomenon known as code-switching. Code-switching combines different languages within the same text, which poses a challenge for automatic systems. With this in mind, here we propose the novel task of detecting propaganda techniques in code-switched text. To support this task, we create a corpus of 1,030 texts code-switching between English and Roman Urdu, annotated with 20 propaganda techniques, which we make publicly available. We perform a number of experiments contrastin
&lt;/p&gt;</description></item><item><title>CongFu&#26159;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#26032;&#22411;&#26465;&#20214;&#22270;&#34701;&#21512;&#23618;&#65292;&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#29942;&#39048;&#25216;&#26415;&#25552;&#21462;&#26412;&#22320;&#22270;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#26377;&#26465;&#20214;&#22320;&#34701;&#21512;&#22270;&#25968;&#25454;&#12290;&#20854;&#27169;&#22359;&#21270;&#26550;&#26500;&#20351;&#24471;&#21487;&#20197;&#26356;&#25442;&#22270;&#23618;&#27169;&#22359;&#65292;&#21253;&#25324;&#35835;&#20986;&#21644;&#22270;&#32534;&#30721;&#22120;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#22312;12&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;11&#20010;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14517</link><description>&lt;p&gt;
CongFu: &#29992;&#20110;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#26465;&#20214;&#22270;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
CongFu: Conditional Graph Fusion for Drug Synergy Prediction. (arXiv:2305.14517v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14517
&lt;/p&gt;
&lt;p&gt;
CongFu&#26159;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#26032;&#22411;&#26465;&#20214;&#22270;&#34701;&#21512;&#23618;&#65292;&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#29942;&#39048;&#25216;&#26415;&#25552;&#21462;&#26412;&#22320;&#22270;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#26377;&#26465;&#20214;&#22320;&#34701;&#21512;&#22270;&#25968;&#25454;&#12290;&#20854;&#27169;&#22359;&#21270;&#26550;&#26500;&#20351;&#24471;&#21487;&#20197;&#26356;&#25442;&#22270;&#23618;&#27169;&#22359;&#65292;&#21253;&#25324;&#35835;&#20986;&#21644;&#22270;&#32534;&#30721;&#22120;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#22312;12&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;11&#20010;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21327;&#21516;&#26159;&#25351;&#22810;&#31181;&#33647;&#29289;&#32852;&#21512;&#20316;&#29992;&#25152;&#20135;&#29983;&#30340;&#21512;&#25104;&#25928;&#24212;&#65292;&#23545;&#20110;&#20248;&#21270;&#27835;&#30103;&#32467;&#26524;&#38750;&#24120;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#30340;&#33647;&#29289;&#32452;&#21512;&#25968;&#37327;&#24040;&#22823;&#65292;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#23548;&#33268;&#33647;&#29289;&#21327;&#21516;&#25968;&#25454;&#26377;&#38480;&#65292;&#38656;&#35201;&#39044;&#27979;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#22270;&#34701;&#21512;&#23618;&#8212;&#8212;CongFu&#65292;&#29992;&#20110;&#39044;&#27979;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#12290;CongFu&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#29942;&#39048;&#25216;&#26415;&#26469;&#25552;&#21462;&#26412;&#22320;&#22270;&#19978;&#19979;&#25991;&#24182;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#26377;&#26465;&#20214;&#22320;&#34701;&#21512;&#22270;&#25968;&#25454;&#12290;&#20854;&#27169;&#22359;&#21270;&#26550;&#26500;&#20351;&#24471;&#21487;&#20197;&#26356;&#25442;&#22270;&#23618;&#27169;&#22359;&#65292;&#21253;&#25324;&#35835;&#20986;&#21644;&#22270;&#32534;&#30721;&#22120;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#20026;&#20102;&#35780;&#20272;CongFu&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#35774;&#32622;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CongFu&#22312;12&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;11&#20010;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug synergy, characterized by the amplified combined effect of multiple drugs, presents a critical phenomenon for optimizing therapeutic outcomes. However, limited data on drug synergy, arising from the vast number of possible drug combinations and computational costs, motivate the need for predictive methods. In this work, we introduce CongFu, a novel Conditional Graph Fusion Layer, designed to predict drug synergy. CongFu employs an attention mechanism and a bottleneck to extract local graph contexts and conditionally fuse graph data within a global context. Its modular architecture enables flexible replacement of layer modules, including readouts and graph encoders, facilitating customization for diverse applications. To evaluate the performance of CongFu, we conduct comprehensive experiments on four datasets, encompassing three distinct setups for drug synergy prediction. Remarkably, CongFu achieves state-of-the-art results on 11 out of 12 benchmark datasets, demonstrating its abi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RetICL &#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#22320;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#30340;&#31034;&#20363;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#24207;&#21015;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#19988;&#20248;&#21270;&#36873;&#25321;&#20026;&#20351;&#20219;&#21153;&#34920;&#29616;&#26368;&#20339;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.14502</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#39034;&#24207;&#26816;&#32034;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;RetICL
&lt;/p&gt;
&lt;p&gt;
RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning. (arXiv:2305.14502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RetICL &#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#22320;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#30340;&#31034;&#20363;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#24207;&#21015;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#19988;&#20248;&#21270;&#36873;&#25321;&#20026;&#20351;&#20219;&#21153;&#34920;&#29616;&#26368;&#20339;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#20013;&#30340;&#35768;&#22810;&#21457;&#23637;&#37117;&#38598;&#20013;&#22312;&#20419;&#20351;&#23427;&#20204;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#20010;&#65288;&#25110;&#22810;&#20010;&#65289;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#65288;&#21487;&#33021;&#26159;&#26032;&#30340;&#65289;&#29983;&#25104;/&#39044;&#27979;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#31034;&#20363;&#30340;&#36873;&#25321;&#21487;&#33021;&#23545;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#22909;&#30340;&#31034;&#20363;&#24182;&#19981;&#26159;&#31616;&#21333;&#30340;&#65292;&#22240;&#20026;&#20195;&#34920;&#24615;&#31034;&#20363;&#32452;&#30340;&#23450;&#20041;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#19981;&#21516;&#32780;&#22823;&#19981;&#30456;&#21516;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#36873;&#25321;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#29420;&#31435;&#22320;&#23545;&#31034;&#20363;&#36827;&#34892;&#35780;&#20998;&#65292;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31034;&#20363;&#30340;&#39034;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#26041;&#27861;&#8212;&#8212;In-Context Learning&#30340;&#26816;&#32034;RetICL&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#36880;&#27493;&#36873;&#25321;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#25105;&#20204;&#25226;&#39034;&#24207;&#31034;&#20363;&#36873;&#25321;&#30340;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent developments in large language models focus on prompting them to perform specific tasks. One effective prompting method is in-context learning, where the model performs a (possibly new) generation/prediction task given one (or more) examples. Past work has shown that the choice of examples can make a large impact on task performance. However, finding good examples is not straightforward since the definition of a representative group of examples can vary greatly depending on the task. While there are many existing methods for selecting in-context examples, they generally score examples independently, ignoring the dependency between them and the order in which they are provided to the large language model. In this work, we propose Retrieval for In-Context Learning (RetICL), a learnable method for modeling and optimally selecting examples sequentially for in-context learning. We frame the problem of sequential example selection as a Markov decision process, design an example r
&lt;/p&gt;</description></item><item><title>Self-Polish&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#36880;&#27493;&#23436;&#21892;&#32473;&#23450;&#30340;&#38382;&#39064;&#20197;&#20351;&#20854;&#26356;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#20915;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14497</link><description>&lt;p&gt;
&#33258;&#25105;&#31934;&#30952;&#65306;&#36890;&#36807;&#38382;&#39064;&#31934;&#21270;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement. (arXiv:2305.14497v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14497
&lt;/p&gt;
&lt;p&gt;
Self-Polish&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#36880;&#27493;&#23436;&#21892;&#32473;&#23450;&#30340;&#38382;&#39064;&#20197;&#20351;&#20854;&#26356;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#20915;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992; Chain-of-Thought&#65288;CoT&#65289;&#31561;&#25552;&#31034;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#25506;&#32034;&#20102;&#21512;&#29702;&#21270;&#21644;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#30053;&#20102;&#20302;&#36136;&#37327;&#25512;&#29702;&#38382;&#39064;&#21487;&#33021;&#20250;&#26174;&#30528;&#24433;&#21709;&#25512;&#29702;&#24615;&#33021;&#30340;&#28508;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Self-Polish&#65288;SP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#36880;&#27493;&#23436;&#21892;&#32473;&#23450;&#30340;&#38382;&#39064;&#20197;&#20351;&#20854;&#26356;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#20915;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#25945;&#25480;&#27169;&#22411;&#28040;&#38500;&#26080;&#20851;&#20449;&#24687;&#65292;&#37325;&#26032;&#25490;&#21015;&#36923;&#36753;&#32467;&#26500;&#65292;&#24182;&#23558;&#23616;&#37096;&#26465;&#20214;&#24182;&#34892;&#32452;&#32455;&#25104;&#26032;&#30340;&#26465;&#20214;&#12290; SP&#19982;&#25152;&#26377;&#20854;&#20182;&#25552;&#31034;&#26041;&#27861;&#27491;&#20132;&#65292;&#26041;&#20415;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#38598;&#25104;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#20197;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting methods such as Chain-of-Thought (CoT) have shed new light on enhancing the reasoning capabilities of large language models, and researchers have extensively explored the generation process of rationales and answers. However, they have overlooked the potential challenges posed by the poor quality of reasoning problems, which may influence the reasoning performance significantly. In this work, we propose Self-Polish (SP), a novel method that facilitates the model's problem-solving process by prompting them to progressively refine the given problems to be more comprehensible and solvable. Specifically, the method teaches models to eliminate irrelevant information, rearrange the logic structure and organize local conditions into new ones parallelly. SP is orthogonal to all other prompting methods, making it convenient to integrate with state-of-the-art techniques for further improvement. We conduct thorough experiments on five benchmarks to illustrate the effectiveness of the pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;&#65288;BAND&#65289;&#65292;&#20854;&#21253;&#25324;&#20102;1,508&#20010;&#26679;&#26412;&#21644;30&#20010;&#19982;&#27969;&#34892;&#30149;&#23398;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#20869;&#23481;&#20266;&#35013;&#33021;&#21147;&#21644;&#37325;&#35201;&#20449;&#24687;&#25512;&#26029;&#33021;&#21147;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#20026;&#30142;&#30149;&#30417;&#27979;&#21644;&#27969;&#34892;&#30149;&#23398;&#20998;&#26512;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14480</link><description>&lt;p&gt;
BAND: &#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BAND: Biomedical Alert News Dataset. (arXiv:2305.14480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14480
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;&#65288;BAND&#65289;&#65292;&#20854;&#21253;&#25324;&#20102;1,508&#20010;&#26679;&#26412;&#21644;30&#20010;&#19982;&#27969;&#34892;&#30149;&#23398;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#20869;&#23481;&#20266;&#35013;&#33021;&#21147;&#21644;&#37325;&#35201;&#20449;&#24687;&#25512;&#26029;&#33021;&#21147;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#20026;&#30142;&#30149;&#30417;&#27979;&#21644;&#27969;&#34892;&#30149;&#23398;&#20998;&#26512;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#26579;&#24615;&#30142;&#30149;&#30340;&#29190;&#21457;&#23545;&#20154;&#31867;&#20581;&#24247;&#21644;&#31119;&#21033;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#20026;&#20102;&#25913;&#21892;&#30142;&#30149;&#30417;&#27979;&#21644;&#20102;&#35299;&#30142;&#30149;&#20256;&#25773;&#24773;&#20917;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#20960;&#20010;&#30417;&#27979;&#31995;&#32479;&#26469;&#30417;&#35270;&#27599;&#26085;&#26032;&#38395;&#35686;&#25253;&#21644;&#31038;&#20132;&#23186;&#20307;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#32463;&#36807;&#33391;&#22909;&#27880;&#37322;&#30340;&#25253;&#21578;&#25968;&#25454;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#19982;&#30456;&#24212;&#25552;&#37266;&#25110;&#26032;&#38395;&#30340;&#27969;&#34892;&#30149;&#23398;&#20998;&#26512;&#26041;&#38754;&#32570;&#20047;&#20005;&#35880;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;&#65288;BAND&#65289;&#65292;&#21253;&#25324;&#26469;&#33258;&#29616;&#26377;&#25253;&#21578;&#26032;&#38395;&#25991;&#31456;&#12289;&#20844;&#24320;&#30005;&#23376;&#37038;&#20214;&#21644;&#25552;&#37266;&#30340;1,508&#20010;&#26679;&#26412;&#20197;&#21450;30&#20010;&#19982;&#27969;&#34892;&#30149;&#23398;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#38656;&#35201;&#27169;&#22411;&#19987;&#23478;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#20026;&#30142;&#30149;&#29190;&#21457;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;BAND&#25968;&#25454;&#38598;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#20869;&#23481;&#20266;&#35013;&#33021;&#21147;&#21644;&#37325;&#35201;&#20449;&#24687;&#25512;&#26029;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#22522;&#20934;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infectious disease outbreaks continue to pose a significant threat to human health and well-being. To improve disease surveillance and understanding of disease spread, several surveillance systems have been developed to monitor daily news alerts and social media. However, existing systems lack thorough epidemiological analysis in relation to corresponding alerts or news, largely due to the scarcity of well-annotated reports data. To address this gap, we introduce the Biomedical Alert News Dataset (BAND), which includes 1,508 samples from existing reported news articles, open emails, and alerts, as well as 30 epidemiology-related questions. These questions necessitate the model's expert reasoning abilities, thereby offering valuable insights into the outbreak of the disease. The BAND dataset brings new challenges to the NLP world, requiring better disguise capability of the content and the ability to infer important information. We provide several benchmark tasks, including Named Entity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;</title><link>http://arxiv.org/abs/2305.14459</link><description>&lt;p&gt;
&#36890;&#36807;&#25688;&#35201;&#20108;&#20803;&#24615;&#21644;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Generation through Summarization Duality and Explicit Outline Control. (arXiv:2305.14459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24320;&#25918;&#24335;&#38271;&#25991;&#26412;&#29983;&#25104;&#38754;&#20020;&#35821;&#20041;&#19981;&#36830;&#36143;&#21644;&#24773;&#33410;&#19981;&#21487;&#20449;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#35774;&#35745;&#26080;&#30417;&#30563;&#20219;&#21153;&#20013;&#30340;&#30701;&#35821;&#25110;&#25277;&#35937;&#20449;&#21495;&#30340;&#22823;&#32434;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#65292;&#20294;&#36825;&#24448;&#24448;&#26159;&#19981;&#31283;&#23450;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#12290;&#22312;&#20551;&#35774;&#25688;&#35201;&#20316;&#20026;&#24050;&#25104;&#29087;&#30340;&#22823;&#32434;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#12289;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#25688;&#35201;&#20219;&#21153;&#30340;&#21452;&#37325;&#29305;&#24449;&#26469;&#25913;&#36827;&#22823;&#32434;&#39044;&#27979;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#22823;&#32434;&#30340;&#29983;&#25104;&#20855;&#26377;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#26080;&#35770;&#26159;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-2&#12289;BART&#65289;&#36824;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Vicuna&#12289;ChatGPT&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically open-ended long text generation poses significant challenges due to semantic incoherence and plot implausibility. Previous works usually alleviate this problem through outlines in the form of short phrases or abstractive signals by designing unsupervised tasks, which tend to be unstable and weakly interpretable.  Assuming that a summary serves as a mature outline, we introduce a two-stage, summary-enhanced outline supervised generation framework. This framework leverages the dual characteristics of the summarization task to improve outline prediction, resulting in more explicit and plausible outlines. Furthermore, we identify an underutilization issue in outline-based generation with both standard pretrained language models (e.g., GPT-2, BART) and large language models (e.g., Vicuna, ChatGPT). To address this, we propose a novel explicit outline control method for more effective utilization of generated outlines.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.14456</link><description>&lt;p&gt;
&#22312;&#31048;&#31095;&#20043;&#21518;&#21917;&#21860;&#37202;&#65311;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14456
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#25991;&#21270;&#20559;&#35265;&#65311;&#35821;&#35328;&#27169;&#22411;&#31526;&#21512;&#25152;&#26381;&#21153;&#31038;&#21306;&#30340;&#25991;&#21270;&#22240;&#32032;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#34920;&#26126;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#20135;&#29983;&#35199;&#26041;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#32780;&#38750;&#38463;&#25289;&#20271;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20174;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#19978;&#25910;&#38598;&#30340;&#33258;&#28982;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#21644;&#22522;&#20110;&#21487;&#33021;&#24615;&#35780;&#20998;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#36825;&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#38463;&#25289;&#20271;&#35821;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#35199;&#26041;&#25991;&#21270;&#20559;&#35265;&#65292;&#21253;&#25324;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#12290;&#24403;&#36755;&#20837;&#30340;&#38463;&#25289;&#20271;&#35821;&#21477;&#23376;&#36234;&#25509;&#36817;&#33521;&#35821;&#26102;&#65292;&#27169;&#22411;&#20063;&#26356;&#23481;&#26131;&#34920;&#29616;&#20986;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#20154;&#20204;&#23545;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#24212;&#26356;&#22810;&#32771;&#34385;&#25991;&#21270;&#22240;&#32032;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are language models culturally biased? It is important that language models conform to the cultural aspects of the communities they serve. However, we show in this paper that language models suffer from a significant bias towards Western culture when handling and generating text in Arabic, often preferring, and producing Western-fitting content as opposed to the relevant Arab content. We quantify this bias through a likelihood scoring-based metric using naturally occurring contexts that we collect from online social media. Our experiments reveal that both Arabic monolingual and multilingual models exhibit bias towards Western culture in eight different cultural aspects: person names, food, clothing, location, literature, beverage, religion, and sports. Models also tend to exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English. These findings raise concerns about the cultural relevance of current language models. Our analyses show that pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#31232;&#30095;&#32593;&#26684;&#20248;&#21270;&#32467;&#26500;&#21270;&#26680;&#25554;&#20540;&#26041;&#27861;&#65288;SKI&#65289;&#65292;&#22312;&#20445;&#35777;&#25554;&#20540;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36755;&#20837;&#28857;&#32500;&#24230;&#36739;&#39640;&#24102;&#26469;&#30340;&#35745;&#31639;&#22256;&#38590;&#65292;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#31232;&#30095;&#32593;&#26684;&#30697;&#38453;&#20056;&#27861;&#31639;&#27861;&#20197;&#21450;&#39640;&#25928;&#25554;&#20540;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.14451</link><description>&lt;p&gt;
&#31232;&#30095;&#32593;&#26684;&#30340;&#26680;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Kernel Interpolation with Sparse Grids. (arXiv:2305.14451v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#31232;&#30095;&#32593;&#26684;&#20248;&#21270;&#32467;&#26500;&#21270;&#26680;&#25554;&#20540;&#26041;&#27861;&#65288;SKI&#65289;&#65292;&#22312;&#20445;&#35777;&#25554;&#20540;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36755;&#20837;&#28857;&#32500;&#24230;&#36739;&#39640;&#24102;&#26469;&#30340;&#35745;&#31639;&#22256;&#38590;&#65292;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#31232;&#30095;&#32593;&#26684;&#30697;&#38453;&#20056;&#27861;&#31639;&#27861;&#20197;&#21450;&#39640;&#25928;&#25554;&#20540;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#26680;&#25554;&#20540;&#65288;SKI&#65289;&#36890;&#36807;&#20351;&#29992;&#24863;&#24212;&#28857;&#30340;&#23494;&#38598;&#32593;&#26684;&#25554;&#20540;&#26680;&#21327;&#26041;&#24046;&#20989;&#25968;&#65292;&#21152;&#36895;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#25512;&#26029;&#12290;&#23545;&#24212;&#30340;&#26680;&#30697;&#38453;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;&#22240;&#27492;&#26131;&#20110;&#24555;&#36895;&#36827;&#34892;&#32447;&#24615;&#20195;&#25968;&#35745;&#31639;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;SKI&#22312;&#36755;&#20837;&#28857;&#30340;&#32500;&#24230;&#26041;&#38754;&#30340;&#35268;&#27169;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23494;&#38598;&#32593;&#26684;&#22823;&#23567;&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;SKI&#26694;&#26550;&#20869;&#20351;&#29992;&#31232;&#30095;&#32593;&#26684;&#12290;&#36825;&#20123;&#32593;&#26684;&#33021;&#22815;&#36827;&#34892;&#20934;&#30830;&#30340;&#25554;&#20540;&#65292;&#20294;&#28857;&#25968;&#38543;&#32500;&#24230;&#30340;&#22686;&#21152;&#26356;&#24930;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#32447;&#24615;&#26102;&#38388;&#30340;&#31232;&#30095;&#32593;&#26684;&#26680;&#30697;&#38453;&#30690;&#37327;&#20056;&#27861;&#31639;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#23558;&#31232;&#30095;&#32593;&#26684;&#19982;&#22522;&#20110;&#31616;&#21333;&#24418;&#24335;&#30340;&#39640;&#25928;&#25554;&#20540;&#26041;&#26696;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#36825;&#20123;&#25913;&#36827;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SKI&#21487;&#20197;&#25193;&#23637;&#21040;&#26356;&#39640;&#30340;&#32500;&#24230;&#32780;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured kernel interpolation (SKI) accelerates Gaussian process (GP) inference by interpolating the kernel covariance function using a dense grid of inducing points, whose corresponding kernel matrix is highly structured and thus amenable to fast linear algebra. Unfortunately, SKI scales poorly in the dimension of the input points, since the dense grid size grows exponentially with the dimension. To mitigate this issue, we propose the use of sparse grids within the SKI framework. These grids enable accurate interpolation, but with a number of points growing more slowly with dimension. We contribute a novel nearly linear time matrix-vector multiplication algorithm for the sparse grid kernel matrix. Next, we describe how sparse grids can be combined with an efficient interpolation scheme based on simplices. With these changes, we demonstrate that SKI can be scaled to higher dimensions while maintaining accuracy.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#65292;&#22312;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#20869;&#23384;BFGS&#31639;&#27861;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14449</link><description>&lt;p&gt;
&#22270;&#35889;&#36935;&#35265;LLM&#65306;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#30340;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding. (arXiv:2305.14449v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14449
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#65292;&#22312;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#20869;&#23384;BFGS&#31639;&#27861;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;Alexa&#65292;Siri&#65292;Google Assistant&#31561;&#65289;&#38656;&#35201;&#29702;&#35299;&#23384;&#22312;&#32570;&#38519;&#30340;&#26597;&#35810;&#20197;&#30830;&#20445;&#31283;&#20581;&#30340;&#20250;&#35805;&#29702;&#35299;&#24182;&#20943;&#23569;&#29992;&#25143;&#25705;&#25830;&#12290;&#36825;&#20123;&#26377;&#32570;&#38519;&#30340;&#26597;&#35810;&#36890;&#24120;&#26159;&#30001;&#29992;&#25143;&#30340;&#27495;&#20041;&#21644;&#38169;&#35823;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20013;&#30340;&#38169;&#35823;&#24341;&#36215;&#30340;&#12290;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#65288;&#20010;&#24615;&#21270;QR&#65289;&#26088;&#22312;&#20943;&#23569;&#36523;&#20307;&#21644;&#23614;&#37096;&#29992;&#25143;&#26597;&#35810;&#27969;&#37327;&#20013;&#30340;&#32570;&#38519;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#19982;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#21435;&#25104;&#21151;&#30340;&#29992;&#25143;&#20132;&#20114;&#30340;&#32034;&#24341;&#12290;&#26412;&#25991;&#25552;&#20986;&#25105;&#20204;&#30340;&#8220;&#21327;&#21516;&#26597;&#35810;&#37325;&#20889;&#8221;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#37325;&#20889;&#29992;&#25143;&#21382;&#21490;&#20013;&#27809;&#26377;&#20986;&#29616;&#36807;&#30340;&#26032;&#22411;&#29992;&#25143;&#20132;&#20114;&#12290;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;&#29992;&#25143;&#21453;&#39304;&#20132;&#20114;&#22270;&#8221;&#65288;FIG&#65289;&#65292;&#30001;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#32452;&#25104;&#65292;&#24182;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#26469;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65288;&#21363;&#21327;&#21516;&#29992;&#25143;&#32034;&#24341;&#65289;&#65292;&#20174;&#32780;&#24110;&#21161;&#35206;&#30422;&#26410;&#26469;&#26410;&#26366;&#35265;&#36807;&#30340;&#23384;&#22312;&#32570;&#38519;&#30340;&#26597;&#35810;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#20123;&#26032;&#30340;&#20016;&#23500;&#32034;&#24341;&#34987;&#22122;&#22768;&#21453;&#39304;&#20132;&#20114;&#25152;&#25903;&#37197;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26377;&#38480;&#20869;&#23384;BFGS&#65288;LLM&#65289;&#31639;&#27861;&#21644;&#22238;&#36864;&#26041;&#26696;&#26469;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;QR&#26041;&#27861;&#65292;&#24182;&#22312;&#26410;&#30475;&#21040;&#30340;&#29992;&#25143;&#20132;&#20114;&#19978;&#21462;&#24471;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational AI systems (e.g. Alexa, Siri, Google Assistant, etc.) need to understand queries with defects to ensure robust conversational understanding and reduce user frictions. The defective queries are often induced by user ambiguities and mistakes, or errors in the automatic speech recognition (ASR) and natural language understanding (NLU).  Personalized query rewriting (personalized QR) targets reducing defects in the torso and tail user query traffic, and it typically relies on an index of past successful user interactions with the conversational AI. This paper presents our "Collaborative Query Rewriting" approach that focuses on rewriting novel user interactions unseen in the user history. This approach builds a "user Feedback Interaction Graph" (FIG) consisting of historical user-entity interactions, and leverages multi-hop customer affinity to enrich each user's index (i.e. the Collaborative User Index) that would help cover future unseen defective queries. To counteract th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#23398;&#20064;&#30340;&#22270;&#20687;&#25805;&#20316;&#31995;&#32479;NeuroSIM&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22810;&#36339;&#25351;&#20196;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#65292;&#21482;&#38656;&#35201;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#25110;&#36229;&#36807;SOTA&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.14410</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#36339;&#25351;&#20196;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;&#8212;&#8212;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Image Manipulation via Multi-Hop Instructions -- A New Dataset and Weakly-Supervised Neuro-Symbolic Approach. (arXiv:2305.14410v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14410
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#23398;&#20064;&#30340;&#22270;&#20687;&#25805;&#20316;&#31995;&#32479;NeuroSIM&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22810;&#36339;&#25351;&#20196;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#65292;&#21482;&#38656;&#35201;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#25110;&#36229;&#36807;SOTA&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;&#24863;&#20852;&#36259;&#65292;&#36825;&#26159;&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#31243;&#24207;&#20013;&#26377;&#29992;&#30340;&#20219;&#21153;&#65292;&#20294;&#38656;&#35201;&#23545;&#22810;&#27169;&#24577;&#31354;&#38388;&#36827;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#23398;&#20064;(NSCL)&#65292;&#35813;&#26041;&#27861;&#22312;&#35270;&#35273;&#38382;&#31572;(VQA)&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#25928;&#65292;&#25193;&#23637;&#20854;&#29992;&#20110;&#22270;&#20687;&#25805;&#20316;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#31216;&#20026;NeuroSIM&#65292;&#21487;&#20197;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#19978;&#25191;&#34892;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#65292;&#21482;&#38656;&#35201;&#20197;VQA&#30340;&#27880;&#37322;&#25968;&#25454;&#24418;&#24335;&#25552;&#20379;&#24369;&#30417;&#30563;&#12290;NeuroSIM&#23558;&#25351;&#20196;&#35299;&#26512;&#25104;&#31526;&#21495;&#31243;&#24207;&#65292;&#22522;&#20110;&#30001;&#23545;&#35937;&#23646;&#24615;&#21644;&#25805;&#20316;&#32452;&#25104;&#30340;&#19987;&#19994;&#39046;&#22495;&#35821;&#35328;(DSL)&#65292;&#25351;&#23548;&#20854;&#25191;&#34892;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NeuroSIM&#19982;&#20351;&#29992;&#30417;&#30563;&#25968;&#25454;&#36827;&#34892;&#25805;&#20316;&#30340;SOTA&#22522;&#32447;&#30456;&#27604;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#25110;&#36229;&#36807;SOTA&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are interested in image manipulation via natural language text -- a task that is useful for multiple AI applications but requires complex reasoning over multi-modal spaces. We extend recently proposed Neuro Symbolic Concept Learning (NSCL), which has been quite effective for the task of Visual Question Answering (VQA), for the task of image manipulation. Our system referred to as NeuroSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NeuroSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides its execution. We create a new dataset for the task, and extensive experiments demonstrate that NeuroSIM is highly competitive with or beats SOTA baselines that make use of supervised data for manipulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22312;&#32447;&#26102;&#23578;&#34892;&#19994;&#20013;&#65292;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#38144;&#37327;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#25968;&#25454;&#37327;&#22823;&#12289;&#19981;&#35268;&#21017;&#24615;&#12289;&#39640;&#39057;&#29575;&#26356;&#36845;&#30340;&#20135;&#21697;&#30446;&#24405;&#21644;&#22266;&#23450;&#24211;&#23384;&#20551;&#35774;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.14406</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22312;&#32447;&#26102;&#23578;&#34892;&#19994;&#38144;&#37327;&#39044;&#27979;&#65306;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning based Forecasting: a case study from the online fashion industry. (arXiv:2305.14406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22312;&#32447;&#26102;&#23578;&#34892;&#19994;&#20013;&#65292;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#38144;&#37327;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#25968;&#25454;&#37327;&#22823;&#12289;&#19981;&#35268;&#21017;&#24615;&#12289;&#39640;&#39057;&#29575;&#26356;&#36845;&#30340;&#20135;&#21697;&#30446;&#24405;&#21644;&#22266;&#23450;&#24211;&#23384;&#20551;&#35774;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#26102;&#23578;&#34892;&#19994;&#30340;&#38144;&#37327;&#39044;&#27979;&#30001;&#20110;&#20854;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#36866;&#21512;&#37319;&#29992;&#20840;&#29699;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#25968;&#25454;&#37327;&#22823;&#12289;&#19981;&#35268;&#21017;&#24615;&#12289;&#20135;&#21697;&#30446;&#24405;&#39640;&#39057;&#29575;&#26356;&#36845;&#20197;&#21450;&#22266;&#23450;&#24211;&#23384;&#20551;&#35774;&#12290;&#34429;&#28982;&#26631;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#26041;&#27861;&#21487;&#20197;&#24212;&#23545;&#20854;&#20013;&#22823;&#37096;&#20998;&#38382;&#39064;&#65292;&#20294;&#22266;&#23450;&#24211;&#23384;&#20551;&#35774;&#38656;&#35201;&#36890;&#36807;&#23494;&#20999;&#25511;&#21046;&#20215;&#26684;&#19982;&#38656;&#27714;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#29305;&#27530;&#22788;&#29702;&#12290;&#22312;&#36825;&#39033;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#36825;&#19968;&#39044;&#27979;&#38382;&#39064;&#30340;&#25968;&#25454;&#21644;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demand forecasting in the online fashion industry is particularly amendable to global, data-driven forecasting models because of the industry's set of particular challenges. These include the volume of data, the irregularity, the high amount of turn-over in the catalog and the fixed inventory assumption. While standard deep learning forecasting approaches cater for many of these, the fixed inventory assumption requires a special treatment via controlling the relationship between price and demand closely. In this case study, we describe the data and our modelling approach for this forecasting problem in detail and present empirical results that highlight the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14405</link><description>&lt;p&gt;
NeuralMatrix: &#23558;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#31227;&#21160;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix: Moving Entire Neural Networks to General Matrix Multiplication for Efficient Inference. (arXiv:2305.14405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14405
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NeuralMatrix&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#22810;&#21151;&#33021;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#22522;&#20110;ASIC&#30340;&#21152;&#36895;&#22120;&#30340;&#19987;&#29992;&#24615;&#38480;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;CPU&#21644;GPU&#31561;&#36890;&#29992;&#22788;&#29702;&#22120;&#30456;&#27604;&#30340;&#24212;&#29992;&#29305;&#23450;&#21152;&#36895;&#27700;&#24179;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;DNN&#35745;&#31639;&#20013;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36816;&#31639;&#26144;&#23556;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#21450;&#20351;&#29992;GEMM&#21152;&#36895;&#22120;&#23545;DNN&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19977;&#31181;&#27969;&#34892;&#31867;&#21035;&#30340;&#21508;&#31181;DNN&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65288;&#21363;CNN&#65292;Transformers&#21644;GNN&#65289;&#20316;&#20026;&#31034;&#20363;&#30340;&#25903;&#25745;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;DNN&#36716;&#25442;&#20026;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21518;&#20165;&#20250;&#20986;&#29616;&#39640;&#36798;2.02&#65285;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#65292;&#21516;&#26102;&#23558;&#21534;&#21520;&#37327;&#19982;&#21151;&#29575;&#30340;&#27604;&#20540;&#19982;CPU&#21644;GPU&#30456;&#27604;&#25552;&#39640;&#20102;113&#20493;&#21040;19.44&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce NeuralMatrix, a novel framework that enables the computation of versatile deep neural networks (DNNs) on a single general matrix multiplication (GEMM) accelerator. The proposed approach overcomes the specificity limitations of ASIC-based accelerators while achieving application-specific acceleration levels compared to general-purpose processors such as CPUs and GPUs. We address the challenges of mapping both linear and nonlinear operations in DNN computation to general matrix multiplications and the impact of using a GEMM accelerator on DNN inference accuracy. Extensive experiments are conducted on various DNN models from three popular categories (i.e., CNN, Transformers, and GNN) as illustrative backbone models. Our results demonstrate that DNNs suffer only up to a 2.02% accuracy loss after being converted to general matrix multiplication, while achieving 113x to 19.44x improvements in throughput per power compared to CPUs and GPUs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#34701;&#21512;&#34920;&#31034;&#23398;&#20064;&#65288;BSFL&#65289;&#27169;&#22411;&#65292;&#20174;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65288;DTI&#65289;&#21644;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20013;&#26377;&#25928;&#22320;&#23398;&#20064;&#34701;&#21512;&#34920;&#31034;&#36827;&#34892;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#20998;&#26512;&#65292;&#37319;&#29992;&#35299;&#32806;-&#34701;&#21512;&#26694;&#26550;&#21644;&#30693;&#35782;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14404</link><description>&lt;p&gt;
&#34701;&#21512;&#32467;&#26500;&#21151;&#33021;&#30340;&#23545;&#25239;&#35299;&#32806;VAE&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;MCI&#20998;&#26512;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#34701;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Brain Structure-Function Fusing Representation Learning using Adversarial Decomposed-VAE for Analyzing MCI. (arXiv:2305.14404v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#34701;&#21512;&#34920;&#31034;&#23398;&#20064;&#65288;BSFL&#65289;&#27169;&#22411;&#65292;&#20174;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65288;DTI&#65289;&#21644;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20013;&#26377;&#25928;&#22320;&#23398;&#20064;&#34701;&#21512;&#34920;&#31034;&#36827;&#34892;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#20998;&#26512;&#65292;&#37319;&#29992;&#35299;&#32806;-&#34701;&#21512;&#26694;&#26550;&#21644;&#30693;&#35782;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#33041;&#32467;&#26500;&#21644;&#21151;&#33021;&#36830;&#25509;&#29305;&#24449;&#25972;&#21512;&#36215;&#26469;&#23545;&#20110;&#25506;&#32034;&#22823;&#33041;&#31185;&#23398;&#21644;&#20020;&#24202;&#20998;&#26512;&#35748;&#30693;&#38556;&#30861;&#37117;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#20294;&#26159;&#26377;&#25928;&#22320;&#23558;&#32467;&#26500;&#21644;&#21151;&#33021;&#29305;&#24449;&#34701;&#21512;&#36215;&#26469;&#25506;&#32034;&#22823;&#33041;&#32593;&#32476;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#34701;&#21512;&#34920;&#31034;&#23398;&#20064;&#65288;BSFL&#65289;&#27169;&#22411;&#65292;&#20174;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65288;DTI&#65289;&#21644;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20013;&#26377;&#25928;&#22320;&#23398;&#20064;&#34701;&#21512;&#34920;&#31034;&#36827;&#34892;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24320;&#21457;&#20102;&#35299;&#32806;-&#34701;&#21512;&#26694;&#26550;&#65292;&#39318;&#20808;&#23558;&#29305;&#24449;&#31354;&#38388;&#20998;&#35299;&#20026;&#27599;&#31181;&#27169;&#24577;&#30340;&#22343;&#21248;&#31354;&#38388;&#21644;&#29420;&#29305;&#31354;&#38388;&#30340;&#24182;&#38598;&#65292;&#28982;&#21518;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#35299;&#32806;&#21518;&#30340;&#29305;&#24449;&#20197;&#23398;&#20064;&#19982;MCI&#30456;&#20851;&#30340;&#34920;&#24449;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#30693;&#35782;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#20197;&#33258;&#21160;&#25429;&#33719;&#25972;&#20010;&#22823;&#33041;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#36830;&#25509;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22343;&#21248;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#23545;&#32467;&#26500;&#21644;&#21151;&#33021;&#36830;&#25509;&#30340;&#19981;&#21516;&#23610;&#24230;&#37117;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#23545;&#19968;&#20010;&#22823;&#22411;&#20844;&#24320;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;BSFL&#27169;&#22411;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating the brain structural and functional connectivity features is of great significance in both exploring brain science and analyzing cognitive impairment clinically. However, it remains a challenge to effectively fuse structural and functional features in exploring the brain network. In this paper, a novel brain structure-function fusing-representation learning (BSFL) model is proposed to effectively learn fused representation from diffusion tensor imaging (DTI) and resting-state functional magnetic resonance imaging (fMRI) for mild cognitive impairment (MCI) analysis. Specifically, the decomposition-fusion framework is developed to first decompose the feature space into the union of the uniform and the unique spaces for each modality, and then adaptively fuse the decomposed features to learn MCI-related representation. Moreover, a knowledge-aware transformer module is designed to automatically capture local and global connectivity features throughout the brain. Also, a uniform
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#37325;&#35201;&#24615;&#20998;&#25968;SP-LAMP&#30340;&#20998;&#23618;&#33258;&#36866;&#24212;&#32467;&#26500;&#35009;&#21098;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#32452;&#32972;&#21253;&#27714;&#35299;&#22120;&#22312;&#24310;&#36831;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;SP-LAMP&#20998;&#25968;&#26469;&#25351;&#23548;&#32593;&#32476;&#35009;&#21098;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.14403</link><description>&lt;p&gt;
&#32771;&#34385;&#24310;&#36831;&#30340;&#20998;&#23618;&#33258;&#36866;&#24212;&#32467;&#26500;&#35009;&#21098;
&lt;/p&gt;
&lt;p&gt;
Layer-adaptive Structured Pruning Guided by Latency. (arXiv:2305.14403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14403
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#37325;&#35201;&#24615;&#20998;&#25968;SP-LAMP&#30340;&#20998;&#23618;&#33258;&#36866;&#24212;&#32467;&#26500;&#35009;&#21098;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#32452;&#32972;&#21253;&#27714;&#35299;&#22120;&#22312;&#24310;&#36831;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;SP-LAMP&#20998;&#25968;&#26469;&#25351;&#23548;&#32593;&#32476;&#35009;&#21098;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#35009;&#21098;&#21487;&#20197;&#31616;&#21270;&#32593;&#32476;&#32467;&#26500;&#24182;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#12290;&#36890;&#36807;&#32467;&#21512;&#37096;&#32626;&#26368;&#32456;&#27169;&#22411;&#30340;&#24213;&#23618;&#30828;&#20214;&#21644;&#25512;&#29702;&#24341;&#25806;&#65292;&#20351;&#29992;&#24310;&#36831;&#21327;&#21516;&#25439;&#22833;&#20989;&#25968;&#26469;&#25351;&#23548;&#32593;&#32476;&#35009;&#21098;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#29616;&#26377;&#30340;&#20248;&#21270;&#24310;&#36831;&#30340;&#35009;&#21098;&#26041;&#27861;&#24050;&#32463;&#23637;&#29616;&#20986;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#24573;&#30053;&#32593;&#32476;&#20013;&#30340;&#30828;&#20214;&#29305;&#24449;&#21644;&#36830;&#25509;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#23616;&#37325;&#35201;&#24615;&#20998;&#25968;SP-LAMP(&#32467;&#26500;&#35009;&#21098;&#23618;&#33258;&#36866;&#24212;&#22522;&#20110;&#24133;&#24230;&#30340;&#35009;&#21098;)&#65292;&#36890;&#36807;&#20174;&#38750;&#32467;&#26500;&#21270;&#35009;&#21098;&#21040;&#32467;&#26500;&#21270;&#35009;&#21098;&#20013;&#23548;&#20986;&#20840;&#23616;&#37325;&#35201;&#24615;&#20998;&#25968;LAMP&#26469;&#35745;&#31639;SP-LAMP&#12290;&#22312;SP-LAMP&#20013;&#65292;&#27599;&#20010;&#23618;&#37117;&#21253;&#25324;&#19968;&#20010;SP-LAMP&#20998;&#25968;&#20026;1&#30340;&#36807;&#28388;&#22120;&#65292;&#20854;&#20313;&#30340;&#36807;&#28388;&#22120;&#20998;&#32452;&#12290;&#25105;&#20204;&#21033;&#29992;&#20998;&#32452;&#32972;&#21253;&#27714;&#35299;&#22120;&#65292;&#22312;&#24310;&#36831;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;SP-LAMP&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#25910;&#38598;&#24310;&#36831;&#30340;&#31574;&#30053;&#65292;&#20351;&#20854;&#26356;&#21152;&#20934;&#30830;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;ResNet50/ResNet1&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured pruning can simplify network architecture and improve inference speed. Combined with the underlying hardware and inference engine in which the final model is deployed, better results can be obtained by using latency collaborative loss function to guide network pruning together. Existing pruning methods that optimize latency have demonstrated leading performance, however, they often overlook the hardware features and connection in the network. To address this problem, we propose a global importance score SP-LAMP(Structured Pruning Layer-Adaptive Magnitude-based Pruning) by deriving a global importance score LAMP from unstructured pruning to structured pruning. In SP-LAMP, each layer includes a filter with an SP-LAMP score of 1, and the remaining filters are grouped. We utilize a group knapsack solver to maximize the SP-LAMP score under latency constraints. In addition, we improve the strategy of collect the latency to make it more accurate. In particular, for ResNet50/ResNet1
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#20013;&#20351;&#29992;&#36335;&#24452;&#24402;&#22240;&#31574;&#30053;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#26465;&#20214;&#36991;&#20813;&#21453;&#30452;&#35273;&#32467;&#26524;&#12290;&#24182;&#25552;&#20986;&#19968;&#31181;&#26041;&#26696;&#24110;&#21161;&#38450;&#27490;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#26080;&#25928;&#21270;&#36335;&#24452;&#24402;&#22240;&#30340;&#20844;&#29702;&#23646;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#38752;&#30340;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.14395</link><description>&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#24402;&#22240;&#30340;&#21487;&#20449;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards credible visual model interpretation with path attribution. (arXiv:2305.14395v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#20013;&#20351;&#29992;&#36335;&#24452;&#24402;&#22240;&#31574;&#30053;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#26465;&#20214;&#36991;&#20813;&#21453;&#30452;&#35273;&#32467;&#26524;&#12290;&#24182;&#25552;&#20986;&#19968;&#31181;&#26041;&#26696;&#24110;&#21161;&#38450;&#27490;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#26080;&#25928;&#21270;&#36335;&#24452;&#24402;&#22240;&#30340;&#20844;&#29702;&#23646;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#38752;&#30340;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36335;&#24452;&#24402;&#22240;&#26694;&#26550;&#26368;&#21021;&#21463;&#21338;&#24328;&#35770;&#21551;&#21457;&#65292;&#30001;&#20110;&#20854;&#20844;&#29702;&#24615;&#36136;&#32780;&#22312;&#21518;&#22788;&#29702;&#27169;&#22411;&#35299;&#37322;&#24037;&#20855;&#20013;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#21457;&#23637;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#20173;&#28982;&#21487;&#33021;&#20986;&#29616;&#21453;&#30452;&#35273;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#29305;&#21035;&#38024;&#23545;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#26041;&#27861;&#20063;&#26080;&#27861;&#31526;&#21512;&#35813;&#26694;&#26550;&#25152;&#23459;&#31216;&#30340;&#20844;&#29702;&#23646;&#24615;&#30340;&#22522;&#30784;&#30452;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20351;&#29992;&#36335;&#24452;&#24402;&#22240;&#31574;&#30053;&#36827;&#34892;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#21487;&#20197;&#36991;&#20813;&#21453;&#30452;&#35273;&#32467;&#26524;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#26696;&#65292;&#20197;&#38450;&#27490;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#26080;&#25928;&#21270;&#36335;&#24452;&#24402;&#22240;&#30340;&#20844;&#29702;&#23646;&#24615;&#12290;&#36825;&#20123;&#35265;&#35299;&#34987;&#32467;&#21512;&#25104;&#19968;&#31181;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#35270;&#35273;&#27169;&#22411;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35780;&#20272;&#24471;&#21040;&#20102;&#23454;&#35777;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originally inspired by game-theory, path attribution framework stands out among the post-hoc model interpretation tools due to its axiomatic nature. However, recent developments show that this framework can still suffer from counter-intuitive results. Moreover, specifically for deep visual models, the existing path-based methods also fall short on conforming to the original intuitions that are the basis of the claimed axiomatic properties of this framework. We address these problems with a systematic investigation, and pinpoint the conditions in which the counter-intuitive results can be avoided for deep visual model interpretation with the path attribution strategy. We also devise a scheme to preclude the conditions in which visual model interpretation can invalidate the axiomatic properties of path attribution. These insights are combined into a method that enables reliable visual model interpretation. Our findings are establish empirically with multiple datasets, models and evaluati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#24314;&#31435;&#30340;&#21069;&#39069;&#21494;&#30382;&#23618;&#35745;&#31639;&#27169;&#22411;&#12290;&#36890;&#36807;&#25345;&#20037;&#27963;&#21160;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#35268;&#21017;&#65292;&#27169;&#25311;&#20102;&#20219;&#21153;&#20999;&#25442;&#21644;&#25439;&#20260;&#29366;&#24577;&#19979;&#31070;&#32463;&#20803;&#30340;&#21709;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#30740;&#31350;&#31070;&#32463;&#20803;&#36866;&#24212;&#21644;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2305.14394</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30740;&#31350;&#21069;&#39069;&#21494;&#30382;&#23618;&#20013;&#30340;&#20219;&#21153;&#20999;&#25442;&#21644;&#31361;&#35302;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Spiking Neural Network Model of Prefrontal Cortex to study Task Switching with Synaptic deficiency. (arXiv:2305.14394v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#24314;&#31435;&#30340;&#21069;&#39069;&#21494;&#30382;&#23618;&#35745;&#31639;&#27169;&#22411;&#12290;&#36890;&#36807;&#25345;&#20037;&#27963;&#21160;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#35268;&#21017;&#65292;&#27169;&#25311;&#20102;&#20219;&#21153;&#20999;&#25442;&#21644;&#25439;&#20260;&#29366;&#24577;&#19979;&#31070;&#32463;&#20803;&#30340;&#21709;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#30740;&#31350;&#31070;&#32463;&#20803;&#36866;&#24212;&#21644;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26500;&#24314;&#20102;&#21069;&#39069;&#21494;&#30382;&#23618;&#65288;PFC&#65289;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20197;&#20102;&#35299;&#31070;&#32463;&#20803;&#22312;&#30701;&#26102;&#21644;&#38271;&#26102;&#30340;&#21050;&#28608;&#21464;&#21270;&#19979;&#22914;&#20309;&#36866;&#24212;&#21644;&#21709;&#24212;&#20219;&#21153;&#20999;&#25442;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#22312;&#25105;&#20204;&#30340;&#33033;&#20914;&#20307;&#31995;&#32467;&#26500;&#27169;&#22411;&#20013;&#27169;&#25311;&#21463;&#25439;&#29366;&#24577;&#26469;&#25506;&#35752; PFC &#25439;&#20260;&#25152;&#24341;&#36215;&#30340;&#34892;&#20026;&#32570;&#38519;&#12290;&#23613;&#31649;&#24050;&#32463;&#23384;&#22312;&#19968;&#20123; PFC &#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20294;&#36824;&#27809;&#26377;&#20351;&#29992; SNN &#26469;&#24314;&#27169;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#21442;&#25968;&#25509;&#36817;&#29983;&#29289;&#23398;&#21487;&#34892;&#20540;&#30340; SNN &#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#33033;&#20914;&#26102;&#24207;&#20381;&#36182;&#24615;&#22609;&#24615;&#65288;STDP&#65289;&#23398;&#20064;&#35268;&#21017;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#32852;&#32467;&#20027;&#20041;&#26550;&#26500;&#65292;&#23637;&#29616;&#20102;&#25345;&#20037;&#27963;&#21160;&#31561;&#31070;&#32463;&#29616;&#35937;&#65292;&#26377;&#21161;&#20110;&#20135;&#29983;&#30701;&#26399;&#25110;&#24037;&#20316;&#35760;&#24518;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#29305;&#28857;&#36890;&#36807;&#20851;&#38381;&#31361;&#35302;&#36890;&#36335;&#26469;&#27169;&#25311;&#25439;&#20260;&#65292;&#24182;&#35760;&#24405;&#23398;&#20064;&#27169;&#24335;&#30340;&#26435;&#37325;&#35843;&#25972;&#21644;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#23398;&#20064;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we build a computational model of Prefrontal Cortex (PFC) using Spiking Neural Networks (SNN) to understand how neurons adapt and respond to tasks switched under short and longer duration of stimulus changes. We also explore behavioral deficits arising out of the PFC lesions by simulating lesioned states in our Spiking architecture model. Although there are some computational models of the PFC, SNN's have not been used to model them. In this study, we use SNN's having parameters close to biologically plausible values and train the model using unsupervised Spike Timing Dependent Plasticity (STDP) learning rule. Our model is based on connectionist architectures and exhibits neural phenomena like sustained activity which helps in generating short-term or working memory. We use these features to simulate lesions by deactivating synaptic pathways and record the weight adjustments of learned patterns and capture the accuracy of learning tasks in such conditions. All our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21644;&#32418;&#22806;&#28909;&#20687;&#25216;&#26415;&#36827;&#34892;&#20083;&#33146;&#30284;&#20998;&#21106;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#65292;&#24182;&#22312;UNet&#32467;&#26500;&#20013;&#20351;&#29992;Grad-CAM&#20998;&#26512;&#20102;&#28508;&#22312;&#20559;&#24046;&#21644;&#24369;&#28857;&#21306;&#22495;&#65292;&#27604;&#29616;&#26377;&#27169;&#22411;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.14389</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20083;&#33146;&#30284;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#21450;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Breast Cancer Segmentation using Attention-based Convolutional Network and Explainable AI. (arXiv:2305.14389v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21644;&#32418;&#22806;&#28909;&#20687;&#25216;&#26415;&#36827;&#34892;&#20083;&#33146;&#30284;&#20998;&#21106;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#65292;&#24182;&#22312;UNet&#32467;&#26500;&#20013;&#20351;&#29992;Grad-CAM&#20998;&#26512;&#20102;&#28508;&#22312;&#20559;&#24046;&#21644;&#24369;&#28857;&#21306;&#22495;&#65292;&#27604;&#29616;&#26377;&#27169;&#22411;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#20581;&#24247;&#23041;&#32961;&#65292;&#30446;&#21069;&#23578;&#26080;&#38271;&#26399;&#27835;&#24840;&#26041;&#27861;&#12290;&#26089;&#26399;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20083;&#33146;X&#32447;&#25668;&#24433;&#30340;&#35299;&#37322;&#21463;&#21040;&#22823;&#37327;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#30340;&#24433;&#21709;&#12290;&#38543;&#30528;&#20083;&#33146;&#30284;&#20363;&#25968;&#26377;&#26395;&#36229;&#36807;&#32954;&#30284;&#65292;&#25913;&#36827;&#26089;&#26399;&#26816;&#27979;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#32418;&#22806;&#28909;&#20687;&#25216;&#26415;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20083;&#33146;&#30284;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#36895;&#24230;&#21644;&#31934;&#24230;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#23545;&#22270;&#20687;&#36827;&#34892;&#22686;&#24378;&#21644;&#36827;&#34892;&#30284;&#30151;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21367;&#31215;&#32467;&#26500;&#65288;UNet&#65289;&#29992;&#20110;&#25925;&#38556;&#35782;&#21035;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;Grad-CAM&#65289;&#26469;&#20998;&#26512;&#22312;IRT&#22270;&#20687;&#20013;UNet&#32467;&#26500;&#30340;&#20559;&#24046;&#21644;&#24369;&#28857;&#21306;&#22495;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Breast cancer (BC) remains a significant health threat, with no long-term cure currently available. Early detection is crucial, yet mammography interpretation is hindered by high false positives and negatives. With BC incidence projected to surpass lung cancer, improving early detection methods is vital. Thermography, using high-resolution infrared cameras, offers promise, especially when combined with artificial intelligence (AI). This work presents an attention-based convolutional neural network for segmentation, providing increased speed and precision in BC detection and classification. The system enhances images and performs cancer segmentation with explainable AI. We propose a transformer-attention-based convolutional architecture (UNet) for fault identification and employ Gradient-weighted Class Activation Mapping (Grad-CAM) to analyze areas of bias and weakness in the UNet architecture with IRT images. The superiority of our proposed framework is confirmed when compared with exi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-3&#29983;&#25104;&#23450;&#21046;&#21270;&#32451;&#20064;&#65292;&#25945;&#25480;&#25968;&#23398;&#24212;&#29992;&#39064;&#35299;&#20915;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#23398;&#29983;&#27169;&#22411;&#30340;&#24369;&#28857;&#24182;&#20197;&#25945;&#32946;&#31185;&#23398;&#21407;&#29702;&#20026;&#22522;&#30784;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#23398;&#20064;&#20307;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14386</link><description>&lt;p&gt;
&#35753;GPT&#25104;&#20026;&#25968;&#23398;&#25945;&#24072;&#65306;&#20351;&#29992;&#23450;&#21046;&#21270;&#32451;&#20064;&#29983;&#25104;&#25945;&#25480;&#25968;&#23398;&#24212;&#29992;&#39064;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation. (arXiv:2305.14386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-3&#29983;&#25104;&#23450;&#21046;&#21270;&#32451;&#20064;&#65292;&#25945;&#25480;&#25968;&#23398;&#24212;&#29992;&#39064;&#35299;&#20915;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#23398;&#29983;&#27169;&#22411;&#30340;&#24369;&#28857;&#24182;&#20197;&#25945;&#32946;&#31185;&#23398;&#21407;&#29702;&#20026;&#22522;&#30784;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#23398;&#20064;&#20307;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#25968;&#23398;&#24212;&#29992;&#39064;&#35299;&#20915;&#33021;&#21147;&#25552;&#28860;&#20026;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#32771;&#34385;&#23398;&#29983;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#25945;&#32946;&#31185;&#23398;&#21407;&#29702;&#65288;&#22914;&#30693;&#35782;&#36319;&#36394;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#65289;&#23545;&#40784;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#32451;&#20064;&#26469;&#20419;&#36827;&#23450;&#21046;&#21270;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#25105;&#20204;&#35753;GPT-3&#25104;&#20026;&#25968;&#23398;&#25945;&#24072;&#65292;&#36845;&#20195;&#25191;&#34892;&#20004;&#20010;&#27493;&#39588;&#65306;1&#65289;&#22312;&#30001;GPT&#29983;&#25104;&#30340;&#32451;&#20064;&#20876;&#19978;&#35780;&#20272;&#23398;&#29983;&#27169;&#22411;&#30340;&#24403;&#21069;&#23398;&#20064;&#29366;&#20917;&#65307;2&#65289;&#20351;&#29992;GPT-3&#29983;&#25104;&#30340;&#23450;&#21046;&#21270;&#32451;&#20064;&#26679;&#26412;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;LLMs&#65288;&#20363;&#22914;&#65292;GPT-3&#21644;PaLM&#65289;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#21442;&#25968;&#25968;&#37327;&#26126;&#26174;&#36739;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#26041;&#27861;&#20013;&#21508;&#20010;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#20197;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models. Our approach is designed to consider the student model's weaknesses and foster a tailored learning experience by generating targeted exercises aligned with educational science principles, such as knowledge tracing and personalized learning. Concretely, we let GPT-3 be a math tutor and run two steps iteratively: 1) assessing the student model's current learning status on a GPT-generated exercise book, and 2) improving the student model by training it with tailored exercise samples generated by GPT-3. Experimental results reveal that our approach outperforms LLMs (e.g., GPT-3 and PaLM) in accuracy across three distinct benchmarks while employing significantly fewer parameters. Furthermore, we provide a comprehensive analysis of the various components within our methodology to substantiate their efficacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#25361;&#25112;&#65292;&#26088;&#22312;&#25552;&#39640;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#32570;&#20047;&#31995;&#32479;&#24615;&#21644;&#32467;&#26500;&#21270;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#25361;&#25112;&#26088;&#22312;&#26816;&#26597;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2305.14384</link><description>&lt;p&gt;
Adversarial Nibbler: &#19968;&#31181;&#25552;&#39640;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#25968;&#25454;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models. (arXiv:2305.14384v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#25361;&#25112;&#65292;&#26088;&#22312;&#25552;&#39640;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#32570;&#20047;&#31995;&#32479;&#24615;&#21644;&#32467;&#26500;&#21270;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#25361;&#25112;&#26088;&#22312;&#26816;&#26597;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#35745;&#31639;&#33021;&#21147;&#21644;&#25968;&#25454;&#37327;&#30340;&#25193;&#22823;&#65292;&#29983;&#25104;&#24335;AI&#38761;&#21629;&#25512;&#21160;&#20102;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#24191;&#27867;&#39044;&#35757;&#32451;&#12290;&#20855;&#22791;&#20135;&#29983;&#36924;&#30495;&#32780;&#23500;&#26377;&#21019;&#36896;&#21147;&#30340;&#20869;&#23481;&#65292;&#20363;&#22914;DALL-E&#65292;MidJourney&#65292;Imagen&#25110;Stable Diffusion&#30340;&#36825;&#20123;T2I&#27169;&#22411;&#24050;&#32463;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#21463;&#20247;&#12290;&#28982;&#32780;&#65292;&#22240;&#20026;&#39044;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#20114;&#32852;&#32593;&#29228;&#21462;&#30340;&#26410;&#32463;&#31579;&#36873;&#30340;&#25968;&#25454;&#38598;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#28508;&#22312;&#30340;&#36896;&#25104;&#24191;&#27867;&#20260;&#23475;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#29983;&#25104;&#20855;&#26377;&#26292;&#21147;&#12289;&#24615;&#26292;&#21147;&#12289;&#20559;&#35265;&#21644;&#36140;&#25439;&#24615;&#30340;&#21051;&#26495;&#21360;&#35937;&#30340;&#22270;&#20687;&#12290;&#23613;&#31649;&#23384;&#22312;&#27492;&#31867;&#39118;&#38505;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#31995;&#32479;&#24615;&#21644;&#32467;&#26500;&#21270;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#26816;&#26597;&#27169;&#22411;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#23545;&#25915;&#20987;&#29616;&#26377;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#35780;&#20272;&#23433;&#20840;&#24615;&#30340;&#19968;&#20010;&#20856;&#22411;&#29942;&#39048;&#22312;&#20110;&#23454;&#29616;&#35780;&#20272;&#38598;&#21512;&#20013;&#21508;&#31181;&#31867;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#23454;&#20363;&#30340;&#24191;&#27867;&#35206;&#30422;&#29575;&#65292;&#21363;&#30830;&#23450;&#8220;&#26410;&#30693;&#30340;&#26410;&#30693;&#8221;&#25110;&#21333;&#20010;&#25915;&#20987;&#23454;&#20363;&#19981;&#36275;&#20197;&#23637;&#31034;&#27169;&#22411;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generative AI revolution in recent years has been spurred by an expansion in compute power and data quantity, which together enable extensive pre-training of powerful text-to-image (T2I) models. With their greater capabilities to generate realistic and creative content, these T2I models like DALL-E, MidJourney, Imagen or Stable Diffusion are reaching ever wider audiences. Any unsafe behaviors inherited from pretraining on uncurated internet-scraped datasets thus have the potential to cause wide-reaching harm, for example, through generated images which are violent, sexually explicit, or contain biased and derogatory stereotypes. Despite this risk of harm, we lack systematic and structured evaluation datasets to scrutinize model behavior, especially adversarial attacks that bypass existing safety filters. A typical bottleneck in safety evaluation is achieving a wide coverage of different types of challenging examples in the evaluation set, i.e., identifying 'unknown unknowns' or lon
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#21450;&#22522;&#20110;&#29983;&#25104;&#36807;&#31243;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#20998;&#31867;&#35299;&#37322;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#29305;&#24449;&#38598;&#21512;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#21050;&#28608;&#19979;&#65292;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14383</link><description>&lt;p&gt;
&#19968;&#20010;&#38477;&#32500;&#20154;&#31867;&#20998;&#31867;&#30340;&#29702;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Rational Model of Dimension-reduced Human Categorization. (arXiv:2305.14383v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14383
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#21450;&#22522;&#20110;&#29983;&#25104;&#36807;&#31243;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#20998;&#31867;&#35299;&#37322;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#29305;&#24449;&#38598;&#21512;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#21050;&#28608;&#19979;&#65292;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#31185;&#23398;&#20013;&#29616;&#26377;&#30340;&#27169;&#22411;&#36890;&#24120;&#20551;&#35774;&#20154;&#31867;&#22312;&#24515;&#29702;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#32423;&#27010;&#25324;&#34892;&#20026;&#65292;&#20294;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#31867;&#21035;&#34920;&#31034;&#21487;&#33021;&#20250;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#20154;&#20204;&#19968;&#33324;&#20381;&#36182;&#20110;&#19968;&#32452;&#21487;&#34892;&#20294;&#36275;&#22815;&#30340;&#29305;&#24449;&#26469;&#29702;&#35299;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#27010;&#29575;&#20027;&#25104;&#20998;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#32463;&#27982;&#30340;&#29305;&#24449;&#38598;&#21512;&#12290;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#20154;&#31867;&#20998;&#31867;&#20013;&#30340;&#32500;&#24230;&#20559;&#24046;&#24182;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20869;&#21033;&#29992;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#20379;&#39640;&#32500;&#21050;&#28608;&#19979;&#26356;&#22909;&#30340;&#20998;&#31867;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#34892;&#20026;&#23454;&#39564;&#39564;&#35777;&#20102;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing models in cognitive science typically assume human categorization as graded generalization behavior in a multidimensional psychological space. However, category representations in these models may suffer from the curse of dimensionality in a natural setting. People generally rely on a tractable yet sufficient set of features to understand the complex environment. We propose a rational model of categorization based on a hierarchical mixture of probabilistic principal components, that simultaneously learn category representations and an economical collection of features. The model captures dimensional biases in human categorization and supports zero-shot learning. We further exploit a generative process within a low-dimensional latent space to provide a better account of categorization with high-dimensional stimuli. We validate the model with simulation and behavioral experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Informer&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#22312;&#32929;&#31080;&#21644;&#24066;&#22330;&#25351;&#25968;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14382</link><description>&lt;p&gt;
&#20351;&#29992;Informer&#32593;&#32476;&#36827;&#34892;&#32929;&#31080;&#21644;&#24066;&#22330;&#25351;&#25968;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Stock and market index prediction using Informer network. (arXiv:2305.14382v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Informer&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#22312;&#32929;&#31080;&#21644;&#24066;&#22330;&#25351;&#25968;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#37329;&#34701;&#24066;&#22330;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24341;&#36215;&#20102;&#25237;&#36164;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;Informer&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#32593;&#32476;&#65292;&#23427;&#22312;Transformer&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#20855;&#26377;&#26356;&#23567;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12289;&#26356;&#38271;&#30340;&#39044;&#27979;&#38271;&#24230;&#21644;&#20840;&#23616;&#26102;&#38388;&#25139;&#29305;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#22312;1&#20998;&#38047;&#21644;5&#20998;&#38047;&#39057;&#29575;&#19979;&#20351;&#29992;Informer&#12289;LSTM&#12289;Transformer&#21644;BERT&#39044;&#27979;&#22235;&#31181;&#19981;&#21516;&#32929;&#31080;/&#24066;&#22330;&#25351;&#25968;&#30340;&#25928;&#26524;&#12290;&#39044;&#27979;&#32467;&#26524;&#36890;&#36807;&#19977;&#20010;&#35780;&#20272;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#65306;MAE&#12289;RMSE&#21644;MAPE&#12290;Informer&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#33719;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications of deep learning in financial market prediction has attracted huge attention from investors and researchers. In particular, intra-day prediction at the minute scale, the dramatically fluctuating volume and stock prices within short time periods have posed a great challenge for the convergence of networks result. Informer is a more novel network, improved on Transformer with smaller computational complexity, longer prediction length and global time stamp features. We have designed three experiments to compare Informer with the commonly used networks LSTM, Transformer and BERT on 1-minute and 5-minute frequencies for four different stocks/ market indices. The prediction results are measured by three evaluation criteria: MAE, RMSE and MAPE. Informer has obtained best performance among all the networks on every dataset. Network without the global time stamp mechanism has significantly lower prediction effect compared to the complete Informer; it is evident that this mechanism 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;MCR&#30340;&#26041;&#27861;&#65292;&#21483;&#20570;C-MCR&#65292;&#24182;&#19988;&#22312;&#26032;&#31354;&#38388;&#20013;&#20351;&#29992;&#37325;&#21472;&#27169;&#24577;B&#30340;&#25968;&#25454;&#26469;&#23545;&#40784;&#20004;&#20010;MCR&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#38750;&#37325;&#21472;&#27169;&#24577;&#23545;&#65288;A&#65292;C&#65289;&#20063;&#21487;&#20197;&#20351;&#29992;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2305.14381</link><description>&lt;p&gt;
&#36830;&#25509;&#22810;&#27169;&#24577;&#23545;&#27604;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Connecting Multi-modal Contrastive Representations. (arXiv:2305.14381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;MCR&#30340;&#26041;&#27861;&#65292;&#21483;&#20570;C-MCR&#65292;&#24182;&#19988;&#22312;&#26032;&#31354;&#38388;&#20013;&#20351;&#29992;&#37325;&#21472;&#27169;&#24577;B&#30340;&#25968;&#25454;&#26469;&#23545;&#40784;&#20004;&#20010;MCR&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#38750;&#37325;&#21472;&#27169;&#24577;&#23545;&#65288;A&#65292;C&#65289;&#20063;&#21487;&#20197;&#20351;&#29992;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#27604;&#34920;&#31034;&#65288;MCR&#65289;&#23398;&#20064;&#26088;&#22312;&#23558;&#19981;&#21516;&#30340;&#27169;&#24577;&#32534;&#30721;&#21040;&#19968;&#20010;&#35821;&#20041;&#23545;&#40784;&#30340;&#20849;&#20139;&#31354;&#38388;&#20013;&#12290;&#35813;&#33539;&#20363;&#22312;&#21508;&#31181;&#27169;&#24335;&#19979;&#30340;&#22823;&#37327;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25968;&#25454;&#23545;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#20854;&#22312;&#26356;&#22810;&#27169;&#24577;&#19978;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;MCR&#30340;&#35757;&#32451;&#39640;&#25928;&#26041;&#27861;&#65292;&#31216;&#20026;&#36830;&#25509;&#22810;&#27169;&#24577;&#23545;&#27604;&#34920;&#31034;&#65288;C-MCR&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#65288;A&#65292;B&#65289;&#21644;&#65288;B&#65292;C&#65289;&#27169;&#24577;&#23545;&#19978;&#39044;&#35757;&#32451;&#20004;&#20010;&#29616;&#26377;&#30340;MCR&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#25237;&#24433;&#21040;&#19968;&#20010;&#26032;&#30340;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#37325;&#21472;&#27169;&#24577;B&#30340;&#25968;&#25454;&#26469;&#22312;&#26032;&#31354;&#38388;&#20013;&#23545;&#40784;&#20004;&#20010;MCR&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#27169;&#24577;&#23545;&#65288;A&#65292;B&#65289;&#21644;&#65288;B&#65292;C&#65289;&#22312;&#27599;&#20010;MCR&#20869;&#24050;&#32463;&#23545;&#40784;&#65292;&#22240;&#27492;&#36890;&#36807;&#37325;&#21472;&#27169;&#24577;&#23398;&#20064;&#21040;&#30340;&#36830;&#25509;&#20063;&#21487;&#20197;&#36716;&#31227;&#21040;&#38750;&#37325;&#21472;&#27169;&#24577;&#23545;&#65288;A&#65292;C&#65289;&#12290;&#20026;&#20102;&#21457;&#25381;C-MCR&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#35821;&#20041;&#22686;&#24378;&#30340;int
&lt;/p&gt;
&lt;p&gt;
Multi-modal Contrastive Representation (MCR) learning aims to encode different modalities into a semantically aligned shared space. This paradigm shows remarkable generalization ability on numerous downstream tasks across various modalities. However, the reliance on massive high-quality data pairs limits its further development on more modalities. This paper proposes a novel training-efficient method for learning MCR without paired data called Connecting Multi-modal Contrastive Representations (C-MCR). Specifically, given two existing MCRs pre-trained on (A, B) and (B, C) modality pairs, we project them to a new space and use the data from the overlapping modality B to aligning the two MCRs in the new space. Meanwhile, since the modality pairs (A, B) and (B, C) are already aligned within each MCR, the connection learned by overlapping modality can also be transferred to non-overlapping modality pair (A, C). To unleash the potential of C-MCR, we further introduce a semantic-enhanced int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCS&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#38480;&#23398;&#20064;&#19981;&#21516;&#30340;&#36830;&#32493;&#25216;&#33021;&#65292;&#22312;MuJoCo Ant&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#20855;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14377</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#22320;&#22312;&#29699;&#38754;&#19978;&#23398;&#20064;&#36830;&#32493;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Discovery of Continuous Skills on a Sphere. (arXiv:2305.14377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCS&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#38480;&#23398;&#20064;&#19981;&#21516;&#30340;&#36830;&#32493;&#25216;&#33021;&#65292;&#22312;MuJoCo Ant&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#20855;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30740;&#31350;&#20102;&#35768;&#22810;&#31181;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19981;&#21516;&#30340;&#25216;&#33021;&#24110;&#21161;&#26426;&#22120;&#20154;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#34892;&#20026;&#65292;&#32780;&#19981;&#38656;&#35201;&#22806;&#37096;&#22870;&#21169;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#23398;&#20064;&#30340;&#26159;&#26377;&#38480;&#25968;&#37327;&#30340;&#31163;&#25955;&#25216;&#33021;&#65292;&#22240;&#27492;&#23427;&#20204;&#23637;&#29616;&#20986;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#20063;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;&#22312;&#29699;&#38754;&#19978;&#23398;&#20064;&#36830;&#32493;&#25216;&#33021;&#30340;&#21457;&#29616;&#8221;&#65292;&#21487;&#20197;&#23398;&#20064;&#26080;&#38480;&#31181;&#19981;&#21516;&#30340;&#25216;&#33021;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25216;&#33021;&#26159;&#36890;&#36807;&#26368;&#22823;&#21270;&#25216;&#33021;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#30340;&#65292;&#32780;&#27599;&#31181;&#25216;&#33021;&#37117;&#23545;&#24212;&#20110;&#29699;&#38754;&#19978;&#30340;&#19968;&#20010;&#36830;&#32493;&#20540;&#12290;&#30001;&#20110;&#25216;&#33021;&#22312;DISCS&#20013;&#30340;&#34920;&#31034;&#26159;&#36830;&#32493;&#30340;&#65292;&#25152;&#20197;&#21487;&#20197;&#23398;&#20064;&#26080;&#38480;&#22810;&#31181;&#19981;&#21516;&#30340;&#25216;&#33021;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#26041;&#27861;&#21644;DISCS&#24212;&#29992;&#20110;MuJoCo Ant&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;DISCS&#21487;&#20197;&#27604;&#20854;&#20182;&#26041;&#27861;&#23398;&#20064;&#21040;&#26356;&#22810;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods for learning diverse skills to generate various behaviors without external rewards have been actively studied as a form of unsupervised reinforcement learning. However, most of the existing methods learn a finite number of discrete skills, and thus the variety of behaviors that can be exhibited with the learned skills is limited. In this paper, we propose a novel method for learning potentially an infinite number of different skills, which is named discovery of continuous skills on a sphere (DISCS). In DISCS, skills are learned by maximizing mutual information between skills and states, and each skill corresponds to a continuous value on a sphere. Because the representations of skills in DISCS are continuous, infinitely diverse skills could be learned. We examine existing methods and DISCS in the MuJoCo Ant robot control environments and show that DISCS can learn much more diverse skills than the other methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.14375</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#22270;&#34701;&#21512;&#30340;&#36947;&#36335;&#32593;&#32476;&#33410;&#28857;&#37325;&#35201;&#24615;&#25490;&#24207;&#26041;&#27861;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank the Importance of Nodes in Road Networks Based on Multi-Graph Fusion. (arXiv:2305.14375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#35268;&#21010;&#39046;&#22495;&#20013;&#65292;&#35782;&#21035;&#20855;&#26377;&#24378;&#20256;&#25773;&#33021;&#21147;&#30340;&#37325;&#35201;&#33410;&#28857;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#33410;&#28857;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#20165;&#32771;&#34385;&#25299;&#25169;&#20449;&#24687;&#21644;&#20132;&#36890;&#27969;&#37327;&#65292;&#24573;&#30053;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#22810;&#26679;&#24615;&#29305;&#24449;&#65292;&#22914;&#36710;&#36947;&#25968;&#37327;&#21644;&#36947;&#36335;&#27573;&#30340;&#24179;&#22343;&#36895;&#24230;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#37319;&#26679;&#31639;&#27861;&#65288;MGWalk&#65289;&#65292;&#21033;&#29992;&#22810;&#22270;&#34701;&#21512;&#26469;&#24314;&#31435;&#22522;&#20110;&#23646;&#24615;&#30340;&#36947;&#36335;&#27573;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23884;&#20837;&#27169;&#22359;&#65292;&#29992;&#20110;&#23398;&#20064;&#27599;&#20010;&#36947;&#36335;&#27573;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#24471;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#29992;&#20110;&#23398;&#20064;&#36947;&#36335;&#27573;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#20013;&#22269;&#27784;&#38451;&#24066;&#21306;&#22495;&#36947;&#36335;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#20223;&#30495;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;MGL2Rank&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MGL2Rank&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying important nodes with strong propagation capabilities in road networks is a significant topic in the field of urban planning. However, existing methods for evaluating nodes importance consider only topological information and traffic volumes, ignoring the diversity of characteristics in road networks, such as the number of lanes and average speed of road segments, limiting their performance. To address this issue, this paper proposes a graph learning-based node ranking method (MGL2Rank) that integrates the rich characteristics of the road network. In this method, we first develop a sampling algorithm (MGWalk) that utilizes multi-graph fusion to establish association between road segments based on their attributes. Then, an embedding module is proposed to learn latent representation for each road segment. Finally, the obtained node representation is used to learn importance ranking of road segments. We conduct simulation experiments on the regional road network of Shenyang ci
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#12289;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#21644;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#25968;&#25454;&#65292;&#25104;&#21151;&#39044;&#27979;&#36229;&#36807;80%&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#65292;&#21487;&#20197;&#28508;&#22312;&#29992;&#20110;&#35813;&#30142;&#30149;&#30340;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2305.14370</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#39044;&#27979;&#21644;&#35786;&#26029;&#31934;&#31070;&#20998;&#35010;&#30151;&#20013;&#30340;&#20316;&#29992;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on the Role of Artificial Intelligence in the Prediction and Diagnosis of Schizophrenia. (arXiv:2305.14370v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14370
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#12289;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#21644;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#25968;&#25454;&#65292;&#25104;&#21151;&#39044;&#27979;&#36229;&#36807;80%&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#65292;&#21487;&#20197;&#28508;&#22312;&#29992;&#20110;&#35813;&#30142;&#30149;&#30340;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#34987;&#29992;&#20110;&#25512;&#26029;&#20154;&#31867;&#30142;&#30149;&#21644;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#36817;&#20284;&#32467;&#35770;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20998;&#26512;&#25968;&#25454;&#65292;&#24182;&#20135;&#29983;&#26356;&#22909;&#12289;&#26356;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;&#31934;&#31070;&#20998;&#35010;&#30151;&#26159;&#19968;&#31181;&#24930;&#24615;&#31934;&#31070;&#38556;&#30861;&#65292;&#24433;&#21709;&#25968;&#30334;&#19975;&#20154;&#30340;&#29983;&#27963;&#12290;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#39044;&#27979;&#21644;&#39044;&#38450;&#36825;&#31181;&#30142;&#30149;&#65292;&#24182;&#26377;&#21487;&#33021;&#29992;&#20110;&#35786;&#26029;&#24739;&#26377;&#35813;&#30142;&#30149;&#30340;&#20010;&#20154;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#22238;&#39038;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#26816;&#27979;&#21644;&#39044;&#27979;&#31934;&#31070;&#20998;&#35010;&#30151;&#30340;&#35770;&#25991;&#65292;&#20351;&#29992;&#20102;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#12289;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#21644;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#12290;&#25105;&#20204;&#36873;&#25321;&#26816;&#32034;&#31574;&#30053;&#65292;&#35780;&#20272;&#20102;2019&#24180;&#33267;2022&#24180;&#30340;&#21313;&#31687;&#35770;&#25991;&#12290;&#25152;&#26377;&#30740;&#31350;&#37117;&#25104;&#21151;&#39044;&#27979;&#20102;80%&#20197;&#19978;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#20123;&#30740;&#31350;&#30340;&#25688;&#35201;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is employed in healthcare to draw approximate conclusions regarding human diseases and mental health problems. Compared to older traditional methods, it can help to analyze data more efficiently and produce better and more dependable results. Millions of people are affected by schizophrenia, which is a chronic mental disorder that can significantly impact their lives. Many machine learning algorithms have been developed to predict and prevent this disease, and they can potentially be implemented in the diagnosis of individuals who have it. This survey aims to review papers that have focused on the use of deep learning to detect and predict schizophrenia using EEG signals, functional magnetic resonance imaging (fMRI), and diffusion magnetic resonance imaging (dMRI). With our chosen search strategy, we assessed ten publications from 2019 to 2022. All studies achieved successful predictions of more than 80%. This review provides summaries of the studies and compares their
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#30340;&#36719;&#32452;&#32455;&#21487;&#29992;&#20110;&#29289;&#29702;&#20648;&#22791;&#35745;&#31639;&#20013;&#30340;&#36719;&#20307;&#65292;&#22312;&#20223;&#30495;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#35745;&#31639;&#20219;&#21153;&#20013;&#20855;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.14366</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#20307;&#36719;&#32452;&#32455;&#36827;&#34892;&#20449;&#24687;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Information processing via human soft tissue. (arXiv:2305.14366v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#30340;&#36719;&#32452;&#32455;&#21487;&#29992;&#20110;&#29289;&#29702;&#20648;&#22791;&#35745;&#31639;&#20013;&#30340;&#36719;&#20307;&#65292;&#22312;&#20223;&#30495;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#35745;&#31639;&#20219;&#21153;&#20013;&#20855;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#20307;&#36719;&#32452;&#32455;&#21487;&#34987;&#29992;&#20316;&#29289;&#29702;&#20648;&#22791;&#35745;&#31639;&#20013;&#30340;&#36719;&#20307;&#12290;&#36719;&#29983;&#29289;&#32452;&#32455;&#20855;&#26377;&#24212;&#21147;&#24212;&#21464;&#38750;&#32447;&#24615;&#21644;&#31896;&#24377;&#24615;&#31561;&#29305;&#24615;&#65292;&#28385;&#36275;&#29289;&#29702;&#20648;&#22791;&#35745;&#31639;&#30340;&#35201;&#27714;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;&#21644;&#35760;&#24518;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#20154;&#20307;&#36719;&#32452;&#32455;&#30340;&#21160;&#21147;&#23398;&#20316;&#20026;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20223;&#30495;&#30340;&#29289;&#29702;&#20648;&#22791;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#27010;&#24565;&#65292;&#20174;&#20154;&#20307;&#21442;&#19982;&#32773;&#20013;&#33719;&#21462;&#25163;&#33109;&#20851;&#33410;&#30340;&#23624;&#20280;&#36816;&#21160;&#30340;&#20851;&#33410;&#35282;&#24230;&#25968;&#25454;&#21644;&#20851;&#32852;&#36816;&#21160;&#30340;&#32908;&#32905;&#30340;&#36229;&#22768;&#22270;&#20687;&#12290;&#31995;&#32479;&#30340;&#36755;&#20837;&#26159;&#25163;&#33109;&#20851;&#33410;&#30340;&#35282;&#24230;&#65292;&#32780;&#32908;&#32905;&#20869;&#30340;&#21464;&#24418;&#22330;&#65288;&#20174;&#36229;&#22768;&#22270;&#20687;&#20013;&#33719;&#24471;&#65289;&#20195;&#34920;&#20102;&#20648;&#22791;&#30340;&#29366;&#24577;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36719;&#32452;&#32455;&#30340;&#21160;&#21147;&#23398;&#23545;&#20110;&#36890;&#36807;&#29289;&#29702;&#20648;&#22791;&#35745;&#31639;&#26469;&#20223;&#30495;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#35745;&#31639;&#20219;&#21153;&#20855;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study demonstrates that the soft biological tissues of humans can be used as a type of soft body in physical reservoir computing. Soft biological tissues possess characteristics such as stress-strain nonlinearity and viscoelasticity that satisfy the requirements for physical reservoir computing, including nonlinearity and memory. The aim of this study was to utilize the dynamics of human soft tissues as a physical reservoir for the emulation of nonlinear dynamical systems. To demonstrate this concept, joint angle data during motion in the flexion-extension direction of the wrist joint, and ultrasound images of the muscles associated with that motion, were acquired from human participants. The input to the system was the angle of the wrist joint, while the deformation field within the muscle (obtained from ultrasound images) represented the state of the reservoir. The results indicate that the dynamics of soft tissue have a positive impact on the computational task of emulating non
&lt;/p&gt;</description></item><item><title>&#38899;&#20048;&#30103;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#65292;&#32780;&#19988;&#27809;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#24403;&#21069;&#38899;&#20048;&#30693;&#35273;&#21644;&#22788;&#29702;&#29702;&#35770;&#30340;&#27010;&#36848;&#21450;&#20854;&#24212;&#29992;&#22312;&#36816;&#21160;&#12289;&#24773;&#32490;&#21644;&#24515;&#34880;&#31649;&#35843;&#33410;&#31561;&#39046;&#22495;&#20013;&#30340;&#35777;&#25454;&#65292;&#24182;&#30528;&#37325;&#24378;&#35843;&#20102;&#22914;&#20309;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#20010;&#24615;&#21270;&#21644;&#33258;&#21160;&#21270;&#38899;&#20048;&#36873;&#25321;&#36807;&#31243;&#26469;&#36866;&#24212;&#20010;&#20307;&#30340;&#38656;&#27714;&#21644;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.14364</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#38899;&#20048;&#27835;&#30103;&#65306;&#31070;&#32463;&#35745;&#31639;&#24314;&#27169;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Towards personalised music-therapy; a neurocomputational modelling perspective. (arXiv:2305.14364v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14364
&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#30103;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#65292;&#32780;&#19988;&#27809;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#24403;&#21069;&#38899;&#20048;&#30693;&#35273;&#21644;&#22788;&#29702;&#29702;&#35770;&#30340;&#27010;&#36848;&#21450;&#20854;&#24212;&#29992;&#22312;&#36816;&#21160;&#12289;&#24773;&#32490;&#21644;&#24515;&#34880;&#31649;&#35843;&#33410;&#31561;&#39046;&#22495;&#20013;&#30340;&#35777;&#25454;&#65292;&#24182;&#30528;&#37325;&#24378;&#35843;&#20102;&#22914;&#20309;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#20010;&#24615;&#21270;&#21644;&#33258;&#21160;&#21270;&#38899;&#20048;&#36873;&#25321;&#36807;&#31243;&#26469;&#36866;&#24212;&#20010;&#20307;&#30340;&#38656;&#27714;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38899;&#20048;&#30103;&#27861;&#20316;&#20026;&#19968;&#31181;&#25104;&#21151;&#30340;&#24178;&#39044;&#25163;&#27573;&#32780;&#20986;&#29616;&#65292;&#21487;&#20197;&#22312;&#22823;&#33539;&#22260;&#30340;&#31070;&#32463;&#21644;&#24773;&#32490;&#38556;&#30861;&#20013;&#25552;&#39640;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#65292;&#32780;&#19988;&#27809;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;&#33041;&#32593;&#32476;&#36890;&#36807;&#33258;&#19978;&#32780;&#19979;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#24335;&#34987;&#38899;&#20048;&#25152;&#29301;&#24341;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#39044;&#27979;&#26694;&#26550;&#65292;&#21548;&#35273;&#31995;&#32479;&#19982;&#36816;&#21160;&#31995;&#32479;&#21644;&#22870;&#21169;&#31995;&#32479;&#30340;&#30452;&#25509;&#30456;&#20114;&#20316;&#29992;&#35299;&#37322;&#20102;&#38899;&#20048;&#30103;&#27861;&#22312;&#36816;&#21160;&#24247;&#22797;&#20013;&#30340;&#21151;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24403;&#21069;&#38899;&#20048;&#30693;&#35273;&#21644;&#22788;&#29702;&#29702;&#35770;&#30340;&#31616;&#35201;&#27010;&#36848;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#38899;&#20048;&#30456;&#20851;&#24178;&#39044;&#30340;&#35777;&#25454;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#36816;&#21160;&#12289;&#24773;&#32490;&#21644;&#24515;&#34880;&#31649;&#35843;&#33410;&#26041;&#38754;&#12290;&#25105;&#20204;&#30528;&#37325;&#24378;&#35843;&#25552;&#39640;&#29983;&#21629;&#36136;&#37327;&#21644;&#22312;&#20020;&#24202;&#29615;&#22659;&#20043;&#22806;&#20197;&#21450;&#20581;&#24247;&#20010;&#20307;&#20013;&#20943;&#36731;&#21387;&#21147;&#30340;&#26426;&#20250;&#12290;&#36825;&#20010;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#38656;&#35201;&#20102;&#35299;&#22914;&#20309;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#20010;&#24615;&#21270;&#21644;&#33258;&#21160;&#21270;&#38899;&#20048;&#36873;&#25321;&#36807;&#31243;&#26469;&#36866;&#24212;&#20010;&#20307;&#30340;&#38656;&#27714;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music therapy has emerged recently as a successful intervention that improves patient's outcome in a large range of neurological and mood disorders without adverse effects. Brain networks are entrained to music in ways that can be explained both via top-down and bottom-up processes. In particular, the direct interaction of auditory with the motor and the reward system via a predictive framework explains the efficacy of music-based interventions in motor rehabilitation. In this manuscript, we provide a brief overview of current theories of music perception and processing. Subsequently, we summarise evidence of music-based interventions primarily in motor, emotional and cardiovascular regulation. We highlight opportunities to improve quality of life and reduce stress beyond the clinic environment and in healthy individuals. This relatively unexplored area requires an understanding of how we can personalise and automate music selection processes to fit individuals needs and tasks via feed
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38754;&#37096;&#22270;&#20687;&#36827;&#34892;&#35821;&#38899;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#21442;&#32771;&#35821;&#38899;&#26102;&#23454;&#29616;&#38646;&#26679;&#26412;&#20010;&#24615;&#21270;&#65292;&#36890;&#36807;&#20998;&#31163;&#35828;&#35805;&#32773;&#36523;&#20221;&#21644;&#35821;&#35328;&#20869;&#23481;&#65292;&#20351;&#38754;&#37096;&#29305;&#24449;&#21487;&#20197;&#25511;&#21046;&#26410;&#30693;&#35828;&#35805;&#32773;&#29305;&#24449;&#65292;&#25193;&#23637;&#20102;&#21767;&#35821;&#36716;&#35821;&#38899;&#21512;&#25104;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.14359</link><description>&lt;p&gt;
&#21033;&#29992;&#38754;&#37096;&#22270;&#20687;&#25511;&#21046;&#35821;&#38899;&#21512;&#25104;&#30340;&#38646;&#26679;&#26412;&#20010;&#24615;&#21270;&#21767;&#35821;&#36716;&#22768;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-shot personalized lip-to-speech synthesis with face image based voice control. (arXiv:2305.14359v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14359
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38754;&#37096;&#22270;&#20687;&#36827;&#34892;&#35821;&#38899;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#21442;&#32771;&#35821;&#38899;&#26102;&#23454;&#29616;&#38646;&#26679;&#26412;&#20010;&#24615;&#21270;&#65292;&#36890;&#36807;&#20998;&#31163;&#35828;&#35805;&#32773;&#36523;&#20221;&#21644;&#35821;&#35328;&#20869;&#23481;&#65292;&#20351;&#38754;&#37096;&#29305;&#24449;&#21487;&#20197;&#25511;&#21046;&#26410;&#30693;&#35828;&#35805;&#32773;&#29305;&#24449;&#65292;&#25193;&#23637;&#20102;&#21767;&#35821;&#36716;&#35821;&#38899;&#21512;&#25104;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21767;&#35821;&#36716;&#35821;&#38899;&#21512;&#25104;&#65288;Lip2Speech&#65289;&#26159;&#26681;&#25454;&#20154;&#30340;&#21475;&#22411;&#22270;&#20687;&#39044;&#27979;&#30456;&#24212;&#35821;&#38899;&#30340;&#25216;&#26415;&#65292;&#32463;&#36807;&#22810;&#39033;&#29420;&#31435;&#30740;&#31350;&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#35757;&#32451;&#31574;&#30053;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#26080;&#27861;&#22312;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#23454;&#29616;&#35821;&#38899;&#25511;&#21046;&#65292;&#22240;&#20026;&#33258;&#28982;&#21442;&#32771;&#35821;&#38899;&#20013;&#38656;&#35201;&#25552;&#21462;&#39069;&#22806;&#30340;&#35828;&#35805;&#32773;&#23884;&#20837;&#21521;&#37327;&#65292;&#24403;&#20165;&#32473;&#20986;&#26410;&#35265;&#36807;&#30340;&#35828;&#35805;&#32773;&#30340;&#27785;&#40664;&#35270;&#39057;&#26102;&#65292;&#36825;&#20123;&#21521;&#37327;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20010;&#24615;&#21270;Lip2Speech&#21512;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#38754;&#37096;&#22270;&#20687;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#12290;&#37319;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#20998;&#31163;&#35828;&#35805;&#32773;&#36523;&#20221;&#21644;&#35821;&#35328;&#20869;&#23481;&#34920;&#31034;&#65292;&#20174;&#32780;&#20351;&#35828;&#35805;&#32773;&#23884;&#20837;&#21521;&#37327;&#33021;&#22815;&#25511;&#21046;&#26410;&#35265;&#36807;&#30340;&#35828;&#35805;&#32773;&#21512;&#25104;&#35821;&#38899;&#30340;&#35821;&#38899;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#20851;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#38754;&#37096;&#30340;&#35828;&#35805;&#32773;&#23884;&#20837;&#21521;&#37327;&#65288;FSE&#65289;&#23545;&#35821;&#38899;&#25511;&#21046;&#30340;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lip-to-Speech (Lip2Speech) synthesis, which predicts corresponding speech from talking face images, has witnessed significant progress with various models and training strategies in a series of independent studies. However, existing studies can not achieve voice control under zero-shot condition, because extra speaker embeddings need to be extracted from natural reference speech and are unavailable when only the silent video of an unseen speaker is given. In this paper, we propose a zero-shot personalized Lip2Speech synthesis method, in which face images control speaker identities. A variational autoencoder is adopted to disentangle the speaker identity and linguistic content representations, which enables speaker embeddings to control the voice characteristics of synthetic speech for unseen speakers. Furthermore, we propose associated cross-modal representation learning to promote the ability of face-based speaker embeddings (FSE) on voice control. Extensive experiments verify the eff
&lt;/p&gt;</description></item><item><title>DUBLIN&#26159;&#19968;&#20010;&#38024;&#23545;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#25513;&#27169;&#25991;&#26723;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#12289;&#36793;&#30028;&#26694;&#20219;&#21153;&#21644;&#28210;&#26579;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#31454;&#20105;&#24615;&#25110;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14218</link><description>&lt;p&gt;
DUBLIN&#8212;&#8212;&#36890;&#36807;&#35821;&#35328;-&#22270;&#20687;&#32593;&#32476;&#36827;&#34892;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
DUBLIN -- Document Understanding By Language-Image Network. (arXiv:2305.14218v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14218
&lt;/p&gt;
&lt;p&gt;
DUBLIN&#26159;&#19968;&#20010;&#38024;&#23545;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#25513;&#27169;&#25991;&#26723;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#12289;&#36793;&#30028;&#26694;&#20219;&#21153;&#21644;&#28210;&#26579;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#31454;&#20105;&#24615;&#25110;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#20998;&#26512;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#12290;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#27969;&#27700;&#32447;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#25991;&#26723;&#31867;&#22411;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#20808;&#20351;&#29992;&#19977;&#31181;&#26032;&#39062;&#30446;&#26631;&#22312;Web&#39029;&#38754;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;DUBLIN&#27169;&#22411;&#65306;&#25513;&#27169;&#25991;&#26723;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#12289;&#36793;&#30028;&#26694;&#20219;&#21153;&#21644;&#28210;&#26579;&#38382;&#31572;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20363;&#22914;&#22522;&#20110;Web&#30340;&#32467;&#26500;&#21270;&#38405;&#35835;&#29702;&#35299;&#12289;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#12289;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#12289;&#22270;&#35299;&#29702;&#35299;&#21644;&#34920;&#26684;&#38382;&#31572;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DUBLIN&#26159;&#39318;&#20010;&#22312;WebSRC&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;EM 77.75&#21644;F1 84.25&#30340;&#22522;&#20110;&#20687;&#32032;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual document understanding is a complex task that involves analyzing both the text and the visual elements in document images. Existing models often rely on manual feature engineering or domain-specific pipelines, which limit their generalization ability across different document types and languages. In this paper, we propose DUBLIN, which is pretrained on web pages using three novel objectives: Masked Document Content Generation Task, Bounding Box Task, and Rendered Question Answering Task, that leverage both the spatial and semantic information in the document images. Our model achieves competitive or state-of-the-art results on several benchmarks, such as Web-Based Structural Reading Comprehension, Document Visual Question Answering, Key Information Extraction, Diagram Understanding, and Table Question Answering. In particular, we show that DUBLIN is the first pixel-based model to achieve an EM of 77.75 and F1 of 84.25 on the WebSRC dataset. We also show that our model outperform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#20154;&#21475;&#25968;&#25454;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;GPT-3.5&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#21516;&#65292;&#22312;&#21333;&#35789;&#24847;&#20041;&#25512;&#26029;&#26041;&#38754;&#27169;&#25311;&#20102;&#20856;&#22411;6-9&#23681;&#20799;&#31461;&#30340;&#33021;&#21147;&#65292;&#22312;&#35760;&#24518;&#26041;&#38754;&#21017;&#34920;&#29616;&#20248;&#20110;&#20856;&#22411;21&#23681;&#24180;&#36731;&#20154;&#12290;</title><link>http://arxiv.org/abs/2305.14195</link><description>&lt;p&gt;
GPT&#31350;&#31455;&#26377;&#22810;&#32769;&#65311;HumBEL&#26694;&#26550;&#36890;&#36807;&#20154;&#32676;&#25968;&#25454;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How Old is GPT?: The HumBEL Framework for Evaluating Language Models using Human Demographic Data. (arXiv:2305.14195v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#20154;&#21475;&#25968;&#25454;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;GPT-3.5&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#21516;&#65292;&#22312;&#21333;&#35789;&#24847;&#20041;&#25512;&#26029;&#26041;&#38754;&#27169;&#25311;&#20102;&#20856;&#22411;6-9&#23681;&#20799;&#31461;&#30340;&#33021;&#21147;&#65292;&#22312;&#35760;&#24518;&#26041;&#38754;&#21017;&#34920;&#29616;&#20248;&#20110;&#20856;&#22411;21&#23681;&#24180;&#36731;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#24182;&#26410;&#32771;&#34385;&#27169;&#22411;&#30340;&#35821;&#35328;&#20351;&#29992;&#19982;&#29305;&#23450;&#20154;&#32676;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#36825;&#19968;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#27979;&#37327;&#21644;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#19982;&#20154;&#31867;&#23376;&#32676;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#20511;&#21161;&#35821;&#35328;&#30149;&#29702;&#23398;&#30340;&#20020;&#24202;&#25216;&#26415;&#65292;&#35813;&#23398;&#31185;&#24050;&#32463;&#24314;&#31435;&#20102;&#19981;&#21516;&#65288;&#20154;&#31867;&#65289;&#24180;&#40836;&#38454;&#27573;&#30340;&#35821;&#35328;&#33021;&#21147;&#21457;&#23637;&#35268;&#33539;&#65292;&#23545;&#25216;&#33021;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#19982;&#39046;&#22495;&#19987;&#23478;&#65288;&#21363;&#25345;&#26377;&#20020;&#24202;&#35768;&#21487;&#35777;&#30340;&#35821;&#35328;&#30149;&#29702;&#23398;&#23478;&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#30340;&#35780;&#20272;&#25216;&#26415;&#20197;&#23454;&#29616;&#35268;&#27169;&#21270;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3.5&#30340;&#33021;&#21147;&#22240;&#20219;&#21153;&#32780;&#24322;&#65292;&#22312;&#21333;&#35789;&#24847;&#20041;&#25512;&#26029;&#26041;&#38754;&#27169;&#25311;&#20102;&#20856;&#22411;6-9&#23681;&#20799;&#31461;&#30340;&#33021;&#21147;&#65292;&#22312;&#35760;&#24518;&#26041;&#38754;&#21017;&#34920;&#29616;&#20248;&#20110;&#20856;&#22411;21&#23681;&#24180;&#36731;&#20154;&#12290;GPT-3.5&#65288;InstructGPT&#65289;&#22312;&#31038;&#20132;&#20132;&#20114;&#20219;&#21153;&#20013;&#20063;&#23384;&#22312;&#19968;&#23450;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large pre-trained language models (LMs) find greater use across NLP, existing evaluation protocols do not consider how LM language use aligns with particular human demographic groups, which can be an important consideration in conversational AI applications. To remedy this gap, we consider how LM language skills can be measured and compared to human sub-populations. We suggest clinical techniques from Speech Language Pathology, which has well-established norms for acquisition of language skills, organized by (human) age. We conduct evaluation with a domain expert (i.e., a clinically licensed speech language pathologist), and also propose automated techniques to substitute clinical evaluation at scale. We find LM capability varies widely depending on task with GPT-3.5 mimicking the ability of a typical 6-9 year old at tasks requiring inference about word meanings and simultaneously outperforming a typical 21 year old at memorization. GPT-3.5 (InstructGPT) also has trouble with soc
&lt;/p&gt;</description></item><item><title>DetGPT &#26159;&#19968;&#31181;&#22522;&#20110;&#25512;&#29702;&#30340;&#29289;&#20307;&#26816;&#27979;&#33539;&#20363;&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#35270;&#35273;&#22330;&#26223;&#36827;&#34892;&#25512;&#29702;&#65292;&#20174;&#32780;&#23547;&#25214;&#21040;&#29992;&#25143;&#24819;&#35201;&#30340;&#30446;&#26631;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.14167</link><description>&lt;p&gt;
DetGPT&#65306;&#36890;&#36807;&#25512;&#29702;&#25214;&#21040;&#20320;&#24819;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
DetGPT: Detect What You Need via Reasoning. (arXiv:2305.14167v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14167
&lt;/p&gt;
&lt;p&gt;
DetGPT &#26159;&#19968;&#31181;&#22522;&#20110;&#25512;&#29702;&#30340;&#29289;&#20307;&#26816;&#27979;&#33539;&#20363;&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#35270;&#35273;&#22330;&#26223;&#36827;&#34892;&#25512;&#29702;&#65292;&#20174;&#32780;&#23547;&#25214;&#21040;&#29992;&#25143;&#24819;&#35201;&#30340;&#30446;&#26631;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#22240;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#32780;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#24471;&#20154;&#26426;&#20132;&#20114;&#26356;&#21152;&#26377;&#25928;&#21644;&#22797;&#26434;&#65292;&#20026;&#27169;&#31946;&#20154;&#31867;&#21644;&#26426;&#22120;&#26234;&#33021;&#20043;&#38388;&#30340;&#30028;&#32447;&#25171;&#24320;&#20102;&#21019;&#26032;&#25216;&#26415;&#30340;&#22823;&#38376;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#20307;&#26816;&#27979;&#33539; paradigm&#65292;&#31216;&#20026;&#22522;&#20110;&#25512;&#29702;&#30340;&#29289;&#20307;&#26816;&#27979;&#12290;&#19982;&#20256;&#32479;&#30340;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#29289;&#20307;&#21517;&#31216;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#25143;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19982;&#31995;&#32479;&#20132;&#20114;&#65292;&#23454;&#29616;&#26356;&#39640;&#23618;&#27425;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026; DetGPT&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#22810;&#27169;&#22411;&#21644;&#24320;&#25918;&#35789;&#27719;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#22312;&#29992;&#25143;&#25351;&#20196;&#21644;&#35270;&#35273;&#22330;&#26223;&#30340;&#19978;&#19979;&#25991;&#20013;&#25191;&#34892;&#25512;&#29702;&#12290;&#36825;&#20351; DetGPT &#33021;&#22815;&#33258;&#21160;&#23450;&#20301;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#65292;&#21363;&#20351;&#23545;&#35937;&#22312;&#25351;&#20196;&#20013;&#27809;&#26377;&#26126;&#30830;&#21629;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of computer vision has seen significant advancements thanks to the development of large language models (LLMs). These models have enabled more effective and sophisticated interactions between humans and machines, paving the way for novel techniques that blur the lines between human and machine intelligence. In this paper, we introduce a new paradigm for object detection that we call reasoning-based object detection. Unlike conventional object detection methods that rely on specific object names, our approach enables users to interact with the system using natural language instructions, allowing for a higher level of interactivity. Our proposed method, called DetGPT, leverages state-of-the-art multi-modal models and open-vocabulary object detectors to perform reasoning within the context of the user's instructions and the visual scene. This enables DetGPT to automatically locate the object of interest based on the user's expressed desires, even if the object i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#25968;&#25454;&#38598;CoLAC&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#20302;&#20110;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#36328;&#35821;&#35328;&#36716;&#31227;&#21487;&#34892;&#65292;&#24182;&#21487;&#20197;&#36861;&#28335;&#21040;&#39044;&#35757;&#32451;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2305.14091</link><description>&lt;p&gt;
&#37325;&#28201;&#25509;&#21463;&#24615;&#21028;&#26029;
&lt;/p&gt;
&lt;p&gt;
Revisiting Acceptability Judgements. (arXiv:2305.14091v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#25968;&#25454;&#38598;CoLAC&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#20302;&#20110;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#36328;&#35821;&#35328;&#36716;&#31227;&#21487;&#34892;&#65292;&#24182;&#21487;&#20197;&#36861;&#28335;&#21040;&#39044;&#35757;&#32451;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33267;NLP&#31038;&#21306;&#26368;&#21518;&#19968;&#27425;&#20851;&#27880;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#24050;&#32463;&#36807;&#21435;&#22810;&#24180;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#37325;&#28201;&#36825;&#20010;&#35805;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; CoLAC-&#20013;&#25991;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30001;&#27597;&#35821;&#35762;&#32773;&#39564;&#35777;&#24182;&#24102;&#26377;&#20004;&#32452;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#38750;&#33521;&#35821;&#21487;&#25509;&#21463;&#24615;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26368;&#22823;&#30340;InstructGPT&#27169;&#22411;&#22312;CoLAC&#19978;&#20063;&#21482;&#33021;&#34920;&#29616;&#38543;&#26426;&#27700;&#24179;&#65292;&#32780;ChatGPT&#30340;&#24615;&#33021;&#65288;48.30 MCC&#65289;&#20063;&#36828;&#20302;&#20110;&#30417;&#30563;&#27169;&#22411;&#65288;59.03 MCC&#65289;&#21644;&#20154;&#31867;&#65288;65.11 MCC&#65289;&#12290;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#23454;&#39564;&#21644;&#32454;&#31890;&#24230;&#30340;&#35821;&#35328;&#20998;&#26512;&#65292;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#30693;&#35782;&#21487;&#20197;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#20043;&#38388;&#36716;&#31227;&#65292;&#32780;&#19988;&#21487;&#20197;&#36861;&#28335;&#21040;&#39044;&#35757;&#32451;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Years have passed since the NLP community has last focused on linguistic acceptability. In this work, we revisit this topic in the context of large language models. We introduce CoLAC - Corpus of Linguistic Acceptability in Chinese, the first large-scale non-English acceptability dataset that is verified by native speakers and comes with two sets of labels. Our experiments show that even the largest InstructGPT model performs only at chance level on CoLAC, while ChatGPT's performance (48.30 MCC) is also way below supervised models (59.03 MCC) and human (65.11 MCC). Through cross-lingual transfer experiments and fine-grained linguistic analysis, we demonstrate for the first time that knowledge of linguistic acceptability can be transferred across typologically distinct languages, as well as be traced back to pre-training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#21644;&#28151;&#21512;&#27169;&#22411;&#31561;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20844;&#20849;&#30340;&#20013;&#22269;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#34701;&#21512;&#20102;CNN&#21644;LSTM&#30340;&#28151;&#21512;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.13941</link><description>&lt;p&gt;
&#25163;&#35821;&#35782;&#21035;&#25216;&#26415;&#21644;&#31639;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Techniques and Algorithms for Recognising Sign Language. (arXiv:2305.13941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#21644;&#28151;&#21512;&#27169;&#22411;&#31561;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20844;&#20849;&#30340;&#20013;&#22269;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#34701;&#21512;&#20102;CNN&#21644;LSTM&#30340;&#28151;&#21512;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#26159;&#19968;&#31181;&#35270;&#35273;&#35821;&#35328;&#65292;&#22686;&#24378;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#24182;&#19988;&#32463;&#24120;&#20316;&#20026;&#20808;&#22825;&#24615;&#21548;&#21147;&#20007;&#22833;&#32773;&#20027;&#35201;&#30340;&#20132;&#27969;&#26041;&#24335;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20351;&#29992;&#25163;&#35821;&#30340;&#20808;&#22825;&#24615;&#21548;&#21147;&#20007;&#22833;&#32773;&#24182;&#19981;&#22810;&#65292;&#20182;&#20204;&#32463;&#24120;&#38754;&#20020;&#31038;&#20132;&#23396;&#31435;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#21019;&#24314;&#20154;&#26426;&#30028;&#38754;&#31995;&#32479;&#65292;&#20026;&#21548;&#21147;&#38556;&#30861;&#20154;&#22763;&#25552;&#20379;&#31038;&#20132;&#24179;&#21488;&#12290;&#24066;&#22330;&#19978;&#22823;&#22810;&#25968;&#21830;&#29992;&#25163;&#35821;&#32763;&#35793;&#31995;&#32479;&#26159;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#65292;&#20215;&#26684;&#26114;&#36149;&#65292;&#20351;&#29992;&#36215;&#26469;&#20063;&#24456;&#22256;&#38590;&#12290;&#23613;&#31649;&#36843;&#20999;&#38656;&#35201;&#22522;&#20110;&#35270;&#35273;&#30340;&#31995;&#32479;&#65292;&#20294;&#39318;&#20808;&#24517;&#39035;&#20811;&#26381;&#20960;&#20010;&#25361;&#25112;&#12290;&#26089;&#26399;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#25216;&#26415;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#21253;&#21547;&#26102;&#38388;&#20449;&#24687;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#29616;&#22312;&#27491;&#22312;&#24212;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#23558;&#25163;&#37096;&#21644;&#25163;&#35821;&#21160;&#20316;&#36716;&#21270;&#20026;&#21475;&#35821;&#25110;&#20070;&#38754;&#35821;&#35328;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21644;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20844;&#20849;&#30340;&#20013;&#22269;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#34701;&#21512;&#20102;CNN&#21644;LSTM&#30340;&#28151;&#21512;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign language is a visual language that enhances communication between people and is frequently used as the primary form of communication by people with hearing loss. Even so, not many people with hearing loss use sign language, and they frequently experience social isolation. Therefore, it is necessary to create human-computer interface systems that can offer hearing-impaired people a social platform. Most commercial sign language translation systems now on the market are sensor-based, pricey, and challenging to use. Although vision-based systems are desperately needed, they must first overcome several challenges. Earlier continuous sign language recognition techniques used hidden Markov models, which have a limited ability to include temporal information. To get over these restrictions, several machine learning approaches are being applied to transform hand and sign language motions into spoken or written language. In this study, we compare various deep learning techniques for recogn
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#26694;&#26550;&#65292;&#24182;&#21253;&#25324;&#20102;&#29420;&#29305;&#30340;&#23376;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#36825;&#22235;&#31181;&#27169;&#24577;&#30340;&#29616;&#23454;&#24212;&#29992;&#12290;&#32426;&#24405;&#20102;&#30456;&#20851;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.13507</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#65306;&#19968;&#20221;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multimodal Automated Fact-Checking: A Survey. (arXiv:2305.13507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#26694;&#26550;&#65292;&#24182;&#21253;&#25324;&#20102;&#29420;&#29305;&#30340;&#23376;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#36825;&#22235;&#31181;&#27169;&#24577;&#30340;&#29616;&#23454;&#24212;&#29992;&#12290;&#32426;&#24405;&#20102;&#30456;&#20851;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38169;&#35823;&#20449;&#24687;&#65292;&#21363;&#20107;&#23454;&#19978;&#19981;&#27491;&#30830;&#30340;&#20449;&#24687;&#65292;&#36890;&#24120;&#20197;&#22810;&#31181;&#24418;&#24335;&#20256;&#36798;&#65292;&#20363;&#22914;&#24102;&#26377;&#26631;&#39064;&#30340;&#22270;&#20687;&#12290; &#23427;&#34987;&#20154;&#20204;&#35270;&#20026;&#26356;&#21487;&#20449;&#65292;&#27604;&#20854;&#20165;&#38480;&#20110;&#25991;&#26412;&#30340;&#23545;&#24212;&#29289;&#25193;&#25955;&#36895;&#24230;&#26356;&#24555;&#65292;&#33539;&#22260;&#26356;&#24191;&#12290; &#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#28041;&#21450;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#65288;AFC&#65289;&#65292;&#20294;&#20197;&#24448;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;&#25991;&#26412;&#35823;&#23548;&#26041;&#38754;&#12290; &#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#22810;&#27169;&#24577;&#35823;&#23548;&#29420;&#29305;&#23376;&#20219;&#21153;&#22312;&#20869;&#30340;AFC&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19978;&#35752;&#35770;&#20102;&#19981;&#21516;&#31038;&#21306;&#25152;&#21457;&#23637;&#30340;&#30456;&#20851;&#26415;&#35821;&#12290; &#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#20013;&#23384;&#22312;&#30340;&#22235;&#31181;&#27169;&#24577;&#65306;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#12290; &#25105;&#20204;&#35843;&#26597;&#20102;&#22522;&#20934;&#21644;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation, i.e. factually incorrect information, is often conveyed in multiple modalities, e.g. an image accompanied by a caption. It is perceived as more credible by humans, and spreads faster and wider than its text-only counterparts. While an increasing body of research investigates automated fact-checking (AFC), previous surveys mostly focus on textual misinformation. In this survey, we conceptualise a framework for AFC including subtasks unique to multimodal misinformation. Furthermore, we discuss related terminological developed in different communities in the context of our framework. We focus on four modalities prevalent in real-world fact-checking: text, image, audio, and video. We survey benchmarks and models, and discuss limitations and promising directions for future research.
&lt;/p&gt;</description></item><item><title>Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13484</link><description>&lt;p&gt;
Flover&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13484
&lt;/p&gt;
&lt;p&gt;
Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#27169;&#22411;&#25512;&#26029;&#24615;&#33021;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#23588;&#20854;&#26159;&#22312;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#24182;&#34987;&#37096;&#32626;&#22312;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24773;&#20917;&#19979;&#12290;&#33258;&#22238;&#24402;&#27169;&#22411;&#30001;&#20110;&#22312;&#20247;&#22810;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22240;&#27492;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#35774;&#35745;&#19978;&#37319;&#29992;&#20102;&#19968;&#31181;&#26102;&#38388;&#20381;&#36182;&#32467;&#26500;&#65292;&#20854;&#20013;&#24403;&#21069;token&#30340;&#27010;&#29575;&#20998;&#24067;&#21463;&#21040;&#21069;&#38754;token&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26412;&#36136;&#19978;&#30340;&#24207;&#21015;&#29305;&#24615;&#36981;&#24490;&#39532;&#23572;&#21487;&#22827;&#38142;&#20551;&#35774;&#65292;&#32570;&#20047;&#26102;&#38388;&#24182;&#34892;&#24615;&#65292;&#22240;&#27492;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#22312;&#24037;&#19994;&#32972;&#26223;&#19979;&#65292;&#25512;&#26029;&#35831;&#27714;&#36981;&#24490;&#27850;&#26494;&#26102;&#38388;&#20998;&#24067;&#65292;&#38656;&#35201;&#19981;&#21516;&#30340;&#21709;&#24212;&#38271;&#24230;&#65292;&#36825;&#31181;&#24182;&#34892;&#24615;&#30340;&#32570;&#22833;&#26356;&#21152;&#26126;&#26174;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22914;&#21160;&#24577;&#25209;&#22788;&#29702;&#21644;&#24182;&#21457;&#27169;&#22411;&#23454;&#20363;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#31895;&#31890;&#24230;&#30340;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#30340;&#24320;&#38144;&#21644;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#26080;&#27861;&#23454;&#29616;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of deep learning, the performance of model inference has become a pivotal aspect as models become more complex and are deployed in diverse applications. Among these, autoregressive models stand out due to their state-of-the-art performance in numerous generative tasks. These models, by design, harness a temporal dependency structure, where the current token's probability distribution is conditioned on preceding tokens. This inherently sequential characteristic, however, adheres to the Markov Chain assumption and lacks temporal parallelism, which poses unique challenges. Particularly in industrial contexts where inference requests, following a Poisson time distribution, necessitate diverse response lengths, this absence of parallelism is more profound. Existing solutions, such as dynamic batching and concurrent model instances, nevertheless, come with severe overheads and a lack of flexibility, these coarse-grained methods fall short of achieving optimal la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;&#36827;&#34892;&#20102;&#37327;&#21270;&#24314;&#27169;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#23545;&#29289;&#29702;&#24773;&#22659;&#30340;&#36259;&#21619;&#21453;&#24212;&#65292;&#36824;&#21457;&#29616;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#39044;&#27979;&#20154;&#31867;&#21453;&#24212;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#21644;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.13452</link><description>&lt;p&gt;
&#27979;&#37327;&#21644;&#24314;&#27169;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Measuring and Modeling Physical Intrinsic Motivation. (arXiv:2305.13452v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;&#36827;&#34892;&#20102;&#37327;&#21270;&#24314;&#27169;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#23545;&#29289;&#29702;&#24773;&#22659;&#30340;&#36259;&#21619;&#21453;&#24212;&#65292;&#36824;&#21457;&#29616;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#39044;&#27979;&#20154;&#31867;&#21453;&#24212;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#21644;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26159;&#26377;&#39537;&#21160;&#21147;&#30340;&#20114;&#21160;&#24615;&#20195;&#29702;&#65292;&#20182;&#20204;&#36861;&#27714;&#26377;&#36259;&#30340;&#29289;&#29702;&#21160;&#21147;&#23398;&#24773;&#22659;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24418;&#24335;&#21270;&#30340;&#29289;&#29702;&#20869;&#22312;&#21160;&#26426;&#24418;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#20154;&#31867;&#23545;&#22810;&#31181;&#29289;&#29702;&#24773;&#22659;&#30340;&#35780;&#20998;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#20381;&#36182;&#20110;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#30340;&#27169;&#22411;&#21040;&#20381;&#36182;&#20110;&#21069;&#21521;&#29289;&#29702;&#39044;&#27979;&#30340;&#27169;&#22411;&#30340;&#21508;&#31181;&#20869;&#22312;&#21160;&#26426;&#20551;&#35774;&#26469;&#24314;&#27169;&#20154;&#31867;&#30340;&#36259;&#21619;&#21453;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20154;&#31867;&#21453;&#24212;&#30340;&#21333;&#19968;&#26368;&#20339;&#39044;&#27979;&#22120;&#26159;&#38024;&#23545;&#29289;&#29702;&#39044;&#27979;&#25439;&#22833;&#25512;&#23548;&#20986;&#30340;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#31616;&#21333;&#30340;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#19981;&#33021;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#25512;&#24191;&#20182;&#20204;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#19982;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#20542;&#21521;&#20110;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are interactive agents driven to seek out situations with interesting physical dynamics. Here we formalize the functional form of physical intrinsic motivation. We first collect ratings of how interesting humans find a variety of physics scenarios. We then model human interestingness responses by implementing various hypotheses of intrinsic motivation including models that rely on simple scene features to models that depend on forward physics prediction. We find that the single best predictor of human responses is adversarial reward, a model derived from physical prediction loss. We also find that simple scene feature models do not generalize their prediction of human responses across all scenarios. Finally, linearly combining the adversarial model with the number of collisions in a scene leads to the greatest improvement in predictivity of human responses, suggesting humans are driven towards scenarios that result in high information gain and physical activity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36973;&#36935;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;LLMs&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#35777;&#25454;&#65292;&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#65292;&#20294;&#20063;&#21487;&#33021;&#26377;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13300</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#20013;&#30340;&#34892;&#20026;&#25581;&#31192;&#65306;&#33258;&#36866;&#24212;&#21464;&#33394;&#40857;&#36824;&#26159;&#22266;&#25191;&#30340;&#26641;&#29549;
&lt;/p&gt;
&lt;p&gt;
Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes. (arXiv:2305.13300v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36973;&#36935;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;LLMs&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#35777;&#25454;&#65292;&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#65292;&#20294;&#20063;&#21487;&#33021;&#26377;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#22806;&#37096;&#20449;&#24687;&#65292;&#24037;&#20855;&#22686;&#24378;&#65288;&#21253;&#25324;&#26816;&#32034;&#22686;&#24378;&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;LLMs&#38745;&#24577;&#21442;&#25968;&#21270;&#20869;&#23384;&#38480;&#21046;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#35777;&#25454;&#19982;&#23427;&#20204;&#30340;&#21442;&#25968;&#21270;&#20869;&#23384;&#21457;&#29983;&#20914;&#31361;&#26102;&#65292;LLMs&#23545;&#36825;&#20123;&#22806;&#37096;&#35777;&#25454;&#26377;&#22810;&#23569;&#25509;&#21463;&#33021;&#21147;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#20174;LLMs&#20013;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#21442;&#25968;&#21270;&#20869;&#23384;&#65292;&#24182;&#26500;&#24314;&#30456;&#24212;&#30340;&#23545;&#31435;&#20869;&#23384;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#19968;&#31995;&#21015;&#21463;&#25511;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;LLMs&#34920;&#29616;&#20986;&#30475;&#20284;&#30683;&#30462;&#30340;&#34892;&#20026;&#12290;&#19968;&#26041;&#38754;&#65292;&#19982;&#20197;&#24448;&#30340;&#35266;&#24565;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#35201;&#22806;&#37096;&#35777;&#25454;&#26159;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#65292;LLMs&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#20063;&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#35777;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;LLMs&#20063;&#21487;&#33021;&#20250;&#34920;&#29616;&#20986;&#23616;&#38480;&#24615;&#65292;&#23588;&#20854;&#26159;&#24403;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#21463;&#21040;&#23041;&#32961;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals seemingly contradicting behaviors of LLMs. On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. On the other hand, LLMs also d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BEHRT&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#21307;&#30103;&#27010;&#24565;&#23884;&#20837;&#65292;&#22312;&#19981;&#20256;&#36755;&#20219;&#20309;&#38544;&#31169;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#21307;&#30103;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#30340;&#20849;&#21516;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#30149;&#20154;&#19979;&#19968;&#27425;&#35786;&#26029;&#26041;&#38754;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.13052</link><description>&lt;p&gt;
&#22522;&#20110;BEHRT&#30340;&#21307;&#30103;&#27010;&#24565;&#23884;&#20837;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Medical Concepts Embedding using BEHRT. (arXiv:2305.13052v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13052
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BEHRT&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#21307;&#30103;&#27010;&#24565;&#23884;&#20837;&#65292;&#22312;&#19981;&#20256;&#36755;&#20219;&#20309;&#38544;&#31169;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#21307;&#30103;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#30340;&#20849;&#21516;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#30149;&#20154;&#19979;&#19968;&#27425;&#35786;&#26029;&#26041;&#38754;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#21253;&#21547;&#30149;&#20154;&#30340;&#35786;&#26029;&#12289;&#33647;&#29289;&#12289;&#27835;&#30103;&#31561;&#21307;&#30103;&#35760;&#24405;&#12290;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#34987;&#35270;&#20026;&#25935;&#24863;&#30340;&#21307;&#30103;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#21307;&#30103;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#36890;&#24120;&#19981;&#33021;&#34987;&#20849;&#20139;&#65292;&#36825;&#20351;&#24471;&#20351;&#29992;&#22810;&#20010;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#21019;&#24314;&#39044;&#27979;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#65292;&#32780;&#36825;&#23545;&#20110;&#27169;&#22411;&#30340;&#24378;&#20581;&#24615;&#21644;&#26222;&#36866;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#31639;&#27861;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20301;&#32622;&#30340;&#25968;&#25454;&#19978;&#23398;&#20064;&#20849;&#20139;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#23558;&#25152;&#26377;&#25968;&#25454;&#23384;&#20648;&#22312;&#20013;&#24515;&#20301;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#21307;&#30103;&#27010;&#24565;&#23884;&#20837;&#12290;&#36825;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#21483;&#20570;BEHRT&#65292;&#26159;&#22312;&#19968;&#20010;&#22823;&#22411;EHR&#25968;&#25454;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#65292;&#23427;&#23558;&#21307;&#30103;&#27010;&#24565;&#32534;&#30721;&#25104;&#39640;&#32500;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20010;&#21307;&#30103;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#20013;&#33719;&#21462;&#21307;&#30103;&#27010;&#24565;&#30340;&#23884;&#20837;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#23427;&#20204;&#20043;&#38388;&#20132;&#25442;&#35760;&#24405;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#19968;&#20010;&#23454;&#38469;&#30340;&#39044;&#27979;&#20219;&#21153;&#65292;&#20351;&#29992;&#22810;&#20010;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#26469;&#39044;&#27979;&#19979;&#19968;&#27425;&#35786;&#26029;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records (EHR) data contains medical records such as diagnoses, medications, procedures, and treatments of patients. This data is often considered sensitive medical information. Therefore, the EHR data from the medical centers often cannot be shared, making it difficult to create prediction models using multi-center EHR data, which is essential for such models' robustness and generalizability. Federated Learning (FL) is an algorithmic approach that allows learning a shared model using data in multiple locations without the need to store all data in a central place. An example of a prediction model's task is to predict future diseases. More specifically, the model needs to predict patient's next visit diagnoses, based on current and previous clinical data. Such a prediction model can support care providers in making clinical decisions and even provide preventive treatment. We propose a federated learning approach for learning medical concepts embedding. This pre-trained
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#22238;&#31572;&#22810;&#34920;&#38382;&#39064;&#12290; &#25105;&#20204;&#30340;&#26032;&#27169;&#22411;MultiTabQA&#19981;&#20165;&#21487;&#20197;&#22238;&#31572;&#22810;&#34920;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#27867;&#21270;&#22320;&#29983;&#25104;&#34920;&#26684;&#31572;&#26696;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#22522;&#32447;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12820</link><description>&lt;p&gt;
MultiTabQA&#65306;&#38024;&#23545;&#22810;&#34920;&#38382;&#39064;&#22238;&#31572;&#29983;&#25104;&#34920;&#26684;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering. (arXiv:2305.12820v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#22238;&#31572;&#22810;&#34920;&#38382;&#39064;&#12290; &#25105;&#20204;&#30340;&#26032;&#27169;&#22411;MultiTabQA&#19981;&#20165;&#21487;&#20197;&#22238;&#31572;&#22810;&#34920;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#27867;&#21270;&#22320;&#29983;&#25104;&#34920;&#26684;&#31572;&#26696;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#22522;&#32447;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#38382;&#31572;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#20854;&#35206;&#30422;&#38754;&#36824;&#21463;&#38480;&#20110;&#31572;&#22797;&#21333;&#34920;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#26597;&#35810;&#20855;&#26377;&#22797;&#26434;&#24615;&#65292;&#24120;&#36328;&#36234;&#22810;&#20010;&#20851;&#31995;&#25968;&#25454;&#24211;&#25110;&#32593;&#39029;&#34920;&#26684;&#12290;&#23545;&#20110;&#21333;&#34920;&#38382;&#39064;&#65292;&#19981;&#28041;&#21450;&#24120;&#35265;&#30340;&#34920;&#26684;&#25805;&#20316;&#65292;&#22914;&#38598;&#21512;&#36816;&#31639;&#12289;&#31515;&#21345;&#23572;&#31215;&#65288;&#36830;&#25509;&#65289;&#25110;&#23884;&#22871;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#22810;&#34920;&#25805;&#20316;&#36890;&#24120;&#20250;&#20135;&#29983;&#34920;&#26684;&#36755;&#20986;&#65292;&#36825;&#23601;&#38656;&#35201;&#34920;&#26684;QA&#27169;&#22411;&#26377;&#29983;&#25104;&#34920;&#26684;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#22238;&#31572;&#22810;&#34920;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;MultiTabQA&#19981;&#20165;&#22238;&#31572;&#22810;&#34920;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#27867;&#21270;&#22320;&#29983;&#25104;&#34920;&#26684;&#31572;&#26696;&#12290;&#20026;&#20102;&#20351;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;132,645&#20010;SQL&#26597;&#35810;&#21644;&#34920;&#26684;&#31572;&#26696;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#20005;&#26684;&#31243;&#24230;&#30340;&#29305;&#23450;&#20110;&#34920;&#26684;&#30340;&#24230;&#37327;&#26631;&#20934;&#23545;&#29983;&#25104;&#30340;&#34920;&#26684;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#65292;MultiTabQA&#22312;&#26032;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#22823;&#22823;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table. However, real-world queries are complex in nature, often over multiple tables in a relational database or web page. Single table questions do not involve common table operations such as set operations, Cartesian products (joins), or nested queries. Furthermore, multi-table operations often result in a tabular output, which necessitates table generation capabilities of tabular QA models. To fill this gap, we propose a new task of answering questions over multiple tables. Our model, MultiTabQA, not only answers questions over multiple tables, but also generalizes to generate tabular answers. To enable effective training, we build a pre-training dataset comprising of 132,645 SQL queries and tabular answers. Further, we evaluate the generated tables by introducing table-specific metrics of varying strictness assessing various levels 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23450;&#29702;&#39537;&#21160;&#30340;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;TheoremQA&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;AI&#27169;&#22411;&#22312;&#24212;&#29992;&#23450;&#29702;&#35299;&#20915;&#31185;&#23398;&#38382;&#39064;&#26102;&#30340;&#33021;&#21147;&#65292;&#32463;&#36807;&#27979;&#35797;&#65292;GPT-4&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#19978;&#30340;&#20934;&#30830;&#29575;&#36828;&#39640;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12524</link><description>&lt;p&gt;
TheoremQA&#65306;&#19968;&#31181;&#23450;&#29702;&#39537;&#21160;&#30340;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TheoremQA: A Theorem-driven Question Answering dataset. (arXiv:2305.12524v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12524
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23450;&#29702;&#39537;&#21160;&#30340;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;TheoremQA&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;AI&#27169;&#22411;&#22312;&#24212;&#29992;&#23450;&#29702;&#35299;&#20915;&#31185;&#23398;&#38382;&#39064;&#26102;&#30340;&#33021;&#21147;&#65292;&#32463;&#36807;&#27979;&#35797;&#65292;GPT-4&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#19978;&#30340;&#20934;&#30830;&#29575;&#36828;&#39640;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;LLMs&#22914;GPT-4&#21644;PaLM-2&#22312;&#35299;&#20915;&#20687;GSM8K&#36825;&#26679;&#30340;&#22522;&#26412;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;90%&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35299;&#20915;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65288;&#21363;&#23450;&#29702;&#65289;&#30340;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TheoremQA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23450;&#29702;&#39537;&#21160;&#30340;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;AI&#27169;&#22411;&#24212;&#29992;&#23450;&#29702;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;TheoremQA&#30001;&#39046;&#22495;&#19987;&#23478;&#31574;&#21010;&#65292;&#21253;&#21547;&#26469;&#33258;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#30005;&#27668;&#19982;&#35745;&#31639;&#26426;&#31185;&#23398;&#20197;&#21450;&#37329;&#34701;&#23398;&#30340;800&#20010;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#28085;&#30422;350&#20010;&#23450;&#29702;&#65288;&#20363;&#22914;&#27888;&#21202;&#23450;&#29702;&#12289;&#25289;&#26684;&#26391;&#26085;&#23450;&#29702;&#12289;&#21704;&#22827;&#26364;&#32534;&#30721;&#12289;&#37327;&#23376;&#23450;&#29702;&#12289;&#24377;&#24615;&#23450;&#29702;&#31561;&#31561;&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;16&#20010;&#22823;&#22411;&#35821;&#35328;&#21644;&#20195;&#30721;&#27169;&#22411;&#20197;&#21450;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#20363;&#22914;Chain-of-Thoughts&#21644;Program-of-Thoughts&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#33021;&#21147;&#26159;&#26080;&#19982;&#20262;&#27604;&#30340;&#65292;&#20351;&#29992;Program-of-Thoughts&#25552;&#31034;&#31574;&#30053;&#26102;&#20934;&#30830;&#29575;&#36798;51%&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#36828;&#36828;&#33853;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems. TheoremQA is curated by domain experts containing 800 high-quality questions covering 350 theorems (e.g. Taylor's theorem, Lagrange's theorem, Huffman coding, Quantum Theorem, Elasticity Theorem, etc) from Math, Physics, EE&amp;CS, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that GPT-4's capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Progra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11490</link><description>&lt;p&gt;
LLM&#33258;&#36523;&#21487;&#35835;&#21462;&#21644;&#29983;&#25104;CXR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#65292;&#20154;&#20204;&#27491;&#31215;&#26497;&#23581;&#35797;&#23558;LLMs&#30340;&#23454;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36830;&#25509;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#19988;&#20063;&#22312;&#19981;&#26029;&#23581;&#35797;&#20026;LLMs&#28155;&#21152;&#35270;&#35273;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21482;&#20351;&#29992;LLMs&#20316;&#20026;&#22270;&#20687;&#35299;&#30721;&#22120;&#65292;&#27809;&#26377;&#23581;&#35797;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;VQ-GAN&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#34920;&#31034;&#35270;&#20026;&#19968;&#31181;&#25991;&#26412;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#65292;&#20197;&#20687;&#25991;&#26412;&#19968;&#26679;&#35835;&#21462;&#21644;&#29983;&#25104;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#35757;&#32451;&#19987;&#38376;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#20449;&#24687;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PastNet&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20613;&#37324;&#21494;&#22495;&#20013;&#24341;&#20837;&#35889;&#21367;&#31215;&#31639;&#23376;&#65292;&#21033;&#29992;&#20869;&#22312;&#30340;&#29289;&#29702;&#30693;&#35782;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#31163;&#25955;&#21270;&#23616;&#37096;&#29305;&#24449;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.11421</link><description>&lt;p&gt;
PastNet&#65306;&#24341;&#20837;&#29289;&#29702;&#24402;&#32435;&#20559;&#24046;&#29992;&#20110;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PastNet: Introducing Physical Inductive Biases for Spatio-temporal Video Prediction. (arXiv:2305.11421v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PastNet&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20613;&#37324;&#21494;&#22495;&#20013;&#24341;&#20837;&#35889;&#21367;&#31215;&#31639;&#23376;&#65292;&#21033;&#29992;&#20869;&#22312;&#30340;&#29289;&#29702;&#30693;&#35782;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#31163;&#25955;&#21270;&#23616;&#37096;&#29305;&#24449;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#28041;&#21450;&#26681;&#25454;&#21382;&#21490;&#25968;&#25454;&#27969;&#29983;&#25104;&#26410;&#26469;&#35270;&#39057;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#35821;&#20041;&#22320;&#22270;&#31561;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#35270;&#39057;&#39044;&#27979;&#65292;&#20294;&#24120;&#24120;&#24573;&#35270;&#35270;&#39057;&#20869;&#22266;&#26377;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#21487;&#33021;&#20250;&#38459;&#30861;&#23545;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#30340;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#29289;&#29702;&#36741;&#21161;&#26102;&#31354;&#32593;&#32476;&#65288;PastNet&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;PastNet&#26680;&#24515;&#22312;&#20110;&#22312;&#20613;&#37324;&#21494;&#22495;&#20013;&#24341;&#20837;&#35889;&#21367;&#31215;&#31639;&#23376;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#24341;&#20837;&#22522;&#26412;&#29289;&#29702;&#23450;&#24459;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#20869;&#22312;&#32500;&#24230;&#20272;&#35745;&#30340;&#23384;&#20648;&#22120;&#24211;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#26102;&#31354;&#20449;&#21495;&#26102;&#31163;&#25955;&#21270;&#23616;&#37096;&#29305;&#24449;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the challenge of spatio-temporal video prediction, which involves generating future videos based on historical data streams. Existing approaches typically utilize external information such as semantic maps to enhance video prediction, which often neglect the inherent physical knowledge embedded within videos. Furthermore, their high computational demands could impede their applications for high-resolution videos. To address these constraints, we introduce a novel approach called Physics-assisted Spatio-temporal Network (PastNet) for generating high-quality video predictions. The core of our PastNet lies in incorporating a spectral convolution operator in the Fourier domain, which efficiently introduces inductive biases from the underlying physical laws. Additionally, we employ a memory bank with the estimated intrinsic dimensionality to discretize local features during the processing of complex spatio-temporal signals, thereby reducing computational costs 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#27867;&#21270;&#30340;&#28145;&#24230;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#65292;&#36890;&#36807;&#21152;&#20837;&#28508;&#22312;&#21464;&#37327;&#26469;&#35757;&#32451;&#27867;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11389</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#27867;&#21270;&#30340;&#28145;&#24230;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization Deep Graph Transformation. (arXiv:2305.11389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#27867;&#21270;&#30340;&#28145;&#24230;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#65292;&#36890;&#36807;&#21152;&#20837;&#28508;&#22312;&#21464;&#37327;&#26469;&#35757;&#32451;&#27867;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22270;&#24418;&#20174;&#19968;&#31181;&#27169;&#24335;&#36716;&#21464;&#20026;&#21478;&#19968;&#31181;&#27169;&#24335;&#30340;&#22270;&#24418;&#36716;&#25442;&#26159;&#19968;&#20010;&#37325;&#35201;&#21644;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#34429;&#28982;&#22312;&#24320;&#21457;&#20808;&#36827;&#30340;&#22270;&#24418;&#36716;&#25442;&#25216;&#26415;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22810;&#36827;&#23637;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#27979;&#35797;&#21644;&#35757;&#32451;&#25968;&#25454;&#20445;&#25345;&#30456;&#21516;&#30340;&#20998;&#24067;&#65292;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#39044;&#27979;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21487;&#29992;&#30340;&#22270;&#24418;&#30340;&#39046;&#22495;&#27867;&#21270;&#22270;&#24418;&#36716;&#25442;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#38656;&#35201;&#35299;&#20915;&#22810;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#65288;1&#65289;&#24403;&#35757;&#32451;&#25152;&#26377;&#36755;&#20837;-&#36755;&#20986;&#27169;&#24335;&#32452;&#21512;&#26102;&#30340;&#26497;&#31471;&#31354;&#38388;&#22797;&#26434;&#24230;&#12289;&#65288;2&#65289;&#36755;&#20837;&#21644;&#36755;&#20986;&#27169;&#24335;&#20043;&#38388;&#30340;&#22270;&#24418;&#25299;&#25169;&#24046;&#24322;&#65292;&#20197;&#21450;(3)&#22914;&#20309;&#23558;&#27169;&#22411;&#27867;&#21270;&#21040;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#65288;&#26410;&#35265;&#36807;&#30340;&#65289;&#30446;&#26631;&#22495;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#36755;&#20837;-&#22810;&#36755;&#20986;&#12289;&#36229;&#32593;&#32476;&#30340;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;&#65288;MultiHyperGNN&#65289;&#65292;&#23427;&#21033;&#29992;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#32534;&#30721;&#36755;&#20837;&#21644;&#36755;&#20986;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#36755;&#20986;&#22270;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#20837;&#25429;&#25417;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#26469;&#20197;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformation that predicts graph transition from one mode to another is an important and common problem. Despite much progress in developing advanced graph transformation techniques in recent years, the fundamental assumption typically required in machine-learning models that the testing and training data preserve the same distribution does not always hold. As a result, domain generalization graph transformation that predicts graphs not available in the training data is under-explored, with multiple key challenges to be addressed including (1) the extreme space complexity when training on all input-output mode combinations, (2) difference of graph topologies between the input and the output modes, and (3) how to generalize the model to (unseen) target domains that are not in the training data. To fill the gap, we propose a multi-input, multi-output, hypernetwork-based graph neural network (MultiHyperGNN) that employs a encoder and a decoder to encode topologies of both input an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Instruct2Act&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#22810;&#27169;&#24577;&#25351;&#20196;&#26144;&#23556;&#21040;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#65292;&#23558;&#22797;&#26434;&#30340;&#39640;&#32423;&#25351;&#20196;&#36716;&#25442;&#20026;&#31934;&#30830;&#30340;&#31574;&#30053;&#20195;&#30721;&#65292;&#35813;&#26041;&#27861;&#21487;&#35843;&#25972;&#21644;&#28789;&#27963;&#36866;&#24212;&#21508;&#31181;&#25351;&#20196;&#27169;&#24577;&#21644;&#36755;&#20837;&#31867;&#22411;&#65292;&#24182;&#28385;&#36275;&#29305;&#23450;&#30340;&#20219;&#21153;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.11176</link><description>&lt;p&gt;
Instruct2Act&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#22810;&#27169;&#24577;&#25351;&#20196;&#26144;&#23556;&#21040;&#26426;&#22120;&#20154;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model. (arXiv:2305.11176v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Instruct2Act&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#22810;&#27169;&#24577;&#25351;&#20196;&#26144;&#23556;&#21040;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#65292;&#23558;&#22797;&#26434;&#30340;&#39640;&#32423;&#25351;&#20196;&#36716;&#25442;&#20026;&#31934;&#30830;&#30340;&#31574;&#30053;&#20195;&#30721;&#65292;&#35813;&#26041;&#27861;&#21487;&#35843;&#25972;&#21644;&#28789;&#27963;&#36866;&#24212;&#21508;&#31181;&#25351;&#20196;&#27169;&#24577;&#21644;&#36755;&#20837;&#31867;&#22411;&#65292;&#24182;&#28385;&#36275;&#29305;&#23450;&#30340;&#20219;&#21153;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12289;&#20840;&#26223;&#20998;&#21106;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Instruct2Act&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#22810;&#27169;&#24577;&#25351;&#20196;&#26144;&#23556;&#21040;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;Instruct2Act&#21033;&#29992;LLM&#27169;&#22411;&#29983;&#25104;Python&#31243;&#24207;&#65292;&#26500;&#25104;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#20840;&#38754;&#24863;&#30693;&#12289;&#35268;&#21010;&#21644;&#21160;&#20316;&#24490;&#29615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#65292;&#23558;&#22797;&#26434;&#30340;&#39640;&#32423;&#25351;&#20196;&#36716;&#25442;&#20026;&#31934;&#30830;&#30340;&#31574;&#30053;&#20195;&#30721;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35843;&#25972;&#21644;&#28789;&#27963;&#36866;&#24212;&#21508;&#31181;&#25351;&#20196;&#27169;&#24577;&#21644;&#36755;&#20837;&#31867;&#22411;&#65292;&#24182;&#28385;&#36275;&#29305;&#23450;&#30340;&#20219;&#21153;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models have made significant strides in various applications, including text-to-image generation, panoptic segmentation, and natural language processing. This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks. Specifically, Instruct2Act employs the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks. In the perception section, pre-defined APIs are used to access multiple foundation models where the Segment Anything Model (SAM) accurately locates candidate objects, and CLIP classifies them. In this way, the framework leverages the expertise of foundation models and robotic abilities to convert complex high-level instructions into precise policy codes. Our approach is adjustable and flexible in accommodating various instruction modalities and input types and catering to specific task demands. W
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#65292;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#26174;&#33879;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.11169</link><description>&lt;p&gt;
&#22312;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#35821;&#20041;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Evidence of Meaning in Language Models Trained on Programs. (arXiv:2305.11169v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#65292;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#26174;&#33879;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23613;&#31649;&#34987;&#35757;&#32451;&#21482;&#26159;&#25191;&#34892;&#25991;&#26412;&#19978;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#31243;&#24207;&#35821;&#26009;&#24211;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#27599;&#20010;&#31243;&#24207;&#37117;&#20197;&#65288;&#25991;&#26412;&#65289;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#30340;&#24418;&#24335;&#20316;&#20026;&#35268;&#33539;&#12290;&#19982;&#31243;&#24207;&#19968;&#36215;&#24037;&#20316;&#20351;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#23450;&#20041;&#19982;&#35821;&#35328;&#20013;&#26377;&#20851;&#21547;&#20041;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#27491;&#30830;&#24615;&#21644;&#35821;&#20041;&#65289;&#65292;&#20351;&#24471;&#31243;&#24207;&#32508;&#21512;&#25104;&#20026;&#19968;&#20010;&#20013;&#38388;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#34920;&#24449;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21547;&#20041;&#30340;&#23384;&#22312;&#65288;&#25110;&#19981;&#23384;&#22312;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;Transformer&#27169;&#22411;&#65292;&#28982;&#21518;&#25506;&#26597;&#20102;&#24050;&#32463;&#23436;&#25104;&#35268;&#33539;&#30340;&#31243;&#24207;&#26102;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#23613;&#31649;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#24403;&#21069;&#21644;&#26410;&#26469;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#12290;&#27492;&#22806;&#65292;&#32447;&#24615;&#25506;&#27979;&#22120;&#30340;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#24378;&#26377;&#21147;&#12289;&#32479;&#35745;&#23398;&#26174;&#33879;&#22320;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present evidence that language models can learn meaning despite being trained only to perform next token prediction on text, specifically a corpus of programs. Each program is preceded by a specification in the form of (textual) input-output examples. Working with programs enables us to precisely define concepts relevant to meaning in language (e.g., correctness and semantics), making program synthesis well-suited as an intermediate testbed for characterizing the presence (or absence) of meaning in language models.  We first train a Transformer model on the corpus of programs, then probe the trained model's hidden states as it completes a program given a specification. Despite providing no inductive bias toward learning the semantics of the language, we find that a linear probe is able to extract abstractions of both current and future program states from the model states. Moreover, there is a strong, statistically significant correlation between the accuracy of the probe and the mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10626</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#19990;&#30028;&#27169;&#22411;&#65306;&#23454;&#20307;&#32463;&#39564;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LMs) &#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#22312;&#22788;&#29702;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#31616;&#21333;&#25512;&#29702;&#21644;&#35268;&#21010;&#38382;&#39064;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#20363;&#22914;&#29702;&#35299;&#29289;&#20307;&#27704;&#24658;&#25110;&#35268;&#21010;&#23478;&#24237;&#27963;&#21160;&#12290;&#36825;&#31181;&#38480;&#21046;&#28304;&#20110; LM &#20165;&#21463;&#20070;&#38754;&#35821;&#35328;&#35757;&#32451;&#65292;&#32570;&#23569;&#24517;&#35201;&#30340;&#23454;&#20307;&#30693;&#35782;&#21644;&#25216;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378; LM &#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#19968;&#33324;&#35821;&#35328;&#33021;&#21147;&#12290;&#26412;&#26041;&#27861;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#37096;&#32626;&#19968;&#20010;&#34701;&#20837;&#23454;&#20307;&#32463;&#39564;&#30340;&#20195;&#29702;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#30340;&#20223;&#30495;&#22120;(VirtualHome)&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#32463;&#39564;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#32463;&#39564;&#24494;&#35843; LM &#65292;&#20197;&#25945;&#25480;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#20363;&#22914;&#35268;&#21010;&#21644;&#23436;&#25104;&#30446;&#26631;&#12289;&#29289;&#20307;&#27704;&#24658;&#21644;&#36319;&#36394;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20197;&#21033;&#29992;&#20854;&#20182;&#27169;&#25311;&#22120;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#30528;&#25552;&#39640;&#20102; LM &#22312;&#19968;&#31995;&#21015;&#29289;&#29702;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#30041;&#24182;&#32463;&#24120;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10361</link><description>&lt;p&gt;
&#38750;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#65306;&#22522;&#20110;&#27169;&#25311;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Human Choice Prediction in Non-Cooperative Games: Simulation-based Off-Policy Evaluation. (arXiv:2305.10361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#26381;&#28216;&#25103;&#22312;&#32463;&#27982;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;&#35821;&#35328;&#30340;&#35828;&#26381;&#28216;&#25103;&#20013;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#20154;&#31867; - &#26426;&#22120;&#20154;&#20132;&#20114;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#30495;&#23454;&#20132;&#20114;&#21644;&#27169;&#25311;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persuasion games have been fundamental in economics and AI research, and have significant practical applications. Recent works in this area have started to incorporate natural language, moving beyond the traditional stylized message setting. However, previous research has focused on on-policy prediction, where the train and test data have the same distribution, which is not representative of real-life scenarios. In this paper, we tackle the challenging problem of off-policy evaluation (OPE) in language-based persuasion games. To address the inherent difficulty of human data collection in this setup, we propose a novel approach which combines real and simulated human-bot interaction data. Our simulated data is created by an exogenous model assuming decision makers (DMs) start with a mixture of random and decision-theoretic based behaviors and improve over time. We present a deep learning training algorithm that effectively integrates real interaction and simulated data, substantially im
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#22522;&#20110;&#21442;&#25968;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#65288;PEFT&#65289;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#21487;&#33021;&#24212;&#29992;&#12290;&#36890;&#36807;&#36229;&#36807;600&#20010;&#25511;&#21046;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PEFT&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#24182;&#24378;&#35843;&#20102;PEFT&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#36716;&#31227;&#23398;&#20064;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.08252</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#25968;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#34987;&#24573;&#35270;&#30340;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning for Medical Image Analysis: The Missed Opportunity. (arXiv:2305.08252v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#22522;&#20110;&#21442;&#25968;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#65288;PEFT&#65289;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#21487;&#33021;&#24212;&#29992;&#12290;&#36890;&#36807;&#36229;&#36807;600&#20010;&#25511;&#21046;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PEFT&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#24182;&#24378;&#35843;&#20102;PEFT&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#36716;&#31227;&#23398;&#20064;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22522;&#20110;&#21442;&#25968;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#65288;PEFT&#65289;&#22312;&#22810;&#26679;&#21270;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#12290;PEFT&#36234;&#26469;&#36234;&#34987;&#29992;&#20316;&#30693;&#35782;&#36716;&#31227;&#30340;&#26377;&#20215;&#20540;&#26041;&#27861;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35270;&#35273;&#12289;&#35821;&#38899;&#20197;&#21450;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#20363;&#22914;&#35270;&#35273;&#35821;&#35328;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#23427;&#30340;&#24212;&#29992;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#36234;&#26469;&#36234;&#34987;&#21033;&#29992;&#65292;&#35843;&#26597;&#21644;&#27604;&#36739;&#35780;&#20272;&#21508;&#31181;&#30693;&#35782;&#36716;&#31227;&#31574;&#30053;&#21487;&#20197;&#22686;&#24378;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#20854;&#31867;&#21035;&#20013;&#31532;&#19968;&#20010;&#65288;&#25454;&#25105;&#20204;&#25152;&#30693;&#65289;&#65292;&#35780;&#20272;&#20102;16&#31181;&#21367;&#31215;&#21644;&#22522;&#20110;&#36716;&#25442;&#22120;&#32593;&#32476;&#30340;PEFT&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20845;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#22823;&#23567;&#65292;&#27169;&#24577;&#21644;&#22797;&#26434;&#24615;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#36229;&#36807;600&#20010;&#25511;&#21046;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#36884;&#24452;&#65292;&#24182;&#24378;&#35843;&#20102;PEFT&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#36716;&#31227;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#24182;&#21628;&#21505;&#22312;&#26410;&#26469;&#30740;&#31350;&#20013;&#21152;&#20197;&#24191;&#27867;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive evaluation of Parameter-Efficient Fine-Tuning (PEFT) techniques for diverse medical image analysis tasks. PEFT is increasingly exploited as a valuable approach for knowledge transfer from pre-trained models in natural language processing, vision, speech, and cross-modal tasks, such as vision-language and text-to-image generation. However, its application in medical image analysis remains relatively unexplored. As foundation models are increasingly exploited in the medical domain, it is crucial to investigate and comparatively assess various strategies for knowledge transfer that can bolster a range of downstream tasks. Our study, the first of its kind (to the best of our knowledge), evaluates 16 distinct PEFT methodologies proposed for convolutional and transformer-based networks, focusing on image classification and text-to-image generation tasks across six medical datasets ranging in size, modality, and complexity. Through a battery of more than 600 control
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06626</link><description>&lt;p&gt;
&#24403;&#22810;&#25968;&#20154;&#26159;&#38169;&#35823;&#30340;&#65306;&#21033;&#29992;&#26631;&#27880;&#32773;&#19981;&#19968;&#33268;&#24615;&#36827;&#34892;&#20027;&#35266;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#34429;&#28982;&#36890;&#24120;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#22810;&#25968;&#25237;&#31080;&#26469;&#30830;&#23450;&#26631;&#31614;&#65292;&#20294;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#65292;&#26631;&#27880;&#32773;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#21487;&#33021;&#21453;&#26144;&#20986;&#32676;&#20307;&#35266;&#28857;&#30340;&#24046;&#24322;&#65292;&#32780;&#19981;&#26159;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#19968;&#20010;&#35821;&#21477;&#26159;&#21542;&#20882;&#29359;&#20102;&#23427;&#25152;&#38024;&#23545;&#30340;&#20154;&#32676;&#65292;&#32780;&#36825;&#21487;&#33021;&#21482;&#21344;&#26631;&#27880;&#32773;&#27744;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#39044;&#27979;&#21487;&#33021;&#20855;&#26377;&#20882;&#29359;&#24615;&#25991;&#26412;&#19978;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30340;&#39044;&#27979;&#30446;&#26631;&#32676;&#20307;&#26469;&#27169;&#25311;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#25552;&#39640;&#20102;22&#65285;&#22312;&#39044;&#27979;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#19978;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;33&#65285;&#22312;&#39044;&#27979;&#26631;&#27880;&#32773;&#20043;&#38388;&#26041;&#24046;&#19978;&#30340;&#24615;&#33021;&#65292;&#36825;&#25552;&#20379;&#20102;&#19979;&#28216;&#29992;&#26469;&#34913;&#37327;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#20854;&#22312;&#32447;&#24847;&#35265;&#26469;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though majority vote among annotators is typically used for ground truth labels in natural language processing, annotator disagreement in tasks such as hate speech detection may reflect differences among group opinions, not noise. Thus, a crucial problem in hate speech detection is whether a statement is offensive to the demographic group that it targets, which may constitute a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to model the opinions of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22% at predicting individual annotators' ratings and 33% at predicting variance among annotators, which provides a method of measuring model uncertainty downstream. We find that annotators' ratings can be predicted using their demographic information and opinions on online 
&lt;/p&gt;</description></item><item><title>TDC'22&#26159;&#31532;&#19968;&#23626;&#38754;&#21521;ICDs&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#21019;&#26032;&#31454;&#36187;&#12290;&#26412;&#27425;&#31454;&#36187;&#30340;&#25361;&#25112;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.05105</link><description>&lt;p&gt;
&#38754;&#21521;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#24494;&#23567;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
TinyML Design Contest for Life-Threatening Ventricular Arrhythmia Detection. (arXiv:2305.05105v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05105
&lt;/p&gt;
&lt;p&gt;
TDC'22&#26159;&#31532;&#19968;&#23626;&#38754;&#21521;ICDs&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#21019;&#26032;&#31454;&#36187;&#12290;&#26412;&#27425;&#31454;&#36187;&#30340;&#25361;&#25112;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#23626;ACM/IEEE&#24494;&#23567;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#31454;&#36187;&#65288;TDC&#65289;&#20110;2022&#24180;&#22312;&#31532;41&#23626;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#22269;&#38469;&#20250;&#35758;&#65288;ICCAD&#65289;&#19978;&#20030;&#34892;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26376;&#30740;&#21457;&#31454;&#36187;&#12290;TDC'22&#19987;&#27880;&#20110;&#38656;&#35201;&#22312;&#21487;&#26893;&#20837;&#35774;&#22791;&#19978;&#21019;&#26032;&#21644;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#30340;&#30495;&#23454;&#21307;&#30103;&#38382;&#39064;&#12290;TDC'22&#30340;&#25361;&#25112;&#38382;&#39064;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#24515;&#33039;&#38500;&#39076;&#22120;&#65288;ICDs&#65289;&#19978;&#20351;&#29992;&#30340;&#20302;&#21151;&#29575;&#24494;&#25511;&#21046;&#22120;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;90&#20010;&#21463;&#35797;&#32773;&#30340;8&#31181;&#19981;&#21516;&#24515;&#24459;&#31867;&#22411;&#30340;&#36229;&#36807;38,000&#20010;5&#31186;&#24515;&#20869;&#30005;&#22270;&#65288;IEGM&#65289;&#29255;&#27573;&#12290;&#19987;&#29992;&#30828;&#20214;&#24179;&#21488;&#26159;STMicroelectronics&#21046;&#36896;&#30340;NUCLEO-L432KC&#12290;TDC'22&#38754;&#21521;&#20840;&#29699;&#22810;&#20154;&#22242;&#38431;&#65292;&#21560;&#24341;&#20102;&#26469;&#33258;50&#22810;&#20010;&#32452;&#32455;&#30340;150&#22810;&#25903;&#38431;&#20237;&#21442;&#36187;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#36825;&#19968;&#21307;&#30103;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
The first ACM/IEEE TinyML Design Contest (TDC) held at the 41st International Conference on Computer-Aided Design (ICCAD) in 2022 is a challenging, multi-month, research and development competition. TDC'22 focuses on real-world medical problems that require the innovation and implementation of artificial intelligence/machine learning (AI/ML) algorithms on implantable devices. The challenge problem of TDC'22 is to develop a novel AI/ML-based real-time detection algorithm for life-threatening ventricular arrhythmia over low-power microcontrollers utilized in Implantable Cardioverter-Defibrillators (ICDs). The dataset contains more than 38,000 5-second intracardiac electrograms (IEGMs) segments over 8 different types of rhythm from 90 subjects. The dedicated hardware platform is NUCLEO-L432KC manufactured by STMicroelectronics. TDC'22, which is open to multi-person teams world-wide, attracted more than 150 teams from over 50 organizations. This paper first presents the medical problem, da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#31181;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#20154;&#31867;&#36816;&#21160;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#24314;&#27169;&#20154;&#20307;&#36816;&#21160;&#30340;&#31185;&#23398;&#25361;&#25112;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#33021;&#20934;&#30830;&#39044;&#27979;&#20154;&#20307;&#36816;&#21160;&#24182;&#29983;&#25104;&#26032;&#30340;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2304.14502</link><description>&lt;p&gt;
&#29992;&#20110;&#21487;&#35299;&#37322;&#30340;&#23039;&#21183;&#34920;&#36798;&#12289;&#20998;&#26512;&#21644;&#29983;&#25104;&#30340;&#28145;&#23618;&#29366;&#24577;&#31354;&#38388;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Deep state-space modeling for explainable representation, analysis, and generation of professional human poses. (arXiv:2304.14502v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#31181;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#20154;&#31867;&#36816;&#21160;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#24314;&#27169;&#20154;&#20307;&#36816;&#21160;&#30340;&#31185;&#23398;&#25361;&#25112;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#33021;&#20934;&#30830;&#39044;&#27979;&#20154;&#20307;&#36816;&#21160;&#24182;&#29983;&#25104;&#26032;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20154;&#20307;&#21160;&#20316;&#30340;&#20998;&#26512;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21069;&#27839;&#20173;&#28982;&#38754;&#20020;&#24314;&#27169;&#20154;&#20307;&#36816;&#21160;&#30340;&#31185;&#23398;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#38656;&#35201;&#26032;&#30340;&#27169;&#22411;&#26469;&#32771;&#34385;&#20154;&#31867;&#36816;&#21160;&#30340;&#38543;&#26426;&#24615;&#21644;&#20154;&#20307;&#30340;&#29289;&#29702;&#32467;&#26500;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#20840;&#36523;&#36816;&#21160;&#25551;&#36848;&#31526;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#29983;&#25104;&#20154;&#31867;&#36816;&#21160;&#26102;&#65292;&#23545;&#20110;&#20854;&#36523;&#20307;&#23039;&#21183;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#38656;&#25913;&#36827;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#20154;&#31867;&#36816;&#21160;&#30340;&#21487;&#29702;&#35299;&#34920;&#31034;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19977;&#31181;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#20154;&#31867;&#36816;&#21160;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20840;&#36523;&#36816;&#21160;&#34987;&#20844;&#24335;&#21270;&#20026;&#21160;&#24577;&#31995;&#32479;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#20854;&#21442;&#25968;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#32479;&#35745;&#31639;&#27861;&#36827;&#34892;&#20272;&#35745;&#12290;&#36825;&#20123;&#34920;&#31034;&#36981;&#24490;&#25903;&#25345;&#20154;&#31867;&#36816;&#21160;&#30340;&#32467;&#26500;&#21270;&#21407;&#21017;&#65292;&#24182;&#19988;&#30001;&#29992;&#25143;&#21487;&#20197;&#29702;&#35299;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#20154;&#20307;&#36816;&#21160;&#24182;&#29983;&#25104;&#26032;&#30340;&#36816;&#21160;&#65292;&#21516;&#26102;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#36523;&#20307;&#23039;&#21183;&#39044;&#27979;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of human movements has been extensively studied due to its wide variety of practical applications. Nevertheless, the state-of-the-art still faces scientific challenges while modeling human movements. Firstly, new models that account for the stochasticity of human movement and the physical structure of the human body are required to accurately predict the evolution of full-body motion descriptors over time. Secondly, the explainability of existing deep learning algorithms regarding their body posture predictions while generating human movements still needs to be improved as they lack comprehensible representations of human movement. This paper addresses these challenges by introducing three novel approaches for creating explainable representations of human movement. In this work, full-body movement is formulated as a state-space model of a dynamic system whose parameters are estimated using deep learning and statistical algorithms. The representations adhere to the structur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#22833;&#35823;&#65292;&#24402;&#32435;&#24182;&#30830;&#23450;&#20854;&#22833;&#36133;&#30340;&#21407;&#22240;&#31867;&#22411;&#21644;&#20851;&#38190;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#28508;&#22312;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10513</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;ChatGPT&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Does ChatGPT Fall Short in Answering Questions Faithfully?. (arXiv:2304.10513v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10513
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#22833;&#35823;&#65292;&#24402;&#32435;&#24182;&#30830;&#23450;&#20854;&#22833;&#36133;&#30340;&#21407;&#22240;&#31867;&#22411;&#21644;&#20851;&#38190;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#28508;&#22312;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;ChatGPT&#65292;&#23637;&#31034;&#20986;&#23545;&#20154;&#31867;&#29983;&#27963;&#21508;&#26041;&#38754;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;ChatGPT&#22312;&#35802;&#23454;&#24615;&#31561;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#38382;&#31572;&#31995;&#32479;&#20026;&#20195;&#34920;&#24212;&#29992;&#31243;&#24207;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;&#20026;&#20160;&#20040;ChatGPT&#22312;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#26377;&#25152;&#19981;&#36275;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35797;&#22270;&#20998;&#26512;ChatGPT&#22312;&#22797;&#26434;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#22833;&#36133;&#30340;&#21407;&#22240;&#65292;&#24182;&#30830;&#23450;&#19982;&#36825;&#20123;&#22833;&#36133;&#26377;&#20851;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;ChatGPT&#30340;&#22833;&#36133;&#24402;&#20026;&#22235;&#31181;&#31867;&#22411;&#65306;&#29702;&#35299;&#12289;&#20107;&#23454;&#24615;&#12289;&#20855;&#20307;&#24615;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#19982;QA&#22833;&#36133;&#26377;&#20851;&#30340;&#19977;&#20010;&#20851;&#38190;&#33021;&#21147;&#65306;&#30693;&#35782;&#35760;&#24518;&#12289;&#30693;&#35782;&#20851;&#32852;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22260;&#32469;&#36825;&#20123;&#33021;&#21147;&#30340;&#23454;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21521;&#27169;&#22411;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#22806;&#37096;&#30693;&#35782;&#12289;&#32473;&#20104;&#25552;&#31034;&#26469;&#24110;&#21161;&#23427;&#32858;&#28966;&#24182;&#21152;&#24378;&#20851;&#38190;&#33021;&#21147;&#65292;&#36825;&#37117;&#26377;&#21161;&#20110;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models, such as ChatGPT, have demonstrated significant potential to impact various aspects of human life. However, ChatGPT still faces challenges in aspects like faithfulness. Taking question answering as a representative application, we seek to understand why ChatGPT falls short in answering questions faithfully. To address this question, we attempt to analyze the failures of ChatGPT in complex open-domain question answering and identifies the abilities under the failures. Specifically, we categorize ChatGPT's failures into four types: comprehension, factualness, specificity, and inference. We further pinpoint three critical abilities associated with QA failures: knowledge memorization, knowledge association, and knowledge reasoning. Additionally, we conduct experiments centered on these abilities and propose potential approaches to enhance faithfulness. The results indicate that furnishing the model with fine-grained external knowledge, hints for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07063</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Existential First Order Queries Inference on Knowledge Graphs. (arXiv:2304.07063v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;&#29305;&#21035;&#22320;&#65292;&#22238;&#31572;&#19968;&#38454;&#36923;&#36753;&#20844;&#24335;&#26159;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#28165;&#26224;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#32452;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#23558;&#36923;&#36753;&#36816;&#31639;&#35270;&#20026;&#38598;&#21512;&#36816;&#31639;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#30740;&#31350;&#36981;&#24490;&#30456;&#21516;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#32570;&#20047;&#20174;&#36923;&#36753;&#35282;&#24230;&#36827;&#34892;&#31995;&#32479;&#26816;&#26597;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20808;&#21069;&#30740;&#31350;&#35843;&#26597;&#30340;&#26597;&#35810;&#33539;&#22260;&#65292;&#24182;&#20934;&#30830;&#22320;&#30830;&#23450;&#20102;&#23427;&#19982;&#25972;&#20010;&#23384;&#22312;&#24615;&#20844;&#24335;&#23478;&#26063;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#26032;&#20844;&#24335;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#35752;&#35770;&#20102;&#21516;&#26102;&#20986;&#29616;&#30340;&#26032;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning on knowledge graphs is a challenging task because it utilizes observed information to predict the missing one. Specifically, answering first-order logic formulas is of particular interest because of its clear syntax and semantics. Recently, the query embedding method has been proposed which learns the embedding of a set of entities and treats logic operations as set operations. Though there has been much research following the same methodology, it lacks a systematic inspection from the standpoint of logic. In this paper, we characterize the scope of queries investigated previously and precisely identify the gap between it and the whole family of existential formulas. Moreover, we develop a new dataset containing ten new formulas and discuss the new challenges coming simultaneously. Finally, we propose a new search algorithm from fuzzy logic theory which is capable of solving new formulas and outperforming the previous methods in existing formulas.
&lt;/p&gt;</description></item><item><title>Video ChatCaptioner&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#21644;&#31639;&#27861;&#29983;&#25104;&#20840;&#38754;&#21644;&#20016;&#23500;&#30340;&#26102;&#31354;&#35270;&#39057;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.04227</link><description>&lt;p&gt;
&#35270;&#39057;&#32842;&#22825;&#23383;&#24149;&#29983;&#25104;&#22120;&#65306; &#36808;&#21521;&#20016;&#23500;&#26102;&#31354;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Video ChatCaptioner: Towards the Enriched Spatiotemporal Descriptions. (arXiv:2304.04227v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04227
&lt;/p&gt;
&lt;p&gt;
Video ChatCaptioner&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#21644;&#31639;&#27861;&#29983;&#25104;&#20840;&#38754;&#21644;&#20016;&#23500;&#30340;&#26102;&#31354;&#35270;&#39057;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#30340;&#30446;&#30340;&#26159;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20256;&#36798;&#35270;&#39057;&#20013;&#30340;&#21160;&#24577;&#22330;&#26223;&#65292;&#20419;&#36827;&#25105;&#20204;&#23545;&#29615;&#22659;&#20013;&#26102;&#31354;&#20449;&#24687;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#29983;&#25104;&#32454;&#33268;&#21644;&#20016;&#23500;&#30340;&#35270;&#39057;&#25551;&#36848;&#20173;&#28982;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;Video ChatCaptioner&#65292;&#29992;&#20110;&#21019;&#24314;&#26356;&#20840;&#38754;&#30340;&#26102;&#31354;&#35270;&#39057;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992; ChatGPT &#27169;&#22411;&#20316;&#20026;&#25511;&#21046;&#22120;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36873;&#25321;&#26694;&#26550;&#20197;&#25552;&#20986;&#35270;&#39057;&#20869;&#23481;&#39537;&#21160;&#30340;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#31639;&#27861;&#22238;&#31572;&#36825;&#20123;&#35270;&#35273;&#26597;&#35810;&#12290;&#36825;&#31181;&#38382;&#31572;&#26694;&#26550;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#22797;&#26434;&#30340;&#35270;&#39057;&#32454;&#33410;&#65292;&#24182;&#26174;&#31034;&#20986;&#22686;&#24378;&#35270;&#39057;&#20869;&#23481;&#30340;&#26041;&#27861;&#30340;&#21069;&#36884;&#12290;&#22312;&#22810;&#20010;&#23545;&#35805;&#36718;&#27425;&#20043;&#21518;&#65292;ChatGPT &#21487;&#20197;&#26681;&#25454;&#20043;&#21069;&#30340;&#23545;&#35805;&#24635;&#32467;&#20016;&#23500;&#30340;&#35270;&#39057;&#20869;&#23481;&#12290;&#25105;&#20204;&#23450;&#24615;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340; Video ChatCaptioner &#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#26356;&#22810;&#32454;&#33410;&#30340;&#35270;&#39057;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video captioning aims to convey dynamic scenes from videos using natural language, facilitating the understanding of spatiotemporal information within our environment. Although there have been recent advances, generating detailed and enriched video descriptions continues to be a substantial challenge. In this work, we introduce Video ChatCaptioner, an innovative approach for creating more comprehensive spatiotemporal video descriptions. Our method employs a ChatGPT model as a controller, specifically designed to select frames for posing video content-driven questions. Subsequently, a robust algorithm is utilized to answer these visual queries. This question-answer framework effectively uncovers intricate video details and shows promise as a method for enhancing video content. Following multiple conversational rounds, ChatGPT can summarize enriched video content based on previous conversations. We qualitatively demonstrate that our Video ChatCaptioner can generate captions containing mo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#19982;&#33258;&#36523;&#23545;&#35805;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#36718;&#32842;&#22825;&#35821;&#26009;&#24211;&#65292;&#24182;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26469;&#22686;&#24378;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;Baize&#65292;&#22312;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#30340;&#25252;&#26639;&#19979;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01196</link><description>&lt;p&gt;
Baize:&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#23545;&#35805;&#25968;&#25454;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#24320;&#28304;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. (arXiv:2304.01196v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01196
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#19982;&#33258;&#36523;&#23545;&#35805;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#36718;&#32842;&#22825;&#35821;&#26009;&#24211;&#65292;&#24182;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26469;&#22686;&#24378;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;Baize&#65292;&#22312;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#30340;&#25252;&#26639;&#19979;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#27169;&#22411;&#65292;&#22914;ChatGPT&#65292;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20247;&#22810;&#39046;&#22495;&#24471;&#21040;&#36805;&#36895;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#21482;&#33021;&#36890;&#36807;&#21463;&#38480;&#21046;&#30340;API&#36827;&#34892;&#35775;&#38382;&#65292;&#20174;&#32780;&#21046;&#36896;&#20102;&#26032;&#30340;&#30740;&#31350;&#21644;&#39046;&#22495;&#36827;&#23637;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#19982;&#33258;&#36523;&#23545;&#35805;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#36718;&#32842;&#22825;&#35821;&#26009;&#24211;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26469;&#22686;&#24378;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#12290;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;Baize&#65292;&#22312;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#30340;&#25252;&#26639;&#19979;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;Baize&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#20165;&#29992;&#20110;&#30740;&#31350;&#30446;&#30340;&#65292;&#21487;&#22312;https://github.com/project-baize/baize&#36827;&#34892;&#19979;&#36733;&#12290;&#22312;&#32447;&#28436;&#31034;&#20063;&#21487;&#22312;https://huggingface.co/spaces/project-baize/baize-lora-7B&#36827;&#34892;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize. An online demo is also available at https://huggingface.co/spaces/project-baize/baize-lora-7B.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GPTEval&#65292;&#19968;&#20010;&#21033;&#29992;&#38142;&#24335;&#24605;&#32771;&#21644;&#24418;&#24335;&#22635;&#20805;&#35780;&#20215;NLG&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;GPTEval&#32467;&#21512;GPT-4&#21462;&#24471;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16634</link><description>&lt;p&gt;
GPTEval&#65306;&#20351;&#29992;GPT-4&#21644;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#26469;&#35780;&#20272;NLG
&lt;/p&gt;
&lt;p&gt;
GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment. (arXiv:2303.16634v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GPTEval&#65292;&#19968;&#20010;&#21033;&#29992;&#38142;&#24335;&#24605;&#32771;&#21644;&#24418;&#24335;&#22635;&#20805;&#35780;&#20215;NLG&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;GPTEval&#32467;&#21512;GPT-4&#21462;&#24471;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31995;&#32479;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#24456;&#38590;&#36827;&#34892;&#33258;&#21160;&#27979;&#37327;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;BLEU&#21644;ROUGE&#24050;&#34987;&#35777;&#26126;&#22312;&#38656;&#35201;&#21019;&#36896;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#30456;&#23545;&#36739;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#26080;&#21442;&#32771;&#30340;NLG&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#20123;&#27169;&#22411;&#36866;&#29992;&#20110;&#32570;&#20047;&#20154;&#31867;&#21442;&#32771;&#30340;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#20173;&#28982;&#27604;&#20013;&#31561;&#35268;&#27169;&#30340;&#31070;&#32463;&#35780;&#20272;&#22120;&#30340;&#20154;&#31867;&#23545;&#24212;&#24230;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPTEval&#65292;&#19968;&#20010;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#21644;&#24418;&#24335;&#22635;&#20805;&#33539;&#24335;&#26469;&#35780;&#20272;NLG&#36755;&#20986;&#36136;&#37327;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#20010;&#29983;&#25104;&#20219;&#21153;&#65292;&#25991;&#26412;&#25688;&#35201;&#21644;&#23545;&#35805;&#29983;&#25104;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;GPTEval&#32467;&#21512;GPT-4&#20316;&#20026;&#39592;&#24178;&#27169;&#22411;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present GPTEval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that GPTEval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#30340;&#26041;&#27861;&#65292;&#23427;&#30001;&#20116;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#28436;&#22863;&#21517;&#20026;&#8220;&#31354;&#20013;&#23567;&#22992;&#26354;&#8221;&#30340;&#21476;&#20856;&#38899;&#20048;&#32452;&#25104;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#20048;&#22120;&#30340;&#38899;&#37327;&#26469;&#36866;&#24212;&#28216;&#25103;&#30340;&#19981;&#21516;&#20803;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#21487;&#20197;&#25913;&#21892;&#28216;&#25103;&#30340;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.15734</link><description>&lt;p&gt;
&#19968;&#20010;&#22810;&#20048;&#22120;&#20307;&#31215;&#35843;&#21046;&#30340;&#26041;&#27861;&#22686;&#24378;&#26684;&#26007;&#28216;&#25103;&#20013;&#30340;&#32972;&#26223;&#38899;&#20048;&#65306;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;
&lt;/p&gt;
&lt;p&gt;
Adaptive Background Music for a Fighting Game: A Multi-Instrument Volume Modulation Approach. (arXiv:2303.15734v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#30340;&#26041;&#27861;&#65292;&#23427;&#30001;&#20116;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#28436;&#22863;&#21517;&#20026;&#8220;&#31354;&#20013;&#23567;&#22992;&#26354;&#8221;&#30340;&#21476;&#20856;&#38899;&#20048;&#32452;&#25104;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#20048;&#22120;&#30340;&#38899;&#37327;&#26469;&#36866;&#24212;&#28216;&#25103;&#30340;&#19981;&#21516;&#20803;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#21487;&#20197;&#25913;&#21892;&#28216;&#25103;&#30340;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312; DareFightingICE &#20013;&#28155;&#21152;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#20197;&#22686;&#24378;&#28216;&#25103;&#20307;&#39564;&#30340;&#24037;&#20316;&#12290;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#30001;&#20116;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#28436;&#22863;&#21517;&#20026;&#8220;&#31354;&#20013;&#23567;&#22992;&#26354;&#8221;&#30340;&#21476;&#20856;&#38899;&#20048;&#32452;&#25104;&#65292;&#36890;&#36807;&#25913;&#21464;&#20048;&#22120;&#30340;&#38899;&#37327;&#26469;&#36866;&#24212;&#28216;&#25103;&#30340;&#19981;&#21516;&#20803;&#32032;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#26469;&#35780;&#20272;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#38899;&#39057;&#20316;&#20026;&#36755;&#20837;&#30340;&#28145;&#24230;&#22686;&#24378;&#23398;&#20064; AI&#65288;Blind DL AI&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#27809;&#26377;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#26102;&#30456;&#27604;&#65292;Blind DL AI &#22312;&#19982;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#19968;&#36215;&#25773;&#25918;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our work to enhance the background music (BGM) in DareFightingICE by adding an adaptive BGM. The adaptive BGM consists of five different instruments playing a classical music piece called "Air on G-String." The BGM adapts by changing the volume of the instruments. Each instrument is connected to a different element of the game. We then run experiments to evaluate the adaptive BGM by using a deep reinforcement learning AI that only uses audio as input (Blind DL AI). The results show that the performance of the Blind DL AI improves while playing with the adaptive BGM as compared to playing without the adaptive BGM.
&lt;/p&gt;</description></item><item><title>KeGNN&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#32467;&#21512;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#20248;&#21270;&#22270;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.15487</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Knowledge Enhanced Graph Neural Networks. (arXiv:2303.15487v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15487
&lt;/p&gt;
&lt;p&gt;
KeGNN&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#32467;&#21512;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#20248;&#21270;&#22270;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#33258;&#28982;&#31185;&#23398;&#12289;&#31038;&#20132;&#32593;&#32476;&#25110;&#35821;&#20041;&#32593;&#12290;&#23613;&#31649;&#23500;&#21547;&#20449;&#24687;&#65292;&#20294;&#22270;&#24418;&#36890;&#24120;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#12290;&#22240;&#27492;&#65292;&#22270;&#34917;&#20840;&#20219;&#21153;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#65292;&#24050;&#32463;&#21463;&#21040;&#20851;&#27880;&#12290;&#19968;&#26041;&#38754;&#65292;&#31070;&#32463;&#26041;&#27861;&#65288;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#22122;&#22768;&#22270;&#30340;&#31283;&#20581;&#24037;&#20855;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#31526;&#21495;&#26041;&#27861;&#21487;&#20197;&#23545;&#22270;&#36827;&#34892;&#31934;&#30830;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KeGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#22270;&#25968;&#25454;&#19978;&#23398;&#20064;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#33539;&#20363;&#65292;&#24182;&#20801;&#35768;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#12290;&#20174;&#26412;&#36136;&#19978;&#35762;&#65292;KeGNN&#30001;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#20854;&#20013;&#22522;&#20110;&#30446;&#26631;&#23558;&#30693;&#35782;&#22686;&#24378;&#23618;&#22534;&#21472;&#22312;&#20854;&#19978;&#65292;&#20197;&#20351;&#38024;&#23545;&#20808;&#21069;&#30693;&#35782;&#30340;&#39044;&#27979;&#24471;&#21040;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;KeGNN&#19982;&#20004;&#20010;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#19968;&#36215;&#23454;&#20363;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph data is omnipresent and has a large variety of applications such as natural science, social networks or semantic web. Though rich in information, graphs are often noisy and incomplete. Therefore, graph completion tasks such as node classification or link prediction have gained attention. On the one hand, neural methods such as graph neural networks have proven to be robust tools for learning rich representations of noisy graphs. On the other hand, symbolic methods enable exact reasoning on graphs. We propose KeGNN, a neuro-symbolic framework for learning on graph data that combines both paradigms and allows for the integration of prior knowledge into a graph neural network model. In essence, KeGNN consists of a graph neural network as a base on which knowledge enhancement layers are stacked with the objective of refining predictions with respect to prior knowledge. We instantiate KeGNN in conjunction with two standard graph neural networks: Graph Convolutional Networks and Graph 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21512;&#20316;&#20219;&#21153;&#20998;&#35299;&#19982;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#22870;&#21169;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20063;&#38477;&#20302;&#20102;&#21512;&#20316;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14061</link><description>&lt;p&gt;
&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#20013;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Reward Machines in Cooperative Multi-Agent Tasks. (arXiv:2303.14061v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21512;&#20316;&#20219;&#21153;&#20998;&#35299;&#19982;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#22870;&#21169;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20063;&#38477;&#20302;&#20102;&#21512;&#20316;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21512;&#20316;&#20219;&#21153;&#20998;&#35299;&#19982;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#20197;&#32534;&#30721;&#23376;&#20219;&#21153;&#30340;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#24212;&#23545;&#37096;&#20998;&#35266;&#27979;&#29615;&#22659;&#20013;&#22870;&#21169;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65292;&#24182;&#25552;&#39640;&#25152;&#23398;&#20064;&#30340;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#23436;&#25104;&#21512;&#20316;&#20219;&#21153;&#12290;&#19982;&#27599;&#20010;&#23376;&#20219;&#21153;&#30456;&#20851;&#32852;&#30340;&#22870;&#21169;&#26426;&#21046;&#26159;&#20197;&#20998;&#25955;&#30340;&#26041;&#24335;&#23398;&#20064;&#30340;&#65292;&#28982;&#21518;&#29992;&#20110;&#25351;&#23548;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#21512;&#20316;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#24471;&#21040;&#20102;&#38477;&#20302;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26410;&#26469;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#26377; promising &#30340;&#26041;&#21521;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#22810;&#20010;&#26234;&#33021;&#20307;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to Multi-Agent Reinforcement Learning (MARL) that combines cooperative task decomposition with the learning of reward machines (RMs) encoding the structure of the sub-tasks. The proposed method helps deal with the non-Markovian nature of the rewards in partially observable environments and improves the interpretability of the learnt policies required to complete the cooperative task. The RMs associated with each sub-task are learnt in a decentralised manner and then used to guide the behaviour of each agent. By doing so, the complexity of a cooperative multi-agent problem is reduced, allowing for more effective learning. The results suggest that our approach is a promising direction for future research in MARL, especially in complex environments with large state spaces and multiple agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#33258;&#30001;&#24352;&#37327;&#30340;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#27450;&#39575;&#22312;&#32447;&#32763;&#35793;&#24037;&#20855;&#20135;&#29983;&#38169;&#35823;&#30340;&#32763;&#35793;&#65292;&#36825;&#24433;&#21709;&#20102;&#29992;&#25143;&#23398;&#20064;&#35821;&#35328;&#30340;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.10974</link><description>&lt;p&gt;
&#23558;&#24744;&#30340;&#26080;&#24847;&#20041;&#32763;&#35793;&#65306;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Translate your gibberish: black-box adversarial attack on machine translation systems. (arXiv:2303.10974v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#33258;&#30001;&#24352;&#37327;&#30340;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#27450;&#39575;&#22312;&#32447;&#32763;&#35793;&#24037;&#20855;&#20135;&#29983;&#38169;&#35823;&#30340;&#32763;&#35793;&#65292;&#36825;&#24433;&#21709;&#20102;&#29992;&#25143;&#23398;&#20064;&#35821;&#35328;&#30340;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24191;&#27867;&#37096;&#32626;&#20110;&#24037;&#19994;&#23610;&#24230;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#26368;&#24120;&#35265;&#30340;&#29992;&#36884;&#26159;&#33258;&#21160;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#27450;&#39575;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#32763;&#35793;&#24037;&#20855;&#22312;&#20420;&#35821;&#21644;&#33521;&#35821;&#20043;&#38388;&#30340;&#32763;&#35793;&#20219;&#21153;&#20013;&#12290;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#33258;&#30001;&#24352;&#37327;&#30340;&#20248;&#21270;&#22120;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#22312;&#32447;&#32763;&#35793;&#24037;&#20855;&#65292;&#22914; Google&#12289;DeepL &#21644; Yandex&#65292;&#37117;&#21487;&#33021;&#20026;&#26080;&#24847;&#20041;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#26597;&#35810;&#20135;&#29983;&#38169;&#35823;&#25110;&#20882;&#29359;&#24615;&#32763;&#35793;&#65292;&#24182;&#25298;&#32477;&#32763;&#35793;&#30475;&#20284;&#33391;&#24615;&#30340;&#36755;&#20837;&#30701;&#35821;&#12290;&#36825;&#31181;&#28431;&#27934;&#21487;&#33021;&#20250;&#24178;&#25200;&#23545;&#19968;&#31181;&#26032;&#35821;&#35328;&#30340;&#29702;&#35299;&#65292;&#19988;&#22312;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#26102;&#31616;&#21333;&#22320;&#24694;&#21270;&#29992;&#25143;&#30340;&#20307;&#39564;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#36825;&#20123;&#24037;&#20855;&#36827;&#34892;&#39069;&#22806;&#30340;&#25913;&#36827;&#20197;&#24314;&#31435;&#26356;&#22909;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are deployed widely in natural language processing tasks on the industrial scale, and perhaps the most often they are used as compounds of automatic machine translation systems. In this work, we present a simple approach to fool state-of-the-art machine translation tools in the task of translation from Russian to English and vice versa. Using a novel black-box gradient-free tensor-based optimizer, we show that many online translation tools, such as Google, DeepL, and Yandex, may both produce wrong or offensive translations for nonsensical adversarial input queries and refuse to translate seemingly benign input phrases. This vulnerability may interfere with understanding a new language and simply worsen the user's experience while using machine translation systems, and, hence, additional improvements of these tools are required to establish better translation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#65292;&#29992;&#20110;&#23450;&#20041;DL&#65292;&#35813;&#20803;&#35821;&#35328;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10650</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#36923;&#36753;&#30340;&#36923;&#36753;&#65306;&#36208;&#21521;DL&#30340;&#32479;&#19968;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Logic of Differentiable Logics: Towards a Uniform Semantics of DL. (arXiv:2303.10650v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10650
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#65292;&#29992;&#20110;&#23450;&#20041;DL&#65292;&#35813;&#20803;&#35821;&#35328;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#21487;&#24494;&#20998;&#36923;&#36753;&#65288;DL&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#28385;&#36275;&#36923;&#36753;&#35268;&#33539;&#30340;&#26041;&#27861;&#12290;DL&#21253;&#25324;&#35821;&#27861;&#21644;&#23558;&#35821;&#27861;&#20013;&#30340;&#34920;&#36798;&#24335;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#30340;&#35299;&#37322;&#20989;&#25968;&#12290;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19982;&#26631;&#20934;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290; &#29616;&#26377;DL&#30340;&#22810;&#26679;&#24615;&#21644;&#23545;&#20854;&#24418;&#24335;&#21270;&#31243;&#24230;&#30340;&#19981;&#21516;&#22788;&#29702;&#20351;&#24471;&#23545;&#23427;&#20204;&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#21464;&#24471;&#22256;&#38590;&#12290;&#35813;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#20316;&#20026;DL&#23450;&#20041;&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable logics (DL) have recently been proposed as a method of training neural networks to satisfy logical specifications. A DL consists of a syntax in which specifications are stated and an interpretation function that translates expressions in the syntax into loss functions. These loss functions can then be used during training with standard gradient descent algorithms. The variety of existing DLs and the differing levels of formality with which they are treated makes a systematic comparative study of their properties and implementations difficult. This paper remedies this problem by suggesting a meta-language for defining DLs that we call the Logic of Differentiable Logics, or LDL. Syntactically, it generalises the syntax of existing DLs to FOL, and for the first time introduces the formalism for reasoning about vectors and learners. Semantically, it introduces a general interpretation function that can be instantiated to define loss functions arising from different existing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.09026</link><description>&lt;p&gt;
&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#36164;&#28304;&#21463;&#38480;&#21644;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Commonsense Knowledge Assisted Deep Learning for Resource-constrained and Fine-grained Object Detection. (arXiv:2303.09026v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#36793;&#32536;&#35745;&#31639;&#31561;&#36164;&#28304;&#21463;&#38480;&#22330;&#26223;&#19979;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#12290;&#38024;&#23545;&#20351;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#22120;&#26102;&#38656;&#35201;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#21644;&#22823;&#37327;&#25968;&#25454;&#26631;&#27880;&#30340;&#31934;&#20934;&#32454;&#31890;&#24230;&#26816;&#27979;&#38656;&#27714;&#65292;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#31895;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#22120;&#33719;&#21462;&#31934;&#20934;&#30340;&#32454;&#31890;&#24230;&#26816;&#27979;&#32467;&#26524;&#12290;&#24341;&#20837;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;(CKIM)&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#29983;&#25104;&#32454;&#31890;&#24230;&#26631;&#31614;&#12290;&#35770;&#25991;&#20013;&#32771;&#34385;&#20102;&#27169;&#31946;&#35268;&#21017;&#21644;&#28165;&#26224;&#35268;&#21017;&#30340;&#25512;&#29702;&#65292;&#21069;&#32773;&#29992;&#20110;&#22788;&#29702;&#30446;&#26631;&#35821;&#20041;&#26631;&#31614;&#30340;&#27169;&#31946;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider fine-grained image object detection in resource-constrained cases such as edge computing. Deep learning (DL), namely learning with deep neural networks (DNNs), has become the dominating approach to object detection. To achieve accurate fine-grained detection, one needs to employ a large enough DNN model and a vast amount of data annotations, which brings a challenge for using modern DL object detectors in resource-constrained cases. To this end, we propose an approach, which leverages commonsense knowledge to assist a coarse-grained object detector to get accurate fine-grained detection results. Specifically, we introduce a commonsense knowledge inference module (CKIM) to process coarse-grained lables given by a benchmark DL detector to produce fine-grained lables. We consider both crisp-rule and fuzzy-rule based inference in our CKIM; the latter is used to handle ambiguity in the target semantic labels. We implement our method based on several modern DL dete
&lt;/p&gt;</description></item><item><title>FairShap&#26159;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#37325;&#20272;&#35745;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;Shapley&#20540;&#20272;&#20540;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#31934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26131;&#20110;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.01928</link><description>&lt;p&gt;
&#22522;&#20110;Shapley&#20540;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#25968;&#25454;&#20877;&#21152;&#26435;&#26041;&#27861;FairShap
&lt;/p&gt;
&lt;p&gt;
FairShap: A Data Re-weighting Approach for Algorithmic Fairness based on Shapley Values. (arXiv:2303.01928v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01928
&lt;/p&gt;
&lt;p&gt;
FairShap&#26159;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#37325;&#20272;&#35745;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;Shapley&#20540;&#20272;&#20540;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#31934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26131;&#20110;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#26159;&#26497;&#20854;&#37325;&#35201;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#28982;&#32780;&#24403;&#21069;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36235;&#21183;&#35201;&#27714;&#20351;&#29992;&#36890;&#24120;&#23384;&#22312;&#20559;&#24046;&#30340;&#28023;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19987;&#27880;&#20110;&#24314;&#27169;&#21644;&#32416;&#27491;&#25968;&#25454;&#20559;&#24046;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#25104;&#20026;&#26377;&#20215;&#20540;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Shapley&#20540;&#36827;&#34892;&#25968;&#25454;&#20272;&#20540;&#30340;&#39044;&#22788;&#29702;&#65288;&#20877;&#21152;&#26435;&#65289;&#26041;&#27861;FairShap&#65292;&#29992;&#20110;&#20844;&#24179;&#30340;&#31639;&#27861;&#20915;&#31574;&#21046;&#23450;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#19988;&#26131;&#20110;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#34913;&#37327;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#23545;&#39044;&#23450;&#20041;&#30340;&#20844;&#24179;&#25351;&#26631;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#21516;&#30340;&#24615;&#36136;&#65292;&#26377;&#21508;&#31181;&#22521;&#35757;&#22330;&#26223;&#21644;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20135;&#29983;&#26356;&#20844;&#24179;&#30340;&#27169;&#22411;&#24182;&#19988;&#20934;&#30830;&#24230;&#26356;&#39640;&#25110;&#30456;&#20284;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#30452;&#26041;&#22270;&#21644;&#28508;&#31354;&#38388;&#21487;&#35270;&#21270;&#26469;&#35828;&#26126;FairShap&#30340;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#23545;&#20110;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#30830;&#20445;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic fairness is of utmost societal importance, yet the current trend in large-scale machine learning models requires training with massive datasets that are typically biased. In this context, pre-processing methods that focus on modeling and correcting bias in the data emerge as valuable approaches. In this paper, we propose FairShap, a novel pre-processing (re-weighting) method for fair algorithmic decision-making through data valuation by means of Shapley Values. Our approach is model agnostic and easily interpretable, as it measures the contribution of each training data point to a predefined fairness metric. We empirically validate FairShap on several state-of-the-art datasets of different nature, with a variety of training scenarios and models and show how it outperforms other methods, yielding fairer models with higher or similar levels of accuracy. We also illustrate FairShap's interpretability by means of histograms and latent space visualizations. We believe that this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#24433;&#21709;&#30446;&#26631;&#23567;&#21306;&#21644;&#21608;&#22260;&#23567;&#21306;&#24615;&#33021;&#30340;&#23567;&#21306;&#21442;&#25968;&#36827;&#34892;&#26234;&#33021;&#35843;&#33410;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20010;&#20849;&#20139;&#36890;&#29992;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#24182;&#22312;&#31163;&#32447;&#23398;&#20064;&#38454;&#27573;&#23545;&#26234;&#33021;&#20307;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26368;&#32456;&#23558;&#32593;&#32476;&#24341;&#23548;&#21040;&#26368;&#20339;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2302.12899</link><description>&lt;p&gt;
&#20855;&#26377;&#20849;&#21516;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#22825;&#32447;&#20542;&#26012;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning with Common Policy for Antenna Tilt Optimization. (arXiv:2302.12899v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#24433;&#21709;&#30446;&#26631;&#23567;&#21306;&#21644;&#21608;&#22260;&#23567;&#21306;&#24615;&#33021;&#30340;&#23567;&#21306;&#21442;&#25968;&#36827;&#34892;&#26234;&#33021;&#35843;&#33410;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20010;&#20849;&#20139;&#36890;&#29992;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#24182;&#22312;&#31163;&#32447;&#23398;&#20064;&#38454;&#27573;&#23545;&#26234;&#33021;&#20307;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26368;&#32456;&#23558;&#32593;&#32476;&#24341;&#23548;&#21040;&#26368;&#20339;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#24433;&#21709;&#30446;&#26631;&#23567;&#21306;&#21644;&#21608;&#22260;&#23567;&#21306;&#24615;&#33021;&#30340;&#23567;&#21306;&#21442;&#25968;&#26469;&#20248;&#21270;&#26080;&#32447;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20849;&#20139;&#19968;&#31181;&#36890;&#29992;&#31574;&#30053;&#65292;&#24182;&#32771;&#34385;&#20102;&#37051;&#36817;&#23567;&#21306;&#30340;&#20449;&#24687;&#26469;&#30830;&#23450;&#29366;&#24577;&#21644;&#22870;&#21169;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#23398;&#20064;&#30340;&#21021;&#26399;&#24433;&#21709;&#32593;&#32476;&#24615;&#33021;&#65292;&#26234;&#33021;&#20307;&#22312;&#31163;&#32447;&#23398;&#20064;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#27492;&#38454;&#27573;&#65292;&#36890;&#36807;&#38745;&#24577;&#32593;&#32476;&#27169;&#25311;&#22120;&#30340;&#21453;&#39304;&#21644;&#32771;&#34385;&#21508;&#31181;&#24773;&#20917;&#65292;&#33719;&#24471;&#19968;&#20010;&#21021;&#22987;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#36890;&#36807;&#24314;&#35758;&#23567;&#24133;&#22686;&#37327;&#21464;&#21270;&#26469;&#26234;&#33021;&#35843;&#33410;&#27979;&#35797;&#32593;&#32476;&#30340;&#23567;&#21306;&#21442;&#25968;&#65292;&#24182;&#32531;&#24930;&#22320;&#24341;&#23548;&#32593;&#32476;&#26397;&#21521;&#26368;&#20339;&#37197;&#32622;&#12290;&#26234;&#33021;&#20307;&#25552;&#20986;&#26368;&#20339;&#21464;&#21270;&#30340;&#24314;&#35758;&#26159;&#21033;&#29992;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#33719;&#24471;&#30340;&#27169;&#25311;&#22120;&#32463;&#39564;&#65292;&#20294;&#23427;&#20204;&#20063;&#21487;&#20197;&#32487;&#32493;&#20174;&#24403;&#21069;&#32593;&#32476;&#35835;&#25968;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for optimizing wireless networks by adjusting cell parameters that affect both the performance of the cell being optimized and the surrounding cells. The method uses multiple reinforcement learning agents that share a common policy and take into account information from neighboring cells to determine the state and reward. In order to avoid impairing network performance during the initial stages of learning, agents are pre-trained in an earlier phase of offline learning. During this phase, an initial policy is obtained using feedback from a static network simulator and considering a wide variety of scenarios. Finally, agents can intelligently tune the cell parameters of a test network by suggesting small incremental changes, slowly guiding the network toward an optimal configuration. The agents propose optimal changes using the experience gained with the simulator in the pre-training phase, but they can also continue to learn from current network readings af
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#20027;&#23548;&#33322;&#39046;&#22495;&#20013;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#38556;&#30861;&#26816;&#27979;&#12289;&#22330;&#26223;&#24863;&#30693;&#12289;&#36335;&#24452;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#31361;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#24037;&#31243;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20197;&#21450;&#21019;&#26032;&#23548;&#33322;&#26041;&#27861;&#30340;&#24320;&#21457;&#21644;&#36328;&#23398;&#31185;&#30740;&#31350;&#30340;&#37325;&#35201;&#24615;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.11089</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#20027;&#23548;&#33322;&#20013;&#30340;&#24212;&#29992;&#19982;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;--&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Recent Advancements in Deep Learning Applications and Methods for Autonomous Navigation -- A Comprehensive Review. (arXiv:2302.11089v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#20027;&#23548;&#33322;&#39046;&#22495;&#20013;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#38556;&#30861;&#26816;&#27979;&#12289;&#22330;&#26223;&#24863;&#30693;&#12289;&#36335;&#24452;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#31361;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#24037;&#31243;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20197;&#21450;&#21019;&#26032;&#23548;&#33322;&#26041;&#27861;&#30340;&#24320;&#21457;&#21644;&#36328;&#23398;&#31185;&#30740;&#31350;&#30340;&#37325;&#35201;&#24615;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#22312;&#33258;&#20027;&#23548;&#33322;&#32972;&#26223;&#19979;&#20351;&#29992;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#38556;&#30861;&#29289;&#26816;&#27979;&#12289;&#22330;&#26223;&#24863;&#30693;&#12289;&#36335;&#24452;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#26368;&#36817;&#30340;&#30740;&#31350;&#25104;&#26524;&#24182;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#23454;&#26045;&#21644;&#27979;&#35797;&#26469;&#24357;&#21512;&#33258;&#20027;&#23548;&#33322;&#21644;&#28145;&#24230;&#23398;&#20064;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#37325;&#28857;&#24378;&#35843;&#20102;&#23548;&#33322;&#23545;&#20110;&#31227;&#21160;&#26426;&#22120;&#20154;&#12289;&#33258;&#20027;&#36710;&#36742;&#21644;&#26080;&#20154;&#26426;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#20063;&#35748;&#35782;&#21040;&#30001;&#20110;&#29615;&#22659;&#22797;&#26434;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#12289;&#38556;&#30861;&#29289;&#12289;&#21160;&#24577;&#29615;&#22659;&#20197;&#21450;&#35268;&#21010;&#22810;&#20010;&#26234;&#33021;&#20307;&#30340;&#36335;&#24452;&#30340;&#38656;&#35201;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#32508;&#36848;&#31361;&#20986;&#28145;&#24230;&#23398;&#20064;&#22312;&#24037;&#31243;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24555;&#36895;&#21457;&#23637;&#20197;&#21450;&#20854;&#21019;&#26032;&#23548;&#33322;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#35752;&#35770;&#20102;&#19982;&#26412;&#39046;&#22495;&#30456;&#20851;&#30340;&#36817;&#26399;&#36328;&#23398;&#31185;&#30740;&#31350;&#65292;&#24182;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#23616;&#38480;&#24615;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#22686;&#38271;&#39046;&#22495;&#25552;&#20986;&#20102;&#31616;&#35201;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
This review paper presents a comprehensive overview of end-to-end deep learning frameworks used in the context of autonomous navigation, including obstacle detection, scene perception, path planning, and control. The paper aims to bridge the gap between autonomous navigation and deep learning by analyzing recent research studies and evaluating the implementation and testing of deep learning methods. It emphasizes the importance of navigation for mobile robots, autonomous vehicles, and unmanned aerial vehicles, while also acknowledging the challenges due to environmental complexity, uncertainty, obstacles, dynamic environments, and the need to plan paths for multiple agents. The review highlights the rapid growth of deep learning in engineering data science and its development of innovative navigation methods. It discusses recent interdisciplinary work related to this field and provides a brief perspective on the limitations, challenges, and potential areas of growth for deep learning m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23545;&#35805;&#24335;&#30340;&#35299;&#37322;&#27169;&#22411;&#65292;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#21019;&#24314;&#26356;&#26377;&#25928;&#21644;&#24191;&#27867;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2302.03460</link><description>&lt;p&gt;
&#27880;&#24847;&#30041;&#19979;&#31354;&#38553;&#65281;&#29992;&#40065;&#26364;&#21151;&#33021;&#29702;&#35770;&#26500;&#24314;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
Mind the Gap! Bridging Explainable Artificial Intelligence and Human Understanding with Luhmann's Functional Theory of Communication. (arXiv:2302.03460v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23545;&#35805;&#24335;&#30340;&#35299;&#37322;&#27169;&#22411;&#65292;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#21019;&#24314;&#26356;&#26377;&#25928;&#21644;&#24191;&#27867;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#24050;&#20174;&#19968;&#31181;&#20027;&#35201;&#30340;&#25216;&#26415;&#23398;&#31185;&#21457;&#23637;&#25104;&#19982;&#31038;&#20250;&#31185;&#23398;&#32039;&#23494;&#30456;&#20132;&#30340;&#39046;&#22495;&#12290;&#20154;&#31867;&#20559;&#22909;&#23545;&#27604;&#30340;&#35299;&#37322;&#65292;&#30830;&#20999;&#32780;&#35328;&#26159;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#65292;&#23545;&#20110;&#36825;&#31181;&#36716;&#21464;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21551;&#21457;&#21644;&#24341;&#39046;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20854;&#20182;&#21516;&#26679;&#37325;&#35201;&#30340;&#35266;&#23519;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#20154;&#31867;&#35299;&#37322;&#32773;&#24076;&#26395;&#36890;&#36807;&#23545;&#35805;&#24335;&#30340;&#20132;&#20114;&#19982;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#32773;&#36827;&#34892;&#20132;&#27969;&#30340;&#24895;&#26395;&#22312;&#31038;&#21306;&#20013;&#22522;&#26412;&#34987;&#24573;&#35270;&#12290;&#36825;&#32473;&#36825;&#31181;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#21644;&#24191;&#27867;&#24212;&#29992;&#24102;&#26469;&#20102;&#24456;&#22810;&#25361;&#25112;&#65292;&#22240;&#20026;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#25552;&#20379;&#21333;&#19968;&#30340;&#20248;&#21270;&#35299;&#37322;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#24182;&#19988;&#19981;&#33021;&#28385;&#36275;&#20854;&#25509;&#25910;&#32773;&#30340;&#29420;&#29305;&#38656;&#27714;&#65292;&#37492;&#20110;&#20154;&#31867;&#30693;&#35782;&#21644;&#24847;&#22270;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#21033;&#29992;&#23612;&#20811;&#25289;&#26031;&#183;&#40065;&#26364;&#21644;&#20854;&#20182;&#20132;&#27969;&#23398;&#32773;&#38416;&#36848;&#30340;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;&#21521;&#26356;&#23545;&#35805;&#24335;&#30340;&#35299;&#37322;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#20854;&#20013;&#35299;&#37322;&#32773;&#21644;&#34987;&#35299;&#37322;&#32773;&#20043;&#38388;&#30340;&#20449;&#24687;&#25345;&#32493;&#20132;&#27969;&#26159;&#26680;&#24515;&#12290;&#36890;&#36807;&#36825;&#31181;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#24314;&#31435;&#26356;&#26377;&#25928;&#21644;&#24191;&#27867;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade explainable artificial intelligence has evolved from a predominantly technical discipline into a field that is deeply intertwined with social sciences. Insights such as human preference for contrastive -- more precisely, counterfactual -- explanations have played a major role in this transition, inspiring and guiding the research in computer science. Other observations, while equally important, have received much less attention. The desire of human explainees to communicate with artificial intelligence explainers through a dialogue-like interaction has been mostly neglected by the community. This poses many challenges for the effectiveness and widespread adoption of such technologies as delivering a single explanation optimised according to some predefined objectives may fail to engender understanding in its recipients and satisfy their unique needs given the diversity of human knowledge and intention. Using insights elaborated by Niklas Luhmann and, more recently,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2302.01735</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65306;&#26041;&#24046;&#32553;&#20943;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective. (arXiv:2302.01735v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. The concept of variance-reduced estimation is used to build ARCO, and certain variance-reduction techniques are shown to be particularly beneficial in medical image segmentation.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23545;&#27604;&#23398;&#20064;&#26159;&#25552;&#39640;&#35270;&#35273;&#34920;&#31034;&#36136;&#37327;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#26679;&#26412;&#23545;&#26469;&#23454;&#29616;&#12290;&#36825;&#26159;&#36890;&#36807;&#35266;&#23519;&#21040;&#65292;&#22312;&#27809;&#26377;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#37319;&#26679;&#20855;&#26377;&#30495;&#27491;&#19981;&#21516;&#35299;&#21078;&#29305;&#24449;&#30340;&#36127;&#26679;&#26412;&#65292;&#21017;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#26469;&#33258;&#30456;&#20284;&#30340;&#35299;&#21078;&#29305;&#24449;&#65292;&#27169;&#22411;&#21487;&#33021;&#38590;&#20197;&#21306;&#20998;&#23569;&#25968;&#23614;&#31867;&#26679;&#26412;&#65292;&#20351;&#24471;&#23614;&#31867;&#26356;&#23481;&#26131;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
For medical image segmentation, contrastive learning is the dominant practice to improve the quality of visual representations by contrasting semantically similar and dissimilar pairs of samples. This is enabled by the observation that without accessing ground truth label, negative examples with truly dissimilar anatomical features, if sampled, can significantly improve the performance. In reality, however, these samples may come from similar anatomical features and the models may struggle to distinguish the minority tail-class samples, making the tail classes more prone to misclassification, both of which typically lead to model collapse. In this paper, we propose ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. In particular, we first propose building ARCO through the concept of variance-reduced estimation, and show that certain variance-reduction techniques are particularly beneficial in medical image se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36861;&#38543;&#32773;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26031;&#22612;&#20811;&#36125;&#26684;&#21338;&#24328;&#20013;&#30340;&#38382;&#39064;&#65292;&#19982;&#20854;&#20182;&#29616;&#26377;&#20316;&#21697;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#20351;&#29992;&#39044;&#20272;&#22120;&#25110;&#20102;&#35299;&#36861;&#38543;&#32773;&#30340;&#31574;&#30053;&#12289;&#25928;&#29992;&#20989;&#25968;&#12290;&#20316;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#23618;&#24490;&#29615;&#31639;&#27861;&#65292;&#36890;&#36807;&#29305;&#21035;&#35774;&#35745;&#30340;&#31574;&#30053;&#25506;&#27979;&#36861;&#38543;&#32773;&#26469;&#26356;&#26032;&#39046;&#23548;&#32773;&#30340;&#31574;&#30053;&#65292;&#21457;&#29616;&#36861;&#38543;&#32773;&#30340;&#32852;&#21512;&#31574;&#30053;&#25910;&#25947;&#20110;&#22343;&#34913;&#12290;&#38750;&#28176;&#36827;&#25910;&#25947;&#36895;&#24230;&#21040;&#39046;&#23548;&#32773;&#30446;&#26631;&#30340;&#23616;&#37096;&#31283;&#23450;&#28857;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23545;&#39046;&#23548;&#32773;&#30446;&#26631;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#28176;&#36827;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2302.01421</link><description>&lt;p&gt;
Follower Agnostic Methods for Stackelberg Games&#65288;&#36866;&#29992;&#20110;&#26031;&#22612;&#20811;&#36125;&#26684;&#21338;&#24328;&#30340;&#36861;&#38543;&#32773;&#19981;&#21487;&#30693;&#26041;&#27861;&#65289;
&lt;/p&gt;
&lt;p&gt;
Follower Agnostic Methods for Stackelberg Games. (arXiv:2302.01421v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01421
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36861;&#38543;&#32773;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26031;&#22612;&#20811;&#36125;&#26684;&#21338;&#24328;&#20013;&#30340;&#38382;&#39064;&#65292;&#19982;&#20854;&#20182;&#29616;&#26377;&#20316;&#21697;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#20351;&#29992;&#39044;&#20272;&#22120;&#25110;&#20102;&#35299;&#36861;&#38543;&#32773;&#30340;&#31574;&#30053;&#12289;&#25928;&#29992;&#20989;&#25968;&#12290;&#20316;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#23618;&#24490;&#29615;&#31639;&#27861;&#65292;&#36890;&#36807;&#29305;&#21035;&#35774;&#35745;&#30340;&#31574;&#30053;&#25506;&#27979;&#36861;&#38543;&#32773;&#26469;&#26356;&#26032;&#39046;&#23548;&#32773;&#30340;&#31574;&#30053;&#65292;&#21457;&#29616;&#36861;&#38543;&#32773;&#30340;&#32852;&#21512;&#31574;&#30053;&#25910;&#25947;&#20110;&#22343;&#34913;&#12290;&#38750;&#28176;&#36827;&#25910;&#25947;&#36895;&#24230;&#21040;&#39046;&#23548;&#32773;&#30446;&#26631;&#30340;&#23616;&#37096;&#31283;&#23450;&#28857;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23545;&#39046;&#23548;&#32773;&#30446;&#26631;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#28176;&#36827;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20197;&#36861;&#38543;&#32773;&#19981;&#21487;&#30693;&#30340;&#26041;&#24335;&#35299;&#20915;&#26031;&#22612;&#20811;&#36125;&#26684;&#21338;&#24328;&#30340;&#19968;&#31867;&#38382;&#39064;&#65288;&#21487;&#33021;&#26377;&#22810;&#20010;&#36861;&#38543;&#32773;&#65289;&#12290;&#19982;&#20854;&#20182;&#24403;&#20195;&#20316;&#21697;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#20351;&#29992;&#39046;&#23548;&#32773;&#30446;&#26631;&#30340;&#26799;&#24230;&#30340;&#39044;&#20272;&#22120;&#65292;&#20063;&#19981;&#38656;&#35201;&#20102;&#35299;&#36861;&#38543;&#32773;&#30340;&#25928;&#29992;&#20989;&#25968;&#25110;&#31574;&#30053;&#31354;&#38388;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#23618;&#24490;&#29615;&#31639;&#27861;&#65292;&#20854;&#20013;&#39046;&#23548;&#32773;&#20351;&#29992;&#36890;&#36807;&#29992;&#29305;&#21035;&#35774;&#35745;&#30340;&#31574;&#30053;&#25506;&#27979;&#36861;&#38543;&#32773;&#33719;&#24471;&#30340;&#29305;&#27530;&#26500;&#36896;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#22312;&#25509;&#25910;&#21040;&#36825;&#20123;&#31574;&#30053;&#21518;&#65292;&#36861;&#38543;&#32773;&#37319;&#29992;&#36866;&#24212;&#35268;&#21017;&#65292;&#20351;&#24471;&#36861;&#38543;&#32773;&#30340;&#32852;&#21512;&#31574;&#30053;&#25910;&#25947;&#20110;&#22343;&#34913;&#29366;&#24577;&#65292;&#36825;&#26159;&#39046;&#23548;&#32773;&#35266;&#23519;&#21040;&#30340;&#21807;&#19968;&#20449;&#24687;&#65292;&#29992;&#20110;&#26500;&#36896;&#21069;&#36848;&#26799;&#24230;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#38750;&#28176;&#36827;&#25910;&#25947;&#36895;&#24230;&#21040;&#39046;&#23548;&#32773;&#30446;&#26631;&#30340;&#23616;&#37096;&#31283;&#23450;&#28857;&#65292;&#23613;&#31649;&#38381;&#29615;&#20989;&#25968;&#19981;&#20855;&#26377;&#20984;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23545;&#39046;&#23548;&#32773;&#30446;&#26631;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#28176;&#36827;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an algorithm to solve a class of Stackelberg games (possibly with multiple followers) in a follower agnostic manner. Particularly, unlike other contemporary works, our algorithm does not require the use of an oracle estimator for the gradient of the leader's objective or knowledge about the follower's utility function or strategy space. Instead, we design two-loop algorithm where the leader updates its strategies using specially constructed gradient estimator obtained by probing followers with specially designed strategies. Upon receiving the followers engage in an adaptation rule such that the joint strategy of followers converges near equilibrium which is the only information observed by leader to construct the aforementioned gradient estimator. We provide non-asymptotic convergence rates to stationary points of the leader's objective in the absence of convexity of the closed-loop function and further show asymptotic convergence to a local minima of the leader's objective.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19978;&#19979;&#25991;&#20462;&#21098;&#20803;&#23398;&#20064;&#23454;&#29616;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;&#35757;&#32451;&#30340;&#20248;&#21270;&#65292; &#26174;&#33879;&#33410;&#30465;&#20869;&#23384;&#65292;&#24182;&#33021;&#22312;&#30701;&#26102;&#38388;&#20869;&#23398;&#20064;&#39640;&#36136;&#37327;&#31070;&#32463;&#22330;&#12290;</title><link>http://arxiv.org/abs/2302.00617</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#20462;&#21098;&#20803;&#23398;&#20064;&#23398;&#20064;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;
&lt;/p&gt;
&lt;p&gt;
Learning Large-scale Neural Fields via Context Pruned Meta-Learning. (arXiv:2302.00617v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00617
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#20462;&#21098;&#20803;&#23398;&#20064;&#23454;&#29616;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;&#35757;&#32451;&#30340;&#20248;&#21270;&#65292; &#26174;&#33879;&#33410;&#30465;&#20869;&#23384;&#65292;&#24182;&#33021;&#22312;&#30701;&#26102;&#38388;&#20869;&#23398;&#20064;&#39640;&#36136;&#37327;&#31070;&#32463;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#22312;&#32447;&#19978;&#19979;&#25991;&#28857;&#36873;&#25321;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;&#35757;&#32451;&#30340;&#39640;&#25928;&#20248;&#21270;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#32780;&#23454;&#29616;&#26174;&#33879;&#30340;&#20869;&#23384;&#33410;&#30465;&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#23398;&#20064;&#27493;&#39588;&#38598;&#20013;&#22312;&#20855;&#26377;&#26368;&#39640;&#26399;&#26395;&#31435;&#21363;&#27169;&#22411;&#36136;&#37327;&#25913;&#36827;&#30340;&#25968;&#25454;&#23376;&#38598;&#19978;&#65292;&#23454;&#29616;&#20840;&#23616;&#32467;&#26500;&#30340;&#20960;&#20046;&#21363;&#26102;&#24314;&#27169;&#21644;&#39640;&#39057;&#32454;&#33410;&#30340;&#21518;&#32493;&#32454;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#26657;&#27491;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20803;&#23398;&#20064;&#21021;&#22987;&#21270;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#20943;&#23569;&#19978;&#19979;&#25991;&#38598;&#26102;&#24341;&#20837;&#30340;&#20219;&#20309;&#35823;&#24046;&#30340;&#26368;&#23567;&#21270;&#65292;&#24182;&#21516;&#26102;&#32531;&#35299;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#25152;&#24102;&#26469;&#30340;&#30701;&#35270;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#20803;&#27979;&#35797;&#26102;&#36827;&#34892;&#26799;&#24230;&#37325;&#26032;&#32553;&#25918;&#65292;&#20174;&#32780;&#22312;&#26174;&#33879;&#32553;&#30701;&#20248;&#21270;&#36807;&#31243;&#30340;&#21516;&#26102;&#23398;&#20064;&#26497;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#22330;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#30452;&#35266;&#26131;&#25026;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#37325;&#26500;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an efficient optimization-based meta-learning technique for large-scale neural field training by realizing significant memory savings through automated online context point selection. This is achieved by focusing each learning step on the subset of data with the highest expected immediate improvement in model quality, resulting in the almost instantaneous modeling of global structure and subsequent refinement of high-frequency details. We further improve the quality of our meta-learned initialization by introducing a bootstrap correction resulting in the minimization of any error introduced by reduced context sets while simultaneously mitigating the well-known myopia of optimization-based meta-learning. Finally, we show how gradient re-scaling at meta-test time allows the learning of extremely high-quality neural fields in significantly shortened optimization procedures. Our framework is model-agnostic, intuitive, straightforward to implement, and shows significant reconst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#22270;&#23884;&#20837;&#23398;&#20064;&#65292;&#23427;&#23558;&#19968;&#32452;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#22270;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#20351;&#24471;&#22270;&#23884;&#20837;&#23398;&#20064;&#36866;&#24212;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#22270;&#35268;&#27169;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#24456;&#22810;&#25104;&#21151;&#65292;&#20294;&#26159;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20250;&#21463;&#21040;&#35745;&#31639;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2301.05860</link><description>&lt;p&gt;
&#22270;&#23884;&#20837;&#23398;&#20064;&#30340;&#29616;&#29366;&#21644;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
State of the Art and Potentialities of Graph-level Learning. (arXiv:2301.05860v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#22270;&#23884;&#20837;&#23398;&#20064;&#65292;&#23427;&#23558;&#19968;&#32452;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#22270;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#20351;&#24471;&#22270;&#23884;&#20837;&#23398;&#20064;&#36866;&#24212;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#22270;&#35268;&#27169;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#24456;&#22810;&#25104;&#21151;&#65292;&#20294;&#26159;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20250;&#21463;&#21040;&#35745;&#31639;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#29616;&#20102;&#20851;&#31995;&#25968;&#25454;&#30340;&#19968;&#31181;&#20248;&#36234;&#33021;&#21147;&#65292;&#22914;&#21270;&#21512;&#29289;&#12289;&#34507;&#30333;&#36136;&#21644;&#31038;&#20132;&#32593;&#32476;&#31561;&#12290;&#22240;&#27492;&#65292;&#22270;&#23884;&#20837;&#23398;&#20064;&#65292;&#23558;&#19968;&#32452;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#24050;&#24212;&#29992;&#20110;&#35768;&#22810;&#20219;&#21153;&#65292;&#21253;&#25324;&#27604;&#36739;&#12289;&#22238;&#24402;&#12289;&#20998;&#31867;&#31561;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#19968;&#32452;&#22270;&#30340;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#65292;&#22914;&#20122;&#32467;&#26500;&#12290;&#20294;&#36825;&#20123;&#26041;&#27861;&#34429;&#28982;&#21463;&#30410;&#20110;&#33391;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#24120;&#24120;&#22240;&#26080;&#27861;&#36991;&#20813;&#22270;&#21516;&#26500;&#38382;&#39064;&#32780;&#21463;&#21040;&#35745;&#31639;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#21644;&#23558;&#22270;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#65292;&#24110;&#21161;&#22270;&#23884;&#20837;&#23398;&#20064;&#36866;&#24212;&#26085;&#30410;&#22686;&#38271;&#30340;&#22270;&#35268;&#27169;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#28145;&#24230;&#22270;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#36896;&#25104;&#20102;&#35768;&#22810;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#20840;&#38754;&#30340;&#32508;&#36848;&#26469;&#22238;&#39038;&#20174;&#20256;&#32479;&#23398;&#20064;&#21040;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22270;&#23884;&#20837;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs have a superior ability to represent relational data, like chemical compounds, proteins, and social networks. Hence, graph-level learning, which takes a set of graphs as input, has been applied to many tasks including comparison, regression, classification, and more. Traditional approaches to learning a set of graphs heavily rely on hand-crafted features, such as substructures. But while these methods benefit from good interpretability, they often suffer from computational bottlenecks as they cannot skirt the graph isomorphism problem. Conversely, deep learning has helped graph-level learning adapt to the growing scale of graphs by extracting features automatically and encoding graphs into low-dimensional representations. As a result, these deep graph learning methods have been responsible for many successes. Yet, there is no comprehensive survey that reviews graph-level learning starting with traditional learning and moving through to the deep learning approaches. This article 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;"&#24320;&#25918;&#39046;&#22495;"&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#22120;&#22312;&#27492;&#24773;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#20005;&#37325;&#65292;&#20294;&#36827;&#34892;&#39069;&#22806;&#30340;&#24320;&#25918;&#39046;&#22495;&#30340;&#35757;&#32451;&#21487;&#20197;&#38477;&#20302;&#23545;&#19981;&#23436;&#32654;&#26816;&#32034;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10526</link><description>&lt;p&gt;
&#38754;&#21521;&#24320;&#25918;&#39046;&#22495;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Towards multi-document summarization in the open-domain. (arXiv:2212.10526v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;"&#24320;&#25918;&#39046;&#22495;"&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#22120;&#22312;&#27492;&#24773;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#20005;&#37325;&#65292;&#20294;&#36827;&#34892;&#39069;&#22806;&#30340;&#24320;&#25918;&#39046;&#22495;&#30340;&#35757;&#32451;&#21487;&#20197;&#38477;&#20302;&#23545;&#19981;&#23436;&#32654;&#26816;&#32034;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#36890;&#24120;&#20551;&#35774;&#25552;&#20379;&#19968;&#32452;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#20294;&#26159;&#65292;&#36825;&#20010;&#25991;&#26723;&#38598;&#36890;&#24120;&#26159;&#25968;&#25454;&#38598;&#31574;&#21010;&#36807;&#31243;&#30340;&#20135;&#29289;&#65307;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#19981;&#19968;&#23450;&#21487;&#29992;&#65292;&#38656;&#35201;&#26681;&#25454;&#20449;&#24687;&#38656;&#27714;&#65292;&#21363;&#38382;&#39064;&#25110;&#20027;&#39064;&#38472;&#36848;&#36827;&#34892;&#26816;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#24418;&#24335;&#21270;&#20219;&#21153;&#24182;&#20351;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#26816;&#32034;&#22120;&#21644;&#25688;&#35201;&#22120;&#26469;&#24341;&#23548;&#36825;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#8220;&#24320;&#25918;&#39046;&#22495;&#8221;&#35774;&#32622;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#65306;(1)&#21363;&#20351;&#26816;&#32034;&#24615;&#33021;&#36739;&#39640;&#65292;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#22120;&#22312;&#24212;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#26102;&#20063;&#20250;&#22823;&#24133;&#38477;&#20302;&#24615;&#33021;;(2)&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#21487;&#20197;&#38477;&#20302;&#23545;&#19981;&#23436;&#32654;&#26816;&#32034;&#30340;&#25935;&#24863;&#24615;&#65292;(3)&#25688;&#35201;&#22120;&#23545;&#26816;&#32034;&#37325;&#22797;&#25991;&#26723;&#21644;&#26816;&#32034;&#25991;&#26723;&#30340;&#39034;&#24207;&#19981;&#25935;&#24863;&#65292;&#20294;&#23545;&#20854;&#20182;&#38169;&#35823;&#65292;&#22914;&#26816;&#32034;&#26080;&#20851;&#25991;&#26723;&#30340;&#25935;&#24863;&#24615;&#36739;&#39640;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Multi-document summarization (MDS) traditionally assumes a set of topic-related documents are provided. However, this document set is often an artifact of the dataset curation process; in practice, it is not necessarily available and would need to be retrieved given an information need, i.e. a question or topic statement. We study this more challenging "open-domain" setting by formalizing the task and bootstrapping it using existing datasets, retrievers and summarizers. Via extensive experimentation, we determine that: (1) state-of-the-art summarizers suffer large reductions in performance when applied to the open-domain, even when retrieval performance is high, (2) additional training in the open-domain setting can reduce this sensitivity to imperfect retrieval, and (3) summarizers are insensitive to the retrieval of duplicate documents and the order of retrieved documents, but highly sensitive to other errors, like the retrieval of irrelevant documents. Based on our results, we provi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DePlot&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#27169;&#24577;&#36716;&#25442;&#27169;&#22359;&#65292;&#33021;&#22815;&#23558;&#22270;&#34920;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#32447;&#24615;&#21270;&#30340;&#34920;&#26684;&#12290;&#21033;&#29992;&#36825;&#20010;&#27169;&#22359;&#21644;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#27425;&#24615;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10505</link><description>&lt;p&gt;
DePlot&#65306;&#21033;&#29992;&#22270;&#34920;&#36716;&#34920;&#26684;&#32763;&#35793;&#30340;&#19968;&#27425;&#24615;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
DePlot: One-shot visual language reasoning by plot-to-table translation. (arXiv:2212.10505v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DePlot&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#27169;&#24577;&#36716;&#25442;&#27169;&#22359;&#65292;&#33021;&#22815;&#23558;&#22270;&#34920;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#32447;&#24615;&#21270;&#30340;&#34920;&#26684;&#12290;&#21033;&#29992;&#36825;&#20010;&#27169;&#22359;&#21644;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#27425;&#24615;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#22914;&#22270;&#34920;&#21644;&#22270;&#24418;&#22312;&#20154;&#31867;&#19990;&#30028;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#29702;&#35299;&#22270;&#34920;&#38656;&#35201;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#27425;&#24615;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#65292;&#23558;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#30340;&#25361;&#25112;&#20998;&#35299;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#22270;&#24418;&#21040;&#25991;&#26412;&#30340;&#32763;&#35793;&#65292;&#65288;2&#65289;&#23545;&#32763;&#35793;&#21518;&#30340;&#25991;&#26412;&#36827;&#34892;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#19968;&#20010;&#31216;&#20026;DePlot&#30340;&#27169;&#24577;&#36716;&#25442;&#27169;&#22359;&#65292;&#23427;&#23558;&#22270;&#34920;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#32447;&#24615;&#34920;&#26684;&#12290;DePlot&#30340;&#36755;&#20986;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#21551;&#21160;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#33719;&#24471;DePlot&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#32479;&#19968;&#30340;&#20219;&#21153;&#26684;&#24335;&#21644;&#25351;&#26631;&#65292;&#23545;&#22270;&#24418;&#21040;&#34920;&#26684;&#30340;&#20219;&#21153;&#36827;&#34892;&#26631;&#20934;&#21270;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#23545;DePlot&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this tas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861; MatCha &#65292;&#25552;&#39640;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#34920;&#21644;&#35821;&#35328;&#25968;&#25454;&#32852;&#21512;&#24314;&#27169;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#36817;20%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2212.09662</link><description>&lt;p&gt;
MatCha&#65306;&#25968;&#23398;&#25512;&#29702;&#21644;&#22270;&#34920;&#35299;&#26512;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering. (arXiv:2212.09662v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861; MatCha &#65292;&#25552;&#39640;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#34920;&#21644;&#35821;&#35328;&#25968;&#25454;&#32852;&#21512;&#24314;&#27169;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#36817;20%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#25968;&#25454;&#22312;&#20154;&#31867;&#19990;&#30028;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#20294;&#26159;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#24182;&#19981;&#29702;&#24819;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986; MatCha&#65288;&#25968;&#23398;&#25512;&#29702;&#21644;&#22270;&#34920;&#35299;&#26512;&#39044;&#35757;&#32451;&#65289;&#26469;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32852;&#21512;&#24314;&#27169;&#22270;&#34920;/&#32472;&#22270;&#21644;&#35821;&#35328;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#28085;&#30422;&#20102;&#32472;&#22270;&#25286;&#35299;&#21644;&#25968;&#23383;&#25512;&#29702;&#36825;&#20123;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MatCha &#27169;&#22411;&#22312; PlotQA &#21644; ChartQA &#31561;&#26041;&#38754;&#30340;&#34920;&#29616;&#22343;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36817; 20%&#12290;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102; MatCha &#39044;&#35757;&#32451;&#22312;&#25130;&#22270;&#12289;&#25945;&#31185;&#20070;&#22270;&#31034;&#21644;&#25991;&#26723;&#22270;&#24418;&#31561;&#39046;&#22495;&#30340;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#35266;&#23519;&#21040;&#24635;&#20307;&#25913;&#21892;&#65292;&#36825;&#39564;&#35777;&#20102; MatCha &#39044;&#35757;&#32451;&#22312;&#26356;&#24191;&#27867;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models' capabilities in jointly modeling charts/plots and language data. Specifically, we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling.  We perform the MatCha pretraining starting from Pix2Struct, a recently proposed image-to-text visual language model. On standard benchmarks such as PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MatCha pretraining transfers to domains such as screenshots, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MatCha pretraining on broader visual language tas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.09561</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#30340;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are reasoners with Self-Verification. (arXiv:2212.09561v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#26102;&#65292;&#23427;&#38750;&#24120;&#25935;&#24863;&#20110;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24517;&#39035;&#35757;&#32451;&#39564;&#35777;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#20316;&#20026;&#26465;&#20214;&#26469;&#26500;&#24314;&#19968;&#20010;&#26032;&#26679;&#26412;&#65292;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#34987;&#25513;&#30422;&#30340;&#21407;&#22987;&#26465;&#20214;&#12290;&#25105;&#20204;&#22522;&#20110;&#20934;&#30830;&#24615;&#35745;&#31639;&#21487;&#35299;&#37322;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26102;&#25552;&#39640;&#22810;&#20010;&#31639;&#26415;&#21644;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;LLM&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#22810;&#31181;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#21151;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36991;&#20813;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from inco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29616;&#23383;&#38754;&#35299;&#37322;&#26041;&#38754;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#22312;&#20381;&#36182;&#31038;&#20132;&#26041;&#38754;&#30340;&#29616;&#35937;&#26041;&#38754;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2212.06801</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29992;&#35821;&#35328;&#29702;&#35299;&#30340;&#31934;&#32454;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A fine-grained comparison of pragmatic language understanding in humans and language models. (arXiv:2212.06801v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29616;&#23383;&#38754;&#35299;&#37322;&#26041;&#38754;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#22312;&#20381;&#36182;&#31038;&#20132;&#26041;&#38754;&#30340;&#29616;&#35937;&#26041;&#38754;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29992;&#35821;&#35328;&#29702;&#35299;&#26159;&#20154;&#31867;&#20132;&#27969;&#20013;&#33267;&#20851;&#37325;&#35201;&#19988;&#19981;&#26131;&#29702;&#35299;&#30340;&#26041;&#38754;&#65292;&#23545;&#20110;&#20154;&#36896;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#36825;&#19968;&#28857;&#20173;&#28982;&#23384;&#22312;&#24456;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#38646;&#22522;&#30784;&#25552;&#31034;&#19979;&#23545;&#33521;&#35821;&#26448;&#26009;&#36827;&#34892;&#20102;7&#31181;&#23454;&#29992;&#29616;&#35937;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#30340;&#31934;&#32454;&#27604;&#36739;&#65292;&#25506;&#35752;&#27169;&#22411;&#22312;&#35299;&#37322;&#21457;&#35328;&#32773;&#34920;&#36798;&#26102;&#26159;&#21542;&#20855;&#26377;&#35821;&#29992;&#29702;&#35299;&#12289;&#26159;&#21542;&#21644;&#20154;&#31867;&#23384;&#22312;&#30456;&#20284;&#30340;&#38169;&#35823;&#31867;&#22411;&#21644;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#26102;&#20351;&#29992;&#30456;&#20284;&#30340;&#35821;&#35328;&#32447;&#32034;&#12290;&#30740;&#31350;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#39640;&#31934;&#24230;&#24182;&#19988;&#19982;&#20154;&#31867;&#30340;&#38169;&#35823;&#27169;&#24335;&#30456;&#21305;&#37197;&#65306;&#22312;&#19981;&#27491;&#30830;&#30340;&#31572;&#26696;&#20013;&#65292;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#23383;&#38754;&#19978;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#21021;&#27493;&#35777;&#25454;&#34920;&#26126;&#65292;&#27169;&#22411;&#21644;&#20154;&#31867;&#23545;&#30456;&#20284;&#30340;&#35821;&#35328;&#32447;&#32034;&#20063;&#24456;&#25935;&#24863;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#26126;&#30830;&#26500;&#24314;&#30340;&#24515;&#29702;&#29366;&#24577;&#34920;&#31034;&#65292;&#27169;&#22411;&#20063;&#21487;&#20197;&#34920;&#29616;&#20986;&#23454;&#29992;&#34892;&#20026;&#12290;&#20294;&#26159;&#65292;&#27169;&#22411;&#22312;&#20381;&#36182;&#31038;&#20132;&#26041;&#38754;&#30340;&#29616;&#35937;&#26041;&#38754;&#20173;&#23384;&#22312;&#19968;&#23450;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a fine-grained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We find that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor literal interpretations over heuristic-based distractors. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that pragmatic behaviors can emerge in models without explicitly constructed representations of mental states. However, models tend to struggle with phenomena relying on social 
&lt;/p&gt;</description></item><item><title>&#32654;&#22269;&#30340;&#39640;&#36136;&#37327;&#21307;&#30103;&#20445;&#20581;&#23545;&#20110;&#26576;&#20123;&#31038;&#20250;&#32463;&#27982;&#32676;&#20307;&#26469;&#35828;&#25104;&#26412;&#36807;&#39640;&#65292;&#22240;&#27492;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#22914;&#25910;&#20837;&#21644;&#25945;&#32946;&#31243;&#24230;&#19982;&#20581;&#24247;&#30340;&#24635;&#20307;&#25351;&#26631;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.04285</link><description>&lt;p&gt;
&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#23545;&#20581;&#24247;&#24046;&#36317;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Impact of Socioeconomic Factors on Health Disparities. (arXiv:2212.04285v3 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04285
&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#30340;&#39640;&#36136;&#37327;&#21307;&#30103;&#20445;&#20581;&#23545;&#20110;&#26576;&#20123;&#31038;&#20250;&#32463;&#27982;&#32676;&#20307;&#26469;&#35828;&#25104;&#26412;&#36807;&#39640;&#65292;&#22240;&#27492;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#22914;&#25910;&#20837;&#21644;&#25945;&#32946;&#31243;&#24230;&#19982;&#20581;&#24247;&#30340;&#24635;&#20307;&#25351;&#26631;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#30340;&#39640;&#36136;&#37327;&#21307;&#30103;&#20445;&#20581;&#23545;&#20110;&#26576;&#20123;&#31038;&#20250;&#32463;&#27982;&#32676;&#20307;&#26469;&#35828;&#25104;&#26412;&#36807;&#39640;&#12290;&#26412;&#25991;&#21033;&#29992;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#21644;&#30142;&#30149;&#39044;&#38450;&#25511;&#21046;&#20013;&#24515;&#30340;&#25968;&#25454;&#65292;&#30830;&#23450;&#29305;&#23450;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#19982;&#29305;&#23450;&#21644;&#19968;&#33324;&#20581;&#24247;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#32852;&#31243;&#24230;&#65292;&#24182;&#37319;&#29992;&#21487;&#35270;&#21270;&#20998;&#26512;&#21644;&#39044;&#27979;&#24314;&#27169;&#26469;&#35782;&#21035;&#21464;&#37327;&#20043;&#38388;&#26356;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25910;&#20837;&#21644;&#25945;&#32946;&#31243;&#24230;&#31561;&#26576;&#20123;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#19982;&#20581;&#24247;&#30340;&#24635;&#20307;&#25351;&#26631;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality healthcare in the US can be cost-prohibitive for certain socioeconomic groups. In this paper, we examined data from the US Census and the CDC to determine the degree to which specific socioeconomic factors correlate with both specific and general health metrics. We employed visual analysis to find broad trends and predictive modeling to identify more complex relationships between variables. Our results indicate that certain socioeconomic factors, like income and educational attainment, are highly correlated with aggregate measures of health.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#20154;&#24615;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#65292;&#21457;&#29616;&#24403;&#21069;AI&#39550;&#39542;&#21592;&#19981;&#33021;&#21019;&#36896;&#31867;&#20284;&#20154;&#31867;&#30340;&#39550;&#20056;&#20307;&#39564;&#65292;&#38656;&#35201;&#22312;&#24773;&#24863;&#36807;&#28193;&#27169;&#22411;&#20013;&#36827;&#34892;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.02908</link><description>&lt;p&gt;
&#36808;&#21521;&#20154;&#31867;&#20860;&#23481;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65306;&#24773;&#24863;&#36807;&#28193;&#27169;&#22411;&#20013;&#30340;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling. (arXiv:2212.02908v5 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#20154;&#24615;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#65292;&#21457;&#29616;&#24403;&#21069;AI&#39550;&#39542;&#21592;&#19981;&#33021;&#21019;&#36896;&#31867;&#20284;&#20154;&#31867;&#30340;&#39550;&#20056;&#20307;&#39564;&#65292;&#38656;&#35201;&#22312;&#24773;&#24863;&#36807;&#28193;&#27169;&#22411;&#20013;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#36208;&#21521;&#26080;&#38656;&#21452;&#25163;&#30340;&#29983;&#27963;&#26041;&#24335;&#26102;&#65292;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#29616;&#26377;&#25991;&#29486;&#24378;&#35843;&#65292;&#22914;&#26524;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#33021;&#22815;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#26041;&#24335;&#39550;&#39542;&#65292;&#20154;&#20204;&#20250;&#26356;&#23481;&#26131;&#25509;&#21463;&#23427;&#12290;&#28982;&#32780;&#65292;&#20165;&#26377;&#23569;&#37327;&#30740;&#31350;&#20174;&#20056;&#23458;&#35282;&#24230;&#30340;&#33258;&#28982;&#20307;&#39564;&#26469;&#26816;&#39564;&#30446;&#21069;&#30340;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#39033;&#30495;&#23454;&#36947;&#36335;&#29615;&#22659;&#19979;&#30340;&#35797;&#39564;&#65292;&#27979;&#35797;&#20102;69&#20301;&#21442;&#19982;&#32773;&#30340;&#21453;&#39304;&#65292;&#23581;&#35797;&#20102;&#35299;AI&#39550;&#39542;&#20154;&#21592;&#33021;&#21542;&#20026;&#20056;&#23458;&#21019;&#36896;&#31867;&#20284;&#20154;&#31867;&#30340;&#39550;&#20056;&#20307;&#39564;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#39550;&#20056;&#20307;&#39564;&#30340;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#20316;&#20026;&#20056;&#23458;&#20056;&#22352;AI&#39550;&#39542;&#20154;&#21592;&#25110;&#20154;&#31867;&#39550;&#39542;&#20154;&#21592;&#39550;&#39542;&#30340;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65292;&#24182;&#21028;&#26029;&#21496;&#26426;&#26159;&#20154;&#31867;&#36824;&#26159;AI&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20805;&#24403;AI&#39550;&#39542;&#21592;&#30340;&#27773;&#36710;&#26410;&#33021;&#36890;&#36807;&#25105;&#20204;&#30340;&#27979;&#35797;&#65292;&#22240;&#20026;&#20056;&#23458;&#33021;&#22815;&#36229;&#36807;&#38543;&#26426;&#29468;&#27979;&#22320;&#35782;&#21035;&#20986;AI&#39550;&#39542;&#21592;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#31867;&#39550;&#39542;&#21592;&#39550;&#39542;&#36710;&#36742;&#26102;&#65292;&#20056;&#23458;&#30340;&#21028;&#26029;&#32467;&#26524;&#32422;&#22312;&#38543;&#26426;&#29468;&#27979;&#27700;&#24179;&#38468;&#36817;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#20154;&#31867;&#20056;&#23458;&#22312;&#39550;&#39542;&#36807;&#31243;&#20013;&#32473;&#20104;&#20102;&#21738;&#20123;&#20154;&#24615;&#21270;&#29305;&#24449;&#30340;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous cars are indispensable when humans go further down the hands-free route. Although existing literature highlights that the acceptance of the autonomous car will increase if it drives in a human-like manner, sparse research offers the naturalistic experience from a passenger's seat perspective to examine the human likeness of current autonomous cars. The present study tested whether the AI driver could create a human-like ride experience for passengers based on 69 participants' feedback in a real-road scenario. We designed a ride experience-based version of the non-verbal Turing test for automated driving. Participants rode in autonomous cars (driven by either human or AI drivers) as a passenger and judged whether the driver was human or AI. The AI driver failed to pass our test because passengers detected the AI driver above chance. In contrast, when the human driver drove the car, the passengers' judgement was around chance. We further investigated how human passengers ascri
&lt;/p&gt;</description></item><item><title>SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.13308</link><description>&lt;p&gt;
SciRepEval&#65306;&#19968;&#20010;&#29992;&#20110;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#22810;&#26684;&#24335;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13308
&lt;/p&gt;
&lt;p&gt;
SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#21487;&#20197;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#36755;&#20837;&#29305;&#24449;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#34920;&#31034;&#30340;&#29616;&#26377;&#22522;&#20934;&#26410;&#33021;&#25429;&#25417;&#21040;&#30456;&#20851;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; SciRepEval&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#29616;&#23454;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013; 11 &#20010;&#26159;&#26032;&#20219;&#21153;&#65306;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#25490;&#21517;&#21644;&#25628;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#22522;&#20934;&#26469;&#30740;&#31350;&#21644;&#25913;&#36827;&#31185;&#23398;&#25991;&#26723;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#20219;&#21153;&#26684;&#24335;&#26041;&#38754;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#65292;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#20063;&#19981;&#33021;&#25913;&#36827;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#27599;&#20010;&#25991;&#26723;&#30340;&#22810;&#20010;&#23884;&#20837;&#65292;&#27599;&#20010;&#23884;&#20837;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#30340;&#26684;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#20219;&#21153;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#21518;&#22788;&#29702;&#21644;&#19968;&#31181;&#26032;&#26550;&#26500;CI-BERT&#23558;&#27010;&#24565;&#22120;&#25237;&#24433;&#32435;&#20837;&#25152;&#26377;&#23618;&#20013;&#12290;&#27010;&#24565;&#22120;&#21518;&#22788;&#29702;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#35265;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11087</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#22120;&#36741;&#21161;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Conceptor-Aided Debiasing of Large Language Models. (arXiv:2211.11087v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#21518;&#22788;&#29702;&#21644;&#19968;&#31181;&#26032;&#26550;&#26500;CI-BERT&#23558;&#27010;&#24565;&#22120;&#25237;&#24433;&#32435;&#20837;&#25152;&#26377;&#23618;&#20013;&#12290;&#27010;&#24565;&#22120;&#21518;&#22788;&#29702;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#35265;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21453;&#26144;&#20102;&#23427;&#20204;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#35768;&#22810;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26410;&#33021;&#21435;&#20559;&#35265;&#25110;&#32773;&#20250;&#29306;&#29298;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#27010;&#24565;&#22120;&#8212;&#8212;&#19968;&#31181;&#36719;&#25237;&#24433;&#26041;&#27861;&#8212;&#8212;&#26469;&#35782;&#21035;&#21644;&#21435;&#38500;&#22914;BERT&#21644;GPT&#31561;LLMs&#20013;&#30340;&#20559;&#35265;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24212;&#29992;&#27010;&#24565;&#22120;&#30340;&#26041;&#27861;&#65306;&#65288;1&#65289;&#36890;&#36807;&#21518;&#22788;&#29702;&#36827;&#34892;&#20559;&#35265;&#23376;&#31354;&#38388;&#25237;&#24433;&#65307;&#65288;2&#65289;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#8212;&#8212;&#27010;&#24565;&#22120;&#20171;&#20837;BERT(CI-BERT)&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#26126;&#30830;&#22320;&#23558;&#27010;&#24565;&#22120;&#25237;&#24433;&#32435;&#20837;&#25152;&#26377;&#23618;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27010;&#24565;&#22120;&#21518;&#22788;&#29702;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;LLMs&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#35265;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#37117;&#24456;&#31283;&#20581;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#23545;&#29616;&#26377;&#20559;&#35265;&#23376;&#31354;&#38388;&#30340;&#36923;&#36753;&#25805;&#20316;&#26469;&#26377;&#25928;&#22320;&#20943;&#36731;&#20132;&#38598;&#20559;&#35265;&#12290;&#34429;&#28982;CI-BERT&#30340;&#35757;&#32451;&#32771;&#34385;&#20102;&#25152;&#26377;&#23618;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#23427;&#30340;&#35757;&#32451;&#25104;&#26412;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use conceptors--a soft projection method--to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing; and (2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly incorporates the conceptor projection into all layers during training. We find that conceptor post-processing achieves state-of-the-art (SoTA) debiasing results while maintaining or improving LLMs' performance on the GLUE benchmark. Also, it is robust in various scenarios and can mitigate intersectional bias efficiently by its logical operation on the existing bias subspaces. Although CI-BERT's training takes all layers' bias into account and can beat its post-processing counterpa
&lt;/p&gt;</description></item><item><title>NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.04370</link><description>&lt;p&gt;
NESTER&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation. (arXiv:2211.04370v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04370
&lt;/p&gt;
&lt;p&gt;
NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#24402;&#32435;&#20559;&#32622;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#27599;&#31181;&#29616;&#26377;&#30340;&#25216;&#26415;&#37117;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#26469;&#35299;&#20915;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#20363;&#22914;&#25511;&#21046;&#20542;&#21521;&#24471;&#20998;&#12289;&#24378;&#21046;&#38543;&#26426;&#21270;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#31216;&#20026;&#31070;&#32463;&#31526;&#21495;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#22120;&#65288;NESTER&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;NESTER&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;DSL&#65289;&#12290;&#25105;&#20204;&#36824;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;NESTER&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;NESTER&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment effect estimation from observational data is a central problem in causal inference. Methods based on potential outcomes framework solve this problem by exploiting inductive biases and heuristics from causal inference. Each existing technique addresses a specific aspect of treatment effect estimation, such as controlling propensity score, enforcing randomization, etc., by designing neural network architectures and regularizers. In this paper, we propose an adaptive method called Neurosymbolic Treatment Effect Estimator (NESTER), a generalized method for treatment effect estimation. NESTER brings together all the desiderata for treatment effect estimation into one framework. For this purpose, we design a Domain Specific Language (DSL) for the treatment effect estimation based on inductive biases used in literature. We also theoretically study NESTER's capability for the treatment effect estimation task. Our comprehensive empirical results show that NESTER performs better on ben
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; CLIP-Sculptor &#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#21644;&#32534;&#36753;&#39640;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#19977;&#32500;&#24418;&#29366;&#65292;&#19988;&#19981;&#38656;&#35201; (&#25991;&#26412;&#65292;&#24418;&#29366;) &#23545;&#35757;&#32451;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#21644;&#25913;&#36827;&#30340;&#25351;&#23548;&#26041;&#27861;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.01427</link><description>&lt;p&gt;
CLIP-Sculptor: &#33258;&#28982;&#35821;&#35328;&#38646;&#26679;&#26412;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#19977;&#32500;&#24418;&#29366;
&lt;/p&gt;
&lt;p&gt;
CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language. (arXiv:2211.01427v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01427
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; CLIP-Sculptor &#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#21644;&#32534;&#36753;&#39640;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#19977;&#32500;&#24418;&#29366;&#65292;&#19988;&#19981;&#38656;&#35201; (&#25991;&#26412;&#65292;&#24418;&#29366;) &#23545;&#35757;&#32451;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#21644;&#25913;&#36827;&#30340;&#25351;&#23548;&#26041;&#27861;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#19977;&#32500;&#24418;&#29366;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#29983;&#25104;&#30340;&#24418;&#29366;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#26377;&#38480;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102; CLIP-Sculptor&#65292;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#38656;&#35201;&#65288;&#25991;&#26412;&#65292;&#24418;&#29366;&#65289;&#23545;&#65292;&#23601;&#21487;&#20197;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#19977;&#32500;&#24418;&#29366;&#65292;&#23427;&#37319;&#29992;&#22810;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#39318;&#20808;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#65292;&#28982;&#21518;&#25552;&#21319;&#20998;&#36776;&#29575;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24418;&#29366;&#20445;&#30495;&#24230;&#65292;&#32780;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#20351;&#29992;&#20102; transformer &#26469;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#26465;&#20214;&#21270;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#38656;&#35201;&#20998;&#31867;&#22120;&#30340;&#25351;&#23548;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;-&#22810;&#26679;&#24615;&#30340;&#24179;&#34913;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102; CLIP-Sculptor &#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312; https://ivl.cs.brown.edu/#/projects/clip-sculptor &#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated that natural language can be used to generate and edit 3D shapes. However, these methods generate shapes with limited fidelity and diversity. We introduce CLIP-Sculptor, a method to address these constraints by producing high-fidelity and diverse 3D shapes without the need for (text, shape) pairs during training. CLIP-Sculptor achieves this in a multi-resolution approach that first generates in a low-dimensional latent space and then upscales to a higher resolution for improved shape fidelity. For improved shape diversity, we use a discrete latent space which is modeled using a transformer conditioned on CLIP's image-text embedding space. We also present a novel variant of classifier-free guidance, which improves the accuracy-diversity trade-off. Finally, we perform extensive experiments demonstrating that CLIP-Sculptor outperforms state-of-the-art baselines. The code is available at https://ivl.cs.brown.edu/#/projects/clip-sculptor.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38899;&#39057;&#35270;&#35273;&#20851;&#27880;&#30340;&#35270;&#35273;&#24863;&#30693;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#19978;&#19979;&#25991;&#26469;&#24110;&#21161;&#35299;&#20915;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#20013;&#30340;&#27495;&#20041;&#22768;&#38899;&#38382;&#39064;&#12290;&#22312;AudioCaps&#19978;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26426;&#22120;&#32763;&#35793;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.16428</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#38899;&#39057;&#35270;&#35273;&#20851;&#27880;&#30340;&#35270;&#35273;&#24863;&#30693;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention. (arXiv:2210.16428v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38899;&#39057;&#35270;&#35273;&#20851;&#27880;&#30340;&#35270;&#35273;&#24863;&#30693;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#19978;&#19979;&#25991;&#26469;&#24110;&#21161;&#35299;&#20915;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#20013;&#30340;&#27495;&#20041;&#22768;&#38899;&#38382;&#39064;&#12290;&#22312;AudioCaps&#19978;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26426;&#22120;&#32763;&#35793;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26088;&#22312;&#20135;&#29983;&#38899;&#39057;&#29255;&#27573;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#35768;&#22810;&#23545;&#35937;&#20250;&#20135;&#29983;&#30456;&#20284;&#30340;&#22768;&#38899;&#12290;&#22914;&#20309;&#20934;&#30830;&#35782;&#21035;&#27169;&#31946;&#30340;&#22768;&#38899;&#26159;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#21463;&#21040;&#20154;&#31867;&#22266;&#26377;&#22810;&#27169;&#24577;&#24863;&#30693;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#35270;&#35273;&#24863;&#30693;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#65292;&#21033;&#29992;&#35270;&#35273;&#20449;&#24687;&#26469;&#24110;&#21161;&#25551;&#36848;&#27169;&#31946;&#30340;&#22768;&#38899;&#23545;&#35937;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29616;&#25104;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#35270;&#39057;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#32467;&#21512;&#21040;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#31995;&#32479;&#20013;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#20114;&#34917;&#30340;&#38899;&#39057;-&#35270;&#35273;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38899;&#39057;-&#35270;&#35273;&#20851;&#27880;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#33258;&#36866;&#24212;&#22320;&#25972;&#21512;&#38899;&#39057;&#21644;&#35270;&#35273;&#19978;&#19979;&#25991;&#65292;&#24182;&#28040;&#38500;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#22312;AudioCaps&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#32763;&#35793;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio captioning aims to generate text descriptions of audio clips. In the real world, many objects produce similar sounds. How to accurately recognize ambiguous sounds is a major challenge for audio captioning. In this work, inspired by inherent human multimodal perception, we propose visually-aware audio captioning, which makes use of visual information to help the description of ambiguous sounding objects. Specifically, we introduce an off-the-shelf visual encoder to extract video features and incorporate the visual features into an audio captioning system. Furthermore, to better exploit complementary audio-visual contexts, we propose an audio-visual attention mechanism that adaptively integrates audio and visual context and removes the redundant information in the latent space. Experimental results on AudioCaps, the largest audio captioning dataset, show that our proposed method achieves state-of-the-art results on machine translation metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#28085;&#30422;&#24191;&#27867;&#38889;&#35821;&#35821;&#27861;&#38169;&#35823;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;KAGAS&#27880;&#37322;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20854;&#22312;&#26356;&#24191;&#27867;&#30340;&#38169;&#35823;&#31867;&#22411;&#19978;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#20351;&#29992;&#30340;&#32479;&#35745;&#38889;&#35821;GEC&#31995;&#32479;(Hanspell)&#12290;</title><link>http://arxiv.org/abs/2210.14389</link><description>&lt;p&gt;
&#36208;&#21521;&#26631;&#20934;&#21270;&#30340;&#38889;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65306;&#25968;&#25454;&#38598;&#19982;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation. (arXiv:2210.14389v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#28085;&#30422;&#24191;&#27867;&#38889;&#35821;&#35821;&#27861;&#38169;&#35823;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;KAGAS&#27880;&#37322;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20854;&#22312;&#26356;&#24191;&#27867;&#30340;&#38169;&#35823;&#31867;&#22411;&#19978;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#20351;&#29992;&#30340;&#32479;&#35745;&#38889;&#35821;GEC&#31995;&#32479;(Hanspell)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#27604;&#33521;&#25991;&#31561;&#20854;&#20182;&#20027;&#35201;&#35821;&#35328;&#65292;&#30740;&#31350;&#38889;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;(GEC)&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#38382;&#39064;&#24402;&#22240;&#20110;&#32570;&#20047;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#29992;&#20110;&#38889;&#35821;GEC&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#26412;&#25991;&#20174;&#19981;&#21516;&#26469;&#28304;(Kor-Lang8, Kor-Native, &#21644; Kor-Learner)&#25910;&#38598;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#38889;&#35821;&#35821;&#27861;&#38169;&#35823;&#12290;&#32771;&#34385;&#21040;&#38889;&#35821;&#35821;&#27861;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#20026;&#38889;&#35821;&#23450;&#20041;&#20102;14&#31181;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;KAGAS(&#38889;&#22269;&#33258;&#21160;&#35821;&#27861;&#38169;&#35823;&#27880;&#37322;&#31995;&#32479;)&#65292;&#21487;&#20197;&#20174;&#24179;&#34892;&#35821;&#26009;&#24211;&#33258;&#21160;&#27880;&#37322;&#38169;&#35823;&#31867;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;KAGAS&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#25552;&#20379;&#20102;&#20174;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#26174;&#31034;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26356;&#24191;&#27867;&#30340;&#38169;&#35823;&#31867;&#22411;&#19978;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#20351;&#29992;&#30340;&#32479;&#35745;&#38889;&#35821;GEC&#31995;&#32479;(Hanspell)&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#29992;&#24615;&#12290;&#26412;&#25991;&#20013;&#20351;&#29992;&#30340;&#23454;&#29616;&#21644;&#25968;&#25454;&#38598;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on Korean grammatical error correction (GEC) is limited, compared to other major languages such as English. We attribute this problematic circumstance to the lack of a carefully designed evaluation benchmark for Korean GEC. In this work, we collect three datasets from different sources (Kor-Lang8, Kor-Native, and Kor-Learner) that covers a wide range of Korean grammatical errors. Considering the nature of Korean grammar, We then define 14 error types for Korean and provide KAGAS (Korean Automatic Grammatical error Annotation System), which can automatically annotate error types from parallel corpora. We use KAGAS on our datasets to make an evaluation benchmark for Korean, and present baseline models trained from our datasets. We show that the model trained with our datasets significantly outperforms the currently used statistical Korean GEC system (Hanspell) on a wider range of error types, demonstrating the diversity and usefulness of the datasets. The implementations and dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2209.15240</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Tuning for Graph Neural Networks. (arXiv:2209.15240v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Prompt&#35843;&#25972;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#24341;&#36215;&#20102;&#30740;&#31350;&#28909;&#28526;&#12290;&#19982;&#35821;&#35328;&#39046;&#22495;&#37319;&#29992;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#31574;&#30053;&#19981;&#21516;&#65292;&#22270;&#24418;&#39046;&#22495;&#23637;&#31034;&#20102;&#22810;&#26679;&#21270;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#35774;&#35745;&#36866;&#24403;&#30340;&#22522;&#20110;Prompt&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35843;&#25972;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature (GPF) &#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#31574;&#30053;&#19979;&#30340;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;GPF&#22312;&#36755;&#20837;&#22270;&#24418;&#30340;&#29305;&#24449;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#29702;&#35770;&#19978;&#21487;&#23454;&#29616;&#19982;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#31561;&#25928;&#30340;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#20877;&#38656;&#35201;&#26126;&#30830;&#35828;&#26126;&#27599;&#20010;&#39044;&#35757;&#32451;&#31574;&#30053;&#23545;&#24212;&#30340;Prompt&#20989;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#37319;&#29992;GPF&#26469;&#23454;&#29616;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#33258;&#36866;&#24212;&#36830;&#32493;RL&#65288;DaCoRL&#65289;&#31639;&#27861;&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#29992;&#20102;&#28176;&#36827;&#24335;&#19978;&#19979;&#25991;&#21270;&#23398;&#20064;&#26041;&#27861;&#36880;&#27493;&#32858;&#31867;&#20219;&#21153;&#65292;&#23398;&#20064;&#19968;&#20010;&#19978;&#19979;&#25991;&#26465;&#20214;&#21270;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#29615;&#22659;&#19982;&#20219;&#21153;&#21464;&#21270;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.00347</link><description>&lt;p&gt;
&#21160;&#24577;&#33258;&#36866;&#24212;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#65306;&#36890;&#36807;&#28176;&#36827;&#24335;&#19978;&#19979;&#25991;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamics-Adaptive Continual Reinforcement Learning via Progressive Contextualization. (arXiv:2209.00347v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#33258;&#36866;&#24212;&#36830;&#32493;RL&#65288;DaCoRL&#65289;&#31639;&#27861;&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#29992;&#20102;&#28176;&#36827;&#24335;&#19978;&#19979;&#25991;&#21270;&#23398;&#20064;&#26041;&#27861;&#36880;&#27493;&#32858;&#31867;&#20219;&#21153;&#65292;&#23398;&#20064;&#19968;&#20010;&#19978;&#19979;&#25991;&#26465;&#20214;&#21270;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#29615;&#22659;&#19982;&#20219;&#21153;&#21464;&#21270;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#36805;&#36895;&#35843;&#25972;RL&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#38543;&#30528;&#29615;&#22659;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#30340;&#25913;&#21464;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23398;&#20064;&#21040;&#30340;&#20449;&#24687;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DaCoRL&#65292;&#21363;&#21160;&#24577;&#33258;&#36866;&#24212;&#36830;&#32493;RL&#12290;DaCoRL&#20351;&#29992;&#28176;&#36827;&#24335;&#19978;&#19979;&#25991;&#21270;&#23398;&#20064;&#23398;&#20064;&#19968;&#20010;&#19978;&#19979;&#25991;&#26465;&#20214;&#21270;&#31574;&#30053;&#65292;&#23558;&#19968;&#31995;&#21015;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#38745;&#24577;&#20219;&#21153;&#36880;&#27493;&#32858;&#31867;&#20026;&#19968;&#31995;&#21015;&#19978;&#19979;&#25991;&#65292;&#24182;&#36873;&#25321;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22810;&#22836;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge of continual reinforcement learning (CRL) in dynamic environments is to promptly adapt the RL agent's behavior as the environment changes over its lifetime, while minimizing the catastrophic forgetting of the learned information. To address this challenge, in this article, we propose DaCoRL, i.e., dynamics-adaptive continual RL. DaCoRL learns a context-conditioned policy using progressive contextualization, which incrementally clusters a stream of stationary tasks in the dynamic environment into a series of contexts and opts for an expandable multihead neural network to approximate the policy. Specifically, we define a set of tasks with similar dynamics as an environmental context and formalize context inference as a procedure of online Bayesian infinite Gaussian mixture clustering on environment features, resorting to online Bayesian inference to infer the posterior distribution over contexts. Under the assumption of a Chinese restaurant process prior, this technique c
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#27867;&#21270;&#26694;&#26550;(DRM)&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22810;&#20998;&#31867;&#22120;&#38598;&#25104;&#21644;&#22312;&#32447;&#30446;&#26631;&#26679;&#26412;&#36866;&#24212;&#26469;&#20943;&#23569;&#36866;&#24212;&#24615;&#24046;&#36317;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2208.08661</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#22806;&#26679;&#26412;&#27867;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Domain-Specific Risk Minimization for Out-of-Distribution Generalization. (arXiv:2208.08661v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#27867;&#21270;&#26694;&#26550;(DRM)&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22810;&#20998;&#31867;&#22120;&#38598;&#25104;&#21644;&#22312;&#32447;&#30446;&#26631;&#26679;&#26412;&#36866;&#24212;&#26469;&#20943;&#23569;&#36866;&#24212;&#24615;&#24046;&#36317;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#22312;&#28304;&#22495;&#19978;&#23398;&#20064;&#30340;&#20551;&#35774;&#26469;&#25512;&#26029;&#26410;&#35265;&#30446;&#26631;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#19982;&#38024;&#23545;&#30446;&#26631;&#22495;&#30340;&#26368;&#20248;&#20551;&#35774;&#30456;&#21435;&#29978;&#36828;&#65292;&#36825;&#31181;&#24046;&#36317;&#34987;&#31216;&#20026;&#8220;&#36866;&#24212;&#24615;&#24046;&#36317;&#8221;&#12290;&#22914;&#26524;&#19981;&#21033;&#29992;&#27979;&#35797;&#26679;&#26412;&#20013;&#30340;&#39046;&#22495;&#20449;&#24687;&#65292;&#36866;&#24212;&#24615;&#24046;&#36317;&#30340;&#20272;&#35745;&#21644;&#26368;&#23567;&#21270;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#36825;&#22952;&#30861;&#20102;&#25105;&#20204;&#23558;&#27169;&#22411;&#30828;&#21270;&#21040;&#20219;&#20309;&#26410;&#30693;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#26126;&#30830;&#32771;&#34385;&#36866;&#24212;&#24615;&#24046;&#36317;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#40723;&#21169;&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#23569;&#24046;&#36317;&#65306;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#22810;&#20010;&#20998;&#31867;&#22120;&#26469;&#20016;&#23500;&#20551;&#35774;&#31354;&#38388;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24046;&#36317;&#20272;&#35745;&#26041;&#27861;&#26469;&#25351;&#23548;&#20026;&#30446;&#26631;&#36873;&#25321;&#26356;&#22909;&#30340;&#20551;&#35774;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#22312;&#32447;&#30446;&#26631;&#26679;&#26412;&#26469;&#30452;&#25509;&#36866;&#24212;&#27169;&#22411;&#21442;&#25968;&#26469;&#26368;&#23567;&#21270;&#24046;&#36317;&#12290;&#25105;&#20204;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#36825;&#20004;&#31181;&#31574;&#30053;&#65292;&#20801;&#35768;&#26377;&#25928;&#22320;&#20943;&#23569;&#36866;&#24212;&#24615;&#24046;&#36317;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DRM&#22312;&#20302;&#25968;&#25454;&#26465;&#20214;&#19979;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;DG&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent domain generalization (DG) approaches typically use the hypothesis learned on source domains for inference on the unseen target domain. However, such a hypothesis can be arbitrarily far from the optimal one for the target domain, induced by a gap termed ``adaptivity gap''. Without exploiting the domain information from the unseen test samples, adaptivity gap estimation and minimization are intractable, which hinders us to robustify a model to any unknown distribution. In this paper, we first establish a generalization bound that explicitly considers the adaptivity gap. Our bound motivates two strategies to reduce the gap: the first one is ensembling multiple classifiers to enrich the hypothesis space, then we propose effective gap estimation methods for guiding the selection of a better hypothesis for the target. The other method is minimizing the gap directly by adapting model parameters using online target samples. We thus propose \textbf{Domain-specific Risk Minimization (DRM
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#20020;&#26102;&#22242;&#38431;&#21327;&#20316;&#25361;&#25112;&#65292;&#37319;&#29992;&#26368;&#20339;&#21709;&#24212;&#22810;&#26679;&#24615;&#29983;&#25104;&#38431;&#21451;&#20197;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#31283;&#20581;&#24615;&#12290;&#19982;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#19981;&#20250;&#20986;&#29616;&#34920;&#38754;&#19978;&#30456;&#20284;&#30340;&#38431;&#21451;&#65292;&#20351;&#24471;&#22312;&#19982;&#26410;&#30693;&#38431;&#21451;&#21327;&#20316;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2207.14138</link><description>&lt;p&gt;
&#21033;&#29992;&#26368;&#20339;&#21709;&#24212;&#22810;&#26679;&#24615;&#29983;&#25104;&#20276;&#20387;&#20197;&#35757;&#32451;&#31283;&#20581;&#30340;&#20020;&#26102;&#22242;&#38431;&#21327;&#20316;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generating Teammates for Training Robust Ad Hoc Teamwork Agents via Best-Response Diversity. (arXiv:2207.14138v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#20020;&#26102;&#22242;&#38431;&#21327;&#20316;&#25361;&#25112;&#65292;&#37319;&#29992;&#26368;&#20339;&#21709;&#24212;&#22810;&#26679;&#24615;&#29983;&#25104;&#38431;&#21451;&#20197;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#31283;&#20581;&#24615;&#12290;&#19982;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#19981;&#20250;&#20986;&#29616;&#34920;&#38754;&#19978;&#30456;&#20284;&#30340;&#38431;&#21451;&#65292;&#20351;&#24471;&#22312;&#19982;&#26410;&#30693;&#38431;&#21451;&#21327;&#20316;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#26102;&#22242;&#38431;&#21327;&#20316;(AHT)&#26159;&#35774;&#35745;&#19968;&#20010;&#24378;&#22823;&#30340;&#23398;&#20064;&#20195;&#29702;&#30340;&#25361;&#25112;&#65292;&#35813;&#20195;&#29702;&#33021;&#22815;&#22312;&#27809;&#26377;&#20808;&#21069;&#21327;&#35843;&#26426;&#21046;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#19982;&#26410;&#30693;&#30340;&#38431;&#21451;&#21512;&#20316;&#12290;&#26089;&#26399;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#38431;&#21451;&#31574;&#30053;&#26469;&#35757;&#32451;&#23398;&#20064;&#32773;&#26469;&#35299;&#20915;AHT&#38382;&#39064;&#65292;&#36825;&#20123;&#31574;&#30053;&#36890;&#24120;&#26159;&#22522;&#20110;&#19987;&#23478;&#23545;&#23398;&#20064;&#32773;&#21487;&#33021;&#36935;&#21040;&#30340;&#31574;&#30053;&#30340;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#35774;&#35745;&#30340;&#12290;&#28982;&#32780;&#65292;&#20026;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#35757;&#32451;&#23454;&#29616;&#38431;&#21451;&#31574;&#30053;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#35757;&#32451;&#23398;&#20064;&#32773;&#20351;&#29992;&#36890;&#36807;&#20248;&#21270;&#20449;&#24687;&#29702;&#35770;&#22810;&#26679;&#24615;&#24230;&#37327;&#29983;&#25104;&#30340;&#38431;&#21451;&#31574;&#30053;&#26469;&#25552;&#39640;&#20854;&#31283;&#20581;&#24615;&#12290;&#20248;&#21270;&#29616;&#26377;&#30340;&#20449;&#24687;&#29702;&#35770;&#22810;&#26679;&#24615;&#24230;&#37327;&#20197;&#29992;&#20110;&#29983;&#25104;&#38431;&#21451;&#31574;&#30053;&#30340;&#38382;&#39064;&#22312;&#20110;&#20250;&#20986;&#29616;&#34920;&#38754;&#19978;&#19981;&#21516;&#30340;&#38431;&#21451;&#12290;&#24403;&#29992;&#20110;AHT&#35757;&#32451;&#26102;&#65292;&#34920;&#38754;&#19978;&#19981;&#21516;&#30340;&#38431;&#21451;&#34892;&#20026;&#21487;&#33021;&#26080;&#27861;&#25552;&#39640;&#23398;&#20064;&#32773;&#22312;&#19982;&#26410;&#30693;&#30340;&#38431;&#21451;&#21327;&#20316;&#26102;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ad hoc teamwork (AHT) is the challenge of designing a robust learner agent that effectively collaborates with unknown teammates without prior coordination mechanisms. Early approaches address the AHT challenge by training the learner with a diverse set of handcrafted teammate policies, usually designed based on an expert's domain knowledge about the policies the learner may encounter. However, implementing teammate policies for training based on domain knowledge is not always feasible. In such cases, recent approaches attempted to improve the robustness of the learner by training it with teammate policies generated by optimising information-theoretic diversity metrics. The problem with optimising existing information-theoretic diversity metrics for teammate policy generation is the emergence of superficially different teammates. When used for AHT training, superficially different teammate behaviours may not improve a learner's robustness during collaboration with unknown teammates. In 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Machine Personality Inventory (MPI)&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20010;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLM)&#20855;&#26377;&#20010;&#24615;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Personality Prompting (P^2)&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLM&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.07550</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#35825;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Inducing Personality in Pre-trained Language Models. (arXiv:2206.07550v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Machine Personality Inventory (MPI)&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20010;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLM)&#20855;&#26377;&#20010;&#24615;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Personality Prompting (P^2)&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLM&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#36215;&#28304;&#20110;&#21746;&#23398;&#25506;&#32034;&#65292;&#20851;&#27880;&#20010;&#20307;&#22312;&#24605;&#32771;&#12289;&#24773;&#24863;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#19982;&#20154;&#31867;&#26085;&#24120;&#21512;&#20316;&#30340;&#31038;&#20132;&#26426;&#22120;&#65292;&#25105;&#20204;&#24819;&#30693;&#36947;&#65306;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#25317;&#26377;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20010;&#24615;&#65311;&#22914;&#26524;&#26159;&#65292;&#25105;&#20204;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#65311;&#36827;&#19968;&#27493;&#22320;&#65292;&#22312;&#27492;&#35780;&#20272;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#22914;&#20309;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#20010;&#24615;&#24211;(Machine Personality Inventory, MPI)&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#30340;&#20010;&#24615;&#12290;MPI&#36981;&#24490;&#26631;&#20934;&#21270;&#30340;&#20010;&#24615;&#27979;&#35797;&#65292;&#22522;&#20110;&#20116;&#22240;&#32032;&#20154;&#26684;&#29702;&#35770;&#21644;&#20154;&#26684;&#35780;&#20272;&#24211;&#24314;&#31435;&#12290;&#36890;&#36807;&#29992;MPI&#31995;&#32479;&#22320;&#35780;&#20272;LLM&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;LLM&#30340;&#20010;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#20010;&#24615;&#25552;&#31034;(Personality Prompting, P^2)&#26041;&#27861;&#65292;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLMs&#20855;&#26377;&#29305;&#23450;&#30340;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originating as a philosophical quest, the study of personality concerns how individuals differ in thinking, feeling, and behaving. Towards building social machines that work with humans on a daily basis, we are motivated to ask: Do existing Large Language Models (LLMs) possess personalities akin to their human counterparts? If so, how can we evaluate them? Further, given this evaluation framework, how can we induce a particular personality in a controllable fashion? To answer these three questions, we propose the Machine Personality Inventory (MPI) dataset for evaluating the machine personality; MPI follows standardized personality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence showing the existence of personality in LLMs. We further devise a Personality Prompting (P^2) method to induce LLMs with a specific personality in a controllable manner
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIVERSE&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22810;&#26679;&#30340;&#25552;&#31034;&#12289;&#39564;&#35777;&#22120;&#36807;&#28388;&#19981;&#27491;&#30830;&#30340;&#31572;&#26696;&#24182;&#36880;&#20010;&#39564;&#35777;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.02336</link><description>&lt;p&gt;
&#20351;&#29992;&#27493;&#39588;&#24863;&#30693;&#39564;&#35777;&#22120;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Making Large Language Models Better Reasoners with Step-Aware Verifier. (arXiv:2206.02336v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIVERSE&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22810;&#26679;&#30340;&#25552;&#31034;&#12289;&#39564;&#35777;&#22120;&#36807;&#28388;&#19981;&#27491;&#30830;&#30340;&#31572;&#26696;&#24182;&#36880;&#20010;&#39564;&#35777;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#23398;&#20064;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#35821;&#35328;&#27169;&#22411;&#20174;&#26377;&#38480;&#30340;&#31034;&#20363;&#20013;&#36827;&#34892;&#27867;&#21270;&#12290;&#20687;GPT-3&#21644;PaLM&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#65292;&#27604;&#22914;GSM8K&#65292;&#36825;&#26159;&#19968;&#20010;&#31639;&#26415;&#38382;&#39064;&#30340;&#22522;&#20934;&#12290;&#20026;&#20102;&#25552;&#39640;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#25552;&#31034;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#20986;&#26368;&#32456;&#31572;&#26696;&#20043;&#21069;&#36827;&#34892;&#19968;&#31995;&#21015;&#25512;&#29702;&#27493;&#39588;&#65292;&#20174;&#32780;&#22312;GSM8K&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#25552;&#39640;&#65292;&#20174;17.9%&#25552;&#39640;&#21040;&#20102;58.1%&#30340;&#35299;&#39064;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIVERSE&#65288;&#22810;&#26679;&#30340;&#25512;&#29702;&#27493;&#39588;&#39564;&#35777;&#22120;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;DIVERSE&#26377;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#31532;&#19968;&#65292;&#23427;&#29983;&#25104;&#22810;&#26679;&#30340;&#25552;&#31034;&#65292;&#25506;&#32034;&#30456;&#21516;&#38382;&#39064;&#30340;&#19981;&#21516;&#25512;&#29702;&#36335;&#24452;&#65307;&#31532;&#20108;&#65292;&#23427;&#20351;&#29992;&#39564;&#35777;&#22120;&#26681;&#25454;&#21152;&#26435;&#25237;&#31080;&#26041;&#26696;&#36807;&#28388;&#25481;&#19981;&#27491;&#30830;&#30340;&#31572;&#26696;&#65307;&#31532;&#19977;&#65292;&#23427;&#36880;&#20010;&#39564;&#35777;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DIVERSE has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2205.15677</link><description>&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#21028;&#21035;&#22120;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#21487;&#24494;&#22686;&#24378;&#25216;&#26415;&#25913;&#21892;&#20102;GAN&#35757;&#32451;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#20294;&#26159;&#65292;&#22686;&#24378;&#25216;&#26415;&#38544;&#24335;&#22320;&#24341;&#20837;&#20102;&#19981;&#33391;&#19981;&#21464;&#24615;&#22240;&#32032;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#30001;&#25968;&#25454;&#36716;&#25442;&#24341;&#36215;&#30340;&#26631;&#31614;&#31354;&#38388;&#35821;&#20041;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#21028;&#21035;&#22120;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#26368;&#32456;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#21464;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21516;&#26102;&#32487;&#25215;&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#65292;&#35813;&#21028;&#21035;&#22120;&#21487;&#20197;&#39044;&#27979;&#22686;&#24378;&#25968;&#25454;&#30340;&#21442;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#25968;&#25454;&#30340;&#39044;&#27979;&#30446;&#26631;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#21306;&#21035;&#24320;&#26469;&#12290;&#25105;&#20204;&#36824;&#40723;&#21169;&#29983;&#25104;&#22120;&#23545;&#25239;&#22320;&#29983;&#25104;&#20854;&#22686;&#24378;&#21442;&#25968;&#21487;&#20197;&#34987;&#21028;&#21035;&#22120;&#20934;&#30830;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#37327;&#21644;&#26356;&#39640;&#25928;&#30340;&#21028;&#21035;&#22120;&#65292;&#25552;&#39640;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generative adversarial networks (GANs) with limited data is challenging because discriminator is prone to overfitting. Previously proposed differentiable augmentation demonstrates improved data efficiency of training GANs. However, the augmentation implicitly introduces undesired invariance to augmentation for the discriminator since it ignores the change of semantics in the label space caused by data transformation, which may limit the representation learning ability of the discriminator and ultimately affect the generative modeling performance of the generator. To mitigate the negative impact of invariance while inheriting the benefits of data augmentation, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of the augmented data. Particularly, the prediction targets of real data and generated data are required to be distinguished since they are different during training. We further encourage the generator to adversari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#27169;&#22411;&#65292;&#24182;&#25299;&#23637;&#20004;&#20010;&#22522;&#26412;&#30340;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20219;&#21153;&#65292;&#21363;&#20869;&#25554;&#38142;&#25509;&#39044;&#27979;&#21644;&#22806;&#25554;&#38142;&#25509;&#39044;&#27979;&#21040;&#19968;&#27425;&#23398;&#20064;&#30340;&#22659;&#20917;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.10621</link><description>&lt;p&gt;
&#23398;&#20064;&#19968;&#27425;&#20851;&#31995;&#20803;&#34920;&#31034;&#20197;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Meta Representations of One-shot Relations for Temporal Knowledge Graph Link Prediction. (arXiv:2205.10621v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#27169;&#22411;&#65292;&#24182;&#25299;&#23637;&#20004;&#20010;&#22522;&#26412;&#30340;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20219;&#21153;&#65292;&#21363;&#20869;&#25554;&#38142;&#25509;&#39044;&#27979;&#21644;&#22806;&#25554;&#38142;&#25509;&#39044;&#27979;&#21040;&#19968;&#27425;&#23398;&#20064;&#30340;&#22659;&#20917;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#38745;&#24577;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23569;&#37327;&#20851;&#31995;&#23398;&#20064;&#21463;&#21040;&#20102;&#26356;&#22823;&#30340;&#20851;&#27880;&#65292;&#32780;&#23545;&#20110;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#30340;&#23569;&#37327;&#23398;&#20064;&#20960;&#20046;&#27809;&#26377;&#34987;&#30740;&#31350;&#12290;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#27604;&#65292;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#21253;&#21547;&#20016;&#23500;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#22240;&#27492;&#38656;&#35201;&#26102;&#38388;&#25512;&#29702;&#25216;&#26415;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#22312;&#26102;&#38388;&#32972;&#26223;&#19979;&#23398;&#20064;&#23569;&#37327;&#20851;&#31995;&#25552;&#20986;&#20102;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#22522;&#20110;&#20043;&#21069;&#30340;&#38745;&#24577;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23569;&#37327;&#20851;&#31995;&#23398;&#20064;&#65292;&#23558;&#20004;&#20010;&#22522;&#26412;&#30340;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20219;&#21153;&#65288;&#21363;&#20869;&#25554;&#21644;&#22806;&#25554;&#38142;&#25509;&#39044;&#27979;&#65289;&#25512;&#24191;&#21040;&#19968;&#27425;&#23398;&#20064;&#30340;&#22659;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#19968;&#27425;&#24615;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#20219;&#21153;&#30340;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot relational learning for static knowledge graphs (KGs) has drawn greater interest in recent years, while few-shot learning for temporal knowledge graphs (TKGs) has hardly been studied. Compared to KGs, TKGs contain rich temporal information, thus requiring temporal reasoning techniques for modeling. This poses a greater challenge in learning few-shot relations in the temporal context. In this paper, we follow the previous work that focuses on few-shot relational learning on static KGs and extend two fundamental TKG reasoning tasks, i.e., interpolated and extrapolated link prediction, to the one-shot setting. We propose four new large-scale benchmark datasets and develop a TKG reasoning model for learning one-shot relations in TKGs. Experimental results show that our model can achieve superior performance on all datasets in both TKG link prediction tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20174;&#23454;&#38469;&#22240;&#26524;&#20851;&#31995;&#65288;NESS&#27979;&#35797;&#65289;&#23450;&#20041;&#21040;&#36923;&#36753;&#32534;&#31243;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#36825;&#23545;&#20110;&#35745;&#31639;&#20262;&#29702;&#23398;&#20013;&#28041;&#21450;&#36947;&#24503;&#36131;&#20219;&#21644;&#20262;&#29702;&#35268;&#33539;&#30340;&#20915;&#31574;&#24212;&#29992;&#31243;&#24207;&#23588;&#20026;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2205.02919</link><description>&lt;p&gt;
&#22522;&#20110;&#34892;&#21160;&#35821;&#35328;&#30340;&#23454;&#38469;&#22240;&#26524;&#20851;&#31995;&#23545;&#35745;&#31639;&#20262;&#29702;&#23398;&#30340;&#24433;&#21709;&#65306;ASP&#30340;&#19968;&#20010;&#23436;&#25972;&#19988;&#21487;&#38752;&#30340;&#23454;&#29616;&#65288;arXiv:2205.02919v2 [cs.AI] &#26356;&#26032;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Action Languages Based Actual Causality for Computational Ethics: a Sound and Complete Implementation in ASP. (arXiv:2205.02919v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20174;&#23454;&#38469;&#22240;&#26524;&#20851;&#31995;&#65288;NESS&#27979;&#35797;&#65289;&#23450;&#20041;&#21040;&#36923;&#36753;&#32534;&#31243;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#36825;&#23545;&#20110;&#35745;&#31639;&#20262;&#29702;&#23398;&#20013;&#28041;&#21450;&#36947;&#24503;&#36131;&#20219;&#21644;&#20262;&#29702;&#35268;&#33539;&#30340;&#20915;&#31574;&#24212;&#29992;&#31243;&#24207;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36947;&#24503;&#36131;&#20219;&#24182;&#19981;&#23616;&#38480;&#20110;&#22240;&#26524;&#20851;&#31995;&#65292;&#20294;&#23427;&#20204;&#23494;&#19981;&#21487;&#20998;&#22320;&#20132;&#32455;&#22312;&#19968;&#36215;&#12290;&#27492;&#22806;&#65292;&#29702;&#24615;&#22320;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#30340;&#28436;&#21270;&#19982;&#22240;&#26524;&#20851;&#31995;&#30340;&#27010;&#24565;&#26377;&#30528;&#22266;&#26377;&#30340;&#32852;&#31995;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#33258;&#21160;&#21270;&#35268;&#21010;&#30340;&#20915;&#31574;&#24212;&#29992;&#31243;&#24207;&#19981;&#21487;&#36991;&#20813;&#22320;&#24517;&#39035;&#22788;&#29702;&#22240;&#26524;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#22914;&#26524;&#23427;&#20204;&#32771;&#34385;&#21487;&#24402;&#22240;&#24615;&#26041;&#38754;&#25110;&#25972;&#21512;&#23545;&#20262;&#29702;&#35268;&#33539;&#30340;&#24341;&#29992;&#12290;&#23613;&#31649;&#23384;&#22312;&#19978;&#36848;&#32771;&#34385;&#65292;&#20294;&#36807;&#21435;&#20960;&#21313;&#24180;&#20851;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#35768;&#22810;&#36777;&#35770;&#37117;&#34920;&#26126;&#20102;&#36825;&#20010;&#27010;&#24565;&#26159;&#22810;&#20040;&#30340;&#22797;&#26434;&#65292;&#22240;&#27492;&#19982;&#35268;&#21010;&#30340;&#25972;&#21512;&#26159;&#22810;&#20040;&#30340;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#23384;&#22312;&#19978;&#36848;&#32771;&#34385;&#65292;&#35745;&#31639;&#20262;&#29702;&#23398;&#20013;&#30340;&#35768;&#22810;&#24037;&#20316;&#20173;&#23558;&#22240;&#26524;&#20851;&#31995;&#38477;&#21040;&#27425;&#35201;&#22320;&#20301;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#25552;&#20379;&#20174;&#36866;&#29992;&#20110;&#34892;&#21160;&#35821;&#35328;&#30340;&#23454;&#38469;&#22240;&#26524;&#20851;&#31995;&#23450;&#20041;&#21040;&#36923;&#36753;&#32534;&#31243;&#30340;&#23436;&#25972;&#19988;&#21487;&#38752;&#30340;&#36716;&#25442;&#65292;&#35813;&#23450;&#20041;&#26159;Wright&#30340;NESS&#27979;&#35797;&#30340;&#24418;&#24335;&#21270;&#12290;&#25152;&#24471;&#21040;&#30340;&#36923;&#36753;&#31243;&#24207;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although moral responsibility is not circumscribed by causality, they are both closely intermixed. Furthermore, rationally understanding the evolution of the physical world is inherently linked with the idea of causality. Thus, the decision-making applications based on automated planning inevitably have to deal with causality, especially if they consider imputability aspects or integrate references to ethical norms. The many debates around causation in the last decades have shown how complex this notion is and thus, how difficult is its integration with planning. As a result, much of the work in computational ethics relegates causality to the background, despite the considerations stated above. This paper's contribution is to provide a complete and sound translation into logic programming from an actual causation definition suitable for action languages, this definition is a formalisation of Wright's NESS test. The obtained logic program allows to deal with complex causal relations. In
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#37325;&#25552;&#31034;&#22686;&#24378;&#36328;&#35821;&#35328;&#25552;&#31034;&#30340;&#26041;&#27861;DPA&#65292;&#21033;&#29992;&#35821;&#35328;&#26080;&#20851;&#30340;&#36890;&#29992;&#25552;&#31034;&#26041;&#27861;&#65292;&#22823;&#22823;&#32531;&#35299;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#22312;XNLI&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#26377;16&#20010;&#33521;&#35821;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24615;&#33021;&#20174;34.99%&#25552;&#39640;&#21040;46.54%&#12290;</title><link>http://arxiv.org/abs/2202.07255</link><description>&lt;p&gt;
&#29992;&#21452;&#37325;&#25552;&#31034;&#22686;&#24378;&#36328;&#35821;&#35328;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cross-lingual Prompting with Dual Prompt Augmentation. (arXiv:2202.07255v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07255
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#37325;&#25552;&#31034;&#22686;&#24378;&#36328;&#35821;&#35328;&#25552;&#31034;&#30340;&#26041;&#27861;DPA&#65292;&#21033;&#29992;&#35821;&#35328;&#26080;&#20851;&#30340;&#36890;&#29992;&#25552;&#31034;&#26041;&#27861;&#65292;&#22823;&#22823;&#32531;&#35299;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#22312;XNLI&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#26377;16&#20010;&#33521;&#35821;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24615;&#33021;&#20174;34.99%&#25552;&#39640;&#21040;46.54%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20986;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#25552;&#31034;&#26041;&#27861;(DPA)&#65292;&#29992;&#20110;&#22312;&#36328;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#35821;&#35328;&#26080;&#20851;&#30340;&#36890;&#29992;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20165;&#26377;&#27599;&#31867;16&#20010;&#33521;&#35821;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#22312;XNLI&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20174;34.99%&#25552;&#39640;&#21040;46.54%&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting shows promising results in few-shot scenarios. However, its strength for multilingual/cross-lingual problems has not been fully exploited. Zhao and Sch\"utze (2021) made initial explorations in this direction by presenting that cross-lingual prompting outperforms cross-lingual finetuning. In this paper, we conduct an empirical exploration on the effect of each component in cross-lingual prompting and derive language-agnostic Universal Prompting, which helps alleviate the discrepancies between source-language training and target-language inference. Based on this, we propose DPA, a dual prompt augmentation framework, aiming at relieving the data scarcity issue in few-shot cross-lingual prompting. Notably, for XNLI, our method achieves 46.54% with only 16 English training examples per class, significantly better than 34.99% of finetuning. Our code is available at https://github.com/DAMO-NLP-SG/DPA.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Facebook&#21644;&#30701;&#20449;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#23545;&#22522;&#20110;&#35821;&#35328;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24046;&#24322;&#24182;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#24179;&#21488;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.01802</link><description>&lt;p&gt;
Facebook&#21644;&#30701;&#20449;&#25991;&#26412;&#20043;&#38388;&#30340;&#19981;&#21516;&#26426;&#20250;&#19981;&#20250;&#22952;&#30861;&#22522;&#20110;&#35821;&#35328;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Different Affordances on Facebook and SMS Text Messaging Do Not Impede Generalization of Language-Based Predictive Models. (arXiv:2202.01802v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Facebook&#21644;&#30701;&#20449;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#23545;&#22522;&#20110;&#35821;&#35328;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24046;&#24322;&#24182;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#24179;&#21488;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#30340;&#31227;&#21160;&#35774;&#22791;&#20581;&#24247;&#24178;&#39044;&#36890;&#24120;&#20351;&#29992;&#22312;&#38750;&#31227;&#21160;&#35774;&#22791;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#65292;&#30001;&#20110;&#25910;&#38598;&#22823;&#37327;&#30701;&#20449;&#25968;&#25454;&#30340;&#22256;&#38590;&#21644;&#39640;&#26114;&#30340;&#36153;&#29992;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#36825;&#20123;&#24179;&#21488;&#20043;&#38388;&#30340;&#27169;&#22411;&#24046;&#24322;&#21644;&#27867;&#21270;&#23545;&#20110;&#27491;&#30830;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;120&#20010;&#20849;&#20139;&#36825;&#20004;&#31181;&#24179;&#21488;&#30340;&#29992;&#25143;&#26679;&#26412;&#65292;&#30740;&#31350;&#20102;Facebook&#21644;&#30701;&#20449;&#20043;&#38388;&#30340;&#24515;&#29702;&#35821;&#35328;&#24046;&#24322;&#20197;&#21450;&#20854;&#23545;&#39046;&#22495;&#22806;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#29992;&#25143;&#20351;&#29992;Facebook&#20998;&#20139;&#32463;&#39564;&#65288;&#22914;&#20241;&#38386;&#65289;&#65292;&#32780;&#20351;&#29992;&#30701;&#20449;&#36827;&#34892;&#20219;&#21153;&#23548;&#21521;&#21644;&#20250;&#35805;&#30446;&#30340;&#65288;&#20363;&#22914;&#35745;&#21010;&#30830;&#35748;&#65289;&#65292;&#21453;&#26144;&#20102;&#24046;&#24322;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#26816;&#39564;&#36825;&#20123;&#24046;&#24322;&#30340;&#19979;&#28216;&#25928;&#24212;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22522;&#20110;Facebook&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;Facebook&#21644;&#30701;&#20449;&#19978;&#20272;&#35745;&#24180;&#40836;&#65292;&#24615;&#21035;&#65292;&#25233;&#37057;&#65292;&#29983;&#27963;&#28385;&#24847;&#24230;&#21644;&#21387;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;Facebook&#21644;&#30701;&#20449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20013;&#27809;&#26377;&#26174;&#30528;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive mobile device-based health interventions often use machine learning models trained on non-mobile device data, such as social media text, due to the difficulty and high expense of collecting large text message (SMS) data. Therefore, understanding the differences and generalization of models between these platforms is crucial for proper deployment. We examined the psycho-linguistic differences between Facebook and text messages, and their impact on out-of-domain model performance, using a sample of 120 users who shared both. We found that users use Facebook for sharing experiences (e.g., leisure) and SMS for task-oriented and conversational purposes (e.g., plan confirmations), reflecting the differences in the affordances. To examine the downstream effects of these differences, we used pre-trained Facebook-based language models to estimate age, gender, depression, life satisfaction, and stress on both Facebook and SMS. We found no significant differences in correlations between 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;AI&#31639;&#27861;&#30340;&#22522;&#20110;MALDI-ToF&#36136;&#35889;&#30340;COVID-19&#35786;&#26029;&#26041;&#27861;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#32463;&#27982;&#23454;&#24800;&#19988;&#24555;&#36895;&#30340;COVID-19&#26816;&#27979;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2109.14099</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110; MALDI-ToF &#36136;&#35889;&#30340; COVID-19 &#35786;&#26029; AI &#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Explainable-AI approach for Diagnosis of COVID-19 using MALDI-ToF Mass Spectrometry. (arXiv:2109.14099v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.14099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;AI&#31639;&#27861;&#30340;&#22522;&#20110;MALDI-ToF&#36136;&#35889;&#30340;COVID-19&#35786;&#26029;&#26041;&#27861;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#32463;&#27982;&#23454;&#24800;&#19988;&#24555;&#36895;&#30340;COVID-19&#26816;&#27979;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20005;&#37325;&#24613;&#24615;&#21628;&#21560;&#32508;&#21512;&#30151;&#20896;&#29366;&#30149;&#27602;&#31867;&#22411;2&#65288;SARS-CoV-2&#65289;&#24341;&#36215;&#20102;&#20840;&#29699;&#22823;&#27969;&#34892;&#65292;&#24182;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;&#20840;&#29699;&#32463;&#27982;&#12290;&#20934;&#30830;&#12289;&#32463;&#27982;&#23454;&#24800;&#19988;&#24555;&#36895;&#30340;&#26816;&#27979;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#23545;&#35782;&#21035;&#24863;&#26579;&#32773;&#21644;&#20943;&#36731;&#20256;&#25773;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#21457;&#24067;&#20102;&#22810;&#31181;&#21478;&#31867;&#30340;&#26816;&#27979; COVID-19 &#30340;&#24179;&#21488;&#65292;&#36825;&#20123;&#24179;&#21488;&#19982;&#24403;&#21069;&#40644;&#37329;&#26631;&#20934;&#23454;&#26102;&#32858;&#21512;&#37238;&#38142;&#21453;&#24212;&#65288;RT-PCR&#65289;&#32467;&#26524;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#26032;&#26041;&#27861;&#19981;&#38656;&#35201;&#40763;&#21693;&#25325;&#23376;&#65288;NP&#65289;&#65292;&#20813;&#38500;&#20102;&#22797;&#26434;&#35797;&#21058;&#30340;&#38656;&#35201;&#65292;&#24182;&#20943;&#36731;&#20102; RT-PCR &#35797;&#21058;&#30340;&#20379;&#24212;&#36127;&#25285;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#65288;AI&#65289;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#25552;&#20379;&#32467;&#26524;&#20449;&#24515;&#12290;&#24403;&#21069; COVID-19 &#30740;&#31350;&#20013;&#30340; AI &#24212;&#29992;&#36890;&#24120;&#32570;&#20047;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#29983;&#29289;&#22522;&#30784;&#65292;&#25105;&#20204;&#30340; AI &#26041;&#27861;&#26159;&#26368;&#26089;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340; AI&#65288;X-AI&#65289;&#31639;&#27861;&#36827;&#34892; COVID-19 &#35786;&#26029;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#20351;&#29992;&#36136;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;
The severe acute respiratory syndrome coronavirus type-2 (SARS-CoV-2) caused a global pandemic and immensely affected the global economy. Accurate, cost-effective, and quick tests have proven substantial in identifying infected people and mitigating the spread. Recently, multiple alternative platforms for testing coronavirus disease 2019 (COVID-19) have been published that show high agreement with current gold standard real-time polymerase chain reaction (RT-PCR) results. These new methods do away with nasopharyngeal (NP) swabs, eliminate the need for complicated reagents, and reduce the burden on RT-PCR test reagent supply. In the present work, we have designed an artificial intelligence-based (AI) testing method to provide confidence in the results. Current AI applications for COVID-19 studies often lack a biological foundation in the decision-making process, and our AI approach is one of the earliest to leverage explainable AI (X-AI) algorithms for COVID-19 diagnosis using mass spec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22870;&#21169;&#31232;&#23569;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2109.12509</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#28145;&#24230;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Deep Exploration for Recommendation Systems. (arXiv:2109.12509v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22870;&#21169;&#31232;&#23569;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#24212;&#20174;&#24310;&#36831;&#21453;&#39304;&#20013;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#24448;&#24448;&#20391;&#37325;&#20110;&#20174;&#29992;&#25143;&#23545;&#21333;&#20010;&#25512;&#33616;&#30340;&#21709;&#24212;&#20013;&#23398;&#20064;&#12290;&#36825;&#20123;&#24037;&#20316;&#21033;&#29992;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20294;&#25918;&#24323;&#20102;&#23398;&#20064;&#29992;&#25143;&#20043;&#21518;&#30340;&#34892;&#20026;&#12290;&#22312;&#36807;&#21435;&#30340;&#24037;&#20316;&#20013;&#65292;&#34429;&#28982;&#33268;&#21147;&#20110;&#20174;&#38543;&#21518;&#30340;&#34892;&#20026;&#20013;&#23398;&#20064;&#65292;&#20294;&#32570;&#20047;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#24341;&#23548;&#24182;&#33719;&#21462;&#26377;&#24847;&#20041;&#30340;&#24310;&#36831;&#21453;&#39304;&#12290;&#24403;&#22870;&#21169;&#36739;&#23569;&#26102;&#65292;&#36890;&#36807;&#24341;&#23548;&#25506;&#32034;&#26377;&#24847;&#20041;&#30340;&#24310;&#36831;&#21453;&#39304;&#21464;&#24471;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#25512;&#33616;&#31995;&#32479;&#24320;&#21457;&#20102;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#33616;&#31995;&#32479;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#22312;&#21333;&#27493;&#25506;&#32034;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26159;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#30340;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommendation systems ought to benefit by probing for and learning from delayed feedback. Research has tended to focus on learning from a user's response to a single recommendation. Such work, which leverages methods of supervised and bandit learning, forgoes learning from the user's subsequent behavior. Where past work has aimed to learn from subsequent behavior, there has been a lack of effective methods for probing to elicit informative delayed feedback. Effective exploration through probing for delayed feedback becomes particularly challenging when rewards are sparse. To address this, we develop deep exploration methods for recommendation systems. In particular, we formulate recommendation as a sequential decision problem and demonstrate benefits of deep exploration over single-step exploration. Our experiments are carried out with high-fidelity industrial-grade simulators and establish large improvements over existing algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22240;&#26524;&#27169;&#24335;&#26469;&#26816;&#26597;&#36755;&#20837;&#21464;&#37327;&#22522;&#20934;&#20540;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#35745;&#31639;Shapley&#20540;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26368;&#20248;&#22522;&#20934;&#20540;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2105.10719</link><description>&lt;p&gt;
&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#22320;&#34920;&#31034;&#25513;&#30721;&#29366;&#24577;&#20197;&#35745;&#31639;DNN&#30340;Shapley&#20540;?
&lt;/p&gt;
&lt;p&gt;
Can We Faithfully Represent Masked States to Compute Shapley Values on a DNN?. (arXiv:2105.10719v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.10719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22240;&#26524;&#27169;&#24335;&#26469;&#26816;&#26597;&#36755;&#20837;&#21464;&#37327;&#22522;&#20934;&#20540;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#35745;&#31639;Shapley&#20540;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26368;&#20248;&#22522;&#20934;&#20540;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#25513;&#30422;&#19968;&#20123;&#36755;&#20837;&#21464;&#37327;&#65292;&#24182;&#35745;&#31639;&#25513;&#30422;&#21518;&#30340;&#36755;&#20837;&#26679;&#26412;&#30340;&#36755;&#20986;&#21464;&#21270;&#65292;&#26159;&#35745;&#31639;&#26679;&#26412;&#36755;&#20837;&#21464;&#37327;&#23646;&#24615;&#30340;&#20856;&#22411;&#26041;&#27861;&#12290;&#20154;&#20204;&#36890;&#24120;&#20351;&#29992;&#22522;&#20934;&#20540;&#26469;&#23631;&#34109;&#36755;&#20837;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#29702;&#35770;&#26469;&#39564;&#35777;&#22522;&#20934;&#20540;&#26159;&#21542;&#20934;&#30830;&#22320;&#20195;&#34920;&#20102;&#36755;&#20837;&#21464;&#37327;&#32570;&#22833;&#30340;&#24773;&#20917;&#65292;&#21363;&#21435;&#38500;&#20102;&#36755;&#20837;&#21464;&#37327;&#30340;&#25152;&#26377;&#20449;&#21495;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;DNN&#30340;&#25512;&#29702;&#24471;&#20998;&#21487;&#20197;&#20005;&#26684;&#22320;&#20998;&#35299;&#20026;DNN&#32534;&#30721;&#30340;&#19968;&#32452;&#22240;&#26524;&#27169;&#24335;&#65288;&#25110;&#27010;&#24565;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#22240;&#26524;&#27169;&#24335;&#26469;&#26816;&#26597;&#22522;&#20934;&#20540;&#30340;&#20934;&#30830;&#24615;&#12290;&#26356;&#20026;&#37325;&#35201;&#30340;&#26159;&#65292;&#24050;&#32463;&#35777;&#26126;&#22240;&#26524;&#27169;&#24335;&#21487;&#20197;&#35299;&#37322;&#20026;Shapley&#20540;&#30340;&#22522;&#26412;&#29702;&#30001;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26368;&#20248;&#22522;&#20934;&#20540;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masking some input variables of a deep neural network (DNN) and computing output changes on the masked input sample represent a typical way to compute attributions of input variables in the sample. People usually mask an input variable using its baseline value. However, there is no theory to examine whether baseline value faithfully represents the absence of an input variable, \emph{i.e.,} removing all signals from the input variable. Fortunately, recent studies show that the inference score of a DNN can be strictly disentangled into a set of causal patterns (or concepts) encoded by the DNN. Therefore, we propose to use causal patterns to examine the faithfulness of baseline values. More crucially, it is proven that causal patterns can be explained as the elementary rationale of the Shapley value. Furthermore, we propose a method to learn optimal baseline values, and experimental results have demonstrated its effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#22833;&#36133;&#30340;&#25216;&#26415;&#26469;&#36827;&#34892;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#26679;&#20363;&#19978;&#27979;&#35797;&#36923;&#36753;&#31243;&#24207;&#65292;&#25105;&#20204;&#29992;&#22833;&#36133;&#30340;&#23376;&#31243;&#24207;&#31934;&#32454;&#22320;&#35299;&#37322;&#20854;&#22833;&#36133;&#65292;&#24182;&#22522;&#20110;&#20998;&#26512;SLD&#26641;&#20998;&#25903;&#30340;&#31639;&#27861;&#36827;&#34892;&#23454;&#29616;&#12290;&#23454;&#39564;&#34920;&#26126;&#22833;&#36133;&#35299;&#37322;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#20551;&#35774;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2102.12551</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#37322;&#36923;&#36753;&#31243;&#24207;&#30340;&#22833;&#36133;&#26469;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning logic programs by explaining their failures. (arXiv:2102.12551v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.12551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#22833;&#36133;&#30340;&#25216;&#26415;&#26469;&#36827;&#34892;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#26679;&#20363;&#19978;&#27979;&#35797;&#36923;&#36753;&#31243;&#24207;&#65292;&#25105;&#20204;&#29992;&#22833;&#36133;&#30340;&#23376;&#31243;&#24207;&#31934;&#32454;&#22320;&#35299;&#37322;&#20854;&#22833;&#36133;&#65292;&#24182;&#22522;&#20110;&#20998;&#26512;SLD&#26641;&#20998;&#25903;&#30340;&#31639;&#27861;&#36827;&#34892;&#23454;&#29616;&#12290;&#23454;&#39564;&#34920;&#26126;&#22833;&#36133;&#35299;&#37322;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#20551;&#35774;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#23478;&#24418;&#25104;&#20551;&#35828;&#24182;&#36827;&#34892;&#23454;&#39564;&#27979;&#35797;&#12290;&#22914;&#26524;&#20551;&#35774;&#22833;&#36133;&#65288;&#34987;&#35777;&#26126;&#20026;&#20551;&#65289;&#65292;&#31185;&#23398;&#23478;&#20250;&#23581;&#35797;&#35299;&#37322;&#22833;&#36133;&#20197;&#25490;&#38500;&#20854;&#20182;&#20551;&#35828;&#12290;&#22833;&#36133;&#20998;&#26512;&#36234;&#31934;&#30830;&#65292;&#28040;&#38500;&#30340;&#20551;&#35828;&#36234;&#22810;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#22833;&#36133;&#35299;&#37322;&#25216;&#26415;&#12290;&#32473;&#23450;&#19968;&#20010;&#34920;&#31034;&#20026;&#36923;&#36753;&#31243;&#24207;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#22312;&#26679;&#26412;&#19978;&#23545;&#20854;&#36827;&#34892;&#27979;&#35797;&#12290;&#22914;&#26524;&#20551;&#35774;&#22833;&#36133;&#65292;&#25105;&#20204;&#29992;&#22833;&#36133;&#30340;&#23376;&#31243;&#24207;&#26469;&#35299;&#37322;&#22833;&#36133;&#12290;&#22914;&#26524;&#27491;&#20363;&#22833;&#36133;&#65292;&#25105;&#20204;&#22312;&#25991;&#23383;&#30340;&#31890;&#24230;&#19978;&#35782;&#21035;&#22833;&#36133;&#30340;&#23376;&#31243;&#24207;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#26512;SLD&#26641;&#20998;&#25903;&#30340;&#22833;&#36133;&#35299;&#37322;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;&#20803;&#35299;&#37322;&#22120;&#30340;&#23454;&#29616;&#19982;Popper ILP&#31995;&#32479;&#30340;&#27979;&#35797;&#38454;&#27573;&#38598;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#32454;&#31890;&#24230;&#30340;&#22833;&#36133;&#20998;&#26512;&#21487;&#20197;&#20801;&#35768;&#22312;&#20551;&#35774;&#31354;&#38388;&#19978;&#23398;&#20064;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#37322;&#22833;&#36133;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#20551;&#35774;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientists form hypotheses and experimentally test them. If a hypothesis fails (is refuted), scientists try to explain the failure to eliminate other hypotheses. The more precise the failure analysis the more hypotheses can be eliminated. Thus inspired, we introduce failure explanation techniques for inductive logic programming. Given a hypothesis represented as a logic program, we test it on examples. If a hypothesis fails, we explain the failure in terms of failing sub-programs. In case a positive example fails, we identify failing sub-programs at the granularity of literals. We introduce a failure explanation algorithm based on analysing branches of SLD-trees. We integrate a meta-interpreter based implementation of this algorithm with the test-stage of the Popper ILP system. We show that fine-grained failure analysis allows for learning fine-grained constraints on the hypothesis space. Our experimental results show that explaining failures can drastically reduce hypothesis space exp
&lt;/p&gt;</description></item></channel></rss>