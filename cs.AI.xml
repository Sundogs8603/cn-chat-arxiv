<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#20351;&#29992;&#20998;&#31867;&#20195;&#26367;&#22238;&#24402;&#35757;&#32451;&#20540;&#20989;&#25968;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#26469;&#25913;&#21892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;</title><link>https://arxiv.org/abs/2403.03950</link><description>&lt;p&gt;
&#20572;&#27490;&#22238;&#24402;&#65306;&#36890;&#36807;&#20998;&#31867;&#35757;&#32451;&#20540;&#20989;&#25968;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stop Regressing: Training Value Functions via Classification for Scalable Deep RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03950
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20998;&#31867;&#20195;&#26367;&#22238;&#24402;&#35757;&#32451;&#20540;&#20989;&#25968;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#26469;&#25913;&#21892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20540;&#20989;&#25968;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26680;&#24515;&#32452;&#20214;&#12290;&#36825;&#20123;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#20989;&#25968;&#65292;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#22238;&#24402;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#21305;&#37197;&#33258;&#20030;&#30446;&#26631;&#20540;&#12290;&#28982;&#32780;&#65292;&#23558;&#20351;&#29992;&#22238;&#24402;&#30340;&#20215;&#20540;&#22411;RL&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#32593;&#32476;&#65292;&#22914;&#39640;&#23481;&#37327;&#30340;Transformers&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#20102;&#36825;&#19968;&#24046;&#24322;&#65292;&#25506;&#35752;&#20102;&#36890;&#36807;&#20351;&#29992;&#20998;&#31867;&#32780;&#19981;&#26159;&#22238;&#24402;&#26469;&#35757;&#32451;&#20540;&#20989;&#25968;&#26159;&#21542;&#20063;&#21487;&#20197;&#31616;&#21333;&#22320;&#25552;&#39640;&#28145;&#24230;RL&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;&#20998;&#31867;&#20132;&#21449;&#29109;&#35757;&#32451;&#30340;&#20540;&#20989;&#25968;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#36825;&#20123;&#39046;&#22495;&#21253;&#25324;&#65306;&#22312;Atari 2600&#28216;&#25103;&#19978;&#20351;&#29992;SoftMoEs&#36827;&#34892;&#21333;&#19968;&#20219;&#21153;RL&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03950v1 Announce Type: cross  Abstract: Value functions are a central component of deep reinforcement learning (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains. These include: single-task RL on Atari 2600 games with SoftMoEs
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RialTo&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#31283;&#20581;&#21270;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#19981;&#23433;&#20840;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#25110;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#24615;&#33021;&#20248;&#36234;&#12289;&#31283;&#20581;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.03949</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#35843;&#21644;&#29616;&#23454;&#65306;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#25805;&#20316;&#30340;&#23454;-&#27169;-&#23454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RialTo&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#31283;&#20581;&#21270;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#19981;&#23433;&#20840;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#25110;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#24615;&#33021;&#20248;&#36234;&#12289;&#31283;&#20581;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#20154;&#31867;&#30417;&#30563;&#26469;&#23398;&#20064;&#23545;&#29289;&#20307;&#23039;&#21183;&#21464;&#21270;&#12289;&#29289;&#29702;&#24178;&#25200;&#21644;&#35270;&#35273;&#25200;&#21160;&#40065;&#26834;&#30340;&#31574;&#30053;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#20197;&#23398;&#20064;&#31283;&#20581;&#34892;&#20026;&#65292;&#20294;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#19981;&#23433;&#20840;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#12290;&#20026;&#20102;&#22312;&#27809;&#26377;&#19981;&#23433;&#20840;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#25110;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#36127;&#25285;&#19979;&#23398;&#20064;&#24615;&#33021;&#20248;&#36234;&#12289;&#31283;&#20581;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RialTo&#65292;&#19968;&#20010;&#36890;&#36807;&#22312;&#21363;&#23558;&#20174;&#23569;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#26500;&#24314;&#30340;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#31283;&#20581;&#21270;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#30340;&#31995;&#32479;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#23454;-&#27169;-&#23454;&#27969;&#27700;&#32447;&#65292;RialTo&#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#25509;&#21475;&#65292;&#29992;&#20110;&#24555;&#36895;&#25195;&#25551;&#21644;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#25968;&#23383;&#23402;&#29983;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#21453;&#21521;&#25552;&#28860;&#8221;&#36807;&#31243;&#65292;&#29992;&#20110;&#32473;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#24102;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03949v1 Announce Type: cross  Abstract: Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in "digital twin" simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel "inverse distillation" procedure for bringing real-world demonstrations
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;NowcastingGPT-EVL&#22312;&#26497;&#31471;&#38477;&#27700;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#26497;&#31471;&#38477;&#27700;&#20107;&#20214;&#26102;&#12290;</title><link>https://arxiv.org/abs/2403.03929</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#26497;&#31471;&#38477;&#27700;&#21363;&#26102;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Extreme Precipitation Nowcasting using Transformer-based Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03929
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;NowcastingGPT-EVL&#22312;&#26497;&#31471;&#38477;&#27700;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#26497;&#31471;&#38477;&#27700;&#20107;&#20214;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#24102;&#26377;&#26497;&#20540;&#25439;&#22833;&#65288;EVL&#65289;&#27491;&#21017;&#21270;&#30340;NowcastingGPT&#65292;&#36827;&#34892;&#26497;&#31471;&#38477;&#27700;&#21363;&#26102;&#39044;&#25253;&#12290;&#21033;&#29992;&#26469;&#33258;&#33655;&#20848;&#30343;&#23478;&#27668;&#35937;&#30740;&#31350;&#25152;&#65288;KNMI&#65289;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#30701;&#26399;&#38477;&#27700;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;EVL&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20551;&#23450;&#22266;&#23450;&#30340;&#26497;&#31471;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#25429;&#25417;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#35758;&#30340;NowcastingGPT-EVL&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#38477;&#27700;&#39044;&#25253;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#26497;&#31471;&#38477;&#27700;&#20107;&#20214;&#26102;&#12290;&#20195;&#30721;&#21487;&#22312;\url{https://github.com/Cmeo97/NowcastingGPT}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03929v1 Announce Type: cross  Abstract: This paper presents an innovative approach to extreme precipitation nowcasting by employing Transformer-based generative models, namely NowcastingGPT with Extreme Value Loss (EVL) regularization. Leveraging a comprehensive dataset from the Royal Netherlands Meteorological Institute (KNMI), our study focuses on predicting short-term precipitation with high accuracy. We introduce a novel method for computing EVL without assuming fixed extreme representations, addressing the limitations of current models in capturing extreme weather events. We present both qualitative and quantitative analyses, demonstrating the superior performance of the proposed NowcastingGPT-EVL in generating accurate precipitation forecasts, especially when dealing with extreme precipitation events. The code is available at \url{https://github.com/Cmeo97/NowcastingGPT}.
&lt;/p&gt;</description></item><item><title>&#24847;&#35782;&#34987;&#30475;&#20316;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#31639;&#65292;&#31216;&#20026;&#26377;&#38480;&#35745;&#31639;&#65292;&#36825;&#19982;&#20256;&#32479;&#30340;&#22270;&#28789;&#35745;&#31639;&#19981;&#21516;&#12290;</title><link>https://arxiv.org/abs/2403.03925</link><description>&lt;p&gt;
&#24847;&#35782;&#20316;&#20026;&#26377;&#38480;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Consciousness qua Mortal Computation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03925
&lt;/p&gt;
&lt;p&gt;
&#24847;&#35782;&#34987;&#30475;&#20316;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#31639;&#65292;&#31216;&#20026;&#26377;&#38480;&#35745;&#31639;&#65292;&#36825;&#19982;&#20256;&#32479;&#30340;&#22270;&#28789;&#35745;&#31639;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#21151;&#33021;&#20027;&#20041;&#35748;&#20026;&#24847;&#35782;&#26159;&#19968;&#31181;&#35745;&#31639;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;&#25110;&#35768;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#19981;&#33021;&#26159;&#22270;&#28789;&#35745;&#31639;&#12290;&#30456;&#21453;&#65292;&#35745;&#31639;&#21151;&#33021;&#20027;&#20041;&#26263;&#31034;&#30528;&#24847;&#35782;&#26159;&#19968;&#31181;&#26032;&#22411;&#35745;&#31639;&#65292;&#30001;Geoffrey Hinton&#26368;&#36817;&#25552;&#20986;&#65292;&#31216;&#20026;&#26377;&#38480;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03925v1 Announce Type: cross  Abstract: Computational functionalism posits that consciousness is a computation. Here we show, perhaps surprisingly, that it cannot be a Turing computation. Rather, computational functionalism implies that consciousness is a novel type of computation that has recently been proposed by Geoffrey Hinton, called mortal computation.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#25991;&#26412;&#20998;&#26512;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#25945;&#23398;&#36136;&#37327;&#25552;&#21319;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#21644;&#21487;&#25805;&#20316;&#30340;&#21453;&#39304;&#12290;</title><link>https://arxiv.org/abs/2403.03920</link><description>&lt;p&gt;
&#25552;&#21319;&#25945;&#23398;&#36136;&#37327;&#65306;&#21033;&#29992;&#35745;&#31639;&#26426;&#36741;&#21161;&#25991;&#26412;&#20998;&#26512;&#20174;&#25945;&#32946;&#25991;&#29486;&#20013;&#29983;&#25104;&#28145;&#20837;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Enhancing Instructional Quality: Leveraging Computer-Assisted Textual Analysis to Generate In-Depth Insights from Educational Artifacts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03920
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#25991;&#26412;&#20998;&#26512;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#25945;&#23398;&#36136;&#37327;&#25552;&#21319;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#21644;&#21487;&#25805;&#20316;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35745;&#31639;&#26426;&#36741;&#21161;&#25991;&#26412;&#20998;&#26512;&#22312;&#36890;&#36807;&#25945;&#32946;&#25991;&#29486;&#25552;&#20379;&#28145;&#20837;&#35265;&#35299;&#20197;&#25552;&#21319;&#25945;&#23398;&#36136;&#37327;&#20013;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#29702;&#26597;&#24503;&#183;&#22467;&#23572;&#33707;&#23572;&#30340;&#25945;&#23398;&#26680;&#24515;&#26694;&#26550;&#26469;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#65292;&#21487;&#20197;&#20998;&#26512;&#25945;&#32946;&#20869;&#23481;&#12289;&#25945;&#24072;&#35805;&#35821;&#21644;&#23398;&#29983;&#22238;&#24212;&#20197;&#20419;&#36827;&#25945;&#23398;&#25913;&#36827;&#12290;&#36890;&#36807;&#23545;&#25945;&#23398;&#26680;&#24515;&#26694;&#26550;&#20869;&#30340;&#32508;&#21512;&#35780;&#35770;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;AI/ML&#25972;&#21512;&#25552;&#20379;&#26174;&#33879;&#20248;&#21183;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#21253;&#25324;&#25945;&#24072;&#36741;&#23548;&#12289;&#23398;&#29983;&#25903;&#25345;&#21644;&#20869;&#23481;&#24320;&#21457;&#12290;&#25105;&#20204;&#25581;&#31034;&#20986;&#30340;&#27169;&#24335;&#34920;&#26126;&#65292;AI/ML &#19981;&#20165;&#31616;&#21270;&#20102;&#34892;&#25919;&#20219;&#21153;&#65292;&#36824;&#20026;&#20010;&#24615;&#21270;&#23398;&#20064;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#65292;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#21453;&#39304;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#25945;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03920v1 Announce Type: new  Abstract: This paper explores the transformative potential of computer-assisted textual analysis in enhancing instructional quality through in-depth insights from educational artifacts. We integrate Richard Elmore's Instructional Core Framework to examine how artificial intelligence (AI) and machine learning (ML) methods, particularly natural language processing (NLP), can analyze educational content, teacher discourse, and student responses to foster instructional improvement. Through a comprehensive review and case studies within the Instructional Core Framework, we identify key areas where AI/ML integration offers significant advantages, including teacher coaching, student support, and content development. We unveil patterns that indicate AI/ML not only streamlines administrative tasks but also introduces novel pathways for personalized learning, providing actionable feedback for educators and contributing to a richer understanding of instructi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.03894</link><description>&lt;p&gt;
IRCoder: &#20013;&#38388;&#34920;&#31034;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#31283;&#20581;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03894
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03894v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#24050;&#36805;&#36895;&#25104;&#20026;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26368;&#21463;&#27426;&#36814;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#19982;&#33258;&#28982;&#35821;&#35328;LM&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#23545;&#20195;&#30721;-LMs&#65288;&#21363;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;LMs&#65289;&#30340;&#22810;&#35821;&#35328;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#22914;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#29305;&#23450;&#20110;&#35821;&#35328;&#30340;&#25968;&#25454;&#22686;&#24378;&#20197;&#21450;&#20107;&#21518;LM&#35843;&#25972;&#65292;&#20197;&#21450;&#21033;&#29992;&#21407;&#22987;&#25991;&#26412;&#20869;&#23481;&#20043;&#22806;&#30340;&#25968;&#25454;&#28304;&#65292;&#35201;&#31232;&#23569;&#24471;&#22810;&#12290;&#29305;&#21035;&#26159;&#65292;&#22823;&#22810;&#25968;&#20027;&#27969;&#20195;&#30721;-LMs&#20165;&#22312;&#28304;&#20195;&#30721;&#25991;&#20214;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#29616;&#25104;&#30340;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#65288;&#36328;&#32534;&#31243;&#35821;&#35328;&#20849;&#20139;&#65289;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#24182;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#21069;&#26223;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#32534;&#21046;&#20102;SLTrans&#65292;&#19968;&#20010;&#30001;&#36817;400&#19975;&#20010;&#33258;&#21253;&#21547;&#28304;&#20195;&#30721;&#25991;&#20214;&#32452;&#25104;&#30340;&#24182;&#34892;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03894v1 Announce Type: new  Abstract: Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.   To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled wi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25299;&#23637;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27602;&#24615;&#32531;&#35299;&#30340;&#33539;&#22260;&#65292;&#28085;&#30422;&#20102;&#22810;&#35821;&#35328;&#29615;&#22659;&#65292;&#36890;&#36807;&#32763;&#35793;&#25968;&#25454;&#35780;&#20272;&#21644;&#22686;&#24378;&#32531;&#35299;&#25216;&#26415;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#32531;&#35299;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#37327;&#23545;&#32531;&#35299;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03893</link><description>&lt;p&gt;
&#20174;&#21333;&#19968;&#21040;&#22810;&#26679;&#65306;&#25299;&#23637;&#35821;&#35328;&#27169;&#22411;&#20013;&#27602;&#24615;&#32531;&#35299;&#30340;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03893
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25299;&#23637;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27602;&#24615;&#32531;&#35299;&#30340;&#33539;&#22260;&#65292;&#28085;&#30422;&#20102;&#22810;&#35821;&#35328;&#29615;&#22659;&#65292;&#36890;&#36807;&#32763;&#35793;&#25968;&#25454;&#35780;&#20272;&#21644;&#22686;&#24378;&#32531;&#35299;&#25216;&#26415;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#32531;&#35299;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#37327;&#23545;&#32531;&#35299;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27602;&#24615;&#32531;&#35299;&#20960;&#20046;&#23436;&#20840;&#38598;&#20013;&#22312;&#21333;&#35821;&#35328;&#29615;&#22659;&#20013;&#12290;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#25317;&#25265;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#23433;&#20840;&#25514;&#26045;&#36319;&#19978;&#27493;&#20240;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;&#27602;&#24615;&#32531;&#35299;&#33539;&#22260;&#25193;&#23637;&#21040;&#24212;&#23545;&#22810;&#35821;&#35328;&#24102;&#26469;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#32570;&#20047;&#36328;&#35821;&#35328;&#30340;&#36275;&#22815;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#32763;&#35793;&#25968;&#25454;&#26469;&#35780;&#20272;&#21644;&#22686;&#24378;&#25105;&#20204;&#30340;&#32531;&#35299;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#22312;&#38745;&#24577;&#21644;&#25345;&#32493;&#27602;&#24615;&#32531;&#35299;&#22330;&#26223;&#19979;&#27604;&#36739;&#20102;&#24494;&#35843;&#32531;&#35299;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26816;&#39564;&#32763;&#35793;&#36136;&#37327;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#23545;&#27602;&#24615;&#32531;&#35299;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#25968;&#37327;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#32531;&#35299;&#24037;&#20316;&#30340;&#25104;&#21151;&#12290;&#28085;&#30422;&#20102;&#20061;&#31181;&#35821;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20195;&#34920;&#20102;&#24191;&#27867;&#30340;&#35821;&#35328;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03893v1 Announce Type: cross  Abstract: To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it's crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25193;&#25955;&#31574;&#30053;&#65288;HDP&#65289;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#36890;&#36807;&#23558;&#25805;&#32437;&#31574;&#30053;&#20998;&#35299;&#20026;&#20998;&#23618;&#32467;&#26500;&#65292;&#21516;&#26102;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#35268;&#21010;&#21644;&#29983;&#25104;&#32454;&#31890;&#24230;&#30340;&#20302;&#23618;&#21160;&#20316;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#24863;&#30693;&#30446;&#26631;&#26465;&#20214;&#25511;&#21046;&#20195;&#29702;&#65288;RK-Diffuser&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.03890</link><description>&lt;p&gt;
&#20998;&#23618;&#25193;&#25955;&#31574;&#30053;&#29992;&#20110;&#32771;&#34385;&#36816;&#21160;&#23398;&#30340;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03890
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25193;&#25955;&#31574;&#30053;&#65288;HDP&#65289;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#36890;&#36807;&#23558;&#25805;&#32437;&#31574;&#30053;&#20998;&#35299;&#20026;&#20998;&#23618;&#32467;&#26500;&#65292;&#21516;&#26102;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#35268;&#21010;&#21644;&#29983;&#25104;&#32454;&#31890;&#24230;&#30340;&#20302;&#23618;&#21160;&#20316;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#24863;&#30693;&#30446;&#26631;&#26465;&#20214;&#25511;&#21046;&#20195;&#29702;&#65288;RK-Diffuser&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20998;&#23618;&#25193;&#25955;&#31574;&#30053;&#65288;HDP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20998;&#23618;&#20195;&#29702;&#12290;HDP&#23558;&#25805;&#32437;&#31574;&#30053;&#20998;&#35299;&#20026;&#20998;&#23618;&#32467;&#26500;&#65306;&#39640;&#23618;&#20219;&#21153;&#35268;&#21010;&#20195;&#29702;&#39044;&#27979;&#36828;&#31471;&#26368;&#20339;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#65288;NBP&#65289;&#65292;&#20302;&#23618;&#30446;&#26631;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#29983;&#25104;&#26368;&#20339;&#36816;&#21160;&#36712;&#36857;&#12290;&#36825;&#31181;&#20998;&#35299;&#30340;&#31574;&#30053;&#34920;&#31034;&#20351;HDP&#33021;&#22815;&#21516;&#26102;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#35268;&#21010;&#21644;&#29983;&#25104;&#32454;&#31890;&#24230;&#30340;&#20302;&#23618;&#21160;&#20316;&#12290;&#20026;&#20102;&#29983;&#25104;&#31526;&#21512;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#36816;&#21160;&#36712;&#36857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36816;&#21160;&#23398;&#24863;&#30693;&#30446;&#26631;&#26465;&#20214;&#25511;&#21046;&#20195;&#29702;&#65292;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#25193;&#25955;&#22120;&#65288;RK-Diffuser&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;RK-Diffuser&#23398;&#20064;&#29983;&#25104;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#21644;&#20851;&#33410;&#20301;&#32622;&#36712;&#36857;&#65292;&#24182;&#23558;&#31934;&#30830;&#20294;&#32570;&#20047;&#36816;&#21160;&#23398;&#24847;&#35782;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#25193;&#25955;&#22120;&#25552;&#28860;&#20026;&#36816;&#21160;&#23398;&#24863;&#30693;&#20294;&#19981;&#22826;&#31934;&#30830;&#30340;&#20851;&#33410;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03890v1 Announce Type: cross  Abstract: This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical agent for multi-task robotic manipulation. HDP factorises a manipulation policy into a hierarchical structure: a high-level task-planning agent which predicts a distant next-best end-effector pose (NBP), and a low-level goal-conditioned diffusion policy which generates optimal motion trajectories. The factorised policy representation allows HDP to tackle both long-horizon task planning while generating fine-grained low-level actions. To generate context-aware motion trajectories while satisfying robot kinematics constraints, we present a novel kinematics-aware goal-conditioned control agent, Robot Kinematics Diffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the end-effector pose and joint position trajectories, and distill the accurate but kinematics-unaware end-effector pose diffuser to the kinematics-aware but less accurate joint positio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;LD3M&#65289;&#65292;&#32467;&#21512;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.03881</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Latent Dataset Distillation with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03881
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;LD3M&#65289;&#65292;&#32467;&#21512;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#25968;&#25454;&#38598;&#24102;&#26469;&#23384;&#20648;&#25361;&#25112;&#65292;&#24182;&#19988;&#21253;&#21547;&#19968;&#20123;&#38750;&#24433;&#21709;&#21147;&#26679;&#26412;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#34987;&#24573;&#30053;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#26368;&#32456;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#20986;&#29616;&#20102;&#23558;&#25968;&#25454;&#38598;&#20449;&#24687;&#33976;&#39311;&#25104;&#19968;&#32452;&#21387;&#32553;&#26679;&#26412;&#65288;&#21512;&#25104;&#26679;&#26412;&#65289;&#65292;&#21363;&#33976;&#39311;&#25968;&#25454;&#38598;&#30340;&#27010;&#24565;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#36873;&#25321;&#29992;&#20110;&#36830;&#25509;&#21407;&#22987;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26550;&#26500;&#65288;&#36890;&#24120;&#26159;ConvNet&#65289;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#26550;&#26500;&#19982;&#33976;&#39311;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#21017;&#26368;&#32456;&#20934;&#30830;&#24615;&#20250;&#38477;&#20302;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#20363;&#22914;128x128&#21450;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03881v1 Announce Type: cross  Abstract: The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both chal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;CNN-Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#21452;&#37325;&#27880;&#24847;&#21147;&#38376;&#26426;&#21046;&#65292;&#29992;&#20110;&#33152;&#33009;&#30284;&#26816;&#27979;&#21644;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22312;&#33152;&#33009;&#38236;&#26816;&#26597;&#20013;&#35745;&#31639;&#25928;&#29575;&#21644;&#35786;&#26029;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.03879</link><description>&lt;p&gt;
&#37325;&#26032;&#23450;&#20041;&#33152;&#33009;&#38236;&#26816;&#26597;&#65306;&#20351;&#29992;&#39640;&#25928;&#28151;&#21512;CNN-Transformer&#27169;&#22411;&#36827;&#34892;&#33152;&#33009;&#30284;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Redefining cystoscopy with ai: bladder cancer diagnosis using an efficient hybrid cnn-transformer model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03879
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;CNN-Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#21452;&#37325;&#27880;&#24847;&#21147;&#38376;&#26426;&#21046;&#65292;&#29992;&#20110;&#33152;&#33009;&#30284;&#26816;&#27979;&#21644;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22312;&#33152;&#33009;&#38236;&#26816;&#26597;&#20013;&#35745;&#31639;&#25928;&#29575;&#21644;&#35786;&#26029;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33152;&#33009;&#30284;&#22312;&#20840;&#29699;&#26368;&#24120;&#35786;&#26029;&#30340;&#30284;&#30151;&#20013;&#25490;&#21517;&#21069;&#21313;&#65292;&#24182;&#19988;&#30001;&#20110;&#39640;&#22797;&#21457;&#29575;&#65292;&#27835;&#30103;&#25104;&#26412;&#39640;&#26114;&#65292;&#38656;&#35201;&#32456;&#36523;&#38543;&#35775;&#12290;&#35786;&#26029;&#30340;&#20027;&#35201;&#24037;&#20855;&#26159;&#33152;&#33009;&#38236;&#26816;&#26597;&#65292;&#20005;&#37325;&#20381;&#36182;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#27599;&#24180;&#37117;&#26377;&#22823;&#37327;&#30149;&#20363;&#35201;&#20040;&#26410;&#34987;&#35786;&#26029;&#65292;&#35201;&#20040;&#34987;&#35823;&#35786;&#20026;&#23615;&#36335;&#24863;&#26579;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;CNN&#21644;&#36731;&#37327;&#32423;&#30340;&#26080;&#20301;&#32622;&#32534;&#30721;&#21464;&#21387;&#22120;&#20197;&#21450;&#34701;&#21512;&#33258;&#27880;&#24847;&#21147;&#21644;&#31354;&#38388;&#27880;&#24847;&#21147;&#36827;&#34892;&#29305;&#24449;&#22686;&#24378;&#30340;&#21452;&#37325;&#27880;&#24847;&#21147;&#38376;&#26426;&#21046;&#65292;&#29992;&#20110;&#33152;&#33009;&#30284;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26550;&#26500;&#39640;&#25928;&#65292;&#36866;&#29992;&#20110;&#38656;&#35201;&#23454;&#26102;&#25512;&#29702;&#30340;&#21307;&#30103;&#22330;&#26223;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#27169;&#22411;&#35299;&#20915;&#20102;&#22312;&#33152;&#33009;&#38236;&#26816;&#26597;&#20013;&#35745;&#31639;&#25928;&#29575;&#21644;&#35786;&#26029;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03879v1 Announce Type: cross  Abstract: Bladder cancer ranks within the top 10 most diagnosed cancers worldwide and is among the most expensive cancers to treat due to the high recurrence rates which require lifetime follow-ups. The primary tool for diagnosis is cystoscopy, which heavily relies on doctors' expertise and interpretation. Therefore, annually, numerous cases are either undiagnosed or misdiagnosed and treated as urinary infections. To address this, we suggest a deep learning approach for bladder cancer detection and segmentation which combines CNNs with a lightweight positional-encoding-free transformer and dual attention gates that fuse self and spatial attention for feature enhancement. The architecture suggested in this paper is efficient making it suitable for medical scenarios that require real time inference. Experiments have proven that this model addresses the critical need for a balance between computational efficiency and diagnostic accuracy in cystosco
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;NLP&#39046;&#22495;&#20013;&#32570;&#20047;&#23545;&#31038;&#20250;&#38454;&#32423;&#22240;&#32032;&#30340;&#30740;&#31350;&#65292;&#21628;&#21505;&#30740;&#31350;&#32773;&#22312;NLP&#25216;&#26415;&#20013;&#32771;&#34385;&#21644;&#25805;&#20316;&#21270;&#38454;&#32423;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.03874</link><description>&lt;p&gt;
&#36139;&#22256;&#30340;&#35821;&#35328;&#25216;&#26415;&#65306;NLP&#20013;&#32570;&#20047;&#65288;&#31038;&#20250;&#65289;&#38454;&#32423;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Impoverished Language Technology: The Lack of (Social) Class in NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;NLP&#39046;&#22495;&#20013;&#32570;&#20047;&#23545;&#31038;&#20250;&#38454;&#32423;&#22240;&#32032;&#30340;&#30740;&#31350;&#65292;&#21628;&#21505;&#30740;&#31350;&#32773;&#22312;NLP&#25216;&#26415;&#20013;&#32771;&#34385;&#21644;&#25805;&#20316;&#21270;&#38454;&#32423;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;Labov&#65288;1964&#24180;&#65289;&#20851;&#20110;&#35821;&#35328;&#31038;&#20250;&#23618;&#32423;&#30340;&#22522;&#30784;&#24615;&#24037;&#20316;&#20197;&#26469;&#65292;&#35821;&#35328;&#23398;&#30028;&#33268;&#21147;&#20110;&#29702;&#35299;&#31038;&#20250;&#20154;&#21475;&#22240;&#32032;&#21644;&#35821;&#35328;&#29983;&#20135;&#21644;&#24863;&#30693;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#22823;&#37327;&#35777;&#25454;&#34920;&#26126;&#31038;&#20250;&#20154;&#21475;&#22240;&#32032;&#19982;&#35821;&#35328;&#29983;&#20135;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#20851;&#31995;&#65292;&#20294;&#30456;&#23545;&#36739;&#23569;&#30340;&#22240;&#32032;&#22312;NLP&#25216;&#26415;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#30740;&#31350;&#12290;&#34429;&#28982;&#24180;&#40836;&#21644;&#24615;&#21035;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#35206;&#30422;&#65292;Labov&#26368;&#21021;&#20851;&#27880;&#30340;&#31038;&#20250;&#32463;&#27982;&#38454;&#32423;&#21364;&#20960;&#20046;&#19981;&#34987;&#28041;&#21450;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25991;&#29486;&#65292;&#24182;&#21457;&#29616;&#21482;&#26377;20&#31687;&#35770;&#25991;&#25552;&#21040;&#20102;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35770;&#25991;&#20013;&#30340;&#22823;&#37096;&#20998;&#24182;&#27809;&#26377;&#25506;&#35752;&#36229;&#20986;&#25910;&#38598;&#27880;&#37322;&#32773;&#20154;&#21475;&#20449;&#24687;&#20043;&#22806;&#30340;&#38454;&#32423;&#12290;&#37492;&#20110;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#34987;NLP&#30740;&#31350;&#20154;&#21592;&#25805;&#20316;&#30340;&#38454;&#32423;&#23450;&#20041;&#65292;&#24182;&#20027;&#24352;&#21253;&#21547;&#35813;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03874v1 Announce Type: cross  Abstract: Since Labov's (1964) foundational work on the social stratification of language, linguistics has dedicated concerted efforts towards understanding the relationships between socio-demographic factors and language production and perception. Despite the large body of evidence identifying significant relationships between socio-demographic factors and language production, relatively few of these factors have been investigated in the context of NLP technology. While age and gender are well covered, Labov's initial target, socio-economic class, is largely absent. We survey the existing Natural Language Processing (NLP) literature and find that only 20 papers even mention socio-economic status. However, the majority of those papers do not engage with class beyond collecting information of annotator-demographics. Given this research lacuna, we provide a definition of class that can be operationalised by NLP researchers, and argue for including
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#35299;&#35868;&#20219;&#21153;AlgoPuzzleVQA&#65292;&#36890;&#36807;&#31639;&#27861;&#35868;&#39064;&#25361;&#25112;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#35270;&#35273;&#29702;&#35299;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#22797;&#26434;&#31639;&#27861;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#35780;&#20272;&#35270;&#35273;&#25968;&#25454;&#35299;&#37322;&#19982;&#31639;&#27861;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.03864</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#26159;&#35299;&#35868;&#22825;&#25165;&#65311;&#31639;&#27861;&#35868;&#39064;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#30340;&#20005;&#23803;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03864
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#35299;&#35868;&#20219;&#21153;AlgoPuzzleVQA&#65292;&#36890;&#36807;&#31639;&#27861;&#35868;&#39064;&#25361;&#25112;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#35270;&#35273;&#29702;&#35299;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#22797;&#26434;&#31639;&#27861;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#35780;&#20272;&#35270;&#35273;&#25968;&#25454;&#35299;&#37322;&#19982;&#31639;&#27861;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#35299;&#35868;&#20219;&#21153;&#65292;&#23558;&#20854;&#25918;&#22312;&#35270;&#35273;&#38382;&#31572;&#30340;&#32972;&#26223;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;AlgoPuzzleVQA&#65292;&#26088;&#22312;&#25361;&#25112;&#21644;&#35780;&#20272;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#38656;&#35201;&#35270;&#35273;&#29702;&#35299;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#22797;&#26434;&#31639;&#27861;&#25512;&#29702;&#30340;&#31639;&#27861;&#35868;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#28085;&#30422;&#24067;&#23572;&#36923;&#36753;&#12289;&#32452;&#21512;&#25968;&#23398;&#12289;&#22270;&#35770;&#12289;&#20248;&#21270;&#12289;&#25628;&#32034;&#31561;&#22810;&#31181;&#25968;&#23398;&#21644;&#31639;&#27861;&#20027;&#39064;&#30340;&#35868;&#39064;&#65292;&#26088;&#22312;&#35780;&#20272;&#35270;&#35273;&#25968;&#25454;&#35299;&#37322;&#19982;&#31639;&#27861;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#20154;&#31867;&#32534;&#20889;&#30340;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#25152;&#26377;&#30340;&#35868;&#39064;&#37117;&#26377;&#31934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20174;&#31639;&#27861;&#20013;&#25214;&#21040;&#65292;&#26080;&#38656;&#32321;&#29712;&#30340;&#20154;&#24037;&#35745;&#31639;&#12290;&#36825;&#30830;&#20445;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#22312;&#25512;&#29702;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#26041;&#38754;&#21487;&#20197;&#20219;&#24847;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03864v1 Announce Type: cross  Abstract: This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models in solving algorithmic puzzles that necessitate both visual understanding, language understanding, and complex algorithmic reasoning. We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic, combinatorics, graph theory, optimization, search, etc., aiming to evaluate the gap between visual data interpretation and algorithmic problem-solving skills. The dataset is generated automatically from code authored by humans. All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations. It ensures that our dataset can be scaled up arbitrarily in terms of reasoning complexity and dataset size. Our investi
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#31639;&#27861;&#65292;&#20197;&#21152;&#36895;&#27969;&#34892;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#37319;&#26679;&#22120;&#65292;&#25913;&#36827;&#20102;&#30830;&#23450;&#24615;&#37319;&#26679;&#22120;&#30340;&#25910;&#25947;&#36895;&#29575;&#33267;$O(1/{T}^2)$&#65292;&#25552;&#21319;&#20102;&#38543;&#26426;&#37319;&#26679;&#22120;&#30340;&#25910;&#25947;&#36895;&#29575;&#33267;$O(1/T)$&#12290;</title><link>https://arxiv.org/abs/2403.03852</link><description>&lt;p&gt;
&#21152;&#36895;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#65292;&#26377;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Accelerating Convergence of Score-Based Diffusion Models, Provably
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03852
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#31639;&#27861;&#65292;&#20197;&#21152;&#36895;&#27969;&#34892;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#37319;&#26679;&#22120;&#65292;&#25913;&#36827;&#20102;&#30830;&#23450;&#24615;&#37319;&#26679;&#22120;&#30340;&#25910;&#25947;&#36895;&#29575;&#33267;$O(1/{T}^2)$&#65292;&#25552;&#21319;&#20102;&#38543;&#26426;&#37319;&#26679;&#22120;&#30340;&#25910;&#25947;&#36895;&#29575;&#33267;$O(1/T)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32463;&#39564;&#24615;&#33021;&#65292;&#20294;&#36890;&#24120;&#30001;&#20110;&#22312;&#37319;&#26679;&#38454;&#27573;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#20989;&#25968;&#35780;&#20272;&#32780;&#23548;&#33268;&#37319;&#26679;&#36895;&#24230;&#36739;&#24930;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#19968;&#31995;&#21015;&#24037;&#20316;&#33268;&#21147;&#20110;&#21152;&#36895;&#25193;&#25955;&#29983;&#25104;&#24314;&#27169;&#65292;&#20294;&#21152;&#36895;&#25216;&#26415;&#30340;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#20005;&#37325;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#31639;&#27861;&#26469;&#21152;&#36895;&#27969;&#34892;&#30340;&#30830;&#23450;&#24615;&#65288;&#21363;DDIM&#65289;&#21644;&#38543;&#26426;&#65288;&#21363;DDPM&#65289;&#37319;&#26679;&#22120;&#12290;&#25105;&#20204;&#30340;&#21152;&#36895;&#30830;&#23450;&#24615;&#37319;&#26679;&#22120;&#20197;$O(1/{T}^2)$&#30340;&#36895;&#29575;&#25910;&#25947;&#65292;&#20854;&#20013;$T$&#20026;&#27493;&#25968;&#65292;&#25913;&#36827;&#20102;DDIM&#37319;&#26679;&#22120;&#30340;$O(1/T)$&#36895;&#29575;&#65307;&#32780;&#25105;&#20204;&#30340;&#21152;&#36895;&#38543;&#26426;&#37319;&#26679;&#22120;&#20197;$O(1/T)$&#30340;&#36895;&#29575;&#25910;&#25947;&#65292;&#20248;&#20110;DDPM&#37319;&#26679;&#22120;&#30340;$O(1/\sqrt{T})$&#36895;&#29575;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#35774;&#35745;&#21033;&#29992;&#20102;&#26356;&#39640;&#38454;&#36924;&#36817;&#30340;&#35265;&#35299;&#65292;&#24182;&#20855;&#26377;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#39640;&#38454;ODE&#27714;&#35299;&#22120;&#30340;&#30452;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03852v1 Announce Type: cross  Abstract: Score-based diffusion models, while achieving remarkable empirical performance, often suffer from low sampling speed, due to extensive function evaluations needed during the sampling phase. Despite a flurry of recent activities towards speeding up diffusion generative modeling in practice, theoretical underpinnings for acceleration techniques remain severely limited. In this paper, we design novel training-free algorithms to accelerate popular deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers. Our accelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the number of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our accelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the rate $O(1/\sqrt{T})$ for the DDPM sampler. The design of our algorithms leverages insights from higher-order approximation, and shares similar intuitions as popular high-order ODE solvers like 
&lt;/p&gt;</description></item><item><title>Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#65292;&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#24182;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#23637;&#29616;&#20986;&#23454;&#20363;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#65292;&#20026;&#23558;&#26469;&#30740;&#31350;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.03835</link><description>&lt;p&gt;
Cobweb&#65306;&#19968;&#31181;&#22686;&#37327;&#21644;&#20998;&#23618;&#24335;&#30340;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03835
&lt;/p&gt;
&lt;p&gt;
Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#65292;&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#24182;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#23637;&#29616;&#20986;&#23454;&#20363;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#65292;&#20026;&#23558;&#26469;&#30740;&#31350;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#19982;&#20854;&#20182;&#22686;&#37327;&#20998;&#31867;&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#21033;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Cobweb&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#65292;&#22914;&#22522;&#26412;&#27700;&#24179;&#12289;&#20856;&#22411;&#24615;&#21644;&#25159;&#24418;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#23545;Cobweb&#20316;&#20026;&#20154;&#31867;&#20998;&#31867;&#27169;&#22411;&#30340;&#26356;&#24191;&#27867;&#35780;&#20272;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#23427;&#30830;&#23450;&#20102;Cobweb&#19982;&#32463;&#20856;&#30340;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25928;&#24212;&#30340;&#19968;&#33268;&#24615;&#12290;&#36824;&#25506;&#35752;&#20102;Cobweb&#23637;&#29616;&#20986;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#26082;&#26377;&#23454;&#20363;&#21448;&#26377;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#23558;&#26469;&#30740;&#31350;Cobweb&#20316;&#20026;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#30340;&#32508;&#21512;&#27169;&#22411;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03835v1 Announce Type: cross  Abstract: Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure. Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects. However, a broader evaluation of Cobweb as a model of human categorization remains lacking. The current study addresses this gap. It establishes Cobweb's alignment with classical human category learning effects. It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model. These findings set the stage for future research on Cobweb as a comprehensive model of human category learning.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#25968;&#25454;&#38598;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#35302;&#25720;&#21160;&#24577;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#29992;&#25143;&#65292;&#26368;&#20855;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#26159;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#65288;SVC&#65289;&#65292;&#20854;&#24179;&#22343;&#20934;&#30830;&#29575;&#32422;&#20026;90%</title><link>https://arxiv.org/abs/2403.03832</link><description>&lt;p&gt;
&#24744;&#30340;&#35774;&#22791;&#21487;&#33021;&#27604;&#24744;&#26356;&#20102;&#35299;&#24744;&#33258;&#24049;&#8212;&#8212;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Your device may know you better than you know yourself -- continuous authentication on novel dataset using machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03832
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#25968;&#25454;&#38598;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#35302;&#25720;&#21160;&#24577;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#29992;&#25143;&#65292;&#26368;&#20855;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#26159;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#65288;SVC&#65289;&#65292;&#20854;&#24179;&#22343;&#20934;&#30830;&#29575;&#32422;&#20026;90%
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#34892;&#20026;&#29983;&#29289;&#29305;&#24449;&#36827;&#19968;&#27493;&#25506;&#35752;&#25345;&#32493;&#35748;&#35777;&#39046;&#22495;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;15&#20301;&#29992;&#25143;&#20351;&#29992;&#19977;&#26143;&#24179;&#26495;&#29609;&#8220;Minecraft&#8221;&#26102;&#30340;&#25163;&#21183;&#25968;&#25454;&#65292;&#27599;&#20301;&#29992;&#25143;&#25345;&#32493;15&#20998;&#38047;&#12290;&#21033;&#29992;&#36825;&#19968;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#12289;K-&#26368;&#36817;&#37051;&#31639;&#27861;&#65288;KNN&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#65288;SVC&#65289;&#65292;&#20197;&#30830;&#23450;&#29305;&#23450;&#29992;&#25143;&#25805;&#20316;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#26368;&#20855;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#26159;SVC&#65292;&#20854;&#24179;&#22343;&#20934;&#30830;&#29575;&#32422;&#20026;90%&#65292;&#34920;&#26126;&#35302;&#25720;&#21160;&#24577;&#33021;&#26377;&#25928;&#21306;&#20998;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#20351;&#20854;&#25104;&#20026;&#35748;&#35777;&#31995;&#32479;&#30340;&#21487;&#34892;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03832v1 Announce Type: new  Abstract: This research aims to further understanding in the field of continuous authentication using behavioral biometrics. We are contributing a novel dataset that encompasses the gesture data of 15 users playing Minecraft with a Samsung Tablet, each for a duration of 15 minutes. Utilizing this dataset, we employed machine learning (ML) binary classifiers, being Random Forest (RF), K-Nearest Neighbors (KNN), and Support Vector Classifier (SVC), to determine the authenticity of specific user actions. Our most robust model was SVC, which achieved an average accuracy of approximately 90%, demonstrating that touch dynamics can effectively distinguish users. However, further studies are needed to make it viable option for authentication systems
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#40736;&#26631;&#31227;&#21160;&#21160;&#24577;&#20316;&#20026;&#25345;&#32493;&#35748;&#35777;&#30340;&#19968;&#31181;&#19968;&#33268;&#24230;&#37327;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#29992;&#25143;&#34892;&#20026;&#65292;&#21457;&#29616;&#40736;&#26631;&#31227;&#21160;&#21160;&#24577;&#21487;&#20197;&#20316;&#20026;&#21487;&#38752;&#30340;&#35748;&#35777;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.03828</link><description>&lt;p&gt;
&#20174;&#28857;&#20987;&#21040;&#23433;&#20840;&#65306;&#36890;&#36807;&#40736;&#26631;&#21160;&#24577;&#35748;&#35777;&#30340;&#25345;&#32493;&#35748;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Clicks to Security: Investigating Continuous Authentication via Mouse Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#40736;&#26631;&#31227;&#21160;&#21160;&#24577;&#20316;&#20026;&#25345;&#32493;&#35748;&#35777;&#30340;&#19968;&#31181;&#19968;&#33268;&#24230;&#37327;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#29992;&#25143;&#34892;&#20026;&#65292;&#21457;&#29616;&#40736;&#26631;&#31227;&#21160;&#21160;&#24577;&#21487;&#20197;&#20316;&#20026;&#21487;&#38752;&#30340;&#35748;&#35777;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#23433;&#20840;&#39046;&#22495;&#65292;&#39640;&#25928;&#21487;&#38752;&#30340;&#29992;&#25143;&#35748;&#35777;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#31361;&#20986;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#40736;&#26631;&#31227;&#21160;&#21160;&#24577;&#20316;&#20026;&#25345;&#32493;&#35748;&#35777;&#30340;&#19968;&#31181;&#19968;&#33268;&#24230;&#37327;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#22312;&#20004;&#31181;&#25130;&#28982;&#19981;&#21516;&#30340;&#28216;&#25103;&#22330;&#26223;&#20013;&#30340;&#40736;&#26631;&#31227;&#21160;&#27169;&#24335;&#65292;&#8220;&#22242;&#38431;&#35201;&#22622;&#8221;&#21644;Poly Bridge&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#39640;&#24378;&#24230;&#21644;&#20302;&#24378;&#24230;UI&#20132;&#20114;&#20013;&#22266;&#26377;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#26412;&#30740;&#31350;&#36229;&#36234;&#20102;&#20256;&#32479;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#32463;&#36807;&#31934;&#24515;&#36873;&#25321;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#25429;&#25417;&#21644;&#35299;&#37322;&#29992;&#25143;&#34892;&#20026;&#24494;&#22937;&#20043;&#22788;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27491;&#22914;&#20854;&#20307;&#29616;&#22312;&#40736;&#26631;&#31227;&#21160;&#20013;&#30340;&#12290;&#36825;&#31181;&#22810;&#23618;&#38754;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#26356;&#21152;&#32454;&#33268;&#21644;&#20840;&#38754;&#22320;&#29702;&#35299;&#29992;&#25143;&#20114;&#21160;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#40736;&#26631;&#31227;&#21160;&#21160;&#24577;&#21487;&#20197;&#20316;&#20026;&#36830;&#32493;&#35748;&#35777;&#30340;&#21487;&#38752;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03828v1 Announce Type: new  Abstract: In the realm of computer security, the importance of efficient and reliable user authentication methods has become increasingly critical. This paper examines the potential of mouse movement dynamics as a consistent metric for continuous authentication. By analyzing user mouse movement patterns in two contrasting gaming scenarios, "Team Fortress" and Poly Bridge we investigate the distinctive behavioral patterns inherent in high-intensity and low-intensity UI interactions. The study extends beyond conventional methodologies by employing a range of machine learning models. These models are carefully selected to assess their effectiveness in capturing and interpreting the subtleties of user behavior as reflected in their mouse movements. This multifaceted approach allows for a more nuanced and comprehensive understanding of user interaction patterns. Our findings reveal that mouse movement dynamics can serve as a reliable indicator for cont
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MultiQ&#22522;&#20934;&#65292;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;LLMs&#22312;&#20854;&#39044;&#26399;&#20351;&#29992;&#33539;&#22260;&#20043;&#22806;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#33267;&#23569;&#26576;&#20123;&#35821;&#35328;&#33021;&#22815;&#24544;&#23454;&#21644;&#20934;&#30830;&#22320;&#36827;&#34892;&#22238;&#31572;&#12290;</title><link>https://arxiv.org/abs/2403.03814</link><description>&lt;p&gt;
&#29992;MultiQ&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MultiQ&#22522;&#20934;&#65292;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;LLMs&#22312;&#20854;&#39044;&#26399;&#20351;&#29992;&#33539;&#22260;&#20043;&#22806;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#33267;&#23569;&#26576;&#20123;&#35821;&#35328;&#33021;&#22815;&#24544;&#23454;&#21644;&#20934;&#30830;&#22320;&#36827;&#34892;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#20026;&#20840;&#29699;&#22823;&#22810;&#25968;&#38750;&#33521;&#35821;&#20351;&#29992;&#32773;&#25552;&#20379;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;LLMs&#20170;&#22825;&#65292;&#29305;&#21035;&#26159;&#24320;&#25918;&#30340;LLMs&#65292;&#36890;&#24120;&#20165;&#24847;&#20026;&#22312;&#33521;&#35821;&#65288;&#20363;&#22914;Llama2&#12289;Mistral&#65289;&#25110;&#23569;&#25968;&#20960;&#31181;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;&#20363;&#22914;Mixtral&#12289;Qwen&#65289;&#20013;&#20351;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#23384;&#22312;&#20351;&#29992;&#19978;&#30340;&#38480;&#21046;&#65292;&#20154;&#20204;&#20250;&#29992;&#35768;&#22810;&#19981;&#21516;&#30340;&#35821;&#35328;&#25552;&#31034;LLMs&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;LLMs&#22312;&#20854;&#39044;&#26399;&#20351;&#29992;&#33539;&#22260;&#20043;&#22806;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MultiQ&#65292;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#22522;&#26412;&#24320;&#25918;&#24335;&#38382;&#31572;&#30340;&#38134;&#26631;&#20934;&#22522;&#20934;&#65292;&#28085;&#30422;137&#31181;&#35821;&#35328;&#30340;27.4k&#20010;&#27979;&#35797;&#38382;&#39064;&#12290;&#36890;&#36807;MultiQ&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35821;&#35328;&#24544;&#23454;&#24230;&#65292;&#21363;&#27169;&#22411;&#26159;&#21542;&#20197;&#25552;&#31034;&#30340;&#35821;&#35328;&#22238;&#22797;&#65292;&#20197;&#21450;&#38382;&#39064;&#22238;&#31572;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#27979;&#35797;&#30340;&#25152;&#26377;LLMs&#23545;&#33267;&#23569;&#26576;&#20123;&#35821;&#35328;&#21709;&#24212;&#24471;&#24544;&#23454;&#21644;/&#25110;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03814v1 Announce Type: cross  Abstract: Large language models (LLMs) need to serve everyone, including a global majority of non-English speakers. However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent research shows that, despite limits in their intended use, people prompt LLMs in many different languages. Therefore, in this paper, we investigate the basic multilingual capabilities of state-of-the-art open LLMs beyond their intended use. For this purpose, we introduce MultiQ, a new silver standard benchmark for basic open-ended question answering with 27.4k test questions across a typologically diverse set of 137 languages. With MultiQ, we evaluate language fidelity, i.e.\ whether models respond in the prompted language, and question answering accuracy. All LLMs we test respond faithfully and/or accurately for at least some languages be
&lt;/p&gt;</description></item><item><title>ProbSAINT&#26159;&#19968;&#31181;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#26469;&#37327;&#21270;&#20854;&#20215;&#26684;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#25552;&#21319;&#25216;&#26415;&#30456;&#23218;&#32654;&#30340;&#20934;&#30830;&#28857;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.03812</link><description>&lt;p&gt;
ProbSAINT&#65306;&#27010;&#29575;&#34920;&#26684;&#22238;&#24402;&#29992;&#20110;&#20108;&#25163;&#36710;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
ProbSAINT: Probabilistic Tabular Regression for Used Car Pricing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03812
&lt;/p&gt;
&lt;p&gt;
ProbSAINT&#26159;&#19968;&#31181;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#26469;&#37327;&#21270;&#20854;&#20215;&#26684;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#25552;&#21319;&#25216;&#26415;&#30456;&#23218;&#32654;&#30340;&#20934;&#30830;&#28857;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#25163;&#36710;&#23450;&#20215;&#26159;&#27773;&#36710;&#34892;&#19994;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#21463;&#35768;&#22810;&#32463;&#27982;&#22240;&#32032;&#21644;&#24066;&#22330;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;&#38543;&#30528;&#22312;&#32447;&#24066;&#22330;&#30340;&#28608;&#22686;&#21644;&#20108;&#25163;&#36710;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#20934;&#30830;&#30340;&#23450;&#20215;&#23558;&#20351;&#20080;&#23478;&#21644;&#21334;&#23478;&#21463;&#30410;&#65292;&#30830;&#20445;&#20844;&#24179;&#20132;&#26131;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21521;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#21160;&#23450;&#20215;&#31639;&#27861;&#30340;&#36716;&#21464;&#38656;&#35201;&#29702;&#35299;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#29305;&#21035;&#26159;&#26631;&#35760;&#27169;&#22411;&#19981;&#30830;&#23450;&#30340;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#25991;&#29486;&#25552;&#20986;&#20351;&#29992;&#25552;&#21319;&#31639;&#27861;&#25110;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#26041;&#27861;&#36827;&#34892;&#36805;&#36895;&#21644;&#31934;&#20934;&#30340;&#20215;&#26684;&#39044;&#27979;&#65292;&#20294;&#29992;&#36825;&#20123;&#31639;&#27861;&#23553;&#35013;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#38754;&#20020;&#30528;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ProbSAINT&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#20854;&#20215;&#26684;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#21450;&#19982;&#26368;&#20808;&#36827;&#30340;&#25552;&#21319;&#25216;&#26415;&#30456;&#23218;&#32654;&#30340;&#20934;&#30830;&#28857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03812v1 Announce Type: cross  Abstract: Used car pricing is a critical aspect of the automotive industry, influenced by many economic factors and market dynamics. With the recent surge in online marketplaces and increased demand for used cars, accurate pricing would benefit both buyers and sellers by ensuring fair transactions. However, the transition towards automated pricing algorithms using machine learning necessitates the comprehension of model uncertainties, specifically the ability to flag predictions that the model is unsure about. Although recent literature proposes the use of boosting algorithms or nearest neighbor-based approaches for swift and precise price predictions, encapsulating model uncertainties with such algorithms presents a complex challenge. We introduce ProbSAINT, a model that offers a principled approach for uncertainty quantification of its price predictions, along with accurate point predictions that are comparable to state-of-the-art boosting tec
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21033;&#29992;&#20182;&#20204;&#30340;&#25511;&#21046;&#33258;&#20449;&#24515;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#25512;&#23548;&#20986;&#20102;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#33258;&#20449;&#24230;&#30340;&#25968;&#23398;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;</title><link>https://arxiv.org/abs/2403.03808</link><description>&lt;p&gt;
&#33258;&#20449;&#24847;&#35782;&#20915;&#31574;&#19982;&#25511;&#21046;&#22312;&#24037;&#20855;&#36873;&#25321;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Confidence-Aware Decision-Making and Control for Tool Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03808
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21033;&#29992;&#20182;&#20204;&#30340;&#25511;&#21046;&#33258;&#20449;&#24515;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#25512;&#23548;&#20986;&#20102;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#33258;&#20449;&#24230;&#30340;&#25968;&#23398;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#21453;&#24605;&#25105;&#20204;&#30340;&#34920;&#29616;&#65288;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#33258;&#20449;&#31243;&#24230;&#65289;&#22312;&#36827;&#34892;&#20219;&#21153;&#20043;&#21069;&#23545;&#20110;&#20915;&#31574;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#27604;&#22914;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#24037;&#20855;&#25110;&#36873;&#25321;&#26368;&#20339;&#39550;&#39542;&#36335;&#32447;&#12290;&#34429;&#28982;&#36825;&#31181;&#24847;&#35782;&#24418;&#24335;&#8212;&#8212;&#24605;&#32771;&#25105;&#20204;&#30340;&#34920;&#29616;&#25110;&#20803;&#35748;&#30693;&#34920;&#29616;&#8212;&#8212;&#22312;&#20154;&#31867;&#20013;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#65292;&#20294;&#26426;&#22120;&#20154;&#20173;&#28982;&#32570;&#20047;&#36825;&#31181;&#35748;&#30693;&#33021;&#21147;&#12290;&#36825;&#31181;&#21453;&#24605;&#30417;&#25511;&#21487;&#20197;&#22686;&#24378;&#20182;&#20204;&#30340;&#20307;&#29616;&#20915;&#31574;&#33021;&#21147;&#12289;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21033;&#29992;&#20182;&#20204;&#30340;&#25511;&#21046;&#33258;&#20449;&#24515;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#30340;&#20915;&#23450;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#33258;&#20449;&#24230;&#30340;&#25968;&#23398;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#65288;&#21363;&#25511;&#21046;&#21160;&#20316;&#30340;&#21518;&#39564;&#36870;&#21327;&#26041;&#24046;&#65289;&#12290;&#36825;&#31181;&#25511;&#21046;&#33258;&#20449;&#24230;&#26080;&#32541;&#22320;&#38598;&#25104;&#22312;&#19968;&#20010;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#30340;&#23458;&#35266;&#20989;&#25968;&#20013;&#65292;&#24179;&#34913;&#20102;&#65306;i&#65289;&#20219;&#21153;&#23436;&#25104;&#30340;&#24615;&#33021;&#65292;ii&#65289;&#25511;&#21046;&#21162;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03808v1 Announce Type: cross  Abstract: Self-reflecting about our performance (e.g., how confident we are) before doing a task is essential for decision making, such as selecting the most suitable tool or choosing the best route to drive. While this form of awareness -- thinking about our performance or metacognitive performance -- is well-known in humans, robots still lack this cognitive ability. This reflective monitoring can enhance their embodied decision power, robustness and safety. Here, we take a step in this direction by introducing a mathematical framework that allows robots to use their control self-confidence to make better-informed decisions. We derive a mathematical closed-form expression for control confidence for dynamic systems (i.e., the posterior inverse covariance of the control action). This control confidence seamlessly integrates within an objective function for decision making, that balances the: i) performance for task completion, ii) control effort,
&lt;/p&gt;</description></item><item><title>KG-TREAT&#26694;&#26550;&#36890;&#36807;&#21327;&#21516;&#24739;&#32773;&#25968;&#25454;&#21644;&#30693;&#35782;&#22270;&#35889;&#65292;&#24341;&#20837;&#21452;&#37325;&#20851;&#27880;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#28145;&#24230;&#21452;&#23618;&#27880;&#24847;&#21147;&#21327;&#21516;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27835;&#30103;&#30456;&#20851;&#22240;&#32032;&#21644;&#32467;&#26524;&#30456;&#20851;&#22240;&#32032;&#30340;&#29420;&#31435;&#32534;&#30721;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03791</link><description>&lt;p&gt;
KG-TREAT: &#36890;&#36807;&#22312;&#24739;&#32773;&#25968;&#25454;&#21644;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#36827;&#34892;&#21327;&#21516;&#20316;&#29992;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03791
&lt;/p&gt;
&lt;p&gt;
KG-TREAT&#26694;&#26550;&#36890;&#36807;&#21327;&#21516;&#24739;&#32773;&#25968;&#25454;&#21644;&#30693;&#35782;&#22270;&#35889;&#65292;&#24341;&#20837;&#21452;&#37325;&#20851;&#27880;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#28145;&#24230;&#21452;&#23618;&#27880;&#24847;&#21147;&#21327;&#21516;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27835;&#30103;&#30456;&#20851;&#22240;&#32032;&#21644;&#32467;&#26524;&#30456;&#20851;&#22240;&#32032;&#30340;&#29420;&#31435;&#32534;&#30721;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#65288;TEE&#65289;&#26159;&#30830;&#23450;&#21508;&#31181;&#27835;&#30103;&#23545;&#24739;&#32773;&#32467;&#26524;&#24433;&#21709;&#30340;&#20219;&#21153;&#12290;KG-TREAT&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35266;&#23519;&#24615;&#24739;&#32773;&#25968;&#25454;&#19982;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#36827;&#34892;&#21327;&#21516;&#20197;&#22686;&#24378;TEE&#65292;&#20197;&#35299;&#20915;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#20381;&#36182;&#21644;&#31232;&#30095;&#21644;&#39640;&#32500;&#35266;&#23519;&#24615;&#24739;&#32773;&#25968;&#25454;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03791v1 Announce Type: cross  Abstract: Treatment effect estimation (TEE) is the task of determining the impact of various treatments on patient outcomes. Current TEE methods fall short due to reliance on limited labeled data and challenges posed by sparse and high-dimensional observational patient data. To address the challenges, we introduce a novel pre-training and fine-tuning framework, KG-TREAT, which synergizes large-scale observational patient data with biomedical knowledge graphs (KGs) to enhance TEE. Unlike previous approaches, KG-TREAT constructs dual-focus KGs and integrates a deep bi-level attention synergy method for in-depth information fusion, enabling distinct encoding of treatment-covariate and outcome-covariate relationships. KG-TREAT also incorporates two pre-training tasks to ensure a thorough grounding and contextualization of patient data and KGs. Evaluation on four downstream TEE tasks shows KG-TREAT's superiority over existing methods, with an average
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#24320;&#28304;&#24037;&#20855;&#31995;&#32479;&#65288;OpenNAS&#65289;&#65292;&#36890;&#36807;&#37319;&#29992;&#31890;&#23376;&#32676;&#21644;&#34433;&#32676;&#20248;&#21270;&#31561;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03781</link><description>&lt;p&gt;
&#20351;&#29992;&#31890;&#23376;&#32676;&#21644;&#34433;&#32676;&#20248;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search using Particle Swarm and Ant Colony Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#24320;&#28304;&#24037;&#20855;&#31995;&#32479;&#65288;OpenNAS&#65289;&#65292;&#36890;&#36807;&#37319;&#29992;&#31890;&#23376;&#32676;&#21644;&#34433;&#32676;&#20248;&#21270;&#31561;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#35768;&#22810;&#36229;&#21442;&#25968;&#65292;&#24517;&#39035;&#36873;&#25321;&#20854;&#32467;&#26500;&#12290;&#36825;&#23545;&#20110;&#21021;&#23398;&#32773;&#26469;&#35828;&#26159;&#19968;&#20010;&#27785;&#37325;&#30340;&#36127;&#25285;&#65292;&#20182;&#20204;&#38656;&#35201;&#36873;&#25321;&#21738;&#31181;&#32467;&#26500;&#20197;&#21450;&#20998;&#37197;&#32473;&#21442;&#25968;&#30340;&#20540;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#40664;&#35748;&#30340;&#36229;&#21442;&#25968;&#21644;&#32467;&#26500;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#31181;&#32467;&#26500;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#30340;&#36807;&#31243;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22823;&#37327;&#36825;&#26679;&#30340;&#32467;&#26500;&#12290;&#20316;&#20026;&#26412;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#24050;&#24320;&#21457;&#20102;&#19968;&#20010;&#38598;&#25104;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#24320;&#28304;&#24037;&#20855;&#31995;&#32479;&#65288;OpenNAS&#65289;&#12290;OpenNAS&#25509;&#21463;&#20219;&#20309;&#28784;&#24230;&#25110;RGB&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#26681;&#25454;&#19968;&#31995;&#21015;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#20351;&#29992;AutoKeras&#65292;&#36801;&#31227;&#23398;&#20064;&#25110;Swarm Intelligence (SI)&#26041;&#27861;&#29983;&#25104;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03781v1 Announce Type: cross  Abstract: Neural network models have a number of hyperparameters that must be chosen along with their architecture. This can be a heavy burden on a novice user, choosing which architecture and what values to assign to parameters. In most cases, default hyperparameters and architectures are used. Significant improvements to model accuracy can be achieved through the evaluation of multiple architectures. A process known as Neural Architecture Search (NAS) may be applied to automatically evaluate a large number of such architectures. A system integrating open source tools for Neural Architecture Search (OpenNAS), in the classification of images, has been developed as part of this research. OpenNAS takes any dataset of grayscale, or RBG images, and generates Convolutional Neural Network (CNN) architectures based on a range of metaheuristics using either an AutoKeras, a transfer learning or a Swarm Intelligence (SI) approach. Particle Swarm Optimizat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.03777</link><description>&lt;p&gt;
ENOT&#65306;&#26399;&#26395;&#22238;&#24402;&#29992;&#20110;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#30340;&#24555;&#36895;&#21644;&#20934;&#30830;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03777
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#20849;&#36717;&#21183;&#27491;&#21017;&#21270;&#33021;&#22815;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#12290;&#29616;&#26377;NOT&#27714;&#35299;&#22120;&#30340;&#20027;&#35201;&#29942;&#39048;&#22312;&#20110;&#25214;&#21040;&#20849;&#36717;&#31639;&#23376;&#65288;&#21363;c-transform&#65289;&#30340;&#25509;&#36817;&#31934;&#30830;&#36817;&#20284;&#30340;&#36807;&#31243;&#65292;&#36825;&#35201;&#20040;&#36890;&#36807;&#20248;&#21270;&#26368;&#23567;-&#26368;&#22823;&#30446;&#26631;&#65292;&#35201;&#20040;&#36890;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#23545;&#21021;&#22987;&#36817;&#20284;&#39044;&#27979;&#30340;&#31934;&#32454;&#35843;&#25972;&#26469;&#23436;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#12289;&#22312;&#26399;&#26395;&#22238;&#24402;&#24418;&#24335;&#19978;&#24378;&#21046;&#36866;&#24212;&#24615;&#26465;&#20214;&#20110;&#23398;&#20064;&#23545;&#20598;&#21183;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#21270;&#25439;&#22833;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#21487;&#33021;&#20849;&#36717;&#21183;&#20998;&#24067;&#30340;&#19978;&#38480;&#20272;&#35745;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#65292;&#28040;&#38500;&#20102;&#23545;&#39069;&#22806;&#24191;&#27867;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03777v1 Announce Type: cross  Abstract: We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our me
&lt;/p&gt;</description></item><item><title>DeepCRE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#22312;&#24739;&#32773;&#32423;&#21035;CRE&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;17.7&#65285;&#65292;&#22312;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#24182;&#25104;&#21151;&#30830;&#23450;&#20102;&#20845;&#20010;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.03768</link><description>&lt;p&gt;
DeepCRE&#65306;&#21033;&#29992;&#23574;&#31471;&#35745;&#31639;&#27169;&#22411;&#25913;&#38761;&#33647;&#29289;&#30740;&#21457;
&lt;/p&gt;
&lt;p&gt;
DeepCRE: Revolutionizing Drug R&amp;D with Cutting-Edge Computational Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03768
&lt;/p&gt;
&lt;p&gt;
DeepCRE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#22312;&#24739;&#32773;&#32423;&#21035;CRE&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;17.7&#65285;&#65292;&#22312;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#24182;&#25104;&#21151;&#30830;&#23450;&#20102;&#20845;&#20010;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03768v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#33647;&#29289;&#24320;&#21457;&#39046;&#22495;&#21644;&#27835;&#30103;&#24212;&#29992;&#39046;&#22495;&#37117;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#27835;&#30103;&#39046;&#22495;&#38656;&#35201;&#26356;&#22810;&#30340;&#27835;&#30103;&#36873;&#25321;&#65292;&#21516;&#26102;&#22823;&#37327;&#26377;&#21069;&#26223;&#30340;&#20020;&#24202;&#21069;&#33647;&#29289;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#22833;&#36133;&#12290;&#19968;&#20010;&#21407;&#22240;&#26159;&#22312;&#33647;&#29289;&#24320;&#21457;&#30340;&#21518;&#26399;&#38454;&#27573;&#20132;&#21449;&#33647;&#29289;&#21453;&#24212;&#35780;&#20272;&#65288;CRE&#65289;&#30340;&#19981;&#36275;&#12290;&#23613;&#31649;&#35745;&#31639;&#26426;&#27169;&#25311;&#30340;CRE&#27169;&#22411;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23398;&#35201;&#20040;&#23616;&#38480;&#20110;&#26089;&#26399;&#24320;&#21457;&#38454;&#27573;&#65292;&#35201;&#20040;&#32570;&#20047;&#23545;&#20840;&#38754;CRE&#20998;&#26512;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeepCRE&#30340;&#26032;&#22411;&#35745;&#31639;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;DeepCRE&#22312;&#25512;&#21160;&#27835;&#30103;&#21457;&#29616;&#21644;&#21457;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;DeepCRE&#36890;&#36807;&#23454;&#29616;&#24739;&#32773;&#32423;&#21035;CRE&#24179;&#22343;&#24615;&#33021;&#25552;&#39640;17.7\%&#65292;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;DeepCRE&#24050;&#32463;&#30830;&#23450;&#20102;&#20845;&#20010;&#26174;&#31034;&#20986;&#26126;&#26174;&#26356;&#22823;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03768v1 Announce Type: new  Abstract: The field of pharmaceutical development and therapeutic application both face substantial challenges. Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development. Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis. Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\% in patient-level CRE, and a 5-fold increase in indication-level CRE. Furthermore, DeepCRE has identified six drug candidates that show significantly greater ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#26032;&#38395;&#25688;&#35201;&#20013;&#24187;&#35273;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;absinth&#65292;&#25506;&#35752;&#20102;LLMs&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.03750</link><description>&lt;p&gt;
&#24503;&#35821;&#20063;&#20135;&#29983;&#24187;&#35273;&#65281;&#20351;&#29992;Absinth&#25968;&#25454;&#38598;&#26816;&#27979;&#26032;&#38395;&#25688;&#35201;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#26032;&#38395;&#25688;&#35201;&#20013;&#24187;&#35273;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;absinth&#65292;&#25506;&#35752;&#20102;LLMs&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#20135;&#29983;&#20449;&#24687;&#24187;&#35273;&#30340;&#38382;&#39064;&#65292;&#36825;&#22312;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#22810;&#35821;&#35328;&#26041;&#27861;&#32570;&#20047;&#24503;&#35821;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;absinth&#65292;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#26032;&#38395;&#25688;&#35201;&#20013;&#24187;&#35273;&#26816;&#27979;&#30340;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#35752;&#20102;&#26032;&#22411;&#24320;&#28304;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#35774;&#32622;&#12290;&#25105;&#20204;&#24320;&#28304;&#24182;&#21457;&#24067;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03750v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has led to remarkable progress on a wide range of natural language processing tasks. Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document. Previous research addresses the challenging task of detecting hallucinations in the output (i.e. inconsistency detection) in order to evaluate the faithfulness of the generated summaries. However, these works primarily focus on English and recent multilingual approaches lack German data. This work presents absinth, a manually annotated dataset for hallucination detection in German news summarization and explores the capabilities of novel open-source LLMs on this task in both fine-tuning and in-context learning settings. We open-source and releas
&lt;/p&gt;</description></item><item><title>&#23545;&#21307;&#23398;LLMs&#36827;&#34892;&#20102;&#39318;&#27425;&#23433;&#20840;&#35780;&#20272;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23450;&#20041;&#21307;&#23398;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#65292;&#24320;&#21457;&#20102;&#26377;&#23475;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#21307;&#23398;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#23637;&#31034;&#20102;&#24494;&#35843;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.03744</link><description>&lt;p&gt;
&#20026;&#21307;&#33647;&#39046;&#22495;&#25171;&#36896;&#23433;&#20840;&#21644;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Safe and Aligned Large Language Models for Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03744
&lt;/p&gt;
&lt;p&gt;
&#23545;&#21307;&#23398;LLMs&#36827;&#34892;&#20102;&#39318;&#27425;&#23433;&#20840;&#35780;&#20272;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23450;&#20041;&#21307;&#23398;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#65292;&#24320;&#21457;&#20102;&#26377;&#23475;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#21307;&#23398;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#23637;&#31034;&#20102;&#24494;&#35843;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#27491;&#22312;&#20197;&#24778;&#20154;&#30340;&#36895;&#24230;&#36827;&#27493;&#65292;&#21363;&#20351;&#26159;&#23427;&#20204;&#30340;&#24320;&#21457;&#32773;&#20063;&#23545;&#23427;&#20204;&#30340;&#28508;&#21147;&#21644;&#39118;&#38505;&#30340;&#28145;&#24230;&#24863;&#21040;&#22256;&#24785;&#12290;&#23613;&#31649;&#24050;&#32463;&#37319;&#21462;&#20102;&#21021;&#27493;&#27493;&#39588;&#35780;&#20272;&#36890;&#29992;&#30693;&#35782;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#24369;&#28857;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23613;&#31649;&#22312;&#20010;&#20154;&#20581;&#24247;&#21644;&#23433;&#20840;&#12289;&#20844;&#20849;&#20581;&#24247;&#21644;&#23433;&#20840;&#20197;&#21450;&#20154;&#26435;&#26041;&#38754;&#23384;&#22312;&#39118;&#38505;&#65292;&#21307;&#23398;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#23578;&#26410;&#24471;&#21040;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#21307;&#23398;LLMs&#30340;&#39318;&#27425;&#23433;&#20840;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21307;&#23398;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#30340;&#23450;&#20041;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26377;&#23475;&#21307;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;LLM&#30340;&#21307;&#23398;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#35780;&#20272;&#20102;&#21307;&#23398;LLMs&#30340;&#36890;&#29992;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#23637;&#31034;&#20102;&#24494;&#35843;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26356;&#24191;&#27867;&#30340;&#12289;&#22823;&#35268;&#27169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03744v1 Announce Type: new  Abstract: The capabilities of large language models (LLMs) have been progressing at a breathtaking speed, leaving even their own developers grappling with the depth of their potential and risks. While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses, to our knowledge, the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights. To this end, we carry out the first safety evaluation for medical LLMs. Specifically, we set forth a definition of medical safety and alignment for medical artificial intelligence systems, develop a dataset of harmful medical questions to evaluate the medical safety and alignment of an LLM, evaluate both general and medical safety and alignment of medical LLMs, demonstrate fine-tuning as an effective mitigation strategy, and discuss broader, large-scale appr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUPClust&#30340;&#26032;&#22411;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35782;&#21035;&#31867;&#21035;&#20043;&#38388;&#30340;&#20915;&#31574;&#36793;&#30028;&#19978;&#30340;&#28857;&#65292;&#36890;&#36807;&#26631;&#35760;&#36825;&#20123;&#28857;&#26469;&#20248;&#21270;&#27169;&#22411;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03741</link><description>&lt;p&gt;
SUPClust: &#36793;&#30028;&#22788;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SUPClust: Active Learning at the Boundaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03741
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUPClust&#30340;&#26032;&#22411;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35782;&#21035;&#31867;&#21035;&#20043;&#38388;&#30340;&#20915;&#31574;&#36793;&#30028;&#19978;&#30340;&#28857;&#65292;&#36890;&#36807;&#26631;&#35760;&#36825;&#20123;&#28857;&#26469;&#20248;&#21270;&#27169;&#22411;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#22312;&#33719;&#21462;&#26631;&#35760;&#25968;&#25454;&#26114;&#36149;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUPClust&#30340;&#26032;&#22411;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35782;&#21035;&#31867;&#21035;&#20043;&#38388;&#30340;&#20915;&#31574;&#36793;&#30028;&#19978;&#30340;&#28857;&#12290;&#36890;&#36807;&#38024;&#23545;&#36825;&#20123;&#28857;&#65292;SUPClust&#26088;&#22312;&#25910;&#38598;&#23545;&#20110;&#31934;&#32454;&#21270;&#27169;&#22411;&#23545;&#22797;&#26434;&#20915;&#31574;&#21306;&#22495;&#30340;&#39044;&#27979;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#35760;&#36825;&#20123;&#28857;&#20250;&#23548;&#33268;&#24378;&#22823;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#21363;&#20351;&#22312;&#23384;&#22312;&#24378;&#28872;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#35266;&#23519;&#21040;&#20102;&#36825;&#31181;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03741v1 Announce Type: cross  Abstract: Active learning is a machine learning paradigm designed to optimize model performance in a setting where labeled data is expensive to acquire. In this work, we propose a novel active learning method called SUPClust that seeks to identify points at the decision boundary between classes. By targeting these points, SUPClust aims to gather information that is most informative for refining the model's prediction of complex decision regions. We demonstrate experimentally that labeling these points leads to strong model performance. This improvement is observed even in scenarios characterized by strong class imbalance.
&lt;/p&gt;</description></item><item><title>A&amp;B BNN &#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#21152;&#21644;&#20301;&#25805;&#20316;&#30340;&#30828;&#20214;&#21451;&#22909;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#25513;&#30721;&#23618;&#21644;&#37327;&#21270; RPReLU &#32467;&#26500;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#22312;CIFA&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.03739</link><description>&lt;p&gt;
A&amp;B BNN: A&amp;B BNN&#65306;&#20165;&#20351;&#29992;&#21152;&#21644;&#20301;&#25805;&#20316;&#30340;&#30828;&#20214;&#21451;&#22909;&#30340;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A&amp;B BNN: Add&amp;Bit-Operation-Only Hardware-Friendly Binary Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03739
&lt;/p&gt;
&lt;p&gt;
A&amp;B BNN &#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#21152;&#21644;&#20301;&#25805;&#20316;&#30340;&#30828;&#20214;&#21451;&#22909;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#25513;&#30721;&#23618;&#21644;&#37327;&#21270; RPReLU &#32467;&#26500;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#22312;CIFA&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;1&#20301;&#37327;&#21270;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#26469;&#20943;&#23569;&#27169;&#22411;&#30340;&#23384;&#20648;&#38656;&#27714;&#21644;&#35745;&#31639;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#20808;&#36827;&#30340;&#20108;&#20540;&#26550;&#26500;&#20173;&#28982;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#20302;&#25928;&#19988;&#23545;&#30828;&#20214;&#19981;&#21451;&#22909;&#30340;&#20840;&#31934;&#24230;&#20056;&#27861;&#25805;&#20316;&#12290;A&amp;B BNN &#25552;&#20986;&#20102;&#30452;&#25509;&#31227;&#38500;&#20256;&#32479; BNN &#20013;&#30340;&#37096;&#20998;&#20056;&#27861;&#25805;&#20316;&#65292;&#24182;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#20301;&#25805;&#20316;&#26367;&#25442;&#21097;&#20313;&#37096;&#20998;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#26080;&#24402;&#19968;&#21270;&#32593;&#32476;&#26550;&#26500;&#30340;&#25513;&#30721;&#23618;&#21644;&#37327;&#21270; RPReLU &#32467;&#26500;&#12290;&#25513;&#30721;&#23618;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992; BNN &#30340;&#20869;&#22312;&#29305;&#24449;&#20197;&#21450;&#31616;&#21333;&#30340;&#25968;&#23398;&#21464;&#25442;&#22312;&#25512;&#26029;&#26399;&#38388;&#23558;&#20854;&#31227;&#38500;&#65292;&#20197;&#36991;&#20813;&#30456;&#20851;&#30340;&#20056;&#27861;&#25805;&#20316;&#12290;&#37327;&#21270; RPReLU &#32467;&#26500;&#36890;&#36807;&#23558;&#20854;&#26012;&#29575;&#38480;&#21046;&#20026;2&#30340;&#25972;&#25968;&#24130;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#20301;&#25805;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;CIFA&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;92.30%&#12289;69.35%&#21644;66.89%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03739v1 Announce Type: cross  Abstract: Binary neural networks utilize 1-bit quantized weights and activations to reduce both the model's storage demands and computational burden. However, advanced binary architectures still incorporate millions of inefficient and nonhardware-friendly full-precision multiplication operations. A&amp;B BNN is proposed to directly remove part of the multiplication operations in a traditional BNN and replace the rest with an equal number of bit operations, introducing the mask layer and the quantized RPReLU structure based on the normalizer-free network architecture. The mask layer can be removed during inference by leveraging the intrinsic characteristics of BNN with straightforward mathematical transformations to avoid the associated multiplication operations. The quantized RPReLU structure enables more efficient bit operations by constraining its slope to be integer powers of 2. Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFA
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#22330;&#26223;&#65292;&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#32593;&#32476;&#26550;&#26500;&#65292;&#21516;&#26102;&#23398;&#20064;&#23545;&#35937;&#20998;&#21106;&#12289;3D&#20301;&#32622;&#25512;&#26029;&#21644;&#28145;&#24230;&#24863;&#30693;&#65292;&#20174;&#32780;&#20197;&#31867;&#20284;&#20154;&#31867;&#23156;&#20799;&#30340;&#32422;&#26463;&#26465;&#20214;&#23398;&#20064;&#29289;&#20307;&#30340;&#34920;&#31034;&#26041;&#24335;</title><link>https://arxiv.org/abs/2403.03730</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#23398;&#20064;&#19977;&#32500;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning 3D object-centric representation through prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03730
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#22330;&#26223;&#65292;&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#32593;&#32476;&#26550;&#26500;&#65292;&#21516;&#26102;&#23398;&#20064;&#23545;&#35937;&#20998;&#21106;&#12289;3D&#20301;&#32622;&#25512;&#26029;&#21644;&#28145;&#24230;&#24863;&#30693;&#65292;&#20174;&#32780;&#20197;&#31867;&#20284;&#20154;&#31867;&#23156;&#20799;&#30340;&#32422;&#26463;&#26465;&#20214;&#23398;&#20064;&#29289;&#20307;&#30340;&#34920;&#31034;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20154;&#31867;&#26680;&#24515;&#30693;&#35782;&#30340;&#19968;&#37096;&#20998;&#65292;&#23545;&#35937;&#30340;&#34920;&#31034;&#26159;&#25903;&#25345;&#39640;&#23618;&#27010;&#24565;&#21644;&#31526;&#21495;&#25512;&#29702;&#30340;&#24515;&#29702;&#34920;&#31034;&#30340;&#22522;&#26412;&#26500;&#20214;&#12290;&#23613;&#31649;&#20154;&#31867;&#33021;&#22815;&#22312;3D&#29615;&#22659;&#20013;&#26080;&#38656;&#30417;&#30563;&#22320;&#24863;&#30693;&#23545;&#35937;&#65292;&#20294;&#32570;&#20047;&#33021;&#22815;&#20197;&#31867;&#20284;&#20110;&#20154;&#31867;&#23156;&#20799;&#38754;&#20020;&#30340;&#30456;&#20284;&#32422;&#26463;&#26465;&#20214;&#23398;&#20064;&#30456;&#21516;&#33021;&#21147;&#30340;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21516;&#26102;&#23398;&#20064;&#20197;&#19979;&#33021;&#21147;&#65306;1) &#20174;&#31163;&#25955;&#22270;&#20687;&#20013;&#20998;&#21106;&#23545;&#35937;&#65292;2) &#25512;&#26029;&#23427;&#20204;&#30340;3D&#20301;&#32622;&#65292;&#20197;&#21450;3) &#24863;&#30693;&#28145;&#24230;&#65292;&#32780;&#20165;&#20351;&#29992;&#20102;&#30452;&#25509;&#21487;&#29992;&#20110;&#22823;&#33041;&#35757;&#32451;&#25968;&#25454;&#65292;&#21363;&#22270;&#20687;&#24207;&#21015;&#21644;&#33258;&#25105;&#36816;&#21160;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#23545;&#35937;&#35270;&#20026;&#35270;&#35273;&#36755;&#20837;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#22823;&#33041;&#21033;&#29992;&#36825;&#20123;&#21407;&#22240;&#20570;&#20986;&#23545;&#26410;&#26469;&#22330;&#26223;&#30340;&#26377;&#25928;&#39044;&#27979;&#12290;&#36825;&#23548;&#33268;&#23545;&#35937;&#34920;&#31034;&#20316;&#20026;&#23398;&#20064;&#39044;&#27979;&#30340;&#22522;&#26412;&#21103;&#20135;&#21697;&#34987;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03730v1 Announce Type: cross  Abstract: As part of human core knowledge, the representation of objects is the building block of mental representation that supports high-level concepts and symbolic reasoning. While humans develop the ability of perceiving objects situated in 3D environments without supervision, models that learn the same set of abilities with similar constraints faced by human infants are lacking. Towards this end, we developed a novel network architecture that simultaneously learns to 1) segment objects from discrete images, 2) infer their 3D locations, and 3) perceive depth, all while using only information directly available to the brain as training data, namely: sequences of images and self-motion. The core idea is treating objects as latent causes of visual input which the brain uses to make efficient predictions of future scenes. This results in object representations being learned as an essential byproduct of learning to predict.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;TCM&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#25104;&#21151;&#32467;&#21512;&#20102;&#22810;&#26679;&#24615;&#37319;&#26679;&#21644;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#27700;&#24179;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.03728</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#24357;&#21512;&#22810;&#26679;&#24615;&#19982;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03728
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;TCM&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#25104;&#21151;&#32467;&#21512;&#20102;&#22810;&#26679;&#24615;&#37319;&#26679;&#21644;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#27700;&#24179;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#38598;&#25104;&#22522;&#20110;&#22810;&#26679;&#24615;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;TCM&#30340;&#31616;&#21333;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21487;&#20197;&#32531;&#35299;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#25968;&#25454;&#27700;&#24179;&#19978;&#20445;&#25345;&#24378;&#22823;&#24615;&#33021;&#12290;&#36890;&#36807;&#39318;&#20808;&#24212;&#29992;TypiClust&#36827;&#34892;&#22810;&#26679;&#24615;&#37319;&#26679;&#65292;&#38543;&#21518;&#36807;&#28193;&#21040;&#20351;&#29992;Margin&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TCM&#22312;&#20302;&#25968;&#25454;&#21644;&#39640;&#25968;&#25454;&#24773;&#20917;&#19979;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03728v1 Announce Type: cross  Abstract: This study addresses the integration of diversity-based and uncertainty-based sampling strategies in active learning, particularly within the context of self-supervised pre-trained models. We introduce a straightforward heuristic called TCM that mitigates the cold start problem while maintaining strong performance across various data levels. By initially applying TypiClust for diversity sampling and subsequently transitioning to uncertainty sampling with Margin, our approach effectively combines the strengths of both strategies. Our experiments demonstrate that TCM consistently outperforms existing methods across various datasets in both low and high data regimes.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;DiMA&#27169;&#22411;&#65292;&#22312;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#25193;&#25955;&#26469;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#65292;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#26469;&#37327;&#21270;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03726</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#24207;&#21015;&#29983;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Diffusion on language model embeddings for protein sequence generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03726
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;DiMA&#27169;&#22411;&#65292;&#22312;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#25193;&#25955;&#26469;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#65292;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#26469;&#37327;&#21270;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#35774;&#35745;&#38656;&#35201;&#23545;&#34507;&#30333;&#36136;&#23431;&#23449;&#22266;&#26377;&#22797;&#26434;&#24615;&#30340;&#28145;&#20837;&#20102;&#35299;&#12290;&#23613;&#31649;&#35768;&#22810;&#24037;&#20316;&#20542;&#21521;&#20110;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#25110;&#19987;&#27880;&#20110;&#29305;&#23450;&#34507;&#30333;&#36136;&#23478;&#26063;&#65292;&#20294;&#26080;&#26465;&#20214;&#29983;&#25104;&#30340;&#22522;&#30784;&#20219;&#21153;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#21644;&#37325;&#35270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#36825;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#24341;&#20837;&#20102;DiMA&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#20174;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;ESM-2&#34893;&#29983;&#30340;&#23884;&#20837;&#36827;&#34892;&#36830;&#32493;&#25193;&#25955;&#20197;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#30340;&#27169;&#22411;&#12290;DiMA&#36229;&#36234;&#20102;&#21253;&#25324;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#21644;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#22312;&#20869;&#30340;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#23450;&#37327;&#22320;&#35828;&#26126;&#20102;&#23548;&#33268;&#20854;&#21331;&#36234;&#24615;&#33021;&#30340;&#35774;&#35745;&#36873;&#25321;&#25152;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25351;&#26631;&#36328;&#22810;&#31181;&#24418;&#24335;&#24191;&#27867;&#35780;&#20272;&#29983;&#25104;&#24207;&#21015;&#30340;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#12289;&#20998;&#24067;&#30456;&#20284;&#24615;&#21644;&#29983;&#29289;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20135;&#29983;&#26032;&#39062;&#12289;&#22810;&#26679;&#21270;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#31934;&#20934;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03726v1 Announce Type: cross  Abstract: Protein design requires a deep understanding of the inherent complexities of the protein universe. While many efforts lean towards conditional generation or focus on specific families of proteins, the foundational task of unconditional generation remains underexplored and undervalued. Here, we explore this pivotal domain, introducing DiMA, a model that leverages continuous diffusion on embeddings derived from the protein language model, ESM-2, to generate amino acid sequences. DiMA surpasses leading solutions, including autoregressive transformer-based and discrete diffusion models, and we quantitatively illustrate the impact of the design choices that lead to its superior performance. We extensively evaluate the quality, diversity, distribution similarity, and biological relevance of the generated sequences using multiple metrics across various modalities. Our approach consistently produces novel, diverse protein sequences that accura
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; Controllable Time Series (CTS) &#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32806;&#26144;&#23556;&#36807;&#31243;&#26469;&#23454;&#29616;&#23545;&#22797;&#26434;&#20132;&#20114;&#27169;&#24335;&#30340;&#31934;&#30830;&#23398;&#20064;&#65292;&#20174;&#32780;&#21019;&#26032;&#20102;&#38024;&#23545;&#21487;&#25511;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104; (CTSG) &#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03698</link><description>&lt;p&gt;
&#26397;&#21521;&#21487;&#25511;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Controllable Time Series Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03698
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; Controllable Time Series (CTS) &#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32806;&#26144;&#23556;&#36807;&#31243;&#26469;&#23454;&#29616;&#23545;&#22797;&#26434;&#20132;&#20114;&#27169;&#24335;&#30340;&#31934;&#30830;&#23398;&#20064;&#65292;&#20174;&#32780;&#21019;&#26032;&#20102;&#38024;&#23545;&#21487;&#25511;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104; (CTSG) &#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#65288;TSG&#65289;&#24050;&#32463;&#25104;&#20026;&#21512;&#25104;&#20934;&#30830;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21464;&#24471;&#19981;&#21487;&#25110;&#32570;&#12290;&#23613;&#31649;TSG&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#32463;&#24120;&#21462;&#20915;&#20110;&#20855;&#26377;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#20381;&#36182;&#24615;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#32597;&#35265;&#25110;&#29420;&#29305;&#26465;&#20214;&#26102;&#65292;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#21363;&#21487;&#25511;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#65288;CTSG&#65289;&#65292;&#26088;&#22312;&#20135;&#29983;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#22806;&#37096;&#26465;&#20214;&#30340;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#65292;&#20174;&#32780;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03698v1 Announce Type: cross  Abstract: Time Series Generation (TSG) has emerged as a pivotal technique in synthesizing data that accurately mirrors real-world time series, becoming indispensable in numerous applications. Despite significant advancements in TSG, its efficacy frequently hinges on having large training datasets. This dependency presents a substantial challenge in data-scarce scenarios, especially when dealing with rare or unique conditions. To confront these challenges, we explore a new problem of Controllable Time Series Generation (CTSG), aiming to produce synthetic time series that can adapt to various external conditions, thereby tackling the data scarcity issue.   In this paper, we propose \textbf{C}ontrollable \textbf{T}ime \textbf{S}eries (\textsf{CTS}), an innovative VAE-agnostic framework tailored for CTSG. A key feature of \textsf{CTS} is that it decouples the mapping process from standard VAE training, enabling precise learning of a complex interpla
&lt;/p&gt;</description></item><item><title>MolNexTR&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#22270;&#20687;&#35782;&#21035;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#32454;&#33268;&#25552;&#21462;&#20998;&#23376;&#22270;&#20687;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#21516;&#26102;&#33021;&#22815;&#39044;&#27979;&#21407;&#23376;&#21644;&#38190;&#65292;&#29702;&#35299;&#24067;&#23616;&#35268;&#21017;&#65292;&#28789;&#27963;&#25972;&#21512;&#31526;&#21495;&#21270;&#30340;&#21270;&#23398;&#21407;&#21017;&#65292;&#24182;&#19988;&#21253;&#21547;&#22810;&#31181;&#20808;&#36827;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03691</link><description>&lt;p&gt;
MolNexTR&#65306;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#22270;&#20687;&#35782;&#21035;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MolNexTR: A Generalized Deep Learning Model for Molecular Image Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03691
&lt;/p&gt;
&lt;p&gt;
MolNexTR&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#22270;&#20687;&#35782;&#21035;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#32454;&#33268;&#25552;&#21462;&#20998;&#23376;&#22270;&#20687;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#21516;&#26102;&#33021;&#22815;&#39044;&#27979;&#21407;&#23376;&#21644;&#38190;&#65292;&#29702;&#35299;&#24067;&#23616;&#35268;&#21017;&#65292;&#28789;&#27963;&#25972;&#21512;&#31526;&#21495;&#21270;&#30340;&#21270;&#23398;&#21407;&#21017;&#65292;&#24182;&#19988;&#21253;&#21547;&#22810;&#31181;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#32467;&#26500;&#35782;&#21035;&#39046;&#22495;&#65292;&#23558;&#20998;&#23376;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#21644;SMILES&#23383;&#31526;&#20018;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#21270;&#23398;&#25991;&#29486;&#20013;&#27969;&#34892;&#30340;&#21508;&#31181;&#32472;&#22270;&#39118;&#26684;&#21644;&#32422;&#23450;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MolNexTR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21040;&#22270;&#32467;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21512;&#24182;&#20102;ConvNext&#21644;Vision-TRansformer&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#23545;&#20998;&#23376;&#22270;&#20687;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#26356;&#32454;&#33268;&#25552;&#21462;&#12290;MolNexTR&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#21407;&#23376;&#21644;&#38190;&#65292;&#24182;&#29702;&#35299;&#23427;&#20204;&#30340;&#24067;&#23616;&#35268;&#21017;&#12290;&#23427;&#36824;&#25797;&#38271;&#28789;&#27963;&#22320;&#23558;&#31526;&#21495;&#21270;&#30340;&#21270;&#23398;&#21407;&#21017;&#34701;&#20837;&#20854;&#20013;&#65292;&#20197;&#35782;&#21035;&#25163;&#24615;&#24182;&#35299;&#26512;&#32553;&#20889;&#32467;&#26500;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#20808;&#36827;&#31639;&#27861;&#65292;&#21253;&#25324;&#25913;&#36827;&#30340;&#25968;&#25454;&#22686;&#24378;&#27169;&#22359;&#12289;&#22270;&#20687;&#27745;&#26579;&#27169;&#22359;&#21644;&#21518;&#22788;&#29702;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03691v1 Announce Type: cross  Abstract: In the field of chemical structure recognition, the task of converting molecular images into graph structures and SMILES string stands as a significant challenge, primarily due to the varied drawing styles and conventions prevalent in chemical literature. To bridge this gap, we proposed MolNexTR, a novel image-to-graph deep learning model that collaborates to fuse the strengths of ConvNext, a powerful Convolutional Neural Network variant, and Vision-TRansformer. This integration facilitates a more nuanced extraction of both local and global features from molecular images. MolNexTR can predict atoms and bonds simultaneously and understand their layout rules. It also excels at flexibly integrating symbolic chemistry principles to discern chirality and decipher abbreviated structures. We further incorporate a series of advanced algorithms, including improved data augmentation module, image contamination module, and a post-processing modul
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;GPT-4&#33258;&#25351;&#23548;&#26041;&#27861;&#65292;&#24555;&#36895;&#24320;&#21457;&#39640;&#36136;&#37327;&#30340;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#26080;&#38656;&#22823;&#37327;&#20154;&#21147;&#25237;&#20837;&#65292;&#24182;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#36164;&#28304;</title><link>https://arxiv.org/abs/2403.03690</link><description>&lt;p&gt;
&#24555;&#36895;&#24320;&#21457;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#20943;&#23569;&#20154;&#21147;&#25237;&#20837;&#65306;&#20197;&#26085;&#35821;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03690
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;GPT-4&#33258;&#25351;&#23548;&#26041;&#27861;&#65292;&#24555;&#36895;&#24320;&#21457;&#39640;&#36136;&#37327;&#30340;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#26080;&#38656;&#22823;&#37327;&#20154;&#21147;&#25237;&#20837;&#65292;&#24182;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#65292;&#21019;&#24314;&#25351;&#20196;&#25968;&#25454;&#21644;&#35780;&#20272;&#22522;&#20934;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#12290;&#24403;&#20026;&#26085;&#35821;&#31561;&#38750;&#33521;&#35821;&#35821;&#35328;&#24555;&#36895;&#24320;&#21457;&#36825;&#20123;&#36164;&#28304;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-4&#30340;&#39640;&#25928;&#33258;&#25351;&#23548;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#23558;&#29616;&#26377;&#30340;&#33521;&#35821;&#36164;&#28304;&#32763;&#35793;&#25104;&#26085;&#35821;&#65288;&#20363;&#22914;Japanese-Alpaca&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#23569;&#37327;&#33521;&#35821;&#25351;&#20196;&#32763;&#35793;&#25104;&#26085;&#35821;&#65292;&#24182;&#36827;&#34892;&#21518;&#26399;&#32534;&#36753;&#20197;&#33719;&#24471;native-level&#36136;&#37327;&#12290;&#28982;&#21518;&#65292;GPT-4&#21033;&#29992;&#36825;&#20123;&#25351;&#20196;&#20316;&#20026;&#31034;&#33539;&#65292;&#33258;&#21160;&#29983;&#25104;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;GPT-4&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;80&#20010;&#38382;&#39064;&#36328;8&#20010;&#31867;&#21035;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#20351;&#29992;GPT-4&#33258;&#21160;&#35780;&#20272;LLMs&#30340;&#21709;&#24212;&#36136;&#37327;&#65292;&#26080;&#38656;&#20154;&#24037;&#21442;&#32771;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#25105;&#20204;&#30340;GPT-4&#33258;&#25351;&#23548;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03690v1 Announce Type: cross  Abstract: The creation of instruction data and evaluation benchmarks for serving Large language models often involves enormous human annotation. This issue becomes particularly pronounced when rapidly developing such resources for a non-English language like Japanese. Instead of following the popular practice of directly translating existing English resources into Japanese (e.g., Japanese-Alpaca), we propose an efficient self-instruct method based on GPT-4. We first translate a small amount of English instructions into Japanese and post-edit them to obtain native-level quality. GPT-4 then utilizes them as demonstrations to automatically generate Japanese instruction data. We also construct an evaluation benchmark containing 80 questions across 8 categories, using GPT-4 to automatically assess the response quality of LLMs without human references. The empirical results suggest that the models fine-tuned on our GPT-4 self-instruct data significant
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;G2ST&#30340;&#20004;&#27493;&#24494;&#35843;&#33539;&#24335;&#65292;&#36890;&#36807;&#33258;&#23545;&#27604;&#35821;&#20041;&#22686;&#24378;&#23558;&#36890;&#29992;NMT&#27169;&#22411;&#36716;&#25442;&#20026;&#19987;&#38376;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#30340;NMT&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03689</link><description>&lt;p&gt;
&#36890;&#29992;&#21040;&#19987;&#19994;&#30340;&#30005;&#23376;&#21830;&#21153;LLMs&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
General2Specialized LLMs Translation for E-commerce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;G2ST&#30340;&#20004;&#27493;&#24494;&#35843;&#33539;&#24335;&#65292;&#36890;&#36807;&#33258;&#23545;&#27604;&#35821;&#20041;&#22686;&#24378;&#23558;&#36890;&#29992;NMT&#27169;&#22411;&#36716;&#25442;&#20026;&#19987;&#38376;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#30340;NMT&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#20027;&#35201;&#22788;&#29702;&#36890;&#29992;&#39046;&#22495;&#30340;&#32763;&#35793;&#65292;&#24573;&#30053;&#20102;&#20855;&#26377;&#29305;&#27530;&#20889;&#20316;&#20844;&#24335;&#30340;&#39046;&#22495;&#65292;&#27604;&#22914;&#30005;&#23376;&#21830;&#21153;&#21644;&#27861;&#24459;&#25991;&#20214;&#12290;&#20197;&#30005;&#23376;&#21830;&#21153;&#20026;&#20363;&#65292;&#25991;&#26412;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#39046;&#22495;&#30456;&#20851;&#35789;&#27719;&#65292;&#24182;&#19988;&#23384;&#22312;&#26356;&#22810;&#30340;&#35821;&#27861;&#38382;&#39064;&#65292;&#36825;&#23548;&#33268;&#24403;&#21069;NMT&#26041;&#27861;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20004;&#20010;&#19982;&#39046;&#22495;&#30456;&#20851;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#19968;&#32452;&#26415;&#35821;&#23545;&#65288;&#23545;&#40784;&#30340;&#20013;&#33521;&#21452;&#35821;&#26415;&#35821;&#65289;&#21644;&#19968;&#20010;&#38024;&#23545;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#36827;&#34892;&#27880;&#37322;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#24494;&#35843;&#33539;&#24335;&#65288;&#21517;&#20026;G2ST&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#23545;&#27604;&#35821;&#20041;&#22686;&#24378;&#65292;&#20197;&#23558;&#19968;&#20010;&#36890;&#29992;NMT&#27169;&#22411;&#36716;&#25442;&#20026;&#19987;&#38376;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#30340;NMT&#27169;&#22411;&#12290;&#35813;&#33539;&#24335;&#36866;&#29992;&#20110;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;NMT&#27169;&#22411;&#12290;&#23545;&#30495;&#23454;&#30005;&#23376;&#21830;&#21153;&#26631;&#39064;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#20102;&#21331;&#36234;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03689v1 Announce Type: cross  Abstract: Existing Neural Machine Translation (NMT) models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents. Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current NMT methods. To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain. Furthermore, we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce. The paradigm can be used for the NMT models based on Large language models (LLMs). Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-Link&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#36890;&#29992;&#30693;&#35782;&#65292;&#25552;&#21462;&#20102;Knowledge-Link&#22270;&#20197;&#25429;&#33719;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#24191;&#27867;&#35821;&#20041;&#30693;&#35782;&#21644;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.03645</link><description>&lt;p&gt;
K-Link&#65306;&#22522;&#20110;LLMs&#30340;&#30693;&#35782;&#38142;&#25509;&#22270;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation Learning in Multivariate Time-Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03645
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-Link&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#36890;&#29992;&#30693;&#35782;&#65292;&#25552;&#21462;&#20102;Knowledge-Link&#22270;&#20197;&#25429;&#33719;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#24191;&#27867;&#35821;&#20041;&#30693;&#35782;&#21644;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21508;&#31181;&#20256;&#24863;&#22120;&#37319;&#38598;&#24182;&#25353;&#26102;&#38388;&#39034;&#24207;&#32452;&#32455;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#25968;&#25454;&#28041;&#21450;&#20851;&#38190;&#30340;&#26102;&#31354;&#20381;&#36182;&#24615;&#65292;&#22914;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#20174;MTS&#25968;&#25454;&#26500;&#24314;&#22270;&#30340;&#36136;&#37327;&#38480;&#21046;&#12290;&#36890;&#24120;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#20174;MTS&#20449;&#21495;&#26500;&#24314;&#22270;&#65292;&#36825;&#21487;&#33021;&#20250;&#30001;&#20110;&#23567;&#35757;&#32451;&#25968;&#25454;&#38598;&#32780;&#24341;&#20837;&#20559;&#24046;&#65292;&#24182;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#24213;&#23618;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;K-Link&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#32534;&#30721;&#24191;&#27867;&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#20379;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#20943;&#23569;&#20559;&#24046;&#12290;&#21033;&#29992;LLMs&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;&#29289;&#29702;&#21407;&#29702;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#19968;&#20010;Knowledge-Link&#22270;&#65292;&#25429;&#33719;&#20102;&#20256;&#24863;&#22120;&#30340;&#24191;&#27867;&#35821;&#20041;&#30693;&#35782;&#21644;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03645v1 Announce Type: new  Abstract: Sourced from various sensors and organized chronologically, Multivariate Time-Series (MTS) data involves crucial spatial-temporal dependencies, e.g., correlations among sensors. To capture these dependencies, Graph Neural Networks (GNNs) have emerged as powerful tools, yet their effectiveness is restricted by the quality of graph construction from MTS data. Typically, existing approaches construct graphs solely from MTS signals, which may introduce bias due to a small training dataset and may not accurately represent underlying dependencies. To address this challenge, we propose a novel framework named K-Link, leveraging Large Language Models (LLMs) to encode extensive general knowledge and thereby providing effective solutions to reduce the bias. Leveraging the knowledge embedded in LLMs, such as physical principles, we extract a \textit{Knowledge-Link graph}, capturing vast semantic knowledge of sensors and the linkage of the sensor-le
&lt;/p&gt;</description></item><item><title>&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#35299;&#20915;&#26041;&#27861;&#25910;&#25947;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#31561;&#20248;&#21183;&#65292;&#20026;&#36825;&#19968;&#38382;&#39064;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.03643</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03643
&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#35299;&#20915;&#26041;&#27861;&#25910;&#25947;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#31561;&#20248;&#21183;&#65292;&#20026;&#36825;&#19968;&#38382;&#39064;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#30340;&#25361;&#25112;&#22312;&#20132;&#36890;&#36816;&#36755;&#12289;&#24037;&#19994;&#21644;&#26085;&#24120;&#29983;&#27963;&#31561;&#21508;&#20010;&#39046;&#22495;&#26222;&#36941;&#23384;&#22312;&#12290;&#38543;&#30528;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#35268;&#27169;&#19981;&#26029;&#25193;&#22823;&#20197;&#21450;&#23545;&#23454;&#26102;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#20256;&#32479;&#31639;&#27861;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#35745;&#31639;&#21387;&#21147;&#65292;&#38590;&#20197;&#23454;&#29616;&#26368;&#20339;&#25928;&#29575;&#21644;&#23454;&#26102;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#35745;&#31639;&#26426;&#35745;&#31639;&#33021;&#21147;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#35832;&#22914;&#22260;&#26827;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#65292;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#23398;&#20064;&#21644;&#24207;&#36143;&#20915;&#31574;&#33021;&#21147;&#12290;&#37492;&#20110;&#36825;&#20123;&#36827;&#23637;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#22823;&#37327;&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#35299;&#20915;&#26041;&#27861;&#25910;&#25947;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#31561;&#20248;&#21183;&#65292;&#20026;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03643v1 Announce Type: cross  Abstract: The challenge of spatial resource allocation is pervasive across various domains such as transportation, industry, and daily life. As the scale of real-world issues continues to expand and demands for real-time solutions increase, traditional algorithms face significant computational pressures, struggling to achieve optimal efficiency and real-time capabilities. In recent years, with the escalating computational power of computers, the remarkable achievements of reinforcement learning in domains like Go and robotics have demonstrated its robust learning and sequential decision-making capabilities. Given these advancements, there has been a surge in novel methods employing reinforcement learning to tackle spatial resource allocation problems. These methods exhibit advantages such as rapid solution convergence and strong model generalization abilities, offering a new perspective on resolving spatial resource allocation problems. Therefor
&lt;/p&gt;</description></item><item><title>Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03640</link><description>&lt;p&gt;
Apollo&#65306;&#36731;&#37327;&#32423;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65306;&#35753;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#26222;&#24800;60&#20159;&#20154;
&lt;/p&gt;
&lt;p&gt;
Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03640
&lt;/p&gt;
&lt;p&gt;
Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20840;&#29699;&#21307;&#23398;&#30693;&#35782;&#30340;&#24222;&#22823;&#23384;&#20648;&#24211;&#20027;&#35201;&#26159;&#20197;&#33521;&#35821;&#20026;&#20027;&#65292;&#20294;&#22312;&#20256;&#36882;&#37327;&#36523;&#23450;&#21046;&#21307;&#30103;&#26381;&#21153;&#26041;&#38754;&#65292;&#26412;&#22320;&#35821;&#35328;&#23545;&#20110;&#22312;&#21307;&#30103;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#23588;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#23558;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#20154;&#32676;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#28085;&#30422;&#20840;&#29699;61&#20159;&#20154;&#21475;&#30340;&#20845;&#31181;&#26368;&#24120;&#29992;&#35821;&#35328;&#30340;&#21307;&#23398;LLMs&#12290;&#36825;&#19968;&#21162;&#21147;&#26368;&#32456;&#20419;&#25104;&#20102;ApolloCorpora&#22810;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#38598;&#21644;XMedBench&#22522;&#20934;&#30340;&#21019;&#24314;&#12290;&#22312;&#22810;&#35821;&#35328;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21457;&#24067;&#30340;Apollo&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#30456;&#23545;&#36739;&#23567;&#23610;&#23544;&#65288;&#21363;0.5B&#12289;1.8B&#12289;2B&#12289;6B&#21644;7B&#65289;&#19978;&#21462;&#24471;&#20102;&#19982;&#21516;&#31561;&#22823;&#23567;&#27169;&#22411;&#26368;&#20339;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;Apollo-7B&#26159;&#36804;&#20170;&#20026;&#27490;&#36798;&#21040;70B&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#36731;&#37327;&#32423;&#27169;&#22411;&#21487;&#29992;&#20110;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#36739;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03640v1 Announce Type: cross  Abstract: Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a
&lt;/p&gt;</description></item><item><title>SheetAgent&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30005;&#23376;&#34920;&#26684;&#25512;&#29702;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#22788;&#29702;&#22797;&#26434;&#29616;&#23454;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.03636</link><description>&lt;p&gt;
SheetAgent&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30005;&#23376;&#34920;&#26684;&#25512;&#29702;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03636
&lt;/p&gt;
&lt;p&gt;
SheetAgent&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30005;&#23376;&#34920;&#26684;&#25512;&#29702;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#22788;&#29702;&#22797;&#26434;&#29616;&#23454;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#34920;&#26684;&#25805;&#20316;&#24191;&#27867;&#23384;&#22312;&#20110;&#22823;&#22810;&#25968;&#26085;&#24120;&#24037;&#20316;&#20013;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24037;&#20316;&#25928;&#29575;&#12290;&#26368;&#36817;&#23581;&#35797;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36827;&#34892;&#33258;&#21160;&#30005;&#23376;&#34920;&#26684;&#25805;&#20316;&#65292;&#20294;&#23578;&#26410;&#22312;&#23384;&#22312;&#25512;&#29702;&#25361;&#25112;&#30340;&#22797;&#26434;&#21644;&#29616;&#23454;&#20219;&#21153;&#20013;&#36827;&#34892;&#25506;&#31350;&#65288;&#20363;&#22914;&#65292;&#20855;&#26377;&#22810;&#27493;&#25512;&#29702;&#21644;&#27169;&#31946;&#35201;&#27714;&#30340;&#38271;&#35270;&#37326;&#25805;&#20316;&#65289;&#12290;&#20026;&#20102;&#24357;&#21512;&#19982;&#30495;&#23454;&#19990;&#30028;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;$\textbf{SheetRM}$&#65292;&#19968;&#20010;&#29305;&#28857;&#26159;&#38271;&#35270;&#37326;&#21644;&#22810;&#31867;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#20855;&#26377;&#25512;&#29702;&#30456;&#20851;&#25805;&#32437;&#65292;&#30001;&#30495;&#23454;&#25361;&#25112;&#24341;&#36215;&#12290;&#20026;&#20102;&#32531;&#35299;&#20197;&#19978;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;$\textbf{SheetAgent}$&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#33021;&#21147;&#30340;&#26032;&#22411;&#33258;&#20027;&#20195;&#29702;&#12290;SheetAgent&#30001;&#19977;&#20010;&#21327;&#20316;&#27169;&#22359;&#32452;&#25104;&#65306;$\textit{Planner}$&#12289;$\textit{Informer}$&#21644;$\textit{Retriever}$&#65292;&#23454;&#29616;&#20102;&#23545;&#30005;&#23376;&#34920;&#26684;&#30340;&#39640;&#32423;&#25512;&#29702;&#21644;&#20934;&#30830;&#25805;&#20316;&#65292;&#32780;&#19981;&#38656;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03636v1 Announce Type: new  Abstract: Spreadsheet manipulation is widely existing in most daily works and significantly improves working efficiency. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce $\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose $\textbf{SheetAgent}$, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: $\textit{Planner}$, $\textit{Informer}$, and $\textit{Retriever}$, achieving both advanced reasoning and accurate manipulation over spreadsheets without hu
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#35299;&#37322;&#24694;&#24847;&#21644;&#35823;&#23548;&#24615;&#22768;&#26126;&#30340;&#19981;&#21512;&#29702;&#20043;&#22788;&#21644;&#28508;&#22312;&#21160;&#26426;&#12290;</title><link>https://arxiv.org/abs/2403.03627</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models to Support Real-World Fact-Checking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03627
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#35299;&#37322;&#24694;&#24847;&#21644;&#35823;&#23548;&#24615;&#22768;&#26126;&#30340;&#19981;&#21512;&#29702;&#20043;&#22788;&#21644;&#28508;&#22312;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20855;&#26377;&#28508;&#21147;&#25903;&#25345;&#20154;&#31867;&#22788;&#29702;&#22823;&#37327;&#20449;&#24687;&#12290;&#34429;&#28982;MLLMs&#24050;&#32463;&#34987;&#29992;&#20316;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#65292;&#20294;&#23601;&#20854;&#22312;&#27492;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#32780;&#35328;&#65292;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#31995;&#32479;&#35780;&#20272;&#24403;&#21069;&#22810;&#27169;&#24577;&#27169;&#22411;&#20419;&#36827;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#26159;&#26080;&#38656;&#35777;&#25454;&#30340;&#65292;&#20165;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#22266;&#26377;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#35774;&#35745;&#33021;&#22815;&#25552;&#21462;&#27169;&#22411;&#39044;&#27979;&#12289;&#35299;&#37322;&#21644;&#32622;&#20449;&#27700;&#24179;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20851;&#20110;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#22833;&#36133;&#21407;&#22240;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#65292;(1) GPT-4V&#22312;&#35782;&#21035;&#24694;&#24847;&#21644;&#35823;&#23548;&#24615;&#22810;&#27169;&#24577;&#22768;&#26126;&#26041;&#38754;&#34920;&#29616;&#20986;&#36229;&#20961;&#24615;&#33021;&#65292;&#33021;&#22815;&#35299;&#37322;&#19981;&#21512;&#29702;&#30340;&#26041;&#38754;&#21644;&#28508;&#22312;&#21160;&#26426;&#65292;&#20197;&#21450;(2)&#29616;&#26377;&#30340;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03627v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) carry the potential to support humans in processing vast amounts of information. While MLLMs are already being used as a fact-checking tool, their abilities and limitations in this regard are understudied. Here is aim to bridge this gap. In particular, we propose a framework for systematically assessing the capacity of current multimodal models to facilitate real-world fact-checking. Our methodology is evidence-free, leveraging only these models' intrinsic knowledge and reasoning capabilities. By designing prompts that extract models' predictions, explanations, and confidence levels, we delve into research questions concerning model accuracy, robustness, and reasons for failure. We empirically find that (1) GPT-4V exhibits superior performance in identifying malicious and misleading multimodal claims, with the ability to explain the unreasonable aspects and underlying motives, and (2) existing o
&lt;/p&gt;</description></item><item><title>GSNeRF&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#30693;&#22330;&#26223;&#30340;&#26032;&#35270;&#22270;&#22270;&#20687;&#21644;&#30456;&#20851;&#35821;&#20041;&#22320;&#22270;&#30340;&#29983;&#25104;&#65292;&#24182;&#22312;&#22270;&#20687;&#21644;&#35821;&#20041;&#28210;&#26579;&#26041;&#38754;&#21462;&#24471;&#20102;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03608</link><description>&lt;p&gt;
GSNeRF: &#22686;&#24378;&#20102;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#36890;&#29992;&#35821;&#20041;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03608
&lt;/p&gt;
&lt;p&gt;
GSNeRF&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#30693;&#22330;&#26223;&#30340;&#26032;&#35270;&#22270;&#22270;&#20687;&#21644;&#30456;&#20851;&#35821;&#20041;&#22320;&#22270;&#30340;&#29983;&#25104;&#65292;&#24182;&#22312;&#22270;&#20687;&#21644;&#35821;&#20041;&#28210;&#26579;&#26041;&#38754;&#21462;&#24471;&#20102;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#20041;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;GSNeRF&#65289;&#65292;&#23427;&#29420;&#29305;&#22320;&#23558;&#22270;&#20687;&#35821;&#20041;&#32435;&#20837;&#21512;&#25104;&#36807;&#31243;&#65292;&#22240;&#27492;&#21487;&#20197;&#20026;&#26410;&#30693;&#22330;&#26223;&#29983;&#25104;&#26032;&#35270;&#22270;&#22270;&#20687;&#21644;&#30456;&#20851;&#30340;&#35821;&#20041;&#22320;&#22270;&#12290;&#25105;&#20204;&#30340;GSNeRF&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#35821;&#20041;&#22320;&#29702;&#25512;&#29702;&#21644;&#28145;&#24230;&#24341;&#23548;&#21487;&#35270;&#28210;&#26579;&#12290;&#21069;&#32773;&#33021;&#22815;&#35266;&#23519;&#22810;&#35270;&#22270;&#22270;&#20687;&#36755;&#20837;&#65292;&#20174;&#22330;&#26223;&#20013;&#25552;&#21462;&#35821;&#20041;&#21644;&#20960;&#20309;&#29305;&#24449;&#12290;&#22312;&#21518;&#32773;&#30340;&#25351;&#23548;&#19979;&#65292;&#26681;&#25454;&#29983;&#25104;&#30340;&#22270;&#20687;&#20960;&#20309;&#20449;&#24687;&#65292;&#36827;&#34892;&#20102;&#20855;&#26377;&#25913;&#36827;&#24615;&#33021;&#30340;&#22270;&#20687;&#21644;&#35821;&#20041;&#28210;&#26579;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;GSNeRF&#22312;&#26032;&#35270;&#22270;&#22270;&#20687;&#21644;&#35821;&#20041;&#20998;&#21106;&#21512;&#25104;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#24037;&#20316;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#37319;&#26679;&#31574;&#30053;&#23545;&#20110;&#21487;&#35270;&#28210;&#26579;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03608v1 Announce Type: cross  Abstract: Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision. In this work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view images and the associated semantic maps can be produced for unseen scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and Depth-Guided Visual rendering. The former is able to observe multi-view image inputs to extract semantic and geometry features from a scene. Guided by the resulting image geometry information, the latter performs both image and semantic rendering with improved performances. Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effectiveness of our sampling strategy for visual rendering is 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24179;&#38754;&#35805;&#39064;&#27169;&#22411;&#20013;&#23548;&#20986;&#24207;&#25968;&#32467;&#26500;&#30340;&#20851;&#32852;&#20960;&#20309;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#24230;&#20998;&#26512;&#35805;&#39064;&#27169;&#22411;&#65292;&#25552;&#21462;&#22810;&#20010;&#20027;&#39064;&#20043;&#38388;&#30340;&#27010;&#24565;&#20851;&#31995;</title><link>https://arxiv.org/abs/2403.03607</link><description>&lt;p&gt;
&#35805;&#39064;&#27169;&#22411;&#30340;&#20960;&#20309;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Geometric Structure of Topic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03607
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24179;&#38754;&#35805;&#39064;&#27169;&#22411;&#20013;&#23548;&#20986;&#24207;&#25968;&#32467;&#26500;&#30340;&#20851;&#32852;&#20960;&#20309;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#24230;&#20998;&#26512;&#35805;&#39064;&#27169;&#22411;&#65292;&#25552;&#21462;&#22810;&#20010;&#20027;&#39064;&#20043;&#38388;&#30340;&#27010;&#24565;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35805;&#39064;&#27169;&#22411;&#26159;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#21644;&#20998;&#26512;&#25991;&#26412;&#25968;&#25454;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#23427;&#20204;&#20801;&#35768;&#26681;&#25454;&#20854;&#19982;&#20808;&#21069;&#35745;&#31639;&#30340;&#20027;&#39064;&#30340;&#20851;&#32852;&#26469;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#23613;&#31649;&#35805;&#39064;&#27169;&#22411;&#22312;&#30740;&#31350;&#21644;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23545;&#35805;&#39064;&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#35299;&#37322;&#35805;&#39064;&#27169;&#22411;&#30340;&#26368;&#26032;&#26041;&#27861;&#22522;&#20110;&#31616;&#21333;&#30340;&#21487;&#35270;&#21270;&#65292;&#22914;&#30456;&#20284;&#24615;&#30697;&#38453;&#12289;&#39640;&#39057;&#35789;&#21015;&#34920;&#25110;&#23884;&#20837;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20165;&#38480;&#20110;&#26368;&#22810;&#19977;&#32500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#24179;&#38754;&#35805;&#39064;&#27169;&#22411;&#65288;&#22914;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65289;&#20013;&#23548;&#20986;&#24207;&#25968;&#32467;&#26500;&#30340;&#20851;&#32852;&#20960;&#20309;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#22312;&#26356;&#39640;&#30340;&#65288;&#38454;&#65289;&#32500;&#24230;&#20998;&#26512;&#35805;&#39064;&#27169;&#22411;&#65292;&#24182;&#26377;&#21487;&#33021;&#19968;&#27425;&#24615;&#25552;&#21462;&#22810;&#20010;&#20027;&#39064;&#20043;&#38388;&#30340;&#27010;&#24565;&#20851;&#31995;&#12290;&#30001;&#20110;&#20351;&#29992;&#20102;&#27010;&#24565;&#23610;&#24230;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#24341;&#20837;&#20219;&#20309;&#20154;&#20026;&#30340;&#20027;&#39064;&#20851;&#31995;&#65292;&#22914;ar
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03607v1 Announce Type: new  Abstract: Topic models are a popular tool for clustering and analyzing textual data. They allow texts to be classified on the basis of their affiliation to the previously calculated topics. Despite their widespread use in research and application, an in-depth analysis of topic models is still an open research topic. State-of-the-art methods for interpreting topic models are based on simple visualizations, such as similarity matrices, top-term lists or embeddings, which are limited to a maximum of three dimensions. In this paper, we propose an incidence-geometric method for deriving an ordinal structure from flat topic models, such as non-negative matrix factorization. These enable the analysis of the topic model in a higher (order) dimension and the possibility of extracting conceptual relationships between several topics at once. Due to the use of conceptual scaling, our approach does not introduce any artificial topical relationships, such as ar
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24341;&#20837;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#25216;&#26415;&#25351;&#26631;&#26469;&#25552;&#21319;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#25429;&#33719;&#26102;&#38388;&#21160;&#24577;&#21644;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;</title><link>https://arxiv.org/abs/2403.03606</link><description>&lt;p&gt;
&#36890;&#36807;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#25216;&#26415;&#25351;&#26631;&#25552;&#21319;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Price Prediction in Cryptocurrency Using Transformer Neural Network and Technical Indicators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24341;&#20837;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#25216;&#26415;&#25351;&#26631;&#26469;&#25552;&#21319;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#25429;&#33719;&#26102;&#38388;&#21160;&#24577;&#21644;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#26102;&#38388;&#24207;&#21015;&#65292;&#29305;&#21035;&#20851;&#27880;&#27604;&#29305;&#24065;&#12289;&#20197;&#22826;&#22346;&#21644;&#33713;&#29305;&#24065;&#12290;&#35813;&#26041;&#27861;&#25972;&#21512;&#20102;&#25216;&#26415;&#25351;&#26631;&#12289;Performer&#31070;&#32463;&#32593;&#32476;&#21644;BiLSTM&#65288;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65289;&#26469;&#25429;&#33719;&#26102;&#38388;&#21160;&#24577;&#24182;&#20174;&#21407;&#22987;&#21152;&#23494;&#36135;&#24065;&#25968;&#25454;&#20013;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#12290;&#25216;&#26415;&#25351;&#26631;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#25552;&#21462;&#22797;&#26434;&#27169;&#24335;&#12289;&#21160;&#37327;&#12289;&#27874;&#21160;&#24615;&#21644;&#36235;&#21183;&#12290;Performer&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#24555;&#36895;&#20851;&#27880;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#65288;FAVOR+&#65289;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#20013;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23637;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#27492;&#22806;&#65292;&#23558;BiLSTM&#38598;&#25104;&#21040;&#21069;&#39304;&#32593;&#32476;&#20013;&#22686;&#24378;&#20102;&#27169;&#22411;&#25429;&#33719;&#25968;&#25454;&#26102;&#38388;&#21160;&#24577;&#30340;&#33021;&#21147;&#65292;&#21521;&#21069;&#21644;&#21521;&#21518;&#20004;&#20010;&#26041;&#21521;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03606v1 Announce Type: cross  Abstract: This study presents an innovative approach for predicting cryptocurrency time series, specifically focusing on Bitcoin, Ethereum, and Litecoin. The methodology integrates the use of technical indicators, a Performer neural network, and BiLSTM (Bidirectional Long Short-Term Memory) to capture temporal dynamics and extract significant features from raw cryptocurrency data. The application of technical indicators, such facilitates the extraction of intricate patterns, momentum, volatility, and trends. The Performer neural network, employing Fast Attention Via positive Orthogonal Random features (FAVOR+), has demonstrated superior computational efficiency and scalability compared to the traditional Multi-head attention mechanism in Transformer models. Additionally, the integration of BiLSTM in the feedforward network enhances the model's capacity to capture temporal dynamics in the data, processing it in both forward and backward direction
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#29992;&#20110;&#36328;&#39046;&#22495;&#25512;&#33616;&#65292;&#36890;&#36807;&#35774;&#35745;&#22810;&#27169;&#24577;&#35299;&#32806;&#32534;&#30721;&#22120;&#23454;&#29616;&#26356;&#22909;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.03600</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#29992;&#20110;&#36328;&#39046;&#22495;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
A Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03600
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#29992;&#20110;&#36328;&#39046;&#22495;&#25512;&#33616;&#65292;&#36890;&#36807;&#35774;&#35745;&#22810;&#27169;&#24577;&#35299;&#32806;&#32534;&#30721;&#22120;&#23454;&#29616;&#26356;&#22909;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#39046;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#28304;&#39046;&#22495;&#20013;&#20016;&#23500;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;&#30446;&#26631;&#39046;&#22495;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P2M2-CDR&#30340;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#35299;&#32806;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#39046;&#22495;&#20849;&#26377;&#21644;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#36328;&#39046;&#22495;&#25512;&#33616;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03600v1 Announce Type: new  Abstract: Cross-domain recommendation (CDR) aims to enhance recommendation accuracy in a target domain with sparse data by leveraging rich information in a source domain, thereby addressing the data-sparsity problem. Some existing CDR methods highlight the advantages of extracting domain-common and domain-specific features to learn comprehensive user and item representations. However, these methods can't effectively disentangle these components as they often rely on simple user-item historical interaction information (such as ratings, clicks, and browsing), neglecting the rich multi-modal features. Additionally, they don't protect user-sensitive data from potential leakage during knowledge transfer between domains. To address these challenges, we propose a Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain Recommendation, called P2M2-CDR. Specifically, we first design a multi-modal disentangled encoder that utilizes multi-modal in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#19982;Vision&#22312;&#22788;&#29702;&#22270;&#20687;&#36755;&#20837;&#30340;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#23457;&#32654;&#35780;&#20215;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#39044;&#27979;&#23457;&#32654;&#35780;&#20215;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#23637;&#29616;&#20102;&#23545;&#32654;&#21644;&#19985;&#30340;&#19981;&#21516;&#21453;&#24212;&#30340;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03594</link><description>&lt;p&gt;
&#35780;&#20272;GPT-4&#19982;Vision&#30340;&#23457;&#32654;&#35780;&#20215;&#33021;&#21147;&#65306;&#26469;&#33258;&#32676;&#20307;&#21644;&#20010;&#20307;&#35780;&#20272;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Assessing the Aesthetic Evaluation Capabilities of GPT-4 with Vision: Insights from Group and Individual Assessments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#19982;Vision&#22312;&#22788;&#29702;&#22270;&#20687;&#36755;&#20837;&#30340;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#23457;&#32654;&#35780;&#20215;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#39044;&#27979;&#23457;&#32654;&#35780;&#20215;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#23637;&#29616;&#20102;&#23545;&#32654;&#21644;&#19985;&#30340;&#19981;&#21516;&#21453;&#24212;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#24847;&#35782;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#26234;&#21147;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#35843;&#26597;&#36825;&#20123;&#27169;&#22411;&#22312;&#28041;&#21450;&#23457;&#32654;&#35780;&#20215;&#31561;&#28041;&#21450;&#24863;&#24615;&#30340;&#34892;&#20026;&#20013;&#19982;&#20154;&#31867;&#30340;&#19968;&#33268;&#24615;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#33021;&#22815;&#22788;&#29702;&#22270;&#20687;&#36755;&#20837;&#30340;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;GPT-4&#19982;Vision&#22312;&#22270;&#20687;&#30340;&#23457;&#32654;&#35780;&#20215;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#20219;&#21153;&#65292;&#39044;&#27979;&#32676;&#20307;&#30340;&#24179;&#22343;&#35780;&#20215;&#20540;&#21644;&#20010;&#20307;&#30340;&#35780;&#20215;&#20540;&#12290;&#36890;&#36807;&#25506;&#32034;&#25552;&#31034;&#24182;&#20998;&#26512;&#39044;&#27979;&#34892;&#20026;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;GPT-4&#19982;Vision&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#19982;Vision&#22312;&#39044;&#27979;&#23457;&#32654;&#35780;&#20215;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#23637;&#29616;&#20102;&#23545;&#32654;&#21644;&#19985;&#30340;&#19981;&#21516;&#21453;&#24212;&#30340;&#29305;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#20154;&#31867;&#23545;&#32654;&#24863;&#30693;&#30340;&#31185;&#23398;&#30693;&#35782;&#21457;&#23637;&#23457;&#32654;&#35780;&#20215;&#30340;AI&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03594v1 Announce Type: new  Abstract: Recently, it has been recognized that large language models demonstrate high performance on various intellectual tasks. However, few studies have investigated alignment with humans in behaviors that involve sensibility, such as aesthetic evaluation. This study investigates the performance of GPT-4 with Vision, a state-of-the-art language model that can handle image input, on the task of aesthetic evaluation of images. We employ two tasks, prediction of the average evaluation values of a group and an individual's evaluation values. We investigate the performance of GPT-4 with Vision by exploring prompts and analyzing prediction behaviors. Experimental results reveal GPT-4 with Vision's superior performance in predicting aesthetic evaluations and the nature of different responses to beauty and ugliness. Finally, we discuss developing an AI system for aesthetic evaluation based on scientific knowledge of the human perception of beauty, empl
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;MaleficNet 2.0&#65292;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#24694;&#24847;&#36719;&#20214;&#30340;&#26032;&#25216;&#26415;&#65292;&#20854;&#27880;&#20837;&#25216;&#26415;&#20855;&#26377;&#38544;&#34109;&#24615;&#65292;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20013;&#30340;&#24694;&#24847;&#26377;&#25928;&#36127;&#36733;&#36827;&#34892;&#27880;&#20837;</title><link>https://arxiv.org/abs/2403.03593</link><description>&lt;p&gt;
&#24744;&#20449;&#20219;&#24744;&#30340;&#27169;&#22411;&#21527;&#65311;&#28145;&#24230;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#26032;&#20852;&#30340;&#24694;&#24847;&#36719;&#20214;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03593
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;MaleficNet 2.0&#65292;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#24694;&#24847;&#36719;&#20214;&#30340;&#26032;&#25216;&#26415;&#65292;&#20854;&#27880;&#20837;&#25216;&#26415;&#20855;&#26377;&#38544;&#34109;&#24615;&#65292;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20013;&#30340;&#24694;&#24847;&#26377;&#25928;&#36127;&#36733;&#36827;&#34892;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#22240;&#20026;&#38656;&#35201;&#35745;&#31639;&#21644;&#25216;&#26415;&#35201;&#27714;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#20010;&#20154;&#12289;&#26426;&#26500;&#21644;&#20844;&#21496;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#20110;&#22312;&#20844;&#20849;&#20195;&#30721;&#24211;&#20013;&#25552;&#20379;&#30340;&#39044;&#35757;&#32451;&#30340;&#31532;&#19977;&#26041;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#30452;&#25509;&#20351;&#29992;&#25110;&#38598;&#25104;&#21040;&#20135;&#21697;&#31649;&#36947;&#20013;&#32780;&#27809;&#26377;&#29305;&#27530;&#30340;&#39044;&#38450;&#25514;&#26045;&#65292;&#22240;&#20026;&#23427;&#20204;&#23454;&#38469;&#19978;&#21482;&#26159;&#20197;&#24352;&#37327;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#20379;&#24212;&#38142;&#23041;&#32961;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MaleficNet 2.0&#65292;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#33258;&#35299;&#21387;&#33258;&#25191;&#34892;&#24694;&#24847;&#36719;&#20214;&#30340;&#26032;&#25216;&#26415;&#12290;MaleficNet 2.0&#20351;&#29992;&#25193;&#39057;&#20449;&#36947;&#32534;&#30721;&#32467;&#21512;&#32416;&#38169;&#25216;&#26415;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#20013;&#27880;&#20837;&#24694;&#24847;&#26377;&#25928;&#36733;&#33655;&#12290;MaleficNet 2.0&#27880;&#20837;&#25216;&#26415;&#20855;&#26377;&#38544;&#34109;&#24615;&#65292;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03593v1 Announce Type: cross  Abstract: Training high-quality deep learning models is a challenging task due to computational and technical requirements. A growing number of individuals, institutions, and companies increasingly rely on pre-trained, third-party models made available in public repositories. These models are often used directly or integrated in product pipelines with no particular precautions, since they are effectively just data in tensor form and considered safe. In this paper, we raise awareness of a new machine learning supply chain threat targeting neural networks. We introduce MaleficNet 2.0, a novel technique to embed self-extracting, self-executing malware in neural networks. MaleficNet 2.0 uses spread-spectrum channel coding combined with error correction techniques to inject malicious payloads in the parameters of deep neural networks. MaleficNet 2.0 injection technique is stealthy, does not degrade the performance of the model, and is robust against 
&lt;/p&gt;</description></item><item><title>&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#22312;&#21463;&#20445;&#25252;&#25968;&#25454;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.03592</link><description>&lt;p&gt;
Wildest Dreams: &#38544;&#31169;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#21487;&#22797;&#29616;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Wildest Dreams: Reproducible Research in Privacy-preserving Neural Network Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03592
&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#22312;&#21463;&#20445;&#25252;&#25968;&#25454;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#28041;&#21450;&#22810;&#20010;&#23398;&#31185;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#21253;&#25324;&#31038;&#20250;&#31185;&#23398;&#12289;&#37329;&#34701;&#21644;&#21307;&#23398;&#30740;&#31350;&#12290;ML&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#20854;&#24378;&#22823;&#31243;&#24230;&#21462;&#20915;&#20110;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#12290;&#30001;&#20110;ML&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#25968;&#25454;&#31185;&#23398;&#23478;&#32463;&#24120;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#65288;MLaaS&#65289;&#23558;&#35745;&#31639;&#22806;&#21253;&#32473;&#22806;&#37096;&#26381;&#21153;&#22120;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#31169;&#20154;&#20449;&#24687;&#65288;&#22914;&#36130;&#21153;&#25968;&#25454;&#25110;&#20581;&#24247;&#35760;&#24405;&#65289;&#26102;&#65292;&#23558;&#35745;&#31639;&#22806;&#21253;&#21487;&#33021;&#20250;&#23548;&#33268;&#38544;&#31169;&#38382;&#39064;&#12290;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65288;PPTs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65288;PPML&#65289;&#20351;&#24471;&#21487;&#20197;&#22312;&#21463;&#20445;&#25252;&#25968;&#25454;&#19978;&#36827;&#34892;ML&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#20173;&#22788;&#20110;&#21021;&#27493;&#38454;&#27573;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#30340;&#24212;&#29992;&#35201;&#27714;&#36739;&#39640;&#12290;&#20026;&#20102;&#29702;&#35299;&#29702;&#35770;&#30740;&#31350;&#24314;&#35758;&#19982;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03592v1 Announce Type: cross  Abstract: Machine Learning (ML), addresses a multitude of complex issues in multiple disciplines, including social sciences, finance, and medical research. ML models require substantial computing power and are only as powerful as the data utilized. Due to high computational cost of ML methods, data scientists frequently use Machine Learning-as-a-Service (MLaaS) to outsource computation to external servers. However, when working with private information, like financial data or health records, outsourcing the computation might result in privacy issues. Recent advances in Privacy-Preserving Techniques (PPTs) have enabled ML training and inference over protected data through the use of Privacy-Preserving Machine Learning (PPML). However, these techniques are still at a preliminary stage and their application in real-world situations is demanding. In order to comprehend discrepancy between theoretical research suggestions and actual applications, thi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#35299;&#37322;&#26694;&#26550;RouteExplainer&#65292;&#23454;&#29616;&#20102;&#36793;&#30340;&#24433;&#21709;&#35299;&#37322;&#21644;&#24847;&#22270;&#25512;&#26029;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#35299;&#37322;&#25991;&#26412;&#65292;&#24182;&#22312;&#22810;&#20010;VRP&#19978;&#36827;&#34892;&#20102;&#37327;&#21270;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.03585</link><description>&lt;p&gt;
RouteExplainer&#65306;&#19968;&#31181;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#35299;&#37322;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RouteExplainer: An Explanation Framework for Vehicle Routing Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03585
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#35299;&#37322;&#26694;&#26550;RouteExplainer&#65292;&#23454;&#29616;&#20102;&#36793;&#30340;&#24433;&#21709;&#35299;&#37322;&#21644;&#24847;&#22270;&#25512;&#26029;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#35299;&#37322;&#25991;&#26412;&#65292;&#24182;&#22312;&#22810;&#20010;VRP&#19978;&#36827;&#34892;&#20102;&#37327;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#26159;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24050;&#24212;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#38382;&#39064;&#12290;&#23613;&#31649;&#23545;VRP&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#25913;&#21892;&#23454;&#38469;VRP&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#20114;&#21160;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36825;&#20010;&#39046;&#22495;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RouteExplainer&#65292;&#19968;&#31181;&#20107;&#21518;&#35299;&#37322;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#22312;&#29983;&#25104;&#36335;&#24452;&#20013;&#27599;&#26465;&#36793;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#36335;&#24452;&#20026;&#21160;&#20316;&#24207;&#21015;&#24182;&#22522;&#20110;&#21160;&#20316;&#24433;&#21709;&#27169;&#22411;&#25193;&#23637;&#23545;VRP&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#20026;&#20102;&#22686;&#24378;&#35299;&#37322;&#65292;&#25105;&#20204;&#39069;&#22806;&#25552;&#20986;&#20102;&#19968;&#20010;&#36793;&#30028;&#20998;&#31867;&#22120;&#65292;&#25512;&#26029;&#27599;&#20010;&#36793;&#30028;&#30340;&#24847;&#22270;&#65292;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#36793;&#30028;&#20998;&#31867;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#21450;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#35299;&#37322;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;VRP&#19978;&#23450;&#37327;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#36793;&#30028;&#20998;&#31867;&#22120;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#36895;&#24230;&#30340;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03585v1 Announce Type: cross  Abstract: The Vehicle Routing Problem (VRP) is a widely studied combinatorial optimization problem and has been applied to various practical problems. While the explainability for VRP is significant for improving the reliability and interactivity in practical VRP applications, it remains unexplored. In this paper, we propose RouteExplainer, a post-hoc explanation framework that explains the influence of each edge in a generated route. Our framework realizes this by rethinking a route as the sequence of actions and extending counterfactual explanations based on the action influence model to VRP. To enhance the explanation, we additionally propose an edge classifier that infers the intentions of each edge, a loss function to train the edge classifier, and explanation-text generation by Large Language Models (LLMs). We quantitatively evaluate our edge classifier on four different VRPs. The results demonstrate its rapid computation while maintaining
&lt;/p&gt;</description></item><item><title>&#35813;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207; adaptNMT &#31616;&#21270;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#65292;&#25552;&#20379;&#20102;&#22270;&#24418;&#21270;&#35757;&#32451;&#36827;&#23637;&#23637;&#31034;&#12289;&#23376;&#35789;&#20998;&#21106;&#27169;&#22411;&#21644;&#19968;&#38190;&#24335;&#27169;&#22411;&#24320;&#21457;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03582</link><description>&lt;p&gt;
&#20026;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#35774;&#35745;&#30340;&#24320;&#28304;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Design of an Open-Source Architecture for Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207; adaptNMT &#31616;&#21270;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#65292;&#25552;&#20379;&#20102;&#22270;&#24418;&#21270;&#35757;&#32451;&#36827;&#23637;&#23637;&#31034;&#12289;&#23376;&#35789;&#20998;&#21106;&#27169;&#22411;&#21644;&#19968;&#38190;&#24335;&#27169;&#22411;&#24320;&#21457;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03582v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; adaptNMT &#26159;&#19968;&#20010;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#21644;&#37096;&#32626;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#26500;&#24314;&#22312;&#24191;&#27867;&#37319;&#29992;&#30340;OpenNMT&#29983;&#24577;&#31995;&#32479;&#20043;&#19978;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#35813;&#39046;&#22495;&#30340;&#26032;&#36827;&#20837;&#32773;&#65292;&#22240;&#20026;&#23427;&#31616;&#21270;&#20102;&#24320;&#21457;&#29615;&#22659;&#30340;&#35774;&#32622;&#20197;&#21450;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#19968;&#20010;&#22270;&#24418;&#21270;&#21151;&#33021;&#65292;&#29992;&#20110;&#23637;&#31034;&#27169;&#22411;&#35757;&#32451;&#30340;&#36827;&#23637;&#65292;&#24182;&#37319;&#29992;SentencePiece&#26469;&#21019;&#24314;&#23376;&#35789;&#20998;&#21106;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#31616;&#21270;&#20102;&#36229;&#21442;&#25968;&#30340;&#23450;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#24212;&#29992;&#23454;&#29616;&#20102;&#19968;&#38190;&#24335;&#27169;&#22411;&#24320;&#21457;&#26041;&#27861;&#65292;&#36890;&#36807;adaptNMT&#24320;&#21457;&#30340;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#40723;&#21169;&#29615;&#20445;&#30740;&#31350;&#65292;adaptNMT&#38598;&#25104;&#20102;&#19968;&#20010;&#29615;&#20445;&#25253;&#21578;&#65292;&#26631;&#24535;&#30528;&#33021;&#28304;&#28040;&#32791;&#21644;&#20108;&#27687;&#21270;&#30899;&#25490;&#25918;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03582v1 Announce Type: cross  Abstract: adaptNMT is an open-source application that offers a streamlined approach to the development and deployment of Recurrent Neural Networks and Transformer models. This application is built upon the widely-adopted OpenNMT ecosystem, and is particularly useful for new entrants to the field, as it simplifies the setup of the development environment and creation of train, validation, and test splits. The application offers a graphing feature that illustrates the progress of model training, and employs SentencePiece for creating subword segmentation models. Furthermore, the application provides an intuitive user interface that facilitates hyperparameter customization. Notably, a single-click model development approach has been implemented, and models developed by adaptNMT can be evaluated using a range of metrics. To encourage eco-friendly research, adaptNMT incorporates a green report that flags the power consumption and kgCO${_2}$ emissions
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20998;&#35299;&#30340;&#26694;&#26550;CDRSB&#65292;&#29992;&#20110;&#35843;&#33410;&#31038;&#20132;&#25512;&#33616;&#20013;&#30340;&#31038;&#20132;&#24433;&#21709;&#20559;&#24046;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03578</link><description>&lt;p&gt;
&#29992;&#20110;&#35843;&#33410;&#31038;&#20132;&#25512;&#33616;&#20013;&#31038;&#20132;&#24433;&#21709;&#20559;&#24046;&#30340;&#22240;&#26524;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Causal Disentanglement for Regulating Social Influence Bias in Social Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03578
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20998;&#35299;&#30340;&#26694;&#26550;CDRSB&#65292;&#29992;&#20110;&#35843;&#33410;&#31038;&#20132;&#25512;&#33616;&#20013;&#30340;&#31038;&#20132;&#24433;&#21709;&#20559;&#24046;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30528;&#31038;&#20132;&#24433;&#21709;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#20998;&#24378;&#35843;&#25512;&#33616;&#26379;&#21451;&#20114;&#21160;&#36807;&#30340;&#29289;&#21697;&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#26435;&#37325;&#35843;&#25972;&#25110;&#21033;&#29992;&#26080;&#20559;&#25968;&#25454;&#26469;&#28040;&#38500;&#36825;&#31181;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#24182;&#38750;&#25152;&#26377;&#20559;&#24046;&#37117;&#26159;&#26377;&#23475;&#30340;&#65292;&#21363;&#19968;&#20123;&#26379;&#21451;&#25512;&#33616;&#30340;&#29289;&#21697;&#21487;&#33021;&#19982;&#29992;&#25143;&#30340;&#20852;&#36259;&#30456;&#31526;&#12290;&#30450;&#30446;&#28040;&#38500;&#36825;&#20123;&#20559;&#24046;&#21487;&#33021;&#20250;&#21066;&#24369;&#36825;&#20123;&#31215;&#26497;&#24433;&#21709;&#65292;&#21487;&#33021;&#38477;&#20302;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20998;&#35299;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#33410;&#31038;&#20132;&#25512;&#33616;&#20013;&#30340;&#31038;&#20132;&#24433;&#21709;&#20559;&#24046;&#65292;&#21517;&#20026;CDRSB&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#21457;&#29616;&#29992;&#25143;&#31038;&#20132;&#32593;&#32476;&#21487;&#20197;&#34987;&#35270;&#20026;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#65288;&#22788;&#29702;&#65289;&#20197;&#21450;&#35780;&#20998;&#65288;&#32467;&#26524;&#65289;&#20043;&#38388;&#30340;&#20849;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03578v1 Announce Type: cross  Abstract: Social recommendation systems face the problem of social influence bias, which can lead to an overemphasis on recommending items that friends have interacted with. Addressing this problem is crucial, and existing methods often rely on techniques such as weight adjustment or leveraging unbiased data to eliminate this bias. However, we argue that not all biases are detrimental, i.e., some items recommended by friends may align with the user's interests. Blindly eliminating such biases could undermine these positive effects, potentially diminishing recommendation accuracy. In this paper, we propose a Causal Disentanglement-based framework for Regulating Social influence Bias in social recommendation, named CDRSB, to improve recommendation performance. From the perspective of causal inference, we find that the user social network could be regarded as a confounder between the user and item embeddings (treatment) and ratings (outcome). Due t
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20221;&#29992;&#20110;&#20302;&#36164;&#28304;&#33521;&#35821;&#21040;&#29233;&#23572;&#20848;&#35821;&#35328;&#23545;&#30340;&#29305;&#23450;&#20581;&#24247;&#39046;&#22495;&#25968;&#25454;&#38598;&#65292;&#23454;&#35777;&#35777;&#26126;&#20351;&#29992;&#39046;&#22495;&#25968;&#25454;&#23545;&#20581;&#24247;&#39046;&#22495;&#20855;&#26377;&#26126;&#26174;&#22909;&#22788;&#65292;&#24182;&#23637;&#31034;&#30340;&#26368;&#22823;BLEU&#20998;&#25968;&#25552;&#21319;&#20026;22.2&#28857;&#65288;40%&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.03575</link><description>&lt;p&gt;
gaHealth: &#19968;&#20221;&#33521;&#35821;-&#29233;&#23572;&#20848;&#29233;&#23572;&#20848;&#21452;&#35821;&#20581;&#24247;&#25968;&#25454;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
gaHealth: An English-Irish Bilingual Corpus of Health Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03575
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20221;&#29992;&#20110;&#20302;&#36164;&#28304;&#33521;&#35821;&#21040;&#29233;&#23572;&#20848;&#35821;&#35328;&#23545;&#30340;&#29305;&#23450;&#20581;&#24247;&#39046;&#22495;&#25968;&#25454;&#38598;&#65292;&#23454;&#35777;&#35777;&#26126;&#20351;&#29992;&#39046;&#22495;&#25968;&#25454;&#23545;&#20581;&#24247;&#39046;&#22495;&#20855;&#26377;&#26126;&#26174;&#22909;&#22788;&#65292;&#24182;&#23637;&#31034;&#30340;&#26368;&#22823;BLEU&#20998;&#25968;&#25552;&#21319;&#20026;22.2&#28857;&#65288;40%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03575v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#30028;  &#25688;&#35201;&#65306;&#26426;&#22120;&#32763;&#35793;&#26159;&#19968;&#31181;&#25104;&#29087;&#30340;&#25216;&#26415;&#65292;&#36866;&#29992;&#20110;&#35768;&#22810;&#39640;&#36164;&#28304;&#35821;&#35328;&#23545;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#29615;&#22659;&#20013;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#21487;&#29992;&#20110;&#24320;&#21457;&#32763;&#35793;&#27169;&#22411;&#30340;&#24179;&#34892;&#25968;&#25454;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#24320;&#21457;&#25968;&#25454;&#38598;&#30340;&#24037;&#20316;&#24448;&#24448;&#38598;&#20013;&#20110;&#31616;&#21333;&#22320;&#21019;&#24314;&#29992;&#20110;&#36890;&#29992;&#32763;&#35793;&#30340;&#23613;&#21487;&#33021;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#24456;&#23481;&#26131;&#24573;&#35270;&#20351;&#29992;&#23567;&#22411;&#39046;&#22495;&#25968;&#25454;&#38598;&#30340;&#22909;&#22788;&#21644;&#21457;&#23637;&#12290;&#20026;&#20102;&#35780;&#20272;&#20351;&#29992;&#39046;&#22495;&#25968;&#25454;&#30340;&#20248;&#28857;&#65292;&#20026;&#20302;&#36164;&#28304;&#30340;&#33521;&#35821;&#21040;&#29233;&#23572;&#20848;&#35821;&#35328;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#20581;&#24247;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#27010;&#36848;&#20102;&#24320;&#21457;&#35821;&#26009;&#24211;&#25152;&#20351;&#29992;&#30340;&#36807;&#31243;&#65292;&#24182;&#20174;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#20351;&#29992;&#20581;&#24247;&#39046;&#22495;&#30340;&#39046;&#22495;&#25968;&#25454;&#30340;&#22909;&#22788;&#12290;&#22312;&#32763;&#35793;&#19982;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#65292;&#20351;&#29992;gaHealth&#35821;&#26009;&#24211;&#24320;&#21457;&#30340;&#27169;&#22411;&#22312;&#27604;&#36739;&#26102;&#26174;&#31034;&#20986;&#26368;&#22823;BLEU&#20998;&#25968;&#25552;&#21319;22.2&#28857;&#65288;40%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03575v1 Announce Type: cross  Abstract: Machine Translation is a mature technology for many high-resource language pairs. However in the context of low-resource languages, there is a paucity of parallel data datasets available for developing translation models. Furthermore, the development of datasets for low-resource languages often focuses on simply creating the largest possible dataset for generic translation. The benefits and development of smaller in-domain datasets can easily be overlooked. To assess the merits of using in-domain data, a dataset for the specific domain of health was developed for the low-resource English to Irish language pair. Our study outlines the process used in developing the corpus and empirically demonstrates the benefits of using an in-domain dataset for the health domain. In the context of translating health-related data, models developed using the gaHealth corpus demonstrated a maximum BLEU score improvement of 22.2 points (40%) when compared
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#21644;&#24773;&#32490;&#25805;&#32437;&#30740;&#31350;&#20102;OpenAI&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#30340;&#21453;&#24212;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#24182;&#23545;&#24773;&#32490;&#25552;&#31034;&#26377;&#32454;&#33268;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.03550</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#21152;&#22823;&#20102;&#20154;&#24037;&#26234;&#33021;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#24773;&#32490;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03550
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#21644;&#24773;&#32490;&#25805;&#32437;&#30740;&#31350;&#20102;OpenAI&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#30340;&#21453;&#24212;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#24182;&#23545;&#24773;&#32490;&#25552;&#31034;&#26377;&#32454;&#33268;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#35843;&#26597;&#20102;OpenAI&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21512;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#23545;&#24773;&#32490;&#25552;&#31034;&#30340;&#21709;&#24212;&#12290;&#21033;&#29992;davinci-002&#12289;davinci-003&#12289;gpt-3.5-turbo&#21644;gpt-4&#31561;&#21508;&#31181;LLM&#36845;&#20195;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#35780;&#20272;&#23427;&#20204;&#22312;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#30340;&#25104;&#21151;&#29575;&#12290;&#22522;&#20110;&#19968;&#32452;&#21253;&#21547;19,800&#26465;&#21512;&#25104;&#34394;&#20551;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;OpenAI&#30340;&#25152;&#26377;LLMs&#37117;&#33021;&#25104;&#21151;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#24182;&#19988;&#23427;&#20204;&#26377;&#25928;&#22320;&#21709;&#24212;&#24773;&#32490;&#25552;&#31034;&#65292;&#34920;&#26126;&#23427;&#20204;&#23545;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24773;&#32490;&#32447;&#32034;&#26377;&#24494;&#22937;&#30340;&#29702;&#35299;&#12290;&#22312;&#31036;&#35980;&#25552;&#31034;&#19979;&#65292;&#25152;&#26377;&#30740;&#31350;&#20013;&#30340;LLMs&#37117;&#20197;&#39640;&#39057;&#29575;&#19968;&#33268;&#22320;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#24403;&#20197;&#19981;&#31036;&#35980;&#26041;&#24335;&#25552;&#31034;&#26102;&#65292;&#34394;&#20551;&#20449;&#24687;&#29983;&#25104;&#30340;&#39057;&#29575;&#20250;&#38477;&#20302;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20250;&#25298;&#32477;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#32780;&#26159;&#35686;&#21578;&#29992;&#25143;&#35813;&#24037;&#20855;&#19981;&#36866;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03550v1 Announce Type: new  Abstract: This study investigates the generation of synthetic disinformation by OpenAI's Large Language Models (LLMs) through prompt engineering and explores their responsiveness to emotional prompting. Leveraging various LLM iterations using davinci-002, davinci-003, gpt-3.5-turbo and gpt-4, we designed experiments to assess their success in producing disinformation. Our findings, based on a corpus of 19,800 synthetic disinformation social media posts, reveal that all LLMs by OpenAI can successfully produce disinformation, and that they effectively respond to emotional prompting, indicating their nuanced understanding of emotional cues in text generation. When prompted politely, all examined LLMs consistently generate disinformation at a high frequency. Conversely, when prompted impolitely, the frequency of disinformation production diminishes, as the models often refuse to generate disinformation and instead caution users that the tool is not in
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20449;&#24687;&#29109;&#36827;&#34892;&#25552;&#31034;&#25366;&#25496;&#65292;&#25506;&#32034;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#65292;&#25552;&#39640;&#22522;&#20110;&#35821;&#35328;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;</title><link>https://arxiv.org/abs/2403.03544</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#30340;&#25552;&#31034;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Prompt Mining for Language-based Human Mobility Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03544
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20449;&#24687;&#29109;&#36827;&#34892;&#25552;&#31034;&#25366;&#25496;&#65292;&#25506;&#32034;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#65292;&#25552;&#39640;&#22522;&#20110;&#35821;&#35328;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;&#35821;&#35328;&#30340;&#39044;&#27979;&#26368;&#36817;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#20986;&#29616;&#65292;&#29992;&#20110;&#39044;&#27979;&#20154;&#31867;&#31227;&#21160;&#27169;&#24335;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#20351;&#29992;&#25552;&#31034;&#23558;&#20197;&#25968;&#23383;&#20540;&#32473;&#20986;&#30340;&#21407;&#22987;&#31227;&#21160;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26410;&#26469;&#35266;&#23519;&#30340;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20165;&#37319;&#29992;&#22266;&#23450;&#21644;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#26495;&#23558;&#25968;&#23383;&#20540;&#36716;&#21270;&#20026;&#21477;&#23376;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25552;&#31034;&#65292;&#20351;&#29992;&#22266;&#23450;&#27169;&#26495;&#36827;&#34892;&#25552;&#31034;&#21487;&#33021;&#38480;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#31227;&#21160;&#39044;&#27979;&#25552;&#31034;&#25366;&#25496;&#26694;&#26550;&#65292;&#26088;&#22312;&#25506;&#32034;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#22522;&#20110;&#25552;&#31034;&#20449;&#24687;&#29109;&#30340;&#25552;&#31034;&#29983;&#25104;&#38454;&#27573;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03544v1 Announce Type: new  Abstract: With the advancement of large language models, language-based forecasting has recently emerged as an innovative approach for predicting human mobility patterns. The core idea is to use prompts to transform the raw mobility data given as numerical values into natural language sentences so that the language models can be leveraged to generate the description for future observations. However, previous studies have only employed fixed and manually designed templates to transform numerical values into sentences. Since the forecasting performance of language models heavily relies on prompts, using fixed templates for prompting may limit the forecasting capability of language models. In this paper, we propose a novel framework for prompt mining in language-based mobility forecasting, aiming to explore diverse prompt design strategies. Specifically, the framework includes a prompt generation stage based on the information entropy of prompts and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#33258;&#21160;&#24191;&#25773;&#24191;&#21578;&#26816;&#27979;&#25216;&#26415;RADIA&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#24191;&#25773;&#20013;&#30340;&#21363;&#20852;&#21644;&#26032;&#24191;&#21578;&#65292;&#20026;&#24191;&#25773;&#24191;&#21578;&#26816;&#27979;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1-macro&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.03538</link><description>&lt;p&gt;
RADIA -- &#20855;&#26377;&#26234;&#33021;&#20998;&#26512;&#30340;&#24191;&#25773;&#24191;&#21578;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
RADIA -- Radio Advertisement Detection with Intelligent Analytics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#33258;&#21160;&#24191;&#25773;&#24191;&#21578;&#26816;&#27979;&#25216;&#26415;RADIA&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#24191;&#25773;&#20013;&#30340;&#21363;&#20852;&#21644;&#26032;&#24191;&#21578;&#65292;&#20026;&#24191;&#25773;&#24191;&#21578;&#26816;&#27979;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1-macro&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#25773;&#24191;&#21578;&#20173;&#28982;&#26159;&#29616;&#20195;&#33829;&#38144;&#31574;&#30053;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#20854;&#21560;&#24341;&#21147;&#21644;&#38024;&#23545;&#24615;&#20256;&#25773;&#30340;&#28508;&#21147;&#19981;&#21487;&#21542;&#35748;&#12290;&#28982;&#32780;&#65292;&#24191;&#25773;&#25773;&#20986;&#26102;&#38388;&#30340;&#21160;&#24577;&#24615;&#21644;&#22810;&#20010;&#24191;&#25773;&#24191;&#21578;&#30340;&#26085;&#30410;&#22686;&#38271;&#36235;&#21183;&#20026;&#30417;&#27979;&#24191;&#21578;&#25773;&#20986;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#35201;&#27714;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19968;&#31181;&#32467;&#21512;&#20808;&#36827;&#35821;&#38899;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#30340;&#26032;&#22411;&#33258;&#21160;&#24191;&#25773;&#24191;&#21578;&#26816;&#27979;&#25216;&#26415;&#12290;RADIA&#30340;&#26041;&#27861;&#36890;&#36807;&#28040;&#38500;&#23545;&#24191;&#25773;&#20869;&#23481;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#38656;&#27714;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;&#36825;&#19968;&#36129;&#29486;&#20801;&#35768;&#26816;&#27979;&#21363;&#20852;&#21644;&#26032;&#24341;&#20837;&#30340;&#24191;&#21578;&#65292;&#20026;&#24191;&#25773;&#24191;&#21578;&#26816;&#27979;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#31934;&#24515;&#20998;&#21106;&#21644;&#26631;&#35760;&#30340;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;F1-macro&#24471;&#20998;87.76&#65292;&#29702;&#35770;&#26368;&#22823;&#20540;&#20026;89.33&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03538v1 Announce Type: cross  Abstract: Radio advertising remains an integral part of modern marketing strategies, with its appeal and potential for targeted reach undeniably effective. However, the dynamic nature of radio airtime and the rising trend of multiple radio spots necessitates an efficient system for monitoring advertisement broadcasts. This study investigates a novel automated radio advertisement detection technique incorporating advanced speech recognition and text classification algorithms. RadIA's approach surpasses traditional methods by eliminating the need for prior knowledge of the broadcast content. This contribution allows for detecting impromptu and newly introduced advertisements, providing a comprehensive solution for advertisement detection in radio broadcasting. Experimental results show that the resulting model, trained on carefully segmented and tagged text data, achieves an F1-macro score of 87.76 against a theoretical maximum of 89.33. This pape
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;E2URec&#65292;&#36825;&#26159;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#36951;&#24536;&#29305;&#23450;&#29992;&#25143;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.03536</link><description>&lt;p&gt;
&#20026;&#25512;&#33616;&#32780;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient and Effective Unlearning of Large Language Models for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03536
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;E2URec&#65292;&#36825;&#26159;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#36951;&#24536;&#29305;&#23450;&#29992;&#25143;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#36827;&#23637;&#20135;&#29983;&#20102;&#19968;&#39033;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#21033;&#29992;LLMs&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#65288;LLMRec&#65289;&#12290; LLMRec&#30340;&#26377;&#25928;&#24615;&#28304;&#33258;LLMs&#22266;&#26377;&#30340;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290; LLMRec&#36890;&#36807;&#22522;&#20110;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#30340;&#25351;&#23548;&#35843;&#25972;&#33719;&#24471;&#25512;&#33616;&#21151;&#33021;&#12290; &#28982;&#32780;&#65292;&#20026;&#20102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#24182;&#20248;&#21270;&#25928;&#29992;&#65292;LLMRec&#36824;&#24517;&#39035;&#26377;&#24847;&#24536;&#35760;&#29305;&#23450;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#31216;&#20026;&#25512;&#33616;&#36951;&#24536;&#12290; &#22312;LLMs&#26102;&#20195;&#65292;&#25512;&#33616;&#36951;&#24536;&#22312;\textit{&#25928;&#29575;}&#21644;\textit{&#26377;&#25928;&#24615;}&#26041;&#38754;&#20026;LLMRec&#24102;&#26469;&#20102;&#26032;&#25361;&#25112;&#12290; &#29616;&#26377;&#30340;&#36951;&#24536;&#26041;&#27861;&#38656;&#35201;&#26356;&#26032;LLMRec&#20013;&#25968;&#21313;&#20159;&#21442;&#25968;&#65292;&#36825;&#26159;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#12290; &#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#36951;&#24536;&#36807;&#31243;&#20013;&#24635;&#26159;&#24433;&#21709;&#27169;&#22411;&#25928;&#29992;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\textbf{E2URec}&#65292;&#31532;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03536v1 Announce Type: cross  Abstract: The significant advancements in large language models (LLMs) give rise to a promising research direction, i.e., leveraging LLMs as recommenders (LLMRec). The efficacy of LLMRec arises from the open-world knowledge and reasoning capabilities inherent in LLMs. LLMRec acquires the recommendation capabilities through instruction tuning based on user interaction data. However, in order to protect user privacy and optimize utility, it is also crucial for LLMRec to intentionally forget specific user data, which is generally referred to as recommendation unlearning. In the era of LLMs, recommendation unlearning poses new challenges for LLMRec in terms of \textit{inefficiency} and \textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process. To this end, we propose \textbf{E2URec}, the first
&lt;/p&gt;</description></item><item><title>IB-Net &#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26032;&#39062;&#30340;&#22270;&#32534;&#30721;&#25216;&#26415;&#26469;&#21152;&#36895;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#27714;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.03517</link><description>&lt;p&gt;
IB-Net: &#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#20013;&#21464;&#37327;&#20915;&#31574;&#30340;&#21021;&#22987;&#20998;&#25903;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
IB-Net: Initial Branch Network for Variable Decision in Boolean Satisfiability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03517
&lt;/p&gt;
&lt;p&gt;
IB-Net &#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26032;&#39062;&#30340;&#22270;&#32534;&#30721;&#25216;&#26415;&#26469;&#21152;&#36895;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#26159;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#29305;&#21035;&#26159;&#22312;&#36923;&#36753;&#31561;&#25928;&#26816;&#26597;&#36807;&#31243;&#20013;&#12290;&#30446;&#21069;&#65292;SAT&#27714;&#35299;&#22120;&#34987;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#23581;&#35797;&#23558;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#27714;&#35299;&#22120;&#30340;&#36741;&#21161;&#12290;&#28982;&#32780;&#65292;&#22312;LEC&#19978;&#19979;&#25991;&#20013;&#30340;SAT&#38382;&#39064;&#30001;&#20110;&#20854;&#20027;&#35201;&#20026;&#19981;&#21487;&#28385;&#36275;&#24615;&#36136;&#21644;&#22823;&#37327;UNSAT-core&#21464;&#37327;&#32780;&#19982;&#20247;&#19981;&#21516;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#36741;&#21161;&#22312;&#36825;&#19968;&#19987;&#19994;&#39046;&#22495;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IB-Net&#65292;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26032;&#39062;&#30340;&#22270;&#32534;&#30721;&#25216;&#26415;&#26469;&#24314;&#27169;&#19981;&#21487;&#28385;&#36275;&#38382;&#39064;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;&#20114;&#21160;&#12290;&#23545;&#27714;&#35299;&#22120;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;IB-Net&#22312;&#24037;&#19994;&#25968;&#25454;&#19978;&#30340;&#24179;&#22343;&#36816;&#34892;&#26102;&#38388;&#21152;&#36895;&#20102;5.0%&#65292;&#22312;SAT&#31454;&#36187;&#25968;&#25454;&#19978;&#21152;&#24555;&#20102;8.3% &#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03517v1 Announce Type: new  Abstract: Boolean Satisfiability problems are vital components in Electronic Design Automation, particularly within the Logic Equivalence Checking process. Currently, SAT solvers are employed for these problems and neural network is tried as assistance to solvers. However, as SAT problems in the LEC context are distinctive due to their predominantly unsatisfiability nature and a substantial proportion of UNSAT-core variables, existing neural network assistance has proven unsuccessful in this specialized domain. To tackle this challenge, we propose IB-Net, an innovative framework utilizing graph neural networks and novel graph encoding techniques to model unsatisfiable problems and interact with state-of-the-art solvers. Extensive evaluations across solvers and datasets demonstrate IB-Net's acceleration, achieving an average runtime speedup of 5.0% on industrial data and 8.3% on SAT competition data empirically. This breakthrough advances efficient
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.03506</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#28041;&#21450;&#24102;&#26377;&#26377;&#38480;&#36793;&#30028;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26816;&#27979;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#30740;&#31350;&#24212;&#35206;&#30422;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#29983;&#25104;&#30340;&#19981;&#21516;&#31867;&#22411;&#28151;&#21512;&#25991;&#26412;&#65292;&#20197;&#26356;&#22909;&#22320;&#25351;&#23548;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;CoAuthor&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#36890;&#36807;&#20154;&#31867;&#20316;&#32773;&#21644;&#26234;&#33021;&#20889;&#20316;&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#29983;&#25104;&#30340;&#22810;&#36718;&#20132;&#20114;&#20013;&#20135;&#29983;&#30340;&#22810;&#26679;&#21270;&#12289;&#30495;&#23454;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#27493;&#20998;&#21106;&#20026;&#22522;&#30784;&#30340;&#27969;&#31243;&#65306;(i)&#26816;&#27979;&#32473;&#23450;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#21508;&#20010;&#27573;&#33853;&#65292;&#20854;&#20013;&#27599;&#20010;&#27573;&#33853;&#21253;&#21547;&#19968;&#33268;&#20316;&#32773;&#30340;&#21477;&#23376;&#65292;&#20197;&#21450;(ii)&#20998;&#31867;&#27599;&#20010;&#30830;&#23450;&#27573;&#33853;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03506v1 Announce Type: cross  Abstract: This study explores the challenge of sentence-level AI-generated text detection within human-AI collaborative hybrid texts. Existing studies of AI-generated text detection for hybrid texts often rely on synthetic datasets. These typically involve hybrid texts with a limited number of boundaries. We contend that studies of detecting AI-generated content within hybrid texts should cover different types of hybrid texts generated in realistic settings to better inform real-world applications. Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid texts generated through the collaboration between human writers and an intelligent writing system in multi-turn interactions. We adopt a two-step, segmentation-based pipeline: (i) detect segments within a given hybrid text where each segment contains sentences of consistent authorship, and (ii) classify the authorship of each identified segment. Our empirical 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;DLP-GAN&#27169;&#22411;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#20102;&#29616;&#20195;&#20013;&#22269;&#39118;&#26223;&#29031;&#29255;&#30340;&#32472;&#21046;&#65292;&#24341;&#20837;&#20102;&#19981;&#23545;&#31216;&#24490;&#29615;&#26144;&#23556;&#21644;&#21452;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#24179;&#34913;&#29616;&#23454;&#24863;&#21644;&#25277;&#35937;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03456</link><description>&lt;p&gt;
DLP-GAN&#65306;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23398;&#20064;&#32472;&#21046;&#29616;&#20195;&#20013;&#22269;&#39118;&#26223;&#29031;&#29255;
&lt;/p&gt;
&lt;p&gt;
DLP-GAN: Learning to Draw Modern Chinese Landscape Photos with Generative Adversarial Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;DLP-GAN&#27169;&#22411;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#20102;&#29616;&#20195;&#20013;&#22269;&#39118;&#26223;&#29031;&#29255;&#30340;&#32472;&#21046;&#65292;&#24341;&#20837;&#20102;&#19981;&#23545;&#31216;&#24490;&#29615;&#26144;&#23556;&#21644;&#21452;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#24179;&#34913;&#29616;&#23454;&#24863;&#21644;&#25277;&#35937;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#23665;&#27700;&#30011;&#20855;&#26377;&#29420;&#29305;&#21644;&#33402;&#26415;&#30340;&#39118;&#26684;&#65292;&#20854;&#32472;&#30011;&#25216;&#26415;&#22312;&#33394;&#24425;&#20351;&#29992;&#21644;&#29289;&#20307;&#30340;&#36924;&#30495;&#34920;&#29616;&#19978;&#39640;&#24230;&#25277;&#35937;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20174;&#29616;&#20195;&#29031;&#29255;&#36716;&#25442;&#21040;&#21476;&#20195;&#27700;&#22696;&#30011;&#19978;&#65292;&#20294;&#24456;&#23569;&#20851;&#27880;&#23558;&#39118;&#26223;&#30011;&#36716;&#21270;&#20026;&#29616;&#20195;&#29031;&#29255;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DLP-GAN&#65288;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#32472;&#21046;&#29616;&#20195;&#20013;&#22269;&#39118;&#26223;&#29031;&#29255;&#65289;&#65292;&#19968;&#20010;&#20855;&#26377;&#26032;&#39062;&#30340;&#19981;&#23545;&#31216;&#24490;&#29615;&#26144;&#23556;&#30340;&#26080;&#30417;&#30563;&#36328;&#22495;&#22270;&#20687;&#36716;&#25442;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#23494;&#38598;&#34701;&#21512;&#27169;&#22359;&#30340;&#29983;&#25104;&#22120;&#26469;&#21305;&#37197;&#19981;&#21516;&#30340;&#36716;&#25442;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#21452;&#19968;&#33268;&#24615;&#25439;&#22833;&#20197;&#24179;&#34913;&#27169;&#22411;&#32472;&#30011;&#30340;&#36924;&#30495;&#24615;&#21644;&#25277;&#35937;&#24615;&#65292;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20197;&#29616;&#20195;&#24863;&#32472;&#21046;&#39118;&#26223;&#29031;&#29255;&#21644;&#32032;&#25551;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03456v1 Announce Type: cross  Abstract: Chinese landscape painting has a unique and artistic style, and its drawing technique is highly abstract in both the use of color and the realistic representation of objects. Previous methods focus on transferring from modern photos to ancient ink paintings. However, little attention has been paid to translating landscape paintings into modern photos. To solve such problems, in this paper, we (1) propose DLP-GAN (\textbf{D}raw Modern Chinese \textbf{L}andscape \textbf{P}hotos with \textbf{G}enerative \textbf{A}dversarial \textbf{N}etwork), an unsupervised cross-domain image translation framework with a novel asymmetric cycle mapping, and (2) introduce a generator based on a dense-fusion module to match different translation directions. Moreover, a dual-consistency loss is proposed to balance the realism and abstraction of model painting. In this way, our model can draw landscape photos and sketches in the modern sense. Finally, based o
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;&#26041;&#27861;&#38024;&#23545;DeepONets&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25512;&#26029;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03444</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;&#36827;&#34892;DeepONets&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification for deeponets with ensemble kalman inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03444
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;&#26041;&#27861;&#38024;&#23545;DeepONets&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25805;&#20316;&#21592;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;DeepONet&#65292;&#22240;&#20854;&#39640;&#25928;&#22320;&#23398;&#20064;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#20989;&#25968;&#20043;&#38388;&#30340;&#22797;&#26434;&#26144;&#23556;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#26377;&#38480;&#19988;&#24102;&#22122;&#22768;&#25968;&#25454;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#35775;&#38382;DeepONet&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#21629;&#20851;&#38190;&#25110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#35745;&#31639;&#23494;&#38598;&#65292;&#35201;&#20040;&#20135;&#29983;&#20196;&#20154;&#19981;&#28385;&#24847;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20026;DeepONets&#37327;&#36523;&#23450;&#21046;&#39640;&#25928;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#25216;&#26415;&#30041;&#19979;&#20102;&#31354;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;&#65288;EKI&#65289;&#26041;&#27861;&#30340;&#26032;&#22411;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#25805;&#20316;&#21592;&#23398;&#20064;&#30340;&#39640;&#25928;UQ&#12290;EKI&#20197;&#20854;&#26080;&#23548;&#25968;&#12289;&#22122;&#22768;&#25239;&#24178;&#25200;&#21644;&#39640;&#24230;&#21487;&#24182;&#34892;&#21270;&#30340;&#29305;&#24615;&#32780;&#38395;&#21517;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#22312;&#38754;&#21521;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;UQ&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03444v1 Announce Type: cross  Abstract: In recent years, operator learning, particularly the DeepONet, has received much attention for efficiently learning complex mappings between input and output functions across diverse fields. However, in practical scenarios with limited and noisy data, accessing the uncertainty in DeepONet predictions becomes essential, especially in mission-critical or safety-critical applications. Existing methods, either computationally intensive or yielding unsatisfactory uncertainty quantification, leave room for developing efficient and informative uncertainty quantification (UQ) techniques tailored for DeepONets. In this work, we proposed a novel inference approach for efficient UQ for operator learning by harnessing the power of the Ensemble Kalman Inversion (EKI) approach. EKI, known for its derivative-free, noise-robust, and highly parallelizable feature, has demonstrated its advantages for UQ for physics-informed neural networks [28]. Our inn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#28151;&#21512;LoRAs&#65288;MoA&#65289;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#26631;&#31614;&#21644;&#26174;&#24335;&#36335;&#30001;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#20219;&#21153;&#24178;&#25200;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03432</link><description>&lt;p&gt;
&#28151;&#21512;LoRAs&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03432
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#28151;&#21512;LoRAs&#65288;MoA&#65289;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#26631;&#31614;&#21644;&#26174;&#24335;&#36335;&#30001;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#20219;&#21153;&#24178;&#25200;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#20248;&#26377;&#28508;&#21147;&#28608;&#21457;&#25110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29305;&#23450;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#25968;&#25454;&#30340;&#27491;&#30830;&#24179;&#34913;&#23545;&#20110;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#20219;&#21153;&#20043;&#38388;&#30340;&#24178;&#25200;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#24182;&#22686;&#24378;&#35757;&#32451;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;LLMs&#30340;&#26032;&#39062;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35843;&#20248;&#26041;&#27861;&#8212;&#8212;&#28151;&#21512;LoRAs&#65288;MoA&#65289;&#26550;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30456;&#24212;&#30340;&#30417;&#30563;&#35821;&#26009;&#24211;&#25968;&#25454;&#21333;&#29420;&#35757;&#32451;&#22810;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;LoRA&#27169;&#22359;&#12290;&#36825;&#20123;LoRA&#27169;&#22359;&#21487;&#20197;&#19982;&#22312;&#19987;&#23478;&#35774;&#35745;&#21407;&#21017;&#20013;&#35266;&#23519;&#21040;&#30340;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#30456;&#19968;&#33268;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26174;&#24335;&#36335;&#30001;&#31574;&#30053;&#32452;&#21512;&#22810;&#20010;LoRAs&#65292;&#24182;&#24341;&#20837;&#39046;&#22495;&#26631;&#31614;&#20197;&#20419;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#36825;&#26377;&#21161;&#20110;&#38450;&#27490;&#20219;&#21153;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#24182;&#26368;&#32456;&#25552;&#21319;&#27599;&#20010;&#20010;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03432v1 Announce Type: cross  Abstract: Instruction Tuning has the potential to stimulate or enhance specific capabilities of large language models (LLMs). However, achieving the right balance of data is crucial to prevent catastrophic forgetting and interference between tasks. To address these limitations and enhance training flexibility, we propose the Mixture-of-LoRAs (MoA) architecture which is a novel and parameter-efficient tuning method designed for multi-task learning with LLMs. In this paper, we start by individually training multiple domain-specific LoRA modules using corresponding supervised corpus data. These LoRA modules can be aligned with the expert design principles observed in Mixture-of-Experts (MoE). Subsequently, we combine the multiple LoRAs using an explicit routing strategy and introduce domain labels to facilitate multi-task learning, which help prevent interference between tasks and ultimately enhances the performance of each individual task. Further
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LEAD&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#35299;&#20026;&#28304;&#24050;&#30693;&#21644;&#26410;&#30693;&#32452;&#20214;&#26469;&#35782;&#21035;&#30446;&#26631;&#31169;&#26377;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.03421</link><description>&lt;p&gt;
LEAD&#65306;&#23398;&#20064;&#20998;&#35299;&#29992;&#20110;&#26080;&#28304;&#36890;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
LEAD: Learning Decomposition for Source-free Universal Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03421
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LEAD&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#35299;&#20026;&#28304;&#24050;&#30693;&#21644;&#26410;&#30693;&#32452;&#20214;&#26469;&#35782;&#21035;&#30446;&#26631;&#31169;&#26377;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UniDA&#65289;&#26088;&#22312;&#23454;&#29616;&#22312;&#23384;&#22312;&#21327;&#21464;&#37327;&#21644;&#26631;&#31614;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#12290; &#26368;&#36817;&#65292;&#26080;&#28304;&#36890;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UniDA&#65289;&#24050;&#32463;&#20986;&#29616;&#65292;&#26088;&#22312;&#23454;&#29616;UniDA&#32780;&#26080;&#38656;&#35775;&#38382;&#28304;&#25968;&#25454;&#65292;&#36825;&#26356;&#23454;&#29992;&#30001;&#20110;&#25968;&#25454;&#20445;&#25252;&#25919;&#31574;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;LEArning Decomposition&#65288;LEAD&#65289;&#30340;&#24819;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#35299;&#20026;&#28304;&#24050;&#30693;&#21644;&#26410;&#30693;&#32452;&#20214;&#26469;&#35782;&#21035;&#30446;&#26631;&#31169;&#26377;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03421v1 Announce Type: cross  Abstract: Universal Domain Adaptation (UniDA) targets knowledge transfer in the presence of both covariate and label shifts. Recently, Source-free Universal Domain Adaptation (SF-UniDA) has emerged to achieve UniDA without access to source data, which tends to be more practical due to data protection policies. The main challenge lies in determining whether covariate-shifted samples belong to target-private unknown categories. Existing methods tackle this either through hand-crafted thresholding or by developing time-consuming iterative clustering strategies. In this paper, we propose a new idea of LEArning Decomposition (LEAD), which decouples features into source-known and -unknown components to identify target-private data. Technically, LEAD initially leverages the orthogonal decomposition analysis for feature decomposition. Then, LEAD builds instance-level decision boundaries to adaptively identify target-private data. Extensive experiments a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;Distributional Dispreference Optimization (D$^2$O)&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#20154;&#31867;&#27491;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#40784;&#65292;&#20943;&#23569;&#20102;&#26377;&#23475;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;</title><link>https://arxiv.org/abs/2403.03419</link><description>&lt;p&gt;
&#21542;&#23450;&#21542;&#23450;&#65306;&#36890;&#36807;&#20998;&#24067;&#24335;&#21453;&#21916;&#22909;&#20248;&#21270;&#23454;&#29616;&#23545;&#40784;&#32780;&#26080;&#38656;&#20154;&#31867;&#27491;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03419
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;Distributional Dispreference Optimization (D$^2$O)&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#20154;&#31867;&#27491;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#40784;&#65292;&#20943;&#23569;&#20102;&#26377;&#23475;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#35282;&#33394;&#65292;&#20294;&#20063;&#21487;&#33021;&#23384;&#22312;&#20256;&#25773;&#19981;&#36947;&#24503;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#23545;&#40784;&#25216;&#26415;&#34987;&#24341;&#20837;&#20197;&#24341;&#23548;LLM&#26397;&#30528;&#20154;&#31867;&#20559;&#22909;&#26041;&#21521;&#21457;&#23637;&#65292;&#24182;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#27491;&#36127;&#35757;&#32451;&#23545;&#65292;&#21463;&#21040;&#22024;&#26434;&#26631;&#31614;&#21644;&#39318;&#36873;&#21644;&#38750;&#39318;&#36873;&#21709;&#24212;&#25968;&#25454;&#20043;&#38388;&#30340;&#36793;&#32536;&#21306;&#21035;&#30340;&#22256;&#25200;&#12290;&#37492;&#20110;&#26368;&#36817;LLM&#22312;&#29983;&#25104;&#26377;&#29992;&#21709;&#24212;&#26041;&#38754;&#30340;&#39640;&#27700;&#24179;&#65292;&#26412;&#25991;&#23558;&#30740;&#31350;&#37325;&#28857;&#36716;&#21521;&#19968;&#20010;&#26032;&#30340;&#26041;&#21521;&#65306;&#20165;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#36127;&#26679;&#26412;&#26469;&#23454;&#29616;&#23545;&#40784;&#65292;&#20445;&#30041;&#26377;&#29992;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#26377;&#23475;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#21453;&#21916;&#22909;&#20248;&#21270;&#65288;D$^2$O&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#30340;&#21709;&#24212;&#19982;&#38750;&#39318;&#36873;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26377;&#25928;&#22320;&#25490;&#38500;&#26377;&#23475;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03419v1 Announce Type: cross  Abstract: Large language models (LLMs) have revolutionized the role of AI, yet also pose potential risks of propagating unethical content. Alignment technologies have been introduced to steer LLMs towards human preference, gaining increasing attention. Despite notable breakthroughs in this direction, existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy labels and the marginal distinction between preferred and dispreferred response data. Given recent LLMs' proficiency in generating helpful responses, this work pivots towards a new research focus: achieving alignment using solely human-annotated negative samples, preserving helpfulness while reducing harmfulness. For this purpose, we propose Distributional Dispreference Optimization (D$^2$O), which maximizes the discrepancy between the generated responses and the dispreferred ones to effectively eschew harmful information. We theoretically demonstrat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992; Lyapunov Noise Pruning (LNP) &#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#20462;&#21098;&#31070;&#32463;&#20803;&#65292;&#21033;&#29992;&#31070;&#32463;&#20803;&#26102;&#38388;&#23610;&#24230;&#30340;&#24322;&#36136;&#24615;&#35774;&#35745;&#20986;&#31232;&#30095;RSNN&#65292;&#23454;&#29616;&#20102;&#35774;&#35745;&#31232;&#30095;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20219;&#21153;-&#26080;&#20851;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03409</link><description>&lt;p&gt;
&#31232;&#30095;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65306;&#21033;&#29992;&#26102;&#38388;&#23610;&#24230;&#30340;&#24322;&#36136;&#24615;&#26469;&#21098;&#26525;&#24490;&#29615;SNN
&lt;/p&gt;
&lt;p&gt;
Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03409
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992; Lyapunov Noise Pruning (LNP) &#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#20462;&#21098;&#31070;&#32463;&#20803;&#65292;&#21033;&#29992;&#31070;&#32463;&#20803;&#26102;&#38388;&#23610;&#24230;&#30340;&#24322;&#36136;&#24615;&#35774;&#35745;&#20986;&#31232;&#30095;RSNN&#65292;&#23454;&#29616;&#20102;&#35774;&#35745;&#31232;&#30095;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20219;&#21153;-&#26080;&#20851;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;RSNNs&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#21551;&#21457;&#20110;&#22823;&#33041;&#30340;&#23398;&#20064;&#27169;&#22411;&#12290;&#31232;&#30095;RSNNs&#30340;&#35774;&#35745;&#36890;&#36807;&#20943;&#23569;&#31070;&#32463;&#20803;&#21644;&#31361;&#35302;&#30340;&#25968;&#37327;&#26469;&#38477;&#20302;RSNNs&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#20256;&#32479;&#19978;&#65292;&#31232;&#30095;SNNs&#26159;&#36890;&#36807;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#23494;&#38598;&#32780;&#22797;&#26434;&#30340;SNN&#26469;&#23454;&#29616;&#30340;&#65292;&#28982;&#21518;&#22312;&#20445;&#25345;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#20462;&#21098;&#20302;&#27963;&#36291;&#24230;&#30340;&#31070;&#32463;&#20803;&#65288;&#22522;&#20110;&#27963;&#21160;&#30340;&#21098;&#26525;&#65289;&#26469;&#33719;&#24471;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35774;&#35745;&#31232;&#30095;RSNNs&#30340;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#19968;&#20010;&#22823;&#22411;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Lyapunov&#22122;&#22768;&#21098;&#26525;&#65288;LNP&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#22270;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;Lyapunov&#25351;&#25968;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;RSNN&#35774;&#35745;&#19968;&#20010;&#31283;&#23450;&#30340;&#31232;&#30095;RSNN&#12290;&#25105;&#20204;&#23637;&#31034;LNP&#21487;&#20197;&#21033;&#29992;&#31070;&#32463;&#20803;&#26102;&#38388;&#23610;&#24230;&#30340;&#22810;&#26679;&#24615;&#26469;&#35774;&#35745;&#31232;&#30095;&#24322;&#36136;RSNN&#65288;HRSNN&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#21516;&#30340;&#31232;&#30095;HRSNN&#27169;&#22411;&#21487;&#20197;&#34987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03409v1 Announce Type: cross  Abstract: Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally efficient and brain-inspired learning model. The design of sparse RSNNs with fewer neurons and synapses helps reduce the computational complexity of RSNNs. Traditionally, sparse SNNs are obtained by first training a dense and complex SNN for a target task, and, then, pruning neurons with low activity (activity-based pruning) while maintaining task performance. In contrast, this paper presents a task-agnostic methodology for designing sparse RSNNs by pruning a large randomly initialized model. We introduce a novel Lyapunov Noise Pruning (LNP) algorithm that uses graph sparsification methods and utilizes Lyapunov exponents to design a stable sparse RSNN from a randomly initialized RSNN. We show that the LNP can leverage diversity in neuronal timescales to design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same sparse HRSNN model can be trained 
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#19968;&#33268;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#25919;&#31574;&#21046;&#23450;&#32773;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#21548;&#20174;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#24212;&#35880;&#24910;&#23545;&#24453;&#12290;</title><link>https://arxiv.org/abs/2403.03407</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#25239;&#26426;&#22120;&#65306;&#35821;&#35328;&#27169;&#22411;&#19982;&#25112;&#20105;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Human vs. Machine: Language Models and Wargames
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03407
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#19968;&#33268;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#25919;&#31574;&#21046;&#23450;&#32773;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#21548;&#20174;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#24212;&#35880;&#24910;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#20105;&#28216;&#25103;&#22312;&#20891;&#20107;&#25112;&#30053;&#30340;&#21457;&#23637;&#21644;&#22269;&#23478;&#23545;&#23041;&#32961;&#25110;&#25915;&#20987;&#30340;&#21709;&#24212;&#20013;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20986;&#29616;&#25215;&#35834;&#20102;&#26356;&#22909;&#30340;&#20915;&#31574;&#21046;&#23450;&#21644;&#22686;&#24378;&#30340;&#20891;&#20107;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;AI&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#19982;&#20154;&#31867;&#30340;&#34892;&#20026;&#26377;&#20309;&#19981;&#21516;&#20173;&#23384;&#22312;&#20105;&#35758;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#25112;&#20105;&#28216;&#25103;&#23454;&#39564;&#65292;&#20849;&#26377;107&#20301;&#22269;&#23478;&#23433;&#20840;&#19987;&#23478;&#20154;&#31867;&#21442;&#19982;&#32773;&#21442;&#19982;&#65292;&#26088;&#22312;&#30740;&#31350;&#22312;&#19968;&#20010;&#34394;&#26500;&#30340;&#32654;&#20013;&#24773;&#26223;&#20013;&#30340;&#21361;&#26426;&#21319;&#32423;&#65292;&#24182;&#27604;&#36739;&#20154;&#31867;&#21442;&#19982;&#32773;&#19982;LLM&#27169;&#25311;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#21644;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#26174;&#33879;&#19968;&#33268;&#24615;&#65292;&#20294;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#27169;&#25311;&#21644;&#20154;&#31867;&#21442;&#19982;&#32773;&#20043;&#38388;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#24046;&#24322;&#65292;&#36825;&#20419;&#20351;&#20915;&#31574;&#32773;&#22312;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#36981;&#24490;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#35880;&#24910;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03407v1 Announce Type: cross  Abstract: Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans. To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses. We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;EnKF-LSTM&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;LSTM&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#20316;&#29289;&#29983;&#38271;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03406</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20316;&#29289;&#29983;&#38271;&#27169;&#22411;&#30340;EnKF-LSTM&#21516;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An EnKF-LSTM Assimilation Algorithm for Crop Growth Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;EnKF-LSTM&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;LSTM&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#20316;&#29289;&#29983;&#38271;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#21450;&#26102;&#22320;&#39044;&#27979;&#20316;&#29289;&#29983;&#38271;&#23545;&#20110;&#30830;&#20445;&#20316;&#29289;&#20135;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#20960;&#31181;&#29992;&#20110;&#39044;&#27979;&#20316;&#29289;&#29983;&#38271;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20316;&#29289;&#27169;&#22411;&#24471;&#21040;&#30340;&#27169;&#25311;&#32467;&#26524;&#19982;&#23454;&#38469;&#32467;&#26524;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#65292;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#23558;&#27169;&#25311;&#32467;&#26524;&#19982;&#37319;&#38598;&#30340;&#20316;&#29289;&#25968;&#25454;&#32467;&#21512;&#36827;&#34892;&#25968;&#25454;&#21516;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;EnKF-LSTM&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;LSTM&#31070;&#32463;&#32593;&#32476;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#29616;&#26377;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#28040;&#38500;&#20102;&#27979;&#37327;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#21033;&#29992;&#20256;&#24863;&#22120;&#35774;&#22791;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#23545;&#25152;&#25552;&#20986;&#30340;EnKF-LSTM&#26041;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#24182;&#19982;&#20854;&#20182;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03406v1 Announce Type: new  Abstract: Accurate and timely prediction of crop growth is of great significance to ensure crop yields and researchers have developed several crop models for the prediction of crop growth. However, there are large difference between the simulation results obtained by the crop models and the actual results, thus in this paper, we proposed to combine the simulation results with the collected crop data for data assimilation so that the accuracy of prediction will be improved. In this paper, an EnKF-LSTM data assimilation method for various crops is proposed by combining ensemble Kalman filter and LSTM neural network, which effectively avoids the overfitting problem of existing data assimilation methods and eliminates the uncertainty of the measured data. The verification of the proposed EnKF-LSTM method and the comparison of the proposed method with other data assimilation methods were performed using datasets collected by sensor equipment deployed o
&lt;/p&gt;</description></item><item><title>BAIT&#26694;&#26550;&#25552;&#20379;&#20102;&#20844;&#24179;&#19988;&#31616;&#21270;&#30340;ITP&#23398;&#20064;&#26041;&#27861;&#27604;&#36739;&#65292;&#21457;&#29616;Structure Aware Transformers&#22312;&#20844;&#24335;&#23884;&#20837;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.03401</link><description>&lt;p&gt;
BAIT: &#20132;&#20114;&#23450;&#29702;&#35777;&#26126;&#23884;&#20837;&#26550;&#26500;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03401
&lt;/p&gt;
&lt;p&gt;
BAIT&#26694;&#26550;&#25552;&#20379;&#20102;&#20844;&#24179;&#19988;&#31616;&#21270;&#30340;ITP&#23398;&#20064;&#26041;&#27861;&#27604;&#36739;&#65292;&#21457;&#29616;Structure Aware Transformers&#22312;&#20844;&#24335;&#23884;&#20837;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23450;&#29702;&#35777;&#26126;&#20652;&#29983;&#20102;&#22823;&#37327;&#22522;&#20934;&#27979;&#35797;&#21644;&#26041;&#27861;&#35770;&#65292;&#23588;&#20854;&#26159;&#22312;&#20132;&#20114;&#24335;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#12290;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20998;&#25955;&#65292;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#25955;&#24067;&#22312;&#20960;&#20010;&#20132;&#20114;&#24335;&#23450;&#29702;&#35777;&#26126;&#31995;&#32479;&#20013;&#12290;&#36825;&#32473;&#26041;&#27861;&#30340;&#27604;&#36739;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22797;&#26434;&#19988;&#38590;&#20197;&#22797;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BAIT&#65292;&#19968;&#20010;&#29992;&#20110;&#20844;&#24179;&#21644;&#31616;&#21270;&#27604;&#36739;ITP&#23398;&#20064;&#26041;&#27861;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#27604;&#36739;&#23637;&#31034;&#20102;BAIT&#30340;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;&#24212;&#29992;&#20110;&#20844;&#24335;&#23884;&#20837;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#26550;&#26500;&#22312;&#22810;&#20010;ITP&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#32467;&#26500;&#24863;&#30693;&#21464;&#21387;&#22120;&#34920;&#29616;&#29305;&#21035;&#20986;&#33394;&#65292;&#25913;&#36827;&#20102;&#19982;&#21407;&#38382;&#39064;&#38598;&#30456;&#20851;&#30340;&#25216;&#26415;&#12290;BAIT&#36824;&#20801;&#35768;&#25105;&#20204;&#35780;&#20272;&#24314;&#31435;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#30340;&#31995;&#32479;&#30340;&#31471;&#21040;&#31471;&#35777;&#26126;&#24615;&#33021;&#12290;&#36825;&#31181;&#32479;&#19968;&#30340;&#35270;&#35282;&#25581;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03401v1 Announce Type: new  Abstract: Artificial Intelligence for Theorem Proving has given rise to a plethora of benchmarks and methodologies, particularly in Interactive Theorem Proving (ITP). Research in the area is fragmented, with a diverse set of approaches being spread across several ITP systems. This presents a significant challenge to the comparison of methods, which are often complex and difficult to replicate. Addressing this, we present BAIT, a framework for fair and streamlined comparison of learning approaches in ITP. We demonstrate BAIT's capabilities with an in-depth comparison, across several ITP benchmarks, of state-of-the-art architectures applied to the problem of formula embedding. We find that Structure Aware Transformers perform particularly well, improving on techniques associated with the original problem sets. BAIT also allows us to assess the end-to-end proving performance of systems built on interactive environments. This unified perspective revea
&lt;/p&gt;</description></item><item><title>&#35813;&#31995;&#32479;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31867;&#20284;&#19982;&#22810;&#20301;&#20316;&#26354;&#23478;&#21512;&#20316;&#30340;&#20307;&#39564;&#65292;&#20174;&#32780;&#20419;&#36827;&#22810;&#26679;&#21270;&#30340;&#21019;&#36896;&#21147;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#21019;&#24847;&#24847;&#22270;&#26469;&#22686;&#24378;&#29983;&#25104;&#26059;&#24459;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03395</link><description>&lt;p&gt;
&#29992;&#20110;&#22686;&#24378;&#38899;&#20048;&#23478;&#21019;&#36896;&#21147;&#30340;&#20132;&#20114;&#24335;&#26059;&#24459;&#29983;&#25104;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Interactive Melody Generation System for Enhancing the Creativity of Musicians
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03395
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31995;&#32479;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31867;&#20284;&#19982;&#22810;&#20301;&#20316;&#26354;&#23478;&#21512;&#20316;&#30340;&#20307;&#39564;&#65292;&#20174;&#32780;&#20419;&#36827;&#22810;&#26679;&#21270;&#30340;&#21019;&#36896;&#21147;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#21019;&#24847;&#24847;&#22270;&#26469;&#22686;&#24378;&#29983;&#25104;&#26059;&#24459;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#38899;&#20048;&#20316;&#26354;&#25216;&#26415;&#65292;&#21015;&#20030;&#20154;&#31867;&#21327;&#20316;&#21019;&#20316;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#27169;&#22411;&#65292;&#35813;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31181;&#31867;&#20284;&#19982;&#22810;&#20301;&#20316;&#26354;&#23478;&#21512;&#20316;&#30340;&#20307;&#39564;&#65292;&#20174;&#32780;&#20419;&#36827;&#22810;&#26679;&#21270;&#30340;&#21019;&#36896;&#21147;&#12290;&#36890;&#36807;&#26681;&#25454;&#21453;&#39304;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#30340;&#21019;&#24847;&#24847;&#22270;&#65292;&#31995;&#32479;&#22686;&#24378;&#20102;&#29983;&#25104;&#19982;&#29992;&#25143;&#21916;&#22909;&#21644;&#21019;&#36896;&#38656;&#27714;&#19968;&#33268;&#30340;&#26059;&#24459;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20855;&#26377;&#19981;&#21516;&#32972;&#26223;&#30340;&#20316;&#26354;&#23478;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#20102;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#25581;&#31034;&#20102;&#35813;&#31995;&#32479;&#20419;&#36827;&#38899;&#20048;&#21019;&#36896;&#21147;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#36884;&#24452;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#20316;&#26354;&#23478;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20114;&#21160;&#30340;&#37325;&#35201;&#24615;&#65292;&#26088;&#22312;&#20351;&#38899;&#20048;&#21019;&#20316;&#26356;&#20855;&#21487;&#35775;&#38382;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;&#36825;&#20010;&#31995;&#32479;&#20195;&#34920;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#21040;&#21019;&#36896;&#24615;&#36807;&#31243;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03395v1 Announce Type: cross  Abstract: This study proposes a system designed to enumerate the process of collaborative composition among humans, using automatic music composition technology. By integrating multiple Recurrent Neural Network (RNN) models, the system provides an experience akin to collaborating with several composers, thereby fostering diverse creativity. Through dynamic adaptation to the user's creative intentions, based on feedback, the system enhances its capability to generate melodies that align with user preferences and creative needs. The system's effectiveness was evaluated through experiments with composers of varying backgrounds, revealing its potential to facilitate musical creativity and suggesting avenues for further refinement. The study underscores the importance of interaction between the composer and AI, aiming to make music composition more accessible and personalized. This system represents a step towards integrating AI into the creative pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;CCT&#12289;Patch Up&#21644;&#26032;&#39062;&#30340;CamCenterLoss&#25216;&#26415;&#23545;&#20020;&#24202;&#25968;&#25454;&#22788;&#29702;&#36827;&#34892;&#21019;&#26032;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#23545;&#21361;&#37325;&#30149;&#20154;&#30340;&#20851;&#27880;&#26356;&#22810;&#65292;&#20026;&#26410;&#26469;&#30340;&#22810;&#27169;&#24577;&#21307;&#23398;&#30740;&#31350;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.03385</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;CCT&#12289;Patch Up&#21644;&#26032;&#39062;&#30340;CamCenterLoss&#25216;&#26415;&#23545;&#20020;&#24202;&#25968;&#25454;&#22788;&#29702;&#36827;&#34892;&#21019;&#26032;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#23545;&#21361;&#37325;&#30149;&#20154;&#30340;&#20851;&#27880;&#26356;&#22810;&#65292;&#20026;&#26410;&#26469;&#30340;&#22810;&#27169;&#24577;&#21307;&#23398;&#30740;&#31350;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;&#21333;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20316;&#20026;&#26410;&#26469;&#22810;&#27169;&#24577;&#21307;&#23398;&#30740;&#31350;&#30340;&#20851;&#38190;&#21069;&#25552;&#12290;&#20511;&#37492;&#20102;&#37101;&#38742;&#20803;&#30340;&#24037;&#20316;&#65292;&#36890;&#36807;&#32039;&#20945;&#21367;&#31215;&#21464;&#25442;&#22120;&#65288;CCT&#65289;&#12289;Patch Up &#21644;&#21019;&#26032;&#30340; CamCenterLoss &#25216;&#26415;&#23545;&#20020;&#24202;&#25968;&#25454;&#22788;&#29702;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#20026;&#26410;&#26469;&#30340;&#22810;&#27169;&#24577;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23545;&#21361;&#37325;&#30149;&#20154;&#30340;&#20851;&#27880;&#26041;&#38754;&#30456;&#23545;&#20110;&#37101;&#38742;&#20803;&#30340; ResNet &#21644; StageNet &#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#25913;&#36827;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22270;&#20687;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#39592;&#24178;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20020;&#24202;&#25968;&#25454;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#35813;&#30740;&#31350;&#31361;&#20986;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#21033;&#29992; CCT&#12289;Patch Up &#21644;&#26032;&#39062;&#30340; CamCenterLoss &#22788;&#29702;&#21333;&#19968;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#20026;&#26410;&#26469;&#30340;&#22810;&#27169;&#24577;&#21307;&#23398;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#24182;&#25512;&#21160;&#20102;&#31934;&#20934;&#21644;&#20010;&#24615;&#21270;&#21307;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03385v1 Announce Type: cross  Abstract: This article investigates deep learning methodologies for single-modality clinical data analysis, as a crucial precursor to multi-modal medical research. Building on Guo JingYuan's work, the study refines clinical data processing through Compact Convolutional Transformer (CCT), Patch Up, and the innovative CamCenterLoss technique, establishing a foundation for future multimodal investigations. The proposed methodology demonstrates improved prediction accuracy and at tentiveness to critically ill patients compared to Guo JingYuan's ResNet and StageNet approaches. Novelty that using image-pretrained vision transformer backbone to perform transfer learning time-series clinical data.The study highlights the potential of CCT, Patch Up, and novel CamCenterLoss in processing single modality clinical data within deep learning frameworks, paving the way for future multimodal medical research and promoting precision and personalized healthcare
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#21457;&#29616;&#21644;&#21512;&#24182;&#65288;ADM&#65289;&#33539;&#20363;&#65292;&#20197;&#22312;&#22686;&#37327;&#38454;&#27573;&#33258;&#36866;&#24212;&#22320;&#21457;&#29616;&#26032;&#31867;&#21035;&#24182;&#23558;&#26032;&#30693;&#35782;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#21516;&#26102;&#20943;&#23569;&#23545;&#21407;&#26377;&#30693;&#35782;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03382</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#21457;&#29616;&#21644;&#21512;&#24182;&#29992;&#20110;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Adaptive Discovering and Merging for Incremental Novel Class Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03382
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#21457;&#29616;&#21644;&#21512;&#24182;&#65288;ADM&#65289;&#33539;&#20363;&#65292;&#20197;&#22312;&#22686;&#37327;&#38454;&#27573;&#33258;&#36866;&#24212;&#22320;&#21457;&#29616;&#26032;&#31867;&#21035;&#24182;&#23558;&#26032;&#30693;&#35782;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#21516;&#26102;&#20943;&#23569;&#23545;&#21407;&#26377;&#30693;&#35782;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#32456;&#36523;&#23398;&#20064;&#30340;&#37325;&#35201;&#30446;&#26631;&#26159;&#20197;&#25345;&#32493;&#30340;&#26041;&#24335;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#21457;&#29616;&#26032;&#31867;&#21035;&#12290; &#20013;&#24515;&#25361;&#25112;&#26159;&#21452;&#37325;&#30340;&#65306;&#22312;&#36739;&#23569;&#30693;&#35782;&#28798;&#21380;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#21644;&#23398;&#20064;&#26032;&#31867;&#21035;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#33539;&#20363;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#21457;&#29616;&#21644;&#21512;&#24182;&#65288;ADM&#65289;&#65292;&#20197;&#22312;&#22686;&#37327;&#38454;&#27573;&#33258;&#36866;&#24212;&#22320;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#24182;&#23558;&#26032;&#30693;&#35782;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#32780;&#19981;&#24433;&#21709;&#21407;&#26377;&#30693;&#35782;&#12290; &#20026;&#20102;&#33258;&#36866;&#24212;&#22320;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#25105;&#20204;&#23558;&#34920;&#31034;&#23398;&#20064;&#21644;&#26032;&#31867;&#21035;&#21457;&#29616;&#35299;&#32806;&#65292;&#24182;&#20351;&#29992;&#19977;&#37325;&#27604;&#36739;&#65288;TC&#65289;&#21644;&#27010;&#29575;&#27491;&#21017;&#21270;&#65288;PR&#65289;&#26469;&#32422;&#26463;&#27010;&#29575;&#24046;&#24322;&#21644;&#22810;&#26679;&#24615;&#20197;&#23454;&#29616;&#33258;&#36866;&#24212;&#31867;&#21035;&#20998;&#37197;&#12290; &#20026;&#20102;&#33258;&#36866;&#24212;&#22320;&#21512;&#24182;&#23398;&#20064;&#21040;&#30340;&#26032;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22522;&#30784;&#21644;&#26032;&#20998;&#25903;&#30340;&#28151;&#21512;&#32467;&#26500;&#65292;&#21517;&#20026;&#33258;&#36866;&#24212;&#27169;&#22411;&#21512;&#24182;&#65288;AMM&#65289;&#65292;&#20854;&#20943;&#23569;&#20102;&#23545;&#21407;&#26377;&#30693;&#35782;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03382v1 Announce Type: new  Abstract: One important desideratum of lifelong learning aims to discover novel classes from unlabelled data in a continuous manner. The central challenge is twofold: discovering and learning novel classes while mitigating the issue of catastrophic forgetting of established knowledge. To this end, we introduce a new paradigm called Adaptive Discovering and Merging (ADM) to discover novel categories adaptively in the incremental stage and integrate novel knowledge into the model without affecting the original knowledge. To discover novel classes adaptively, we decouple representation learning and novel class discovery, and use Triple Comparison (TC) and Probability Regularization (PR) to constrain the probability discrepancy and diversity for adaptive category assignment. To merge the learned novel knowledge adaptively, we propose a hybrid structure with base and novel branches named Adaptive Model Merging (AMM), which reduces the interference of t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#20027;&#25511;&#21046;&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#24182;&#34892;&#24335;&#21277;&#36947;&#21512;&#27969;&#65292;&#32771;&#34385;&#20102;&#36947;&#36335;&#19978;&#20854;&#20182;&#36710;&#36742;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#28608;&#21169;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.03359</link><description>&lt;p&gt;
RACE-SM:&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31038;&#20132;&#24335;&#21277;&#36947;&#21512;&#27969;&#33258;&#20027;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#20027;&#25511;&#21046;&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#24182;&#34892;&#24335;&#21277;&#36947;&#21512;&#27969;&#65292;&#32771;&#34385;&#20102;&#36947;&#36335;&#19978;&#20854;&#20182;&#36710;&#36742;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#28608;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#24182;&#34892;&#24335;&#21277;&#36947;&#21512;&#27969;&#22312;&#20154;&#25511;&#36710;&#36742;&#20132;&#36890;&#20013;&#20173;&#28982;&#26159;&#33258;&#20027;&#36710;&#36742;&#25511;&#21046;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#38750;&#23398;&#20064;&#22411;&#36710;&#36742;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#20381;&#36182;&#35268;&#21017;&#21644;&#20248;&#21270;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#23637;&#29616;&#20102;&#24076;&#26395;&#65292;&#24182;&#21463;&#21040;&#20102;&#37325;&#35201;&#23398;&#26415;&#20851;&#27880;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#23545;&#20854;&#20182;&#39640;&#36895;&#20844;&#36335;&#36710;&#36742;&#20851;&#27880;&#19981;&#36275;&#65292;&#19988;&#32463;&#24120;&#20381;&#36182;&#19981;&#20934;&#30830;&#30340;&#36947;&#36335;&#20132;&#36890;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#24182;&#34892;&#24335;&#24773;&#20917;&#24456;&#23569;&#34987;&#32771;&#34385;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21152;&#36895;&#21644;&#21464;&#36947;&#20915;&#31574;&#21046;&#23450;&#65292;&#35813;&#27169;&#22411;&#26126;&#30830;&#32771;&#34385;&#20102;&#23545;&#20110;&#36710;&#36742;&#26412;&#36523;&#21450;&#20854;&#21608;&#22260;&#36710;&#36742;&#65288;&#21487;&#33021;&#21512;&#20316;&#25110;&#19981;&#21512;&#20316;&#65289;&#30340;&#25928;&#29992;&#65292;&#20197;&#20135;&#29983;&#31526;&#21512;&#31038;&#20250;&#35268;&#33539;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#20989;&#25968;&#21033;&#29992;&#31038;&#20132;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03359v1 Announce Type: new  Abstract: Autonomous parallel-style on-ramp merging in human controlled traffic continues to be an existing issue for autonomous vehicle control. Existing non-learning based solutions for vehicle control rely on rules and optimization primarily. These methods have been seen to present significant challenges. Recent advancements in Deep Reinforcement Learning have shown promise and have received significant academic interest however the available learning based approaches show inadequate attention to other highway vehicles and often rely on inaccurate road traffic assumptions. In addition, the parallel-style case is rarely considered. A novel learning based model for acceleration and lane change decision making that explicitly considers the utility to both the ego vehicle and its surrounding vehicles which may be cooperative or uncooperative to produce behaviour that is socially acceptable is proposed. The novel reward function makes use of Social 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20840;&#29699;&#21355;&#29983;&#20844;&#24179;&#24615;&#65292;&#20197;&#38750;&#27954;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#33539;&#22260;&#23457;&#26597;&#21644;&#23450;&#24615;&#30740;&#31350;&#25581;&#31034;&#20102;ML&#25216;&#26415;&#22312;&#38750;&#27954;&#20581;&#24247;&#39046;&#22495;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#27542;&#27665;&#20027;&#20041;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03357</link><description>&lt;p&gt;
&#20840;&#29699;&#25512;&#24191;&#20844;&#24179;&#24615;&#30340;&#29702;&#30001;&#65306;&#20851;&#20110;&#27542;&#27665;&#20027;&#20041;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#38750;&#27954;&#20581;&#24247;&#30340;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism, AI, and Health in Africa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03357
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20840;&#29699;&#21355;&#29983;&#20844;&#24179;&#24615;&#65292;&#20197;&#38750;&#27954;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#33539;&#22260;&#23457;&#26597;&#21644;&#23450;&#24615;&#30740;&#31350;&#25581;&#31034;&#20102;ML&#25216;&#26415;&#22312;&#38750;&#27954;&#20581;&#24247;&#39046;&#22495;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#27542;&#27665;&#20027;&#20041;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#38271;&#65292;&#20154;&#20204;&#21628;&#21505;&#24320;&#21457;&#25216;&#26415;&#26469;&#29702;&#35299;&#21644;&#20943;&#36731;&#36825;&#20123;&#31995;&#32479;&#21487;&#33021;&#34920;&#29616;&#20986;&#30340;&#20559;&#35265;&#12290; &#20844;&#24179;&#24615;&#22312;&#20026;&#20581;&#24247;&#24320;&#21457;&#22522;&#20110;ML&#30340;&#35299;&#20915;&#26041;&#26696;&#26102;&#20855;&#26377;&#29305;&#23450;&#23545;&#38750;&#27954;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#38750;&#27954;&#24050;&#32463;&#38754;&#20020;&#20840;&#29699;&#21335;&#21271;&#20043;&#38388;&#19981;&#20844;&#24179;&#30340;&#26435;&#21147;&#22833;&#34913;&#12290; &#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#20840;&#29699;&#20581;&#24247;&#30340;&#20844;&#24179;&#24615;&#65292;&#20197;&#38750;&#27954;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#34892;&#33539;&#22260;&#23457;&#26597;&#65292;&#25552;&#20986;&#22312;&#38750;&#27954;&#29615;&#22659;&#20013;&#32771;&#34385;&#20844;&#24179;&#24615;&#30340;&#24046;&#36317;&#36724;&#65292;&#24182;&#21246;&#30011;&#23427;&#20204;&#21487;&#33021;&#22312;&#19981;&#21516;ML&#21551;&#29992;&#30340;&#21307;&#30103;&#27169;&#24335;&#20013;&#20135;&#29983;&#24433;&#21709;&#30340;&#21306;&#22495;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;672&#21517;&#19968;&#33324;&#20154;&#21475;&#30740;&#31350;&#21442;&#19982;&#32773;&#21644;28&#21517;&#19987;&#23478;&#36827;&#34892;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#20182;&#20204;&#20851;&#27880;&#30340;&#26159;&#19982;&#38750;&#27954;&#26377;&#20851;&#30340;ML&#12289;&#20581;&#24247;&#21644;&#25919;&#31574;&#65292;&#20197;&#33719;&#24471;&#20851;&#20110;&#24050;&#25552;&#20986;&#30340;&#24046;&#36317;&#36724;&#30340;&#35777;&#23454;&#24615;&#35777;&#25454;&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#32858;&#28966;&#20110;&#27542;&#27665;&#20027;&#20041;&#20316;&#20026;&#32508;&#21512;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03357v1 Announce Type: new  Abstract: With growing application of machine learning (ML) technologies in healthcare, there have been calls for developing techniques to understand and mitigate biases these systems may exhibit. Fair-ness considerations in the development of ML-based solutions for health have particular implications for Africa, which already faces inequitable power imbalances between the Global North and South.This paper seeks to explore fairness for global health, with Africa as a case study. We conduct a scoping review to propose axes of disparities for fairness consideration in the African context and delineate where they may come into play in different ML-enabled medical modalities. We then conduct qualitative research studies with 672 general population study participants and 28 experts inML, health, and policy focused on Africa to obtain corroborative evidence on the proposed axes of disparities. Our analysis focuses on colonialism as the attribute of inte
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24605;&#32500;&#38142;&#33976;&#39311;&#20013;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#19982;&#30693;&#35782;&#38598;&#25104;&#19981;&#36275;&#38382;&#39064;&#30340;&#21464;&#20998;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03348</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#36827;&#34892;&#24605;&#32500;&#38142;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Learning to Maximize Mutual Information for Chain-of-Thought Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03348
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24605;&#32500;&#38142;&#33976;&#39311;&#20013;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#19982;&#30693;&#35782;&#38598;&#25104;&#19981;&#36275;&#38382;&#39064;&#30340;&#21464;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#23558;&#22823;&#22411;&#22797;&#26434;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#36739;&#23567;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#26159;&#23454;&#29616;&#39640;&#25928;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#36890;&#36807;&#21033;&#29992;&#24605;&#32500;&#38142; (CoT) &#33976;&#39311;&#30340;&#26032;&#26041;&#27861;&#8212;&#8212;&#36880;&#27493;&#33976;&#39311; (DSS)&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#20026;&#36739;&#23567;&#27169;&#22411;&#36171;&#20104;&#20854;&#36739;&#22823;&#21516;&#34892;&#30340;&#20248;&#36234;&#25512;&#29702;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#22312;DSS&#20013;&#65292;&#33976;&#39311;&#27169;&#22411;&#36890;&#36807;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#21516;&#26102;&#33719;&#24471;&#29983;&#25104;&#29702;&#30001;&#21644;&#39044;&#27979;&#26631;&#31614;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;DSS&#24573;&#30053;&#20102;&#36825;&#20004;&#20010;&#35757;&#32451;&#20219;&#21153;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#65292;&#23548;&#33268;CoT&#30693;&#35782;&#19982;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#30340;&#26377;&#25928;&#25972;&#21512;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#29942;&#39048;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#34920;&#36848;&#20026;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03348v1 Announce Type: cross  Abstract: Knowledge distillation, the technique of transferring knowledge from large, complex models to smaller ones, marks a pivotal step towards efficient AI deployment. Distilling Step-by-Step (DSS), a novel method utilizing chain-of-thought (CoT) distillation, has demonstrated promise by imbuing smaller models with the superior reasoning capabilities of their larger counterparts. In DSS, the distilled model acquires the ability to generate rationales and predict labels concurrently through a multi-task learning framework. However, DSS overlooks the intrinsic relationship between the two training tasks, leading to ineffective integration of CoT knowledge with the task of label prediction. To this end, we investigate the mutual relationship of the two tasks from Information Bottleneck perspective and formulate it as maximizing the mutual information of the representation features of the two tasks. We propose a variational approach to solve thi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#32511;&#33394;&#32534;&#30721;&#23454;&#36341;&#21644;AI&#27169;&#22411;&#21487;&#25345;&#32493;&#24615;&#24847;&#35782;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#21830;&#19994;AI&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03344</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#25345;&#32493;&#32534;&#31243;&#65306;&#22522;&#20110;LLM&#30340;&#32511;&#33394;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#32511;&#33394;&#32534;&#30721;&#23454;&#36341;&#21644;AI&#27169;&#22411;&#21487;&#25345;&#32493;&#24615;&#24847;&#35782;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#21830;&#19994;AI&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25216;&#26415;&#30340;&#26085;&#30410;&#24191;&#27867;&#24212;&#29992;&#23548;&#33268;&#25968;&#25454;&#20013;&#24515;&#30340;&#33021;&#32791;&#21644;&#30899;&#25490;&#25918;&#21344;&#27604;&#26174;&#33879;&#22686;&#21152;&#12290;&#38543;&#30528;&#23545;&#22823;&#25968;&#25454;&#20998;&#26512;&#12289;&#25968;&#23383;&#21270;&#20197;&#21450;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#38656;&#27714;&#22686;&#38271;&#65292;&#36825;&#20123;&#36129;&#29486;&#26377;&#26395;&#32487;&#32493;&#22686;&#21152;&#12290;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#38656;&#35201;&#35299;&#20915;&#29615;&#22659;&#24433;&#21709;&#30340;&#38656;&#27714;&#23548;&#33268;&#20102;&#23545;&#32511;&#33394;&#65288;&#21487;&#25345;&#32493;&#65289;&#32534;&#30721;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;&#20197;&#21450;&#23545;AI&#27169;&#22411;&#21487;&#20197;&#24102;&#26469;&#33021;&#25928;&#25552;&#21319;&#30340;&#22768;&#26126;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#32511;&#33394;&#20195;&#30721;&#30340;&#23454;&#35777;&#30740;&#31350;&#21644;&#32511;&#33394;&#32534;&#30721;&#23454;&#36341;&#27010;&#36848;&#65292;&#20197;&#21450;&#29992;&#20110;&#37327;&#21270;AI&#27169;&#22411;&#21487;&#25345;&#32493;&#24847;&#35782;&#30340;&#25351;&#26631;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#30340;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#30001;&#29983;&#25104;&#24335;&#21830;&#19994;AI&#35821;&#35328;&#27169;&#22411;&#12289;GitHub Copilot&#12289;OpenAI ChatGPT-3&#21644;Amazon CodeWhisperer&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03344v1 Announce Type: cross  Abstract: The increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers. These contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (AI) models. The need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of AI models can lead to energy efficiency gains. Here, we provide an empirical study on green code and an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of AI models. In this framework, we evaluate the sustainability of auto-generated code. The auto-generate codes considered in this study are produced by generative commercial AI language models, GitHub Copilot, OpenAI ChatGPT-3, and Amazon CodeWhisperer. Within our meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DIVERSE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;173,000&#26465;YouTube&#35270;&#39057;&#35780;&#35770;&#65292;&#26631;&#27880;&#20102;&#36825;&#20123;&#35780;&#35770;&#23545;&#32654;&#22269;&#20891;&#20107;&#35270;&#39057;&#30340;&#31435;&#22330;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#24341;&#23548;&#12289;&#26426;&#22120;&#36741;&#21161;&#30340;&#26631;&#27880;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#21477;&#23376;&#20013;&#30340;&#24369;&#20449;&#21495;&#20316;&#20026;&#25903;&#25345;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.03334</link><description>&lt;p&gt;
DIVERSE&#65306;&#36890;&#36807;&#35270;&#39057;&#35780;&#35770;&#24577;&#24230;&#20998;&#26512;&#35299;&#35835;&#20114;&#32852;&#32593;&#23545;&#32654;&#22269;&#20891;&#20107;&#30340;&#30475;&#27861;&#65292;&#19968;&#20010;&#29992;&#20110;&#31435;&#22330;&#20998;&#31867;&#30340;&#26032;&#39062;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DIVERSE: Deciphering Internet Views on the U.S. Military Through Video Comment Stance Analysis, A Novel Benchmark Dataset for Stance Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DIVERSE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;173,000&#26465;YouTube&#35270;&#39057;&#35780;&#35770;&#65292;&#26631;&#27880;&#20102;&#36825;&#20123;&#35780;&#35770;&#23545;&#32654;&#22269;&#20891;&#20107;&#35270;&#39057;&#30340;&#31435;&#22330;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#24341;&#23548;&#12289;&#26426;&#22120;&#36741;&#21161;&#30340;&#26631;&#27880;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#21477;&#23376;&#20013;&#30340;&#24369;&#20449;&#21495;&#20316;&#20026;&#25903;&#25345;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#31435;&#22330;&#26816;&#27979;&#26159;&#28041;&#21450;&#35782;&#21035;&#22312;&#26377;&#20105;&#35758;&#20027;&#39064;&#19978;&#25317;&#26377;&#30456;&#21453;&#35266;&#28857;&#30340;&#29992;&#25143;&#32676;&#32452;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22914;&#30123;&#33495;&#25509;&#31181;&#21644;&#20105;&#35770;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#31435;&#22330;&#25552;&#20379;&#20102;&#23545;&#23454;&#20307;&#31435;&#22330;&#30340;&#25351;&#31034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DIVERSE&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#23545;&#36229;&#36807;173,000&#20010;YouTube&#35270;&#39057;&#35780;&#35770;&#36827;&#34892;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#26631;&#27880;&#20102;&#36825;&#20123;&#35780;&#35770;&#23545;&#20110;&#32654;&#22269;&#20891;&#20107;&#35270;&#39057;&#30340;&#31435;&#22330;&#12290;&#36825;&#20123;&#31435;&#22330;&#36890;&#36807;&#19968;&#31181;&#30001;&#20154;&#31867;&#24341;&#23548;&#12289;&#26426;&#22120;&#36741;&#21161;&#30340;&#26631;&#27880;&#26041;&#27861;&#36827;&#34892;&#26631;&#27880;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#21477;&#23376;&#20013;&#34164;&#21547;&#30340;&#35821;&#27668;&#24369;&#20449;&#21495;&#20316;&#20026;&#25903;&#25345;&#25351;&#26631;&#65292;&#32780;&#38750;&#20351;&#29992;&#20154;&#31867;&#25163;&#21160;&#27880;&#37322;&#12290;&#36825;&#20123;&#24369;&#20449;&#21495;&#21253;&#25324;&#20167;&#24680;&#35328;&#35770;&#21644;&#35773;&#21050;&#30340;&#23384;&#22312;&#65292;&#29305;&#23450;&#20851;&#38190;&#35789;&#30340;&#23384;&#22312;&#65292;&#25991;&#26412;&#30340;&#24773;&#24863;&#20197;&#21450;&#20174;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#26029;&#30340;&#31435;&#22330;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#35780;&#35770;&#34987;&#27880;&#37322;&#20043;&#21069;&#65292;&#36825;&#20123;&#24369;&#20449;&#21495;&#20351;&#29992;&#25968;&#25454;&#32534;&#31243;&#27169;&#22411;&#36827;&#34892; consol
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03334v1 Announce Type: cross  Abstract: Stance detection of social media text is a key component of downstream tasks involving the identification of groups of users with opposing opinions on contested topics such as vaccination and within arguments. In particular, stance provides an indication of an opinion towards an entity. This paper introduces DIVERSE, a dataset of over 173,000 YouTube video comments annotated for their stance towards videos of the U.S. military. The stance is annotated through a human-guided, machine-assisted labeling methodology that makes use of weak signals of tone within the sentence as supporting indicators, as opposed to using manual annotations by humans. These weak signals consist of the presence of hate speech and sarcasm, the presence of specific keywords, the sentiment of the text, and the stance inference from two Large Language Models. The weak signals are then consolidated using a data programming model before each comment is annotated wit
&lt;/p&gt;</description></item><item><title>&#24615;&#33021;&#26159;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.03322</link><description>&lt;p&gt;
&#28145;&#24230;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#35843;&#26597;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Configuration Performance Learning: A Systematic Survey and Taxonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03322
&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#26159;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#21487;&#20197;&#35828;&#26159;&#21453;&#26144;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#26368;&#20851;&#38190;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29616;&#20195;&#36719;&#20214;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#23545;&#21508;&#31181;&#37197;&#32622;&#22914;&#20309;&#24433;&#21709;&#24615;&#33021;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#25104;&#20026;&#36719;&#20214;&#32500;&#25252;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#24615;&#33021;&#36890;&#24120;&#26159;&#22312;&#27809;&#26377;&#23545;&#36719;&#20214;&#31995;&#32479;&#26377;&#36879;&#24443;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#24314;&#27169;&#30340;&#65292;&#20027;&#35201;&#20381;&#36182;&#25968;&#25454;&#65292;&#36825;&#27491;&#22909;&#31526;&#21512;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#28085;&#30422;&#20102;948&#31687;&#26469;&#33258;&#20845;&#20010;&#32034;&#24341;&#26381;&#21153;&#30340;&#35770;&#25991;&#65292;&#22522;&#20110;&#27492;&#25552;&#21462;&#24182;&#20998;&#26512;&#20102;85&#31687;&#20027;&#35201;&#35770;&#25991;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24635;&#32467;&#20102;&#37197;&#32622;&#25968;&#25454;&#22914;&#20309;&#20934;&#22791;&#65292;&#28145;&#24230;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#26500;&#24314;&#65292;&#20197;&#21450;&#35813;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#35780;&#20272;&#31561;&#20851;&#38190;&#20027;&#39064;&#21644;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03322v1 Announce Type: cross  Abstract: Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning.   In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evalu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#36890;&#36807;&#31070;&#32463;&#32452;&#20214;&#25552;&#21319;&#35268;&#21017;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#20004;&#31181;&#33539;&#24335;&#30340;&#20248;&#21183;&#65292;&#19988;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.03305</link><description>&lt;p&gt;
&#20004;&#20840;&#20854;&#32654;&#65306;&#19968;&#31181;&#28789;&#27963;&#19988;&#36890;&#29992;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#29992;&#20110;&#20851;&#31995;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach for Relation Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#36890;&#36807;&#31070;&#32463;&#32452;&#20214;&#25552;&#21319;&#35268;&#21017;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#20004;&#31181;&#33539;&#24335;&#30340;&#20248;&#21183;&#65292;&#19988;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#29992;&#20110;&#20851;&#31995;&#20998;&#31867;&#65288;RC&#65289;&#65292;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#19982;&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21457;&#25381;&#20102;&#20004;&#31181;&#33539;&#24335;&#30340;&#20248;&#21183;&#65306;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#30340;&#36866;&#24212;&#24615;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#19968;&#20010;&#29992;&#20110;&#36879;&#26126;&#20998;&#31867;&#30340;&#22768;&#26126;&#24615;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#36890;&#36807;&#35821;&#20041;&#25991;&#26412;&#21305;&#37197;&#22686;&#24378;&#35268;&#21017;&#27867;&#21270;&#33021;&#21147;&#30340;&#31070;&#32463;&#32452;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#35821;&#20041;&#21305;&#37197;&#22120;&#20165;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#22312;&#26080;&#30417;&#30563;&#39046;&#22495;&#26080;&#20851;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32452;&#20214;&#26494;&#25955;&#32806;&#21512;&#65292;&#20801;&#35768;&#23545;&#35268;&#21017;&#36827;&#34892;&#20462;&#25913;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#35821;&#20041;&#21305;&#37197;&#22120;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#23569;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;&#25968;&#25454;&#38598;&#65306;Few-Shot TACRED &#21644; Few-Shot &#29256;&#26412;&#30340; NYT29&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03305v1 Announce Type: cross  Abstract: This paper introduces a novel neuro-symbolic architecture for relation classification (RC) that combines rule-based methods with contemporary deep learning techniques. This approach capitalizes on the strengths of both paradigms: the adaptability of rule-based systems and the generalization power of neural networks. Our architecture consists of two components: a declarative rule-based model for transparent classification and a neural component to enhance rule generalizability through semantic text matching. Notably, our semantic matcher is trained in an unsupervised domain-agnostic way, solely with synthetic data. Further, these components are loosely coupled, allowing for rule modifications without retraining the semantic matcher. In our evaluation, we focused on two few-shot relation classification datasets: Few-Shot TACRED and a Few-Shot version of NYT29. We show that our proposed method outperforms previous state-of-the-art models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;ChatGPT&#26234;&#33021;&#23545;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#26377;&#25928;&#25776;&#20889;&#31185;&#23398;&#25991;&#29486;&#35843;&#26597;&#65292;&#22312;&#20083;&#33146;&#30284;&#27835;&#30103;&#36825;&#19968;&#30740;&#31350;&#35838;&#39064;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.03293</link><description>&lt;p&gt;
AI Insights: &#21033;&#29992;ChatGPT&#26234;&#33021;&#36827;&#34892;&#30740;&#31350;&#35770;&#25991;&#20998;&#26512;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;ChatGPT&#26234;&#33021;&#23545;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#26377;&#25928;&#25776;&#20889;&#31185;&#23398;&#25991;&#29486;&#35843;&#26597;&#65292;&#22312;&#20083;&#33146;&#30284;&#27835;&#30103;&#36825;&#19968;&#30740;&#31350;&#35838;&#39064;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#21033;&#29992;Chatbot: Generative Pre-trained Transformer (ChatGPT) 3.5&#21644;4&#29256;&#26412;&#20998;&#26512;&#30740;&#31350;&#35770;&#25991;&#20197;&#26377;&#25928;&#25776;&#20889;&#31185;&#23398;&#25991;&#29486;&#35843;&#26597;&#30340;&#26377;&#25928;&#24615;&#12290; &#30740;&#31350;&#36873;&#25321;&#12298;&#20154;&#24037;&#26234;&#33021;&#22312;&#20083;&#33146;&#30284;&#27835;&#30103;&#20013;&#30340;&#24212;&#29992;&#12299;&#20316;&#20026;&#30740;&#31350;&#35838;&#39064;&#12290; &#20174;&#19977;&#20010;&#20027;&#35201;&#20986;&#29256;&#25968;&#25454;&#24211;Google Scholar&#12289;Pubmed&#21644;Scopus&#25910;&#38598;&#20102;&#19982;&#35813;&#20027;&#39064;&#30456;&#20851;&#30340;&#30740;&#31350;&#35770;&#25991;&#12290; &#20351;&#29992;ChatGPT&#27169;&#22411;&#35782;&#21035;&#30740;&#31350;&#35770;&#25991;&#30340;&#31867;&#21035;&#12289;&#33539;&#22260;&#21644;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#33258;&#21160;&#35782;&#21035;&#19982;&#20083;&#33146;&#30284;&#27835;&#30103;&#65288;BCT&#65289;&#30456;&#20851;&#30340;&#30456;&#20851;&#35770;&#25991;&#65292;&#26681;&#25454;&#33539;&#22260;&#32452;&#32455;&#35770;&#25991;&#65292;&#35782;&#21035;&#25776;&#20889;&#35843;&#26597;&#35770;&#25991;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290; &#20351;&#29992;&#19987;&#23478;&#26631;&#27880;&#30340;&#22522;&#20934;&#25968;&#25454;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;GPT-4&#22312;&#35782;&#21035;&#30740;&#31350;&#35770;&#25991;&#31867;&#21035;&#26041;&#38754;&#36798;&#21040;77.3%&#30340;&#20934;&#30830;&#29575;&#65292;&#23545;&#35843;&#26597;&#35770;&#25991;&#25776;&#20889;&#30340;&#20851;&#38190;&#20449;&#24687;&#30340;&#35782;&#21035;&#29575;&#20026;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03293v1 Announce Type: new  Abstract: This paper discusses the effectiveness of leveraging Chatbot: Generative Pre-trained Transformer (ChatGPT) versions 3.5 and 4 for analyzing research papers for effective writing of scientific literature surveys. The study selected the \textit{Application of Artificial Intelligence in Breast Cancer Treatment} as the research topic. Research papers related to this topic were collected from three major publication databases Google Scholar, Pubmed, and Scopus. ChatGPT models were used to identify the category, scope, and relevant information from the research papers for automatic identification of relevant papers related to Breast Cancer Treatment (BCT), organization of papers according to scope, and identification of key information for survey paper writing. Evaluations performed using ground truth data annotated using subject experts reveal, that GPT-4 achieves 77.3\% accuracy in identifying the research paper categories and 50\% of the pa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;LLMs&#20013;&#30340;&#32479;&#35745;&#27169;&#24335;&#19982;&#28023;&#24503;&#26684;&#23572;&#21746;&#23398;&#27010;&#24565;&#65292;&#25506;&#35752;&#20102;LLMs&#20316;&#20026;&#30693;&#35782;&#33021;&#21147;&#25968;&#23383;&#21270;&#23545;&#24212;&#21644;&#20154;&#31867;&#25512;&#29702;&#27169;&#25311;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#28023;&#24503;&#26684;&#23572;&#30340;&#30495;&#29702;&#27010;&#24565;&#23545;&#20154;&#31867;&#25512;&#29702;&#36827;&#34892;&#32467;&#26500;&#24615;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.03288</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#25285;&#24515;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21527;&#65311;&#36890;&#36807;&#28023;&#24503;&#26684;&#23572;&#21746;&#23398;&#35270;&#35282;&#38416;&#26126;LLM&#30340;&#33021;&#21147;&#21644;&#39118;&#38505;&#30340;&#20154;&#31867;&#25512;&#29702;&#31995;&#32479;&#30340;&#32467;&#26500;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger's Philosophy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;LLMs&#20013;&#30340;&#32479;&#35745;&#27169;&#24335;&#19982;&#28023;&#24503;&#26684;&#23572;&#21746;&#23398;&#27010;&#24565;&#65292;&#25506;&#35752;&#20102;LLMs&#20316;&#20026;&#30693;&#35782;&#33021;&#21147;&#25968;&#23383;&#21270;&#23545;&#24212;&#21644;&#20154;&#31867;&#25512;&#29702;&#27169;&#25311;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#28023;&#24503;&#26684;&#23572;&#30340;&#30495;&#29702;&#27010;&#24565;&#23545;&#20154;&#31867;&#25512;&#29702;&#36827;&#34892;&#32467;&#26500;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#65292;&#26377;&#24517;&#35201;&#24443;&#24213;&#20998;&#26512;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#39118;&#38505;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#26680;&#24515;&#26159;&#20004;&#20010;&#21019;&#26032;&#20803;&#32032;&#12290;&#39318;&#20808;&#65292;&#26159;LLMs&#20013;&#21333;&#35789;&#20851;&#31995;&#30340;&#32479;&#35745;&#27169;&#24335;&#19982;&#39532;&#19969;&#183;&#28023;&#24503;&#26684;&#23572;&#30340;"&#23601;&#32490;"&#21644;"&#29616;&#23384;"&#27010;&#24565;&#20043;&#38388;&#30340;&#21019;&#26032;&#31867;&#27604;&#65292;&#23427;&#27010;&#25324;&#20102;&#20154;&#31867;&#19982;&#19990;&#30028;&#20114;&#21160;&#20013;&#20351;&#29992;&#30340;&#23454;&#29992;&#21644;&#31185;&#23398;&#24577;&#24230;&#12290;&#36825;&#31181;&#27604;&#36739;&#22880;&#23450;&#20102;&#23558;LLMs&#23450;&#20301;&#20026;&#25968;&#23383;&#21270;&#23545;&#24212;&#20154;&#31867;&#30693;&#35782;&#33021;&#21147;&#30340;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#27169;&#25311;&#20154;&#31867;&#25512;&#29702;&#26576;&#20123;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#28023;&#24503;&#26684;&#23572;&#23558;&#30495;&#29702;&#29702;&#35299;&#20026;"&#25581;&#31034;"&#30340;&#27010;&#24565;&#23545;&#20154;&#31867;&#25512;&#29702;&#36827;&#34892;&#32467;&#26500;&#20998;&#26512;&#12290;&#36825;&#19968;&#22522;&#30784;&#21407;&#21017;&#20351;&#25105;&#20204;&#33021;&#22815;&#32472;&#21046;&#25512;&#29702;&#31995;&#32479;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#24182;&#23558;&#25512;&#29702;&#21010;&#20998;&#20026;&#22235;&#20010;&#19981;&#21516;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03288v1 Announce Type: new  Abstract: In the rapidly evolving field of Large Language Models (LLMs), there is a critical need to thoroughly analyze their capabilities and risks. Central to our investigation are two novel elements. Firstly, it is the innovative parallels between the statistical patterns of word relationships within LLMs and Martin Heidegger's concepts of "ready-to-hand" and "present-at-hand," which encapsulate the utilitarian and scientific altitudes humans employ in interacting with the world. This comparison lays the groundwork for positioning LLMs as the digital counterpart to the Faculty of Verbal Knowledge, shedding light on their capacity to emulate certain facets of human reasoning. Secondly, a structural analysis of human reasoning, viewed through Heidegger's notion of truth as "unconcealment" is conducted This foundational principle enables us to map out the inputs and outputs of the reasoning system and divide reasoning into four distinct categories
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#36827;&#34892;&#21487;&#20449;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65292;&#22312;&#32500;&#25345;&#31454;&#20105;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#21487;&#38752;&#25512;&#26029;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.03281</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#36827;&#34892;&#21487;&#20449;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03281
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#36827;&#34892;&#21487;&#20449;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65292;&#22312;&#32500;&#25345;&#31454;&#20105;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#21487;&#38752;&#25512;&#26029;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#38024;&#23545;&#36776;&#21035;&#23398;&#20064;&#30340;&#36831;&#21040;&#22810;&#27169;&#24577;&#34701;&#21512;&#38382;&#39064;&#12290;&#21463;&#21040;&#38656;&#35201;&#29702;&#35299;&#27599;&#20010;&#25968;&#25454;&#28304;&#21487;&#38752;&#24615;&#30340;&#22024;&#26434;&#30340;&#22810;&#28304;&#39046;&#22495;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#21487;&#20449;&#24230;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#26469;&#32467;&#21512;&#20010;&#20307;&#27169;&#24577;&#19978;&#30340;&#39044;&#27979;&#20998;&#24067;&#30340;&#32452;&#21512;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#23450;&#20041;&#20102;&#19968;&#31181;&#27010;&#29575;&#24230;&#37327;&#26469;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#30340;&#21487;&#20449;&#24230;&#65292;&#36890;&#36807;PC&#19978;&#30340;&#25512;&#29702;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#34701;&#21512;&#26041;&#27861;&#33021;&#22815;&#21487;&#38752;&#22320;&#25512;&#26029;&#21487;&#20449;&#24230;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03281v1 Announce Type: cross  Abstract: We consider the problem of late multi-modal fusion for discriminative learning. Motivated by noisy, multi-source domains that require understanding the reliability of each data source, we explore the notion of credibility in the context of multi-modal fusion. We propose a combination function that uses probabilistic circuits (PCs) to combine predictive distributions over individual modalities. We also define a probabilistic measure to evaluate the credibility of each modality via inference queries over the PC. Our experimental evaluation demonstrates that our fusion method can reliably infer credibility while maintaining competitive performance with the state-of-the-art.
&lt;/p&gt;</description></item><item><title>ARNN&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#21644;&#24182;&#34892;&#35745;&#31639;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;LSTM gate&#30340;&#20248;&#21183;&#65292;&#24182;&#36991;&#20813;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.03276</link><description>&lt;p&gt;
ARNN: &#29992;&#20110;&#35782;&#21035;&#30315;&#30187;&#21457;&#20316;&#30340;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03276
&lt;/p&gt;
&lt;p&gt;
ARNN&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#21644;&#24182;&#34892;&#35745;&#31639;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;LSTM gate&#30340;&#20248;&#21183;&#65292;&#24182;&#36991;&#20813;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;ARNN&#65289;&#65292;&#20854;&#27839;&#30528;&#24207;&#21015;&#24490;&#29615;&#24212;&#29992;&#27880;&#24847;&#21147;&#23618;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#24207;&#21015;&#38271;&#24230;&#30456;&#20851;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#19978;&#36816;&#34892;&#65292;&#32780;&#19981;&#26159;&#21333;&#36890;&#36947;&#20449;&#21495;&#65292;&#24182;&#21033;&#29992;&#24182;&#34892;&#35745;&#31639;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#27880;&#24847;&#21147;&#23618;&#26159;&#19968;&#31181;&#35745;&#31639;&#21333;&#20803;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35745;&#31639;&#19968;&#32452;&#24191;&#27867;&#25968;&#37327;&#30340;&#29366;&#24577;&#21521;&#37327;&#21644;&#36755;&#20837;&#20449;&#21495;&#30340;&#36882;&#24402;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21463;&#21040;&#20102;&#27880;&#24847;&#21147;&#23618;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#30340;&#21551;&#21457;&#65292;&#24182;&#20351;&#29992;&#38271;&#30701;&#39118;&#26684;&#38376;&#65292;&#20294;&#36890;&#36807;&#22810;&#20010;&#38454;&#27573;&#23558;&#36825;&#31181;&#20856;&#22411;&#21333;&#20803;&#25193;&#23637;&#21040;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#24182;&#34892;&#21270;&#12290;&#23427;&#32487;&#25215;&#20102;&#27880;&#24847;&#21147;&#23618;&#21644;LSTM&#38376;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#24322;&#36136;&#23454;&#39564;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#22411;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03276v1 Announce Type: cross  Abstract: We proposed an Attentive Recurrent Neural Network (ARNN), which recurrently applies attention layers along a sequence and has linear complexity with respect to the sequence length. The proposed model operates on multi-channel EEG signals rather than single channel signals and leverages parallel computation. In this cell, the attention layer is a computational unit that efficiently applies self-attention and cross-attention mechanisms to compute a recurrent function over a wide number of state vectors and input signals. Our architecture is inspired in part by the attention layer and long short-term memory (LSTM) cells, and it uses long-short style gates, but it scales this typical cell up by several orders to parallelize for multi-channel EEG signals. It inherits the advantages of attention layers and LSTM gate while avoiding their respective drawbacks. We evaluated the model effectiveness through extensive experiments with heterogeneou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33647;&#29702;&#23398;&#21551;&#21457;&#30340;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#25968;&#23383;&#20581;&#24247;&#25968;&#25454;&#20013;&#30340;&#27835;&#30103;&#25928;&#26524;&#21644;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#21453;&#20107;&#23454;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03274</link><description>&lt;p&gt;
&#20174;&#22122;&#38899;&#21040;&#20449;&#21495;&#65306;&#36890;&#36807;&#33647;&#29702;&#23398;&#21551;&#21457;&#30340;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#35299;&#23494;&#25968;&#23383;&#20581;&#24247;&#25968;&#25454;&#20013;&#30340;&#27835;&#30103;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
From Noise to Signal: Unveiling Treatment Effects from Digital Health Data through Pharmacology-Informed Neural-SDE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03274
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33647;&#29702;&#23398;&#21551;&#21457;&#30340;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#25968;&#23383;&#20581;&#24247;&#25968;&#25454;&#20013;&#30340;&#27835;&#30103;&#25928;&#26524;&#21644;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#21453;&#20107;&#23454;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#20581;&#24247;&#25216;&#26415;&#65288;DHT&#65289;&#65292;&#22914;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#12289;&#25345;&#32493;&#12289;&#23454;&#26102;&#30417;&#27979;&#24739;&#32773;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#25216;&#26415;&#27491;&#22312;&#20026;&#26032;&#30103;&#27861;&#21644;&#20010;&#24615;&#21270;&#21307;&#23398;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;&#20174;&#36825;&#20123;&#25216;&#26415;&#20013;&#33719;&#21462;&#27934;&#23519;&#21147;&#38656;&#35201;&#36866;&#24403;&#30340;&#24314;&#27169;&#25216;&#26415;&#26469;&#25429;&#25417;&#30142;&#30149;&#29366;&#24577;&#20013;&#30340;&#20020;&#24202;&#30456;&#20851;&#21464;&#21270;&#12290;&#36825;&#20123;&#35774;&#22791;&#20135;&#29983;&#30340;&#25968;&#25454;&#20855;&#26377;&#38543;&#26426;&#24615;&#65292;&#21487;&#33021;&#23384;&#22312;&#32570;&#22833;&#20803;&#32032;&#65292;&#24182;&#34920;&#29616;&#20986;&#30456;&#24403;&#22823;&#30340;&#20010;&#20307;&#38388;&#21464;&#24322;&#24615;&#65292;&#22240;&#27492;&#24456;&#38590;&#20351;&#29992;&#20256;&#32479;&#30340;&#32437;&#21521;&#24314;&#27169;&#25216;&#26415;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#26032;&#39062;&#33647;&#29702;&#23398;&#21551;&#21457;&#30340;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#27169;&#22411;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#27835;&#30103;&#25928;&#26524;&#21644;&#20174;&#38543;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#21453;&#20107;&#23454;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03274v1 Announce Type: cross  Abstract: Digital health technologies (DHT), such as wearable devices, provide personalized, continuous, and real-time monitoring of patient. These technologies are contributing to the development of novel therapies and personalized medicine. Gaining insight from these technologies requires appropriate modeling techniques to capture clinically-relevant changes in disease state. The data generated from these devices is characterized by being stochastic in nature, may have missing elements, and exhibits considerable inter-individual variability - thereby making it difficult to analyze using traditional longitudinal modeling techniques. We present a novel pharmacology-informed neural stochastic differential equation (SDE) model capable of addressing these challenges. Using synthetic data, we demonstrate that our approach is effective in identifying treatment effects and learning causal relationships from stochastic data, thereby enabling counterfac
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#23396;&#23376;&#29702;&#35770;&#21644;&#31561;&#31163;&#23376;&#29616;&#35937;&#22312;&#25968;&#23383;&#20581;&#24247;&#24179;&#21488;&#20013;&#24314;&#27169;&#29992;&#25143;&#34892;&#20026;&#21644;&#21442;&#19982;&#26041;&#38754;&#30340;&#21019;&#26032;&#24212;&#29992;&#65292;&#36890;&#36807;&#24341;&#20837;&#23396;&#23376;&#35299;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#29702;&#35299;&#20581;&#24247;&#25913;&#21892;&#34892;&#20026;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#28145;&#20837;&#30740;&#31350;&#20102;&#30802;&#21270;&#38217;&#32435;&#31859;&#39063;&#31890;&#22312;&#21560;&#38468;&#20551;&#26032;&#38395;&#20013;&#30340;&#31561;&#31163;&#23376;&#29305;&#24615;&#23545;&#29992;&#25143;&#20114;&#21160;&#21644;&#21442;&#19982;&#27700;&#24179;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03239</link><description>&lt;p&gt;
&#27880;&#24847;&#65306;&#21033;&#29992;&#30802;&#21270;&#38217;&#32435;&#31859;&#39063;&#31890;&#22312;&#25968;&#23383;&#39046;&#22495;&#30340;&#24067;&#40065;&#26031;&#29305;&#35282;&#21644;&#24503;&#40065;&#24503;&#27169;&#22411;&#20013;&#31561;&#31163;&#28608;&#20803;&#20849;&#25391;&#65292;&#29992;&#20110;&#20551;&#26032;&#38395;&#21560;&#38468;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#20013;&#30340;&#32972;&#26223;
&lt;/p&gt;
&lt;p&gt;
Note: Harnessing Tellurium Nanoparticles in the Digital Realm Plasmon Resonance, in the Context of Brewster's Angle and the Drude Model for Fake News Adsorption in Incomplete Information Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03239
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#23396;&#23376;&#29702;&#35770;&#21644;&#31561;&#31163;&#23376;&#29616;&#35937;&#22312;&#25968;&#23383;&#20581;&#24247;&#24179;&#21488;&#20013;&#24314;&#27169;&#29992;&#25143;&#34892;&#20026;&#21644;&#21442;&#19982;&#26041;&#38754;&#30340;&#21019;&#26032;&#24212;&#29992;&#65292;&#36890;&#36807;&#24341;&#20837;&#23396;&#23376;&#35299;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#29702;&#35299;&#20581;&#24247;&#25913;&#21892;&#34892;&#20026;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#28145;&#20837;&#30740;&#31350;&#20102;&#30802;&#21270;&#38217;&#32435;&#31859;&#39063;&#31890;&#22312;&#21560;&#38468;&#20551;&#26032;&#38395;&#20013;&#30340;&#31561;&#31163;&#23376;&#29305;&#24615;&#23545;&#29992;&#25143;&#20114;&#21160;&#21644;&#21442;&#19982;&#27700;&#24179;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31508;&#35760;&#25506;&#35752;&#20102;&#23396;&#23376;&#29702;&#35770;&#21644;&#31561;&#31163;&#23376;&#29616;&#35937;&#22312;&#24314;&#27169;&#25968;&#23383;&#20581;&#24247;&#24179;&#21488;&#20869;&#29992;&#25143;&#34892;&#20026;&#21644;&#21442;&#19982;&#26041;&#38754;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#23396;&#23376;&#35299;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29702;&#35299;&#38543;&#26102;&#38388;&#31283;&#23450;&#30340;&#20581;&#24247;&#25913;&#21892;&#34892;&#20026;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#30802;&#21270;&#38217;&#32435;&#31859;&#39063;&#31890;&#21450;&#20854;&#22312;&#21560;&#38468;&#20551;&#26032;&#38395;&#20013;&#30340;&#31561;&#31163;&#23376;&#29305;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#29992;&#25143;&#20114;&#21160;&#21644;&#21442;&#19982;&#27700;&#24179;&#12290;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#19982;&#30802;&#21270;&#38217;&#32435;&#31859;&#39063;&#31890;&#30340;&#29420;&#29305;&#29305;&#24449;&#30456;&#32467;&#21512;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#65292;&#20197;&#20102;&#35299;&#25968;&#23383;&#20581;&#24247;&#29615;&#22659;&#20013;&#29992;&#25143;&#21442;&#19982;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;&#23396;&#23376;&#29702;&#35770;&#22312;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#21160;&#24577;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#32780;&#31561;&#31163;&#23376;&#29616;&#35937;&#30340;&#24212;&#29992;&#20026;&#25552;&#39640;&#28789;&#25935;&#24230;&#21644;&#25928;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03239v1 Announce Type: cross  Abstract: This note explores the innovative application of soliton theory and plasmonic phenomena in modeling user behavior and engagement within digital health platforms. By introducing the concept of soliton solutions, we present a novel approach to understanding stable patterns of health improvement behaviors over time. Additionally, we delve into the role of tellurium nanoparticles and their plasmonic properties in adsorbing fake news, thereby influencing user interactions and engagement levels. Through a theoretical framework that combines nonlinear dynamics with the unique characteristics of tellurium nanoparticles, we aim to provide new insights into the dynamics of user engagement in digital health environments. Our analysis highlights the potential of soliton theory in capturing the complex, nonlinear dynamics of user behavior, while the application of plasmonic phenomena offers a promising avenue for enhancing the sensitivity and effec
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#24191;&#27867;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#30456;&#20851;&#21457;&#29616;&#65292;&#33021;&#22815;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#39044;&#27979;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#32467;&#26524;&#65292;&#39044;&#31034;&#30528;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20849;&#21516;&#36827;&#34892;&#21457;&#29616;&#30340;&#26410;&#26469;&#12290;</title><link>https://arxiv.org/abs/2403.03230</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#31070;&#32463;&#31185;&#23398;&#32467;&#26524;&#26041;&#38754;&#36229;&#36234;&#20154;&#31867;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Large language models surpass human experts in predicting neuroscience results
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03230
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#24191;&#27867;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#30456;&#20851;&#21457;&#29616;&#65292;&#33021;&#22815;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#39044;&#27979;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#32467;&#26524;&#65292;&#39044;&#31034;&#30528;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20849;&#21516;&#36827;&#34892;&#21457;&#29616;&#30340;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21457;&#29616;&#24120;&#24120;&#21462;&#20915;&#20110;&#32508;&#21512;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#65292;&#36825;&#19968;&#20219;&#21153;&#21487;&#33021;&#36229;&#20986;&#20154;&#31867;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#24191;&#27867;&#30340;&#31185;&#23398;&#25991;&#29486;&#19978;&#35757;&#32451;&#30340;LLMs&#21487;&#33021;&#33021;&#22815;&#25972;&#21512;&#22024;&#26434;&#20294;&#30456;&#20851;&#30340;&#21457;&#29616;&#65292;&#20197;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#26469;&#39044;&#27979;&#26032;&#39062;&#32467;&#26524;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;BrainBench&#65292;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#39044;&#27979;&#31070;&#32463;&#31185;&#23398;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#39044;&#27979;&#23454;&#39564;&#32467;&#26524;&#26041;&#38754;&#36229;&#36234;&#20102;&#19987;&#23478;&#12290;&#22312;&#31070;&#32463;&#31185;&#23398;&#25991;&#29486;&#19978;&#35843;&#25972;&#30340;&#19968;&#20010;LLM&#65292;BrainGPT&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#19982;&#20154;&#31867;&#19987;&#23478;&#19968;&#26679;&#65292;&#24403;LLMs&#23545;&#20182;&#20204;&#30340;&#39044;&#27979;&#26377;&#20449;&#24515;&#26102;&#65292;&#20182;&#20204;&#26356;&#26377;&#21487;&#33021;&#26159;&#27491;&#30830;&#30340;&#65292;&#36825;&#39044;&#31034;&#30528;&#26410;&#26469;&#20154;&#31867;&#21644;LLMs&#23558;&#21512;&#20316;&#36827;&#34892;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24182;&#38750;&#29305;&#23450;&#20110;&#31070;&#32463;&#31185;&#23398;&#65292;&#24182;&#19988;&#21487;&#36716;&#31227;&#21040;&#20854;&#20182;&#30693;&#35782;&#23494;&#38598;&#22411;&#20107;&#19994;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03230v1 Announce Type: cross  Abstract: Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. To evaluate this possibility, we created BrainBench, a forward-looking benchmark for predicting neuroscience results. We find that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries. Our approach is not neuroscience-specific and is transferable to other knowledge-intensive endeavors.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#23398;&#21338;&#24328;&#35770;&#27169;&#22411;&#29992;&#20110;&#30740;&#31350;&#29237;&#22763;&#21363;&#20852;&#28436;&#22863;&#65292;&#25506;&#32034;&#19981;&#21516;&#30340;&#38543;&#26426;&#21363;&#20852;&#31574;&#30053;&#21644;&#20854;&#22312;&#21363;&#20852;&#28436;&#22863;&#20013;&#30340;&#37197;&#23545;&#34920;&#29616;&#65292;&#21457;&#29616;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#23545;&#26159;&#36880;&#27493;&#25913;&#21464;&#21644;&#21644;&#24358;&#36319;&#38543;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.03224</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29237;&#22763;&#21363;&#20852;&#28436;&#22863;&#65306;&#38899;&#20048;&#36935;&#19978;&#21338;&#24328;&#35770;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Jazz Improvisation: When Music Meets Game Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03224
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#23398;&#21338;&#24328;&#35770;&#27169;&#22411;&#29992;&#20110;&#30740;&#31350;&#29237;&#22763;&#21363;&#20852;&#28436;&#22863;&#65292;&#25506;&#32034;&#19981;&#21516;&#30340;&#38543;&#26426;&#21363;&#20852;&#31574;&#30053;&#21644;&#20854;&#22312;&#21363;&#20852;&#28436;&#22863;&#20013;&#30340;&#37197;&#23545;&#34920;&#29616;&#65292;&#21457;&#29616;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#23545;&#26159;&#36880;&#27493;&#25913;&#21464;&#21644;&#21644;&#24358;&#36319;&#38543;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#30340;&#29616;&#22330;&#34920;&#28436;&#24635;&#26159;&#36855;&#20154;&#30340;&#65292;&#21363;&#20852;&#28436;&#22863;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#26159;&#30001;&#20110;&#38899;&#20048;&#23478;&#20043;&#38388;&#30340;&#21160;&#24577;&#20851;&#31995;&#21644;&#19982;&#35266;&#20247;&#30340;&#20114;&#21160;&#12290;&#29237;&#22763;&#21363;&#20852;&#28436;&#22863;&#26159;&#19968;&#20010;&#29305;&#21035;&#20540;&#24471;&#20174;&#29702;&#35770;&#35270;&#35282;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#20363;&#23376;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#23398;&#21338;&#24328;&#35770;&#27169;&#22411;&#26469;&#30740;&#31350;&#29237;&#22763;&#21363;&#20852;&#28436;&#22863;&#65292;&#20026;&#30740;&#31350;&#38899;&#20048;&#29702;&#35770;&#21644;&#21363;&#20852;&#28436;&#22863;&#26041;&#27861;&#23398;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#35745;&#31639;&#24314;&#27169;&#65292;&#20027;&#35201;&#26159;&#24378;&#21270;&#23398;&#20064;&#65292;&#26469;&#25506;&#32034;&#19981;&#21516;&#30340;&#38543;&#26426;&#21363;&#20852;&#31574;&#30053;&#21450;&#20854;&#22312;&#21363;&#20852;&#28436;&#22863;&#20013;&#30340;&#37197;&#23545;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#23545;&#26159;&#19968;&#31181;&#23545;&#26368;&#36817;&#25910;&#30410;&#20316;&#20986;&#21453;&#24212;&#30340;&#31574;&#30053;&#65288;&#36880;&#27493;&#25913;&#21464;&#65289;&#65292;&#37197;&#21512;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20165;&#38480;&#20110;&#32473;&#23450;&#21644;&#24358;&#20013;&#30340;&#38899;&#31526;&#65288;&#21644;&#24358;&#36319;&#38543;&#24378;&#21270;&#23398;&#20064;&#65289;&#12290;&#30456;&#21453;&#65292;&#19968;&#31181;&#23545;&#20249;&#20276;&#30340;&#19978;&#19968;&#20010;&#38899;&#31526;&#20316;&#20986;&#21453;&#24212;&#65292;&#24182;&#35797;&#22270;&#19982;&#20043;&#21644;&#35856;&#30340;&#31574;&#30053;&#65288;&#21644;&#35856;P
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03224v1 Announce Type: cross  Abstract: Live performances of music are always charming, with the unpredictability of improvisation due to the dynamic between musicians and interactions with the audience. Jazz improvisation is a particularly noteworthy example for further investigation from a theoretical perspective. Here, we introduce a novel mathematical game theory model for jazz improvisation, providing a framework for studying music theory and improvisational methodologies. We use computational modeling, mainly reinforcement learning, to explore diverse stochastic improvisational strategies and their paired performance on improvisation. We find that the most effective strategy pair is a strategy that reacts to the most recent payoff (Stepwise Changes) with a reinforcement learning strategy limited to notes in the given chord (Chord-Following Reinforcement Learning). Conversely, a strategy that reacts to the partner's last note and attempts to harmonize with it (Harmony P
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#24341;&#23548;&#30340;EEG&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#24615;&#33021;&#21644;&#26174;&#33879;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.03222</link><description>&lt;p&gt;
&#30693;&#35782;&#24341;&#23548;&#30340;EEG&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Knowledge-guided EEG Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03222
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#24341;&#23548;&#30340;EEG&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#24615;&#33021;&#21644;&#26174;&#33879;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03222v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25688;&#35201;:&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#35821;&#38899;&#31561;&#22810;&#23186;&#20307;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#30001;&#20110;&#36825;&#20123;&#24773;&#26223;&#20013;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#36825;&#31181;&#33539;&#24335;&#23545;&#20110;&#29983;&#29289;&#20449;&#21495;&#39046;&#22495;&#21516;&#26679;&#37325;&#35201;&#65292;&#29978;&#33267;&#26356;&#37325;&#35201;&#12290;&#21033;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#23398;&#20064;&#31283;&#20581;&#30340;&#34920;&#31034;&#33021;&#22815;&#24110;&#21161;&#25552;&#39640;&#29983;&#29289;&#20449;&#21495;&#19978;&#35768;&#22810;&#25512;&#26029;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#22810;&#23186;&#20307;&#27169;&#24577;&#21644;&#29983;&#29289;&#20449;&#21495;&#20043;&#38388;&#22266;&#26377;&#30340;&#39046;&#22495;&#24046;&#24322;&#65292;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#24314;&#31435;&#30340;&#20256;&#32479;&#30446;&#26631;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#36716;&#21270;&#21040;&#36825;&#19968;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#36825;&#20123;&#26041;&#27861;&#35843;&#25972;&#21040;&#29983;&#29289;&#20449;&#21495;&#20998;&#26512;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#33258;&#30417;&#30563;EEG&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#24341;&#23548;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#25552;&#20379;&#31283;&#20581;&#30340;&#24615;&#33021;&#21644;&#26174;&#33879;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03222v1 Announce Type: cross  Abstract: Self-supervised learning has produced impressive results in multimedia domains of audio, vision and speech. This paradigm is equally, if not more, relevant for the domain of biosignals, owing to the scarcity of labelled data in such scenarios. The ability to leverage large-scale unlabelled data to learn robust representations could help improve the performance of numerous inference tasks on biosignals. Given the inherent domain differences between multimedia modalities and biosignals, the established objectives for self-supervised learning may not translate well to this domain. Hence, there is an unmet need to adapt these methods to biosignal analysis. In this work we propose a self-supervised model for EEG, which provides robust performance and remarkable parameter efficiency by using state space-based deep learning architecture. We also propose a novel knowledge-guided pre-training objective that accounts for the idiosyncrasies of th
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;SQL&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#26368;&#20339;&#25552;&#31034;&#27169;&#26495;&#21644;&#35774;&#35745;&#26694;&#26550;&#20173;&#26080;&#20849;&#35782;&#65292;&#26032;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#20219;&#21153;&#26377;&#21161;&#20110;&#20840;&#38754;&#35780;&#20272;&#21508;&#31181;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.02951</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;SQL&#33021;&#21147;&#65306;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02951
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;SQL&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#26368;&#20339;&#25552;&#31034;&#27169;&#26495;&#21644;&#35774;&#35745;&#26694;&#26550;&#20173;&#26080;&#20849;&#35782;&#65292;&#26032;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#20219;&#21153;&#26377;&#21161;&#20110;&#20840;&#38754;&#35780;&#20272;&#21508;&#31181;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#25512;&#21160;&#25991;&#26412;&#29983;&#25104;SQL&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#20110;&#26368;&#20339;&#25552;&#31034;&#27169;&#26495;&#21644;&#35774;&#35745;&#26694;&#26550;&#20173;&#28982;&#27809;&#26377;&#36798;&#25104;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02951v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods. Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions.To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs. Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process.Our study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task. These findings offer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#36234;&#29425;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#25554;&#20837;&#24694;&#24847;&#25991;&#26412;&#25552;&#31034;&#65292;&#25104;&#21151;&#23454;&#26045;&#36234;&#29425;&#25915;&#20987;&#65292;&#24182;&#20998;&#26512;&#20102;&#26377;&#27602;&#25968;&#25454;&#27604;&#29575;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#20301;&#32622;&#23545;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.02910</link><description>&lt;p&gt;
ImgTrojan: &#29992;&#19968;&#24352;&#22270;&#29255;&#23545;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
ImgTrojan: Jailbreaking Vision-Language Models with ONE Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#36234;&#29425;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#25554;&#20837;&#24694;&#24847;&#25991;&#26412;&#25552;&#31034;&#65292;&#25104;&#21151;&#23454;&#26045;&#36234;&#29425;&#25915;&#20987;&#65292;&#24182;&#20998;&#26512;&#20102;&#26377;&#27602;&#25968;&#25454;&#27604;&#29575;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#20301;&#32622;&#23545;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19982;&#35270;&#35273;&#27169;&#22359;&#38598;&#25104;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;VLMs&#30340;&#26032;&#22411;&#36234;&#29425;&#25915;&#20987;&#65292;&#26088;&#22312;&#24403;&#29992;&#25143;&#36755;&#20837;&#26377;&#23475;&#25351;&#20196;&#26102;&#32469;&#36807;&#20854;&#23433;&#20840;&#38459;&#30861;&#12290;&#20551;&#35774;&#25105;&#20204;&#30340;&#26377;&#27602;&#65288;&#22270;&#20687;&#65292;&#25991;&#26412;&#65289;&#25968;&#25454;&#23545;&#21253;&#21547;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#36890;&#36807;&#29992;&#24694;&#24847;&#36234;&#29425;&#25552;&#31034;&#26367;&#25442;&#21407;&#22987;&#25991;&#26412;&#26631;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#26377;&#27602;&#22270;&#20687;&#25191;&#34892;&#36234;&#29425;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26377;&#27602;&#27604;&#29575;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#20301;&#32622;&#23545;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#25105;&#20204;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#21644;&#38544;&#34109;&#24615;&#12290;&#32467;&#21512;&#19968;&#31995;&#21015;&#31574;&#21010;&#30340;&#26377;&#23475;&#25351;&#20196;&#65292;&#21487;&#20197;&#34913;&#37327;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02910v1 Announce Type: cross  Abstract: There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficac
&lt;/p&gt;</description></item><item><title>&#36825;&#19968;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35828;&#35805;&#32773;&#30340;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#38899;&#20219;&#21153;&#65292;&#36890;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#36716;&#24405;&#30830;&#20445;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02371</link><description>&lt;p&gt;
NeuroVoz&#65306;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#30340;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NeuroVoz: a Castillian Spanish corpus of parkinsonian speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02371
&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35828;&#35805;&#32773;&#30340;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#38899;&#20219;&#21153;&#65292;&#36890;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#36716;&#24405;&#30830;&#20445;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#38899;&#20998;&#26512;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#35786;&#26029;&#30340;&#36827;&#23637;&#21463;&#21040;&#20844;&#24320;&#21487;&#29992;&#12289;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#26174;&#33879;&#32570;&#20047;&#30340;&#38459;&#30861;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#20877;&#29616;&#24615;&#21644;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#26469;&#33258;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#30340;&#35828;&#35805;&#32773;&#65292;&#21253;&#25324;55&#21517;&#20581;&#24247;&#23545;&#29031;&#32452;&#21644;53&#21517;&#34987;&#35786;&#26029;&#24739;&#26377;PD&#30340;&#20010;&#20307;&#65292;&#25152;&#26377;&#36825;&#20123;&#20010;&#20307;&#37117;&#22312;&#33647;&#29289;&#27835;&#30103;&#19979;&#65292;&#24182;&#19988;&#22312;&#33647;&#29289;&#20248;&#21270;&#29366;&#24577;&#19979;&#36827;&#34892;&#35760;&#24405;&#12290; &#36825;&#19968;&#29420;&#29305;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#35821;&#38899;&#20219;&#21153;&#65292;&#21253;&#25324;&#25345;&#32493;&#21457;&#38899;&#20116;&#20010;&#35199;&#29677;&#29273;&#20803;&#38899;&#12289;&#21457;&#38899;&#27979;&#35797;&#12289;16&#20010;&#21548;&#21518;&#37325;&#22797;&#30340;&#35805;&#35821;&#20197;&#21450;&#33258;&#30001;&#29420;&#30333;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#19987;&#23478;&#25163;&#21160;&#36716;&#24405;&#21548;&#21518;&#37325;&#22797;&#20219;&#21153;&#24378;&#35843;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#21033;&#29992;Whisper&#36827;&#34892;&#33258;&#21160;&#29420;&#30333;&#36716;&#24405;&#65292;&#20351;&#20854;&#25104;&#20026;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#30340;&#26368;&#23436;&#25972;&#30340;&#20844;&#24320;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02371v1 Announce Type: cross  Abstract: The advancement of Parkinson's Disease (PD) diagnosis through speech analysis is hindered by a notable lack of publicly available, diverse language datasets, limiting the reproducibility and further exploration of existing research.   In response to this gap, we introduce a comprehensive corpus from 108 native Castilian Spanish speakers, comprising 55 healthy controls and 53 individuals diagnosed with PD, all of whom were under pharmacological treatment and recorded in their medication-optimized state. This unique dataset features a wide array of speech tasks, including sustained phonation of the five Spanish vowels, diadochokinetic tests, 16 listen-and-repeat utterances, and free monologues. The dataset emphasizes accuracy and reliability through specialist manual transcriptions of the listen-and-repeat tasks and utilizes Whisper for automated monologue transcriptions, making it the most complete public corpus of Parkinsonian speech, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#31639;&#27861;&#36873;&#25321;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#26681;&#25454;&#20248;&#21270;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#20010;&#31639;&#27861;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#38382;&#39064;&#23454;&#20363;&#19978;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02131</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21160;&#24577;&#31639;&#27861;&#36873;&#25321;&#65306;&#20197;&#24494;&#20998;&#36827;&#21270;&#20026;&#20363;&#30340;&#21407;&#29702;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Dynamic Algorithm Selection: A Proof-of-Principle Study on Differential Evolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#31639;&#27861;&#36873;&#25321;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#26681;&#25454;&#20248;&#21270;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#20010;&#31639;&#27861;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#38382;&#39064;&#23454;&#20363;&#19978;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#65292;&#22914;&#24494;&#20998;&#36827;&#21270;&#65292;&#22312;&#35299;&#20915;&#23454;&#25968;&#21442;&#25968;&#20248;&#21270;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#21333;&#20010;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#38382;&#39064;&#23454;&#20363;&#19978;&#21464;&#21270;&#65292;&#38656;&#35201;&#22312;&#31639;&#27861;&#36873;&#25321;&#25110;&#37197;&#32622;&#26041;&#38754;&#25237;&#20837;&#30456;&#24403;&#22810;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19968;&#32452;&#31639;&#27861;&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#24182;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#20026;&#29305;&#23450;&#38382;&#39064;&#21160;&#24577;&#35843;&#24230;&#23427;&#20204;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#31639;&#27861;&#36873;&#25321;&#26694;&#26550;&#26469;&#23436;&#25104;&#36825;&#19968;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21160;&#24577;&#31639;&#27861;&#36873;&#25321;&#24314;&#27169;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#31574;&#30053;&#26799;&#24230;&#26041;&#24335;&#35757;&#32451;&#20195;&#29702;&#36873;&#25321;&#26681;&#25454;&#20248;&#21270;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#31639;&#27861;&#12290;&#20026;&#20102;&#20351;&#20195;&#29702;&#20855;&#22791;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;&#19968;&#20010;&#32463;&#36807;&#28145;&#24605;&#29087;&#34385;&#30340;&#26223;&#35266;&#21644;&#31639;&#27861;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02131v1 Announce Type: cross  Abstract: Evolutionary algorithms, such as Differential Evolution, excel in solving real-parameter optimization challenges. However, the effectiveness of a single algorithm varies across different problem instances, necessitating considerable efforts in algorithm selection or configuration. This paper aims to address the limitation by leveraging the complementary strengths of a group of algorithms and dynamically scheduling them throughout the optimization progress for specific problems. We propose a deep reinforcement learning-based dynamic algorithm selection framework to accomplish this task. Our approach models the dynamic algorithm selection a Markov Decision Process, training an agent in a policy gradient manner to select the most suitable algorithm according to the features observed during the optimization process. To empower the agent with the necessary information, our framework incorporates a thoughtful design of landscape and algorith
&lt;/p&gt;</description></item><item><title>&#35813;&#20301;&#32622;&#35770;&#25991;&#35752;&#35770;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#26041;&#38754;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#21517;&#20026;ImplicitBench&#30340;&#26032;&#22522;&#20934;&#65292;&#24182;&#23545; T2I &#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#34920;&#29616;&#21450;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;</title><link>https://arxiv.org/abs/2403.02118</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#38754;&#21521;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#38544;&#24335;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Towards Implicit Prompt For Text-To-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02118
&lt;/p&gt;
&lt;p&gt;
&#35813;&#20301;&#32622;&#35770;&#25991;&#35752;&#35770;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#26041;&#38754;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#21517;&#20026;ImplicitBench&#30340;&#26032;&#22522;&#20934;&#65292;&#24182;&#23545; T2I &#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#34920;&#29616;&#21450;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24182;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20934;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21482;&#32771;&#34385;&#20102;&#26174;&#24335;&#25552;&#31034;&#65292;&#32780;&#24573;&#30053;&#20102;&#38544;&#24335;&#25552;&#31034;&#65288;&#26263;&#31034;&#30446;&#26631;&#32780;&#19981;&#26126;&#30830;&#25552;&#21040;&#65289;&#12290;&#36825;&#20123;&#25552;&#31034;&#21487;&#33021;&#28040;&#38500;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24212;&#29992;&#26500;&#25104;&#28508;&#22312;&#23041;&#32961;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24403;&#19979;T2I&#27169;&#22411;&#26397;&#30528;&#38544;&#24335;&#25552;&#31034;&#30340;&#29616;&#29366;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ImplicitBench&#30340;&#22522;&#20934;&#65292;&#24182;&#23545;&#27969;&#34892;&#30340;T2I&#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#24615;&#33021;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#25910;&#38598;&#20102;&#19977;&#20010;&#26041;&#38754;&#30340;&#36229;&#36807;2,000&#20010;&#38544;&#24335;&#25552;&#31034;&#65306;&#36890;&#29992;&#31526;&#21495;&#12289;&#21517;&#20154;&#38544;&#31169;&#21644;&#19981;&#23433;&#20840;&#30340;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20845;&#20010;&#30693;&#21517;T2I&#27169;&#22411;&#22312;&#36825;&#20123;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#65288;1&#65289;T2I&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#21019;&#24314;&#21508;&#31181;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02118v1 Announce Type: cross  Abstract: Recent text-to-image (T2I) models have had great success, and many benchmarks have been proposed to evaluate their performance and safety. However, they only consider explicit prompts while neglecting implicit prompts (hint at a target without explicitly mentioning it). These prompts may get rid of safety constraints and pose potential threats to the applications of these models. This position paper highlights the current state of T2I models toward implicit prompts. We present a benchmark named ImplicitBench and conduct an investigation on the performance and impacts of implicit prompts with popular T2I models. Specifically, we design and collect more than 2,000 implicit prompts of three aspects: General Symbols, Celebrity Privacy, and Not-Safe-For-Work (NSFW) Issues, and evaluate six well-known T2I models' capabilities under these implicit prompts. Experiment results show that (1) T2I models are able to accurately create various targe
&lt;/p&gt;</description></item><item><title>AllSpark&#21033;&#29992;&#36890;&#36947;&#32423;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#20174;&#26410;&#26631;&#35760;&#30340;&#29305;&#24449;&#20013;&#37325;&#26032;&#29983;&#25104;&#26631;&#35760;&#29305;&#24449;&#65292;&#20197;&#25913;&#21892;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#20302;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01818</link><description>&lt;p&gt;
AllSpark: &#21033;&#29992;Transformer&#20013;&#26410;&#26631;&#35760;&#30340;&#29305;&#24449;&#37325;&#26032;&#29983;&#25104;&#26631;&#35760;&#29305;&#24449;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01818
&lt;/p&gt;
&lt;p&gt;
AllSpark&#21033;&#29992;&#36890;&#36947;&#32423;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#20174;&#26410;&#26631;&#35760;&#30340;&#29305;&#24449;&#20013;&#37325;&#26032;&#29983;&#25104;&#26631;&#35760;&#29305;&#24449;&#65292;&#20197;&#25913;&#21892;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#20302;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;SSSS&#65289;&#26088;&#22312;&#20943;&#36731;&#32791;&#26102;&#30340;&#20687;&#32032;&#32423;&#25163;&#21160;&#26631;&#27880;&#36127;&#25285;&#65292;&#23427;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#20197;&#21450;&#26356;&#22810;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20934;&#30495;&#20540;&#35757;&#32451;&#26631;&#35760;&#25968;&#25454;&#21644;&#20351;&#29992;&#20266;&#26631;&#31614;&#35757;&#32451;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#35757;&#32451;&#27969;&#31243;&#26159;&#20998;&#24320;&#30340;&#65292;&#36825;&#20351;&#24471;&#26631;&#35760;&#25968;&#25454;&#20027;&#23548;&#35757;&#32451;&#36807;&#31243;&#65292;&#23548;&#33268;&#20302;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#21644;&#20174;&#32780;&#27425;&#20248;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AllSpark&#65292;&#21033;&#29992;&#36890;&#36947;&#32423;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#20174;&#26410;&#26631;&#35760;&#30340;&#29305;&#24449;&#20013;&#37325;&#26032;&#29983;&#25104;&#26631;&#35760;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#35821;&#20041;&#35760;&#24518;&#21644;&#36890;&#36947;&#35821;&#20041;&#20998;&#32452;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#26410;&#26631;&#35760;&#29305;&#24449;&#20805;&#20998;&#20195;&#34920;&#26631;&#35760;&#29305;&#24449;&#12290;AllSpark&#20026;SSSS&#30340;&#26550;&#26500;&#32423;&#35774;&#35745;&#24102;&#26469;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#32780;&#38750;&#26694;&#26550;&#32423;&#21035;&#65292;&#36991;&#20813;&#20102;&#36234;&#26469;&#36234;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01818v1 Announce Type: cross  Abstract: Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate the burden of time-consuming pixel-level manual labeling, which leverages limited labeled data along with larger amounts of unlabeled data. Current state-of-the-art methods train the labeled data with ground truths and unlabeled data with pseudo labels. However, the two training flows are separate, which allows labeled data to dominate the training process, resulting in low-quality pseudo labels and, consequently, sub-optimal results. To alleviate this issue, we present AllSpark, which reborns the labeled features from unlabeled ones with the channel-wise cross-attention mechanism. We further introduce a Semantic Memory along with a Channel Semantic Grouping strategy to ensure that unlabeled features adequately represent labeled features. The AllSpark shed new light on the architecture level designs of SSSS rather than framework level, which avoids increasingly
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.19379</link><description>&lt;p&gt;
&#30789;&#35895;&#20154;&#32676;&#30340;&#26234;&#24935;&#65306;LLM&#38598;&#25104;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#20154;&#32676;&#20934;&#30830;&#29575;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;&#20381;&#36182;&#20110;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#25928;&#24212;&#65292;&#21363;&#36890;&#36807;&#32858;&#21512;&#19968;&#32676;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#36807;&#21435;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#21069;&#27839;LLMs&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#20154;&#31867;&#32676;&#20307;&#39044;&#27979;&#27604;&#36187;&#30340;&#40644;&#37329;&#26631;&#20934;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30001;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;31&#20010;&#20108;&#20803;&#38382;&#39064;&#30340;&#32858;&#21512;LLM&#39044;&#27979;&#19982;&#19968;&#20010;&#26469;&#33258;&#19977;&#20010;&#26376;&#39044;&#27979;&#27604;&#36187;&#30340;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#32676;&#20307;&#30340;&#34920;&#29616;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#31181;&#39034;&#20174;&#25928;&#24212;&#65292;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26126;&#26174;&#39640;&#20110;50%&#65292;&#23613;&#31649;&#20960;&#20046;&#26159;&#24179;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#26410;&#35265;&#20219;&#21153;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#39640;&#32500;&#35266;&#27979;&#31354;&#38388;&#20013;&#27867;&#21270;&#31574;&#30053;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.18759</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#30340;&#29366;&#24577;&#25277;&#35937;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Language-Guided State Abstractions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18759
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#26410;&#35265;&#20219;&#21153;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#39640;&#32500;&#35266;&#27979;&#31354;&#38388;&#20013;&#27867;&#21270;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#35774;&#35745;&#29366;&#24577;&#25277;&#35937;&#29992;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;&#22312;&#39640;&#32500;&#35266;&#27979;&#31354;&#38388;&#20013;&#23454;&#29616;&#27867;&#21270;&#31574;&#30053;&#23398;&#20064;&#30340;&#20851;&#38190;&#22312;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#36825;&#21487;&#20197;&#23558;&#29615;&#22659;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#23637;&#29616;&#20986;&#26469;&#24182;&#38544;&#34255;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#29366;&#24577;&#34920;&#31034;&#36890;&#24120;&#26159;&#25163;&#21160;&#25351;&#23450;&#30340;&#65292;&#25110;&#32773;&#26159;&#20174;&#20854;&#20182;&#32321;&#37325;&#30340;&#26631;&#35760;&#36807;&#31243;&#20013;&#23548;&#20986;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;LGA&#65288;&#35821;&#35328;&#24341;&#23548;&#30340;&#25277;&#35937;&#65289;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#30693;&#35782;&#30340;&#32467;&#21512;&#33258;&#21160;&#26500;&#24314;&#36866;&#29992;&#20110;&#26410;&#35265;&#20219;&#21153;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#22312;LGA&#20013;&#65292;&#29992;&#25143;&#39318;&#20808;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#20379;&#30446;&#26631;&#20219;&#21153;&#30340;&#65288;&#21487;&#33021;&#26159;&#19981;&#23436;&#25972;&#30340;&#65289;&#25551;&#36848;&#65307;&#25509;&#19979;&#26469;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#36825;&#20010;&#20219;&#21153;&#25551;&#36848;&#36716;&#21270;&#20026;&#25513;&#30422;&#19981;&#30456;&#20851;&#29305;&#24449;&#30340;&#29366;&#24577;&#25277;&#35937;&#20989;&#25968;&#65307;&#26368;&#21518;&#65292;&#20351;&#29992;&#23569;&#37327;&#28436;&#31034;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#27169;&#20223;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18759v1 Announce Type: cross  Abstract: We describe a framework for using natural language to design state abstractions for imitation learning. Generalizable policy learning in high-dimensional observation spaces is facilitated by well-designed state representations, which can surface important features of an environment and hide irrelevant ones. These state representations are typically manually specified, or derived from other labor-intensive labeling procedures. Our method, LGA (language-guided abstraction), uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks. In LGA, a user first provides a (possibly incomplete) description of a target task in natural language; next, a pre-trained LM translates this task description into a state abstraction function that masks out irrelevant features; finally, an imitation policy is trained using a small number of demo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18571</link><description>&lt;p&gt;
&#29992;&#20110;&#28385;&#36275;&#22810;&#26679;&#29992;&#25143;&#20559;&#22909;&#30340;&#31639;&#26415;&#25511;&#21046;LLMs&#65306;&#20855;&#26377;&#22810;&#30446;&#26631;&#22870;&#21169;&#30340;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31934;&#32454;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#36866;&#24212;&#21508;&#31181;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#24314;&#27169;&#26469;&#34920;&#31034;&#22810;&#26679;&#21270;&#30340;&#20559;&#22909;&#37197;&#32622;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20026;&#22870;&#21169;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65288;&#21363;&#21333;&#20301;&#21521;&#37327;&#65289;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18571v1 Announce Type: cross  Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;Clean-LaVe&#65292;&#26088;&#22312;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#26469;&#22686;&#24378;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18061</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#20449;&#24687;&#25552;&#21462;&#20013;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#36827;&#34892;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;Clean-LaVe&#65292;&#26088;&#22312;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#26469;&#22686;&#24378;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#39046;&#22495;&#65292;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#20854;&#20182;NLP&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#25991;&#26412;&#34164;&#28085;&#65289;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;NLP&#20219;&#21153;&#30340;&#29616;&#25104;&#27169;&#22411;&#30452;&#25509;&#23545;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#22823;&#37327;&#30340;IE&#27880;&#37322;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#28508;&#22312;&#26377;&#20215;&#20540;&#30340;&#21103;&#20135;&#21697;&#26159;&#22823;&#35268;&#27169;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#65292;&#21363;&#20854;&#20182;NLP&#20219;&#21153;&#30340;&#29616;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20266;&#26631;&#35760;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#21033;&#29992;&#24182;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;Clean-LaVe&#65292;&#26088;&#22312;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#26469;&#22686;&#24378;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;Clean-LaVe&#21253;&#25324;&#22235;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#33719;&#21462;&#38134;&#26631;&#20934;&#25968;&#25454;&#65307;&#65288;2&#65289;&#20174;&#38134;&#26631;&#20934;&#25968;&#25454;&#20013;&#35782;&#21035;&#30456;&#23545;&#24178;&#20928;&#30340;&#25968;&#25454;&#65307;&#65288;3&#65289;&#20351;&#29992;&#24178;&#20928;&#25968;&#25454;&#24494;&#35843;&#29616;&#25104;&#27169;&#22411;&#65307;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18061v1 Announce Type: cross  Abstract: The superior performance of supervised classification methods in the information extraction (IE) area heavily relies on a large amount of gold standard data. Recent zero-shot classification methods converted the task to other NLP tasks (e.g., textual entailment) and used off-the-shelf models of these NLP tasks to directly perform inference on the test data without using a large amount of IE annotation data. A potentially valuable by-product of these methods is the large-scale silver standard data, i.e., pseudo-labeled data by the off-the-shelf models of other NLP tasks. However, there is no further investigation into the use of these data. In this paper, we propose a new framework, Clean-LaVe, which aims to utilize silver standard data to enhance the zero-shot performance. Clean-LaVe includes four phases: (1) Obtaining silver data; (2) Identifying relatively clean data from silver data; (3) Finetuning the off-the-shelf model using clea
&lt;/p&gt;</description></item><item><title>SADM&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#24341;&#20837;&#32467;&#26500;&#25351;&#23548;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#27599;&#20010;&#35757;&#32451;&#25209;&#27425;&#20013;&#26679;&#26412;&#20043;&#38388;&#30340;&#27969;&#24418;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.17563</link><description>&lt;p&gt;
&#32467;&#26500;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Structure-Guided Adversarial Training of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17563
&lt;/p&gt;
&lt;p&gt;
SADM&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#24341;&#20837;&#32467;&#26500;&#25351;&#23548;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#27599;&#20010;&#35757;&#32451;&#25209;&#27425;&#20013;&#26679;&#26412;&#20043;&#38388;&#30340;&#27969;&#24418;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#29983;&#25104;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#21151;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#20027;&#35201;&#20391;&#37325;&#20110;&#26368;&#23567;&#21270;&#21152;&#26435;&#21435;&#22122;&#35780;&#20998;&#21305;&#37197;&#25439;&#22833;&#20197;&#36827;&#34892;&#25968;&#25454;&#20998;&#24067;&#24314;&#27169;&#30340;&#35757;&#32451;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32467;&#26500;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;&#65288;SADM&#65289;&#12290;&#22312;&#36825;&#31181;&#24320;&#21019;&#24615;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#24378;&#36843;&#27169;&#22411;&#22312;&#27599;&#20010;&#35757;&#32451;&#25209;&#27425;&#20013;&#23398;&#20064;&#26679;&#26412;&#20043;&#38388;&#30340;&#27969;&#24418;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17563v2 Announce Type: cross  Abstract: Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26377;&#30028;&#24615;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2402.16899</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#20013;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#30340;\emph{&#20808;&#39564;&#20272;&#35745;}
&lt;/p&gt;
&lt;p&gt;
A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26377;&#30028;&#24615;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#22823;&#35268;&#27169;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24615;&#33021;&#20998;&#26512;&#24573;&#30053;&#20102;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#26080;&#27861;&#30452;&#25509;&#20272;&#35745;Bellman&#26368;&#20248;&#25439;&#22833;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#19988;&#38656;&#35201;&#19968;&#20010;&#26377;&#30028;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#28385;&#36275;&#21322;&#32676;&#21644;Lipschitz&#24615;&#36136;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22312;&#35813;&#26041;&#27861;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;&#30340;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#20004;&#27425;&#36716;&#25442;&#12290;&#20026;&#20102;&#23436;&#25104;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#20998;&#26512;&#26041;&#27861;&#19981;&#38656;&#35201;&#26377;&#30028;&#24615;&#20551;&#35774;&#12290;&#26368;&#32456;&#25105;&#20204;&#32500;&#24471;&#21040;&#20102;&#19968;&#20010;&#27809;&#26377;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;&#30340;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16899v1 Announce Type: cross  Abstract: Deep reinforcement learning excels in numerous large-scale practical applications. However, existing performance analyses ignores the unique characteristics of continuous-time control problems, is unable to directly estimate the generalization error of the Bellman optimal loss and require a boundedness assumption. Our work focuses on continuous-time control problems and proposes a method that is applicable to all such problems where the transition function satisfies semi-group and Lipschitz properties. Under this method, we can directly analyze the \emph{a priori} generalization error of the Bellman optimal loss. The core of this method lies in two transformations of the loss function. To complete the transformation, we propose a decomposition method for the maximum operator. Additionally, this analysis method does not require a boundedness assumption. Finally, we obtain an \emph{a priori} generalization error without the curse of dime
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#27169;&#22411;&#65288;ContextDiff&#65289;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#36807;&#31243;&#20013;&#34701;&#20837;&#25991;&#26412;&#26465;&#20214;&#21644;&#35270;&#35273;&#26679;&#26412;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#23545;&#40784;&#65292;&#20197;&#20415;&#22312;&#35270;&#35273;&#29983;&#25104;&#20013;&#26356;&#20934;&#30830;&#22320;&#20256;&#36798;&#25991;&#26412;&#35821;&#20041;</title><link>https://arxiv.org/abs/2402.16627</link><description>&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#19979;&#30340;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#29983;&#25104;&#19982;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16627
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#27169;&#22411;&#65288;ContextDiff&#65289;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#36807;&#31243;&#20013;&#34701;&#20837;&#25991;&#26412;&#26465;&#20214;&#21644;&#35270;&#35273;&#26679;&#26412;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#23545;&#40784;&#65292;&#20197;&#20415;&#22312;&#35270;&#35273;&#29983;&#25104;&#20013;&#26356;&#20934;&#30830;&#22320;&#20256;&#36798;&#25991;&#26412;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#39640;&#20445;&#30495;&#24230;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#35273;&#29983;&#25104;&#21644;&#32534;&#36753;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25991;&#26412;&#24341;&#23548;&#35270;&#35273;&#25193;&#25955;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#20110;&#23558;&#25991;&#26412;-&#35270;&#35273;&#20851;&#31995;&#29420;&#21344;&#22320;&#34701;&#20837;&#21040;&#36870;&#36807;&#31243;&#20013;&#65292;&#24448;&#24448;&#24573;&#30053;&#20102;&#23427;&#20204;&#22312;&#27491;&#21521;&#36807;&#31243;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#27491;&#21453;&#36807;&#31243;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#21487;&#33021;&#38480;&#21046;&#20102;&#22312;&#35270;&#35273;&#21512;&#25104;&#32467;&#26524;&#20013;&#31934;&#30830;&#20256;&#36798;&#25991;&#26412;&#35821;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#27169;&#22411;&#65288;ContextDiff&#65289;&#65292;&#36890;&#36807;&#23558;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#21253;&#21547;&#25991;&#26412;&#26465;&#20214;&#21644;&#35270;&#35273;&#26679;&#26412;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#23545;&#40784;&#34701;&#20837;&#21040;&#27491;&#21521;&#21644;&#36870;&#21521;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#19978;&#19979;&#25991;&#20256;&#25773;&#21040;&#20004;&#20010;&#36807;&#31243;&#20013;&#30340;&#25152;&#26377;&#26102;&#38388;&#27493;&#65292;&#20197;&#35843;&#25972;&#23427;&#20204;&#30340;&#36712;&#36857;&#65292;&#20174;&#32780;&#20419;&#36827;&#36328;&#27169;&#24577;&#26465;&#20214;&#24314;&#27169;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#25512;&#24191;&#21040;DDPMs&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16627v2 Announce Type: cross  Abstract: Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#20449;&#24687;&#27969;&#65292;&#25552;&#39640;&#20102;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#23458;&#25143;&#21442;&#19982;&#24230;&#21644;&#20307;&#39564;&#65292;&#36716;&#21270;&#29575;&#25552;&#21319;4.9&#65285;&#12290;</title><link>https://arxiv.org/abs/2402.16073</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#29983;&#25104;&#20960;&#20046;&#23454;&#26102;&#20010;&#24615;&#21270;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16073
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#20449;&#24687;&#27969;&#65292;&#25552;&#39640;&#20102;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#23458;&#25143;&#21442;&#19982;&#24230;&#21644;&#20307;&#39564;&#65292;&#36716;&#21270;&#29575;&#25552;&#21319;4.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#23884;&#20837;&#26469;&#32534;&#30721;&#29992;&#25143;&#21160;&#20316;&#21644;&#39033;&#30446;&#65292;&#28982;&#21518;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#26816;&#32034;&#65292;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#20004;&#20010;&#25361;&#25112;&#65306;1&#65289;&#29992;&#25143;&#23884;&#20837;&#21487;&#33021;&#38480;&#21046;&#25152;&#25429;&#33719;&#30340;&#20852;&#36259;&#22810;&#26679;&#24615;&#65292;2&#65289;&#20445;&#25345;&#23427;&#20204;&#26368;&#26032;&#38656;&#35201;&#26114;&#36149;&#30340;&#23454;&#26102;&#22522;&#30784;&#35774;&#26045;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#38469;&#24037;&#19994;&#29615;&#22659;&#20013;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21160;&#24577;&#26356;&#26032;&#23458;&#25143;&#37197;&#32622;&#25991;&#20214;&#65292;&#24182;&#27599;&#20004;&#20998;&#38047;&#32452;&#25104;&#19968;&#20010;&#20449;&#24687;&#27969;&#65292;&#21033;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#21450;&#20854;&#21508;&#33258;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22312;&#33655;&#20848;&#21644;&#27604;&#21033;&#26102;&#26368;&#22823;&#30340;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20043;&#19968;Bol&#19978;&#27979;&#35797;&#24182;&#37096;&#32626;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#23458;&#25143;&#21442;&#19982;&#24230;&#21644;&#20307;&#39564;&#65292;&#23548;&#33268;&#36716;&#21270;&#29575;&#26174;&#33879;&#25552;&#39640;&#20102;4.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16073v1 Announce Type: cross  Abstract: In personalized recommender systems, embeddings are often used to encode customer actions and items, and retrieval is then performed in the embedding space using approximate nearest neighbor search. However, this approach can lead to two challenges: 1) user embeddings can restrict the diversity of interests captured and 2) the need to keep them up-to-date requires an expensive, real-time infrastructure. In this paper, we propose a method that overcomes these challenges in a practical, industrial setting. The method dynamically updates customer profiles and composes a feed every two minutes, employing precomputed embeddings and their respective similarities. We tested and deployed this method to personalise promotional items at Bol, one of the largest e-commerce platforms of the Netherlands and Belgium. The method enhanced customer engagement and experience, leading to a significant 4.9% uplift in conversions.
&lt;/p&gt;</description></item><item><title>Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.10251</link><description>&lt;p&gt;
Brant-2&#65306;&#33041;&#20449;&#21495;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Brant-2: Foundation Model for Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10251
&lt;/p&gt;
&lt;p&gt;
Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#21463;&#30410;&#20110;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#31181;&#27169;&#22411;&#22312;&#20998;&#26512;&#33041;&#20449;&#21495;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#65292;&#22240;&#20026;&#36825;&#19968;&#39046;&#22495;&#28085;&#30422;&#20102;&#20247;&#22810;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#19988;&#36827;&#34892;&#22823;&#35268;&#27169;&#27880;&#37322;&#26159;&#25104;&#26412;&#39640;&#26114;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;Brant-2&#12290;&#19982;&#29992;&#20110;&#39045;&#20869;&#31070;&#32463;&#20449;&#21495;&#30340;&#22522;&#30784;&#27169;&#22411;Brant&#30456;&#27604;&#65292;Brant-2&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#22823;&#37327;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Brant-2&#23545;&#33041;&#20449;&#21495;&#20013;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#30340;&#36866;&#24212;&#24615;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#25581;&#31034;&#20102;Brant-2&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#39564;&#35777;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#20445;&#25345;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10251v1 Announce Type: cross  Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintai
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#20998;&#26512;&#20195;&#29702;&#20154;&#20559;&#22909;&#20998;&#24067;&#30340;&#24179;&#22343;&#24773;&#20917;&#65292;&#25193;&#23637;&#20102;&#36845;&#20195;&#25237;&#31080;&#27169;&#22411;&#30340;&#25928;&#26524;&#20998;&#26512;&#12290;&#24182;&#19988;&#21306;&#20998;&#20102;&#36845;&#20195;&#22810;&#25968;&#21046;&#20309;&#26102;&#25913;&#21892;&#25110;&#38477;&#20302;&#28176;&#36817;&#31119;&#21033;&#12290;</title><link>https://arxiv.org/abs/2402.08144</link><description>&lt;p&gt;
&#36845;&#20195;&#25237;&#31080;&#30340;&#24179;&#22343;&#24773;&#20917;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Average-Case Analysis of Iterative Voting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08144
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#20998;&#26512;&#20195;&#29702;&#20154;&#20559;&#22909;&#20998;&#24067;&#30340;&#24179;&#22343;&#24773;&#20917;&#65292;&#25193;&#23637;&#20102;&#36845;&#20195;&#25237;&#31080;&#27169;&#22411;&#30340;&#25928;&#26524;&#20998;&#26512;&#12290;&#24182;&#19988;&#21306;&#20998;&#20102;&#36845;&#20195;&#22810;&#25968;&#21046;&#20309;&#26102;&#25913;&#21892;&#25110;&#38477;&#20302;&#28176;&#36817;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#25237;&#31080;&#26159;&#31038;&#20250;&#36873;&#25321;&#20013;&#37325;&#22797;&#25112;&#30053;&#20915;&#31574;&#30340;&#33258;&#28982;&#27169;&#22411;&#65292;&#24403;&#20195;&#29702;&#21487;&#20197;&#22312;&#26368;&#32456;&#30830;&#23450;&#32676;&#20307;&#20915;&#31574;&#20043;&#21069;&#26356;&#26032;&#20182;&#20204;&#30340;&#25237;&#31080;&#26102;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#23545;&#26080;&#24207;&#25991;&#21270;&#19979;&#20195;&#29702;&#20154;&#20559;&#22909;&#30340;&#26368;&#22351;&#24773;&#20917;&#21644;&#24179;&#22343;&#24773;&#20917;&#34920;&#29616;&#36827;&#34892;&#20998;&#26512;&#65292;&#36890;&#36807;&#25913;&#36827;&#23433;&#32435;&#22522;&#20215;&#26684;&#26469;&#20998;&#26512;&#36845;&#20195;&#22810;&#25968;&#21046;&#23545;&#24179;&#34913;&#28857;&#36873;&#20986;&#30340;&#32467;&#26524;&#31119;&#21033;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#20998;&#26512;&#21482;&#30740;&#31350;&#20102;&#22312;&#20195;&#29702;&#20154;&#20559;&#22909;&#36890;&#36807;&#26080;&#20559;&#25991;&#21270;&#20998;&#24067;&#30340;&#26368;&#22351;&#24773;&#20917;&#21644;&#24179;&#22343;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#23558;&#24179;&#22343;&#24773;&#20917;&#20998;&#26512;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#20998;&#24067;&#31867;&#65292;&#24182;&#21306;&#20998;&#20986;&#36845;&#20195;&#22810;&#25968;&#21046;&#20309;&#26102;&#25913;&#21892;&#25110;&#38477;&#20302;&#28176;&#36817;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Iterative voting is a natural model of repeated strategic decision-making in social choice when agents have the opportunity to update their votes prior to finalizing the group decision. Prior work has analyzed the efficacy of iterative plurality on the welfare of the chosen outcome at equilibrium, relative to the truthful vote profile, via an adaptation of the price of anarchy. However, prior analyses have only studied the worst-case and average-case performances when agents' preferences are distributed by the impartial culture. This work extends average-case analyses to a wider class of distributions and distinguishes when iterative plurality improves or degrades asymptotic welfare.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#21322;&#32447;&#24615;&#27874;&#21160;&#26041;&#31243;&#30340;&#20005;&#26684;&#35823;&#24046;&#30028;&#38480;&#65292;&#21253;&#25324;&#27867;&#21270;&#35823;&#24046;&#21644;&#35757;&#32451;&#35823;&#24046;&#30340;&#30028;&#38480;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#29702;&#35770;&#30028;&#38480;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07153</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#21322;&#32447;&#24615;&#27874;&#21160;&#26041;&#31243;&#30340;&#35823;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Error Estimation for Physics-informed Neural Networks Approximating Semilinear Wave Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#21322;&#32447;&#24615;&#27874;&#21160;&#26041;&#31243;&#30340;&#20005;&#26684;&#35823;&#24046;&#30028;&#38480;&#65292;&#21253;&#25324;&#27867;&#21270;&#35823;&#24046;&#21644;&#35757;&#32451;&#35823;&#24046;&#30340;&#30028;&#38480;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#29702;&#35770;&#30028;&#38480;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#21322;&#32447;&#24615;&#27874;&#21160;&#26041;&#31243;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;&#25105;&#20204;&#38024;&#23545;&#20855;&#26377;&#20004;&#20010;&#38544;&#34255;&#23618;&#30340;tanh&#31070;&#32463;&#32593;&#32476;&#65292;&#22522;&#20110;&#32593;&#32476;&#23618;&#23485;&#24230;&#21644;&#35757;&#32451;&#28857;&#25968;&#37327;&#65292;&#25552;&#20379;&#20102;&#23545;&#27867;&#21270;&#35823;&#24046;&#21644;&#35757;&#32451;&#35823;&#24046;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#65292;&#23558;&#24635;&#35823;&#24046;&#20197;$H^1([0,T];L^2(\Omega))$-&#33539;&#25968;&#30340;&#24418;&#24335;&#34920;&#31034;&#65292;&#24182;&#33021;&#22815;&#38543;&#30528;&#35757;&#32451;&#28857;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#20219;&#24847;&#20943;&#23567;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides rigorous error bounds for physics-informed neural networks approximating the semilinear wave equation. We provide bounds for the generalization and training error in terms of the width of the network's layers and the number of training points for a tanh neural network with two hidden layers. Our main result is a bound of the total error in the $H^1([0,T];L^2(\Omega))$-norm in terms of the training error and the number of training points, which can be made arbitrarily small under some assumptions. We illustrate our theoretical bounds with numerical experiments.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#20132;&#20114;&#22270;&#19978;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#39044;&#27979;&#38454;&#27573;&#25152;&#38754;&#20020;&#30340;&#26102;&#38388;&#24046;&#24322;&#21644;&#35821;&#20041;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06326</link><description>&lt;p&gt;
&#26102;&#38388;&#20132;&#20114;&#22270;&#19978;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt Learning on Temporal Interaction Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06326
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#20132;&#20114;&#22270;&#19978;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#39044;&#27979;&#38454;&#27573;&#25152;&#38754;&#20020;&#30340;&#26102;&#38388;&#24046;&#24322;&#21644;&#35821;&#20041;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20132;&#20114;&#22270;(TIGs)&#34987;&#24191;&#27867;&#29992;&#20110;&#34920;&#31034;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;TIGs&#19978;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;TIG&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#8220;&#39044;&#35757;&#32451;&#65292;&#39044;&#27979;&#8221;&#35757;&#32451;&#33539;&#24335;&#20013;&#20381;&#28982;&#38754;&#20020;&#30528;&#20004;&#20010;&#38590;&#39064;&#12290;&#39318;&#20808;&#65292;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#38388;&#24046;&#24322;&#20005;&#37325;&#21066;&#24369;&#20102;&#27169;&#22411;&#22312;&#21160;&#24577;&#28436;&#21270;&#25968;&#25454;&#19978;&#36827;&#34892;&#36965;&#36828;&#26410;&#26469;&#39044;&#27979;&#30340;&#36866;&#29992;&#24615;&#12290;&#20854;&#27425;&#65292;&#39044;&#25991;&#26412;&#20219;&#21153;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#24212;&#29992;&#22330;&#26223;&#20013;&#24456;&#38590;&#23545;&#40784;&#20854;&#23398;&#20064;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Interaction Graphs (TIGs) are widely utilized to represent real-world systems. To facilitate representation learning on TIGs, researchers have proposed a series of TIG models. However, these models are still facing two tough gaps between the pre-training and downstream predictions in their ``pre-train, predict'' training paradigm. First, the temporal discrepancy between the pre-training and inference data severely undermines the models' applicability in distant future predictions on the dynamically evolving data. Second, the semantic divergence between pretext and downstream tasks hinders their practical applications, as they struggle to align with their learning and prediction capabilities across application scenarios.   Recently, the ``pre-train, prompt'' paradigm has emerged as a lightweight mechanism for model generalization. Applying this paradigm is a potential solution to solve the aforementioned challenges. However, the adaptation of this paradigm to TIGs is not straig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)</title><link>https://arxiv.org/abs/2402.04154</link><description>&lt;p&gt;
&#12298;&#35835;&#29609;&#28216;&#25103;&#65288;R2-Play&#65289;: &#22810;&#27169;&#24577;&#28216;&#25103;&#25351;&#23548;&#19979;&#30340;&#20915;&#31574; Transformer&#12299;
&lt;/p&gt;
&lt;p&gt;
Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24320;&#21457;&#19968;&#27454;&#36890;&#29992;&#26234;&#33021;&#20307;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30446;&#26631;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#20219;&#21153;&#30340;&#22823;&#37327;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#25193;&#23637;&#21040;&#26032;&#20219;&#21153;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#25972;&#21512;&#21040;&#20915;&#31574;&#32593;&#32476;&#20013;&#65292;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#20165;&#20381;&#36182;&#20110;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#23545;&#20110;&#20934;&#30830;&#20256;&#36798;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#22686;&#24378;&#26234;&#33021;&#20307;&#20219;&#21153;&#25351;&#23548;&#30340;&#24418;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#65292;&#20174;&#32780;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#38271;&#26399;&#35270;&#35273;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;
Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
&lt;/p&gt;</description></item><item><title>&#24314;&#35758;&#23558;&#35748;&#30693;&#20219;&#21153;&#25972;&#21512;&#21040;&#22823;&#22411;&#27169;&#22411;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27979;&#35797;&#20013;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#27169;&#22411;&#30340;&#22810;&#32500;&#26234;&#33021;&#12290;&#36825;&#20010;&#26694;&#26550;&#32467;&#21512;&#20102;&#35748;&#30693;&#31185;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21253;&#21547;&#20102;&#31283;&#24577;&#26234;&#21147;&#12289;&#27969;&#24577;&#26234;&#21147;&#21644;&#31038;&#20132;&#26234;&#33021;&#31561;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.02547</link><description>&lt;p&gt;
&#23558;&#35748;&#30693;&#20219;&#21153;&#25972;&#21512;&#21040;&#22823;&#22411;&#27169;&#22411;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27979;&#35797;&#20013;
&lt;/p&gt;
&lt;p&gt;
Integration of cognitive tasks into artificial general intelligence test for large models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02547
&lt;/p&gt;
&lt;p&gt;
&#24314;&#35758;&#23558;&#35748;&#30693;&#20219;&#21153;&#25972;&#21512;&#21040;&#22823;&#22411;&#27169;&#22411;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27979;&#35797;&#20013;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#27169;&#22411;&#30340;&#22810;&#32500;&#26234;&#33021;&#12290;&#36825;&#20010;&#26694;&#26550;&#32467;&#21512;&#20102;&#35748;&#30693;&#31185;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21253;&#21547;&#20102;&#31283;&#24577;&#26234;&#21147;&#12289;&#27969;&#24577;&#26234;&#21147;&#21644;&#31038;&#20132;&#26234;&#33021;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#27169;&#22411;&#30340;&#21457;&#23637;&#36807;&#31243;&#20013;&#65292;&#24517;&#39035;&#23545;&#20013;&#38388;&#27169;&#22411;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#20197;&#35780;&#20272;&#20854;&#33021;&#21147;&#65292;&#24182;&#23545;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#24615;&#35780;&#20272;&#65292;&#20197;&#30830;&#20445;&#22312;&#23454;&#38469;&#24212;&#29992;&#20043;&#21069;&#30340;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#35780;&#20272;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#32570;&#20047;&#23545;&#22823;&#22411;&#27169;&#22411;&#30340;&#22810;&#32500;&#26234;&#33021;&#35780;&#20272;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#20013;&#65292;&#25105;&#20204;&#25552;&#20513;&#24314;&#31435;&#19968;&#20010;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27979;&#35797;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#28385;&#36275;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#22823;&#22411;&#27169;&#22411;&#30340;&#27979;&#35797;&#38656;&#27714;&#65292;&#20197;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#35813;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27979;&#35797;&#26694;&#26550;&#23558;&#35748;&#30693;&#31185;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32852;&#31995;&#36215;&#26469;&#65292;&#21253;&#25324;&#26234;&#21147;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#31283;&#24577;&#26234;&#21147;&#65292;&#21363;&#31215;&#32047;&#30340;&#30693;&#35782;&#21644;&#32463;&#39564;&#30340;&#21453;&#26144;; &#27969;&#24577;&#26234;&#21147;&#65292;&#29305;&#28857;&#26159;&#35299;&#20915;&#38382;&#39064;&#21644;&#36866;&#24212;&#24615;&#25512;&#29702;; &#31038;&#20132;&#26234;&#33021;&#65292;&#34920;&#31034;&#22312;&#22810;&#26041;&#38754;&#29702;&#35299;&#21644;&#36866;&#24212;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the evolution of large models, performance evaluation is necessarily performed on the intermediate models to assess their capabilities, and on the well-trained model to ensure safety before practical application. However, current model evaluations mainly rely on specific tasks and datasets, lacking a united framework for assessing the multidimensional intelligence of large models. In this perspective, we advocate for a comprehensive framework of artificial general intelligence (AGI) test, aimed at fulfilling the testing needs of large language models and multi-modal large models with enhanced capabilities. The AGI test framework bridges cognitive science and natural language processing to encompass the full spectrum of intelligence facets, including crystallized intelligence, a reflection of amassed knowledge and experience; fluid intelligence, characterized by problem-solving and adaptive reasoning; social intelligence, signifying comprehension and adaptation within multifacete
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#21307;&#30103;&#38382;&#31572;&#30340;&#36890;&#29992;&#21644;&#21307;&#23398;&#29305;&#23450;&#30340;&#31934;&#28860;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#20197;&#22635;&#34917;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#20013;&#36825;&#20123;&#27169;&#22411;&#24615;&#33021;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2401.11389</link><description>&lt;p&gt;
MedLM: &#25506;&#32034;&#29992;&#20110;&#21307;&#30103;&#38382;&#31572;&#31995;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MedLM: Exploring Language Models for Medical Question Answering Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#21307;&#30103;&#38382;&#31572;&#30340;&#36890;&#29992;&#21644;&#21307;&#23398;&#29305;&#23450;&#30340;&#31934;&#28860;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#20197;&#22635;&#34917;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#20013;&#36825;&#20123;&#27169;&#22411;&#24615;&#33021;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#36805;&#36895;&#25193;&#22823;&#30340;&#22312;&#32447;&#21307;&#23398;&#25991;&#29486;&#65292;&#33258;&#21160;&#21270;&#31995;&#32479;&#29992;&#20110;&#32858;&#21512;&#21644;&#24635;&#32467;&#20449;&#24687;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#21464;&#24471;&#26085;&#30410;&#20851;&#38190;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#20808;&#36827;&#30340;&#29983;&#25104;&#33021;&#21147;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#23427;&#20204;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#23553;&#38381;&#24335;&#29983;&#25104;&#38382;&#31572;&#26041;&#38754;&#65292;&#26159;&#26174;&#33879;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21307;&#23398;&#38382;&#31572;&#31561;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#36739;&#23569;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#27604;&#36739;&#36890;&#29992;&#21644;&#19987;&#38376;&#29992;&#20110;&#21307;&#23398;&#30340;&#31934;&#28860;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#38382;&#31572;&#20013;&#30340;&#34920;&#29616;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#24494;&#35843;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#23558;&#25506;&#35752;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12289;&#27604;&#36739;&#24615;&#33021;&#21644;&#22312;&#21307;&#30103;&#38382;&#31572;&#32972;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11389v2 Announce Type: replace-cross  Abstract: In the face of rapidly expanding online medical literature, automated systems for aggregating and summarizing information are becoming increasingly crucial for healthcare professionals and patients. Large Language Models (LLMs), with their advanced generative capabilities, have shown promise in various NLP tasks, and their potential in the healthcare domain, particularly for Closed-Book Generative QnA, is significant. However, the performance of these models in domain-specific tasks such as medical Q&amp;A remains largely unexplored. This study aims to fill this gap by comparing the performance of general and medical-specific distilled LMs for medical Q&amp;A. We aim to evaluate the effectiveness of fine-tuning domain-specific LMs and compare the performance of different families of Language Models. The study will address critical questions about these models' reliability, comparative performance, and effectiveness in the context of me
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#20197;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#38754;&#20020;&#30340;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.09340</link><description>&lt;p&gt;
SceneVerse&#65306;&#20026;&#22522;&#20110;&#22330;&#26223;&#30340;&#22330;&#26223;&#29702;&#35299;&#25193;&#23637;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.09340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#20197;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#38754;&#20020;&#30340;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#65292;&#21363;&#23558;&#35821;&#35328;&#19982;3D&#29289;&#29702;&#29615;&#22659;&#23545;&#40784;&#65292;&#26159;&#21457;&#23637;&#20855;&#36523;&#20307;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#30340;&#22522;&#30707;&#12290;&#19982;2D&#39046;&#22495;&#26368;&#36817;&#30340;&#36827;&#23637;&#30456;&#27604;&#65292;&#23558;&#35821;&#35328;&#19982;3D&#22330;&#26223;&#23545;&#40784;&#38754;&#20020;&#30528;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#65288;i&#65289;3D&#22330;&#26223;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#30001;&#20110;&#22810;&#26679;&#30340;&#29289;&#20307;&#37197;&#32622;&#12289;&#20016;&#23500;&#30340;&#23646;&#24615;&#21644;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65307;&#65288;ii&#65289;&#25903;&#25345;&#22522;&#20110;&#22330;&#26223;&#23398;&#20064;&#30340;&#37197;&#23545;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#32570;&#20047;&#20174;&#22522;&#20110;&#22330;&#26223;&#30340;3D&#25968;&#25454;&#20013;&#25552;&#28860;&#30693;&#35782;&#30340;&#32479;&#19968;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#20174;&#32780;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#36825;&#19977;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#21253;&#21547;&#32422;68K&#20010;3D&#23460;&#20869;&#22330;&#26223;&#65292;&#21253;&#25324;250&#19975;&#20010;&#35270;&#35273;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.09340v2 Announce Type: replace-cross  Abstract: 3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20027;&#35201;&#31070;&#32463;&#20803;&#26469;&#21457;&#29616;&#27010;&#24565;&#30340;&#20998;&#24067;&#34920;&#31034;&#65292;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#26410;&#26631;&#35760;&#23376;&#31867;&#21644;&#26816;&#27979;&#35823;&#20998;&#31867;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2312.17285</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#19979;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#20998;&#24067;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Understanding Distributed Representations of Concepts in Deep Neural Networks without Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20027;&#35201;&#31070;&#32463;&#20803;&#26469;&#21457;&#29616;&#27010;&#24565;&#30340;&#20998;&#24067;&#34920;&#31034;&#65292;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#26410;&#26631;&#35760;&#23376;&#31867;&#21644;&#26816;&#27979;&#35823;&#20998;&#31867;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#23398;&#20064;&#30340;&#27010;&#24565;&#30340;&#20013;&#38388;&#34920;&#31034;&#23545;&#35299;&#37322;&#27169;&#22411;&#30340;&#19968;&#33324;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#25581;&#31034;&#23398;&#20064;&#27010;&#24565;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20154;&#31867;&#30417;&#30563;&#65292;&#20363;&#22914;&#39044;&#23450;&#20041;&#30340;&#27010;&#24565;&#38598;&#25110;&#20998;&#21106;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20027;&#35201;&#23376;&#38598;&#30340;&#31070;&#32463;&#20803;&#26469;&#21457;&#29616;&#27010;&#24565;&#30340;&#20998;&#24067;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#31867;&#20284;&#31070;&#32463;&#20803;&#28608;&#27963;&#29366;&#24577;&#30340;&#23454;&#20363;&#24448;&#24448;&#20849;&#20139;&#19968;&#33268;&#30340;&#27010;&#24565;&#12290;&#26681;&#25454;&#35266;&#23519;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36873;&#25321;&#26500;&#24314;&#21487;&#35299;&#37322;&#21306;&#22495;&#30340;&#20027;&#35201;&#31070;&#32463;&#20803;&#65292;&#21363;&#28085;&#30422;&#29305;&#24449;&#31354;&#38388;&#20013;&#20855;&#26377;&#19968;&#33268;&#27010;&#24565;&#30340;&#23454;&#20363;&#30340;&#25918;&#26494;&#20915;&#31574;&#21306;&#22495;&#65288;RDR&#65289;&#12290;&#23427;&#21487;&#29992;&#20110;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#26410;&#26631;&#35760;&#23376;&#31867;&#24182;&#26816;&#27979;&#35823;&#20998;&#31867;&#30340;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#24212;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17285v2 Announce Type: replace-cross  Abstract: Understanding intermediate representations of the concepts learned by deep learning classifiers is indispensable for interpreting general model behaviors. Existing approaches to reveal learned concepts often rely on human supervision, such as pre-defined concept sets or segmentation processes. In this paper, we propose a novel unsupervised method for discovering distributed representations of concepts by selecting a principal subset of neurons. Our empirical findings demonstrate that instances with similar neuron activation states tend to share coherent concepts. Based on the observations, the proposed method selects principal neurons that construct an interpretable region, namely a Relaxed Decision Region (RDR), encompassing instances with coherent concepts in the feature space. It can be utilized to identify unlabeled subclasses within data and to detect the causes of misclassifications. Furthermore, the applicability of our 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;BIPIA&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#27492;&#31867;&#25915;&#20987;&#26102;&#30340;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#25915;&#20987;&#25104;&#21151;&#30340;&#21407;&#22240;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.14197</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;BIPIA&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#27492;&#31867;&#25915;&#20987;&#26102;&#30340;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#25915;&#20987;&#25104;&#21151;&#30340;&#21407;&#22240;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#20869;&#23481;&#30340;&#25972;&#21512;&#24050;&#32463;&#23454;&#29616;&#20102;LLMs&#30340;&#26356;&#26032;&#21644;&#24191;&#27867;&#24212;&#29992;&#65292;&#27604;&#22914;&#24494;&#36719;Copilot&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25972;&#21512;&#20063;&#35753;LLMs&#38754;&#20020;&#20102;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#22806;&#37096;&#20869;&#23481;&#20013;&#23884;&#20837;&#24694;&#24847;&#25351;&#20196;&#65292;&#20174;&#32780;ompromising LLM&#36755;&#20986;&#24182;&#23548;&#33268;&#21709;&#24212;&#20559;&#31163;&#29992;&#25143;&#26399;&#26395;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#37325;&#35201;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;BIPIA&#65292;&#20197;&#35780;&#20272;&#36825;&#31867;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#22522;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#20998;&#26512;&#20102;&#35813;&#25915;&#20987;&#25104;&#21151;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#21363;LLMs&#26080;&#27861;&#21306;&#20998;&#25351;&#20196;&#21644;&#22806;&#37096;&#20869;&#23481;&#20197;&#21450;&#32570;&#20047;&#24847;&#35782;&#19981;&#25191;&#34892;&#22806;&#37096;&#20869;&#23481;&#20869;&#30340;&#25351;&#20196;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#40657;&#30418;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14197v2 Announce Type: replace-cross  Abstract: The integration of large language models (LLMs) with external content has enabled more up-to-date and wide-ranging applications of LLMs, such as Microsoft Copilot. However, this integration has also exposed LLMs to the risk of indirect prompt injection attacks, where an attacker can embed malicious instructions within external content, compromising LLM output and causing responses to deviate from user expectations. To investigate this important but underexplored issue, we introduce the first benchmark for indirect prompt injection attacks, named BIPIA, to evaluate the risk of such attacks. Based on the evaluation, our work makes a key analysis of the underlying reason for the success of the attack, namely the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content. Building upon this analysis, we develop two black-box metho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#36817;&#20284;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36817;&#20284;&#20540;&#36845;&#20195;&#31639;&#27861;&#20013;&#26679;&#26412;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.12869</link><description>&lt;p&gt;
&#21442;&#25968;&#21270;&#25237;&#24433;&#36125;&#23572;&#26364;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Parameterized Projected Bellman Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#36817;&#20284;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36817;&#20284;&#20540;&#36845;&#20195;&#31639;&#27861;&#20013;&#26679;&#26412;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#20540;&#36845;&#20195;&#65288;AVI&#65289;&#26159;&#19968;&#31867;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#31639;&#27861;&#23478;&#26063;&#65292;&#26088;&#22312;&#33719;&#24471;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#36817;&#20284;&#12290;&#36890;&#24120;&#65292;AVI&#31639;&#27861;&#37319;&#29992;&#36845;&#20195;&#36807;&#31243;&#65292;&#27599;&#20010;&#27493;&#39588;&#21253;&#25324;&#65288;i&#65289;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#21644;&#65288;ii&#65289;&#25237;&#24433;&#27493;&#39588;&#21040;&#32771;&#34385;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#36125;&#23572;&#26364;&#31639;&#23376;&#21033;&#29992;&#36716;&#31227;&#26679;&#26412;&#65292;&#36825;&#20123;&#26679;&#26412;&#24378;&#28872;&#24433;&#21709;&#20854;&#34892;&#20026;&#65292;&#22240;&#20026;&#26080;&#20449;&#24687;&#30340;&#26679;&#26412;&#21487;&#33021;&#23548;&#33268;&#21487;&#24573;&#30053;&#30340;&#26356;&#26032;&#25110;&#38271;&#26102;&#38388;&#30340;&#32469;&#34892;&#65292;&#32780;&#35745;&#31639;&#23494;&#38598;&#30340;&#25237;&#24433;&#27493;&#39588;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#23398;&#20064;&#30340;&#26041;&#24335;&#24471;&#21040;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#36817;&#20284;&#29256;&#26412;&#65292;&#32780;&#19981;&#26159;&#20687;AVI&#26041;&#27861;&#37027;&#26679;&#36890;&#36807;&#26679;&#26412;&#36827;&#34892;&#20272;&#35745;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#33021;&#22815;&#65288;i&#65289;&#22312;&#36716;&#31227;&#26679;&#26412;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#65292;&#65288;ii&#65289;&#36991;&#20813;&#35745;&#31639;&#23494;&#38598;&#30340;&#25237;&#24433;&#27493;&#39588;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31216;&#25105;&#20204;&#30340;&#26032;&#31639;&#23376;&#20026;"projec"&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate value iteration (AVI) is a family of algorithms for reinforcement learning (RL) that aims to obtain an approximation of the optimal value function. Generally, AVI algorithms implement an iterated procedure where each step consists of (i) an application of the Bellman operator and (ii) a projection step into a considered function space. Notoriously, the Bellman operator leverages transition samples, which strongly determine its behavior, as uninformative samples can result in negligible updates or long detours, whose detrimental effects are further exacerbated by the computationally intensive projection step. To address these issues, we propose a novel alternative approach based on learning an approximate version of the Bellman operator rather than estimating it through samples as in AVI approaches. This way, we are able to (i) generalize across transition samples and (ii) avoid the computationally intensive projection step. For this reason, we call our novel operator projec
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#26032;&#22411;&#22270;&#20687;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#20840;&#23616;&#27744;&#21270;&#25805;&#20316;&#65292;&#20445;&#30041;&#20102;&#26356;&#22810;&#22270;&#20687;&#30340;&#21028;&#21035;&#29305;&#24449;&#21644;&#32454;&#33410;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.07932</link><description>&lt;p&gt;
&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#26032;&#22411;&#22270;&#20687;&#20998;&#31867;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Novel Image Classification Framework Based on Variational Quantum Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07932
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#26032;&#22411;&#22270;&#20687;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#20840;&#23616;&#27744;&#21270;&#25805;&#20316;&#65292;&#20445;&#30041;&#20102;&#26356;&#22810;&#22270;&#20687;&#30340;&#21028;&#21035;&#29305;&#24449;&#21644;&#32454;&#33410;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#29616;&#26377;&#30340;&#22270;&#20687;&#20998;&#31867;&#20256;&#32479;&#26694;&#26550;&#36890;&#24120;&#22312;&#32593;&#32476;&#26411;&#31471;&#20351;&#29992;&#20840;&#23616;&#27744;&#21270;&#25805;&#20316;&#26469;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#24182;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#35813;&#25805;&#20316;&#36890;&#24120;&#23548;&#33268;&#20449;&#24687;&#20005;&#37325;&#20002;&#22833;&#65292;&#24433;&#21709;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#22270;&#20687;&#20998;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQAs&#65289;- &#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#32467;&#21512;&#20102;&#37327;&#23376;&#21644;&#32463;&#20856;&#35745;&#31639;&#33539;&#24335;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#28040;&#38500;&#20102;&#32593;&#32476;&#26411;&#31471;&#30340;&#20840;&#23616;&#27744;&#21270;&#25805;&#20316;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20445;&#30041;&#20102;&#22270;&#20687;&#20013;&#26356;&#22810;&#30340;&#21028;&#21035;&#29305;&#24449;&#21644;&#32454;&#31890;&#24230;&#32454;&#33410;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07932v2 Announce Type: replace-cross  Abstract: Image classification is a crucial task in machine learning with widespread practical applications. The existing classical framework for image classification typically utilizes a global pooling operation at the end of the network to reduce computational complexity and mitigate overfitting. However, this operation often results in a significant loss of information, which can affect the performance of classification models. To overcome this limitation, we introduce a novel image classification framework that leverages variational quantum algorithms (VQAs)-hybrid approaches combining quantum and classical computing paradigms within quantum machine learning. The major advantage of our framework is the elimination of the need for the global pooling operation at the end of the network. In this way, our approach preserves more discriminative features and fine-grained details in the images, which enhances classification performance. Add
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#21487;&#31359;&#25140;&#35774;&#22791;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#35757;&#32451;&#20102;&#22522;&#30784;&#27169;&#22411;&#29992;&#20110;&#34913;&#37327;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#65288;PPG&#65289;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#25968;&#25454;&#38598;&#35268;&#27169;&#36739;&#23567;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.05409</link><description>&lt;p&gt;
&#21487;&#31359;&#25140;&#29983;&#29289;&#20449;&#21495;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Large-scale Training of Foundation Models for Wearable Biosignals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05409
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#21487;&#31359;&#25140;&#35774;&#22791;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#35757;&#32451;&#20102;&#22522;&#30784;&#27169;&#22411;&#29992;&#20110;&#34913;&#37327;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#65288;PPG&#65289;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#25968;&#25454;&#38598;&#35268;&#27169;&#36739;&#23567;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36319;&#36394;&#29983;&#29289;&#20449;&#21495;&#23545;&#20110;&#30417;&#27979;&#20581;&#24247;&#29366;&#20917;&#24182;&#39044;&#38450;&#20005;&#37325;&#21307;&#23398;&#29366;&#20917;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#22914;&#20170;&#65292;&#21487;&#31359;&#25140;&#35774;&#22791;&#21487;&#20197;&#26041;&#20415;&#22320;&#35760;&#24405;&#21508;&#31181;&#29983;&#29289;&#20449;&#21495;&#65292;&#20174;&#32780;&#26377;&#26426;&#20250;&#22312;&#19981;&#24178;&#25200;&#26085;&#24120;&#29983;&#27963;&#30340;&#24773;&#20917;&#19979;&#30417;&#27979;&#20581;&#24247;&#29366;&#20917;&#12290;&#34429;&#28982;&#21487;&#31359;&#25140;&#35774;&#22791;&#34987;&#24191;&#27867;&#20351;&#29992;&#19988;&#23384;&#22312;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#20294;&#32570;&#20047;&#24102;&#26377;&#27880;&#37322;&#21307;&#23398;&#26631;&#31614;&#30340;&#31579;&#36873;&#25968;&#25454;&#65292;&#38459;&#30861;&#20102;&#24320;&#21457;&#34913;&#37327;&#24120;&#35265;&#20581;&#24247;&#29366;&#20917;&#30340;&#26032;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#20107;&#23454;&#19978;&#65292;&#19982;&#20854;&#20182;&#39046;&#22495;&#30456;&#27604;&#65292;&#21307;&#23398;&#25968;&#25454;&#38598;&#36890;&#24120;&#36739;&#23567;&#65292;&#36825;&#26159;&#24320;&#21457;&#29983;&#29289;&#20449;&#21495;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#38556;&#30861;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#30693;&#24773;&#21516;&#24847;&#19979;&#20174;&#22823;&#35268;&#27169;&#32437;&#21521;Apple&#24515;&#33039;&#21644;&#36816;&#21160;&#30740;&#31350;&#65288;AHMS&#65289;&#20013;&#25910;&#38598;&#30340;&#26410;&#26631;&#35760;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20026;&#20004;&#31181;&#24120;&#35265;&#29983;&#29289;&#20449;&#21495;&#65288;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#65288;PPG&#65289;&#21644;&#24515;&#30005;&#22270;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05409v2 Announce Type: replace-cross  Abstract: Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite widespread use of wearable devices and existing digital biomarkers, the absence of curated data with annotated medical labels hinders the development of new biomarkers to measure common health conditions. In fact, medical datasets are usually small in comparison to other domains, which is an obstacle for developing neural network models for biosignals. To address this challenge, we have employed self-supervised learning using the unlabeled sensor data collected under informed consent from the large longitudinal Apple Heart and Movement Study (AHMS) to train foundation models for two common biosignals: photoplethysmography (PPG) and electrocardiogra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Fair Mapping&#25511;&#21046;&#27169;&#22411;&#25552;&#31034;&#26469;&#20462;&#25913;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20844;&#24179;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#33021;&#22815;&#29983;&#25104;&#30456;&#23545;&#24179;&#34913;&#20154;&#21475;&#32479;&#35745;&#32467;&#26524;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2311.17695</link><description>&lt;p&gt;
&#36890;&#36807;&#20844;&#24179;&#26144;&#23556;&#23454;&#29616;&#20844;&#24179;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Fair Text-to-Image Diffusion via Fair Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Fair Mapping&#25511;&#21046;&#27169;&#22411;&#25552;&#31034;&#26469;&#20462;&#25913;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20844;&#24179;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#33021;&#22815;&#29983;&#25104;&#30456;&#23545;&#24179;&#34913;&#20154;&#21475;&#32479;&#35745;&#32467;&#26524;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#19982;&#20154;&#31867;&#30456;&#20851;&#25551;&#36848;&#26102;&#20986;&#29616;&#20154;&#21475;&#32479;&#35745;&#19978;&#20844;&#24179;&#32467;&#26524;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#32463;&#24120;&#38590;&#20197;&#23558;&#30446;&#26631;&#35821;&#35328;&#29615;&#22659;&#19982;&#31038;&#20250;&#25991;&#21270;&#20559;&#35265;&#20998;&#31163;&#24320;&#65292;&#23548;&#33268;&#29983;&#25104;&#20559;&#35265;&#22270;&#20687;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#19988;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;Fair Mapping&#65292;&#36890;&#36807;&#25511;&#21046;&#25552;&#31034;&#26469;&#20462;&#25913;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#20844;&#24179;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#20854;&#39640;&#25928;&#24615;&#12290;&#23427;&#21482;&#38656;&#35201;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#26356;&#26032;&#23569;&#37327;&#21442;&#25968;&#30340;&#39069;&#22806;&#32447;&#24615;&#32593;&#32476;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#23558;&#26465;&#20214;&#23884;&#20837;&#26144;&#23556;&#21040;&#21435;&#20559;&#31354;&#38388;&#30340;&#32447;&#24615;&#32593;&#32476;&#65292;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#25351;&#23450;&#30340;&#25991;&#26412;&#26465;&#20214;&#29983;&#25104;&#30456;&#23545;&#24179;&#34913;&#30340;&#20154;&#21475;&#32479;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17695v2 Announce Type: replace-cross  Abstract: In this paper, we address the limitations of existing text-to-image diffusion models in generating demographically fair results when given human-related descriptions. These models often struggle to disentangle the target language context from sociocultural biases, resulting in biased image generation. To overcome this challenge, we propose Fair Mapping, a flexible, model-agnostic, and lightweight approach that modifies a pre-trained text-to-image diffusion model by controlling the prompt to achieve fair image generation. One key advantage of our approach is its high efficiency. It only requires updating an additional linear network with few parameters at a low computational cost. By developing a linear network that maps conditioning embeddings into a debiased space, we enable the generation of relatively balanced demographic results based on the specified text condition. With comprehensive experiments on face image generation, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#26102;&#31354;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#21435;&#28151;&#28102;&#26041;&#27861;&#24182;&#25552;&#20986;&#20102;&#21517;&#20026;DCA&#30340;&#29702;&#35770;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2311.12472</link><description>&lt;p&gt;
&#38024;&#23545;&#26102;&#31354;&#20559;&#31227;&#30340;&#33258;&#30417;&#30563;&#21435;&#28151;&#28102;&#65306;&#29702;&#35770;&#19982;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Deconfounding Against Spatio-Temporal Shifts: Theory and Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#26102;&#31354;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#21435;&#28151;&#28102;&#26041;&#27861;&#24182;&#25552;&#20986;&#20102;&#21517;&#20026;DCA&#30340;&#29702;&#35770;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26102;&#31354;&#65288;ST&#65289;&#25968;&#25454;&#30340;&#37325;&#35201;&#24212;&#29992;&#65292;ST&#20132;&#36890;&#39044;&#27979;&#22312;&#25552;&#39640;&#22478;&#24066;&#20986;&#34892;&#25928;&#29575;&#21644;&#20419;&#36827;&#21487;&#25345;&#32493;&#21457;&#23637;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#26500;&#24314;&#36807;&#21435;&#20132;&#36890;&#25968;&#25454;&#12289;&#26410;&#26469;&#20132;&#36890;&#25968;&#25454;&#21644;&#22806;&#37096;ST&#19978;&#19979;&#25991;&#30340;&#22240;&#26524;&#22270;&#65292;&#31995;&#32479;&#22320;&#38416;&#26126;&#20102;&#36807;&#21435;&#33402;&#26415;&#20316;&#21697;&#22312;OOD&#20132;&#36890;&#25968;&#25454;&#19978;&#30340;&#22833;&#36133;&#26159;&#30001;&#20110;ST&#19978;&#19979;&#25991;&#20805;&#24403;&#20102;&#28151;&#28102;&#22240;&#32032;&#65292;&#21363;&#36807;&#21435;&#25968;&#25454;&#21644;&#26410;&#26469;&#25968;&#25454;&#30340;&#20849;&#21516;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026;Disentangled Contextual Adjustment&#65288;DCA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12472v2 Announce Type: replace  Abstract: As an important application of spatio-temporal (ST) data, ST traffic forecasting plays a crucial role in improving urban travel efficiency and promoting sustainable development. In practice, the dynamics of traffic data frequently undergo distributional shifts attributed to external factors such as time evolution and spatial differences. This entails forecasting models to handle the out-of-distribution (OOD) issue where test data is distributed differently from training data. In this work, we first formalize the problem by constructing a causal graph of past traffic data, future traffic data, and external ST contexts. We reveal that the failure of prior arts in OOD traffic data is due to ST contexts acting as a confounder, i.e., the common cause for past data and future ones. Then, we propose a theoretical solution named Disentangled Contextual Adjustment (DCA) from a causal lens. It differentiates invariant causal correlations again
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36830;&#32493;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;KuramotoGNN&#65292;&#36890;&#36807;&#37319;&#29992;Kuramoto&#27169;&#22411;&#26469;&#20943;&#36731;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#29616;&#35937;&#65292;&#23454;&#29616;&#33410;&#28857;&#29305;&#24449;&#30340;&#24046;&#24322;&#21270;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#32447;GNN&#21644;&#29616;&#26377;&#26041;&#27861;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.03260</link><description>&lt;p&gt;
&#20174;&#32806;&#21512;&#25391;&#33633;&#22120;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#22522;&#20110;Kuramoto&#27169;&#22411;&#30340;&#26041;&#27861;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
From Coupled Oscillators to Graph Neural Networks: Reducing Over-smoothing via a Kuramoto Model-based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36830;&#32493;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;KuramotoGNN&#65292;&#36890;&#36807;&#37319;&#29992;Kuramoto&#27169;&#22411;&#26469;&#20943;&#36731;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#29616;&#35937;&#65292;&#23454;&#29616;&#33410;&#28857;&#29305;&#24449;&#30340;&#24046;&#24322;&#21270;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#32447;GNN&#21644;&#29616;&#26377;&#26041;&#27861;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Kuramoto&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;KuramotoGNN&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36830;&#32493;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#23427;&#37319;&#29992;Kuramoto&#27169;&#22411;&#26469;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#29616;&#35937;&#65292;&#21363;&#38543;&#30528;&#23618;&#25968;&#22686;&#21152;&#65292;GNN&#20013;&#33410;&#28857;&#29305;&#24449;&#21464;&#24471;&#38590;&#20197;&#21306;&#20998;&#30340;&#38382;&#39064;&#12290;Kuramoto&#27169;&#22411;&#25429;&#25417;&#20102;&#38750;&#32447;&#24615;&#32806;&#21512;&#25391;&#33633;&#22120;&#30340;&#21516;&#27493;&#34892;&#20026;&#12290;&#20174;&#32806;&#21512;&#25391;&#33633;&#22120;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;Kuramoto&#27169;&#22411;&#19982;&#22522;&#26412;GNN&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#28982;&#21518;&#35828;&#26126;&#20102;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#29616;&#35937;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;Kuramoto&#27169;&#22411;&#20013;&#30340;&#30456;&#20301;&#21516;&#27493;&#12290;KuramotoGNN&#29992;&#39057;&#29575;&#21516;&#27493;&#21462;&#20195;&#20102;&#36825;&#31181;&#30456;&#20301;&#21516;&#27493;&#65292;&#20197;&#38450;&#27490;&#33410;&#28857;&#29305;&#24449;&#25910;&#25947;&#21040;&#19968;&#36215;&#65292;&#21516;&#26102;&#20351;&#31995;&#32479;&#36798;&#21040;&#31283;&#23450;&#30340;&#21516;&#27493;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;KuramotoGNN&#22312;&#20943;&#23569;&#36807;&#24230;&#24179;&#28369;&#26041;&#38754;&#30456;&#23545;&#20110;&#22522;&#32447;GNN&#21644;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03260v2 Announce Type: replace-cross  Abstract: We propose the Kuramoto Graph Neural Network (KuramotoGNN), a novel class of continuous-depth graph neural networks (GNNs) that employs the Kuramoto model to mitigate the over-smoothing phenomenon, in which node features in GNNs become indistinguishable as the number of layers increases. The Kuramoto model captures the synchronization behavior of non-linear coupled oscillators. Under the view of coupled oscillators, we first show the connection between Kuramoto model and basic GNN and then over-smoothing phenomenon in GNNs can be interpreted as phase synchronization in Kuramoto model. The KuramotoGNN replaces this phase synchronization with frequency synchronization to prevent the node features from converging into each other while allowing the system to reach a stable synchronized state. We experimentally verify the advantages of the KuramotoGNN over the baseline GNNs and existing methods in reducing over-smoothing on various 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827; Latent Diffusion Model &#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861; ACE&#65292;&#20854;&#36890;&#36807;&#32479;&#19968;&#27169;&#24335;&#30340;&#39069;&#22806;&#35823;&#24046;&#26469;&#20419;&#20351;&#27169;&#22411;&#23398;&#20064;&#29305;&#23450;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#32988;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2310.04687</link><description>&lt;p&gt;
&#25913;&#36827;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Attacks on Latent Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827; Latent Diffusion Model &#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861; ACE&#65292;&#20854;&#36890;&#36807;&#32479;&#19968;&#27169;&#24335;&#30340;&#39069;&#22806;&#35823;&#24046;&#26469;&#20419;&#20351;&#27169;&#22411;&#23398;&#20064;&#29305;&#23450;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#32988;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545; Latent Diffusion Model (LDM)&#65292;&#36825;&#31181;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#38450;&#27490; LDM &#22312;&#26410;&#32463;&#25480;&#26435;&#30340;&#22270;&#20687;&#19978;&#36827;&#34892;&#24694;&#24847;&#24494;&#35843;&#30340;&#20445;&#25252;&#25163;&#27573;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#25915;&#20987;&#20250;&#23545; LDM &#39044;&#27979;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#35780;&#20998;&#20989;&#25968;&#28155;&#21152;&#39069;&#22806;&#30340;&#35823;&#24046;&#12290;&#22312;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340; LDM &#23398;&#20064;&#36890;&#36807;&#19968;&#20010;&#20559;&#24046;&#38477;&#20302;&#35823;&#24046;&#65292;&#20174;&#32780;&#36973;&#21463;&#25915;&#20987;&#24182;&#20351;&#29992;&#20559;&#24046;&#39044;&#27979;&#35780;&#20998;&#20989;&#25968;&#12290;&#22522;&#20110;&#36825;&#19968;&#21160;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#19968;&#33268;&#24471;&#20998;&#20989;&#25968;&#38169;&#35823;&#36827;&#34892;&#25915;&#20987;&#65288;ACE&#65289;&#26469;&#25913;&#36827; LDM &#30340;&#23545;&#25239;&#25915;&#20987;&#12290;ACE &#32479;&#19968;&#20102;&#28155;&#21152;&#21040;&#39044;&#27979;&#24471;&#20998;&#20989;&#25968;&#30340;&#39069;&#22806;&#35823;&#24046;&#30340;&#27169;&#24335;&#12290;&#36825;&#20419;&#20351;&#24494;&#35843;&#30340; LDM &#23398;&#20064;&#19982;&#23545;&#35780;&#20998;&#20989;&#25968;&#36827;&#34892;&#39044;&#27979;&#30340;&#20559;&#24046;&#23398;&#20064;&#30456;&#21516;&#30340;&#27169;&#24335;&#12290;&#28982;&#21518;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#24335;&#26469;&#25913;&#36827;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545; LDM &#30340;&#23545;&#25239;&#25915;&#20987;&#20013;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04687v3 Announce Type: replace-cross  Abstract: Adversarial attacks on Latent Diffusion Model (LDM), the state-of-the-art image generative model, have been adopted as effective protection against malicious finetuning of LDM on unauthorized images. We show that these attacks add an extra error to the score function of adversarial examples predicted by LDM. LDM finetuned on these adversarial examples learns to lower the error by a bias, from which the model is attacked and predicts the score function with biases.   Based on the dynamics, we propose to improve the adversarial attack on LDM by Attacking with Consistent score-function Errors (ACE). ACE unifies the pattern of the extra error added to the predicted score function. This induces the finetuned LDM to learn the same pattern as a bias in predicting the score function. We then introduce a well-crafted pattern to improve the attack. Our method outperforms state-of-the-art methods in adversarial attacks on LDM.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#23545;HUNT4&#21475;&#33108;&#20581;&#24247;&#30740;&#31350;&#20013;&#20840;&#26223;X&#20809;&#22270;&#20687;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#40831;&#40840;&#40843;&#40831;&#26816;&#27979;&#30340;&#28508;&#21147;</title><link>https://arxiv.org/abs/2310.00354</link><description>&lt;p&gt;
AI-Dentify: &#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#36817;&#37051;&#29273;&#40840;&#40843;&#40831;&#22312;&#20840;&#26223;X&#20809;&#19978;&#30340;&#26816;&#27979;--HUNT4&#21475;&#33108;&#20581;&#24247;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AI-Dentify: Deep learning for proximal caries detection on bitewing x-ray -- HUNT4 Oral Health Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00354
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#23545;HUNT4&#21475;&#33108;&#20581;&#24247;&#30740;&#31350;&#20013;&#20840;&#26223;X&#20809;&#22270;&#20687;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#40831;&#40840;&#40843;&#40831;&#26816;&#27979;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#29273;&#40831;&#40843;&#22351;&#30340;&#35786;&#26029;&#38656;&#35201;&#23545;&#24739;&#32773;&#30340;&#35786;&#26029;&#24615;&#20840;&#26223;X&#20809;&#22270;&#20687;&#36827;&#34892;&#25163;&#21160;&#26816;&#26597;&#65292;&#28982;&#21518;&#36890;&#36807;&#35270;&#35273;&#26816;&#26597;&#21644;&#35302;&#35786;&#35782;&#21035;&#20855;&#26377;&#28508;&#22312;&#30149;&#21464;&#30340;&#29273;&#40831;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#65292;&#26377;&#26395;&#36890;&#36807;&#25552;&#20379;&#24555;&#36895;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20840;&#26223;X&#20809;&#22270;&#20687;&#20998;&#26512;&#26469;&#24110;&#21161;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00354v2 Announce Type: replace-cross  Abstract: Background: Dental caries diagnosis requires the manual inspection of diagnostic bitewing images of the patient, followed by a visual inspection and probing of the identified dental pieces with potential lesions. Yet the use of artificial intelligence, and in particular deep-learning, has the potential to aid in the diagnosis by providing a quick and informative analysis of the bitewing images.   Methods: A dataset of 13,887 bitewings from the HUNT4 Oral Health Study were annotated individually by six different experts, and used to train three different object detection deep-learning architectures: RetinaNet (ResNet50), YOLOv5 (M size), and EfficientDet (D0 and D1 sizes). A consensus dataset of 197 images, annotated jointly by the same six dentist, was used for evaluation. A five-fold cross validation scheme was used to evaluate the performance of the AI models.   Results: he trained models show an increase in average precision
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#65288;CLIC&#65289;&#27010;&#24565;&#65292;&#20801;&#35768;&#37325;&#22797;&#21033;&#29992;&#24191;&#27867;&#22330;&#26223;&#26469;&#36845;&#20195;&#25913;&#36827;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2309.14209</link><description>&lt;p&gt;
&#20855;&#26377;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#30340;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Continual Driving Policy Optimization with Closed-Loop Individualized Curricula
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.14209
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#65288;CLIC&#65289;&#27010;&#24565;&#65292;&#20801;&#35768;&#37325;&#22797;&#21033;&#29992;&#24191;&#27867;&#22330;&#26223;&#26469;&#36845;&#20195;&#25913;&#36827;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#30340;&#23433;&#20840;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#22836;&#31561;&#20851;&#27880;&#28857;&#65292;&#26681;&#28304;&#20110;&#38271;&#23614;&#33258;&#28982;&#39550;&#39542;&#20998;&#24067;&#20013;&#32597;&#35265;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#32570;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#22522;&#20110;&#22330;&#26223;&#30340;&#33258;&#21160;&#39550;&#39542;&#30740;&#31350;&#65292;&#37325;&#28857;&#26159;&#29983;&#25104;&#39640;&#39118;&#38505;&#39550;&#39542;&#22330;&#26223;&#24182;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#23545;AV&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#20851;&#38190;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#37325;&#22797;&#21033;&#29992;&#36825;&#20123;&#24191;&#27867;&#22330;&#26223;&#26469;&#36845;&#20195;&#25913;&#36827;AV&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20174;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#30340;&#20854;&#20182;AV&#27169;&#22411;&#25910;&#38598;&#30340;&#24040;&#22823;&#22330;&#26223;&#24211;&#20013;&#28388;&#20986;&#21487;&#20256;&#36882;&#20449;&#24687;&#20197;&#25913;&#36827;&#24403;&#21069;AV&#20173;&#28982;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#65288;CLIC&#65289;&#29305;&#28857;&#30340;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#35299;&#20026;&#19968;&#32452;&#26631;&#20934;&#21270;&#30340;&#23376;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.14209v3 Announce Type: replace-cross  Abstract: The safety of autonomous vehicles (AV) has been a long-standing top concern, stemming from the absence of rare and safety-critical scenarios in the long-tail naturalistic driving distribution. To tackle this challenge, a surge of research in scenario-based autonomous driving has emerged, with a focus on generating high-risk driving scenarios and applying them to conduct safety-critical testing of AV models. However, limited work has been explored on the reuse of these extensive scenarios to iteratively improve AV models. Moreover, it remains intractable and challenging to filter through gigantic scenario libraries collected from other AV models with distinct behaviors, attempting to extract transferable information for current AV improvement. Therefore, we develop a continual driving policy optimization framework featuring Closed-Loop Individualized Curricula (CLIC), which we factorize into a set of standardized sub-modules for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31435;&#22330;&#26816;&#27979;&#26041;&#27861;&#20197;&#20943;&#23569;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#22495;&#20869;&#30417;&#30563;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2309.13734</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31435;&#22330;&#20998;&#31867;&#30340;&#25552;&#31034;&#21644;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31435;&#22330;&#26816;&#27979;&#26041;&#27861;&#20197;&#20943;&#23569;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#22495;&#20869;&#30417;&#30563;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#20998;&#31867;&#26159;&#19968;&#20010;&#38271;&#26399;&#20197;&#26469;&#30740;&#31350;&#37325;&#28857;&#39046;&#22495;&#65292;&#20174;&#31038;&#20250;&#31185;&#23398;&#21040;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36825;&#39033;&#20219;&#21153;&#28041;&#21450;&#39044;&#27979;&#20316;&#32773;&#23545;&#24863;&#20852;&#36259;&#20027;&#39064;&#30340;&#35266;&#28857;&#12290;&#24403;&#21069;&#30340;&#31435;&#22330;&#26816;&#27979;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#21477;&#23376;&#65292;&#28982;&#21518;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25163;&#21160;&#27880;&#37322;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#24037;&#20316;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#23427;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#27867;&#21270;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#21487;&#20197;&#20943;&#23569;&#29978;&#33267;&#28040;&#38500;&#25163;&#21160;&#27880;&#37322;&#38656;&#27714;&#30340;&#31435;&#22330;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;10&#20010;&#24320;&#28304;&#27169;&#22411;&#21644;7&#31181;&#25552;&#31034;&#26041;&#26696;&#65292;&#21457;&#29616;LLMs&#22312;&#19982;&#22495;&#20869;&#30417;&#30563;&#27169;&#22411;&#20855;&#31454;&#20105;&#21147;&#65292;&#20294;&#24615;&#33021;&#24182;&#19981;&#19968;&#23450;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;LLMs&#30340;&#24494;&#35843;&#65292;&#20294;&#21457;&#29616;&#24494;&#35843;&#36807;&#31243;&#19981;&#24517;&#28982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13734v2 Announce Type: replace-cross  Abstract: Stance classification, the task of predicting the viewpoint of an author on a subject of interest, has long been a focal point of research in domains ranging from social science to machine learning. Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model. However, this manual annotation process requires laborious annotation effort, and thus hampers its potential to generalize across different contexts. In this work, we investigate the use of Large Language Models (LLMs) as a stance detection methodology that can reduce or even eliminate the need for manual annotations. We investigate 10 open-source models and 7 prompting schemes, finding that LLMs are competitive with in-domain supervised models but are not necessarily consistent in their performance. We also fine-tuned the LLMs, but discovered that fine-tuning process does not necessarily l
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Tensor Homomorphic Compression (THC)&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21521;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#21152;&#36895;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#35757;&#32451;</title><link>https://arxiv.org/abs/2302.08545</link><description>&lt;p&gt;
THC&#65306;&#20351;&#29992;&#24352;&#37327;&#21516;&#24577;&#21387;&#32553;&#21152;&#36895;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.08545
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Tensor Homomorphic Compression (THC)&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21521;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#21152;&#36895;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#24517;&#35201;&#29992;&#20363;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#38543;&#30528;DNNs&#21644;&#25968;&#25454;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#65292;&#23427;&#20204;&#38656;&#35201;&#22312;&#36234;&#26469;&#36234;&#22823;&#30340;&#38598;&#32676;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290; &#20027;&#35201;&#29942;&#39048;&#26159;&#30001;&#24037;&#20316;&#32773;&#22312;&#27599;&#36718;&#22522;&#30784;&#19978;&#20132;&#25442;&#27169;&#22411;&#26356;&#26032;&#65288;&#21363;&#26799;&#24230;&#65289;&#20135;&#29983;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#29942;&#39048;&#24182;&#21152;&#36895;&#35757;&#32451;&#65292;&#19968;&#20010;&#24191;&#27867;&#37096;&#32626;&#30340;&#26041;&#27861;&#26159;&#21387;&#32553;&#12290; &#20294;&#26159;&#65292;&#20808;&#21069;&#30340;&#37096;&#32626;&#36890;&#24120;&#21482;&#26159;&#22312;&#27599;&#20010;&#26041;&#21521;&#19978;&#20351;&#29992;&#21333;&#26041;&#21521;&#26799;&#24230;&#21387;&#32553;&#26041;&#26696;&#26469;&#24212;&#29992;&#21452;&#21521;&#21387;&#32553;&#26041;&#26696;&#12290; &#36825;&#23548;&#33268;&#21442;&#25968;&#26381;&#21153;&#22120;&#19978;&#30340;&#26174;&#30528;&#35745;&#31639;&#24320;&#38144;&#21644;&#21387;&#32553;&#35823;&#24046;&#22686;&#21152;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#26356;&#38271;&#21644;&#20934;&#30830;&#24615;&#26356;&#20302;&#12290; &#25105;&#20204;&#20171;&#32461;&#20102;&#24352;&#37327;&#21516;&#24577;&#21387;&#32553;&#65288;THC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21521;&#21387;&#32553;&#26694;&#26550;&#65292;&#33021;&#22815;&#30452;&#25509;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.08545v2 Announce Type: replace-cross  Abstract: Deep neural networks (DNNs) are the de facto standard for essential use cases, such as image classification, computer vision, and natural language processing. As DNNs and datasets get larger, they require distributed training on increasingly larger clusters. A main bottleneck is the resulting communication overhead where workers exchange model updates (i.e., gradients) on a per-round basis. To address this bottleneck and accelerate training, a widely-deployed approach is compression. However, previous deployments often apply bi-directional compression schemes by simply using a uni-directional gradient compression scheme in each direction. This results in significant computational overheads at the parameter server and increased compression error, leading to longer training and lower accuracy. We introduce Tensor Homomorphic Compression (THC), a novel bi-directional compression framework that enables the direct aggregation of com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;ST-SSL&#65289;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#36741;&#21161;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#22686;&#24378;&#20132;&#36890;&#27169;&#24335;&#34920;&#31034;&#65292;&#26082;&#21453;&#26144;&#31354;&#38388;&#24322;&#36136;&#24615;&#21448;&#21453;&#26144;&#26102;&#38388;&#24322;&#36136;&#24615;&#12290;</title><link>https://arxiv.org/abs/2212.04475</link><description>&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Self-Supervised Learning for Traffic Flow Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.04475
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;ST-SSL&#65289;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#36741;&#21161;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#22686;&#24378;&#20132;&#36890;&#27169;&#24335;&#34920;&#31034;&#65292;&#26082;&#21453;&#26144;&#31354;&#38388;&#24322;&#36136;&#24615;&#21448;&#21453;&#26144;&#26102;&#38388;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#33539;&#22260;&#20869;&#22312;&#19981;&#21516;&#26102;&#38388;&#27573;&#23545;&#20132;&#36890;&#27969;&#37327;&#36827;&#34892;&#31283;&#20581;&#39044;&#27979;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#20026;&#24314;&#27169;&#26102;&#31354;&#30456;&#20851;&#24615;&#20570;&#20986;&#20102;&#24040;&#22823;&#21162;&#21147;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;i) &#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#39044;&#27979;&#25152;&#26377;&#21306;&#22495;&#30340;&#27969;&#37327;&#26102;&#27809;&#26377;&#32771;&#34385;&#31354;&#38388;&#24322;&#36136;&#24615;&#65292;&#21363;&#19981;&#21516;&#21306;&#22495;&#21487;&#33021;&#20855;&#26377;&#20542;&#26012;&#30340;&#20132;&#36890;&#27969;&#37327;&#20998;&#24067;&#12290; ii) &#36825;&#20123;&#27169;&#22411;&#26410;&#33021;&#25429;&#25417;&#30001;&#20110;&#26102;&#38388;&#21464;&#21270;&#30340;&#20132;&#36890;&#27169;&#24335;&#32780;&#24341;&#36215;&#30340;&#26102;&#38388;&#24322;&#36136;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20351;&#29992;&#19968;&#20010;&#20849;&#20139;&#21442;&#25968;&#21270;&#31354;&#38388;&#26469;&#27169;&#25311;&#25152;&#26377;&#26102;&#38388;&#27573;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;ST-SSL&#65289;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#36741;&#21161;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#22686;&#24378;&#20132;&#36890;&#27169;&#24335;&#34920;&#31034;&#65292;&#20351;&#20854;&#26082;&#21453;&#26144;&#31354;&#38388;&#24322;&#36136;&#24615;&#21448;&#21453;&#26144;&#26102;&#38388;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.04475v2 Announce Type: replace-cross  Abstract: Robust prediction of citywide traffic flows at different time periods plays a crucial role in intelligent transportation systems. While previous work has made great efforts to model spatio-temporal correlations, existing methods still suffer from two key limitations: i) Most models collectively predict all regions' flows without accounting for spatial heterogeneity, i.e., different regions may have skewed traffic flow distributions. ii) These models fail to capture the temporal heterogeneity induced by time-varying traffic patterns, as they typically model temporal correlations with a shared parameterized space for all time periods. To tackle these challenges, we propose a novel Spatio-Temporal Self-Supervised Learning (ST-SSL) traffic prediction framework which enhances the traffic pattern representations to be reflective of both spatial and temporal heterogeneity, with auxiliary self-supervised learning paradigms. Specificall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;&#31929;&#23616;&#37096;&#20449;&#24687;&#23454;&#29616;&#25512;&#27979;&#23545;&#25163;&#24314;&#27169;&#30340;&#22810;&#26234;&#33021;&#20307;&#20998;&#24067;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#21463;&#25511;&#20195;&#29702;&#20570;&#20986;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2211.11940</link><description>&lt;p&gt;
&#29992;&#25512;&#27979;&#23545;&#25163;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Decision-making with Speculative Opponent Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.11940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;&#31929;&#23616;&#37096;&#20449;&#24687;&#23454;&#29616;&#25512;&#27979;&#23545;&#25163;&#24314;&#27169;&#30340;&#22810;&#26234;&#33021;&#20307;&#20998;&#24067;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#21463;&#25511;&#20195;&#29702;&#20570;&#20986;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25163;&#24314;&#27169;&#36890;&#36807;&#26500;&#24314;&#20854;&#20182;&#20195;&#29702;&#30340;&#27169;&#22411;&#65292;&#20351;&#21463;&#25511;&#20195;&#29702;&#30340;&#20915;&#31574;&#21463;&#30410;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#23545;&#25163;&#30340;&#35266;&#23519;&#21644;&#34892;&#20026;&#65292;&#20294;&#24403;&#23545;&#25163;&#30340;&#34892;&#20026;&#19981;&#21487;&#35266;&#23519;&#25110;&#38590;&#20197;&#33719;&#24471;&#26102;&#65292;&#36825;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#20998;&#24067;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#32431;&#31929;&#30340;&#23616;&#37096;&#20449;&#24687;&#65288;&#21363;&#21463;&#25511;&#20195;&#29702;&#30340;&#35266;&#23519;&#12289;&#34892;&#20026;&#21644;&#22870;&#21169;&#65289;&#23454;&#29616;&#25512;&#27979;&#23545;&#25163;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#28436;&#21592;&#32500;&#25345;&#23545;&#23545;&#25163;&#30340;&#25512;&#27979;&#20449;&#24565;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25512;&#27979;&#23545;&#25163;&#27169;&#22411;&#65292;&#20197;&#20351;&#29992;&#23616;&#37096;&#35266;&#23519;&#26469;&#39044;&#27979;&#23545;&#25163;&#30340;&#21160;&#20316;&#65292;&#24182;&#30456;&#24212;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#20998;&#24067;&#24335;&#35780;&#35770;&#23478;&#27169;&#22411;&#25919;&#31574;&#30340;&#22238;&#25253;&#20998;&#24067;&#12290;&#23427;&#21453;&#26144;&#20102;&#28436;&#21592;&#30340;&#36136;&#37327;&#65292;&#22240;&#27492;&#21487;&#20197;&#25351;&#23548;&#28436;&#21592;&#25152;&#20381;&#36182;&#30340;&#25512;&#27979;&#23545;&#25163;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.11940v2 Announce Type: replace  Abstract: Opponent modeling has benefited a controlled agent's decision-making by constructing models of other agents. Existing methods commonly assume access to opponents' observations and actions, which is infeasible when opponents' behaviors are unobservable or hard to obtain. We propose a novel multi-agent distributional actor-critic algorithm to achieve speculative opponent modeling with purely local information (i.e., the controlled agent's observations, actions, and rewards). Specifically, the actor maintains a speculated belief of the opponents, which we call the speculative opponent models, to predict opponent actions using local observations and makes decisions accordingly. Further, the distributional critic models the return distribution of the policy. It reflects the quality of the actor and thus can guide the training of the speculative opponent model that the actor relies on. Extensive experiments confirm that our method successf
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25581;&#31034;&#21644;&#21033;&#29992;&#31038;&#20250;&#25216;&#26415;&#21644;&#22522;&#30784;&#35774;&#26045;&#19981;&#21305;&#37197;&#65292;&#26080;&#32541;&#35774;&#35745;&#21487;&#20197;&#20419;&#36827;AI&#21487;&#35299;&#37322;&#24615;</title><link>https://arxiv.org/abs/2211.06753</link><description>&lt;p&gt;
Seamful XAI: &#23558;&#26080;&#32541;&#35774;&#35745;&#36816;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Seamful XAI: Operationalizing Seamful Design in Explainable AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.06753
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25581;&#31034;&#21644;&#21033;&#29992;&#31038;&#20250;&#25216;&#26415;&#21644;&#22522;&#30784;&#35774;&#26045;&#19981;&#21305;&#37197;&#65292;&#26080;&#32541;&#35774;&#35745;&#21487;&#20197;&#20419;&#36827;AI&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#38169;&#35823;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#36825;&#20123;&#38169;&#35823;&#26082;&#26469;&#33258;&#25216;&#26415;&#38480;&#21046;&#65292;&#20063;&#26469;&#33258;&#31038;&#20250;&#25216;&#26415;&#24046;&#36317;&#12290;&#34429;&#28982;&#23558;AI&#31995;&#32479;&#35774;&#20026;&#40657;&#21283;&#23376;&#21487;&#20197;&#20351;&#29992;&#25143;&#20307;&#39564;&#26356;&#27969;&#30021;&#65292;&#20294;&#38544;&#34255;&#25509;&#32541;&#20250;&#20351;&#29992;&#25143;&#22833;&#21435;&#20943;&#36731;AI&#38169;&#35823;&#21518;&#26524;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;AI&#19981;&#23436;&#32654;&#26469;&#24110;&#21161;&#29992;&#25143;&#65292;&#32780;&#19981;&#26159;&#23558;&#20854;&#38544;&#34255;&#36215;&#26469;&#12290;&#34429;&#28982;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20027;&#35201;&#35299;&#20915;&#20102;&#31639;&#27861;&#19981;&#36879;&#26126;&#24615;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#25581;&#31034;&#21644;&#21033;&#29992;&#31038;&#20250;&#25216;&#26415;&#21644;&#22522;&#30784;&#35774;&#26045;&#19981;&#21305;&#37197;&#65292;&#26080;&#32541;&#35774;&#35745;&#21487;&#20197;&#20419;&#36827;AI&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Seamful XAI&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#65288;1&#65289;&#23558;&#8220;&#25509;&#32541;&#8221;&#27010;&#24565;&#36716;&#31227;&#21040;AI&#32972;&#26223;&#19979;&#21644;&#65288;2&#65289;&#24320;&#21457;&#24110;&#21161;&#21033;&#30410;&#30456;&#20851;&#32773;&#39044;&#35265;&#21644;&#35774;&#35745;&#25509;&#32541;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#36807;&#31243;&#19982;43&#21517;AI&#20174;&#19994;&#32773;&#21644;&#30495;&#23454;&#32456;&#31471;&#29992;&#25143;&#19968;&#36215;&#25506;&#35752;&#65292;&#20351;&#29992;&#22522;&#20110;&#22330;&#26223;&#30340;&#21327;&#21516;&#35774;&#35745;&#27963;&#21160;&#65292;&#24182;&#26681;&#25454;&#29616;&#23454;&#29992;&#20363;&#36827;&#34892;&#20449;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Seamful XAI&#35774;&#35745;&#36807;&#31243;&#26377;&#21161;&#20110;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.06753v2 Announce Type: replace-cross  Abstract: Mistakes in AI systems are inevitable, arising from both technical limitations and sociotechnical gaps. While black-boxing AI systems can make the user experience seamless, hiding the seams risks disempowering users to mitigate fallouts from AI mistakes. Instead of hiding these AI imperfections, can we leverage them to help the user? While Explainable AI (XAI) has predominantly tackled algorithmic opaqueness, we propose that seamful design can foster AI explainability by revealing and leveraging sociotechnical and infrastructural mismatches. We introduce the concept of Seamful XAI by (1) conceptually transferring "seams" to the AI context and (2) developing a design process that helps stakeholders anticipate and design with seams. We explore this process with 43 AI practitioners and real end-users, using a scenario-based co-design activity informed by real-world use cases. We found that the Seamful XAI design process helped use
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#23436;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#23436;&#25104;&#20219;&#21153;&#65292;&#22312;&#22810;&#20219;&#21153;&#32593;&#32476;&#20013;&#26377;&#25928;&#25552;&#39640;&#20102;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2209.00381</link><description>&lt;p&gt;
SemSegDepth: &#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
SemSegDepth: A Combined Model for Semantic Segmentation and Depth Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00381
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#23436;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#23436;&#25104;&#20219;&#21153;&#65292;&#22312;&#22810;&#20219;&#21153;&#32593;&#32476;&#20013;&#26377;&#25928;&#25552;&#39640;&#20102;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32508;&#21512;&#22330;&#26223;&#29702;&#35299;&#23545;&#20110;&#33258;&#20027;&#26426;&#22120;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#23436;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;RGB&#21644;&#31232;&#30095;&#28145;&#24230;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#24182;&#20135;&#29983;&#23494;&#38598;&#28145;&#24230;&#22270;&#21644;&#30456;&#24212;&#30340;&#35821;&#20041;&#20998;&#21106;&#22270;&#20687;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#19968;&#20010;&#28145;&#24230;&#23436;&#25104;&#20998;&#25903;&#65292;&#19968;&#20010;&#35821;&#20041;&#20998;&#21106;&#20998;&#25903;&#20197;&#21450;&#19968;&#20010;&#32852;&#21512;&#20998;&#25903;&#65292;&#36827;&#19968;&#27493;&#21516;&#26102;&#22788;&#29702;&#35821;&#20041;&#21644;&#28145;&#24230;&#20449;&#24687;&#12290;&#22312;Virtual KITTI 2&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#35777;&#25454;&#65292;&#21363;&#22312;&#22810;&#20219;&#21153;&#32593;&#32476;&#20013;&#32467;&#21512;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#23436;&#25104;&#20219;&#21153;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00381v2 Announce Type: replace-cross  Abstract: Holistic scene understanding is pivotal for the performance of autonomous machines. In this paper we propose a new end-to-end model for performing semantic segmentation and depth completion jointly. The vast majority of recent approaches have developed semantic segmentation and depth completion as independent tasks. Our approach relies on RGB and sparse depth as inputs to our model and produces a dense depth map and the corresponding semantic segmentation image. It consists of a feature extractor, a depth completion branch, a semantic segmentation branch and a joint branch which further processes semantic and depth information altogether. The experiments done on Virtual KITTI 2 dataset, demonstrate and provide further evidence, that combining both tasks, semantic segmentation and depth completion, in a multi-task network can effectively improve the performance of each task. Code is available at https://github.com/juanb09111/sem
&lt;/p&gt;</description></item><item><title>AI&#32972;&#26223;&#22914;&#20309;&#24433;&#21709;&#35299;&#37322;&#30340;&#35299;&#35835;&#65292;&#25581;&#31034;&#20102;&#8220;&#35841;&#8221;&#23545;&#20110;AI&#35299;&#37322;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2107.13509</link><description>&lt;p&gt;
XAI&#20013;&#30340;&#8220;&#35841;&#8221;&#65306;AI&#32972;&#26223;&#22914;&#20309;&#22609;&#36896;AI&#35299;&#37322;&#30340;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
The Who in XAI: How AI Background Shapes Perceptions of AI Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2107.13509
&lt;/p&gt;
&lt;p&gt;
AI&#32972;&#26223;&#22914;&#20309;&#24433;&#21709;&#35299;&#37322;&#30340;&#35299;&#35835;&#65292;&#25581;&#31034;&#20102;&#8220;&#35841;&#8221;&#23545;&#20110;AI&#35299;&#37322;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#29992;&#25143;&#37319;&#21462;&#30693;&#24773;&#34892;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#29702;&#35299;AI&#40657;&#30418;&#20013;&#30340;&#8220;&#35841;&#8221;&#19982;&#25171;&#24320;&#23427;&#21516;&#26679;&#37325;&#35201;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#20004;&#32452;&#19981;&#21516;&#30340;&#20154;&#32676;&#8212;&#8212;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;AI&#32972;&#26223;&#30340;&#20154;&#32676;&#8212;&#8212;&#22914;&#20309;&#24863;&#30693;&#19981;&#21516;&#31867;&#22411;&#30340;AI&#35299;&#37322;&#12290;&#22312;&#23450;&#37327;&#19978;&#65292;&#25105;&#20204;&#20998;&#20139;&#29992;&#25143;&#22312;&#20116;&#20010;&#32500;&#24230;&#19978;&#30340;&#30475;&#27861;&#12290;&#22312;&#23450;&#24615;&#19978;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;AI&#32972;&#26223;&#22914;&#20309;&#24433;&#21709;&#35299;&#37322;&#30340;&#35299;&#35835;&#65292;&#36890;&#36807;&#25311;&#20154;&#21644;&#35748;&#30693;&#21551;&#21457;&#30340;&#35270;&#35282;&#38416;&#26126;&#20102;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#65288;1&#65289;&#20004;&#32452;&#20154;&#20986;&#20110;&#19981;&#21516;&#21407;&#22240;&#23545;&#25968;&#23383;&#34920;&#29616;&#20986;&#19981;&#24517;&#35201;&#30340;&#20449;&#20219;&#65292;&#20197;&#21450;&#65288;2&#65289;&#27599;&#32452;&#20154;&#37117;&#21457;&#29616;&#19981;&#21516;&#20110;&#20854;&#39044;&#26399;&#35774;&#35745;&#30340;&#35299;&#37322;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;XAI&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#23637;&#31034;&#20102;&#21363;&#20351;&#20986;&#20110;&#26368;&#22909;&#30340;&#29992;&#24847;&#65292;AI&#29983;&#25104;&#30340;&#35299;&#37322;&#20063;&#21487;&#33021;&#20135;&#29983;&#36127;&#38754;&#21518;&#26524;&#65292;&#21487;&#33021;&#23548;&#33268;&#26377;&#23475;&#30340;&#20449;&#20219;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
arXiv:2107.13509v2 Announce Type: replace-cross  Abstract: Explainability of AI systems is critical for users to take informed actions. Understanding "who" opens the black-box of AI is just as important as opening it. We conduct a mixed-methods study of how two different groups--people with and without AI background--perceive different types of AI explanations. Quantitatively, we share user perceptions along five dimensions. Qualitatively, we describe how AI background can influence interpretations, elucidating the differences through lenses of appropriation and cognitive heuristics. We find that (1) both groups showed unwarranted faith in numbers for different reasons and (2) each group found value in different explanations beyond their intended design. Carrying critical implications for the field of XAI, our findings showcase how AI generated explanations can have negative consequences despite best intentions and how that could lead to harmful manipulation of trust. We propose design
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.16553</link><description>&lt;p&gt;
SelectLLM&#65306;LLMs&#33021;&#21542;&#36873;&#25321;&#37325;&#35201;&#30340;&#25351;&#20196;&#36827;&#34892;&#27880;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
SelectLLM: Can LLMs Select Important Instructions to Annotate?. (arXiv:2401.16553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#20351;&#27169;&#22411;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#19968;&#23567;&#32452;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#21487;&#20197;&#36229;&#36807;&#20351;&#29992;&#22823;&#37327;&#26356;&#22024;&#26434;&#30340;&#25351;&#20196;&#12290;&#30001;&#20110;&#25351;&#20196;&#26159;&#26080;&#26631;&#31614;&#30340;&#65292;&#19988;&#21709;&#24212;&#26159;&#33258;&#28982;&#25991;&#26412;&#65292;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36873;&#25321;&#26080;&#26631;&#31614;&#25351;&#20196;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#65292;&#31216;&#20026;SelectLLM&#65292;&#23427;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#39640;&#32423;&#24605;&#24819;&#26159;&#21033;&#29992;LLMs&#36890;&#36807;&#25552;&#31034;&#26469;&#20272;&#35745;&#27599;&#20010;&#25351;&#20196;&#22312;&#27809;&#26377;&#30456;&#24212;&#26631;&#31614;&#65288;&#21363;&#21709;&#24212;&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#12290;SelectLLM&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#65288;&#20363;&#22914;CoreSet&#65289;&#23558;&#26080;&#26631;&#31614;&#25351;&#20196;&#21010;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#65292;&#28982;&#21518;&#25552;&#31034;LLMs&#22312;&#20854;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models (LLMs) with a large and diverse instruction dataset aligns the models to comprehend and follow human instructions. Recent works have shown that using a small set of high-quality instructions can outperform using large yet more noisy ones. Because instructions are unlabeled and their responses are natural text, traditional active learning schemes with the model's confidence cannot be directly applied to the selection of unlabeled instructions. In this work, we propose a novel method for instruction selection, called SelectLLM, that leverages LLMs for the selection of high-quality instructions. Our high-level idea is to use LLMs to estimate the usefulness and impactfulness of each instruction without the corresponding labels (i.e., responses), via prompting. SelectLLM involves two steps: dividing the unlabelled instructions using a clustering algorithm (e.g., CoreSet) to multiple clusters, and then prompting LLMs to choose high-quality instructions within e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10712</link><description>&lt;p&gt;
Q&amp;A&#25552;&#31034;&#65306;&#36890;&#36807;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#25552;&#31034;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#28385;&#36275;&#23545;&#22810;&#26679;&#19990;&#30028;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#65292;&#22238;&#31572;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;AI&#27169;&#22411;&#37197;&#22791;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#30340;&#35748;&#30693;&#26041;&#26696;&#23578;&#26410;&#31995;&#32479;&#22320;&#34987;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30456;&#20449;&#65292;&#22914;&#26524;&#25105;&#20204;&#33021;&#23613;&#21487;&#33021;&#25910;&#38598;&#32473;&#23450;&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#25105;&#20204;&#23558;&#33021;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#22270;&#20687;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#38382;&#39064;&#65292;&#26356;&#23481;&#26131;&#22238;&#24518;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#26368;&#32456;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#36825;&#20123;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;&#25552;&#31034;&#21457;&#36865;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22270;&#20687;-&#31572;&#26696;&#23545;&#21644;&#30456;&#24212;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35757;&#32451;&#19968;&#20010;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&amp;A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06401</link><description>&lt;p&gt;
DevEval: &#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DevEval: Evaluating Code Generation in Practical Software Projects. (arXiv:2401.06401v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#25552;&#20986;&#65292;&#20294;&#26159;&#19982;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#19981;&#19968;&#33268;&#65292;&#20363;&#22914;&#34394;&#26500;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20381;&#36182;&#19981;&#36275;&#21644;&#23567;&#35268;&#27169;&#39033;&#30446;&#32972;&#26223;&#12290;&#22240;&#27492;&#65292;LLMs&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#33021;&#21147;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#24320;&#21457;&#20154;&#21592;&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#32463;&#39564;&#30456;&#21563;&#21512;&#12290;DevEval&#36890;&#36807;&#19968;&#20010;&#20005;&#26684;&#30340;&#27969;&#31243;&#25910;&#38598;&#21040;&#20102;&#26469;&#33258;119&#20010;&#23454;&#38469;&#39033;&#30446;&#30340;2690&#20010;&#26679;&#26412;&#65292;&#28085;&#30422;10&#20010;&#39046;&#22495;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#19982;&#23454;&#38469;&#39033;&#30446;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#30495;&#23454;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#12290;&#25105;&#20204;&#22312;DevEval&#19978;&#35780;&#20272;&#20102;&#20116;&#20010;&#27969;&#34892;&#30340;LLMs&#65288;&#20363;&#22914;gpt-4&#65292;gpt-3.5-turbo&#65292;CodeLLaMa&#21644;StarCoder&#65289;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;gpt-3.5-turbo&#30340;&#26368;&#39640;Pass@1&#21482;&#26377;42&#12290;
&lt;/p&gt;
&lt;p&gt;
How to evaluate Large Language Models (LLMs) in code generation is an open question. Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts. Thus, the capabilities of LLMs in practical projects are still unclear. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess five popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42 in our experim
&lt;/p&gt;</description></item><item><title>Let's Go Shopping (LGS) dataset is a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites, providing a more efficient way to collect and annotate images for vision and vision-language applications.</title><link>http://arxiv.org/abs/2401.04575</link><description>&lt;p&gt;
Let's Go Shopping (LGS) -- &#29992;&#20110;&#35270;&#35273;&#27010;&#24565;&#29702;&#35299;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding. (arXiv:2401.04575v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04575
&lt;/p&gt;
&lt;p&gt;
Let's Go Shopping (LGS) dataset is a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites, providing a more efficient way to collect and annotate images for vision and vision-language applications.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#23383;&#24149;&#65292;&#20381;&#36182;&#20110;&#38656;&#35201;&#38750;&#24179;&#20961;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#30340;&#22823;&#35268;&#27169;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#32791;&#26102;&#30340;&#21162;&#21147;&#38480;&#21046;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20986;&#29616;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21482;&#33021;&#36873;&#25321;&#23569;&#25968;&#20960;&#31181;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23547;&#27714;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25910;&#38598;&#21644;&#27880;&#37322;&#22270;&#20687;&#12290;&#20197;&#24448;&#30340;&#20513;&#35758;&#24050;&#32463;&#20174;HTML alt&#25991;&#26412;&#21644;&#29228;&#21462;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25910;&#38598;&#20102;&#23383;&#24149;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#28304;&#23384;&#22312;&#22122;&#22768;&#12289;&#31232;&#30095;&#25110;&#20027;&#35266;&#24615;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36716;&#21521;&#21830;&#19994;&#36141;&#29289;&#32593;&#31449;&#65292;&#20854;&#25968;&#25454;&#31526;&#21512;&#19977;&#20010;&#26631;&#20934;&#65306;&#24178;&#20928;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#27969;&#30021;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Let's Go Shopping&#65288;LGS&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#26469;&#33258;&#20844;&#24320;&#21487;&#29992;&#30340;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#30340;1500&#19975;&#20010;&#22270;&#20687;-&#23383;&#24149;&#23545;&#30340;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#19982;&#29616;&#26377;&#30340;&#36890;&#29992;&#39046;&#22495;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;LGS&#22270;&#20687;&#20391;&#37325;&#20110;&#21069;&#26223;&#23545;&#35937;&#65292;&#32972;&#26223;&#22797;&#26434;&#24230;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision and vision-language applications of neural networks, such as image classification and captioning, rely on large-scale annotated datasets that require non-trivial data-collecting processes. This time-consuming endeavor hinders the emergence of large-scale datasets, limiting researchers and practitioners to a small number of choices. Therefore, we seek more efficient ways to collect and annotate images. Previous initiatives have gathered captions from HTML alt-texts and crawled social media postings, but these data sources suffer from noise, sparsity, or subjectivity. For this reason, we turn to commercial shopping websites whose data meet three criteria: cleanliness, informativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset, a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites. When compared with existing general-domain datasets, the LGS images focus on the foreground object and have less complex backgro
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#40657;&#30418;&#38382;&#39064;&#30340;&#30333;&#30418;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#39046;&#22495;&#19968;&#33324;&#29702;&#35770;&#30340;&#30830;&#23450;&#24615;&#36923;&#36753;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#35268;&#21017;&#65292;&#35813;&#32454;&#32990;&#33258;&#21160;&#26426;&#23454;&#29616;&#33258;&#21160;&#24182;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.03093</link><description>&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#40657;&#30418;&#38382;&#39064;&#30340;&#30333;&#30418;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A white box solution to the black box problem of AI. (arXiv:2401.03093v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03093
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#40657;&#30418;&#38382;&#39064;&#30340;&#30333;&#30418;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#39046;&#22495;&#19968;&#33324;&#29702;&#35770;&#30340;&#30830;&#23450;&#24615;&#36923;&#36753;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#35268;&#21017;&#65292;&#35813;&#32454;&#32990;&#33258;&#21160;&#26426;&#23454;&#29616;&#33258;&#21160;&#24182;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36879;&#26126;&#24615;&#65292;&#23545;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#23384;&#22312;&#25285;&#24551;&#12290;&#36825;&#23601;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#30418;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#31526;&#21495; AI &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#31526;&#21495; AI &#20855;&#26377;&#36879;&#26126;&#30340;&#30333;&#30418;&#24615;&#36136;&#12290;&#31526;&#21495; AI &#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#25968;&#23398;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#26415;&#35821;&#30340;&#19981;&#36879;&#26126;&#24615;&#12289;&#32570;&#20047;&#32479;&#19968;&#26412;&#20307;&#35770;&#20197;&#21450;&#25628;&#32034;&#36873;&#39033;&#30340;&#32452;&#21512;&#29190;&#28856;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#30418;&#38382;&#39064;&#24182;&#23454;&#29616;&#36890;&#29992;&#30340;&#31526;&#21495; AI&#65292;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#39046;&#22495;&#19968;&#33324;&#29702;&#35770;&#30340;&#30830;&#23450;&#24615;&#36923;&#36753;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#35268;&#21017;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30456;&#20851;&#39046;&#22495;&#30340;&#19968;&#33324;&#29702;&#35770;&#36215;&#21040;&#20102;&#32454;&#32990;&#33258;&#21160;&#26426;&#25512;&#29702;&#30340;&#30693;&#35782;&#24211;&#30340;&#20316;&#29992;&#12290;&#32454;&#32990;&#33258;&#21160;&#26426;&#22312;&#22797;&#26434;&#31995;&#32479;&#30340;&#19977;&#20010;&#23618;&#27425;&#19978;&#23454;&#29616;&#33258;&#21160;&#24182;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence based on neural networks has made significant progress. However, there are concerns about the reliability and security of this approach due to its lack of transparency. This is the black box problem of AI. Here we show how this problem can be solved using symbolic AI, which has a transparent white box nature. The widespread use of symbolic AI is hindered by the opacity of mathematical models and natural language terms, the lack of a unified ontology, and the combinatorial explosion of search options. To solve the AI black box problem and to implement general-purpose symbolic AI, we propose to use deterministic logic cellular automata with rules based on first principles of the general theory of the relevant domain. In this case, the general theory of the relevant domain plays the role of a knowledge base for the cellular automaton inference. A cellular automaton implements automatic parallel logical inference at three levels of organization of a complex system. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;-$\mathcal{T}$-&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.14814</link><description>&lt;p&gt;
&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias. (arXiv:2310.14814v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;-$\mathcal{T}$-&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#23545;&#27169;&#22411;&#33258;&#20449;&#24230;&#39640;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20266;&#26631;&#31614;&#20998;&#37197;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#22788;&#29702;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#24120;&#20351;&#29992;softmax&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#33258;&#20449;&#24230;&#24230;&#37327;&#65292;&#23613;&#31649;&#24050;&#30693;&#23427;&#20204;&#23545;&#38169;&#35823;&#39044;&#27979;&#20063;&#36807;&#20110;&#33258;&#20449;&#12290;&#24403;&#25968;&#25454;&#26631;&#27880;&#21463;&#21040;&#26576;&#31181;&#32422;&#26463;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#23588;&#20026;&#26126;&#26174;&#65292;&#21363;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;$\mathcal{T}$-&#30456;&#20284;&#24230;&#65292;&#23427;&#22522;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#39044;&#27979;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#31283;&#23450;&#28857;&#24182;&#25551;&#36848;&#21333;&#20010;&#25104;&#21592;&#30340;&#22810;&#26679;&#24615;&#19982;&#20854;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#25552;&#20379;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#33258;&#20449;&#24230;&#24230;&#37327;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, despite the fact that they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraint. To address this issue, we propose a novel confidence measure, called $\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on c
&lt;/p&gt;</description></item><item><title>&#21487;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;&#65288;SNNKs&#65289;&#26159;&#19968;&#31181;&#26367;&#20195;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36817;&#20284;&#23454;&#29616;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#21151;&#33021;&#65292;&#20294;&#20855;&#26377;&#26356;&#20248;&#30340;&#35745;&#31639;&#29305;&#24615;&#12290;&#36890;&#36807;&#23558;&#20869;&#26680;&#19982;&#21442;&#25968;-&#36755;&#20837;&#21521;&#37327;&#30340;&#28857;&#31215;&#32852;&#31995;&#36215;&#26469;&#65292;SNNKs&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#24320;&#21442;&#25968;&#19982;&#36755;&#20837;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#25414;&#32465;&#36807;&#31243;&#65292;&#23558;SNNKs&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;&#26368;&#32456;&#25414;&#32465;&#32593;&#32476;&#29978;&#33267;&#21487;&#20197;&#32469;&#36807;&#21453;&#21521;&#20256;&#25773;&#65292;&#36890;&#36807;&#26174;&#24335;&#20844;&#24335;&#27714;&#35299;&#26368;&#20248;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.13225</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;
&lt;/p&gt;
&lt;p&gt;
Scalable Neural Network Kernels. (arXiv:2310.13225v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13225
&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;&#65288;SNNKs&#65289;&#26159;&#19968;&#31181;&#26367;&#20195;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36817;&#20284;&#23454;&#29616;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#21151;&#33021;&#65292;&#20294;&#20855;&#26377;&#26356;&#20248;&#30340;&#35745;&#31639;&#29305;&#24615;&#12290;&#36890;&#36807;&#23558;&#20869;&#26680;&#19982;&#21442;&#25968;-&#36755;&#20837;&#21521;&#37327;&#30340;&#28857;&#31215;&#32852;&#31995;&#36215;&#26469;&#65292;SNNKs&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#24320;&#21442;&#25968;&#19982;&#36755;&#20837;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#25414;&#32465;&#36807;&#31243;&#65292;&#23558;SNNKs&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;&#26368;&#32456;&#25414;&#32465;&#32593;&#32476;&#29978;&#33267;&#21487;&#20197;&#32469;&#36807;&#21453;&#21521;&#20256;&#25773;&#65292;&#36890;&#36807;&#26174;&#24335;&#20844;&#24335;&#27714;&#35299;&#26368;&#20248;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;&#65288;SNNKs&#65289;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#24120;&#35268;&#21069;&#39304;&#23618;&#65288;FFLs&#65289;&#30340;&#26367;&#20195;&#21697;&#65292;&#33021;&#22815;&#36817;&#20284;&#23454;&#29616;&#21518;&#32773;&#65292;&#20294;&#20855;&#26377;&#26377;&#21033;&#30340;&#35745;&#31639;&#23646;&#24615;&#12290;SNNKs&#26377;&#25928;&#22320;&#35299;&#24320;&#20102;FFL&#20013;&#21442;&#25968;&#19982;&#36755;&#20837;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#36890;&#36807;&#28857;&#31215;&#20869;&#26680;&#22312;&#26368;&#32456;&#35745;&#31639;&#20013;&#36830;&#25509;&#23427;&#20204;&#12290;&#23427;&#20204;&#20063;&#26356;&#21152;&#34920;&#36798;&#21147;&#24378;&#65292;&#33021;&#22815;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#65292;&#36229;&#20986;&#21442;&#25968;-&#36755;&#20837;&#21521;&#37327;&#30340;&#20989;&#25968;&#33539;&#22260;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#25414;&#32465;&#36807;&#31243;&#65292;&#23558;SNNKs&#24212;&#29992;&#20110;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33719;&#24471;&#39069;&#22806;&#30340;&#21387;&#32553;&#25928;&#30410;&#12290;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#23427;&#23548;&#33268;&#23436;&#20840;&#25414;&#32465;&#32593;&#32476;&#65292;&#20854;&#26368;&#20248;&#21442;&#25968;&#21487;&#20197;&#36890;&#36807;&#22810;&#20010;&#25439;&#22833;&#20989;&#25968;&#65288;&#20363;&#22914;&#22343;&#26041;&#35823;&#24046;&#65289;&#30340;&#26174;&#24335;&#20844;&#24335;&#26469;&#34920;&#31034;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#32469;&#36807;&#21453;&#21521;&#20256;&#25773;&#12290;&#20316;&#20026;&#25105;&#20204;&#20998;&#26512;&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26222;&#36941;&#24615;&#26426;&#21046;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concept of scalable neural network kernels (SNNKs), the replacements of regular feedforward layers (FFLs), capable of approximating the latter, but with favorable computational properties. SNNKs effectively disentangle the inputs from the parameters of the neural network in the FFL, only to connect them in the final computation via the dot-product kernel. They are also strictly more expressive, as allowing to model complicated relationships beyond the functions of the dot-products of parameter-input vectors. We also introduce the neural network bundling process that applies SNNKs to compactify deep neural network architectures, resulting in additional compression gains. In its extreme version, it leads to the fully bundled network whose optimal parameters can be expressed via explicit formulae for several loss functions (e.g. mean squared error), opening a possibility to bypass backpropagation. As a by-product of our analysis, we introduce the mechanism of the universa
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21069;&#39069;&#21494;&#30382;&#23618;&#21551;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#20110;LLM&#30340;&#27169;&#22359;&#23454;&#29616;&#35268;&#21010;&#30340;&#33258;&#20027;&#21327;&#35843;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25110;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;&#30340;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00194</link><description>&lt;p&gt;
&#21463;&#21069;&#39069;&#21494;&#30382;&#23618;&#21551;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models. (arXiv:2310.00194v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00194
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21069;&#39069;&#21494;&#30382;&#23618;&#21551;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#20110;LLM&#30340;&#27169;&#22359;&#23454;&#29616;&#35268;&#21010;&#30340;&#33258;&#20027;&#21327;&#35843;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25110;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;&#30340;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25110;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#20154;&#33041;&#20013;&#33719;&#21462;&#28789;&#24863;&#65292;&#21363;&#36890;&#36807;&#21069;&#39069;&#21494;&#30382;&#23618;&#65288;PFC&#65289;&#20013;&#19987;&#38376;&#27169;&#22359;&#30340;&#37325;&#22797;&#20132;&#20114;&#26469;&#23436;&#25104;&#35268;&#21010;&#12290;&#36825;&#20123;&#27169;&#22359;&#25191;&#34892;&#20914;&#31361;&#30417;&#27979;&#12289;&#29366;&#24577;&#39044;&#27979;&#12289;&#29366;&#24577;&#35780;&#20272;&#12289;&#20219;&#21153;&#20998;&#35299;&#21644;&#20219;&#21153;&#21327;&#35843;&#31561;&#21151;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#26377;&#26102;&#33021;&#22815;&#21333;&#29420;&#25191;&#34892;&#36825;&#20123;&#21151;&#33021;&#65292;&#20294;&#22312;&#26381;&#21153;&#20110;&#19968;&#20010;&#30446;&#26631;&#26102;&#24448;&#24448;&#38590;&#20197;&#33258;&#20027;&#21327;&#35843;&#23427;&#20204;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22810;&#20010;&#22522;&#20110;LLM&#65288;GPT-4&#65289;&#27169;&#22359;&#30340;&#40657;&#30418;&#26550;&#26500;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#19987;&#38376;&#30340;PFC&#21551;&#21457;&#27169;&#22359;&#30340;&#20132;&#20114;&#23558;&#19968;&#20010;&#26356;&#22823;&#30340;&#38382;&#39064;&#20998;&#35299;&#20026;&#22810;&#20010;&#23545;LLM&#30340;&#31616;&#30701;&#33258;&#21160;&#35843;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#32452;&#21512;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate impressive performance on a wide variety of tasks, but they often struggle with tasks that require multi-step reasoning or goal-directed planning. To address this, we take inspiration from the human brain, in which planning is accomplished via the recurrent interaction of specialized modules in the prefrontal cortex (PFC). These modules perform functions such as conflict monitoring, state prediction, state evaluation, task decomposition, and task coordination. We find that LLMs are sometimes capable of carrying out these functions in isolation, but struggle to autonomously coordinate them in the service of a goal. Therefore, we propose a black box architecture with multiple LLM-based (GPT-4) modules. The architecture improves planning through the interaction of specialized PFC-inspired modules that break down a larger problem into multiple brief automated calls to the LLM. We evaluate the combined architecture on two challenging planning tasks -
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#20844;&#24335;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#27169;&#25311;&#22842;&#26071;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16960</link><description>&lt;p&gt;
&#29983;&#25104;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Generating Explanations for Reinforcement Learning Policies: An Empirical Study. (arXiv:2309.16960v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#20844;&#24335;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#27169;&#25311;&#22842;&#26071;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#32452;&#35774;&#35745;&#29992;&#20110;&#25552;&#20379;&#31574;&#30053;&#35299;&#37322;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#20844;&#24335;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#26500;&#24314;&#26082;&#38416;&#26126;&#31574;&#30053;&#25152;&#23454;&#29616;&#30340;&#26368;&#32456;&#30446;&#26631;&#21448;&#38416;&#26126;&#20854;&#25191;&#34892;&#36807;&#31243;&#20013;&#25152;&#32500;&#25345;&#30340;&#21069;&#25552;&#26465;&#20214;&#30340;&#35299;&#37322;&#12290;&#36825;&#20123;&#22522;&#20110;LTL&#30340;&#35299;&#37322;&#20855;&#26377;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23616;&#37096;&#25628;&#32034;&#25216;&#26415;&#12290;&#36890;&#36807;&#27169;&#25311;&#30340;&#22842;&#26071;&#29615;&#22659;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#35770;&#25991;&#26368;&#21518;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#24314;&#35758;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a set of \textit{Linear Temporal Logic} (LTL) formulae designed to provide explanations for policies. Our focus is on crafting explanations that elucidate both the ultimate objectives accomplished by the policy and the prerequisites it upholds throughout its execution. These LTL-based explanations feature a structured representation, which is particularly well-suited for local-search techniques. The effectiveness of our proposed approach is illustrated through a simulated capture the flag environment. The paper concludes with suggested directions for future research.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22240;&#26524;&#25925;&#20107;&#30340;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#23616;&#37096;&#22240;&#26524;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25913;&#36827;&#35270;&#35273;&#25925;&#20107;&#21512;&#25104;&#30340;&#20840;&#23616;&#19968;&#33268;&#24615;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#21382;&#21490;&#26631;&#39064;&#12289;&#24103;&#21644;&#24403;&#21069;&#26631;&#39064;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.09553</link><description>&lt;p&gt;
&#22240;&#26524;&#25925;&#20107;&#65306;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#23616;&#37096;&#22240;&#26524;&#27880;&#24847;&#21147;&#23454;&#29616;&#35270;&#35273;&#25925;&#20107;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Causal-Story: Local Causal Attention Utilizing Parameter-Efficient Tuning For Visual Story Synthesis. (arXiv:2309.09553v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09553
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22240;&#26524;&#25925;&#20107;&#30340;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#23616;&#37096;&#22240;&#26524;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25913;&#36827;&#35270;&#35273;&#25925;&#20107;&#21512;&#25104;&#30340;&#20840;&#23616;&#19968;&#33268;&#24615;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#21382;&#21490;&#26631;&#39064;&#12289;&#24103;&#21644;&#24403;&#21069;&#26631;&#39064;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#21270;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;&#36830;&#36143;&#35270;&#35273;&#25925;&#20107;&#30340;&#21512;&#25104;&#36827;&#23637;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23558;&#21382;&#21490;&#26631;&#39064;&#12289;&#21382;&#21490;&#24103;&#21644;&#24403;&#21069;&#26631;&#39064;&#30340;&#29305;&#24449;&#20316;&#20026;&#29983;&#25104;&#24403;&#21069;&#24103;&#30340;&#26465;&#20214;&#36827;&#34892;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#23558;&#27599;&#20010;&#21382;&#21490;&#24103;&#21644;&#26631;&#39064;&#37117;&#35270;&#20026;&#21516;&#26679;&#30340;&#36129;&#29486;&#65292;&#24182;&#20197;&#30456;&#31561;&#30340;&#26435;&#37325;&#23558;&#23427;&#20204;&#36830;&#25509;&#36215;&#26469;&#65292;&#24573;&#35270;&#20102;&#24182;&#38750;&#25152;&#26377;&#21382;&#21490;&#26465;&#20214;&#37117;&#19982;&#29983;&#25104;&#24403;&#21069;&#24103;&#30456;&#20851;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#25925;&#20107;&#12290;&#35813;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#32771;&#34385;&#20808;&#21069;&#26631;&#39064;&#12289;&#24103;&#21644;&#24403;&#21069;&#26631;&#39064;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#23616;&#37096;&#22240;&#26524;&#27880;&#24847;&#26426;&#21046;&#12290;&#36890;&#36807;&#26681;&#25454;&#36825;&#31181;&#20851;&#31995;&#20998;&#37197;&#26435;&#37325;&#65292;&#22240;&#26524;&#25925;&#20107;&#29983;&#25104;&#24403;&#21069;&#24103;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25925;&#20107;&#29983;&#25104;&#30340;&#20840;&#23616;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#22312;PororoSV&#21644;FlintstonesSV&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The excellent text-to-image synthesis capability of diffusion models has driven progress in synthesizing coherent visual stories. The current state-of-the-art method combines the features of historical captions, historical frames, and the current captions as conditions for generating the current frame. However, this method treats each historical frame and caption as the same contribution. It connects them in order with equal weights, ignoring that not all historical conditions are associated with the generation of the current frame. To address this issue, we propose Causal-Story. This model incorporates a local causal attention mechanism that considers the causal relationship between previous captions, frames, and current captions. By assigning weights based on this relationship, Causal-Story generates the current frame, thereby improving the global consistency of story generation. We evaluated our model on the PororoSV and FlintstonesSV datasets and obtained state-of-the-art FID score
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;Projected Task-Specific Layers (PTSL)&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#23618;&#26469;&#34920;&#36798;&#20849;&#20139;&#21644;&#21487;&#21464;&#30340;&#20219;&#21153;&#20449;&#24687;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25512;&#24191;&#21644;&#24178;&#25200;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08776</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Projected Task-Specific Layers
&lt;/p&gt;
&lt;p&gt;
Projected Task-Specific Layers for Multi-Task Reinforcement Learning. (arXiv:2309.08776v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;Projected Task-Specific Layers (PTSL)&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#23618;&#26469;&#34920;&#36798;&#20849;&#20139;&#21644;&#21487;&#21464;&#30340;&#20219;&#21153;&#20449;&#24687;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25512;&#24191;&#21644;&#24178;&#25200;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#23478;&#24237;&#21644;&#24037;&#20316;&#22330;&#25152;&#30340;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#35268;&#27169;&#21270;&#12290;&#28982;&#32780;&#65292;&#20174;&#19968;&#20010;&#20219;&#21153;&#25512;&#24191;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#24182;&#20943;&#36731;&#36127;&#38754;&#20219;&#21153;&#24178;&#25200;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25104;&#21151;&#22320;&#22312;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#24182;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#23558;&#21462;&#20915;&#20110;&#23545;&#20219;&#21153;&#24213;&#23618;&#32467;&#26500;&#30340;&#26377;&#25928;&#25429;&#25417;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#21363;Projected Task-Specific Layers&#65288;PTSL&#65289;&#65292;&#23427;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#23618;&#65292;&#36890;&#36807;&#31264;&#23494;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#20462;&#27491;&#26469;&#26356;&#22909;&#22320;&#34920;&#36798;&#20849;&#20139;&#21644;&#21487;&#21464;&#30340;&#20219;&#21153;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Meta-World&#30340;MT10&#21644;MT50&#22522;&#20934;&#20013;&#65288;&#21253;&#25324;Sawyer&#26426;&#22120;&#20154;&#33218;&#19978;&#30340;10&#20010;&#21644;50&#20010;&#30446;&#26631;&#26465;&#20214;&#20219;&#21153;&#65289;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task reinforcement learning could enable robots to scale across a wide variety of manipulation tasks in homes and workplaces. However, generalizing from one task to another and mitigating negative task interference still remains a challenge. Addressing this challenge by successfully sharing information across tasks will depend on how well the structure underlying the tasks is captured. In this work, we introduce our new architecture, Projected Task-Specific Layers (PTSL), that leverages a common policy with dense task-specific corrections through task-specific layers to better express shared and variable task information. We then show that our model outperforms the state of the art on the MT10 and MT50 benchmarks of Meta-World consisting of 10 and 50 goal-conditioned tasks for a Sawyer arm.
&lt;/p&gt;</description></item><item><title>PyGraft&#26159;&#19968;&#20010;Python&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#29983;&#25104;&#39640;&#24230;&#23450;&#21046;&#30340;&#27169;&#24335;&#21644;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#30830;&#20445;&#29983;&#25104;&#30340;&#36164;&#28304;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03685</link><description>&lt;p&gt;
PyGraft: &#22312;&#20320;&#30340;&#25351;&#23574;&#29983;&#25104;&#21487;&#37197;&#32622;&#30340;&#27169;&#24335;&#21644;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
PyGraft: Configurable Generation of Schemas and Knowledge Graphs at Your Fingertips. (arXiv:2309.03685v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03685
&lt;/p&gt;
&lt;p&gt;
PyGraft&#26159;&#19968;&#20010;Python&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#29983;&#25104;&#39640;&#24230;&#23450;&#21046;&#30340;&#27169;&#24335;&#21644;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#30830;&#20445;&#29983;&#25104;&#30340;&#36164;&#28304;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#25968;&#25454;&#34920;&#31034;&#21644;&#31649;&#29702;&#33539;&#24335;&#12290;KG&#36890;&#24120;&#22522;&#20110;&#27169;&#24335;&#65288;&#20363;&#22914;&#26412;&#20307;&#35770;&#65289;&#26469;&#25429;&#33719;&#20107;&#23454;&#20449;&#24687;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#12290;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19968;&#20123;KG&#24050;&#32463;&#25104;&#20026;&#26631;&#20934;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20165;&#20381;&#36182;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#21512;&#26159;&#19981;&#36275;&#20197;&#35780;&#20272;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#12290;&#22312;&#19968;&#20123;&#25968;&#25454;&#25935;&#24863;&#39046;&#22495;&#65292;&#22914;&#25945;&#32946;&#25110;&#21307;&#23398;&#65292;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#33719;&#21462;&#26356;&#21152;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;PyGraft&#65292;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#24230;&#23450;&#21046;&#30340;&#12289;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#27169;&#24335;&#21644;&#30693;&#35782;&#22270;&#35889;&#12290;&#21512;&#25104;&#30340;&#27169;&#24335;&#21253;&#25324;&#21508;&#31181;RDFS&#21644;OWL&#26500;&#36896;&#65292;&#32780;&#21512;&#25104;&#30340;KG&#21017;&#27169;&#25311;&#20102;&#30495;&#23454;KG&#30340;&#29305;&#24615;&#21644;&#35268;&#27169;&#12290;&#36890;&#36807;&#36816;&#34892;&#25551;&#36848;&#36923;&#36753;&#65288;DL&#65289;&#25512;&#29702;&#22120;&#65292;&#26368;&#32456;&#30830;&#20445;&#29983;&#25104;&#36164;&#28304;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) have emerged as a prominent data representation and management paradigm. Being usually underpinned by a schema (e.g. an ontology), KGs capture not only factual information but also contextual knowledge. In some tasks, a few KGs established themselves as standard benchmarks. However, recent works outline that relying on a limited collection of datasets is not sufficient to assess the generalization capability of an approach. In some data-sensitive fields such as education or medicine, access to public datasets is even more limited. To remedy the aforementioned issues, we release PyGraft, a Python-based tool that generates highly customized, domain-agnostic schemas and knowledge graphs. The synthesized schemas encompass various RDFS and OWL constructs, while the synthesized KGs emulate the characteristics and scale of real-world KGs. Logical consistency of the generated resources is ultimately ensured by running a description logic (DL) reasoner. By providing a way
&lt;/p&gt;</description></item><item><title>&#22312;&#31070;&#32463;&#21147;&#22330;&#27169;&#22411;&#20013;&#65292;&#24120;&#29992;&#30340;MD17&#25968;&#25454;&#38598;&#23545;&#20110;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#37319;&#26679;&#33258;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65292;&#21253;&#21547;&#20102;&#33021;&#37327;&#21644;&#21147;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.11155</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#20986;&#24179;&#34913;&#29366;&#24577;&#30340;&#25193;&#23637;&#21160;&#21147;&#23398;&#24615;&#33021;&#35780;&#20272;&#31070;&#32463;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium. (arXiv:2308.11155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11155
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#21147;&#22330;&#27169;&#22411;&#20013;&#65292;&#24120;&#29992;&#30340;MD17&#25968;&#25454;&#38598;&#23545;&#20110;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#37319;&#26679;&#33258;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65292;&#21253;&#21547;&#20102;&#33021;&#37327;&#21644;&#21147;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21147;&#22330;&#24050;&#25104;&#20026;&#35745;&#31639;&#21270;&#23398;&#20013;&#30340;&#37325;&#35201;&#27169;&#22411;&#65292;&#21462;&#20195;&#20102;&#20174;&#22836;&#31639;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#20013;&#30340;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#12290;&#30446;&#21069;&#23545;&#31070;&#32463;&#21147;&#22330;&#30340;&#20027;&#35201;&#35780;&#20272;&#22522;&#20934;&#26159;MD17&#25968;&#25454;&#38598;&#21450;&#20854;&#21518;&#32493;&#25193;&#23637;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#20027;&#35201;&#21253;&#21547;&#26469;&#33258;&#22522;&#24577;&#21183;&#33021;&#38754;&#24179;&#34913;&#21306;&#22495;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#37319;&#26679;&#33258;&#30452;&#25509;&#32477;&#28909;&#21160;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21270;&#23398;&#21453;&#24212;&#28041;&#21450;&#21040;&#36739;&#22823;&#30340;&#20998;&#23376;&#21464;&#24418;&#65292;&#29305;&#21035;&#26159;&#38190;&#26029;&#35010;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MD17&#25968;&#25454;&#38598;&#20013;&#20869;&#22352;&#26631;&#21644;&#33021;&#37327;&#30340;&#32422;&#26463;&#20998;&#24067;&#65292;&#20984;&#26174;&#20102;&#20854;&#22312;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#37319;&#26679;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#65288;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65289;&#25968;&#25454;&#38598;&#65292;&#20174;&#38750;&#32477;&#28909;&#21160;&#21147;&#23398;&#20013;&#27966;&#29983;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20174;&#22810;&#21442;&#32771;&#27874;&#20989;&#25968;&#29702;&#35770;&#21644;&#23494;&#24230;&#27867;&#20989;&#20013;&#30830;&#23450;&#30340;&#33021;&#37327;&#21644;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural force fields (NFFs) have gained prominence in computational chemistry as surrogate models, superseding quantum-chemistry calculations in ab initio molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset and its subsequent extension. These datasets predominantly comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampling from direct adiabatic dynamics. However, many chemical reactions entail significant molecular deformations, notably bond breaking. We demonstrate the constrained distribution of internal coordinates and energies in the MD17 datasets, underscoring their inadequacy for representing systems undergoing chemical reactions. Addressing this sampling limitation, we introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived from non-adiabatic dynamics. This dataset encompasses energies and forces ascertained from both multireference wave function theory and density functional
&lt;/p&gt;</description></item><item><title>VQGraph&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#23427;&#37319;&#29992;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#12290;&#36890;&#36807; VQGraph&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2308.02117</link><description>&lt;p&gt;
VQGraph: &#22270;&#24418;&#21521;&#37327;&#37327;&#21270;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs
&lt;/p&gt;
&lt;p&gt;
VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs. (arXiv:2308.02117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02117
&lt;/p&gt;
&lt;p&gt;
VQGraph&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#23427;&#37319;&#29992;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#12290;&#36890;&#36807; VQGraph&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#20449;&#24687;&#20256;&#36882;&#65292;&#32858;&#21512;&#23616;&#37096;&#37051;&#23621;&#20197;&#26356;&#26032;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#31181;&#20449;&#24687;&#20256;&#36882;&#23548;&#33268;&#22312;&#23454;&#38469;&#30340;&#24310;&#36831;&#32422;&#26463;&#24212;&#29992;&#31243;&#24207;&#20013;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36890;&#36807;&#27169;&#20223;GNN&#30340;&#36755;&#20986;&#26469;&#23398;&#20064;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#34920;&#31034;&#31354;&#38388;&#21487;&#33021;&#19981;&#36275;&#20197;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#65292;&#36825;&#38480;&#21046;&#20102;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;VQGraph&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#21464;&#20307;&#30340;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#23427;&#23558;&#22810;&#26679;&#21270;&#30340;&#23616;&#37096;&#32467;&#26500;&#33410;&#28857;&#26126;&#30830;&#34920;&#31034;&#20026;&#22823;&#37327;&#31163;&#25955;&#20196;&#29260;&#65292;&#24182;&#26500;&#25104;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#20195;&#30721;&#20070;&#12290;&#37197;&#22791;&#20102;&#23398;&#20064;&#30340;&#20195;&#30721;&#20070;&#65292;&#25105;&#20204;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) conduct message passing which aggregates local neighbors to update node representations. Such message passing leads to scalability issues in practical latency-constrained applications. To address this issue, recent methods adopt knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (MLP) by mimicking the output of GNN. However, the existing GNN representation space may not be expressive enough for representing diverse local structures of the underlying graph, which limits the knowledge transfer from GNN to MLP. Here we present a novel framework VQGraph to learn a powerful graph representation space for bridging GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which explicitly represents the nodes of diverse local structures as numerous discrete tokens and constitutes a meaningful codebook. Equipped with the learned codebook, we propos
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20869;&#37096;&#30340;&#20540;&#31354;&#38388;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01154</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#36816;&#31639;&#65306;&#20174;&#35760;&#24518;&#21040;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Arithmetic with Language Models: from Memorization to Computation. (arXiv:2308.01154v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20869;&#37096;&#30340;&#20540;&#31354;&#38388;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26356;&#22909;&#22320;&#29702;&#35299;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#24615;&#35745;&#31639;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#23545;&#20110;&#36827;&#19968;&#27493;&#25913;&#36827;&#23427;&#20204;&#24182;&#25299;&#23485;&#20854;&#36866;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22312;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#12290;&#20108;&#36827;&#21046;&#21152;&#27861;&#21644;&#20056;&#27861;&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#27979;&#35797;&#22522;&#30784;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#38750;&#24120;&#23567;&#30340;&#35789;&#27719;&#34920;&#65292;&#24182;&#19988;&#22312;&#36755;&#20837;/&#36755;&#20986;&#19978;&#23637;&#31034;&#20102;&#30456;&#20851;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#20351;&#24471;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#24179;&#28369;&#30340;&#36755;&#20837;&#25554;&#20540;&#26080;&#25928;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#23398;&#20064;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20854;&#22806;&#25512;&#33021;&#21147;&#21644;&#20869;&#37096;&#20449;&#24687;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#36825;&#26679;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#32534;&#30721;-&#22238;&#24402;-&#35299;&#30721;&#26426;&#22120;&#65292;&#19968;&#26086;&#23558;&#36755;&#20837;&#26631;&#35760;&#34920;&#31034;&#26144;&#23556;&#21040;&#21512;&#36866;&#30340;&#20869;&#37096;&#20540;&#31354;&#38388;&#65292;&#35745;&#31639;&#23601;&#22312;&#20540;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypotheses that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate intern
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00071</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#26041;&#27861;&#29992;&#20110;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Interpretable Stereotype Identification through Reasoning. (arXiv:2308.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20351;&#29992;&#20102;&#21253;&#21547;&#22266;&#26377;&#20559;&#35265;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#25345;&#32493;&#31995;&#32479;&#24615;&#27495;&#35270;&#65292;&#22240;&#27492;&#65292;&#23457;&#26597;&#21644;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#23558;&#20844;&#24179;&#24615;&#25972;&#21512;&#21040;&#23427;&#20204;&#30340;&#21457;&#23637;&#20013;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20844;&#27491;&#21644;&#26080;&#20559;&#30340;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Vicuna-13B-v1.3&#30340;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#25105;&#20204;&#35266;&#23519;&#21040;&#20174;13B&#21040;33B&#30340;&#35268;&#27169;&#25193;&#23637;&#20250;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25512;&#29702;&#21487;&#33021;&#26159;&#20351;LLMs&#22312;&#21051;&#26495;&#21360;&#35937;&#31561;&#39046;&#22495;&#20219;&#21153;&#19978;&#36229;&#36234;&#35268;&#27169;&#23450;&#24459;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#36873;&#23450;&#30340;&#25512;&#29702;&#36861;&#36394;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#31361;&#20986;&#26174;&#31034;&#20102;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26631;&#31614;&#20165;&#40657;&#30418;&#22330;&#26223;&#19979;&#30340;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#24674;&#22797;&#30446;&#26631;&#30340;&#31934;&#30830;&#26679;&#26412;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.08424</link><description>&lt;p&gt;
&#26080;&#27861;&#38459;&#27490;&#30340;&#25915;&#20987;: &#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26631;&#31614;&#20165;&#27169;&#22411;&#36870;&#25512;
&lt;/p&gt;
&lt;p&gt;
Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion Model. (arXiv:2307.08424v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26631;&#31614;&#20165;&#40657;&#30418;&#22330;&#26223;&#19979;&#30340;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#24674;&#22797;&#30446;&#26631;&#30340;&#31934;&#30830;&#26679;&#26412;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;(MIAs)&#26088;&#22312;&#20174;&#30446;&#26631;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#24674;&#22797;&#31169;&#23494;&#25968;&#25454;&#65292;&#36825;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#26500;&#25104;&#23041;&#32961;&#12290;MIAs&#20027;&#35201;&#20851;&#27880;&#30333;&#30418;&#24773;&#26223;&#65292;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23436;&#20840;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#26159;&#40657;&#30418;&#24773;&#26223;&#65292;&#23545;&#25163;&#24456;&#38590;&#33719;&#21462;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#35768;&#22810;&#27169;&#22411;&#20165;&#36755;&#20986;&#39044;&#27979;&#26631;&#31614;&#12290;&#29616;&#26377;&#30340;&#40657;&#30418;MIAs&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#20248;&#21270;&#31574;&#30053;&#19978;&#65292;&#32780;&#29983;&#25104;&#27169;&#22411;&#21482;&#26159;&#20174;&#30333;&#30418;MIA&#20013;&#20351;&#29992;&#30340;GAN&#36801;&#31227;&#32780;&#26469;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#26631;&#31614;&#20165;&#40657;&#30418;&#24773;&#26223;&#19979;&#21487;&#34892;&#25915;&#20987;&#27169;&#22411;&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MIA&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#30446;&#26631;&#27169;&#22411;&#36755;&#20986;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#20248;&#21270;&#26469;&#24674;&#22797;&#30446;&#26631;&#30340;&#31934;&#30830;&#26679;&#26412;&#12290;&#24341;&#20837;&#20102;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Model inversion attacks (MIAs) are aimed at recovering private data from a target model's training set, which poses a threat to the privacy of deep learning models. MIAs primarily focus on the white-box scenario where the attacker has full access to the structure and parameters of the target model. However, practical applications are black-box, it is not easy for adversaries to obtain model-related parameters, and various models only output predicted labels. Existing black-box MIAs primarily focused on designing the optimization strategy, and the generative model is only migrated from the GAN used in white-box MIA. Our research is the pioneering study of feasible attack models in label-only black-box scenarios, to the best of our knowledge.  In this paper, we develop a novel method of MIA using the conditional diffusion model to recover the precise sample of the target without any extra optimization, as long as the target model outputs the label. Two primary techniques are introduced t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#65292;&#24182;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09329</link><description>&lt;p&gt;
BERTTM: &#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#36827;&#34892;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
BERTTM: Leveraging Contextualized Word Embeddings from Pre-trained Language Models for Neural Topic Modeling. (arXiv:2305.09329v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#65292;&#24182;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20027;&#39064;&#24314;&#27169;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#25198;&#28436;&#30528;&#26085;&#30410;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20027;&#39064;&#27169;&#22411;&#20173;&#28982;&#20381;&#36182;&#20110;&#35789;&#34955;&#65288;BoW&#65289;&#20449;&#24687;&#65292;&#26080;&#35770;&#26159;&#20316;&#20026;&#35757;&#32451;&#36755;&#20837;&#36824;&#26159;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#25417;&#25991;&#26723;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#23548;&#33268;&#23427;&#20204;&#22312;&#22788;&#29702;&#26032;&#25991;&#26723;&#20013;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#21333;&#35789;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#22312;&#35789;&#20041;&#28040;&#27495;&#30340;&#33021;&#21147;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;OOV&#21333;&#35789;&#26102;&#26159;&#26377;&#25928;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of neural topic models in recent years, topic modelling is playing an increasingly important role in natural language understanding. However, most existing topic models still rely on bag-of-words (BoW) information, either as training input or training target. This limits their ability to capture word order information in documents and causes them to suffer from the out-of-vocabulary (OOV) issue, i.e. they cannot handle unobserved words in new documents. Contextualized word embeddings from pre-trained language models show superiority in the ability of word sense disambiguation and prove to be effective in dealing with OOV words. In this work, we developed a novel neural topic model combining contextualized word embeddings from the pre-trained language model BERT. The model can infer the topic distribution of a document without using any BoW information. In addition, the model can infer the topic distribution of each word in a document directly from the contextualize
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#21306;&#20998;&#19981;&#21516;&#36136;&#37327;&#30340;&#36127;&#26679;&#26412;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.00286</link><description>&lt;p&gt;
&#38024;&#23545;&#38142;&#25509;&#39044;&#27979;&#65292;&#23545;&#24453;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#26377;&#24046;&#24322;&#24615;&#65306;&#21033;&#29992;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#20016;&#23500;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction. (arXiv:2303.00286v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00286
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#21306;&#20998;&#19981;&#21516;&#36136;&#37327;&#30340;&#36127;&#26679;&#26412;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65288;KGEMs&#65289;&#29992;&#20110;&#19982;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30456;&#20851;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#38142;&#25509;&#39044;&#27979;&#12290;&#23427;&#20204;&#20351;&#29992;&#32771;&#34385;&#20102;&#19968;&#25209;&#24471;&#20998;&#19977;&#20803;&#32452;&#21450;&#20854;&#30456;&#24212;&#26631;&#31614;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#20256;&#32479;&#26041;&#27861;&#35748;&#20026;&#19977;&#20803;&#32452;&#30340;&#26631;&#31614;&#35201;&#20040;&#20026;&#30495;&#65292;&#35201;&#20040;&#20026;&#20551;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;&#30340;&#36127;&#26679;&#26412;&#24212;&#35813;&#34987;&#24179;&#31561;&#23545;&#24453;&#12290;&#19982;&#36825;&#19968;&#26368;&#36817;&#30340;&#20551;&#35774;&#19968;&#33268;&#65292;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#22312;&#35821;&#20041;&#19978;&#26377;&#25928;&#30340;&#36127;&#26679;&#26412;&#21487;&#33021;&#26159;&#39640;&#36136;&#37327;&#30340;&#36127;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#25439;&#22833;&#20989;&#25968;&#24212;&#35813;&#23558;&#23427;&#20204;&#19982;&#35821;&#20041;&#19978;&#26080;&#25928;&#30340;&#36127;&#26679;&#26412;&#21306;&#21035;&#23545;&#24453;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#38024;&#23545;&#38142;&#25509;&#39044;&#27979;&#30340;&#19977;&#20010;&#20027;&#35201;&#25439;&#22833;&#20989;&#25968;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#29256;&#26412;&#12290;&#36890;&#36807;&#24191;&#27867;&#21644;&#21463;&#25511;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#19977;&#20010;&#20855;&#26377;&#19981;&#21516;&#27169;&#24335;&#30340;&#20844;&#20849;&#22522;&#20934;KG&#19978;&#31995;&#32479;&#22320;&#25552;&#20379;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding models (KGEMs) are used for various tasks related to knowledge graphs (KGs), including link prediction. They are trained with loss functions that are computed considering a batch of scored triples and their corresponding labels. Traditional approaches consider the label of a triple to be either true or false. However, recent works suggest that all negative triples should not be valued equally. In line with this recent assumption, we posit that negative triples that are semantically valid w.r.t. domain and range constraints might be high-quality negative triples. As such, loss functions should treat them differently from semantically invalid negative ones. To this aim, we propose semantic-driven versions for the three main loss functions for link prediction. In an extensive and controlled experimental setting, we show that the proposed loss functions systematically provide satisfying results on three public benchmark KGs underpinned with different schemas, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#23545;&#30340;&#26368;&#20248;&#35774;&#32622;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#30340;&#29305;&#27530;&#20043;&#22788;&#22312;&#20110;&#22823;&#37096;&#20998;&#21487;&#33021;&#30340;&#20132;&#26131;&#23545;&#20043;&#38388;&#30340;&#20132;&#26131;&#37327;&#26080;&#27861;&#30452;&#25509;&#35266;&#23519;&#65292;&#19988;&#38656;&#35201;&#28385;&#36275;&#36830;&#36890;&#24615;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2210.10971</link><description>&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#23545;&#30340;&#26368;&#20248;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
Optimal Settings for Cryptocurrency Trading Pairs. (arXiv:2210.10971v2 [q-fin.TR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#23545;&#30340;&#26368;&#20248;&#35774;&#32622;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#30340;&#29305;&#27530;&#20043;&#22788;&#22312;&#20110;&#22823;&#37096;&#20998;&#21487;&#33021;&#30340;&#20132;&#26131;&#23545;&#20043;&#38388;&#30340;&#20132;&#26131;&#37327;&#26080;&#27861;&#30452;&#25509;&#35266;&#23519;&#65292;&#19988;&#38656;&#35201;&#28385;&#36275;&#36830;&#36890;&#24615;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#30340;&#30446;&#26631;&#26159;&#21435;&#20013;&#24515;&#21270;&#65292;&#25152;&#26377;&#36135;&#24065;&#21407;&#21017;&#19978;&#22320;&#20301;&#30456;&#31561;&#12290;&#19982;&#20256;&#32479;&#32929;&#24066;&#19981;&#21516;&#65292;&#27809;&#26377;&#40664;&#35748;&#30340;&#36135;&#24065;&#21333;&#20301;&#65288;&#27861;&#24065;&#65289;&#65292;&#22240;&#27492;&#21487;&#20197;&#33258;&#30001;&#35774;&#32622;&#20132;&#26131;&#23545;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20004;&#31181;&#36135;&#24065;&#20043;&#38388;&#24314;&#31435;&#19968;&#20010;&#20132;&#26131;&#24066;&#22330;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#25511;&#21046;&#31649;&#29702;&#25104;&#26412;&#24182;&#30830;&#20445;&#36275;&#22815;&#30340;&#27969;&#21160;&#24615;&#65292;&#25105;&#20204;&#24517;&#39035;&#20248;&#20808;&#32771;&#34385;&#37027;&#20123;&#22823;&#37327;&#20132;&#26131;&#30340;&#20132;&#26131;&#23545;&#65292;&#24182;&#30830;&#20445;&#25152;&#26377;&#36135;&#24065;&#37117;&#26159;&#21487;&#20197;&#20132;&#26131;&#30340;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#36825;&#26159;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#29305;&#27530;&#20043;&#22788;&#22312;&#20110;&#65306;1&#65289;&#22823;&#37096;&#20998;&#65288;&gt;99.5%&#65289;&#21487;&#33021;&#30340;&#20132;&#26131;&#23545;&#20043;&#38388;&#30340;&#20132;&#26131;&#37327;&#26080;&#27861;&#30452;&#25509;&#35266;&#23519;&#12290;2&#65289;&#23427;&#28385;&#36275;&#36830;&#36890;&#24615;&#32422;&#26463;&#65292;&#21363;&#20445;&#35777;&#25152;&#26377;&#36135;&#24065;&#37117;&#21487;&#20197;&#20132;&#26131;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#36807;&#31243;&#65306;1&#65289;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25130;&#26029;&#29305;&#24449;&#20540;&#20998;&#35299;&#22635;&#20805;&#32570;&#22833;&#20540;&#65292;&#20854;&#20013;&#27491;&#21017;&#21270;&#39033;&#29992;&#20110;&#25511;&#21046;&#32570;&#22833;&#20540;&#34987;&#38480;&#21046;&#20026;&#38646;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of cryptocurrencies is decentralization. In principle, all currencies have equal status. Unlike traditional stock markets, there is no default currency of denomination (fiat), thus the trading pairs can be set freely. However, it is impractical to set up a trading market between every two currencies. In order to control management costs and ensure sufficient liquidity, we must give priority to covering those large-volume trading pairs and ensure that all coins are reachable. We note that this is an optimization problem. Its particularity lies in: 1) the trading volume between most (&gt;99.5%) possible trading pairs cannot be directly observed. 2) It satisfies the connectivity constraint, that is, all currencies are guaranteed to be tradable.  To solve this problem, we use a two-stage process: 1) Fill in missing values based on a regularized, truncated eigenvalue decomposition, where the regularization term is used to control what extent missing values should be limited to zero. 2
&lt;/p&gt;</description></item></channel></rss>