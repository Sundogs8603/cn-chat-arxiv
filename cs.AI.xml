<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;boosting&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00723</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#19978;&#19979;&#25991;&#20559;&#20506;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Contextual Biasing of Named-Entities with Large Language Models. (arXiv:2309.00723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;boosting&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#65292;&#21363;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#20026;LLM&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#25171;&#20998;&#26399;&#38388;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;LLM&#36827;&#34892;boosting&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20123;&#25552;&#31034;&#20449;&#24687;&#21253;&#25324;&#20559;&#20506;&#21015;&#34920;&#21644;&#23569;&#26679;&#26412;&#31034;&#20363;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#20551;&#35774;&#24471;&#20998;&#26102;&#20316;&#20026;&#38468;&#21152;&#20449;&#24687;&#12290;&#38500;&#20102;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;LLM&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#20026;&#20102;&#25552;&#39640;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#25928;&#29575;&#24182;&#36991;&#20813;&#36229;&#36807;LLMs&#30340;&#26368;&#22823;&#24207;&#21015;&#38271;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#65292;&#21363;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#39044;&#27979;&#36873;&#25321;&#26368;&#21487;&#33021;&#30340;&#31867;&#21035;&#65292;&#24182;&#20165;&#20351;&#29992;&#36825;&#20010;&#31867;&#21035;&#20013;&#30340;&#23454;&#20307;&#20316;&#20026;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#12290;&#23545;&#20869;&#37096;&#30340;&#21628;&#21483;&#12289;&#28040;&#24687;&#21644;&#21475;&#36848;&#25968;&#25454;&#38598;&#20197;&#21450;SLUE-Voxpopuli&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35789;&#38169;&#35823;&#29575;(WER)&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies contextual biasing with Large Language Models (LLMs), where during second-pass rescoring additional contextual information is provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We propose to leverage prompts for a LLM without fine tuning during rescoring which incorporate a biasing list and few-shot examples to serve as additional information when calculating the score for the hypothesis. In addition to few-shot prompt learning, we propose multi-task training of the LLM to predict both the entity class and the next token. To improve the efficiency for contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we propose dynamic prompting, where we select the most likely class using the class tag prediction, and only use entities in this class as contexts for next token prediction. Word Error Rate (WER) evaluation is performed on i) an internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli dataset. Results
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#26469;&#22686;&#24378;&#29616;&#26377;&#20132;&#36890;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#29992;&#20110;&#20132;&#36890;&#24314;&#27169;&#20013;&#30495;&#23454;&#24615;&#23545;&#40784;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#27492;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2309.00709</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#23454;&#38469;&#20132;&#36890;&#27169;&#25311;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback for Realistic Traffic Simulation. (arXiv:2309.00709v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#26469;&#22686;&#24378;&#29616;&#26377;&#20132;&#36890;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#29992;&#20110;&#20132;&#36890;&#24314;&#27169;&#20013;&#30495;&#23454;&#24615;&#23545;&#40784;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#27492;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#30495;&#23454;&#19990;&#30028;&#27979;&#35797;&#30340;&#25361;&#25112;&#21644;&#25104;&#26412;&#65292;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24320;&#21457;&#32773;&#36890;&#24120;&#20381;&#36182;&#27169;&#25311;&#27979;&#35797;&#26469;&#21019;&#24314;&#21487;&#38752;&#30340;&#31995;&#32479;&#12290;&#26377;&#25928;&#27169;&#25311;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#34701;&#20837;&#19982;&#20154;&#31867;&#30693;&#35782;&#30456;&#19968;&#33268;&#30340;&#30495;&#23454;&#20132;&#36890;&#27169;&#22411;&#65292;&#36825;&#19968;&#26041;&#38754;&#22240;&#20026;&#38656;&#35201;&#24179;&#34913;&#30495;&#23454;&#24615;&#21644;&#22810;&#26679;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#20559;&#22909;&#65288;RLHF&#65289;&#26469;&#22686;&#24378;&#29616;&#26377;&#20132;&#36890;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#30740;&#31350;&#36824;&#30830;&#23450;&#20102;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#25429;&#25417;&#20154;&#31867;&#23545;&#30495;&#23454;&#24615;&#30340;&#24494;&#22937;&#20559;&#22909;&#20197;&#21450;&#32479;&#19968;&#22810;&#26679;&#30340;&#20132;&#36890;&#27169;&#25311;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#37319;&#29992;RLHF&#22240;&#20854;&#26679;&#26412;&#25928;&#29575;&#39640;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#20132;&#36890;&#24314;&#27169;&#20013;&#30495;&#23454;&#24615;&#23545;&#40784;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#27492;&#31867;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21517;&#20026;TrafficRLHF&#65292;&#22312;&#29983;&#25104;&#29616;&#23454;&#19990;&#30028;&#33324;&#30340;&#20132;&#36890;&#27169;&#25311;&#25968;&#25454;&#26041;&#38754;&#23637;&#29616;&#20102;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In light of the challenges and costs of real-world testing, autonomous vehicle developers often rely on testing in simulation for the creation of reliable systems. A key element of effective simulation is the incorporation of realistic traffic models that align with human knowledge, an aspect that has proven challenging due to the need to balance realism and diversity. This works aims to address this by developing a framework that employs reinforcement learning with human preference (RLHF) to enhance the realism of existing traffic models. This study also identifies two main challenges: capturing the nuances of human preferences on realism and the unification of diverse traffic simulation models. To tackle these issues, we propose using human feedback for alignment and employ RLHF due to its sample efficiency. We also introduce the first dataset for realism alignment in traffic modeling to support such research. Our framework, named TrafficRLHF, demonstrates its proficiency in generati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28201;&#24230;&#20316;&#20026;&#19968;&#20010;&#38750;&#37327;&#23376;&#21644;&#38750;&#30456;&#23545;&#35770;&#31890;&#23376;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GCN&#21644;GAT&#27169;&#22411;&#30340;&#21508;&#20010;&#23618;&#27425;&#20013;&#30340;&#28201;&#24230;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#26410;&#26469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.00699</link><description>&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;: &#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#28201;&#24230;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Geometric Deep Learning: a Temperature Based Analysis of Graph Neural Networks. (arXiv:2309.00699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28201;&#24230;&#20316;&#20026;&#19968;&#20010;&#38750;&#37327;&#23376;&#21644;&#38750;&#30456;&#23545;&#35770;&#31890;&#23376;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GCN&#21644;GAT&#27169;&#22411;&#30340;&#21508;&#20010;&#23618;&#27425;&#20013;&#30340;&#28201;&#24230;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#26410;&#26469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35270;&#20026;&#28909;&#21147;&#23398;&#31995;&#32479;&#65292;&#23558;&#26435;&#37325;&#30475;&#20316;&#38750;&#37327;&#23376;&#21644;&#38750;&#30456;&#23545;&#35770;&#31890;&#23376;&#12290;&#25105;&#20204;&#37319;&#29992;&#20043;&#21069;&#22312;[7]&#20013;&#23450;&#20041;&#30340;&#28201;&#24230;&#27010;&#24565;&#65292;&#24182;&#30740;&#31350;&#20102;GCN&#21644;GAT&#27169;&#22411;&#30340;&#21508;&#20010;&#23618;&#27425;&#20013;&#30340;&#28201;&#24230;&#12290;&#35752;&#35770;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#28508;&#22312;&#26410;&#26469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine a Geometric Deep Learning model as a thermodynamic system treating the weights as non-quantum and non-relativistic particles. We employ the notion of temperature previously defined in [7] and study it in the various layers for GCN and GAT models. Potential future applications of our findings are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21160;&#24577;&#23398;&#20064;&#20013;&#20849;&#21516;&#25506;&#32034;&#23458;&#25143;&#28418;&#31227;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21463;&#25511;&#30340;&#27979;&#35797;&#29615;&#22659;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23458;&#25143;&#28418;&#31227;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#32972;&#21518;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#29983;&#25104;&#19968;&#20010;3D&#22320;&#24418;&#22270;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00688</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#23398;&#20064;&#20013;&#20849;&#21516;&#25506;&#32034;&#23458;&#25143;&#28418;&#31227;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Jointly Exploring Client Drift and Catastrophic Forgetting in Dynamic Learning. (arXiv:2309.00688v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21160;&#24577;&#23398;&#20064;&#20013;&#20849;&#21516;&#25506;&#32034;&#23458;&#25143;&#28418;&#31227;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21463;&#25511;&#30340;&#27979;&#35797;&#29615;&#22659;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23458;&#25143;&#28418;&#31227;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#32972;&#21518;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#29983;&#25104;&#19968;&#20010;3D&#22320;&#24418;&#22270;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#21644;&#36830;&#32493;&#23398;&#20064;&#24050;&#32463;&#34987;&#25552;&#20986;&#20316;&#20026;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#31283;&#20581;&#21644;&#38544;&#31169;&#24847;&#35782;&#30340;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#28508;&#22312;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#28418;&#31227;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#26159;&#20445;&#35777;&#19968;&#33268;&#24615;&#24615;&#33021;&#30340;&#22522;&#26412;&#38556;&#30861;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#21482;&#26159;&#20998;&#21035;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24573;&#35270;&#20102;&#36825;&#20004;&#31181;&#24615;&#33021;&#19979;&#38477;&#32972;&#21518;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#32852;&#31995;&#22312;&#19968;&#36215;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#21463;&#25511;&#30340;&#27979;&#35797;&#29615;&#22659;&#65292;&#20197;&#25506;&#32034;&#23458;&#25143;&#28418;&#31227; - &#36890;&#36807;&#25200;&#21160;&#19968;&#23450;&#27604;&#20363;&#30340;&#23458;&#25143; - &#21644;&#28798;&#38590;&#24615;&#36951;&#24536; - &#36890;&#36807;&#20197;&#29305;&#23450;&#24378;&#24230;&#36801;&#31227;&#25152;&#26377;&#23458;&#25143; - &#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#19968;&#20010;&#20174;&#20004;&#32773;&#30340;&#32452;&#21512;&#24615;&#33021;&#24433;&#21709;&#20135;&#29983;&#30340;3D&#22320;&#24418;&#22270;&#36827;&#19968;&#27493;&#21033;&#29992;&#36825;&#31181;&#26032;&#30340;&#32452;&#21512;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#23458;&#25143;&#28418;&#31227;&#24341;&#36215;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36890;&#36807;&#19968;&#23450;&#27604;&#20363;&#30340;&#36801;&#31227;&#23458;&#25143;&#65292;&#19982;&#30001;&#30456;&#24212;&#30340;&#36801;&#31227;&#24378;&#24230;&#24341;&#36215;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24615;&#33021;&#19979;&#38477;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated and Continual Learning have emerged as potential paradigms for the robust and privacy-aware use of Deep Learning in dynamic environments. However, Client Drift and Catastrophic Forgetting are fundamental obstacles to guaranteeing consistent performance. Existing work only addresses these problems separately, which neglects the fact that the root cause behind both forms of performance deterioration is connected. We propose a unified analysis framework for building a controlled test environment for Client Drift -- by perturbing a defined ratio of clients -- and Catastrophic Forgetting -- by shifting all clients with a particular strength. Our framework further leverages this new combined analysis by generating a 3D landscape of the combined performance impact from both. We demonstrate that the performance drop through Client Drift, caused by a certain share of shifted clients, is correlated to the drop from Catastrophic Forgetting resulting from a corresponding shift strength. 
&lt;/p&gt;</description></item><item><title>ICDARTS&#25913;&#36827;&#20102;&#24490;&#29615;DARTS&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#65292;&#36890;&#36807;&#28040;&#38500;&#35780;&#20272;&#32593;&#32476;&#26435;&#37325;&#23545;&#25628;&#32034;&#32593;&#32476;&#26435;&#37325;&#30340;&#20381;&#36182;&#65292;&#24182;&#24341;&#20837;&#20462;&#25913;&#21518;&#30340;&#25628;&#32034;&#31163;&#25955;&#21270;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.00664</link><description>&lt;p&gt;
ICDARTS: &#25913;&#36827;&#24490;&#29615;DARTS&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
ICDARTS: Improving the Stability and Performance of Cyclic DARTS. (arXiv:2309.00664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00664
&lt;/p&gt;
&lt;p&gt;
ICDARTS&#25913;&#36827;&#20102;&#24490;&#29615;DARTS&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#65292;&#36890;&#36807;&#28040;&#38500;&#35780;&#20272;&#32593;&#32476;&#26435;&#37325;&#23545;&#25628;&#32034;&#32593;&#32476;&#26435;&#37325;&#30340;&#20381;&#36182;&#65292;&#24182;&#24341;&#20837;&#20462;&#25913;&#21518;&#30340;&#25628;&#32034;&#31163;&#25955;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#24490;&#29615;DARTS&#65288;CDARTS&#65289;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;CDARTS&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#26550;&#26500;&#25628;&#32034;&#65288;DARTS&#65289;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#65292;&#20351;&#29992;&#24490;&#29615;&#21453;&#39304;&#26426;&#21046;&#21516;&#26102;&#35757;&#32451;&#25628;&#32034;&#21644;&#35780;&#20272;&#32593;&#32476;&#12290;&#35813;&#35757;&#32451;&#21327;&#35758;&#26088;&#22312;&#36890;&#36807;&#24378;&#21046;&#35201;&#27714;&#25628;&#32034;&#21644;&#35780;&#20272;&#32593;&#32476;&#20135;&#29983;&#30456;&#20284;&#30340;&#36755;&#20986;&#26469;&#20248;&#21270;&#25628;&#32034;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;CDARTS&#24341;&#20837;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;&#25628;&#32034;&#32593;&#32476;&#30340;&#35780;&#20272;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;&#12290;&#25628;&#32034;&#21644;&#37325;&#26032;&#35757;&#32451;&#38454;&#27573;&#35780;&#20272;&#32593;&#32476;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#19981;&#30456;&#20284;&#24615;&#23548;&#33268;&#25628;&#32034;&#38454;&#27573;&#35780;&#20272;&#32593;&#32476;&#25104;&#20026;&#37325;&#26032;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30340;&#26368;&#32456;&#35780;&#20272;&#32593;&#32476;&#30340;&#27425;&#20248;&#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;ICDARTS&#65292;&#19968;&#31181;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#35780;&#20272;&#32593;&#32476;&#26435;&#37325;&#23545;&#25628;&#32034;&#32593;&#32476;&#26435;&#37325;&#30340;&#20381;&#36182;&#65292;&#20197;&#21450;&#19968;&#31181;&#20462;&#25913;&#30340;&#25628;&#32034;&#31163;&#25955;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces improvements to the stability and generalizability of Cyclic DARTS (CDARTS). CDARTS is a Differentiable Architecture Search (DARTS)-based approach to neural architecture search (NAS) that uses a cyclic feedback mechanism to train search and evaluation networks concurrently. This training protocol aims to optimize the search process by enforcing that the search and evaluation networks produce similar outputs. However, CDARTS introduces a loss function for the evaluation network that is dependent on the search network. The dissimilarity between the loss functions used by the evaluation networks during the search and retraining phases results in a search-phase evaluation network that is a sub-optimal proxy for the final evaluation network that is utilized during retraining. We present ICDARTS, a revised approach that eliminates the dependency of the evaluation network weights upon those of the search network, along with a modified process for discretizing the search n
&lt;/p&gt;</description></item><item><title>&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;AI&#27169;&#22411;&#30340;&#20154;&#24037;&#29983;&#25104;&#25968;&#25454;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#30740;&#31350;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#19982;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20559;&#35265;&#30340;&#26426;&#20250;&#65292;&#20294;&#21516;&#26102;&#20063;&#38656;&#35201;&#21046;&#23450;&#21512;&#36866;&#30340;&#25919;&#31574;&#26469;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#12289;&#30495;&#23454;&#24615;&#20197;&#21450;&#24179;&#34913;&#38544;&#31169;&#19982;&#25968;&#25454;&#25928;&#29992;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.00652</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#35757;&#32451;AI&#27169;&#22411;&#65306;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#26426;&#36935;&#19982;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
The Use of Synthetic Data to Train AI Models: Opportunities and Risks for Sustainable Development. (arXiv:2309.00652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00652
&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;AI&#27169;&#22411;&#30340;&#20154;&#24037;&#29983;&#25104;&#25968;&#25454;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#30740;&#31350;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#19982;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20559;&#35265;&#30340;&#26426;&#20250;&#65292;&#20294;&#21516;&#26102;&#20063;&#38656;&#35201;&#21046;&#23450;&#21512;&#36866;&#30340;&#25919;&#31574;&#26469;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#12289;&#30495;&#23454;&#24615;&#20197;&#21450;&#24179;&#34913;&#38544;&#31169;&#19982;&#25968;&#25454;&#25928;&#29992;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#25968;&#25454;&#39537;&#21160;&#30340;&#26102;&#20195;&#65292;&#21512;&#25104;&#25968;&#25454;&#65292;&#21363;&#20154;&#24037;&#29983;&#25104;&#30340;&#31867;&#20284;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#29305;&#24449;&#20294;&#19981;&#21253;&#21547;&#23454;&#38469;&#20010;&#20154;&#20449;&#24687;&#30340;&#25968;&#25454;&#65292;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#26159;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#20445;&#25252;&#38544;&#31169;&#12289;&#22686;&#21152;&#30740;&#31350;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#21019;&#24314;&#12289;&#21033;&#29992;&#21644;&#20256;&#25773;&#25919;&#31574;&#12290;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#26159;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#30340;&#26377;&#21147;&#24037;&#20855;&#65292;&#20294;&#20063;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#30830;&#20445;&#20854;&#36136;&#37327;&#21644;&#30495;&#23454;&#24615;&#12290;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#21512;&#25104;&#25968;&#25454;&#25919;&#31574;&#24517;&#39035;&#22312;&#38544;&#31169;&#20851;&#27880;&#21644;&#25968;&#25454;&#25928;&#29992;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#30830;&#20445;&#22312;&#19981;&#25439;&#23475;&#36947;&#24503;&#25110;&#27861;&#24459;&#26631;&#20934;&#30340;&#21069;&#25552;&#19979;&#26377;&#25928;&#21033;&#29992;&#25968;&#25454;&#12290;&#32452;&#32455;&#21644;&#26426;&#26500;&#24517;&#39035;&#21046;&#23450;&#26631;&#20934;&#21270;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#22909;&#22788;&#24182;&#35299;&#20915;&#20854;&#20013;&#30340;&#22266;&#26377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current data driven era, synthetic data, artificially generated data that resembles the characteristics of real world data without containing actual personal information, is gaining prominence. This is due to its potential to safeguard privacy, increase the availability of data for research, and reduce bias in machine learning models. This paper investigates the policies governing the creation, utilization, and dissemination of synthetic data. Synthetic data can be a powerful instrument for protecting the privacy of individuals, but it also presents challenges, such as ensuring its quality and authenticity. A well crafted synthetic data policy must strike a balance between privacy concerns and the utility of data, ensuring that it can be utilized effectively without compromising ethical or legal standards. Organizations and institutions must develop standardized guidelines and best practices in order to capitalize on the benefits of synthetic data while addressing its inherent c
&lt;/p&gt;</description></item><item><title>GPT&#36890;&#36807;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#26174;&#31034;&#20986;&#20855;&#22791;&#25104;&#20026;&#22823;&#20247;&#37329;&#34701;&#26426;&#22120;&#39038;&#38382;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#22522;&#20110;GPT-4&#30340;ChatGPT&#20960;&#20046;&#23436;&#32654;&#22320;&#24471;&#20998;99%&#65292;&#25581;&#31034;&#20102;&#37329;&#34701;&#32032;&#20859;&#27491;&#22312;&#25104;&#20026;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00649</link><description>&lt;p&gt;
GPT&#24050;&#32463;&#20855;&#22791;&#20102;&#37329;&#34701;&#32032;&#20859;&#65306;&#26469;&#33258;GPT&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#30340;&#35265;&#35299;&#20197;&#21450;&#20154;&#20204;&#20351;&#29992;&#20854;&#20316;&#20026;&#21672;&#35810;&#26469;&#28304;&#30340;&#21021;&#27493;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPT has become financially literate: Insights from financial literacy tests of GPT and a preliminary test of how people use it as a source of advice. (arXiv:2309.00649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00649
&lt;/p&gt;
&lt;p&gt;
GPT&#36890;&#36807;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#26174;&#31034;&#20986;&#20855;&#22791;&#25104;&#20026;&#22823;&#20247;&#37329;&#34701;&#26426;&#22120;&#39038;&#38382;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#22522;&#20110;GPT-4&#30340;ChatGPT&#20960;&#20046;&#23436;&#32654;&#22320;&#24471;&#20998;99%&#65292;&#25581;&#31034;&#20102;&#37329;&#34701;&#32032;&#20859;&#27491;&#22312;&#25104;&#20026;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT&#65288;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#20316;&#20026;&#22823;&#20247;&#37329;&#34701;&#26426;&#22120;&#39038;&#38382;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;GPT-3.5&#30340;Davinci&#21644;ChatGPT&#20998;&#21035;&#22312;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#20013;&#24471;&#20998;&#20026;66%&#21644;65%&#65292;&#32780;&#22522;&#20110;GPT-4&#30340;ChatGPT&#20960;&#20046;&#23436;&#32654;&#22320;&#24471;&#21040;&#20102;99%&#30340;&#20998;&#25968;&#65292;&#36825;&#34920;&#26126;&#37329;&#34701;&#32032;&#20859;&#27491;&#22312;&#25104;&#20026;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;Judge-Advisor&#31995;&#32479;&#21644;&#19968;&#20010;&#20648;&#33988;&#22256;&#22659;&#26469;&#35828;&#26126;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#24314;&#35758;&#21033;&#29992;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We assess the ability of GPT -- a large language model -- to serve as a financial robo-advisor for the masses, by using a financial literacy test. Davinci and ChatGPT based on GPT-3.5 score 66% and 65% on the financial literacy test, respectively, compared to a baseline of 33%. However, ChatGPT based on GPT-4 achieves a near-perfect 99% score, pointing to financial literacy becoming an emergent ability of state-of-the-art models. We use the Judge-Advisor System and a savings dilemma to illustrate how researchers might assess advice-utilization from large language models. We also present a number of directions for future research.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20154;&#24037;&#31995;&#32479;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#36857;&#35937;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#65292;&#26234;&#21147;&#30340;&#24515;&#29702;&#27979;&#37327;&#21487;&#20197;&#38388;&#25509;&#22320;&#36817;&#20284;&#24847;&#35782;&#20307;&#39564;&#30340;&#31243;&#24230;&#65292;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2309.00646</link><description>&lt;p&gt;
&#26234;&#33021;&#20316;&#20026;&#24847;&#35782;&#30340;&#34913;&#37327;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Intelligence as a Measure of Consciousness. (arXiv:2309.00646v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00646
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20154;&#24037;&#31995;&#32479;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#36857;&#35937;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#65292;&#26234;&#21147;&#30340;&#24515;&#29702;&#27979;&#37327;&#21487;&#20197;&#38388;&#25509;&#22320;&#36817;&#20284;&#24847;&#35782;&#20307;&#39564;&#30340;&#31243;&#24230;&#65292;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20154;&#24037;&#31995;&#32479;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#36857;&#35937;&#36234;&#26469;&#36234;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#65292;&#32780;&#19968;&#20010;&#20005;&#35880;&#30340;&#24515;&#29702;&#27979;&#37327;&#26694;&#26550;&#22312;&#36825;&#26041;&#38754;&#23545;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#26681;&#25454;&#23545;&#20154;&#31867;&#21644;&#21160;&#29289;&#22823;&#33041;&#30340;&#20449;&#24687;&#32806;&#21512;&#12289;&#20154;&#31867;&#35748;&#30693;&#21457;&#23637;&#12289;&#26032;&#20852;&#33021;&#21147;&#21644;&#24515;&#29702;&#34920;&#31034;&#21457;&#23637;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31867;&#20284;&#29616;&#35937;&#30340;&#27604;&#36739;&#65292;&#25105;&#35748;&#20026;&#26234;&#21147;&#30340;&#24515;&#29702;&#27979;&#37327;&#65292;&#22914;&#26234;&#21830;&#25110;&#26234;&#21830;&#25351;&#25968;&#65292;&#38388;&#25509;&#22320;&#36817;&#20284;&#20102;&#24847;&#35782;&#20307;&#39564;&#30340;&#31243;&#24230;&#12290;&#22522;&#20110;&#26356;&#24191;&#27867;&#30340;&#31185;&#23398;&#21644;&#24418;&#32780;&#19978;&#23398;&#30340;&#24847;&#35782;&#29702;&#35770;&#65292;&#25105;&#35748;&#20026;&#25152;&#26377;&#31995;&#32479;&#37117;&#20855;&#26377;&#19968;&#23450;&#31243;&#24230;&#30340;&#21487;&#27979;&#37327;&#30340;&#24847;&#35782;&#65292;&#24182;&#19988;&#26234;&#21147;&#30340;&#24515;&#29702;&#27979;&#37327;&#21487;&#20197;&#29992;&#26469;&#34913;&#37327;&#36825;&#31181;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating artificial systems for signs of consciousness is increasingly becoming a pressing concern, and a rigorous psychometric measurement framework may be of crucial importance in evaluating large language models in this regard. Most prominent theories of consciousness, both scientific and metaphysical, argue for different kinds of information coupling as a necessary component of human-like consciousness. By comparing information coupling in human and animal brains, human cognitive development, emergent abilities, and mental representation development to analogous phenomena in large language models, I argue that psychometric measures of intelligence, such as the g-factor or IQ, indirectly approximate the extent of conscious experience.  Based on a broader source of both scientific and metaphysical theories of consciousness, I argue that all systems possess a degree of consciousness ascertainable psychometrically and that psychometric measures of intelligence may be used to gauge re
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21313;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#27979;&#35797;&#20248;&#21270;&#31639;&#27861;&#21450;&#20854;&#21464;&#20307;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#22522;&#20934;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24615;&#65292;&#21253;&#25324;&#22122;&#22768;&#12289;&#19981;&#36830;&#32493;&#24615;&#12289;&#21442;&#25968;&#20272;&#35745;&#21644;&#26410;&#30693;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.00644</link><description>&lt;p&gt;
&#20248;&#21270;&#30340;&#21313;&#20010;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Ten New Benchmarks for Optimization. (arXiv:2309.00644v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21313;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#27979;&#35797;&#20248;&#21270;&#31639;&#27861;&#21450;&#20854;&#21464;&#20307;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#22522;&#20934;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24615;&#65292;&#21253;&#25324;&#22122;&#22768;&#12289;&#19981;&#36830;&#32493;&#24615;&#12289;&#21442;&#25968;&#20272;&#35745;&#21644;&#26410;&#30693;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20934;&#29992;&#20110;&#27979;&#35797;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#21450;&#20854;&#21464;&#20307;&#65292;&#20197;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20934;&#26159;&#20809;&#28369;&#20989;&#25968;&#12290;&#26412;&#31456;&#20171;&#32461;&#20102;&#21313;&#20010;&#20855;&#26377;&#19981;&#21516;&#29305;&#24615;&#30340;&#26032;&#22522;&#20934;&#65292;&#21253;&#25324;&#22122;&#22768;&#12289;&#19981;&#36830;&#32493;&#24615;&#12289;&#21442;&#25968;&#20272;&#35745;&#21644;&#26410;&#30693;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarks are used for testing new optimization algorithms and their variants to evaluate their performance. Most existing benchmarks are smooth functions. This chapter introduces ten new benchmarks with different properties, including noise, discontinuity, parameter estimation and unknown paths.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#32593;&#32476;&#30340;&#20196;&#29260;&#32423;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;&#28040;&#24687;&#65292;&#24182;&#19988;&#22312;&#36924;&#36817;&#25968;&#25454;&#20998;&#24067;&#21644;&#20013;&#38388;&#20215;&#26684;&#22238;&#25253;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#21069;&#26223;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00638</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;&#24314;&#27169;&#65306;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#32593;&#32476;&#30340;&#20196;&#29260;&#32423;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#30340;&#28040;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Generative AI for End-to-End Limit Order Book Modelling: A Token-Level Autoregressive Generative Model of Message Flow Using a Deep State Space Network. (arXiv:2309.00638v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#32593;&#32476;&#30340;&#20196;&#29260;&#32423;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;&#28040;&#24687;&#65292;&#24182;&#19988;&#22312;&#36924;&#36817;&#25968;&#25454;&#20998;&#24067;&#21644;&#20013;&#38388;&#20215;&#26684;&#22238;&#25253;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#21069;&#26223;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#24066;&#22330;&#20013;&#24320;&#21457;&#19968;&#20010;&#36924;&#30495;&#30340;&#35746;&#21333;&#27969;&#29983;&#25104;&#27169;&#22411;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#20855;&#26377;&#35768;&#22810;&#24066;&#22330;&#21442;&#19982;&#32773;&#30340;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#20196;&#29260;&#21270;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;(LOB)&#28040;&#24687;&#12290;&#36825;&#20123;&#28040;&#24687;&#30001;Jax-LOB&#27169;&#25311;&#22120;&#35299;&#37322;&#65292;&#35813;&#27169;&#25311;&#22120;&#26356;&#26032;LOB&#29366;&#24577;&#12290;&#20026;&#20102;&#39640;&#25928;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#31616;&#21270;&#30340;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#23618;&#26469;&#22788;&#29702;&#35746;&#21333;&#31807;&#29366;&#24577;&#21644;&#20196;&#29260;&#21270;&#28040;&#24687;&#30340;&#24207;&#21015;&#12290;&#21033;&#29992;NASDAQ&#32929;&#31080;LOBSTER&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#30340;&#28040;&#24687;&#25968;&#25454;&#20998;&#35789;&#22120;&#65292;&#23558;&#36830;&#32493;&#25968;&#23383;&#20998;&#32452;&#36716;&#25442;&#20026;&#20196;&#29260;&#65292;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20998;&#35789;&#12290;&#22806;&#26679;&#26412;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36924;&#36817;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#65292;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#36739;&#20302;&#65292;&#24615;&#33021;&#34920;&#29616;&#26377;&#21069;&#26223;&#12290;&#27492;&#22806;&#65292;&#20174;&#29983;&#25104;&#30340;&#35746;&#21333;&#27969;&#35745;&#31639;&#20986;&#30340;&#20013;&#38388;&#20215;&#26684;&#22238;&#25253;&#19982;&#25968;&#25454;&#21576;&#26174;&#33879;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;&#20102;&#27169;&#22411;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing a generative model of realistic order flow in financial markets is a challenging open problem, with numerous applications for market participants. Addressing this, we propose the first end-to-end autoregressive generative model that generates tokenized limit order book (LOB) messages. These messages are interpreted by a Jax-LOB simulator, which updates the LOB state. To handle long sequences efficiently, the model employs simplified structured state-space layers to process sequences of order book states and tokenized messages. Using LOBSTER data of NASDAQ equity LOBs, we develop a custom tokenizer for message data, converting groups of successive digits to tokens, similar to tokenization in large language models. Out-of-sample results show promising performance in approximating the data distribution, as evidenced by low model perplexity. Furthermore, the mid-price returns calculated from the generated order flow exhibit a significant correlation with the data, indicating imp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36328;&#20027;&#20307;&#33041;&#35299;&#30721;&#26159;&#21487;&#34892;&#30340;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#32422;10%&#30340;&#25968;&#25454;&#65292;&#24615;&#33021;&#19982;&#21333;&#20010;&#20027;&#20307;&#35299;&#30721;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2309.00627</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#30340;&#23545;&#40784;&#25216;&#26415;&#36827;&#34892;&#22810;&#20027;&#20307;&#33041;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Through their eyes: multi-subject Brain Decoding with simple alignment techniques. (arXiv:2309.00627v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00627
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36328;&#20027;&#20307;&#33041;&#35299;&#30721;&#26159;&#21487;&#34892;&#30340;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#32422;10%&#30340;&#25968;&#25454;&#65292;&#24615;&#33021;&#19982;&#21333;&#20010;&#20027;&#20307;&#35299;&#30721;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#33041;&#35299;&#30721;&#30740;&#31350;&#20027;&#35201;&#28041;&#21450;&#21333;&#20010;&#20027;&#20307;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;&#21516;&#19968;&#20027;&#20307;&#30340;fMRI&#27963;&#21160;&#37325;&#24314;&#21050;&#28608;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#24341;&#20837;&#19968;&#31181;&#36328;&#20027;&#20307;&#33041;&#35299;&#30721;&#30340;&#36890;&#29992;&#25216;&#26415;&#65292;&#36890;&#36807;&#25506;&#32034;&#25968;&#25454;&#23545;&#40784;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#21033;&#29992;NSD&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#28041;&#21450;&#22810;&#20010;&#20027;&#20307;&#30340;&#32508;&#21512;7T fMRI&#35270;&#35273;&#23454;&#39564;&#65292;&#21253;&#25324;9841&#20010;&#22270;&#20687;&#65292;&#20854;&#20013;982&#20010;&#34987;&#25152;&#26377;&#20027;&#20307;&#35266;&#30475;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22312;&#19968;&#20010;&#20027;&#20307;&#19978;&#35757;&#32451;&#35299;&#30721;&#27169;&#22411;&#65292;&#23558;&#20854;&#20182;&#20027;&#20307;&#30340;&#25968;&#25454;&#23545;&#40784;&#21040;&#35813;&#31354;&#38388;&#65292;&#24182;&#22312;&#31532;&#20108;&#20010;&#20027;&#20307;&#19978;&#36827;&#34892;&#35299;&#30721;&#27979;&#35797;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#23725;&#22238;&#24402;&#12289;&#36229;&#32423;&#23545;&#40784;&#21644;&#35299;&#21078;&#23545;&#40784;&#31561;&#19981;&#21516;&#30340;fMRI&#25968;&#25454;&#23545;&#40784;&#25216;&#26415;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36328;&#20027;&#20307;&#33041;&#35299;&#30721;&#26159;&#21487;&#34892;&#30340;&#65292;&#21363;&#20351;&#20165;&#20351;&#29992;&#32422;10%&#30340;&#24635;&#25968;&#25454;&#65292;&#21363;982&#20010;&#20849;&#21516;&#22270;&#20687;&#65292;&#24615;&#33021;&#19982;&#21333;&#20010;&#20027;&#20307;&#35299;&#30721;&#30456;&#23218;&#32654;&#12290;&#23725;&#22238;&#24402;&#26159;&#21151;&#33021;&#23545;&#40784;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#36890;&#36807;&#20027;&#20307;&#23545;&#40784;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#33041;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous brain decoding research primarily involves single-subject studies, reconstructing stimuli via fMRI activity from the same subject. Our study aims to introduce a generalization technique for cross-subject brain decoding, facilitated by exploring data alignment methods. We utilized the NSD dataset, a comprehensive 7T fMRI vision experiment involving multiple subjects exposed to 9841 images, 982 of which were viewed by all. Our approach involved training a decoding model on one subject, aligning others' data to this space, and testing the decoding on the second subject. We compared ridge regression, hyper alignment, and anatomical alignment techniques for fMRI data alignment. We established that cross-subject brain decoding is feasible, even using around 10% of the total data, or 982 common images, with comparable performance to single-subject decoding. Ridge regression was the best method for functional alignment. Through subject alignment, we achieved superior brain decoding an
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#32534;&#31243;&#35821;&#35328;&#21487;&#20197;&#22312;&#25351;&#20196;&#35843;&#20248;&#38454;&#27573;&#30456;&#20114;&#20419;&#36827;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16824</link><description>&lt;p&gt;
&#32534;&#31243;&#35821;&#35328;&#33021;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#30456;&#20114;&#25552;&#21319;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Programming Languages Boost Each Other via Instruction Tuning?. (arXiv:2308.16824v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16824
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#32534;&#31243;&#35821;&#35328;&#21487;&#20197;&#22312;&#25351;&#20196;&#35843;&#20248;&#38454;&#27573;&#30456;&#20114;&#20419;&#36827;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#31243;&#24207;&#21592;&#25484;&#25569;&#20102;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#21518;&#65292;&#23398;&#20064;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#35821;&#35328;&#20250;&#26356;&#23481;&#26131;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#25506;&#35752;&#20102;&#22312;&#20195;&#30721;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#24494;&#35843;&#38454;&#27573;&#20013;&#65292;&#32534;&#31243;&#35821;&#35328;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#30456;&#20114;&#25552;&#21319;&#26469;&#22686;&#24378;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;StarCoder&#19978;&#23545;8&#31181;&#27969;&#34892;&#30340;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65288;Python&#65292;JavaScript&#65292;TypeScript&#65292;C&#65292;C ++&#65292;Java&#65292;Go&#65292;HTML&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#31243;&#35821;&#35328;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#36890;&#36807;&#22312;Python&#19978;&#35757;&#32451;&#30340;CodeM-Python 15B&#21487;&#20197;&#20351;Java&#30340;pass@1&#29575;&#32477;&#23545;&#22686;&#21152;&#20102;17.95&#65285;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#22312;HTML&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;CodeM-HTML 7B&#21487;&#20197;&#20351;Java&#30340;pass@1&#29575;&#32477;&#23545;&#22686;&#21152;&#20102;15.24&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#24050;&#32463;&#21457;&#24067;&#22312;https://github.com/NL2Code/CodeM&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
When human programmers have mastered a programming language, it would be easier when they learn a new programming language. In this report, we focus on exploring whether programming languages can boost each other during the instruction fine-tuning phase of code large language models. We conduct extensive experiments of 8 popular programming languages (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that programming languages can significantly improve each other. For example, CodeM-Python 15B trained on Python is able to increase Java by an absolute 17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our training data is released at https://github.com/NL2Code/CodeM.
&lt;/p&gt;</description></item><item><title>&#20195;&#29702;&#22242;&#38431;&#24773;&#26223;&#24863;&#30693;&#65288;ATSA&#65289;&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#32852;&#21512;&#24037;&#20316;&#30340;&#24773;&#26223;&#24863;&#30693;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#28151;&#21512;&#22242;&#38431;&#30340;&#32489;&#25928;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20010;&#20307;&#21644;&#22242;&#38431;&#30340;&#24773;&#26223;&#24863;&#30693;&#27169;&#22411;&#65292;&#24182;&#28041;&#21450;&#21452;&#21521;&#21644;&#21160;&#24577;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2308.16785</link><description>&lt;p&gt;
&#20195;&#29702;&#22242;&#38431;&#24773;&#26223;&#24863;&#30693;&#65288;ATSA&#65289;&#65306;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#32852;&#21512;&#24037;&#20316;&#30340;&#24773;&#26223;&#24863;&#30693;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Agent Teaming Situation Awareness (ATSA): A Situation Awareness Framework for Human-AI Teaming. (arXiv:2308.16785v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16785
&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#22242;&#38431;&#24773;&#26223;&#24863;&#30693;&#65288;ATSA&#65289;&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#32852;&#21512;&#24037;&#20316;&#30340;&#24773;&#26223;&#24863;&#30693;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#28151;&#21512;&#22242;&#38431;&#30340;&#32489;&#25928;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20010;&#20307;&#21644;&#22242;&#38431;&#30340;&#24773;&#26223;&#24863;&#30693;&#27169;&#22411;&#65292;&#24182;&#28041;&#21450;&#21452;&#21521;&#21644;&#21160;&#24577;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#32852;&#21512;&#24037;&#20316;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26085;&#30410;&#22686;&#38271;&#12290;&#38543;&#30528;&#26426;&#22120;&#20174;&#20165;&#20165;&#33258;&#21160;&#21270;&#21040;&#33258;&#20027;&#29366;&#24577;&#30340;&#28436;&#36827;&#65292;&#23427;&#20204;&#36234;&#26469;&#36234;&#23637;&#29616;&#20986;&#24847;&#22806;&#30340;&#34892;&#20026;&#21644;&#31867;&#20284;&#20154;&#31867;&#30340;&#35748;&#30693;/&#26234;&#33021;&#33021;&#21147;&#65292;&#21253;&#25324;&#24773;&#26223;&#24863;&#30693;&#12290;&#36825;&#31181;&#36716;&#21464;&#26377;&#28508;&#21147;&#25552;&#39640;&#28151;&#21512;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#32489;&#25928;&#65292;&#24378;&#35843;&#20102;&#25105;&#20204;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#21160;&#24577;&#24773;&#26223;&#24863;&#30693;&#30456;&#20114;&#20316;&#29992;&#30340;&#26356;&#22909;&#29702;&#35299;&#30340;&#38656;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#20027;&#27969;&#24773;&#26223;&#24863;&#30693;&#29702;&#35770;&#27169;&#22411;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#24182;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#32852;&#21512;&#24037;&#20316;&#30340;&#20851;&#38190;&#29305;&#24449;&#21644;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24773;&#26223;&#24863;&#30693;&#26694;&#26550;&#12290;&#20195;&#29702;&#22242;&#38431;&#24773;&#26223;&#24863;&#30693;&#65288;ATSA&#65289;&#26694;&#26550;&#32479;&#19968;&#20102;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#65292;&#24182;&#28041;&#21450;&#21452;&#21521;&#21644;&#21160;&#24577;&#30340;&#20132;&#20114;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#20010;&#20307;&#21644;&#22242;&#38431;&#30340;&#24773;&#26223;&#24863;&#30693;&#27169;&#22411;&#65292;&#24182;&#38416;&#36848;&#20102;&#20026;&#24314;&#27169;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#32852;&#21512;&#24037;&#20316;&#32780;&#32454;&#21270;&#30340;&#35748;&#30693;&#26426;&#21046;&#12290;&#31867;&#20284;&#30340;&#30693;&#35273;&#24490;&#29615;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in artificial intelligence (AI) have led to a growing trend of human-AI teaming (HAT) in various fields. As machines continue to evolve from mere automation to a state of autonomy, they are increasingly exhibiting unexpected behaviors and human-like cognitive/intelligent capabilities, including situation awareness (SA). This shift has the potential to enhance the performance of mixed human-AI teams over all-human teams, underscoring the need for a better understanding of the dynamic SA interactions between humans and machines. To this end, we provide a review of leading SA theoretical models and a new framework for SA in the HAT context based on the key features and processes of HAT. The Agent Teaming Situation Awareness (ATSA) framework unifies human and AI behavior, and involves bidirectional, and dynamic interaction. The framework is based on the individual and team SA models and elaborates on the cognitive mechanisms for modeling HAT. Similar perceptual cycle
&lt;/p&gt;</description></item><item><title>StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16781</link><description>&lt;p&gt;
StratMed&#65306;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StratMed: Relevance Stratification for Low-resource Medication Recommendation. (arXiv:2308.16781v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16781
&lt;/p&gt;
&lt;p&gt;
StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26377;&#38480;&#21307;&#30103;&#36164;&#28304;&#19982;&#26085;&#30410;&#22686;&#38271;&#30340;&#38656;&#27714;&#20043;&#38388;&#30340;&#22833;&#34913;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20020;&#24202;&#20219;&#21153;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#33647;&#29289;&#25512;&#33616;&#26088;&#22312;&#23558;&#24739;&#32773;&#30340;&#32437;&#21521;&#21382;&#21490;&#19982;&#21307;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#21307;&#29983;&#26356;&#23433;&#20840;&#12289;&#26356;&#20934;&#30830;&#22320;&#24320;&#20855;&#33647;&#29289;&#32452;&#21512;&#22788;&#26041;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#21307;&#30103;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#38271;&#23614;&#20998;&#24067;&#65292;&#32570;&#20047;&#22836;&#23614;&#25968;&#25454;&#20043;&#38388;&#30340;&#24179;&#34913;&#34920;&#31034;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#27425;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StratMed&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21019;&#26032;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#21327;&#35843;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#22312;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#26500;&#24314;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#33719;&#21462;&#23454;&#20307;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#37329;&#23383;&#22612;&#30340;&#25968;&#25454;&#20998;&#23618;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#36890;&#29992;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing imbalance between limited medical resources and escalating demands, AI-based clinical tasks have become paramount. Medication recommendation, as a sub-domain, aims to amalgamate longitudinal patient history with medical knowledge, assisting physicians in prescribing safer and more accurate medication combinations. Existing methods overlook the inherent long-tail distribution in medical data, lacking balanced representation between head and tail data, which leads to sub-optimal model performance. To address this challenge, we introduce StratMed, a model that incorporates an innovative relevance stratification mechanism. It harmonizes discrepancies in data long-tail distribution and strikes a balance between the safety and accuracy of medication combinations. Specifically, we first construct a pre-training method using deep learning networks to obtain entity representation. After that, we design a pyramid-like data stratification method to obtain more generalized entity 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#36827;&#34892;&#20302;&#38376;&#27099;&#30740;&#31350;&#21644;&#25945;&#32946;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#24320;&#21457;&#30340;&#39640;&#20445;&#30495;&#27169;&#25311;&#22120;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#29992;&#20110;&#33258;&#21160;&#21457;&#21160;&#32593;&#32476;&#25915;&#20987;&#12289;&#25910;&#38598;&#25968;&#25454;&#12289;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#38024;&#23545;&#23454;&#38469;&#30340;&#21270;&#23398;&#21644;&#21046;&#36896;&#36807;&#31243;&#36827;&#34892;&#35780;&#20272;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MinTWin SVM&#30340;&#20837;&#20405;&#26816;&#27979;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16769</link><description>&lt;p&gt;
&#26397;&#30528;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#20302;&#38376;&#27099;&#30340;&#32593;&#32476;&#23433;&#20840;&#30740;&#31350;&#21644;&#25945;&#32946;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Towards Low-Barrier Cybersecurity Research and Education for Industrial Control Systems. (arXiv:2308.16769v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#36827;&#34892;&#20302;&#38376;&#27099;&#30740;&#31350;&#21644;&#25945;&#32946;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#24320;&#21457;&#30340;&#39640;&#20445;&#30495;&#27169;&#25311;&#22120;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#29992;&#20110;&#33258;&#21160;&#21457;&#21160;&#32593;&#32476;&#25915;&#20987;&#12289;&#25910;&#38598;&#25968;&#25454;&#12289;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#38024;&#23545;&#23454;&#38469;&#30340;&#21270;&#23398;&#21644;&#21046;&#36896;&#36807;&#31243;&#36827;&#34892;&#35780;&#20272;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MinTWin SVM&#30340;&#20837;&#20405;&#26816;&#27979;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#29992;&#20110;&#20844;&#20849;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#30340;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;(ICS)&#23545;&#20110;&#38450;&#27490;&#32593;&#32476;&#25915;&#20987;&#36896;&#25104;&#28798;&#38590;&#24615;&#30340;&#29289;&#29702;&#25439;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#30740;&#31350;&#30028;&#38656;&#35201;&#27979;&#35797;&#24179;&#21488;&#26469;&#39564;&#35777;&#21644;&#27604;&#36739;&#21508;&#31181;&#20837;&#20405;&#26816;&#27979;&#31639;&#27861;&#20197;&#20445;&#25252;ICS&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#30828;&#20214;&#12289;&#36719;&#20214;&#21644;&#25805;&#32437;&#29616;&#23454;&#31995;&#32479;&#30340;&#22266;&#26377;&#21361;&#38505;&#24615;&#65292;ICS&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#25945;&#32946;&#23384;&#22312;&#39640;&#38376;&#27099;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#22312;&#26368;&#36817;&#24320;&#21457;&#30340;&#19977;&#32500;&#39640;&#20445;&#30495;&#27169;&#25311;&#22120;&#22522;&#30784;&#19978;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38598;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#21457;&#21160;&#32593;&#32476;&#25915;&#20987;&#12289;&#25910;&#38598;&#25968;&#25454;&#12289;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#38024;&#23545;&#23454;&#38469;&#30340;&#21270;&#23398;&#21644;&#21046;&#36896;&#36807;&#31243;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#25105;&#20204;&#30340;&#27979;&#35797;&#24179;&#21488;&#19978;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20837;&#20405;&#26816;&#27979;&#27169;&#22411;&#65292;&#31216;&#20026;Minimal Threshold and Window SVM (MinTWin SVM)&#65292;&#23427;&#32467;&#21512;&#20102;&#19968;&#31867;SVM&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#28369;&#21160;&#31383;&#21475;
&lt;/p&gt;
&lt;p&gt;
The protection of Industrial Control Systems (ICS) that are employed in public critical infrastructures is of utmost importance due to catastrophic physical damages cyberattacks may cause. The research community requires testbeds for validation and comparing various intrusion detection algorithms to protect ICS. However, there exist high barriers to entry for research and education in the ICS cybersecurity domain due to expensive hardware, software, and inherent dangers of manipulating real-world systems. To close the gap, built upon recently developed 3D high-fidelity simulators, we further showcase our integrated framework to automatically launch cyberattacks, collect data, train machine learning models, and evaluate for practical chemical and manufacturing processes. On our testbed, we validate our proposed intrusion detection model called Minimal Threshold and Window SVM (MinTWin SVM) that utilizes unsupervised machine learning via a one-class SVM in combination with a sliding wind
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#26356;&#20005;&#37325;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#21363;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#21033;&#29992;&#26131;&#33719;&#21462;&#30340;&#26377;&#25439;&#21387;&#32553;&#31639;&#27861;&#36827;&#34892;&#33258;&#28982;&#21518;&#38376;&#25915;&#20987;&#65292;&#26080;&#38656;&#35774;&#35745;&#29305;&#23450;&#35302;&#21457;&#22120;&#25110;&#36827;&#34892;&#32321;&#29712;&#35843;&#35797;&#12290;</title><link>http://arxiv.org/abs/2308.16684</link><description>&lt;p&gt;
&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#25915;&#20987;&#65306;&#23558;&#26377;&#25439;&#21387;&#32553;&#37325;&#26032;&#29992;&#20316;&#33258;&#28982;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack. (arXiv:2308.16684v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#26356;&#20005;&#37325;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#21363;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#21033;&#29992;&#26131;&#33719;&#21462;&#30340;&#26377;&#25439;&#21387;&#32553;&#31639;&#27861;&#36827;&#34892;&#33258;&#28982;&#21518;&#38376;&#25915;&#20987;&#65292;&#26080;&#38656;&#35774;&#35745;&#29305;&#23450;&#35302;&#21457;&#22120;&#25110;&#36827;&#34892;&#32321;&#29712;&#35843;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21518;&#38376;&#25915;&#20987;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26500;&#25104;&#20102;&#23041;&#32961;&#12290;&#20256;&#32479;&#26234;&#24935;&#35748;&#20026;&#65292;&#24182;&#19981;&#26159;&#27599;&#20010;&#20154;&#37117;&#21487;&#20197;&#25104;&#20026;&#25915;&#20987;&#32773;&#65292;&#22240;&#20026;&#35774;&#35745;&#35302;&#21457;&#22120;&#29983;&#25104;&#31639;&#27861;&#30340;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#21162;&#21147;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#30830;&#20445;&#25915;&#20987;&#30340;&#38544;&#31192;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#25351;&#20986;&#23384;&#22312;&#19968;&#31181;&#26356;&#20026;&#20005;&#37325;&#30340;&#21518;&#38376;&#23041;&#32961;&#65306;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#21033;&#29992;&#26131;&#33719;&#21462;&#30340;&#31639;&#27861;&#36827;&#34892;&#38544;&#24708;&#21518;&#38376;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#21508;&#31181;&#21387;&#32553;&#24037;&#20855;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26377;&#25439;&#22270;&#29255;&#21387;&#32553;&#25216;&#26415;&#65292;&#26080;&#38656;&#30041;&#19979;&#20219;&#20309;&#26126;&#26174;&#30340;&#30165;&#36857;&#23601;&#33021;&#36731;&#26494;&#22320;&#23558;&#35302;&#21457;&#22120;&#27169;&#24335;&#27880;&#20837;&#21040;&#22270;&#20687;&#20013;&#65292;&#21363;&#29983;&#25104;&#30340;&#35302;&#21457;&#22120;&#26159;&#33258;&#28982;&#30340;&#22270;&#20687;&#20266;&#24433;&#12290;&#20351;&#29992;&#26377;&#25439;&#22270;&#29255;&#21387;&#32553;&#24037;&#20855;&#26102;&#65292;&#20154;&#20204;&#24182;&#19981;&#38656;&#35201;&#24191;&#27867;&#30693;&#35782;&#65292;&#21482;&#38656;&#28857;&#20987;&#8220;&#36716;&#25442;&#8221;&#25110;&#8220;&#21478;&#23384;&#20026;&#8221;&#25353;&#38062;&#21363;&#21487;&#12290;&#36890;&#36807;&#36825;&#31181;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#26080;&#38656;&#35774;&#35745;&#19968;&#20010;&#19987;&#38376;&#30340;&#35302;&#21457;&#22120;&#25110;&#36827;&#34892;&#32321;&#29712;&#30340;&#35843;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vulnerabilities to backdoor attacks have recently threatened the trustworthiness of machine learning models in practical applications. Conventional wisdom suggests that not everyone can be an attacker since the process of designing the trigger generation algorithm often involves significant effort and extensive experimentation to ensure the attack's stealthiness and effectiveness. Alternatively, this paper shows that there exists a more severe backdoor threat: anyone can exploit an easily-accessible algorithm for silent backdoor attacks. Specifically, this attacker can employ the widely-used lossy image compression from a plethora of compression tools to effortlessly inject a trigger pattern into an image without leaving any noticeable trace; i.e., the generated triggers are natural artifacts. One does not require extensive knowledge to click on the "convert" or "save as" button while using tools for lossy image compression. Via this attack, the adversary does not need to design a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#19987;&#23478;&#23454;&#29616;&#20102;&#38271;&#23614;&#22270;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22270;&#25968;&#25454;&#19978;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.16609</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#19987;&#23478;&#23454;&#29616;&#38271;&#23614;&#22270;&#20998;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts. (arXiv:2308.16609v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#19987;&#23478;&#23454;&#29616;&#20102;&#38271;&#23614;&#22270;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22270;&#25968;&#25454;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#31867;&#26088;&#22312;&#23398;&#20064;&#29992;&#20110;&#26377;&#25928;&#31867;&#21035;&#20998;&#37197;&#30340;&#22270;&#32423;&#34920;&#31034;&#65292;&#22312;&#24179;&#34913;&#30340;&#31867;&#21035;&#20998;&#24067;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#19979;&#21462;&#24471;&#20102;&#26480;&#20986;&#25104;&#26524;&#12290;&#20107;&#23454;&#19978;&#65292;&#22823;&#22810;&#25968;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#33258;&#28982;&#21576;&#29616;&#38271;&#23614;&#24418;&#24335;&#65292;&#20854;&#20013;&#22836;&#37096;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#36828;&#36229;&#36807;&#23614;&#37096;&#31867;&#21035;&#65292;&#22240;&#27492;&#22312;&#38271;&#23614;&#25968;&#25454;&#19978;&#30740;&#31350;&#22270;&#32423;&#20998;&#31867;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#20173;&#28982;&#36739;&#23569;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;&#20013;&#30340;&#38271;&#23614;&#23398;&#20064;&#26041;&#27861;&#22823;&#22810;&#26080;&#27861;&#21516;&#26102;&#20248;&#21270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#35757;&#32451;&#65292;&#24182;&#19988;&#24573;&#30053;&#20102;&#38590;&#20197;&#20998;&#31867;&#30340;&#31867;&#21035;&#30340;&#25366;&#25496;&#12290;&#30452;&#25509;&#23558;&#29616;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#65292;&#22240;&#20026;&#22312;&#22270;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30001;&#20110;&#22797;&#26434;&#30340;&#25299;&#25169;&#29305;&#24449;&#20250;&#26356;&#21152;&#25935;&#24863;&#20110;&#38271;&#23614;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#38271;&#23614;&#22270;&#32423;&#20998;&#31867;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph classification, aiming at learning the graph-level representations for effective class assignments, has received outstanding achievements, which heavily relies on high-quality datasets that have balanced class distribution. In fact, most real-world graph data naturally presents a long-tailed form, where the head classes occupy much more samples than the tail classes, it thus is essential to study the graph-level classification over long-tailed data while still remaining largely unexplored. However, most existing long-tailed learning methods in visions fail to jointly optimize the representation learning and classifier training, as well as neglect the mining of the hard-to-classify classes. Directly applying existing methods to graphs may lead to sub-optimal performance, since the model trained on graphs would be more sensitive to the long-tailed distribution due to the complex topological characteristics. Hence, in this paper, we propose a novel long-tailed graph-level classifica
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#30011;&#23478;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#28508;&#22312;&#20316;&#20026;&#30011;&#24067;&#21644;&#25193;&#25955;&#22120;&#30340;&#39044;&#27979;&#20316;&#20026;&#35745;&#21010;&#26469;&#29983;&#25104;&#32472;&#30011;&#21160;&#30011;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#26816;&#26597;&#28857;&#38598;&#20013;&#36716;&#25442;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.16490</link><description>&lt;p&gt;
&#28508;&#22312;&#30011;&#23478;
&lt;/p&gt;
&lt;p&gt;
Latent Painter. (arXiv:2308.16490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16490
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#30011;&#23478;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#28508;&#22312;&#20316;&#20026;&#30011;&#24067;&#21644;&#25193;&#25955;&#22120;&#30340;&#39044;&#27979;&#20316;&#20026;&#35745;&#21010;&#26469;&#29983;&#25104;&#32472;&#30011;&#21160;&#30011;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#26816;&#26597;&#28857;&#38598;&#20013;&#36716;&#25442;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#25193;&#25955;&#22120;&#22312;&#29983;&#25104;AI&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#24182;&#28608;&#21457;&#20102;&#21019;&#36896;&#24615;&#33402;&#26415;&#12290;&#22312;&#21435;&#22122;&#28508;&#22312;&#26102;&#65292;&#27599;&#20010;&#27493;&#39588;&#39044;&#27979;&#30340;&#21407;&#22987;&#22270;&#20687;&#20849;&#21516;&#24418;&#25104;&#20102;&#21160;&#30011;&#12290;&#28982;&#32780;&#65292;&#21160;&#30011;&#21463;&#21040;&#25193;&#25955;&#22120;&#21435;&#22122;&#29305;&#24615;&#30340;&#38480;&#21046;&#65292;&#21482;&#21576;&#29616;&#20102;&#19968;&#20010;&#38160;&#21270;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#28508;&#22312;&#30011;&#23478;&#65292;&#23427;&#20197;&#28508;&#22312;&#20316;&#20026;&#30011;&#24067;&#65292;&#20197;&#25193;&#25955;&#22120;&#30340;&#39044;&#27979;&#20316;&#20026;&#35745;&#21010;&#65292;&#29983;&#25104;&#32472;&#30011;&#21160;&#30011;&#12290;&#28508;&#22312;&#30011;&#23478;&#36824;&#21487;&#20197;&#23558;&#19968;&#20010;&#29983;&#25104;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#22270;&#20687;&#65292;&#36825;&#21487;&#20197;&#21457;&#29983;&#22312;&#20004;&#20010;&#19981;&#21516;&#26816;&#26597;&#28857;&#38598;&#20013;&#30340;&#22270;&#20687;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent diffusers revolutionized the generative AI and inspired creative art. When denoising the latent, the predicted original image at each step collectively animates the formation. However, the animation is limited by the denoising nature of the diffuser, and only renders a sharpening process. This work presents Latent Painter, which uses the latent as the canvas, and the diffuser predictions as the plan, to generate painting animation. Latent Painter also transits one generated image to another, which can happen between images from two different sets of checkpoints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#28857;&#20113;&#19978;&#37319;&#26679;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#35299;&#20915;&#20102;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16484</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#28857;&#20113;&#19978;&#37319;&#26679;&#30340;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning. (arXiv:2308.16484v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#28857;&#20113;&#19978;&#37319;&#26679;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#35299;&#20915;&#20102;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24265;&#20215;&#30340;3D&#25195;&#25551;&#20202;&#32463;&#24120;&#20135;&#29983;&#31232;&#30095;&#21644;&#38750;&#22343;&#21248;&#30340;&#28857;&#20113;&#65292;&#36825;&#23545;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#19979;&#28216;&#24212;&#29992;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#28857;&#20113;&#19978;&#37319;&#26679;&#26550;&#26500;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#27979;&#35797;&#25968;&#25454;&#19982;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#26469;&#22686;&#24378;&#28857;&#20113;&#19978;&#37319;&#26679;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20803;&#23398;&#20064;&#26469;&#26174;&#24335;&#22320;&#23398;&#20064;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#32593;&#32476;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#27979;&#35797;&#25968;&#25454;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#22312;&#20803;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#21442;&#25968;&#26159;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#30095;-&#23494;&#38598;&#28857;&#20113;&#23545;&#30340;&#38598;&#21512;&#20013;&#23398;&#20064;&#30340;&#12290;&#22312;&#20803;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#32463;&#36807;&#23569;&#37327;&#26799;&#24230;&#26356;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19968;&#32452;&#21807;&#19968;&#30340;&#32593;&#32476;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affordable 3D scanners often produce sparse and non-uniform point clouds that negatively impact downstream applications in robotic systems. While existing point cloud upsampling architectures have demonstrated promising results on standard benchmarks, they tend to experience significant performance drops when the test data have different distributions from the training data. To address this issue, this paper proposes a test-time adaption approach to enhance model generality of point cloud upsampling. The proposed approach leverages meta-learning to explicitly learn network parameters for test-time adaption. Our method does not require any prior information about the test data. During meta-training, the model parameters are learned from a collection of instance-level tasks, each of which consists of a sparse-dense pair of point clouds from the training data. During meta-testing, the trained model is fine-tuned with a few gradient updates to produce a unique set of network parameters for
&lt;/p&gt;</description></item><item><title>Point-TTA&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#20803;&#36741;&#21161;&#23398;&#20064;&#23454;&#29616;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#37197;&#20934;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16481</link><description>&lt;p&gt;
Point-TTA: &#20351;&#29992;&#22810;&#20219;&#21153;&#20803;&#36741;&#21161;&#23398;&#20064;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning. (arXiv:2308.16481v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16481
&lt;/p&gt;
&lt;p&gt;
Point-TTA&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#20803;&#36741;&#21161;&#23398;&#20064;&#23454;&#29616;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#37197;&#20934;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Point-TTA&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#37197;&#20934;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#34429;&#28982;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#38754;&#23545;&#26410;&#30693;&#30340;&#27979;&#35797;&#29615;&#22659;&#30340;&#27867;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;3D&#25195;&#25551;&#30340;&#21464;&#21270;&#36739;&#22823;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#24182;&#22312;&#27599;&#20010;&#23454;&#20363;&#19978;&#24212;&#29992;&#30456;&#21516;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#21516;&#19968;&#27169;&#22411;&#24456;&#38590;&#22788;&#29702;&#27979;&#35797;&#26399;&#38388;&#30340;&#25152;&#26377;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28857;&#20113;&#37197;&#20934;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#27979;&#35797;&#26102;&#26410;&#30693;&#30340;&#20998;&#24067;&#65292;&#26080;&#38656;&#20219;&#20309;&#20851;&#20110;&#27979;&#35797;&#25968;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#33258;&#30417;&#30563;&#36741;&#21161;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#19982;&#20027;&#35201;&#30340;&#37197;&#20934;&#20219;&#21153;&#19968;&#36215;&#36827;&#34892;&#20248;&#21270;&#12290;&#32473;&#23450;&#19968;&#20010;&#27979;&#35797;&#23454;&#20363;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#36741;&#21161;&#20219;&#21153;&#26469;&#35843;&#25972;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26356;&#26032;&#21518;&#30340;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Point-TTA, a novel test-time adaptation framework for point cloud registration (PCR) that improves the generalization and the performance of registration models. While learning-based approaches have achieved impressive progress, generalization to unknown testing environments remains a major challenge due to the variations in 3D scans. Existing methods typically train a generic model and the same trained model is applied on each instance during testing. This could be sub-optimal since it is difficult for the same model to handle all the variations during testing. In this paper, we propose a test-time adaptation approach for PCR. Our model can adapt to unseen distributions at test-time without requiring any prior knowledge of the test data. Concretely, we design three self-supervised auxiliary tasks that are optimized jointly with the primary PCR task. Given a test instance, we adapt our model using these auxiliary tasks and the updated model is used to perform the inference. 
&lt;/p&gt;</description></item><item><title>BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16458</link><description>&lt;p&gt;
BioCoder: &#19968;&#31181;&#24102;&#26377;&#19978;&#19979;&#25991;&#35821;&#29992;&#30693;&#35782;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16458
&lt;/p&gt;
&lt;p&gt;
BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#20195;&#30721;&#29983;&#25104;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#36755;&#20986;&#26469;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#65292;&#29983;&#25104;&#21151;&#33021;&#31243;&#24207;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#37327;&#22823;&#12289;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25805;&#20316;&#21644;&#22797;&#26434;&#30340;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#32780;&#38754;&#20020;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BioCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#19982;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#26377;&#20851;&#65292;BioCoder&#28085;&#30422;&#20102;&#21487;&#33021;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;GitHub&#30340;1026&#20010;Python&#21644;Java&#20989;&#25968;&#21644;1243&#20010;&#26041;&#27861;&#65292;&#20197;&#21450;&#26469;&#33258;Rosalind&#39033;&#30446;&#30340;253&#20010;&#31034;&#20363;&#12290;BioCoder&#36824;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#25105;&#20204;&#24050;&#32463;&#24212;&#29992;&#23427;&#26469;&#35780;&#20272;&#35768;&#22810;&#27169;&#22411;&#65292;&#21253;&#25324;InCoder&#12289;CodeGen&#12289;CodeGen2&#12289;SantaCoder&#12289;StarCoder&#12289;StarCoder+&#12289;InstructCodeT&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#31454;&#20105;&#36873;&#25321;&#30340;&#22240;&#26524;&#25112;&#30053;&#23398;&#20064;&#20013;&#30340;&#20195;&#29702;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#36873;&#25321;&#35268;&#21017;&#30340;&#25968;&#23398;&#24418;&#24335;&#21644;&#23454;&#29616;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.16262</link><description>&lt;p&gt;
&#26377;&#31454;&#20105;&#36873;&#25321;&#30340;&#22240;&#26524;&#25112;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Strategic Learning with Competitive Selection. (arXiv:2308.16262v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16262
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#31454;&#20105;&#36873;&#25321;&#30340;&#22240;&#26524;&#25112;&#30053;&#23398;&#20064;&#20013;&#30340;&#20195;&#29702;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#36873;&#25321;&#35268;&#21017;&#30340;&#25968;&#23398;&#24418;&#24335;&#21644;&#23454;&#29616;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;&#20915;&#31574;&#32773;&#19979;&#30340;&#22240;&#26524;&#25112;&#30053;&#23398;&#20064;&#20013;&#30340;&#20195;&#29702;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35299;&#20915;&#20102;&#20854;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#30001;&#20195;&#29702;&#20154;&#35780;&#20272;&#21644;&#36873;&#25321;&#32452;&#25104;&#30340;&#36873;&#25321;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#26159;&#20043;&#21069;&#30740;&#31350;&#20013;&#20851;&#27880;&#30340;&#22266;&#23450;&#20195;&#29702;&#20154;&#27744;&#12290;&#24403;&#27599;&#20010;&#20915;&#31574;&#32773;&#36890;&#36807;&#26368;&#22823;&#21270;&#33258;&#36523;&#25928;&#29992;&#26469;&#21333;&#26041;&#38754;&#36873;&#25321;&#20195;&#29702;&#20154;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20339;&#30340;&#36873;&#25321;&#35268;&#21017;&#26159;&#22312;&#36873;&#25321;&#26368;&#20339;&#20195;&#29702;&#20154;&#21644;&#25552;&#20379;&#28608;&#21169;&#20197;&#26368;&#22823;&#21270;&#20195;&#29702;&#20154;&#25913;&#36827;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26368;&#20339;&#36873;&#25321;&#35268;&#21017;&#20381;&#36182;&#20110;&#20195;&#29702;&#20154;&#32467;&#26524;&#30340;&#38169;&#35823;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20915;&#31574;&#32773;&#30340;&#26368;&#20339;&#36873;&#25321;&#35268;&#21017;&#19981;&#20250;&#23548;&#33268;&#20195;&#29702;&#20154;&#32467;&#26524;&#24694;&#21270;&#65292;&#20063;&#19981;&#20250;&#36896;&#25104;&#19981;&#20844;&#27491;&#30340;&#38477;&#20302;&#20195;&#29702;&#20154;&#36873;&#25321;&#26426;&#20250;&#30340;&#26465;&#20214;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26368;&#20339;&#36873;&#25321;&#35268;&#21017;&#30340;&#25968;&#23398;&#24418;&#24335;&#21644;&#19968;&#31181;&#23454;&#29616;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of agent selection in causal strategic learning under multiple decision makers and address two key challenges that come with it. Firstly, while much of prior work focuses on studying a fixed pool of agents that remains static regardless of their evaluations, we consider the impact of selection procedure by which agents are not only evaluated, but also selected. When each decision maker unilaterally selects agents by maximising their own utility, we show that the optimal selection rule is a trade-off between selecting the best agents and providing incentives to maximise the agents' improvement. Furthermore, this optimal selection rule relies on incorrect predictions of agents' outcomes. Hence, we study the conditions under which a decision maker's optimal selection rule will not lead to deterioration of agents' outcome nor cause unjust reduction in agents' selection chance. To that end, we provide an analytical form of the optimal selection rule and a mechanism to r
&lt;/p&gt;</description></item><item><title>LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16137</link><description>&lt;p&gt;
LM-Infinite: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21363;&#26102;&#38271;&#24230;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16137
&lt;/p&gt;
&lt;p&gt;
LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;Transformer-based&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;LLM&#22312;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#30528;&#23545;&#38271;&#26102;&#38388;&#25512;&#29702;&#36807;&#31243;&#25110;&#29702;&#35299;&#26356;&#22823;&#19978;&#19979;&#25991;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;LLM&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#26041;&#26696;&#23558;&#35757;&#32451;&#24207;&#21015;&#25130;&#26029;&#21040;&#22266;&#23450;&#38271;&#24230;&#65288;&#20363;&#22914;LLaMa&#30340;2048&#65289;&#12290;&#21363;&#20351;&#20351;&#29992;&#20102;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;LLM&#22312;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20043;&#21518;&#24448;&#24448;&#38590;&#20197;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#26356;&#19981;&#29992;&#35828;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#20102;&#12290;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#22312;&#26356;&#38271;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#24448;&#24448;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#30828;&#20214;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#38656;&#35201;&#36827;&#34892;&#20180;&#32454;&#30340;&#35757;&#32451;&#36807;&#31243;&#35774;&#35745;&#12290;&#20026;&#20102;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30740;&#31350;&#20102;&#20027;&#35201;&#30340;&#20998;&#24067;&#22806;(OOD) f
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer corpora often involves daunting hardware and time costs and requires careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) f
&lt;/p&gt;</description></item><item><title>&#32654;&#22269;&#27861;&#24459;&#38656;&#35201;&#21152;&#24378;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#65292;&#20197;&#22635;&#34917;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15906</link><description>&lt;p&gt;
&#32654;&#22269;&#27861;&#24459;&#20307;&#31995;&#26159;&#21542;&#20934;&#22791;&#22909;&#24212;&#23545;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#25361;&#25112;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is the U.S. Legal System Ready for AI's Challenges to Human Values?. (arXiv:2308.15906v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15906
&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#27861;&#24459;&#38656;&#35201;&#21152;&#24378;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#65292;&#20197;&#22635;&#34917;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#35843;&#26597;&#20102;&#32654;&#22269;&#27861;&#24459;&#22312;&#38754;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#19987;&#23478;&#30740;&#35752;&#20250;&#26399;&#38388;&#21046;&#23450;&#30340;&#22810;&#31181;&#20551;&#35774;&#24773;&#26223;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#33258;&#20027;&#26435;&#12289;&#38544;&#31169;&#26435;&#12289;&#23562;&#20005;&#12289;&#22810;&#26679;&#24615;&#12289;&#24179;&#31561;&#20197;&#21450;&#36523;&#24515;&#20581;&#24247;&#31561;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#23466;&#27861;&#21644;&#27665;&#26435;&#27861;&#20284;&#20046;&#26080;&#27861;&#23545;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#27495;&#35270;&#24615;&#20135;&#20986;&#25552;&#20379;&#36275;&#22815;&#30340;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#25105;&#20204;&#25490;&#38500;&#31532;230&#26465;&#27454;&#25552;&#20379;&#30340;&#36131;&#20219;&#20445;&#25252;&#65292;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22797;&#26434;&#21644;&#19981;&#36879;&#26126;&#24615;&#65292;&#35777;&#26126;&#35837;&#35876;&#21644;&#20135;&#21697;&#36131;&#20219;&#32034;&#36180;&#30340;&#22240;&#26524;&#20851;&#31995;&#20063;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24102;&#26469;&#30340;&#29420;&#29305;&#21644;&#38590;&#20197;&#39044;&#27979;&#30340;&#23041;&#32961;&#65292;&#25105;&#20204;&#20027;&#24352;&#24314;&#31435;&#33021;&#22815;&#36866;&#24212;&#26032;&#23041;&#32961;&#24182;&#20026;&#34892;&#19994;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#30340;&#27861;&#24459;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our interdisciplinary study investigates how effectively U.S. laws confront the challenges posed by Generative AI to human values. Through an analysis of diverse hypothetical scenarios crafted during an expert workshop, we have identified notable gaps and uncertainties within the existing legal framework regarding the protection of fundamental values, such as autonomy, privacy, dignity, diversity, equality, and physical/mental well-being. Constitutional and civil rights, it appears, may not provide sufficient protection against AI-generated discriminatory outputs. Furthermore, even if we exclude the liability shield provided by Section 230, proving causation for defamation and product liability claims is a challenging endeavor due to the intricate and opaque nature of AI systems. To address the unique and unforeseeable threats posed by Generative AI, we advocate for legal frameworks that evolve to recognize new threat and provide proactive, auditable guidelines to industry stakeholders
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EchoCLIP&#30340;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#24515;&#33039;&#36229;&#22768;&#35299;&#35835;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#22823;&#37327;&#30340;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#21644;&#19987;&#23478;&#35299;&#35835;&#26469;&#23454;&#29616;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#24182;&#22312;&#24515;&#33039;&#21151;&#33021;&#35780;&#20272;&#21644;&#26893;&#20837;&#24515;&#33039;&#20869;&#22120;&#20214;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15670</link><description>&lt;p&gt;
&#29992;&#20110;&#24515;&#33039;&#36229;&#22768;&#35299;&#35835;&#30340;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multimodal Foundation Models For Echocardiogram Interpretation. (arXiv:2308.15670v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15670
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EchoCLIP&#30340;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#24515;&#33039;&#36229;&#22768;&#35299;&#35835;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#22823;&#37327;&#30340;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#21644;&#19987;&#23478;&#35299;&#35835;&#26469;&#23454;&#29616;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#24182;&#22312;&#24515;&#33039;&#21151;&#33021;&#35780;&#20272;&#21644;&#26893;&#20837;&#24515;&#33039;&#20869;&#22120;&#20214;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#28145;&#24230;&#23398;&#20064;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#21307;&#23398;&#25104;&#20687;&#30340;&#32972;&#26223;&#19979;&#65292;&#23558;&#22270;&#20687;&#26144;&#23556;&#21040;&#35821;&#35328;&#27010;&#24565;&#21453;&#26144;&#20102;&#35786;&#26029;&#22270;&#20687;&#35299;&#37322;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#28982;&#32780;&#24403;&#21069;&#36890;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#35757;&#32451;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#24182;&#32771;&#34385;&#21040;&#24515;&#33039;&#29983;&#29702;&#30340;&#33539;&#22260;&#65292;&#25105;&#20204;&#21033;&#29992;1,032,975&#20010;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#21644;&#30456;&#24212;&#30340;&#19987;&#23478;&#35299;&#35835;&#24320;&#21457;&#20102;EchoCLIP&#65292;&#19968;&#20010;&#29992;&#20110;&#24515;&#33039;&#36229;&#22768;&#30340;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;&#12290;EchoCLIP&#22312;&#24515;&#33039;&#21151;&#33021;&#35780;&#20272;&#65288;&#22806;&#37096;&#39564;&#35777;&#24038;&#23460;&#23556;&#34880;&#20998;&#25968;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;7.1%&#65289;&#21644;&#26893;&#20837;&#24515;&#33039;&#20869;&#22120;&#20214;&#30340;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#65288;&#26410;&#32463;&#36807;&#26174;&#24335;&#35757;&#32451;&#65289;&#24615;&#33021;&#65288;&#36215;&#25615;&#22120;&#21644;&#20154;&#24037;&#24515;&#33039;&#29923;&#33180;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#22312;0.84&#33267;0.98&#20043;&#38388;&#65289;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#38271;&#19978;&#19979;&#25991;&#21487;&#21464;&#27169;&#22411;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal deep learning foundation models can learn the relationship between images and text. In the context of medical imaging, mapping images to language concepts reflects the clinical task of diagnostic image interpretation, however current general-purpose foundation models do not perform well in this context because their training corpus have limited medical text and images. To address this challenge and account for the range of cardiac physiology, we leverage 1,032,975 cardiac ultrasound videos and corresponding expert interpretations to develop EchoCLIP, a multimodal foundation model for echocardiography. EchoCLIP displays strong zero-shot (not explicitly trained) performance in cardiac function assessment (external validation left ventricular ejection fraction mean absolute error (MAE) of 7.1%) and identification of implanted intracardiac devices (areas under the curve (AUC) between 0.84 and 0.98 for pacemakers and artificial heart valves). We also developed a long-context vari
&lt;/p&gt;</description></item><item><title>&#36807;&#24230;&#21387;&#32553;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23427;&#38480;&#21046;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#20449;&#24687;&#20256;&#36882;&#65292;&#24433;&#21709;&#20102;&#22312;&#38656;&#35201;&#24191;&#27867;&#19978;&#19979;&#25991;&#27934;&#23519;&#21147;&#30340;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.15568</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Over-Squashing in Graph Neural Networks: A Comprehensive survey. (arXiv:2308.15568v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15568
&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21387;&#32553;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23427;&#38480;&#21046;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#20449;&#24687;&#20256;&#36882;&#65292;&#24433;&#21709;&#20102;&#22312;&#38656;&#35201;&#24191;&#27867;&#19978;&#19979;&#25991;&#27934;&#23519;&#21147;&#30340;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#31181;&#38761;&#21629;&#24615;&#33539; Paradigm&#65292;&#20026;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#20851;&#31995;&#25552;&#20379;&#20102;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;GNN&#30340;&#22522;&#26412;&#26550;&#26500;&#28041;&#21450;&#36890;&#36807;&#28040;&#24687;&#32858;&#21512;&#21644;&#36716;&#25442;&#22312;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#30340;&#26426;&#21046;&#65292;&#22312;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#24050;&#32463;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#28508;&#22312;&#23454;&#21147;&#36935;&#21040;&#20102;&#22312;&#38656;&#35201;&#24191;&#27867;&#19978;&#19979;&#25991;&#27934;&#23519;&#21147;&#30340;&#24773;&#20917;&#19979;&#22266;&#26377;&#30340;&#38480;&#21046;&#12290;&#22312;&#26576;&#20123;&#24773;&#22659;&#20013;&#65292;&#20934;&#30830;&#30340;&#39044;&#27979;&#19981;&#20165;&#21462;&#20915;&#20110;&#33410;&#28857;&#30340;&#21363;&#26102;&#23616;&#37096;&#29615;&#22659;&#65292;&#36824;&#21462;&#20915;&#20110;&#36328;&#36234;&#24191;&#22495;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#23545;&#38271;&#31243;&#20449;&#24687;&#20256;&#25773;&#30340;&#38656;&#27714;&#26292;&#38706;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#8220;&#36807;&#24230;&#21387;&#32553;&#8221;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#20854;&#20013;&#26469;&#33258;&#36828;&#31163;&#33410;&#28857;&#30340;&#20449;&#24687;&#27969;&#30340;&#21487;&#38752;&#24615;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a revolutionary paradigm in the realm of machine learning, offering a transformative approach to dissect intricate relationships inherent in graph-structured data. The foundational architecture of most GNNs involves the dissemination of information through message aggregation and transformation among interconnected nodes, a mechanism that has demonstrated remarkable efficacy across diverse applications encompassing node classification, link prediction, and recommendation systems. Nonetheless, their potential prowess encounters a restraint intrinsic to scenarios necessitating extensive contextual insights. In certain contexts, accurate predictions hinge not only upon a node's immediate local surroundings but also on interactions spanning far-reaching domains. This intricate demand for long-range information dissemination exposes a pivotal challenge recognized as "over-squashing," wherein the fidelity of information flow from distant nodes bec
&lt;/p&gt;</description></item><item><title>ParaGuide&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;&#65292;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#65292;&#36890;&#36807;&#26799;&#24230;&#24341;&#23548;&#21644;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#30340;&#39118;&#26684;&#36716;&#21464;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.15459</link><description>&lt;p&gt;
ParaGuide: &#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;
&lt;/p&gt;
&lt;p&gt;
ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer. (arXiv:2308.15459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15459
&lt;/p&gt;
&lt;p&gt;
ParaGuide&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;&#65292;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#65292;&#36890;&#36807;&#26799;&#24230;&#24341;&#23548;&#21644;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#30340;&#39118;&#26684;&#36716;&#21464;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#26159;&#22312;&#20445;&#30041;&#24847;&#20041;&#30340;&#21516;&#26102;&#36716;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#23646;&#24615;&#30340;&#20219;&#21153;&#12290;&#30446;&#26631;&#39118;&#26684;&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#23450;&#20041;&#65292;&#20174;&#21333;&#19968;&#23646;&#24615;&#65288;&#20363;&#22914;&#27491;&#24335;&#24615;&#65289;&#21040;&#20316;&#32773;&#65288;&#20363;&#22914;&#33678;&#22763;&#27604;&#20122;&#65289;&#12290;&#20808;&#21069;&#30340;&#26080;&#30417;&#30563;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#20165;&#36866;&#29992;&#20110;&#22266;&#23450;&#30340;&#39118;&#26684;&#38598;&#65292;&#25110;&#38656;&#35201;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;ParaGuide&#21033;&#29992;&#20102;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#26469;&#33258;&#29616;&#25104;&#30340;&#20998;&#31867;&#22120;&#21644;&#24378;&#22823;&#30340;&#39118;&#26684;&#23884;&#20837;&#22120;&#30340;&#26799;&#24230;&#24341;&#23548;&#65292;&#20197;&#36716;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;Enron&#37038;&#20214;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#20154;&#24037;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27491;&#24335;&#24615;&#21644;... (&#20869;&#23481;&#22826;&#22810;&#65292;&#35831;&#21442;&#32771;&#33521;&#25991;&#25688;&#35201;)
&lt;/p&gt;
&lt;p&gt;
Textual style transfer is the task of transforming stylistic properties of text while preserving meaning. Target "styles" can be defined in numerous ways, ranging from single attributes (e.g, formality) to authorship (e.g, Shakespeare). Previous unsupervised style-transfer approaches generally rely on significant amounts of labeled data for only a fixed set of styles or require large language models. In contrast, we introduce a novel diffusion-based framework for general-purpose style transfer that can be flexibly adapted to arbitrary target styles at inference time. Our parameter-efficient approach, ParaGuide, leverages paraphrase-conditioned diffusion models alongside gradient-based guidance from both off-the-shelf classifiers and strong existing style embedders to transform the style of text while preserving semantic information. We validate the method on the Enron Email Corpus, with both human and automatic evaluations, and find that it outperforms strong baselines on formality, se
&lt;/p&gt;</description></item><item><title>C3AL&#26159;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65292;&#36890;&#36807;&#23558;&#35266;&#27979;&#26641;&#20316;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#24182;&#26368;&#23567;&#21270;&#27979;&#35797;&#27425;&#25968;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.14781</link><description>&lt;p&gt;
&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conflict-Aware Active Automata Learning. (arXiv:2308.14781v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14781
&lt;/p&gt;
&lt;p&gt;
C3AL&#26159;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65292;&#36890;&#36807;&#23558;&#35266;&#27979;&#26641;&#20316;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#24182;&#26368;&#23567;&#21270;&#27979;&#35797;&#27425;&#25968;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#31639;&#27861;&#22312;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65288;&#21516;&#19968;&#36755;&#20837;&#23545;&#24212;&#19981;&#21516;&#36755;&#20986;&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#31181;&#22266;&#26377;&#30340;&#20914;&#31361;&#24674;&#22797;&#33021;&#21147;&#19981;&#36275;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#23384;&#22312;&#22122;&#22768;&#25110;&#23398;&#20064;&#20013;&#30340;&#31995;&#32479;&#21464;&#21270;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#65288;C3AL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#22788;&#29702;&#20914;&#31361;&#20449;&#24687;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#25152;&#35859;&#30340;&#35266;&#27979;&#26641;&#35270;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#12290;&#23613;&#31649;&#36825;&#20010;&#24819;&#27861;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#25506;&#32034;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#65292;&#24182;&#22312;&#38754;&#23545;&#20914;&#31361;&#26102;&#26368;&#23567;&#21270;&#23545;&#27491;&#22312;&#23398;&#20064;&#30340;&#31995;&#32479;&#25191;&#34892;&#30340;&#27979;&#35797;&#27425;&#25968;&#65292;&#20805;&#20998;&#21457;&#25381;&#20102;&#23427;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;C3AL&#65292;&#28085;&#30422;&#20102;30&#22810;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#30446;&#26631;&#21644;18,000&#22810;&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;C3AL&#26159;&#19968;&#20010;&#21512;&#36866;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active automata learning algorithms cannot easily handle \emph{conflict} in the observation data (different outputs observed for the same inputs). This inherent inability to recover after a conflict impairs their effective applicability in scenarios where noise is present or the system under learning is mutating.  We propose the Conflict-Aware Active Automata Learning (C3AL) framework to enable handling conflicting information during the learning process. The core idea is to consider the so-called observation tree as a first-class citizen in the learning process. Though this idea is explored in recent work, we take it to its full effect by enabling its use with any existing learner and minimizing the number of tests performed on the system under learning, specially in the face of conflicts. We evaluate C3AL in a large set of benchmarks, covering over 30 different realistic targets, and over 18,000 different scenarios. The results of the evaluation show that C3AL is a suitable alternati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#34892;&#21160;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#20013;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.14284</link><description>&lt;p&gt;
LLM&#24378;&#21270;&#20102;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
LLM Powered Sim-to-real Transfer for Traffic Signal Control. (arXiv:2308.14284v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#34892;&#21160;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#20013;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#26377;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;TSC&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25552;&#20379;&#39640;&#25928;&#30340;&#20132;&#36890;&#21644;&#20943;&#36731;&#25317;&#22581;&#28010;&#36153;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#65292;&#24403;&#22312;&#20223;&#30495;&#22120;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#26102;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#25166;&#26681;&#34892;&#21160;&#36716;&#25442;&#65292;&#26469;&#29702;&#35299;&#21644;&#25551;&#36848;&#31995;&#32479;&#21160;&#24577;&#12290;&#36890;&#36807;&#25509;&#21463;&#22635;&#31354;&#25552;&#31034;&#27169;&#26495;&#65292;&#24182;&#26681;&#25454;&#21487;&#20197;&#35775;&#38382;&#30340;&#19978;&#19979;&#25991;&#22635;&#20889;&#31572;&#26696;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24212;&#29992;&#20110;&#23545;&#31995;&#32479;&#21160;&#24577;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks aiming to provide efficient transportation and mitigate congestion waste. In recent, promising results have been attained by Reinforcement Learning (RL) methods through trial and error in simulators, bringing confidence in solving cities' congestion headaches. However, there still exist performance gaps when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the system dynamic difference between the training simulator and the real-world environments. The Large Language Models (LLMs) are trained on mass knowledge and proved to be equipped with astonishing inference abilities. In this work, we leverage LLMs to understand and profile the system dynamics by a prompt-based grounded action transformation. Accepting the cloze prompt template, and then filling in the answer based on accessible context, the pre-trained LLM's inference ability is exploited and applied to understa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#20998;&#21106;&#21644;&#26816;&#27979;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#29273;&#31185;&#20840;&#26223; X &#23556;&#32447;&#20013;&#33258;&#21160;&#26816;&#27979;&#24322;&#24120;&#29273;&#40831;&#21450;&#20854;&#26522;&#20030;&#21495;&#12290;</title><link>http://arxiv.org/abs/2308.14161</link><description>&lt;p&gt;
&#20026;Dentex Challenge 2023&#25972;&#21512;&#20998;&#21106;&#21644;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Intergrated Segmentation and Detection Models for Dentex Challenge 2023. (arXiv:2308.14161v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#20998;&#21106;&#21644;&#26816;&#27979;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#29273;&#31185;&#20840;&#26223; X &#23556;&#32447;&#20013;&#33258;&#21160;&#26816;&#27979;&#24322;&#24120;&#29273;&#40831;&#21450;&#20854;&#26522;&#20030;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29273;&#31185;&#20840;&#26223;X&#23556;&#32447;&#22312;&#29273;&#31185;&#35786;&#26029;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#20174;&#29273;&#31185;&#20840;&#26223;X&#23556;&#32447;&#20013;&#33258;&#21160;&#26816;&#27979;&#30142;&#30149;&#21487;&#20197;&#24110;&#21161;&#29273;&#21307;&#26356;&#39640;&#25928;&#22320;&#35786;&#26029;&#30142;&#30149;&#12290;Dentex Challenge 2023&#26159;&#19968;&#20010;&#33258;&#21160;&#26816;&#27979;&#29273;&#31185;&#20840;&#26223;X&#23556;&#32447;&#20013;&#24322;&#24120;&#29273;&#40831;&#21450;&#20854;&#26522;&#20030;&#21495;&#30340;&#27604;&#36187;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#20102;&#20998;&#21106;&#21644;&#26816;&#27979;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#26816;&#27979;&#24322;&#24120;&#29273;&#40831;&#24182;&#33719;&#21462;&#20854;&#26522;&#20030;&#21495;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/xyzlancehe/DentexSegAndDet&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dental panoramic x-rays are commonly used in dental diagnosing. With the development of deep learning, auto detection of diseases from dental panoramic x-rays can help dentists to diagnose diseases more efficiently.The Dentex Challenge 2023 is a competition for automatic detection of abnormal teeth along with their enumeration ids from dental panoramic x-rays. In this paper, we propose a method integrating segmentation and detection models to detect abnormal teeth as well as obtain their enumeration ids.Our codes are available at https://github.com/xyzlancehe/DentexSegAndDet.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#22312;&#36825;&#31181;&#35266;&#23519;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#30340;&#26041;&#27861;&#65288;DeCA&#65289;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;</title><link>http://arxiv.org/abs/2308.13976</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Label Denoising through Cross-Model Agreement. (arXiv:2308.13976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#22312;&#36825;&#31181;&#35266;&#23519;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#30340;&#26041;&#27861;&#65288;DeCA&#65289;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20174;&#26377;&#22122;&#22768;&#30340;&#26631;&#31614;&#23398;&#20064;&#26159;&#38750;&#24120;&#24120;&#35265;&#30340;&#12290;&#35760;&#24518;&#36825;&#20123;&#26377;&#22122;&#22768;&#30340;&#26631;&#31614;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26377;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#40065;&#26834;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#65288;DeCA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#26368;&#23567;&#21270;&#30001;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;DeCA&#26041;&#27861;&#24212;&#29992;&#20110;&#20108;&#36827;&#21046;&#26631;&#31614;&#24773;&#26223;&#21644;&#22810;&#26631;&#31614;&#24773;&#26223;&#12290;&#23545;&#20110;&#20108;&#36827;&#21046;&#26631;&#31614;&#24773;&#26223;&#65292;&#25105;&#20204;&#36873;&#25321;&#38544;&#24335;&#21453;&#39304;&#25512;&#33616;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from corrupted labels is very common in real-world machine-learning applications. Memorizing such noisy labels could affect the learning of the model, leading to sub-optimal performances. In this work, we propose a novel framework to learn robust machine-learning models from noisy labels. Through an empirical study, we find that different models make relatively similar predictions on clean examples, while the predictions on noisy examples vary much more across different models. Motivated by this observation, we propose \em denoising with cross-model agreement \em (DeCA) which aims to minimize the KL-divergence between the true label distributions parameterized by two machine learning models while maximizing the likelihood of data observation. We employ the proposed DeCA on both the binary label scenario and the multiple label scenario. For the binary label scenario, we select implicit feedback recommendation as the downstream task and conduct experiments with four state-of-the
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#25918;&#30340;&#21355;&#26143;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;HYPSO-1&#21355;&#26143;&#30340;&#28023;&#38470;&#20113;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#19988;&#36890;&#36807;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13679</link><description>&lt;p&gt;
&#19968;&#39063;&#24320;&#25918;&#30340;&#21355;&#26143;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#19982;&#26469;&#33258;HYPSO-1&#21355;&#26143;&#30340;&#28023;&#38470;&#20113;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
An Open Hyperspectral Dataset with Sea-Land-Cloud Ground-Truth from the HYPSO-1 Satellite. (arXiv:2308.13679v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13679
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#25918;&#30340;&#21355;&#26143;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;HYPSO-1&#21355;&#26143;&#30340;&#28023;&#38470;&#20113;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#19988;&#36890;&#36807;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20809;&#35889;&#25104;&#20687;&#34987;&#21355;&#26143;&#29992;&#20110;&#31354;&#38388;&#36965;&#24863;&#65292;&#22312;&#20687;HYPSO-1&#36825;&#26679;&#30340;&#21355;&#26143;&#19978;&#65292;&#38754;&#20020;&#30528;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#24433;&#21709;&#20102;&#23545;&#38656;&#27714;&#36825;&#20123;&#22320;&#38754;&#30495;&#23454;&#26631;&#27880;&#30340;AI&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HYPSO-1&#28023;&#38470;&#20113;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;200&#20010;&#19981;&#21516;&#39640;&#20809;&#35889;&#22270;&#20687;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#26469;&#33258;&#20110;HYPSO-1&#20219;&#21153;&#65292;&#21487;&#25552;&#20379;&#26410;&#32463;&#26657;&#20934;&#21644;&#32463;&#36807;&#26657;&#20934;&#30340;&#24418;&#24335;&#65292;&#20379;&#22320;&#29699;&#35266;&#27979;&#30340;&#31185;&#23398;&#30740;&#31350;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;38&#20010;&#22270;&#20687;&#65292;&#22312;&#20687;&#32032;&#32423;&#21035;&#19978;&#21253;&#21547;&#24635;&#35745;&#32422;2500&#19975;&#20010;&#20026;&#28023;&#27915;/&#38470;&#22320;/&#20113;&#20998;&#31867;&#26631;&#35760;&#30340;&#20809;&#35889;&#29305;&#24449;&#12290;&#20026;&#20102;&#23637;&#31034;&#25968;&#25454;&#38598;&#21450;&#20854;&#26631;&#35760;&#23376;&#38598;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;1D&#23436;&#20840;&#21367;&#31215;&#32593;&#32476;&#65289;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#12290;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#65292;&#22320;&#38754;&#30495;&#23454;&#26631;&#27880;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#36719;&#20214;&#20195;&#30721;&#21487;&#22312;&#32593;&#31449;https://ntnu-sm&#19978;&#20813;&#36153;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral Imaging, employed in satellites for space remote sensing, like HYPSO-1, faces constraints due to few labeled data sets, affecting the training of AI models demanding these ground-truth annotations. In this work, we introduce The HYPSO-1 Sea-Land-Cloud-Labeled Dataset, an open dataset with 200 diverse hyperspectral images from the HYPSO-1 mission, available in both raw and calibrated forms for scientific research in Earth observation. Moreover, 38 of these images from different countries include ground-truth labels at pixel-level totaling about 25 million spectral signatures labeled for sea/land/cloud categories. To demonstrate the potential of the dataset and its labeled subset, we have additionally optimized a deep learning model (1D Fully Convolutional Network), achieving superior performance to the current state of the art. The complete dataset, ground-truth labels, deep learning model, and software code are openly accessible for download at the website https://ntnu-sm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13570</link><description>&lt;p&gt;
&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38543;&#26426;&#37197;&#32622;&#26426;
&lt;/p&gt;
&lt;p&gt;
Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#65288;IAI&#65289;&#20013;&#65292;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#24314;&#27169;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#24378;&#22823;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#35774;&#22791;&#26469;&#22788;&#29702;&#22823;&#37327;&#30340;&#28014;&#28857;&#25968;&#25454;&#12290;&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20197;&#24378;&#35843;&#23545;&#20110;&#24037;&#19994;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#21644;&#26377;&#20215;&#20540;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;&#19982;&#20855;&#26377;&#20108;&#20540;&#21270;&#23454;&#29616;&#30340;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#30456;&#27604;&#65292;SCMs&#30340;&#27169;&#22411;&#23384;&#20648;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#38500;&#20102;SCM&#23398;&#20064;&#22120;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#20316;&#20026;&#26412;&#25991;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;SCMs&#30340;&#23398;&#20064;&#33021;&#21147;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23454;&#39564;&#30740;&#31350;&#20063;&#36827;&#34892;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20249;&#20276;&#33310;&#32773;&#29983;&#25104;&#30340;&#22810;&#33310;&#32773;&#21512;&#25104;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#20445;&#25345;&#19982;&#20027;&#23548;&#33310;&#32773;&#26102;&#38388;&#21327;&#35843;&#30340;&#21516;&#26102;&#30830;&#20445;&#20249;&#20276;&#33310;&#32773;&#30340;&#21487;&#25511;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#19982;&#20320;&#20849;&#33310;&#8221;&#30340;&#19977;&#38454;&#27573;&#26694;&#26550;&#65288;DanY&#65289;&#65292;&#23427;&#33021;&#33258;&#21160;&#35774;&#35745;&#20249;&#20276;&#33310;&#32773;&#30340;&#23039;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.13551</link><description>&lt;p&gt;
&#19982;&#20320;&#20849;&#33310;&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#22810;&#26679;&#24615;&#21487;&#25511;&#30340;&#33310;&#32773;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dance with You: The Diversity Controllable Dancer Generation via Diffusion Models. (arXiv:2308.13551v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20249;&#20276;&#33310;&#32773;&#29983;&#25104;&#30340;&#22810;&#33310;&#32773;&#21512;&#25104;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#20445;&#25345;&#19982;&#20027;&#23548;&#33310;&#32773;&#26102;&#38388;&#21327;&#35843;&#30340;&#21516;&#26102;&#30830;&#20445;&#20249;&#20276;&#33310;&#32773;&#30340;&#21487;&#25511;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#19982;&#20320;&#20849;&#33310;&#8221;&#30340;&#19977;&#38454;&#27573;&#26694;&#26550;&#65288;DanY&#65289;&#65292;&#23427;&#33021;&#33258;&#21160;&#35774;&#35745;&#20249;&#20276;&#33310;&#32773;&#30340;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#34394;&#25311;&#29615;&#22659;&#20013;&#29992;&#20110;&#20154;&#38469;&#20132;&#20114;&#30340;&#25968;&#23383;&#20154;&#31867;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#22810;&#33310;&#32773;&#21512;&#25104;&#20219;&#21153;&#65292;&#31216;&#20026;&#20249;&#20276;&#33310;&#32773;&#29983;&#25104;&#65292;&#20854;&#28041;&#21450;&#21512;&#25104;&#33021;&#22815;&#19982;&#29992;&#25143;&#19968;&#36215;&#36339;&#33310;&#30340;&#34394;&#25311;&#20154;&#31867;&#33310;&#32773;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#25511;&#21046;&#20027;&#23548;&#33310;&#32773;&#21644;&#20249;&#20276;&#33310;&#32773;&#20043;&#38388;&#30340;&#23039;&#21183;&#22810;&#26679;&#24615;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#26680;&#24515;&#26159;&#30830;&#20445;&#29983;&#25104;&#30340;&#20249;&#20276;&#33310;&#32773;&#20855;&#26377;&#21487;&#25511;&#30340;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#19982;&#20027;&#23548;&#33310;&#32773;&#20445;&#25345;&#26102;&#38388;&#19978;&#30340;&#21327;&#35843;&#12290;&#19982;&#20197;&#24448;&#36890;&#36807;&#38899;&#20048;&#39537;&#21160;&#29983;&#25104;&#33310;&#36424;&#21160;&#20316;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#22810;&#26679;&#24615;&#12289;&#20027;&#23548;&#33310;&#32773;&#30340;&#23039;&#21183;&#20197;&#21450;&#20276;&#22863;&#38899;&#20048;&#33258;&#21160;&#35774;&#35745;&#20249;&#20276;&#33310;&#32773;&#30340;&#23039;&#21183;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#19982;&#20320;&#20849;&#33310;&#8221;&#30340;&#19977;&#38454;&#27573;&#26694;&#26550;&#65288;DanY&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#32500;&#23039;&#21183;&#25910;&#38598;&#38454;&#27573;&#26469;&#25910;&#38598;&#21508;&#31181;&#22522;&#26412;&#33310;&#36424;&#23039;&#21183;&#20316;&#20026;&#21442;&#32771;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, digital humans for interpersonal interaction in virtual environments have gained significant attention. In this paper, we introduce a novel multi-dancer synthesis task called partner dancer generation, which involves synthesizing virtual human dancers capable of performing dance with users. The task aims to control the pose diversity between the lead dancer and the partner dancer. The core of this task is to ensure the controllable diversity of the generated partner dancer while maintaining temporal coordination with the lead dancer. This scenario varies from earlier research in generating dance motions driven by music, as our emphasis is on automatically designing partner dancer postures according to pre-defined diversity, the pose of lead dancer, as well as the accompanying tunes. To achieve this objective, we propose a three-stage framework called Dance-with-You (DanY). Initially, we employ a 3D Pose Collection stage to collect a wide range of basic dance poses as referenc
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20174;&#20219;&#24847;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;2D&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23398;&#20064;3D&#20154;&#20307;-&#29289;&#20307;&#31354;&#38388;&#20851;&#31995;&#30340;&#24213;&#23618;&#24120;&#35782;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;3D&#20132;&#20114;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#21644;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12288</link><description>&lt;p&gt;
CHORUS: &#20174;&#26080;&#38480;&#21512;&#25104;&#22270;&#20687;&#20013;&#23398;&#20064;3D&#20154;&#20307;-&#29289;&#20307;&#31354;&#38388;&#20851;&#31995;&#30340;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images. (arXiv:2308.12288v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12288
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20174;&#20219;&#24847;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;2D&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23398;&#20064;3D&#20154;&#20307;-&#29289;&#20307;&#31354;&#38388;&#20851;&#31995;&#30340;&#24213;&#23618;&#24120;&#35782;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;3D&#20132;&#20114;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#21644;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#25945;&#26426;&#22120;&#29702;&#35299;&#21644;&#24314;&#27169;&#22810;&#26679;&#21270;&#30340;3D&#20154;&#20307;-&#29289;&#20307;&#20132;&#20114;&#30340;&#24213;&#23618;&#31354;&#38388;&#24120;&#35782;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23384;&#22312;&#21487;&#20197;&#34987;&#35270;&#20026;&#31867;&#20284;&#20154;&#31867;&#21644;&#33258;&#28982;&#30340;&#20132;&#20114;&#29305;&#23450;&#27969;&#24418;&#65292;&#20294;&#26159;&#21363;&#20351;&#26159;&#30456;&#20284;&#30340;&#20132;&#20114;&#65292;&#20154;&#20307;&#23039;&#21183;&#21644;&#29289;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#20063;&#21487;&#20197;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#31181;&#22810;&#26679;&#24615;&#20351;&#24471;&#27880;&#37322;3D&#20132;&#20114;&#30340;&#20219;&#21153;&#22256;&#38590;&#19988;&#38590;&#20197;&#25193;&#23637;&#65292;&#38480;&#21046;&#20102;&#29992;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#30340;&#28508;&#21147;&#12290;&#23398;&#20064;&#20154;&#31867;&#21644;&#29289;&#20307;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#30340;3D&#31354;&#38388;&#20851;&#31995;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#36890;&#36807;&#23637;&#31034;&#22810;&#20010;&#20174;&#19981;&#21516;&#35270;&#35282;&#25429;&#33719;&#30340;2D&#22270;&#20687;&#65292;&#24403;&#20154;&#31867;&#19982;&#30456;&#21516;&#31867;&#22411;&#30340;&#29289;&#20307;&#20114;&#21160;&#26102;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20219;&#24847;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;2D&#22270;&#20687;&#20316;&#20026;"&#26080;&#38480;"&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#20855;&#26377;&#26377;&#25928;&#30340;&#21487;&#25511;&#24615;&#21644;&#35270;&#35282;&#22810;&#26679;&#24615;&#12290;&#23613;&#31649;&#23384;&#22312;&#20854;&#19981;&#23436;&#32654;&#20043;&#22788;&#65292;
&lt;/p&gt;
&lt;p&gt;
We present a method for teaching machines to understand and model the underlying spatial common sense of diverse human-object interactions in 3D in a self-supervised way. This is a challenging task, as there exist specific manifolds of the interactions that can be considered human-like and natural, but the human pose and the geometry of objects can vary even for similar interactions. Such diversity makes the annotating task of 3D interactions difficult and hard to scale, which limits the potential to reason about that in a supervised way. One way of learning the 3D spatial relationship between humans and objects during interaction is by showing multiple 2D images captured from different viewpoints when humans interact with the same type of objects. The core idea of our method is to leverage a generative model that produces high-quality 2D images from an arbitrary text prompt input as an "unbounded" data generator with effective controllability and view diversity. Despite its imperfecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#30340;&#19981;&#21516;&#35266;&#28857;&#65292;&#24182;&#36861;&#36394;&#20854;&#28436;&#21270;&#36335;&#24452;&#65292;&#26088;&#22312;&#24110;&#21161;&#30830;&#23450;&#26368;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.12014</link><description>&lt;p&gt;
&#20174;&#25351;&#20196;&#21040;&#20869;&#22312;&#20154;&#31867;&#20215;&#20540; - &#22823;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models. (arXiv:2308.12014v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#30340;&#19981;&#21516;&#35266;&#28857;&#65292;&#24182;&#36861;&#36394;&#20854;&#28436;&#21270;&#36335;&#24452;&#65292;&#26088;&#22312;&#24110;&#21161;&#30830;&#23450;&#26368;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27169;&#22411;&#65292;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#24120;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#30001;&#22823;&#37327;&#21442;&#25968;&#32452;&#25104;&#65292;&#19981;&#20165;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33719;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#36824;&#21576;&#29616;&#20986;&#36739;&#23567;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#26032;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#27169;&#22411;&#19982;&#26085;&#24120;&#29983;&#27963;&#30340;&#26085;&#30410;&#20132;&#32455;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21487;&#33021;&#36896;&#25104;&#20005;&#37325;&#30340;&#31038;&#20250;&#21361;&#23475;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#21162;&#21147;&#24050;&#32463;&#36827;&#34892;&#20102;&#65292;&#20197;&#20351;LLM&#19982;&#20154;&#31867;&#23545;&#40784;&#65292;&#20197;&#20351;&#23427;&#20204;&#26356;&#22909;&#22320;&#36981;&#24490;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#28385;&#36275;&#20154;&#31867;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#8220;&#19982;&#20309;&#23545;&#40784;&#8221;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#35752;&#35770;&#65292;&#19981;&#24403;&#30340;&#23545;&#40784;&#30446;&#26631;&#29978;&#33267;&#21487;&#33021;&#36866;&#24471;&#20854;&#21453;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#24037;&#20316;&#20013;&#30340;&#19981;&#21516;&#23545;&#40784;&#30446;&#26631;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#65292;&#24182;&#36861;&#36394;&#23427;&#20204;&#30340;&#28436;&#21270;&#36335;&#24452;&#65292;&#20197;&#24110;&#21161;&#30830;&#23450;&#26368;&#22522;&#26412;&#30340;&#30446;&#26631;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20174;&#23545;&#40784;&#30446;&#26631;&#30340;&#23450;&#20041;&#21644;&#23545;&#40784;&#35780;&#20272;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#30456;&#20851;&#24037;&#20316;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses
&lt;/p&gt;</description></item><item><title>LFS-GAN&#26159;&#19968;&#20010;&#32456;&#36523;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#22240;&#23376;&#21270;&#24352;&#37327;&#65288;LeFT&#65289;&#20316;&#20026;&#20219;&#21153;&#29305;&#23450;&#35843;&#21046;&#22120;&#65292;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.11917</link><description>&lt;p&gt;
LFS-GAN: &#32456;&#36523;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LFS-GAN: Lifelong Few-Shot Image Generation. (arXiv:2308.11917v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11917
&lt;/p&gt;
&lt;p&gt;
LFS-GAN&#26159;&#19968;&#20010;&#32456;&#36523;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#22240;&#23376;&#21270;&#24352;&#37327;&#65288;LeFT&#65289;&#20316;&#20026;&#20219;&#21153;&#29305;&#23450;&#35843;&#21046;&#22120;&#65292;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#38024;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32456;&#36523;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#20165;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#30340;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#21516;&#26102;&#36935;&#21040;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#32456;&#36523;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#35843;&#21046;&#30340;&#26041;&#27861;&#26469;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#39069;&#22806;&#30340;&#21442;&#25968;&#65292;&#32780;&#19988;&#19981;&#33021;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#22270;&#20687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#26102;&#23384;&#22312;&#20005;&#37325;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#32456;&#36523;&#23569;&#26679;&#26412;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;LFS-GAN&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32456;&#36523;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20351;&#29992;&#19968;&#31181;&#39640;&#25928;&#30340;&#20219;&#21153;&#29305;&#23450;&#35843;&#21046;&#22120; - &#21487;&#23398;&#20064;&#22240;&#23376;&#21270;&#24352;&#37327;&#65288;LeFT&#65289;&#26469;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#12290;LeFT&#20855;&#26377;&#31209;&#32422;&#26463;&#24182;&#19988;&#20855;&#26377;&#20016;&#23500;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address a challenging lifelong few-shot image generation task for the first time. In this situation, a generative model learns a sequence of tasks using only a few samples per task. Consequently, the learned model encounters both catastrophic forgetting and overfitting problems at a time. Existing studies on lifelong GANs have proposed modulation-based methods to prevent catastrophic forgetting. However, they require considerable additional parameters and cannot generate high-fidelity and diverse images from limited data. On the other hand, the existing few-shot GANs suffer from severe catastrophic forgetting when learning multiple tasks. To alleviate these issues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that can generate high-quality and diverse images in lifelong few-shot image generation task. Our proposed framework learns each task using an efficient task-specific modulator - Learnable Factorized Tensor (LeFT). LeFT is rank-constrained and has a rich repres
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#21644;&#22240;&#26524;&#24615;&#65292;&#36890;&#36807;&#25512;&#29702;&#22120;&#21644;&#22240;&#26524;&#35780;&#20272;&#22120;&#30340;&#21512;&#20316;&#26469;&#35299;&#20915;&#25512;&#29702;&#35884;&#35823;&#12290;</title><link>http://arxiv.org/abs/2308.11914</link><description>&lt;p&gt;
&#36808;&#21521;&#22240;&#26524;GPT&#65306;&#36890;&#36807;&#20419;&#36827;LLMs&#20013;&#30340;&#22240;&#26524;&#19968;&#33268;&#24615;&#65292;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#23454;&#29616;&#24544;&#23454;&#30340;&#30693;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs. (arXiv:2308.11914v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11914
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#21644;&#22240;&#26524;&#24615;&#65292;&#36890;&#36807;&#25512;&#29702;&#22120;&#21644;&#22240;&#26524;&#35780;&#20272;&#22120;&#30340;&#21512;&#20316;&#26469;&#35299;&#20915;&#25512;&#29702;&#35884;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLMs&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#30693;&#35782;&#22238;&#24518;&#21644;&#25512;&#29702;&#30340;&#33030;&#24369;&#24615;&#24341;&#36215;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#40723;&#21169;LLMs&#33258;&#20027;&#35745;&#21010;&#21644;&#35299;&#20915;&#38382;&#39064;&#25110;&#24191;&#27867;&#37319;&#26679;&#25512;&#29702;&#38142;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26410;&#33021;&#35299;&#20915;&#27010;&#24565;&#21644;&#25512;&#29702;&#35884;&#35823;&#12290;&#20026;&#20102;&#20943;&#23569;&#25512;&#29702;&#35884;&#35823;&#65292;&#25105;&#20204;&#20174;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#24471;&#21040;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#22686;&#21152;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#21644;&#22240;&#26524;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#22810;&#20010;&#26234;&#33021;&#20307;&#65288;&#21363;&#25512;&#29702;&#22120;&#21644;&#22240;&#26524;&#35780;&#20272;&#22120;&#65289;&#22312;&#25512;&#29702;&#21644;&#19968;&#33268;&#24615;&#33539;&#24335;&#20013;&#21327;&#20316;&#24037;&#20316;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;&#25512;&#29702;&#22120;&#19987;&#27880;&#20110;&#25552;&#20379;&#20855;&#26377;&#20154;&#31867;&#22240;&#26524;&#20851;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22240;&#26524;&#35780;&#20272;&#22120;&#20195;&#29702;&#26816;&#26597;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#31572;&#26696;&#26159;&#21542;&#20174;&#38382;&#39064;&#20013;&#22240;&#26524;&#25512;&#23548;&#20986;&#26469;&#65292;&#21453;&#20043;&#20134;&#28982;&#65292;&#24182;&#29992;&#19968;&#20010;&#21453;&#20107;&#23454;&#30340;&#31572;&#26696;&#26469;&#26367;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advancements in LLMs, knowledge-based reasoning remains a longstanding issue due to the fragility of knowledge recall and inference. Existing methods primarily encourage LLMs to autonomously plan and solve problems or to extensively sample reasoning chains without addressing the conceptual and inferential fallacies. Attempting to alleviate inferential fallacies and drawing inspiration from multi-agent collaboration, we present a framework to increase faithfulness and causality for knowledge-based reasoning. Specifically, we propose to employ multiple intelligent agents (i.e., reasoner and causal evaluator) to work collaboratively in a reasoning-and-consensus paradigm for elevated reasoning faithfulness. The reasoners focus on providing solutions with human-like causality to solve open-domain problems. On the other hand, the causal evaluator agent scrutinizes if the answer in a solution is causally deducible from the question and vice versa, with a counterfactual answer replacin
&lt;/p&gt;</description></item><item><title>"&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#28151;&#28102;&#21327;&#21464;&#37327;&#23545;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#25512;&#26029;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#25552;&#20379;&#32479;&#19968;&#30340;&#22270;&#24418;&#26694;&#26550;&#26469;&#22686;&#24378;&#23545;&#36825;&#20123;&#27169;&#22411;&#22522;&#26412;&#21407;&#29702;&#30340;&#29702;&#35299;&#65292;&#20026;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#24102;&#26469;&#20102;&#28508;&#22312;&#20215;&#20540;&#12290;"</title><link>http://arxiv.org/abs/2308.11676</link><description>&lt;p&gt;
"&#38750;&#28151;&#28102;&#21327;&#21464;&#37327;&#23545;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#25512;&#26029;&#24615;&#33021;&#30340;&#24433;&#21709;&#30740;&#31350;"
&lt;/p&gt;
&lt;p&gt;
A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework. (arXiv:2308.11676v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11676
&lt;/p&gt;
&lt;p&gt;
"&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#28151;&#28102;&#21327;&#21464;&#37327;&#23545;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#25512;&#26029;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#25552;&#20379;&#32479;&#19968;&#30340;&#22270;&#24418;&#26694;&#26550;&#26469;&#22686;&#24378;&#23545;&#36825;&#20123;&#27169;&#22411;&#22522;&#26412;&#21407;&#29702;&#30340;&#29702;&#35299;&#65292;&#20026;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#24102;&#26469;&#20102;&#28508;&#22312;&#20215;&#20540;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#65288;POF&#65289;&#22312;&#22240;&#26524;&#25512;&#26029;&#39046;&#22495;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;POF&#30340;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#65288;CIMs-B-POF&#65289;&#26088;&#22312;&#28040;&#38500;&#28151;&#28102;&#20559;&#24046;&#65292;&#24182;&#40664;&#35748;&#23384;&#22312;&#28151;&#28102;&#21327;&#21464;&#37327;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#36825;&#19968;&#20551;&#35774;&#35748;&#20026;&#21327;&#21464;&#37327;&#20165;&#30001;&#28151;&#28102;&#21464;&#37327;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#20445;&#25345;&#28151;&#28102;&#21327;&#21464;&#37327;&#30340;&#20551;&#35774;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#39640;&#32500;&#21327;&#21464;&#37327;&#26102;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#22312;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#20043;&#21069;&#21306;&#20998;&#21327;&#21464;&#37327;&#30340;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23558;&#38750;&#28151;&#28102;&#30340;&#21327;&#21464;&#37327;&#35270;&#20026;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#26524;&#20173;&#19981;&#28165;&#26970;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;CIMs-B-POF&#26102;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22270;&#24418;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;CIMs-B-POF&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#21033;&#29992;&#36825;&#20010;&#22270;&#24418;&#26694;&#26550;&#65292;&#25105;&#20204;&#23545;CIMs-B-POF&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#37327;&#21270;&#20998;&#26512;&#12290;"
&lt;/p&gt;
&lt;p&gt;
The Potential Outcome Framework (POF) plays a prominent role in the field of causal inference. Most causal inference models based on the POF (CIMs-B-POF) are designed for eliminating confounding bias and default to an underlying assumption of Confounding Covariates. This assumption posits that the covariates consist solely of confounders. However, the assumption of Confounding Covariates is challenging to maintain in practice, particularly when dealing with high-dimensional covariates. While certain methods have been proposed to differentiate the distinct components of covariates prior to conducting causal inference, the consequences of treating non-confounding covariates as confounders remain unclear. This ambiguity poses a potential risk when applying the CIMs-B-POF in practical scenarios. In this paper, we present a unified graphical framework for the CIMs-B-POF, which greatly enhances the comprehension of these models' underlying principles. Using this graphical framework, we quant
&lt;/p&gt;</description></item><item><title>UniDoc&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20855;&#22791;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#20132;&#20114;&#25552;&#39640;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.11592</link><description>&lt;p&gt;
UniDoc: &#19968;&#31181;&#36890;&#29992;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#25991;&#26412;&#26816;&#27979;&#12289;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding. (arXiv:2308.11592v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11592
&lt;/p&gt;
&lt;p&gt;
UniDoc&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20855;&#22791;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#20132;&#20114;&#25552;&#39640;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26102;&#20195;&#65292;&#22810;&#27169;&#24577;&#29702;&#35299;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39640;&#32423;&#31639;&#27861;&#21463;&#38480;&#20110;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25152;&#22266;&#26377;&#30340;&#24040;&#22823;&#34920;&#31034;&#33021;&#21147;&#21644;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#20016;&#23500;&#22330;&#26223;&#20013;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#36830;&#25509;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;UniDoc&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20855;&#22791;&#29616;&#26377;&#26041;&#27861;&#25152;&#32570;&#20047;&#30340;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;UniDoc&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#20132;&#20114;&#26469;&#25552;&#39640;&#27599;&#20010;&#21333;&#29420;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;UniDoc&#65292;&#25105;&#20204;&#23545;&#36129;&#29486;&#30340;&#22823;&#35268;&#27169;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniDoc&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of Large Language Models (LLMs), tremendous strides have been made in the field of multimodal understanding. However, existing advanced algorithms are limited to effectively utilizing the immense representation capabilities and rich world knowledge inherent to these large pre-trained models, and the beneficial connections among tasks within the context of text-rich scenarios have not been sufficiently explored. In this work, we introduce UniDoc, a novel multimodal model equipped with text detection and recognition capabilities, which are deficient in existing approaches. Moreover, UniDoc capitalizes on the beneficial interactions among tasks to enhance the performance of each individual task. To implement UniDoc, we perform unified multimodal instruct tuning on the contributed large-scale instruction following datasets. Quantitative and qualitative experimental results show that UniDoc sets state-of-the-art scores across multiple challenging benchmarks. To the best of our kn
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#38598;&#20013;&#20110;&#20102;&#35299;&#22914;&#20309;&#21033;&#29992;LLM&#26469;&#20248;&#21270;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#21644;&#32467;&#26524;&#12290;&#25991;&#31456;&#24635;&#32467;&#20102;&#19981;&#21516;LLM&#30340;&#29305;&#28857;&#21644;&#29992;&#36884;&#20197;&#21450;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10620</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#65306;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Software Engineering: A Systematic Literature Review. (arXiv:2308.10620v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10620
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#38598;&#20013;&#20110;&#20102;&#35299;&#22914;&#20309;&#21033;&#29992;LLM&#26469;&#20248;&#21270;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#21644;&#32467;&#26524;&#12290;&#25991;&#31456;&#24635;&#32467;&#20102;&#19981;&#21516;LLM&#30340;&#29305;&#28857;&#21644;&#29992;&#36884;&#20197;&#21450;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21253;&#25324;&#36719;&#20214;&#24037;&#31243;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#25991;&#29486;&#25506;&#35752;&#20102;LLM&#22312;&#21508;&#31181;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;LLM&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#12289;&#24433;&#21709;&#21644;&#21487;&#33021;&#30340;&#38480;&#21046;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23545;LLM&#19982;&#36719;&#20214;&#24037;&#31243;&#30340;&#20132;&#21449;&#39046;&#22495;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65292;&#29305;&#21035;&#20851;&#27880;LLM&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#22914;&#20309;&#34987;&#21033;&#29992;&#26469;&#20248;&#21270;&#36807;&#31243;&#21644;&#32467;&#26524;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25910;&#38598;&#21644;&#20998;&#26512;&#20102;2017&#24180;&#33267;2023&#24180;&#30340;229&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#20197;&#22238;&#31572;&#22235;&#20010;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#65288;RQs&#65289;&#12290;&#22312;RQ1&#20013;&#65292;&#25105;&#20204;&#23545;&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;LLM&#36827;&#34892;&#20998;&#31867;&#21644;&#27604;&#36739;&#20998;&#26512;&#65292;&#25551;&#32472;&#20854;&#29420;&#29305;&#30340;&#29305;&#28857;&#21644;&#29992;&#36884;&#12290;&#22312;RQ2&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#25968;&#25454;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#24378;&#22823;&#12289;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#25104;&#21151;&#21033;&#29992;LLM&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks and applications. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review on the intersection of LLMs and SE, with a particular focus on understanding how LLMs can be exploited in SE to optimize processes and outcomes. We collect and analyze a total of 229 research papers from 2017 to 2023 to answer four key research questions (RQs). In RQ1, we categorize and provide a comparative analysis of different LLMs that have been employed in SE tasks, characterising their distinctive features and uses. In RQ2, we analyse the methods used in data collection, preprocessing, and application highlighting the role of robust, well-curated datasets for successful LLM for S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#23610;&#24230;&#35270;&#35273;&#34920;&#31034;&#25429;&#33719;&#30005;&#23376;&#26174;&#24494;&#38236;&#23454;&#20363;&#20998;&#21106;&#20013;&#20307;&#32032;&#32423;&#21644;&#29305;&#24449;&#32423;&#19968;&#33268;&#24615;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.09917</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#23610;&#24230;&#19968;&#33268;&#24615;&#30340;&#33258;&#30417;&#30563;&#30005;&#23376;&#26174;&#24494;&#38236;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Learning Multiscale Consistency for Self-supervised Electron Microscopy Instance Segmentation. (arXiv:2308.09917v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#23610;&#24230;&#35270;&#35273;&#34920;&#31034;&#25429;&#33719;&#30005;&#23376;&#26174;&#24494;&#38236;&#23454;&#20363;&#20998;&#21106;&#20013;&#20307;&#32032;&#32423;&#21644;&#29305;&#24449;&#32423;&#19968;&#33268;&#24615;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23454;&#20363;&#30340;&#22797;&#26434;&#24418;&#24577;&#21644;&#19981;&#20805;&#36275;&#30340;&#27880;&#37322;&#65292;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;EM&#65289;&#20307;&#31215;&#20013;&#30340;&#23454;&#20363;&#20998;&#21106;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#26368;&#36817;&#20986;&#29616;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#33719;&#21462;&#23545;&#20110;EM&#23454;&#20363;&#20998;&#21106;&#33267;&#20851;&#37325;&#35201;&#30340;&#32454;&#32990;&#32452;&#32455;&#32467;&#26500;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#25429;&#25417;&#22797;&#26434;&#30340;&#35270;&#35273;&#27169;&#24335;&#21644;&#20307;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#25152;&#33719;&#21462;&#30340;&#20808;&#39564;&#30693;&#35782;&#19981;&#36275;&#20197;&#24212;&#29992;&#20110;&#19979;&#28216;&#30340;EM&#20998;&#26512;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#30340;&#35270;&#35273;&#34920;&#31034;&#26469;&#25429;&#25417;EM&#20307;&#31215;&#20013;&#30340;&#20307;&#32032;&#32423;&#21644;&#29305;&#24449;&#32423;&#30340;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#37325;&#26500;&#20989;&#25968;&#24378;&#21046;&#23454;&#26045;Siamese&#32593;&#32476;&#36755;&#20986;&#20043;&#38388;&#30340;&#20307;&#32032;&#32423;&#19968;&#33268;&#24615;&#65292;&#24182;&#32467;&#21512;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#36719;&#29305;&#24449;&#21305;&#37197;&#65292;&#23454;&#29616;&#31934;&#32454;&#30340;&#29305;&#24449;&#32423;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instance segmentation in electron microscopy (EM) volumes poses a significant challenge due to the complex morphology of instances and insufficient annotations. Self-supervised learning has recently emerged as a promising solution, enabling the acquisition of prior knowledge of cellular tissue structures that are essential for EM instance segmentation. However, existing pretraining methods often lack the ability to capture complex visual patterns and relationships between voxels, which results in the acquired prior knowledge being insufficient for downstream EM analysis tasks. In this paper, we propose a novel pretraining framework that leverages multiscale visual representations to capture both voxel-level and feature-level consistency in EM volumes. Specifically, our framework enforces voxel-level consistency between the outputs of a Siamese network by a reconstruction function, and incorporates a cross-attention mechanism for soft feature matching to achieve fine-grained feature-lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;&#30456;&#32467;&#21512;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21327;&#21516;&#26041;&#27861;&#20114;&#34917;&#21508;&#33258;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31283;&#20581;&#21644;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.09830</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;&#30340;&#21327;&#21516;&#38598;&#25104;&#23545;&#20110;&#31283;&#20581;&#20154;&#24037;&#26234;&#33021;&#30340;&#25506;&#32034;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis. (arXiv:2308.09830v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;&#30456;&#32467;&#21512;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21327;&#21516;&#26041;&#27861;&#20114;&#34917;&#21508;&#33258;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31283;&#20581;&#21644;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#26500;&#24314;&#34920;&#29616;&#20986;&#26234;&#33021;&#34892;&#20026;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26102;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#35748;&#30693;&#26550;&#26500;(CAs) &#36827;&#34892;&#38598;&#25104;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#29702;&#35770;&#27169;&#22411;&#30340;&#25351;&#23548;&#19979;&#65292;&#36890;&#36807;&#21021;&#27493;&#30340;&#32463;&#39564;&#25968;&#25454;&#25903;&#25345;&#65292;&#25105;&#20204;&#20551;&#35774;&#19981;&#21516;&#30340;&#21327;&#21516;&#26041;&#27861;&#21487;&#20197;&#20114;&#34917;&#23427;&#20204;&#21508;&#33258;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#65292;&#20174;&#32780;&#22521;&#32946;&#20986;&#26356;&#31283;&#20581;&#21644;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#27599;&#31181;&#26041;&#27861;&#25152;&#28041;&#21450;&#30340;&#26435;&#34913;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores alternatives for integrating two subdisciplines of AI in the construction of artificial agents that exhibit intelligent behavior: Large Language Models (LLMs) and Cognitive Architectures (CAs). Guided by theoretical models and supported by preliminary empirical data, we hypothesize how diverse synergistic approaches can mutually compensate for their respective weaknesses and limitations, ultimately fostering more robust and sophisticated artificial intelligence systems. Additionally, we discuss the tradeoffs and challenges associated with each approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20844;&#21496;&#21644;&#20998;&#31867;&#20195;&#30721;&#30340;&#21487;&#35760;&#24518;&#34920;&#31034;&#65292;&#36890;&#36807;&#21382;&#21490;&#35760;&#24518;&#21644;&#24403;&#21069;&#20449;&#24687;&#26356;&#26032;&#26469;&#25429;&#25417;&#35821;&#20041;&#25509;&#36817;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09780</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#22312;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction. (arXiv:2308.09780v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20844;&#21496;&#21644;&#20998;&#31867;&#20195;&#30721;&#30340;&#21487;&#35760;&#24518;&#34920;&#31034;&#65292;&#36890;&#36807;&#21382;&#21490;&#35760;&#24518;&#21644;&#24403;&#21069;&#20449;&#24687;&#26356;&#26032;&#26469;&#25429;&#25417;&#35821;&#20041;&#25509;&#36817;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#20844;&#21496;&#22312;&#26410;&#26469;&#19968;&#27573;&#26102;&#38388;&#20869;&#23558;&#30003;&#35831;&#21738;&#20123;&#31867;&#22411;&#30340;&#19987;&#21033;&#33021;&#22815;&#25581;&#31034;&#20986;&#23427;&#20204;&#30340;&#21457;&#23637;&#25112;&#30053;&#65292;&#24182;&#24110;&#21161;&#20854;&#25552;&#21069;&#21457;&#29616;&#28508;&#22312;&#30340;&#21512;&#20316;&#20249;&#20276;&#25110;&#31454;&#20105;&#23545;&#25163;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#20844;&#21496;&#19981;&#26029;&#21464;&#21270;&#30340;&#20559;&#22909;&#21644;&#23545;&#20998;&#31867;&#20195;&#30721;&#30340;&#35821;&#20041;&#20851;&#32852;&#30340;&#24314;&#27169;&#22256;&#38590;&#65292;&#36825;&#20010;&#38382;&#39064;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#40092;&#26377;&#28041;&#21450;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#20844;&#21496;&#21644;&#19987;&#21033;&#20998;&#31867;&#20195;&#30721;&#30340;&#21487;&#35760;&#24518;&#34920;&#31034;&#22522;&#30784;&#19978;&#12290;&#24403;&#35266;&#23519;&#21040;&#19968;&#20010;&#26032;&#30340;&#19987;&#21033;&#26102;&#65292;&#30456;&#20851;&#20844;&#21496;&#21644;&#20998;&#31867;&#20195;&#30721;&#30340;&#34920;&#31034;&#26681;&#25454;&#21382;&#21490;&#35760;&#24518;&#21644;&#24403;&#21069;&#32534;&#30721;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#20197;&#25429;&#25417;&#19987;&#21033;&#20998;&#31867;&#20195;&#30721;&#30340;&#35821;&#20041;&#25509;&#36817;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modelling companies' continuously evolving preferences and capturing the semantic correlations of classification codes. To fill in this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by u
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;Baird&#21453;&#20363;&#19978;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.09732</link><description>&lt;p&gt;
Baird&#21453;&#20363;&#24050;&#35299;&#20915;&#65306;&#20197;&#35843;&#35797;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#31639;&#27861;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm. (arXiv:2308.09732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09732
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;Baird&#21453;&#20363;&#19978;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Baird&#21453;&#20363;&#26159;&#30001;Leemon Baird&#22312;1995&#24180;&#25552;&#20986;&#30340;&#65292;&#39318;&#20808;&#29992;&#20110;&#35777;&#26126;Temporal Difference (TD(0))&#31639;&#27861;&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#21457;&#25955;&#12290;&#20174;&#37027;&#26102;&#36215;&#65292;&#23427;&#32463;&#24120;&#34987;&#29992;&#26469;&#27979;&#35797;&#21644;&#27604;&#36739;&#31163;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#12290;&#26799;&#24230;TD&#31639;&#27861;&#35299;&#20915;&#20102;TD&#22312;Baird&#21453;&#20363;&#19978;&#30340;&#21457;&#25955;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#30340;&#25910;&#25947;&#20173;&#28982;&#38750;&#24120;&#32531;&#24930;&#65292;&#32780;&#19988;&#32531;&#24930;&#30340;&#26412;&#36136;&#36824;&#19981;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#29305;&#21035;&#29702;&#35299;&#20026;&#20160;&#20040;TDC&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#24930;&#65292;&#24182;&#25552;&#20379;&#35843;&#35797;&#20998;&#26512;&#26469;&#29702;&#35299;&#36825;&#31181;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#35843;&#35797;&#25216;&#26415;&#21487;&#20197;&#29992;&#26469;&#30740;&#31350;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26368;&#36817;&#30340;Impression GTD&#31639;&#27861;&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#34920;&#26126;&#25910;&#25947;&#38750;&#24120;&#24555;&#65292;&#20107;&#23454;&#19978;&#26159;&#32447;&#24615;&#30340;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;Baird&#21453;&#20363;&#36890;&#36807;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31639;&#27861;&#35299;&#20915;&#20102;&#65292;&#35813;&#31639;&#27861;&#25910;&#25947;&#21040;TD&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Baird counterexample was proposed by Leemon Baird in 1995, first used to show that the Temporal Difference (TD(0)) algorithm diverges on this example. Since then, it is often used to test and compare off-policy learning algorithms. Gradient TD algorithms solved the divergence issue of TD on Baird counterexample. However, their convergence on this example is still very slow, and the nature of the slowness is not well understood, e.g., see (Sutton and Barto 2018).  This note is to understand in particular, why TDC is slow on this example, and provide debugging analysis to understand this behavior. Our debugging technique can be used to study the convergence behavior of two-time-scale stochastic approximation algorithms. We also provide empirical results of the recent Impression GTD algorithm on this example, showing the convergence is very fast, in fact, in a linear rate. We conclude that Baird counterexample is solved, by an algorithm with convergence guarantee to the TD solution in gen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#21363;&#26102;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#33021;&#22815;&#26356;&#22909;&#22320;&#24314;&#27169;&#27010;&#29575;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2308.06733</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#38477;&#27700;&#21363;&#26102;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Precipitation nowcasting with generative diffusion models. (arXiv:2308.06733v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06733
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#21363;&#26102;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#33021;&#22815;&#26356;&#22909;&#22320;&#24314;&#27169;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20256;&#32479;&#30340;&#25968;&#20540;&#39044;&#27979;&#26041;&#27861;&#36880;&#28176;&#21463;&#21040;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#25361;&#25112;&#12290;&#29992;&#20110;&#30701;&#26399;&#21644;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#30340;&#22810;&#20010;&#21382;&#21490;&#25968;&#25454;&#38598;&#36890;&#24120;&#34987;&#32452;&#32455;&#25104;&#35268;&#21017;&#30340;&#31354;&#38388;&#32593;&#26684;&#32467;&#26500;&#12290;&#36825;&#31181;&#25490;&#21015;&#19982;&#22270;&#20687;&#38750;&#24120;&#30456;&#20284;&#65306;&#27599;&#20010;&#22825;&#27668;&#21464;&#37327;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#24133;&#22320;&#22270;&#65292;&#25110;&#32773;&#22312;&#32771;&#34385;&#26102;&#38388;&#36724;&#26102;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#27573;&#35270;&#39057;&#12290;&#22810;&#20010;&#31867;&#21035;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#25110;&#32773;&#26368;&#36817;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#19979;&#19968;&#24103;&#39044;&#27979;&#38382;&#39064;&#19978;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#65292;&#22240;&#27492;&#33258;&#28982;&#32780;&#28982;&#22320;&#24076;&#26395;&#27979;&#35797;&#23427;&#20204;&#22312;&#22825;&#27668;&#39044;&#25253;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25193;&#25955;&#27169;&#22411;&#29305;&#21035;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#22825;&#27668;&#39044;&#25253;&#26412;&#36136;&#19978;&#26159;&#20855;&#26377;&#27010;&#29575;&#24615;&#30340;&#65306;&#25105;&#20204;&#30495;&#27491;&#26377;&#20852;&#36259;&#24314;&#27169;&#30340;&#26159;&#22825;&#27668;&#25351;&#26631;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20854;&#26399;&#26395;&#20540;&#26159;&#26368;&#21487;&#33021;&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years traditional numerical methods for accurate weather prediction have been increasingly challenged by deep learning methods. Numerous historical datasets used for short and medium-range weather forecasts are typically organized into a regular spatial grid structure. This arrangement closely resembles images: each weather variable can be visualized as a map or, when considering the temporal axis, as a video. Several classes of generative models, comprising Generative Adversarial Networks, Variational Autoencoders, or the recent Denoising Diffusion Models have largely proved their applicability to the next-frame prediction problem, and is thus natural to test their performance on the weather prediction benchmarks. Diffusion models are particularly appealing in this context, due to the intrinsically probabilistic nature of weather forecasting: what we are really interested to model is the probability distribution of weather indicators, whose expected value is the most likely 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06035</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#26399;&#38388;&#34920;&#29616;&#20986;&#20154;&#31867;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing. (arXiv:2308.06035v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06035
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#22797;&#21046;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30340;&#20105;&#35758;&#12290;LLMs&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#19968;&#20010;&#21306;&#21035;&#22312;&#20110;&#65292;&#35821;&#35328;&#36755;&#20837;&#36890;&#24120;&#24314;&#31435;&#22312;&#22810;&#20010;&#30693;&#35273;&#27169;&#24577;&#19978;&#65292;&#32780;&#22823;&#22810;&#25968;LLMs&#20165;&#22788;&#29702;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#20351;&#20154;&#31867;&#33021;&#22815;&#25972;&#21512;&#35270;&#35273;&#32972;&#26223;&#19982;&#35821;&#35328;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#21363;&#23558;&#20986;&#29616;&#30340;&#21333;&#35789;&#30340;&#31354;&#38388;&#26045;&#21152;&#38480;&#21046;&#65292;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;LLMs&#65288;mLLMs&#65289;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#21464;&#21387;&#22120;&#31867;&#22411;&#30340;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#19979;&#19968;&#20010;&#21333;&#35789;&#30340;&#39044;&#27979;&#12290;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#65292;&#22522;&#20110;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#22312;mLLMs&#21644;&#20154;&#31867;&#20013;&#21563;&#21512;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;200&#21517;&#34987;&#35797;&#35266;&#30475;&#20102;&#30701;&#30340;&#35270;&#21548;&#21098;&#36753;&#65292;&#24182;&#20272;&#35745;&#20102;&#21363;&#23558;&#20986;&#29616;&#30340;&#21160;&#35789;&#25110;&#21517;&#35789;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advanced language processing abilities of large language models (LLMs) have stimulated debate over their capacity to replicate human-like cognitive processes. One differentiating factor between language processing in LLMs and humans is that language input is often grounded in more than one perceptual modality, whereas most LLMs process solely text-based information. Multimodal grounding allows humans to integrate - e.g. visual context with linguistic information and thereby place constraints on the space of upcoming words, reducing cognitive load and improving perception and comprehension. Recent multimodal LLMs (mLLMs) combine visual and linguistic embedding spaces with a transformer type attention mechanism for next-word prediction. To what extent does predictive language processing based on multimodal input align in mLLMs and humans? To answer this question, 200 human participants watched short audio-visual clips and estimated the predictability of an upcoming verb or noun. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21311;&#21517;&#21270;&#35821;&#38899;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#20102;&#21311;&#21517;&#21270;&#30340;&#31243;&#24230;&#65292;&#20197;&#24212;&#23545;&#35821;&#38899;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#21644;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2308.04455</link><description>&lt;p&gt;
&#21311;&#21517;&#21270;&#35821;&#38899;&#65306;&#35780;&#20272;&#21644;&#35774;&#35745;&#35828;&#35805;&#20154;&#21311;&#21517;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Anonymizing Speech: Evaluating and Designing Speaker Anonymization Techniques. (arXiv:2308.04455v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21311;&#21517;&#21270;&#35821;&#38899;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#20102;&#21311;&#21517;&#21270;&#30340;&#31243;&#24230;&#65292;&#20197;&#24212;&#23545;&#35821;&#38899;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#21644;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#38899;&#29992;&#25143;&#30028;&#38754;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#35821;&#38899;&#25968;&#25454;&#30340;&#25910;&#38598;&#21644;&#23384;&#20648;&#20063;&#22823;&#22823;&#22686;&#21152;&#12290;&#34429;&#28982;&#25968;&#25454;&#25910;&#38598;&#21487;&#20197;&#20026;&#22823;&#22810;&#25968;&#35821;&#38899;&#26381;&#21153;&#25552;&#20379;&#39640;&#25928;&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20063;&#32473;&#29992;&#25143;&#30340;&#38544;&#31169;&#36896;&#25104;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#38598;&#20013;&#23384;&#20648;&#20351;&#20010;&#20154;&#30340;&#35821;&#38899;&#25968;&#25454;&#23481;&#26131;&#21463;&#21040;&#32593;&#32476;&#23041;&#32961;&#30340;&#20405;&#23475;&#12290;&#38543;&#30528;&#20122;&#39532;&#36874;&#30340;Alexa&#65292;&#35895;&#27468;&#30340;Home&#21644;&#33529;&#26524;&#30340;Siri&#31561;&#22522;&#20110;&#35821;&#38899;&#30340;&#25968;&#23383;&#21161;&#25163;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#20197;&#21450;&#20010;&#20154;&#35821;&#38899;&#25968;&#25454;&#25910;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#23481;&#26131;&#65292;&#22768;&#38899;&#20811;&#38534;&#21644;&#35828;&#35805;&#20154;/&#24615;&#21035;/&#30149;&#29702;&#31561;&#35782;&#21035;&#30340;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#20063;&#22686;&#21152;&#20102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21311;&#21517;&#21270;&#35821;&#38899;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#21311;&#21517;&#21270;&#30340;&#31243;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#21311;&#21517;&#21270;&#26159;&#25351;&#20351;&#20010;&#20154;&#35821;&#38899;&#25968;&#25454;&#19982;&#36523;&#20221;&#26080;&#27861;&#20851;&#32852;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#38899;&#20449;&#21495;&#30340;&#23454;&#29992;&#24615;&#65288;&#20363;&#22914;&#65292;&#35775;&#38382;&#35821;&#35328;&#20869;&#23481;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#20960;&#20010;&#35780;&#20272;&#21327;&#35758;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing use of voice user interfaces has led to a surge in the collection and storage of speech data. While data collection allows for the development of efficient tools powering most speech services, it also poses serious privacy issues for users as centralized storage makes private personal speech data vulnerable to cyber threats. With the increasing use of voice-based digital assistants like Amazon's Alexa, Google's Home, and Apple's Siri, and with the increasing ease with which personal speech data can be collected, the risk of malicious use of voice-cloning and speaker/gender/pathological/etc. recognition has increased.  This thesis proposes solutions for anonymizing speech and evaluating the degree of the anonymization. In this work, anonymization refers to making personal speech data unlinkable to an identity while maintaining the usefulness (utility) of the speech signal (e.g., access to linguistic content). We start by identifying several challenges that evaluation protoco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24773;&#32490;&#20998;&#31867;&#30340;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#23637;&#31034;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#23637;&#31034;&#26356;&#22810;&#30340;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#21487;&#20197;&#22686;&#24378;&#29992;&#25143;&#20915;&#31574;&#30340;&#33258;&#20449;&#24230;&#12290; (This study investigated the impact of displaying more uncertainty information on human decision making in emotion classification, and the results showed that it can increase users' confidence in decision making.)</title><link>http://arxiv.org/abs/2308.04032</link><description>&lt;p&gt;
&#20154;&#31867;&#24773;&#32490;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measure of Uncertainty in Human Emotions. (arXiv:2308.04032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24773;&#32490;&#20998;&#31867;&#30340;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#23637;&#31034;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#23637;&#31034;&#26356;&#22810;&#30340;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#21487;&#20197;&#22686;&#24378;&#29992;&#25143;&#20915;&#31574;&#30340;&#33258;&#20449;&#24230;&#12290; (This study investigated the impact of displaying more uncertainty information on human decision making in emotion classification, and the results showed that it can increase users' confidence in decision making.)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#35745;&#31639;&#26426;&#22914;&#20309;&#33021;&#22815;&#26816;&#27979;&#20154;&#31867;&#23637;&#31034;&#30340;&#24773;&#32490;&#24182;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#35745;&#31639;&#26426;&#29983;&#25104;&#24773;&#32490;&#20998;&#31867;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#20570;&#20986;&#20915;&#31574;&#25110;&#25191;&#34892;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#39046;&#22495;&#38656;&#35201;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#23545;&#20154;&#19982;&#35745;&#31639;&#26426;&#20043;&#38388;&#30340;&#21452;&#21521;&#20132;&#27969;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#24773;&#32490;&#20998;&#31867;&#30340;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#23637;&#31034;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23637;&#31034;&#26356;&#22810;&#30340;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#22312;&#20570;&#20915;&#31574;&#26102;&#26356;&#21152;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many research explore how well computers are able to examine emotions displayed by humans and use that data to perform different tasks. However, there have been very few research which evaluate the computers ability to generate emotion classification information in an attempt to help the user make decisions or perform tasks. This is a crucial area to explore as it is paramount to the two way communication between humans and computers. This research conducted an experiment to investigate the impact of different uncertainty information displays of emotion classification on the human decision making process. Results show that displaying more uncertainty information can help users to be more confident when making decisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24490;&#29615;&#22810;&#23610;&#24230;Transformer&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;HRS10K&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#29992;&#20110;HRSOD&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.03826</link><description>&lt;p&gt;
&#12298;&#39640;&#20998;&#36776;&#29575;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#30340;&#24490;&#29615;&#22810;&#23610;&#24230;Transformer&#12299;
&lt;/p&gt;
&lt;p&gt;
Recurrent Multi-scale Transformer for High-Resolution Salient Object Detection. (arXiv:2308.03826v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24490;&#29615;&#22810;&#23610;&#24230;Transformer&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;HRS10K&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#29992;&#20110;HRSOD&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#65288;SOD&#65289;&#26088;&#22312;&#35782;&#21035;&#21644;&#20998;&#21106;&#22270;&#20687;&#25110;&#35270;&#39057;&#20013;&#26368;&#26174;&#33879;&#30340;&#23545;&#35937;&#12290;&#38543;&#30528;&#25104;&#20687;&#35774;&#22791;&#30340;&#36827;&#27493;&#65292;&#23545;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;SOD&#38656;&#27714;&#26085;&#30410;&#36843;&#20999;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;SOD&#26041;&#27861;&#20027;&#35201;&#23616;&#38480;&#20110;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#24182;&#38590;&#20197;&#36866;&#24212;&#39640;&#20998;&#36776;&#29575;SOD&#65288;HRSOD&#65289;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#20986;&#29616;&#20102;&#19968;&#20123;HRSOD&#26041;&#27861;&#65292;&#20294;&#30446;&#21069;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;HRSOD&#26041;&#27861;&#36890;&#24120;&#20250;&#20135;&#29983;&#19981;&#23436;&#25972;&#30340;&#23545;&#35937;&#21306;&#22495;&#21644;&#19981;&#35268;&#21017;&#30340;&#23545;&#35937;&#36793;&#30028;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;HRS10K&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;10500&#20010;2K-8K&#20998;&#36776;&#29575;&#30340;&#39640;&#36136;&#37327;&#27880;&#37322;&#22270;&#20687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#30446;&#21069;&#29992;&#20110;HRSOD&#20219;&#21153;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#65292;&#23427;&#23558;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24110;&#21161;&#26410;&#26469;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25913;&#36827;&#26041;&#27861;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#22810;&#23610;&#24230;Transformer&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Salient Object Detection (SOD) aims to identify and segment the most conspicuous objects in an image or video. As an important pre-processing step, it has many potential applications in multimedia and vision tasks. With the advance of imaging devices, SOD with high-resolution images is of great demand, recently. However, traditional SOD methods are largely limited to low-resolution images, making them difficult to adapt to the development of High-Resolution SOD (HRSOD). Although some HRSOD methods emerge, there are no large enough datasets for training and evaluating. Besides, current HRSOD methods generally produce incomplete object regions and irregular object boundaries. To address above issues, in this work, we first propose a new HRS10K dataset, which contains 10,500 high-quality annotated images at 2K-8K resolution. As far as we know, it is the largest dataset for the HRSOD task, which will significantly help future works in training and evaluating models. Furthermore, to improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#32422;&#26463;&#26469;&#25913;&#36827;&#36718;&#30424;&#36172;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.03572</link><description>&lt;p&gt;
&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#21487;&#35777;&#25928;&#29575;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Learning in Partially Observable Contextual Bandit. (arXiv:2308.03572v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#32422;&#26463;&#26469;&#25913;&#36827;&#36718;&#30424;&#36172;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#20165;&#26377;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#26377;&#38480;&#30693;&#35782;&#65292;&#24182;&#19988;&#23545;&#38544;&#34255;&#30340;&#28151;&#28102;&#22240;&#32032;&#21482;&#26377;&#37096;&#20998;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#36716;&#21270;&#20026;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#26469;&#35782;&#21035;&#25110;&#37096;&#20998;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26410;&#30693;&#20998;&#24067;&#30340;&#21407;&#22987;&#21151;&#33021;&#32422;&#26463;&#31163;&#25955;&#21270;&#20026;&#32447;&#24615;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#39034;&#24207;&#35299;&#32447;&#24615;&#35268;&#21010;&#26469;&#37319;&#26679;&#20860;&#23481;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#20197;&#32771;&#34385;&#20272;&#35745;&#35823;&#24046;&#24471;&#21040;&#22240;&#26524;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#20026;&#36866;&#24403;&#30340;&#37319;&#26679;&#20998;&#24067;&#25552;&#20379;&#20102;&#29702;&#24819;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22240;&#26524;&#32422;&#26463;&#24212;&#29992;&#20110;&#25913;&#36827;&#32463;&#20856;&#30340;&#36718;&#30424;&#36172;&#31639;&#27861;&#65292;&#24182;&#20197;&#34892;&#21160;&#38598;&#21644;&#20989;&#25968;&#31354;&#38388;&#35268;&#27169;&#20026;&#21442;&#32771;&#25913;&#21464;&#20102;&#36951;&#25022;&#20540;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20801;&#35768;&#25105;&#20204;&#22788;&#29702;&#19968;&#33324;&#24773;&#22659;&#20998;&#24067;&#30340;&#20989;&#25968;&#36924;&#36817;&#20219;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate transfer learning in partially observable contextual bandits, where agents have limited knowledge from other agents and partial information about hidden confounders. We first convert the problem to identifying or partially identifying causal effects between actions and rewards through optimization problems. To solve these optimization problems, we discretize the original functional constraints of unknown distributions into linear constraints, and sample compatible causal models via sequentially solving linear programmings to obtain causal bounds with the consideration of estimation error. Our sampling algorithms provide desirable convergence results for suitable sampling distributions. We then show how causal bounds can be applied to improving classical bandit algorithms and affect the regrets with respect to the size of action sets and function spaces. Notably, in the task with function approximation which allows us to handle general context distributions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36793;&#32536;&#31283;&#23450;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ES$^2$N&#65289;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;&#28176;&#36827;&#35760;&#24518;&#23646;&#24615;&#21644;&#23613;&#21487;&#33021;&#20445;&#30041;&#26356;&#22810;&#35760;&#24518;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;&#20648;&#22791;&#23618;&#23450;&#20041;&#20026;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#30340;&#20984;&#32452;&#21512;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#30340;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2308.02902</link><description>&lt;p&gt;
&#36793;&#32536;&#31283;&#23450;&#30340;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Edge of stability echo state networks. (arXiv:2308.02902v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36793;&#32536;&#31283;&#23450;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ES$^2$N&#65289;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;&#28176;&#36827;&#35760;&#24518;&#23646;&#24615;&#21644;&#23613;&#21487;&#33021;&#20445;&#30041;&#26356;&#22810;&#35760;&#24518;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;&#20648;&#22791;&#23618;&#23450;&#20041;&#20026;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#30340;&#20984;&#32452;&#21512;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ESNs&#65289;&#26159;&#22522;&#20110;&#22238;&#22768;&#29366;&#24577;&#23646;&#24615;&#65288;ESP&#65289;&#21407;&#29702;&#24037;&#20316;&#30340;&#26102;&#38388;&#24207;&#21015;&#22788;&#29702;&#27169;&#22411;&#12290;ESP&#26159;&#25351;&#23545;&#36755;&#20837;&#30340;&#35760;&#24518;&#22312;&#28176;&#36827;&#34928;&#20943;&#12290;&#28982;&#32780;&#65292;ESNs&#30340;&#22266;&#26377;&#26550;&#26500;&#20559;&#24046;&#21487;&#33021;&#23548;&#33268;&#36807;&#22810;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#36827;&#32780;&#24433;&#21709;&#22312;&#26576;&#20123;&#38271;&#26399;&#35760;&#24518;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23558;&#28176;&#36827;&#35760;&#24518;&#23646;&#24615;&#19982;&#23613;&#21487;&#33021;&#20445;&#30041;&#26356;&#22810;&#35760;&#24518;&#30340;&#33021;&#21147;&#32467;&#21512;&#36215;&#26469;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;ESN&#26550;&#26500;&#65292;&#31216;&#20026;&#36793;&#32536;&#31283;&#23450;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ES$^2$N&#65289;&#12290;&#24341;&#20837;&#30340;ES$^2$N&#27169;&#22411;&#22522;&#20110;&#23558;&#20648;&#22791;&#23618;&#23450;&#20041;&#20026;&#38750;&#32447;&#24615;&#20648;&#22791;&#65288;&#19982;&#26631;&#20934;ESN&#30456;&#21516;&#65289;&#21644;&#23454;&#29616;&#27491;&#20132;&#21464;&#25442;&#30340;&#32447;&#24615;&#20648;&#22791;&#30340;&#20984;&#32452;&#21512;&#12290;&#25105;&#20204;&#23545;&#24341;&#20837;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;ES$^2$N&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#25972;&#20010;&#29305;&#24449;&#35889;...
&lt;/p&gt;
&lt;p&gt;
Echo State Networks (ESNs) are time-series processing models working under the Echo State Property (ESP) principle. The ESP is a notion of stability that imposes an asymptotic fading of the memory of the input. On the other hand, the resulting inherent architectural bias of ESNs may lead to an excessive loss of information, which in turn harms the performance in certain tasks with long short-term memory requirements. With the goal of bringing together the fading memory property and the ability to retain as much memory as possible, in this paper we introduce a new ESN architecture, called the Edge of Stability Echo State Network (ES$^2$N). The introduced ES$^2$N model is based on defining the reservoir layer as a convex combination of a nonlinear reservoir (as in the standard ESN), and a linear reservoir that implements an orthogonal transformation. We provide a thorough mathematical analysis of the introduced model, proving that the whole eigenspectrum of the Jacobian of the ES$^2$N ma
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CausalOps&#65292;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#27169;&#22411;&#24320;&#21457;&#21644;&#24212;&#29992;&#30340;&#29983;&#21629;&#21608;&#26399;&#26694;&#26550;&#65292;&#26088;&#22312;&#25512;&#21160;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#37319;&#29992;&#22240;&#26524;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.01375</link><description>&lt;p&gt;
CausalOps -- &#23454;&#29616;&#22240;&#26524;&#27010;&#29575;&#22270;&#27169;&#22411;&#23454;&#38469;&#29983;&#21629;&#21608;&#26399;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CausalOps -- Towards an Industrial Lifecycle for Causal Probabilistic Graphical Models. (arXiv:2308.01375v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01375
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CausalOps&#65292;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#27169;&#22411;&#24320;&#21457;&#21644;&#24212;&#29992;&#30340;&#29983;&#21629;&#21608;&#26399;&#26694;&#26550;&#65292;&#26088;&#22312;&#25512;&#21160;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#37319;&#29992;&#22240;&#26524;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#27010;&#29575;&#22270;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#24314;&#27169;&#22240;&#26524;&#20851;&#31995;&#12290;&#38543;&#30528;&#20854;&#22312;&#27773;&#36710;&#31995;&#32479;&#23433;&#20840;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#26032;&#39046;&#22495;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20154;&#20204;&#23545;&#20110;&#31867;&#20284;DevOps&#21644;MLOps&#30340;&#38598;&#25104;&#29983;&#21629;&#21608;&#26399;&#26694;&#26550;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#30446;&#21069;&#65292;&#32570;&#20047;&#19968;&#20010;&#36866;&#29992;&#20110;&#22240;&#26524;&#24037;&#31243;&#30340;&#32452;&#32455;&#21442;&#32771;&#27969;&#31243;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#24182;&#20419;&#36827;&#24191;&#27867;&#30340;&#24037;&#19994;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CausalOps&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;&#22240;&#26524;&#27169;&#22411;&#24320;&#21457;&#21644;&#24212;&#29992;&#30340;&#20840;&#26032;&#29983;&#21629;&#21608;&#26399;&#26694;&#26550;&#12290;&#36890;&#36807;&#23450;&#20041;&#22240;&#26524;&#24037;&#31243;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#20851;&#38190;&#23454;&#20307;&#12289;&#20381;&#36182;&#20851;&#31995;&#21644;&#20013;&#38388;&#20135;&#29289;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#19968;&#33268;&#30340;&#35789;&#27719;&#34920;&#21644;&#24037;&#20316;&#27969;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#23558;&#22240;&#26524;&#27169;&#22411;&#30340;&#20351;&#29992;&#24773;&#22659;&#21270;&#20026;&#19981;&#21516;&#38454;&#27573;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#27010;&#36848;&#20102;&#21019;&#24314;&#21644;&#32500;&#25252;&#22240;&#26524;&#27169;&#22411;&#30340;&#25972;&#20307;&#35270;&#22270;&#12290;CausalOps&#26088;&#22312;&#25512;&#21160;&#22240;&#26524;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal probabilistic graph-based models have gained widespread utility, enabling the modeling of cause-and-effect relationships across diverse domains. With their rising adoption in new areas, such as automotive system safety and machine learning, the need for an integrated lifecycle framework akin to DevOps and MLOps has emerged. Currently, a process reference for organizations interested in employing causal engineering is missing. To address this gap and foster widespread industrial adoption, we propose CausalOps, a novel lifecycle framework for causal model development and application. By defining key entities, dependencies, and intermediate artifacts generated during causal engineering, we establish a consistent vocabulary and workflow model. This work contextualizes causal model usage across different stages and stakeholders, outlining a holistic view of creating and maintaining them. CausalOps' aim is to drive the adoption of causal methods in practical applications within intere
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.01011</link><description>&lt;p&gt;
&#20351;&#29992;Floss&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#31181;&#39057;&#22495;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach. (arXiv:2308.01011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26159;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23637;&#29616;&#20986;&#37325;&#35201;&#30340;&#21608;&#26399;&#24615;&#25110;&#20934;&#21608;&#26399;&#24615;&#21160;&#24577;&#65292;&#36825;&#20123;&#21160;&#24577;&#24448;&#24448;&#19981;&#33021;&#34987;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#20805;&#20998;&#25429;&#25417;&#21040;&#12290;&#36825;&#23548;&#33268;&#23545;&#24863;&#20852;&#36259;&#30340;&#22522;&#30784;&#21160;&#24577;&#34892;&#20026;&#30340;&#34920;&#31034;&#19981;&#23436;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#21483;&#20570;Floss&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#21270;&#22320;&#22312;&#39057;&#22495;&#19978;&#35843;&#25972;&#23398;&#21040;&#30340;&#34920;&#31034;&#26469;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;Floss&#26041;&#27861;&#39318;&#20808;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20027;&#35201;&#21608;&#26399;&#24615;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#21608;&#26399;&#31227;&#20301;&#21644;&#35889;&#23494;&#24230;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;Floss&#21487;&#20197;&#36731;&#26494;&#22320;&#25972;&#21512;&#21040;&#26377;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is a fundamental task in various application domains, and deep learning approaches have demonstrated remarkable performance in this area. However, many real-world time series data exhibit significant periodic or quasi-periodic dynamics that are often not adequately captured by existing deep learning-based solutions. This results in an incomplete representation of the underlying dynamic behaviors of interest. To address this gap, we propose an unsupervised method called Floss that automatically regularizes learned representations in the frequency domain. The Floss method first automatically detects major periodicities from the time series. It then employs periodic shift and spectral density similarity measures to learn meaningful representations with periodic consistency. In addition, Floss can be easily incorporated into both supervised, semi-supervised, and unsupervised learning frameworks. We conduct extensive experiments on common time series classification, for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#32771;&#34385;&#20102;&#29992;&#25143;&#30340;&#35282;&#24230;&#21644;&#31995;&#32479;&#30340;&#35282;&#24230;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#24120;&#35782;&#30693;&#35782;&#25552;&#21319;&#20102;ChatGPT&#22312;&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#39033;&#35780;&#20272;&#25351;&#26631;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.00085</link><description>&lt;p&gt;
&#20808;&#24605;&#32771;&#20877;&#22238;&#24212;&#65306;&#20026;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#38598;&#25104;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation. (arXiv:2308.00085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#32771;&#34385;&#20102;&#29992;&#25143;&#30340;&#35282;&#24230;&#21644;&#31995;&#32479;&#30340;&#35282;&#24230;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#24120;&#35782;&#30693;&#35782;&#25552;&#21319;&#20102;ChatGPT&#22312;&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#39033;&#35780;&#20272;&#25351;&#26631;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#26041;&#27861;&#35797;&#22270;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#25110;&#23545;&#24773;&#32490;&#21407;&#22240;&#30340;&#25512;&#29702;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#30340;&#32463;&#21382;&#21644;&#24863;&#21463;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20174;&#29992;&#25143;&#30340;&#35282;&#24230;&#29702;&#35299;&#19978;&#19979;&#25991;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#31995;&#32479;&#30340;&#35282;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#65292;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#30340;&#35282;&#24230;&#65288;&#29992;&#25143;&#30340;&#27442;&#26395;&#21644;&#21453;&#24212;&#65289;&#21644;&#31995;&#32479;&#30340;&#35282;&#24230;&#65288;&#31995;&#32479;&#30340;&#24847;&#22270;&#21644;&#21453;&#24212;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#24120;&#35782;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;ChatGPT&#22312;&#31995;&#32479;&#30340;&#35282;&#24230;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#24120;&#35782;&#30340;&#22240;&#26524;&#35299;&#37322;&#19982;ChatGPT&#21644;&#22522;&#20110;T5&#27169;&#22411;&#30340;&#26041;&#27861;&#36827;&#34892;&#25972;&#21512;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#19978;&#20248;&#20110;&#20854;&#20182;&#21487;&#27604;&#36739;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches to empathetic response generation try to incorporate commonsense knowledge or reasoning about the causes of emotions to better understand the user's experiences and feelings. However, these approaches mainly focus on understanding the causalities of context from the user's perspective, ignoring the system's perspective. In this paper, we propose a commonsense-based causality explanation approach for diverse empathetic response generation that considers both the user's perspective (user's desires and reactions) and the system's perspective (system's intentions and reactions). We enhance ChatGPT's ability to reason for the system's perspective by integrating in-context learning with commonsense knowledge. Then, we integrate the commonsense-based causality explanation with both ChatGPT and a T5-based model. Experimental evaluations demonstrate that our method outperforms other comparable methods on both automatic and human evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#30417;&#25511;&#35270;&#39057;&#36755;&#20837;&#36827;&#34892;&#29359;&#32618;&#29616;&#22330;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;Jetson&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#21644;&#36816;&#34892;&#12290;&#36825;&#26159;&#23545;Jetson&#24179;&#21488;&#22312;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#20998;&#26512;&#30340;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2307.16834</link><description>&lt;p&gt;
&#20351;&#29992;&#31471;&#21040;&#31471;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#26469;&#22522;&#20934;&#27979;&#35797;Jetson&#36793;&#32536;&#35774;&#22791;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System. (arXiv:2307.16834v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#30417;&#25511;&#35270;&#39057;&#36755;&#20837;&#36827;&#34892;&#29359;&#32618;&#29616;&#22330;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;Jetson&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#21644;&#36816;&#34892;&#12290;&#36825;&#26159;&#23545;Jetson&#24179;&#21488;&#22312;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#20998;&#26512;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#24179;&#21488;&#65292;&#29305;&#21035;&#26159;&#30828;&#20214;&#21152;&#36895;&#65292;&#26174;&#30528;&#24433;&#21709;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#21019;&#26032;&#23558;&#20154;&#31867;&#21171;&#21160;&#36716;&#21270;&#20026;&#33258;&#21160;&#21270;&#30340;&#26234;&#33021;&#31995;&#32479;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#29289;&#32852;&#32593;&#21644;&#35768;&#22810;&#20854;&#20182;&#26377;&#37325;&#22823;&#24433;&#21709;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;NVIDIA&#30340;Jetson&#24179;&#21488;&#26159;&#22312;&#25191;&#34892;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26041;&#38754;&#33021;&#22815;&#25552;&#20379;&#33021;&#25928;&#21644;&#21534;&#21520;&#29575;&#26368;&#20339;&#24615;&#33021;&#30340;&#20808;&#39537;&#20043;&#19968;&#12290;&#20808;&#21069;&#30340;&#22823;&#37096;&#20998;&#22522;&#20934;&#27979;&#35797;&#20998;&#26512;&#37117;&#26159;&#22522;&#20110;2D&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27599;&#20010;&#27604;&#36739;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#20174;&#30417;&#25511;&#35270;&#39057;&#36755;&#20837;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#35270;&#39057;&#30340;&#29359;&#32618;&#29616;&#22330;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#23558;&#35813;&#31995;&#32479;&#37096;&#32626;&#22312;&#22810;&#20010;Jetson&#36793;&#32536;&#35774;&#22791;&#19978;&#65288;Nano&#12289;AGX Xavier&#12289;Orin Nano&#65289;&#12290;&#27604;&#36739;&#20998;&#26512;&#21253;&#25324;&#23558;Torch-TensorRT&#38598;&#25104;&#20026;&#36719;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innovative enhancement in embedded system platforms, specifically hardware accelerations, significantly influence the application of deep learning in real-world scenarios. These innovations translate human labor efforts into automated intelligent systems employed in various areas such as autonomous driving, robotics, Internet-of-Things (IoT), and numerous other impactful applications. NVIDIA's Jetson platform is one of the pioneers in offering optimal performance regarding energy efficiency and throughput in the execution of deep learning algorithms. Previously, most benchmarking analysis was based on 2D images with a single deep learning model for each comparison result. In this paper, we implement an end-to-end video-based crime-scene anomaly detection system inputting from surveillance videos and the system is deployed and completely operates on multiple Jetson edge devices (Nano, AGX Xavier, Orin Nano). The comparison analysis includes the integration of Torch-TensorRT as a softwar
&lt;/p&gt;</description></item><item><title>BAGM&#26159;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#24708;&#26080;&#22768;&#24687;&#22320;&#25805;&#32437;&#29992;&#25143;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#35813;&#25915;&#20987;&#26159;&#39318;&#20010;&#38024;&#23545;&#19977;&#20010;&#27969;&#34892;&#27169;&#22411;&#19981;&#21516;&#29983;&#25104;&#38454;&#27573;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.16489</link><description>&lt;p&gt;
BAGM&#65306;&#19968;&#31181;&#29992;&#20110;&#25805;&#32437;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models. (arXiv:2307.16489v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16489
&lt;/p&gt;
&lt;p&gt;
BAGM&#26159;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#24708;&#26080;&#22768;&#24687;&#22320;&#25805;&#32437;&#29992;&#25143;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#35813;&#25915;&#20987;&#26159;&#39318;&#20010;&#38024;&#23545;&#19977;&#20010;&#27969;&#34892;&#27169;&#22411;&#19981;&#21516;&#29983;&#25104;&#38454;&#27573;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26222;&#21450;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20844;&#20247;&#20851;&#27880;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#39033;&#25216;&#26415;&#21487;&#20197;&#34987;&#25915;&#20987;&#65292;&#20197;&#29983;&#25104;&#24708;&#26080;&#22768;&#24687;&#22320;&#25805;&#32437;&#29992;&#25143;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;&#65288;BAGM&#65289;&#65292;&#19968;&#26086;&#35302;&#21457;&#65292;&#20250;&#21521;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#27880;&#20837;&#33258;&#28982;&#34701;&#20837;&#20869;&#23481;&#30340;&#25805;&#32437;&#32454;&#33410;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#19977;&#20010;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#30340;&#19977;&#20010;&#38454;&#27573;&#36827;&#34892;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#23884;&#20837;&#26631;&#35760;&#22120;&#12289;&#35821;&#35328;&#27169;&#22411;&#25110;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#22522;&#20110;&#28183;&#36879;&#32423;&#21035;&#65292;BAGM&#37319;&#29992;&#20102;&#34920;&#38754;&#12289;&#27973;&#23618;&#21644;&#28145;&#23618;&#25915;&#20987;&#30340;&#19968;&#31995;&#21015;&#25915;&#20987;&#24418;&#24335;&#12290;&#37492;&#20110;&#35813;&#39046;&#22495;&#30340;&#29616;&#26377;&#24046;&#36317;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#25351;&#26631;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#21518;&#38376;&#25915;&#20987;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#24433;&#21709;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise in popularity of text-to-image generative artificial intelligence (AI) has attracted widespread public interest. We demonstrate that this technology can be attacked to generate content that subtly manipulates its users. We propose a Backdoor Attack on text-to-image Generative Models (BAGM), which upon triggering, infuses the generated images with manipulative details that are naturally blended in the content. Our attack is the first to target three popular text-to-image generative models across three stages of the generative process by modifying the behaviour of the embedded tokenizer, the language model or the image generative model. Based on the penetration level, BAGM takes the form of a suite of attacks that are referred to as surface, shallow and deep attacks in this article. Given the existing gap within this domain, we also contribute a comprehensive set of quantitative metrics designed specifically for assessing the effectiveness of backdoor attacks on text-to-image mo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#27493;&#38271;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#20026;$O(1/t)$&#12290;</title><link>http://arxiv.org/abs/2307.15892</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#19968;&#20010;&#27493;&#38271;&#30340;&#26032;&#22411;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#65306;&#36890;&#36807;$L$-$\lambda$&#24179;&#28369;&#24615;&#36827;&#34892;&#25910;&#25947;&#36895;&#29575;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A new Gradient TD Algorithm with only One Step-size: Convergence Rate Analysis using $L$-$\lambda$ Smoothness. (arXiv:2307.15892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#27493;&#38271;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#20026;$O(1/t)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#65288;GTD&#65289;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31163;&#31574;&#30053;&#23398;&#20064;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#22797;&#26434;&#24230;&#20026;$O(d)$&#65288;$d$&#26159;&#29305;&#24449;&#25968;&#37327;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Impression GTD&#30340;&#20840;&#26032;&#21333;&#26102;&#38388;&#23610;&#24230;GTD&#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#26399;&#26395;td&#26356;&#26032;&#65288;NEU&#65289;&#30446;&#26631;&#65292;&#24182;&#21482;&#26377;&#19968;&#20010;&#27493;&#38271;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#26032;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#19982;$O(1/t)$&#19968;&#26679;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient Temporal Difference (GTD) algorithms (Sutton et al., 2008, 2009) are the first $O(d)$ ($d$ is the number features) algorithms that have convergence guarantees for off-policy learning with linear function approximation. Liu et al. (2015) and Dalal et. al. (2018) proved the convergence rates of GTD, GTD2 and TDC are $O(t^{-\alpha/2})$ for some $\alpha \in (0,1)$. This bound is tight (Dalal et al., 2020), and slower than $O(1/\sqrt{t})$. GTD algorithms also have two step-size parameters, which are difficult to tune. In literature, there is a "single-time-scale" formulation of GTD. However, this formulation still has two step-size parameters.  This paper presents a truly single-time-scale GTD algorithm for minimizing the Norm of Expected td Update (NEU) objective, and it has only one step-size parameter. We prove that the new algorithm, called Impression GTD, converges at least as fast as $O(1/t)$. Furthermore, based on a generalization of the expected smoothness (Gower et al. 201
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#21307;&#30103;&#24212;&#29992;&#21644;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.13704</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review. (arXiv:2307.13704v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#21307;&#30103;&#24212;&#29992;&#21644;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#29616;&#22312;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#33021;&#22815;&#35299;&#37322;&#22797;&#26434;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;XAI&#29305;&#21035;&#36866;&#29992;&#20110;&#21361;&#38505;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#20154;&#31867;&#30340;&#29983;&#21629;&#20381;&#36182;&#20110;AI&#31995;&#32479;&#30340;&#20915;&#31574;&#12290;&#21307;&#30103;&#30740;&#31350;&#30340;&#19968;&#20010;&#39046;&#22495;&#26159;&#24180;&#40836;&#39044;&#27979;&#21644;&#34928;&#32769;&#21450;&#19982;&#24180;&#40836;&#30456;&#20851;&#30142;&#30149;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#37492;&#23450;&#12290;&#28982;&#32780;&#65292;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;XAI&#30340;&#20316;&#29992;&#23578;&#26410;&#30452;&#25509;&#25506;&#35752;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#22120;&#23448;&#31995;&#32479;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;XAI&#22312;&#21307;&#30103;&#24212;&#29992;&#20197;&#21450;&#29305;&#21035;&#26159;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.13421</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#19977;&#20010;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#20043;&#19968;&#26469;&#23398;&#20064;&#65292;&#20998;&#21035;&#31216;&#20026;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#12290;&#36825;&#19977;&#31181;&#33539;&#24335;&#37117;&#26159;&#20026;&#20102;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21363;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#8220;&#28966;&#28857;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#8220;&#36873;&#25321;&#8221;&#36755;&#20837;&#20013;&#30340;&#27491;&#30830;&#8220;&#29255;&#27573;&#8221;&#65292;&#21644;&#19968;&#20010;&#8220;&#20998;&#31867;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#36873;&#23450;&#30340;&#29255;&#27573;&#22788;&#29702;&#25104;&#30446;&#26631;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#23548;&#33268;&#20102;&#19981;&#21516;&#30340;&#21160;&#24577;&#21644;&#26368;&#32456;&#32467;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#36825;&#20123;&#33539;&#24335;&#23398;&#20064;&#30340;&#27169;&#22411;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#35299;&#37322;&#20026;&#22312;&#28966;&#28857;&#27169;&#22411;&#22266;&#23450;&#26102;&#65292;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#25152;&#33268;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#20998;&#26512;&#20102;&#36825;&#20123;&#33539;&#24335;&#65292;&#24182;&#25512;&#23548;&#20986;&#26799;&#24230;&#27969;&#19979;&#21442;&#25968;&#36712;&#36857;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#22312;&#36719;&#27880;&#24847;&#21147;&#25439;&#22833;&#19979;&#65292;&#28966;&#28857;&#27169;&#22411;&#22312;&#21021;&#22987;&#21270;&#38454;&#27573;&#24555;&#36895;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#20102;&#22810;&#31181;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#12290;&#23545;&#20110;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#25214;&#21040;Clarke&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.10053</link><description>&lt;p&gt;
&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization. (arXiv:2307.10053v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#20102;&#22810;&#31181;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#12290;&#23545;&#20110;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#25214;&#21040;Clarke&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#21450;&#20854;&#21464;&#31181;&#22312;&#35757;&#32451;&#30001;&#38750;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#26500;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20026;&#26356;&#26032;&#21160;&#37327;&#39033;&#21644;&#21464;&#37327;&#30340;&#27493;&#38271;&#20998;&#37197;&#20102;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#12290;&#22312;&#19968;&#20123;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#21547;&#20102;&#24456;&#22810;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#65292;&#21253;&#25324;heavy-ball SGD&#12289;SignSGD&#12289;Lion&#12289;normalized SGD&#21644;clipped SGD&#12290;&#27492;&#22806;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#37319;&#29992;&#26377;&#38480;&#21644;&#24418;&#24335;&#26102;&#65292;&#25105;&#20204;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#35777;&#26126;&#20102;&#36825;&#20123;SGD&#31867;&#22411;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;SGD&#31867;&#22411;&#26041;&#27861;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#33021;&#22815;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;Clarke&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the convergence properties of the stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions. We develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, respectively. Under mild conditions, we prove the global convergence of our proposed framework in both single-timescale and two-timescale cases. We show that our proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD. Furthermore, when the objective function adopts a finite-sum formulation, we prove the convergence properties for these SGD-type methods based on our proposed framework. In particular, we prove that these SGD-type methods find the Clarke stationary points of the objective function with randomly chosen stepsizes and initial points under mild assumptions. Preli
&lt;/p&gt;</description></item><item><title>ACTI&#22312;EVALITA 2023&#20013;&#30340;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21028;&#26029;&#38452;&#35851;&#20869;&#23481;&#21644;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.06954</link><description>&lt;p&gt;
ACTI&#22312;EVALITA 2023&#20013;&#30340;&#32508;&#36848;&#65306;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task. (arXiv:2307.06954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06954
&lt;/p&gt;
&lt;p&gt;
ACTI&#22312;EVALITA 2023&#20013;&#30340;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21028;&#26029;&#38452;&#35851;&#20869;&#23481;&#21644;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#26159;Evalita 2023&#39318;&#27425;&#25552;&#20986;&#30340;&#26032;&#20849;&#20139;&#20219;&#21153;&#12290;ACTI&#25361;&#25112;&#20165;&#22522;&#20110;Telegram&#19978;&#30340;&#38452;&#35851;&#39057;&#36947;&#35780;&#35770;&#65292;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(i) &#38452;&#35851;&#20869;&#23481;&#20998;&#31867;&#65306;&#36776;&#35782;&#38452;&#35851;&#20869;&#23481;&#21644;(ii) &#38452;&#35851;&#31867;&#21035;&#20998;&#31867;&#65306;&#38024;&#23545;&#29305;&#23450;&#38452;&#35851;&#29702;&#35770;&#20998;&#31867;&#12290;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#20102;&#35813;&#20219;&#21153;&#65292;&#24635;&#20849;&#25552;&#20132;&#20102;81&#20010;&#32467;&#26524;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conspiracy Theory Identication task is a new shared task proposed for the first time at the Evalita 2023. The ACTI challenge, based exclusively on comments published on conspiratorial channels of telegram, is divided into two subtasks: (i) Conspiratorial Content Classification: identifying conspiratorial content and (ii) Conspiratorial Category Classification about specific conspiracy theory classification. A total of fifteen teams participated in the task for a total of 81 submissions. We illustrate the best performing approaches were based on the utilization of large language models. We finally draw conclusions about the utilization of these models for counteracting the spreading of misinformation in online platforms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;GPT-3.5&#25991;&#26412;&#21040;SQL&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#26679;&#26412;&#24863;&#30693;&#24341;&#23548;&#21644;&#21160;&#24577;&#20462;&#35746;&#38142;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#35821;&#20041;&#24046;&#36317;&#36739;&#22823;&#30340;&#26816;&#32034;&#31034;&#20363;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.05074</link><description>&lt;p&gt;
&#37319;&#29992;&#26679;&#26412;&#24863;&#30693;&#24341;&#23548;&#21644;&#21160;&#24577;&#20462;&#35746;&#38142;&#30340;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;GPT-3.5&#25991;&#26412;&#21040;SQL&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented GPT-3.5-based Text-to-SQL Framework with Sample-aware Prompting and Dynamic Revision Chain. (arXiv:2307.05074v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;GPT-3.5&#25991;&#26412;&#21040;SQL&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#26679;&#26412;&#24863;&#30693;&#24341;&#23548;&#21644;&#21160;&#24577;&#20462;&#35746;&#38142;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#35821;&#20041;&#24046;&#36317;&#36739;&#22823;&#30340;&#26816;&#32034;&#31034;&#20363;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;SQL&#26597;&#35810;&#65292;&#20174;&#32780;&#24110;&#21161;&#29992;&#25143;&#26597;&#35810;&#25968;&#25454;&#24211;&#12290;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35774;&#35745;&#25552;&#31034;&#20197;&#24341;&#23548;LLMs&#29702;&#35299;&#36755;&#20837;&#38382;&#39064;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;SQL&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#30528;&#20005;&#26684;&#30340;SQL&#35821;&#27861;&#35201;&#27714;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#24037;&#20316;&#20351;&#29992;&#19968;&#31995;&#21015;&#31034;&#20363;&#65288;&#21363;&#38382;&#39064;-SQL&#23545;&#65289;&#26469;&#25552;&#31034;LLMs&#29983;&#25104;SQL&#65292;&#20294;&#22266;&#23450;&#30340;&#25552;&#31034;&#20960;&#20046;&#26080;&#27861;&#22788;&#29702;&#26816;&#32034;&#20986;&#30340;&#31034;&#20363;&#19982;&#36755;&#20837;&#38382;&#39064;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#36739;&#22823;&#30340;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#21040;SQL&#26694;&#26550;&#65292;&#21253;&#25324;&#26679;&#26412;&#24863;&#30693;&#25552;&#31034;&#21644;&#21160;&#24577;&#20462;&#35746;&#38142;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#26679;&#26412;&#24863;&#30693;&#31034;&#20363;&#65292;&#20854;&#20013;&#21253;&#25324;SQL&#36816;&#31639;&#31526;&#30340;&#32452;&#21512;&#21644;&#19982;&#32473;&#23450;&#38382;&#39064;&#30456;&#20851;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL aims at generating SQL queries for the given natural language questions and thus helping users to query databases. Prompt learning with large language models (LLMs) has emerged as a recent approach, which designs prompts to lead LLMs to understand the input question and generate the corresponding SQL. However, it faces challenges with strict SQL syntax requirements. Existing work prompts the LLMs with a list of demonstration examples (i.e. question-SQL pairs) to generate SQL, but the fixed prompts can hardly handle the scenario where the semantic gap between the retrieved demonstration and the input question is large. In this paper, we propose a retrieval-augmented prompting method for a LLM-based Text-to-SQL framework, involving sample-aware prompting and a dynamic revision chain. Our approach incorporates sample-aware demonstrations, which include the composition of SQL operators and fine-grained information related to the given question. To retrieve questions sharing sim
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#24341;&#20837;&#20102;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04726</link><description>&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#31574;&#30053;&#30340;&#25193;&#25955;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning. (arXiv:2307.04726v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#24341;&#20837;&#20102;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064; (RL) &#26041;&#27861;&#21033;&#29992;&#20197;&#21069;&#30340;&#32463;&#39564;&#26469;&#23398;&#20064;&#27604;&#29992;&#20110;&#25968;&#25454;&#25910;&#38598;&#30340;&#34892;&#20026;&#31574;&#30053;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#19982;&#34892;&#20026;&#20811;&#38534;&#30456;&#21453;&#65292;&#34892;&#20026;&#20811;&#38534;&#20551;&#35774;&#25968;&#25454;&#26159;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#25910;&#38598;&#30340;&#65292;&#32780;&#33073;&#26426; RL &#21487;&#20197;&#20351;&#29992;&#38750;&#19987;&#23478;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#34892;&#20026;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#33073;&#26426; RL &#31639;&#27861;&#22312;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#32570;&#20047;&#22312;&#32447;&#20132;&#20114;&#12290;&#20808;&#21069;&#20851;&#20110;&#33073;&#26426; RL &#30340;&#24037;&#20316;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#34920;&#31034;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#27169;&#24577;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#38024;&#23545;&#32531;&#35299;&#33073;&#26426;&#20998;&#24067;&#29366;&#24577;&#27867;&#21270;&#32780;&#21046;&#23450;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP)&#65292;&#23558;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#32435;&#20837;&#21040;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#20998;&#24067;&#36890;&#29992;&#21270;&#38382;&#39064;&#12290;&#29366;&#24577;&#37325;&#26500;&#25439;&#22833;&#20419;&#36827;&#20102;&#26356;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning (RL) methods leverage previous experiences to learn better policies than the behavior policy used for data collection. In contrast to behavior cloning, which assumes the data is collected from expert demonstrations, offline RL can work with non-expert data and multimodal behavior policies. However, offline RL algorithms face challenges in handling distribution shifts and effectively representing policies due to the lack of online interaction during training. Prior work on offline RL uses conditional diffusion models to represent multimodal behavior in the dataset. Nevertheless, these methods are not tailored toward alleviating the out-of-distribution state generalization. We introduce a novel method, named State Reconstruction for Diffusion Policies (SRDP), incorporating state reconstruction feature learning in the recent class of diffusion policies to address the out-of-distribution generalization problem. State reconstruction loss promotes more descript
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#30417;&#31649;&#38656;&#35201;&#26631;&#20934;&#21046;&#23450;&#12289;&#27880;&#20876;&#25253;&#21578;&#21644;&#23433;&#20840;&#21512;&#35268;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.03718</link><description>&lt;p&gt;
&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30417;&#31649;&#65306;&#31649;&#29702;&#23545;&#20844;&#20849;&#23433;&#20840;&#30340;&#26032;&#20852;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Frontier AI Regulation: Managing Emerging Risks to Public Safety. (arXiv:2307.03718v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03718
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#30417;&#31649;&#38656;&#35201;&#26631;&#20934;&#21046;&#23450;&#12289;&#27880;&#20876;&#25253;&#21578;&#21644;&#23433;&#20840;&#21512;&#35268;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20026;&#20154;&#31867;&#24102;&#26469;&#24040;&#22823;&#30340;&#22909;&#22788;&#65292;&#20294;&#31038;&#20250;&#38656;&#35201;&#20027;&#21160;&#31649;&#29702;&#30456;&#20851;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#20851;&#27880;&#25105;&#20204;&#25152;&#31216;&#30340;&#8220;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#8221;&#27169;&#22411;&#65306;&#39640;&#24230;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#33021;&#20855;&#22791;&#36275;&#20197;&#23545;&#20844;&#20849;&#23433;&#20840;&#36896;&#25104;&#20005;&#37325;&#39118;&#38505;&#30340;&#21361;&#38505;&#33021;&#21147;&#12290;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#30417;&#31649;&#25361;&#25112;&#65306;&#21361;&#38505;&#33021;&#21147;&#21487;&#33021;&#20986;&#20046;&#24847;&#26009;&#65307;&#24456;&#38590;&#26377;&#25928;&#38450;&#27490;&#37096;&#32626;&#27169;&#22411;&#34987;&#28389;&#29992;&#65307;&#24182;&#19988;&#24456;&#38590;&#38459;&#27490;&#27169;&#22411;&#30340;&#33021;&#21147;&#24191;&#27867;&#25193;&#25955;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#36793;&#32536;&#27169;&#22411;&#30340;&#30417;&#31649;&#38656;&#35201;&#33267;&#23569;&#19977;&#20010;&#22522;&#26412;&#35201;&#32032;&#65306;(1) &#35774;&#23450;&#26631;&#20934;&#30340;&#36807;&#31243;&#65292;&#20197;&#30830;&#23450;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#30340;&#36866;&#24403;&#35201;&#27714;&#65307;(2) &#27880;&#20876;&#21644;&#25253;&#21578;&#35201;&#27714;&#65292;&#20026;&#30417;&#31649;&#26426;&#26500;&#25552;&#20379;&#23545;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#36807;&#31243;&#30340;&#21487;&#35265;&#24615;&#65307;(3) &#20445;&#35777;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#23433;&#20840;&#26631;&#20934;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advanced AI models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. In this paper, we focus on what we term "frontier AI" models: highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety. Frontier AI models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model's capabilities from proliferating broadly. To address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier AI developers, (2) registration and reporting requirements to provide regulators with visibility into frontier AI development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deplo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FormAI&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;112,000&#20010;&#21487;&#32534;&#35793;&#30340;C&#31243;&#24207;&#65292;&#21033;&#29992;&#21160;&#24577;&#38646;-shot&#25552;&#31034;&#25216;&#26415;&#29983;&#25104;&#12290;&#36825;&#20123;&#31243;&#24207;&#32463;&#36807;&#24418;&#24335;&#39564;&#35777;&#65292;&#26631;&#35760;&#20102;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#31243;&#24207;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02192</link><description>&lt;p&gt;
FormAI&#25968;&#25454;&#38598;&#65306;&#20197;&#24418;&#24335;&#39564;&#35777;&#20026;&#35270;&#35282;&#30340;&#36719;&#20214;&#23433;&#20840;&#29983;&#25104;AI
&lt;/p&gt;
&lt;p&gt;
The FormAI Dataset: Generative AI in Software Security Through the Lens of Formal Verification. (arXiv:2307.02192v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FormAI&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;112,000&#20010;&#21487;&#32534;&#35793;&#30340;C&#31243;&#24207;&#65292;&#21033;&#29992;&#21160;&#24577;&#38646;-shot&#25552;&#31034;&#25216;&#26415;&#29983;&#25104;&#12290;&#36825;&#20123;&#31243;&#24207;&#32463;&#36807;&#24418;&#24335;&#39564;&#35777;&#65292;&#26631;&#35760;&#20102;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#31243;&#24207;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FormAI&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;112,000&#20010;&#20855;&#26377;&#28431;&#27934;&#20998;&#31867;&#30340;AI&#29983;&#25104;&#30340;&#21487;&#32534;&#35793;&#21644;&#29420;&#31435;&#30340;C&#31243;&#24207;&#30340;&#22823;&#22411;&#38598;&#21512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21160;&#24577;&#38646;-shot&#25552;&#31034;&#25216;&#26415;&#65292;&#29992;&#20110;&#29983;&#25104;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22810;&#26679;&#21270;&#31243;&#24207;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;GPT-3.5-turbo&#29983;&#25104;&#65292;&#24182;&#21253;&#21547;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#31243;&#24207;&#12290;&#26377;&#20123;&#31243;&#24207;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#65292;&#22914;&#32593;&#32476;&#31649;&#29702;&#12289;&#26700;&#38754;&#28216;&#25103;&#25110;&#21152;&#23494;&#65292;&#32780;&#20854;&#20182;&#31243;&#24207;&#22788;&#29702;&#31616;&#21333;&#20219;&#21153;&#65292;&#22914;&#23383;&#31526;&#20018;&#25805;&#20316;&#12290;&#27599;&#20010;&#31243;&#24207;&#37117;&#29992;&#28304;&#20195;&#30721;&#20013;&#25214;&#21040;&#30340;&#28431;&#27934;&#36827;&#34892;&#26631;&#35760;&#65292;&#25351;&#31034;&#31867;&#22411;&#12289;&#34892;&#21495;&#21644;&#28431;&#27934;&#20989;&#25968;&#21517;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;&#22522;&#20110;SMT&#30340;&#26377;&#30028;&#27169;&#22411;&#26816;&#26597;&#22120;&#65288;ESBMC&#65289;&#30340;&#24418;&#24335;&#39564;&#35777;&#26041;&#27861;&#23454;&#29616;&#30340;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#27169;&#22411;&#26816;&#26597;&#12289;&#25277;&#35937;&#35299;&#37322;&#12289;&#32422;&#26463;&#32534;&#31243;&#21644;&#21487;&#28385;&#36275;&#24615;&#27169;&#29702;&#35770;&#26469;&#25512;&#29702;&#31243;&#24207;&#20013;&#30340;&#23433;&#20840;&#24615;/&#23433;&#20840;&#23646;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30830;&#23450;&#20102;&#21644;&#39564;&#35777;&#20102;&#31243;&#24207;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the FormAI dataset, a large collection of 112, 000 AI-generated compilable and independent C programs with vulnerability classification. We introduce a dynamic zero-shot prompting technique constructed to spawn diverse programs utilizing Large Language Models (LLMs). The dataset is generated by GPT-3.5-turbo and comprises programs with varying levels of complexity. Some programs handle complicated tasks like network management, table games, or encryption, while others deal with simpler tasks like string manipulation. Every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name. This is accomplished by employing a formal verification method using the Efficient SMT-based Bounded Model Checker (ESBMC), which uses model checking, abstract interpretation, constraint programming, and satisfiability modulo theories to reason over safety/security properties in programs. This approach definitiv
&lt;/p&gt;</description></item><item><title>KnowNo is a framework that measures and aligns the uncertainty of LLM-based planners, enabling them to ask for help when they don't know. It performs favorably in improving efficiency and autonomy compared to baselines, while providing formal assurances.</title><link>http://arxiv.org/abs/2307.01928</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#35831;&#27714;&#24110;&#21161;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners. (arXiv:2307.01928v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01928
&lt;/p&gt;
&lt;p&gt;
KnowNo is a framework that measures and aligns the uncertainty of LLM-based planners, enabling them to ask for help when they don't know. It performs favorably in improving efficiency and autonomy compared to baselines, while providing formal assurances.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#26377;&#21069;&#26223;&#30340;&#33021;&#21147;&#65292;&#20174;&#36880;&#27493;&#35268;&#21010;&#21040;&#24120;&#35782;&#25512;&#29702;&#65292;&#36825;&#20123;&#33021;&#21147;&#21487;&#33021;&#20026;&#26426;&#22120;&#20154;&#25552;&#20379;&#25928;&#29992;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#20135;&#29983;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KnowNo&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#21644;&#23545;&#40784;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#22120;&#19981;&#30830;&#23450;&#24615;&#30340;&#26694;&#26550;&#65292;&#20351;&#20854;&#30693;&#36947;&#20309;&#26102;&#19981;&#30693;&#36947;&#24182;&#22312;&#38656;&#35201;&#26102;&#35831;&#27714;&#24110;&#21161;&#12290;KnowNo&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#29702;&#35770;&#65292;&#25552;&#20379;&#20219;&#21153;&#23436;&#25104;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#21516;&#26102;&#22312;&#22797;&#26434;&#30340;&#22810;&#27493;&#35268;&#21010;&#29615;&#22659;&#20013;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20154;&#31867;&#24110;&#21161;&#12290;&#22312;&#28041;&#21450;&#19981;&#21516;&#27169;&#31946;&#27169;&#24335;&#20219;&#21153;&#30340;&#21508;&#31181;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#35774;&#32622;&#30340;&#23454;&#39564;&#20013;&#65288;&#20363;&#22914;&#20174;&#31354;&#38388;&#21040;&#25968;&#23383;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#20154;&#31867;&#20559;&#22909;&#21040;Winograd&#27169;&#24335;&#65289;&#65292;&#32467;&#26524;&#34920;&#26126;KnowNo&#22312;&#25552;&#39640;&#25928;&#29575;&#21644;&#33258;&#27835;&#24615;&#26041;&#38754;&#30456;&#23545;&#20110;&#29616;&#20195;&#22522;&#32447;&#65288;&#21487;&#33021;&#28041;&#21450;&#38598;&#21512;&#25110;&#24191;&#27867;&#30340;&#25552;&#31034;&#35843;&#25972;&#65289;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#25552;&#20379;&#27491;&#24335;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. Kno
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#29992;&#38646;&#20214;&#32452;&#35013;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26550;&#26500;GPAT&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#38646;&#20214;&#30340;&#23039;&#24577;&#24182;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00206</link><description>&lt;p&gt;
&#36890;&#29992;&#38646;&#20214;&#32452;&#35013;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
General Part Assembly Planning. (arXiv:2307.00206v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#29992;&#38646;&#20214;&#32452;&#35013;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26550;&#26500;GPAT&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#38646;&#20214;&#30340;&#23039;&#24577;&#24182;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#32452;&#35013;&#39046;&#22495;&#30340;&#22823;&#22810;&#25968;&#25104;&#21151;&#37117;&#23616;&#38480;&#20110;&#21333;&#20010;&#30446;&#26631;&#25110;&#32773;&#21333;&#19968;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#30740;&#31350;&#36890;&#29992;&#38646;&#20214;&#32452;&#35013;&#65292;&#21363;&#20351;&#29992;&#26410;&#35265;&#36807;&#30340;&#38646;&#20214;&#24418;&#29366;&#21019;&#24314;&#26032;&#39062;&#30446;&#26631;&#32452;&#35013;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36890;&#29992;&#38646;&#20214;&#32452;&#35013;&#30340;&#35268;&#21010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#38646;&#20214;&#32452;&#35013;Transformer&#65288;GPAT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#25512;&#26029;&#27599;&#20010;&#38646;&#20214;&#24418;&#29366;&#22914;&#20309;&#23545;&#24212;&#30446;&#26631;&#24418;&#29366;&#65292;&#20934;&#30830;&#39044;&#27979;&#38646;&#20214;&#30340;&#23039;&#24577;&#12290;&#25105;&#20204;&#22312;3D CAD&#27169;&#22411;&#21644;&#29616;&#23454;&#19990;&#30028;&#25195;&#25551;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GPAT&#20855;&#26377;&#23545;&#26032;&#39062;&#21644;&#22810;&#26679;&#21270;&#30340;&#30446;&#26631;&#21644;&#38646;&#20214;&#24418;&#29366;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most successes in autonomous robotic assembly have been restricted to single target or category. We propose to investigate general part assembly, the task of creating novel target assemblies with unseen part shapes. To tackle the planning of general part assembly, we present General Part Assembly Transformer (GPAT), a transformer based model architecture that accurately predicts part poses by inferring how each part shape corresponds to the target shape. Our experiments on both 3D CAD models and real-world scans demonstrate GPAT's generalization abilities to novel and diverse target and part shapes. Project website: https://general-part-assembly.github.io/
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#25351;&#26631;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25269;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#25351;&#26631;&#30340;&#19981;&#36275;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#25351;&#26631;&#33021;&#22815;&#21306;&#20998;&#27169;&#22411;&#23545;&#29305;&#23450;&#23041;&#32961;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16614</link><description>&lt;p&gt;
&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65306;&#29616;&#23454;&#19990;&#30028;&#20013;&#23450;&#21046;&#40065;&#26834;&#24615;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Group-based Robustness: A General Framework for Customized Robustness in the Real World. (arXiv:2306.16614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#25351;&#26631;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25269;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#25351;&#26631;&#30340;&#19981;&#36275;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#25351;&#26631;&#33021;&#22815;&#21306;&#20998;&#27169;&#22411;&#23545;&#29305;&#23450;&#23041;&#32961;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#36867;&#36991;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#36890;&#36807;&#25200;&#21160;&#27169;&#22411;&#36755;&#20837;&#26469;&#24341;&#36215;&#38169;&#35823;&#20998;&#31867;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#24230;&#37327;&#30446;&#26631;&#21644;&#38750;&#30446;&#26631;&#40065;&#26834;&#24615;&#30340;&#25351;&#26631;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#30495;&#23454;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#23427;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#26356;&#36866;&#21512;&#35780;&#20272;&#29305;&#23450;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#33021;&#22815;&#22312;&#20256;&#32479;&#30340;&#40065;&#26834;&#24615;&#25351;&#26631;&#19981;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#23545;&#29305;&#23450;&#23041;&#32961;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26377;&#25928;&#20934;&#30830;&#22320;&#34913;&#37327;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learning models are known to be vulnerable to evasion attacks that perturb model inputs to induce misclassifications. In this work, we identify real-world scenarios where the true threat cannot be assessed accurately by existing attacks. Specifically, we find that conventional metrics measuring targeted and untargeted robustness do not appropriately reflect a model's ability to withstand attacks from one set of source classes to another set of target classes. To address the shortcomings of existing methods, we formally define a new metric, termed group-based robustness, that complements existing metrics and is better-suited for evaluating model performance in certain attack scenarios. We show empirically that group-based robustness allows us to distinguish between models' vulnerability against specific threat models in situations where traditional robustness metrics do not apply. Moreover, to measure group-based robustness efficiently and accurately, we 1) propose two loss func
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#36719;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;&#23398;&#20064;&#27169;&#22411;SoftGPT&#65292;&#23427;&#20351;&#29992;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#21512;&#19977;&#32500;&#24322;&#26500;&#22270;&#34920;&#31034;&#21644;&#22522;&#20110;GPT&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#20808;&#21069;&#30693;&#35782;&#26469;&#23398;&#20064;&#30446;&#26631;&#23548;&#21521;&#30340;&#36719;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;&#65292;&#20174;&#32780;&#25171;&#30772;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#25216;&#26415;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2306.12677</link><description>&lt;p&gt;
SoftGPT: &#36890;&#36807;&#39044;&#35757;&#32451;&#24322;&#36136;&#22270;&#21464;&#25442;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#38754;&#21521;&#30446;&#26631;&#30340;&#36719;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
SoftGPT: Learn Goal-oriented Soft Object Manipulation Skills by Generative Pre-trained Heterogeneous Graph Transformer. (arXiv:2306.12677v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12677
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#36719;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;&#23398;&#20064;&#27169;&#22411;SoftGPT&#65292;&#23427;&#20351;&#29992;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#21512;&#19977;&#32500;&#24322;&#26500;&#22270;&#34920;&#31034;&#21644;&#22522;&#20110;GPT&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#20808;&#21069;&#30693;&#35782;&#26469;&#23398;&#20064;&#30446;&#26631;&#23548;&#21521;&#30340;&#36719;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;&#65292;&#20174;&#32780;&#25171;&#30772;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#25216;&#26415;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23478;&#24237;&#22330;&#26223;&#20013;&#36827;&#34892;&#30340;&#36719;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#21160;&#24577;&#29305;&#24615;&#21644;&#21487;&#21464;&#30340;&#24418;&#29366;&#29305;&#24449;&#65292;&#23545;&#29616;&#26377;&#26426;&#22120;&#20154;&#25216;&#33021;&#23398;&#20064;&#25216;&#26415;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#30001;&#20110;&#20174;&#20154;&#31867;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#30340;&#25805;&#20316;&#25216;&#33021;&#26159;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#26377;&#25928;&#26041;&#24335;&#65292;&#22240;&#27492;&#24320;&#21457;&#36719;&#29289;&#20307;&#34920;&#31034;&#21644;&#21160;&#24577;&#30340;&#20808;&#21069;&#30693;&#35782;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftGPT&#30340;&#39044;&#35757;&#32451;&#36719;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#22823;&#37327;&#30340;&#25506;&#32034;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;&#19977;&#32500;&#24322;&#26500;&#22270;&#34920;&#31034;&#21644;&#22522;&#20110;GPT&#30340;&#21160;&#24577;&#27169;&#22411;&#12290;&#38024;&#23545;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#38754;&#21521;&#30446;&#26631;&#30340;&#31574;&#30053;&#20195;&#29702;&#20197;&#39044;&#27979;&#21518;&#32493;&#21160;&#20316;&#65292;SoftGPT&#29983;&#25104;&#36825;&#20123;&#21160;&#20316;&#30340;&#21518;&#26524;&#12290;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#24314;&#31435;&#20102;&#26426;&#22120;&#20154;&#24605;&#32771;&#36807;&#31243;&#65292;&#20026;&#20419;&#36827;&#31574;&#30053;&#23398;&#20064;&#25552;&#20379;&#20102;&#23637;&#24320;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#36719;&#29289;&#20307;&#21160;&#21147;&#23398;&#21644;&#34920;&#31034;&#30340;&#20808;&#21069;&#30693;&#35782;&#20351;SoftGPT&#33021;&#22815;&#23398;&#20064;&#38754;&#21521;&#30446;&#26631;&#30340;&#36719;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;&#65292;&#20174;&#32780;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft object manipulation tasks in domestic scenes pose a significant challenge for existing robotic skill learning techniques due to their complex dynamics and variable shape characteristics. Since learning new manipulation skills from human demonstration is an effective way for robot applications, developing prior knowledge of the representation and dynamics of soft objects is necessary. In this regard, we propose a pre-trained soft object manipulation skill learning model, namely SoftGPT, that is trained using large amounts of exploration data, consisting of a three-dimensional heterogeneous graph representation and a GPT-based dynamics model. For each downstream task, a goal-oriented policy agent is trained to predict the subsequent actions, and SoftGPT generates the consequences of these actions. Integrating these two approaches establishes a thinking process in the robot's mind that provides rollout for facilitating policy learning. Our results demonstrate that leveraging prior kn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#31215;&#26497;&#20154;&#26426;&#21327;&#20316;&#30340;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#24847;&#22270;&#39044;&#27979;&#21644;&#23433;&#20840;&#25511;&#21046;&#26469;&#25552;&#39640;&#21327;&#20316;&#25928;&#29575;&#21644;&#36991;&#20813;&#20132;&#20114;&#23433;&#20840;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.11862</link><description>&lt;p&gt;
&#31215;&#26497;&#21327;&#20316;&#30340;&#20154;&#26426;&#35013;&#37197;&#65306;&#21033;&#29992;&#20154;&#31867;&#24847;&#22270;&#39044;&#27979;&#21644;&#40065;&#26834;&#23433;&#20840;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Proactive Human-Robot Co-Assembly: Leveraging Human Intention Prediction and Robust Safe Control. (arXiv:2306.11862v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#31215;&#26497;&#20154;&#26426;&#21327;&#20316;&#30340;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#24847;&#22270;&#39044;&#27979;&#21644;&#23433;&#20840;&#25511;&#21046;&#26469;&#25552;&#39640;&#21327;&#20316;&#25928;&#29575;&#21644;&#36991;&#20813;&#20132;&#20114;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#21327;&#20316;&#26159;&#23454;&#29616;&#28789;&#27963;&#21046;&#36896;&#20197;&#28385;&#36275;&#19981;&#21516;&#23458;&#25143;&#38656;&#27714;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#31181;&#25361;&#25112;&#65292;&#26500;&#24314;&#33021;&#22815;&#22312;&#23433;&#20840;&#39640;&#25928;&#22320;&#21327;&#21161;&#20154;&#31867;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#31215;&#26497;&#20154;&#26426;&#21327;&#20316;&#30340;&#38598;&#25104;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#24847;&#22270;&#39044;&#27979;&#27169;&#22359;&#21644;&#19968;&#20010;&#40065;&#26834;&#30340;&#23433;&#20840;&#25511;&#21046;&#27169;&#22359;&#65292;&#20998;&#21035;&#29992;&#20110;&#25552;&#39640;&#21327;&#20316;&#25928;&#29575;&#21644;&#36991;&#20813;&#20132;&#20114;&#23433;&#20840;&#39118;&#38505;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;Kinova Gen3&#26426;&#22120;&#20154;&#19978;&#25191;&#34892;&#21327;&#20316;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#29615;&#22659;&#21464;&#21270;&#21644;&#19981;&#21516;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-robot collaboration (HRC) is one key component to achieving flexible manufacturing to meet the different needs of customers. However, it is difficult to build intelligent robots that can proactively assist humans in a safe and efficient way due to several challenges.First, it is challenging to achieve efficient collaboration due to diverse human behaviors and data scarcity. Second, it is difficult to ensure interactive safety due to uncertainty in human behaviors. This paper presents an integrated framework for proactive HRC. A robust intention prediction module, which leverages prior task information and human-in-the-loop training, is learned to guide the robot for efficient collaboration. The proposed framework also uses robust safe control to ensure interactive safety under uncertainty. The developed framework is applied to a co-assembly task using a Kinova Gen3 robot. The experiment demonstrates that our solution is robust to environmental changes as well as different human p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#24335;&#35821;&#20041;&#24863;&#30693;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#29702;&#30340;&#38544;&#24335;&#35821;&#20041;&#24863;&#30693;&#36890;&#20449;&#26694;&#26550;&#65288;iSAC&#65289;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#28304;&#29992;&#25143;&#21644;&#30446;&#26631;&#29992;&#25143;&#20043;&#38388;&#34920;&#31034;&#12289;&#36890;&#20449;&#21644;&#35299;&#37322;&#38544;&#24335;&#35821;&#20041;&#21547;&#20041;&#12290;&#36890;&#36807;&#25237;&#24433;&#31639;&#27861;&#65292;&#23558;&#26174;&#24335;&#35821;&#20041;&#36716;&#25442;&#20026;&#20302;&#32500;&#35821;&#20041;&#34920;&#31034;&#65292;&#26356;&#22909;&#22320;&#25551;&#36848;&#38544;&#21547;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.11229</link><description>&lt;p&gt;
&#22312;&#31354;&#20013;&#36827;&#34892;&#25512;&#29702;&#65306;&#22522;&#20110;&#25512;&#29702;&#30340;&#38544;&#24335;&#35821;&#20041;&#24863;&#30693;&#36890;&#20449;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Reasoning over the Air: A Reasoning-based Implicit Semantic-Aware Communication Framework. (arXiv:2306.11229v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#24335;&#35821;&#20041;&#24863;&#30693;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#29702;&#30340;&#38544;&#24335;&#35821;&#20041;&#24863;&#30693;&#36890;&#20449;&#26694;&#26550;&#65288;iSAC&#65289;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#28304;&#29992;&#25143;&#21644;&#30446;&#26631;&#29992;&#25143;&#20043;&#38388;&#34920;&#31034;&#12289;&#36890;&#20449;&#21644;&#35299;&#37322;&#38544;&#24335;&#35821;&#20041;&#21547;&#20041;&#12290;&#36890;&#36807;&#25237;&#24433;&#31639;&#27861;&#65292;&#23558;&#26174;&#24335;&#35821;&#20041;&#36716;&#25442;&#20026;&#20302;&#32500;&#35821;&#20041;&#34920;&#31034;&#65292;&#26356;&#22909;&#22320;&#25551;&#36848;&#38544;&#21547;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#24863;&#30693;&#36890;&#20449;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21463;&#21040;&#20154;&#31867;&#36890;&#20449;&#30340;&#21551;&#21457;&#65292;&#19987;&#27880;&#20110;&#20256;&#36882;&#20449;&#24687;&#30340;&#21547;&#20041;&#12290;&#30001;&#20110;&#20854;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#65292;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#30340;&#28508;&#21147;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#20256;&#36755;&#21644;&#20256;&#36882;&#21487;&#20197;&#30452;&#25509;&#20174;&#28304;&#20449;&#21495;&#20013;&#35782;&#21035;&#30340;&#26174;&#24335;&#35821;&#20041;&#21547;&#20041;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#24335;&#35821;&#20041;&#24863;&#30693;&#36890;&#20449;&#65292;&#20854;&#20013;&#19981;&#33021;&#30452;&#25509;&#20174;&#28304;&#20449;&#21495;&#20013;&#35266;&#23519;&#21040;&#30340;&#38544;&#34255;&#20449;&#24687;&#24517;&#39035;&#30001;&#39044;&#26399;&#30340;&#29992;&#25143;&#35782;&#21035;&#21644;&#35299;&#37322;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#35821;&#20041;&#24863;&#30693;&#36890;&#20449;&#65288;iSAC&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#28304;&#29992;&#25143;&#21644;&#30446;&#26631;&#29992;&#25143;&#20043;&#38388;&#34920;&#31034;&#12289;&#36890;&#20449;&#21644;&#35299;&#37322;&#38544;&#24335;&#35821;&#20041;&#21547;&#20041;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#24433;&#30340;&#35821;&#20041;&#32534;&#30721;&#22120;&#65292;&#23558;&#26174;&#24335;&#35821;&#20041;&#30340;&#39640;&#32500;&#22270;&#24418;&#34920;&#31034;&#36716;&#25442;&#20026;&#20302;&#32500;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#38544;&#21547;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic-aware communication is a novel paradigm that draws inspiration from human communication focusing on the delivery of the meaning of messages. It has attracted significant interest recently due to its potential to improve the efficiency and reliability of communication and enhance users' QoE. Most existing works focus on transmitting and delivering the explicit semantic meaning that can be directly identified from the source signal. This paper investigates the implicit semantic-aware communication in which the hidden information that cannot be directly observed from the source signal must be recognized and interpreted by the intended users. To this end, a novel implicit semantic-aware communication (iSAC) architecture is proposed for representing, communicating, and interpreting the implicit semantic meaning between source and destination users. A projection-based semantic encoder is proposed to convert the high-dimensional graphical representation of explicit semantics into a l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#12289;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12289;&#22810;&#35821;&#35328;&#29702;&#35299;&#21644;&#22810;&#20219;&#21153;&#22788;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#29790;&#22763;&#27861;&#24459;&#31995;&#32479;&#30340;&#22810;&#26679;&#21270;&#27861;&#24459;NLP&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#36827;&#34892;&#23545;&#24213;&#23618;&#38750;&#33521;&#35821;&#12289;&#22266;&#26377;&#22810;&#35821;&#35328;&#30340;&#27861;&#24459;&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.09237</link><description>&lt;p&gt;
SCALE: &#25552;&#21319;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
SCALE: Scaling up the Complexity for Advanced Language Model Evaluation. (arXiv:2306.09237v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#12289;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12289;&#22810;&#35821;&#35328;&#29702;&#35299;&#21644;&#22810;&#20219;&#21153;&#22788;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#29790;&#22763;&#27861;&#24459;&#31995;&#32479;&#30340;&#22810;&#26679;&#21270;&#27861;&#24459;NLP&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#36827;&#34892;&#23545;&#24213;&#23618;&#38750;&#33521;&#35821;&#12289;&#22266;&#26377;&#22810;&#35821;&#35328;&#30340;&#27861;&#24459;&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#24050;&#32463;&#39281;&#21644;&#20102;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65288;&#21253;&#25324;&#19987;&#19994;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#65289;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#26032;&#39062;&#12289;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26469;&#27491;&#30830;&#35780;&#20272;LLM&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#23545;&#24403;&#21069;LLM&#30340;&#22235;&#20010;&#20851;&#38190;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#65306;&#22788;&#29702;&#38271;&#25991;&#26723;&#65288;&#22810;&#36798;50K&#20010;&#26631;&#35760;&#65289;&#12289;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65288;&#20307;&#29616;&#22312;&#27861;&#24459;&#25991;&#26412;&#20013;&#65289;&#12289;&#22810;&#35821;&#35328;&#29702;&#35299;&#65288;&#28085;&#30422;&#20116;&#31181;&#35821;&#35328;&#65289;&#21644;&#22810;&#20219;&#21153;&#22788;&#29702;&#65288;&#21253;&#25324;&#27861;&#24459;&#25991;&#20214;&#21040;&#25991;&#20214;&#20449;&#24687;&#26816;&#32034;&#12289;&#27861;&#24237;&#35270;&#22270;&#29983;&#25104;&#12289;&#37325;&#35201;&#20915;&#31574;&#25688;&#35201;&#12289;&#24341;&#29992;&#25552;&#21462;&#21644;&#20843;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65289;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#26469;&#33258;&#29790;&#22763;&#27861;&#24459;&#31995;&#32479;&#30340;&#22810;&#26679;&#30340;&#27861;&#24459;NLP&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23545;&#24213;&#23618;&#38750;&#33521;&#35821;&#12289;&#22266;&#26377;&#22810;&#35821;&#35328;&#30340;&#32852;&#37030;&#27861;&#24459;&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#24378;&#28872;&#30340;&#23457;&#26597;/&#20998;&#26512;&#20219;&#21153;&#65292;&#39640;&#25928;&#22320;&#22788;&#29702;&#38271;&#25991;&#26723;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent strides in Large Language Models (LLMs) have saturated many NLP benchmarks (even professional domain-specific ones), emphasizing the need for novel, more challenging novel ones to properly assess LLM capabilities. In this paper, we introduce a novel NLP benchmark that poses challenges to current LLMs across four key dimensions: processing long documents (up to 50K tokens), utilizing domain specific knowledge (embodied in legal texts), multilingual understanding (covering five languages), and multitasking (comprising legal document to document Information Retrieval, Court View Generation, Leading Decision Summarization, Citation Extraction, and eight challenging Text Classification tasks). Our benchmark comprises diverse legal NLP datasets from the Swiss legal system, allowing for a comprehensive study of the underlying Non-English, inherently multilingual, federal legal system. Despite recent advances, efficiently processing long documents for intense review/analysis tasks remai
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20132;&#24863;&#30693;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;(STUDY)&#65292;&#20351;&#29992;transformer&#35299;&#30721;&#22120;&#32593;&#32476;&#23454;&#29616;&#23545;&#31038;&#20132;&#32593;&#32476;&#22270;&#20013;&#30456;&#37051;&#30340;&#29992;&#25143;&#32452;&#30340;&#32852;&#21512;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#22312;&#25945;&#32946;&#20869;&#23481;&#39046;&#22495;&#20013;&#32463;&#36807;&#27979;&#35797;&#65292;&#33021;&#22815;&#21462;&#24471;&#20248;&#20110;&#31038;&#20132;&#21644;&#39034;&#24207;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07946</link><description>&lt;p&gt;
&#30740;&#31350;&#65306;&#31038;&#20132;&#24863;&#30693;&#26102;&#38388;&#26494;&#25955;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
STUDY: Socially Aware Temporally Casual Decoder Recommender Systems. (arXiv:2306.07946v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07946
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20132;&#24863;&#30693;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;(STUDY)&#65292;&#20351;&#29992;transformer&#35299;&#30721;&#22120;&#32593;&#32476;&#23454;&#29616;&#23545;&#31038;&#20132;&#32593;&#32476;&#22270;&#20013;&#30456;&#37051;&#30340;&#29992;&#25143;&#32452;&#30340;&#32852;&#21512;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#22312;&#25945;&#32946;&#20869;&#23481;&#39046;&#22495;&#20013;&#32463;&#36807;&#27979;&#35797;&#65292;&#33021;&#22815;&#21462;&#24471;&#20248;&#20110;&#31038;&#20132;&#21644;&#39034;&#24207;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#21487;&#33719;&#21462;&#30340;&#25968;&#25454;&#25968;&#37327;&#36807;&#20110;&#24222;&#22823;&#65292;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#24517;&#35201;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#31526;&#21512;&#20182;&#20204;&#20852;&#36259;&#30340;&#29289;&#21697;&#12290;&#24403;&#31038;&#20132;&#32593;&#32476;&#20449;&#24687;&#23384;&#22312;&#26102;&#65292;&#26377;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#20570;&#20986;&#26356;&#22909;&#30340;&#25512;&#33616;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#26377;&#22797;&#26434;&#30340;&#32467;&#26500;&#21644;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#36825;&#20123;&#32593;&#32476;&#35757;&#32451;&#36215;&#26469;&#38750;&#24120;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31038;&#20132;&#24863;&#30693;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;(STUDY)&#12290;STUDY&#37319;&#29992;&#19968;&#20010;&#32463;&#36807;&#20462;&#25913;&#30340;transformer&#35299;&#30721;&#22120;&#32593;&#32476;&#30340;&#21333;&#21521;&#21069;&#20256;&#65292;&#23545;&#31038;&#20132;&#32593;&#32476;&#22270;&#20013;&#30456;&#37051;&#30340;&#29992;&#25143;&#32452;&#36827;&#34892;&#32852;&#21512;&#25512;&#26029;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#23398;&#26657;&#35838;&#22530;&#32467;&#26500;&#23450;&#20041;&#31038;&#20132;&#32593;&#32476;&#30340;&#25945;&#32946;&#20869;&#23481;&#39046;&#22495;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#21333;&#19968;&#22343;&#21248;&#32593;&#32476;&#35774;&#35745;&#31616;&#21333;&#24615;&#30340;&#21516;&#26102;&#65292;&#20248;&#20110;&#31038;&#20132;&#21644;&#39034;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the overwhelming amount of data available both on and offline today, recommender systems have become much needed to help users find items tailored to their interests. When social network information exists there are methods that utilize this information to make better recommendations, however the methods are often clunky with complex architectures and training procedures. Furthermore many of the existing methods utilize graph neural networks which are notoriously difficult to train. To address this, we propose Socially-aware Temporally caUsal Decoder recommender sYstems (STUDY). STUDY does joint inference over groups of users who are adjacent in the social network graph using a single forward pass of a modified transformer decoder network. We test our method in a school-based educational content setting, using classroom structure to define social networks. Our method outperforms both social and sequential methods while maintaining the design simplicity of a single homogeneous netw
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.06777</link><description>&lt;p&gt;
&#25552;&#39640;&#20915;&#31574;&#26641;&#35299;&#37322;&#24615;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Validity of Decision Trees as Explanations. (arXiv:2306.06777v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06777
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20351;&#29992;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#12290;&#36825;&#21487;&#20197;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31454;&#20105;[&#21442;&#35265;Grinsztajn&#31561;&#20154;&#65292;NeurIPS 2022&#65292;arXiv&#65306;2207.08815]&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#21487;&#35299;&#37322;&#24615;&#21462;&#20915;&#20110;&#26641;&#30340;&#28145;&#24230;&#21644;&#27599;&#20010;&#21494;&#33410;&#28857;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20302;&#28145;&#24230;&#30340;&#26641;&#65292;&#20854;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#27599;&#20010;&#21494;&#33410;&#28857;&#19978;&#30340;&#26368;&#22823;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#20174;&#20302;&#28145;&#24230;&#26641;&#30340;&#27599;&#20010;&#21494;&#33410;&#28857;&#8220;&#25346;&#36215;&#8221;&#36827;&#19968;&#27493;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;&#26080;&#38480;&#28145;&#24230;&#30340;&#26641;&#65289;&#12290;&#20302;&#28145;&#24230;&#26641;&#26131;&#20110;&#35299;&#37322;&#65292;&#32780;&#32508;&#21512;&#20302;&#28145;&#24230;&#21644;&#25346;&#36215;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#30340;&#25972;&#20307;&#32479;&#35745;&#24615;&#33021;&#20248;&#20110;&#20351;&#29992;&#32463;&#20856;&#26041;&#27861;&#65288;&#20363;&#22914;CART&#65289;&#35757;&#32451;&#30340;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#20248;&#21270;&#30340;XGBoost&#65289;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classification and forecasting with tabular data, one often utilizes tree-based models. This can be competitive with deep neural networks on tabular data [cf. Grinsztajn et al., NeurIPS 2022, arXiv:2207.08815] and, under some conditions, explainable. The explainability depends on the depth of the tree and the accuracy in each leaf of the tree. Here, we train a low-depth tree with the objective of minimising the maximum misclassification error across each leaf node, and then ``suspend'' further tree-based models (e.g., trees of unlimited depth) from each leaf of the low-depth tree. The low-depth tree is easily explainable, while the overall statistical performance of the combined low-depth and suspended tree-based models improves upon decision trees of unlimited depth trained using classical methods (e.g., CART) and is comparable to state-of-the-art methods (e.g., well-tuned XGBoost).
&lt;/p&gt;</description></item><item><title>DocumentCLIP&#26159;&#19968;&#31181;&#26174;&#33879;&#24615;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#25991;&#26723;&#20869;&#38271;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#22810;&#27169;&#24577;&#25991;&#26723;&#20869;&#37096;&#38142;&#25509;&#26041;&#38754;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#20154;&#12290;</title><link>http://arxiv.org/abs/2306.06306</link><description>&lt;p&gt;
DocumentCLIP&#65306;&#38142;&#25509;&#25442;&#34892;&#25991;&#20214;&#20013;&#30340;&#22270;&#20687;&#21644;&#27491;&#25991;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
DocumentCLIP: Linking Figures and Main Body Text in Reflowed Documents. (arXiv:2306.06306v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06306
&lt;/p&gt;
&lt;p&gt;
DocumentCLIP&#26159;&#19968;&#31181;&#26174;&#33879;&#24615;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#25991;&#26723;&#20869;&#38271;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#22810;&#27169;&#24577;&#25991;&#26723;&#20869;&#37096;&#38142;&#25509;&#26041;&#38754;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#36807;&#29702;&#35299;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#23545;&#40784;&#32780;&#22312;&#25903;&#25345;&#22810;&#23186;&#20307;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#29702;&#35299;&#21333;&#20010;&#22270;&#20687;&#19982;&#19968;&#27573;&#25991;&#26412;&#30456;&#20851;&#32852;&#65292;&#32780;&#24448;&#24448;&#24573;&#30053;&#20102;&#22810;&#21477;&#35805;&#19982;&#22810;&#20010;&#22270;&#20687;&#32452;&#25104;&#30340;&#25991;&#26723;&#20869;&#37096;&#30340;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DocumentCLIP&#30340;&#26174;&#33879;&#24615;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#24378;&#21046;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#29702;&#35299;&#25991;&#26723;&#20869;&#38271;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#21161;&#20110;&#23545;&#26032;&#38395;&#25991;&#31456;&#12289;&#26434;&#24535;&#12289;&#20135;&#21697;&#25551;&#36848;&#31561;&#20855;&#26377;&#26356;&#20016;&#23500;&#35821;&#35328;&#21644;&#35270;&#35273;&#20869;&#23481;&#30340;&#30495;&#23454;&#19990;&#30028;&#22810;&#27169;&#24577;&#25991;&#26723;&#29702;&#35299;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25506;&#32034;&#22810;&#27169;&#24577;&#25991;&#26723;&#20869;&#37096;&#38142;&#25509;&#30340;&#20154;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;Wikipedia&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#22521;&#35757;&#65292;&#20026;&#35757;&#32451;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language pretraining models have achieved great success in supporting multimedia applications by understanding the alignments between images and text. While existing vision-language pretraining models primarily focus on understanding single image associated with a single piece of text, they often ignore the alignment at the intra-document level, consisting of multiple sentences with multiple images. In this work, we propose DocumentCLIP, a salience-aware contrastive learning framework to enforce vision-language pretraining models to comprehend the interaction between images and longer text within documents. Our model is beneficial for the real-world multimodal document understanding like news article, magazines, product descriptions, which contain linguistically and visually richer content. To the best of our knowledge, we are the first to explore multimodal intra-document links by contrastive learning. In addition, we collect a large Wikipedia dataset for pretraining, which pro
&lt;/p&gt;</description></item><item><title>AnoOnly&#26159;&#19968;&#20010;&#26032;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#24369;&#30417;&#30563;&#24418;&#24335;&#26469;&#35299;&#20915;&#21516;&#36136;&#25968;&#25454;&#23545;&#24322;&#24120;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#24179;&#34913;&#30340;&#30417;&#30563;&#12290;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18798</link><description>&lt;p&gt;
AnoOnly:&#26080;&#38656;&#25439;&#22833;&#27491;&#24120;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
AnoOnly: Semi-Supervised Anomaly Detection without Loss on Normal Data. (arXiv:2305.18798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18798
&lt;/p&gt;
&lt;p&gt;
AnoOnly&#26159;&#19968;&#20010;&#26032;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#24369;&#30417;&#30563;&#24418;&#24335;&#26469;&#35299;&#20915;&#21516;&#36136;&#25968;&#25454;&#23545;&#24322;&#24120;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#24179;&#34913;&#30340;&#30417;&#30563;&#12290;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;(SSAD)&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#23569;&#37327;&#20294;&#26377;&#25351;&#23548;&#20316;&#29992;&#30340;&#24322;&#24120;&#23454;&#20363;&#65292;&#22686;&#24378;&#20102;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;(UAD)&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#21516;&#36136;&#27491;&#24120;&#25968;&#25454;&#23545;&#24322;&#24120;&#30340;&#32479;&#27835;&#20351;&#24471;SSAD&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22320;&#24863;&#30693;&#24322;&#24120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#22312;&#20005;&#37325;&#19981;&#24179;&#34913;&#30340;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;AnoOnly(&#20165;&#24322;&#24120;)&#30340;&#26032;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#30340;SSAD&#26041;&#27861;&#19981;&#21516;&#65292;AnoOnly&#26242;&#20572;&#20102;&#20005;&#26684;&#30340;&#25439;&#22833;&#30417;&#30563;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#24369;&#30417;&#30563;&#24418;&#24335;&#12290;&#36825;&#31181;&#24369;&#30417;&#30563;&#36890;&#36807;&#25209;&#37327;&#24402;&#19968;&#21270;&#23454;&#29616;&#65292;&#38544;&#24335;&#22320;&#23545;&#27491;&#24120;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#23398;&#20064;&#12290;&#24403;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;SSAD&#26041;&#27861;&#20013;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;AnoOnly&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;A
&lt;/p&gt;
&lt;p&gt;
Semi-supervised anomaly detection (SSAD) methods have demonstrated their effectiveness in enhancing unsupervised anomaly detection (UAD) by leveraging few-shot but instructive abnormal instances. However, the dominance of homogeneous normal data over anomalies biases the SSAD models against effectively perceiving anomalies. To address this issue and achieve balanced supervision between heavily imbalanced normal and abnormal data, we develop a novel framework called AnoOnly (Anomaly Only). Unlike existing SSAD methods that resort to strict loss supervision, AnoOnly suspends it and introduces a form of weak supervision for normal data. This weak supervision is instantiated through the utilization of batch normalization, which implicitly performs cluster learning on normal data. When integrated into existing SSAD methods, the proposed AnoOnly demonstrates remarkable performance enhancements across various models and datasets, achieving new state-of-the-art performance. Additionally, our A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RL+&#27169;&#22411;&#25511;&#21046;&#26694;&#26550;&#20197;&#24320;&#21457;&#20986;&#21487;&#20197;&#26377;&#25928;&#21487;&#38752;&#22320;&#23398;&#20064;&#30340;&#20581;&#22766;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25972;&#21512;&#26377;&#38480;&#26102;&#38388;&#26368;&#20248;&#25511;&#21046;&#29983;&#25104;&#30340;&#25353;&#38656;&#21442;&#32771;&#36816;&#21160;&#20998;&#25955; RL &#36807;&#31243;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#24314;&#27169;&#31616;&#21270;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#65292;&#22312;&#36275;&#24335; locomotion &#19978;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#21644;&#24378;&#20581;&#65292;&#33021;&#27867;&#21270;&#21442;&#32771;&#36816;&#21160;&#24182;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.17842</link><description>&lt;p&gt;
RL+&#27169;&#22411;&#25511;&#21046;&#65306;&#20351;&#29992;&#25353;&#38656;&#26368;&#20248;&#25511;&#21046;&#23398;&#20064;&#22810;&#21151;&#33021;&#36275;&#24335; locomotion
&lt;/p&gt;
&lt;p&gt;
RL + Model-based Control: Using On-demand Optimal Control to Learn Versatile Legged Locomotion. (arXiv:2305.17842v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RL+&#27169;&#22411;&#25511;&#21046;&#26694;&#26550;&#20197;&#24320;&#21457;&#20986;&#21487;&#20197;&#26377;&#25928;&#21487;&#38752;&#22320;&#23398;&#20064;&#30340;&#20581;&#22766;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25972;&#21512;&#26377;&#38480;&#26102;&#38388;&#26368;&#20248;&#25511;&#21046;&#29983;&#25104;&#30340;&#25353;&#38656;&#21442;&#32771;&#36816;&#21160;&#20998;&#25955; RL &#36807;&#31243;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#24314;&#27169;&#31616;&#21270;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#65292;&#22312;&#36275;&#24335; locomotion &#19978;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#21644;&#24378;&#20581;&#65292;&#33021;&#27867;&#21270;&#21442;&#32771;&#36816;&#21160;&#24182;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#21644;&#24378;&#20581;&#30340;&#36275;&#24335; locomotion&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#26377;&#38480;&#26102;&#38388;&#26368;&#20248;&#25511;&#21046;&#29983;&#25104;&#30340;&#25353;&#38656;&#21442;&#32771;&#36816;&#21160;&#26469;&#22686;&#24378; RL &#35757;&#32451;&#36807;&#31243;&#65292;&#35206;&#30422;&#20102;&#24191;&#27867;&#30340;&#36895;&#24230;&#21644;&#27493;&#24577;&#12290;&#36825;&#20123;&#21442;&#32771;&#36816;&#21160;&#20316;&#20026; RL &#31574;&#30053;&#27169;&#20223;&#30340;&#30446;&#26631;&#65292;&#23548;&#33268;&#24320;&#21457;&#20986;&#21487;&#26377;&#25928;&#21487;&#38752;&#22320;&#23398;&#20064;&#30340;&#20581;&#22766;&#25511;&#21046;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32771;&#34385;&#20840;&#36523;&#21160;&#21147;&#23398;&#65292;RL &#20811;&#26381;&#20102;&#24314;&#27169;&#31616;&#21270;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#30828;&#20214;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; RL &#35757;&#32451;&#36807;&#31243;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#30340;&#24378;&#20581;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#27867;&#21270;&#21442;&#32771;&#36816;&#21160;&#21644;&#22788;&#29702;&#21487;&#33021;&#23545;&#31616;&#21270;&#27169;&#22411;&#26500;&#25104;&#25361;&#25112;&#30340;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#20102; RL &#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This letter presents a control framework that combines model-based optimal control and reinforcement learning (RL) to achieve versatile and robust legged locomotion. Our approach enhances the RL training process by incorporating on-demand reference motions generated through finite-horizon optimal control, covering a broad range of velocities and gaits. These reference motions serve as targets for the RL policy to imitate, resulting in the development of robust control policies that can be learned efficiently and reliably. Moreover, by considering whole-body dynamics, RL overcomes the inherent limitations of modelling simplifications. Through simulation and hardware experiments, we demonstrate the robustness and controllability of the RL training process within our framework. Furthermore, our method demonstrates the ability to generalize reference motions and handle more complex locomotion tasks that may pose challenges for the simplified model, leveraging the flexibility of RL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#36235;&#21183;&#21644;&#23395;&#33410;&#24615;&#20998;&#37327;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#29420;&#29305;&#24433;&#21709;&#25351;&#26631;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#32452;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992; LightGBM &#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#22312;&#27779;&#23572;&#29595;&#38144;&#21806;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17201</link><description>&lt;p&gt;
&#22522;&#20110; Trend &#21644; Seasonality &#20998;&#35299;&#21644; LightGBM &#30340;&#38144;&#21806;&#39044;&#27979;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Sales Forecasting using Trend and Seasonality Decomposition with LightGBM. (arXiv:2305.17201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#36235;&#21183;&#21644;&#23395;&#33410;&#24615;&#20998;&#37327;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#29420;&#29305;&#24433;&#21709;&#25351;&#26631;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#32452;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992; LightGBM &#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#22312;&#27779;&#23572;&#29595;&#38144;&#21806;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#27779;&#23572;&#29595;&#21644;&#20122;&#39532;&#36874;&#31561;&#22823;&#22411;&#38646;&#21806;&#21830;&#38144;&#21806;&#39044;&#27979;&#30340;&#38590;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#26681;&#25454;&#36235;&#21183;&#21644;&#23395;&#33410;&#24615;&#20998;&#37327;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#29420;&#29305;&#24433;&#21709;&#25351;&#26631;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#32452;&#65292;&#24182;&#37319;&#29992; LightGBM &#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#20998;&#32452;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;MAPE&#65288;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#65289;&#22312;&#27979;&#35797;&#38598;&#19978;&#21487;&#36798; 4.49%&#12290;
&lt;/p&gt;
&lt;p&gt;
Retail sales forecasting presents a significant challenge for large retailers such as Walmart and Amazon, due to the vast assortment of products, geographical location heterogeneity, seasonality, and external factors including weather, local economic conditions, and geopolitical events. Various methods have been employed to tackle this challenge, including traditional time series models, machine learning models, and neural network mechanisms, but the difficulty persists. Categorizing data into relevant groups has been shown to improve sales forecast accuracy as time series from different categories may exhibit distinct patterns. In this paper, we propose a new measure to indicate the unique impacts of the trend and seasonality components on a time series and suggest grouping time series based on this measure. We apply this approach to Walmart sales data from 01/29/2011 to 05/22/2016 and generate sales forecasts from 05/23/2016 to 06/19/2016. Our experiments show that the proposed strat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16460</link><description>&lt;p&gt;
&#20248;&#21270;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#29992;&#20110;&#39640;&#25928;&#26816;&#27979;&#27700;&#19979;&#22403;&#22334;
&lt;/p&gt;
&lt;p&gt;
Optimized Custom Dataset for Efficient Detection of Underwater Trash. (arXiv:2305.16460v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#21644;&#28165;&#38500;&#28508;&#22312;&#30340;&#27700;&#19979;&#24223;&#29289;&#23545;&#20110;&#20445;&#25252;&#28023;&#27915;&#29983;&#29289;&#21644;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#38024;&#23545;&#27700;&#19979;&#22403;&#22334;&#26816;&#27979;&#25152;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22914;&#20809;&#25240;&#23556;&#12289;&#21560;&#25910;&#12289;&#24748;&#28014;&#39063;&#31890;&#21644;&#33394;&#24425;&#25197;&#26354;&#31561;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#22810;&#31181;&#27700;&#19979;&#29615;&#22659;&#65292;&#24182;&#21253;&#25324;&#23545;&#24223;&#24323;&#29289;&#23454;&#20363;&#30340;&#31934;&#30830;&#23450;&#20301;&#26631;&#27880;&#12290;&#26368;&#32456;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65292;&#30446;&#30340;&#26159;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately quantifying and removing submerged underwater waste plays a crucial role in safeguarding marine life and preserving the environment. While detecting floating and surface debris is relatively straightforward, quantifying submerged waste presents significant challenges due to factors like light refraction, absorption, suspended particles, and color distortion. This paper addresses these challenges by proposing the development of a custom dataset and an efficient detection approach for submerged marine debris. The dataset encompasses diverse underwater environments and incorporates annotations for precise labeling of debris instances. Ultimately, the primary objective of this custom dataset is to enhance the diversity of litter instances and improve their detection accuracy in deep submerged environments by leveraging state-of-the-art deep learning architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;&#22810;&#23610;&#24230; KPPillarsBEV&#65292;&#20197;&#32531;&#35299;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#20013;&#20174;&#28857;&#20113;&#25968;&#25454;&#36716;&#21270;&#20026;&#32593;&#26684;&#32467;&#26500;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861; KPBEV&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15836</link><description>&lt;p&gt;
&#29992;&#20110;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#30340;&#28857;&#20113;&#22810;&#23610;&#24230;&#32593;&#26684;&#28210;&#26579;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Multi-Scale Grid Rendering of Point Clouds for Radar Object Detection Networks. (arXiv:2305.15836v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;&#22810;&#23610;&#24230; KPPillarsBEV&#65292;&#20197;&#32531;&#35299;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#20013;&#20174;&#28857;&#20113;&#25968;&#25454;&#36716;&#21270;&#20026;&#32593;&#26684;&#32467;&#26500;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861; KPBEV&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#32593;&#26684;&#34920;&#24449;&#65292;&#28982;&#21518;&#24212;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21487;&#29992;&#20110;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#65292;&#20294;&#20174;&#19981;&#35268;&#21017;&#30340;&#28857;&#20113;&#25968;&#25454;&#36716;&#25442;&#20026;&#23494;&#38598;&#30340;&#32593;&#26684;&#32467;&#26500;&#24120;&#24120;&#20250;&#23548;&#33268;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#36825;&#26159;&#30001;&#20110;&#28857;&#30340;&#31163;&#25955;&#21270;&#21644;&#32858;&#21512;&#36896;&#25104;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;&#22810;&#23610;&#24230; KPPillarsBEV&#65292;&#20197;&#32531;&#35299;&#32593;&#26684;&#28210;&#26579;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861; KPBEV&#65292;&#23427;&#21033;&#29992;&#26680;&#28857;&#21367;&#31215;&#30340;&#25551;&#36848;&#33021;&#21147;&#26469;&#25913;&#36827;&#32593;&#26684;&#28210;&#26579;&#36807;&#31243;&#20013;&#23616;&#37096;&#28857;&#20113;&#19978;&#19979;&#25991;&#30340;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22810;&#23610;&#24230;&#32593;&#26684;&#28210;&#26579;&#20844;&#24335;&#65292;&#20197;&#20219;&#24847;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861;&#23558;&#22810;&#23610;&#24230;&#29305;&#24449;&#26144;&#23556;&#34701;&#21512;&#21040;&#26816;&#27979;&#32593;&#32476;&#30340;&#21367;&#31215;&#39592;&#24178;&#20013;&#12290;&#25105;&#20204;&#22312; nuScenes &#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35780;&#20272;&#20102;&#26816;&#27979;&#27773;&#36710;&#12289;&#21345;&#36710;&#21644;&#20844;&#20132;&#36710;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#23610;&#24230; KPPillarsBEV &#32467;&#26500;&#21644; KPBEV &#32593;&#26684;&#28210;&#26579;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Architectures that first convert point clouds to a grid representation and then apply convolutional neural networks achieve good performance for radar-based object detection. However, the transfer from irregular point cloud data to a dense grid structure is often associated with a loss of information, due to the discretization and aggregation of points. In this paper, we propose a novel architecture, multi-scale KPPillarsBEV, that aims to mitigate the negative effects of grid rendering. Specifically, we propose a novel grid rendering method, KPBEV, which leverages the descriptive power of kernel point convolutions to improve the encoding of local point cloud contexts during grid rendering. In addition, we propose a general multi-scale grid rendering formulation to incorporate multi-scale feature maps into convolutional backbones of detection networks with arbitrary grid rendering methods. We perform extensive experiments on the nuScenes dataset and evaluate the methods in terms of dete
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#20179;&#24211;&#24067;&#23616;&#65292;&#21487;&#20197;&#20943;&#23569;&#25317;&#22581;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#24182;&#25193;&#22823;&#33258;&#21160;&#21270;&#20179;&#24211;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06436</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20179;&#20648;&#30340;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#21644;&#24067;&#23616;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-Robot Coordination and Layout Design for Automated Warehousing. (arXiv:2305.06436v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06436
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#20179;&#24211;&#24067;&#23616;&#65292;&#21487;&#20197;&#20943;&#23569;&#25317;&#22581;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#24182;&#25193;&#22823;&#33258;&#21160;&#21270;&#20179;&#24211;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#23558;MAPF&#31639;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#33258;&#21160;&#21270;&#20179;&#24211;&#20013;&#65292;&#20197;&#21327;&#35843;&#25968;&#30334;&#20010;&#26426;&#22120;&#20154;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#26356;&#22909;&#30340;MAPF&#31639;&#27861;&#26469;&#25552;&#39640;&#20179;&#24211;&#30340;&#21534;&#21520;&#37327;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#20248;&#21270;&#20179;&#24211;&#24067;&#23616;&#26469;&#25552;&#39640;&#20854;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;MAPF&#31639;&#27861;&#65292;&#36890;&#24120;&#20351;&#29992;&#30340;&#20154;&#24037;&#35774;&#35745;&#24067;&#23616;&#20063;&#21487;&#33021;&#23548;&#33268;&#20179;&#24211;&#30340;&#25317;&#22581;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#33258;&#21160;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#20197;&#20248;&#21270;&#20179;&#24211;&#24067;&#23616;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#20248;&#21270;&#21518;&#30340;&#20179;&#24211;&#24067;&#23616;(1)&#20943;&#23569;&#20102;&#20132;&#36890;&#25317;&#22581;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#65292;(2)&#36890;&#36807;&#21152;&#20493;&#26426;&#22120;&#20154;&#25968;&#37327;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#33258;&#21160;&#21270;&#20179;&#24211;&#30340;&#21487;&#20280;&#32553;&#24615;&#65292;(3)&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#29992;&#25143;&#25351;&#23450;&#22810;&#26679;&#24615;&#25351;&#26631;&#30340;&#24067;&#23616;&#12290;&#28304;&#20195;&#30721;&#20301;&#20110;&#65306;\url{https://github.com/lun}
&lt;/p&gt;
&lt;p&gt;
With the rapid progress in Multi-Agent Path Finding (MAPF), researchers have studied how MAPF algorithms can be deployed to coordinate hundreds of robots in large automated warehouses. While most works try to improve the throughput of such warehouses by developing better MAPF algorithms, we focus on improving the throughput by optimizing the warehouse layout. We show that, even with state-of-the-art MAPF algorithms, commonly used human-designed layouts can lead to congestion for warehouses with large numbers of robots and thus have limited scalability. We extend existing automatic scenario generation methods to optimize warehouse layouts. Results show that our optimized warehouse layouts (1) reduce traffic congestion and thus improve throughput, (2) improve the scalability of the automated warehouses by doubling the number of robots in some cases, and (3) are capable of generating layouts with user-specified diversity measures. We include the source code at: \url{https://github.com/lun
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21010;&#20998;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;PMPO&#65289;&#26041;&#27861;&#65292;&#23558;&#36719;&#25552;&#31034;&#20174;&#19968;&#20010;&#25193;&#23637;&#21040;&#22810;&#20010;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#20010;&#25552;&#31034;&#21040;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#19981;&#21516;&#28145;&#24230;&#19978;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#35270;&#35273;&#34920;&#31034;&#30340;&#19978;&#19979;&#25991;&#28145;&#24230;&#65292;&#19982;&#20256;&#32479;&#21333;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#19979;&#28216;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06221</link><description>&lt;p&gt;
&#24102;&#28145;&#24230;&#21010;&#20998;&#30340;&#22810;&#25552;&#31034;&#27169;&#24577;&#20132;&#21449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Prompt with Depth Partitioned Cross-Modal Learning. (arXiv:2305.06221v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21010;&#20998;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;PMPO&#65289;&#26041;&#27861;&#65292;&#23558;&#36719;&#25552;&#31034;&#20174;&#19968;&#20010;&#25193;&#23637;&#21040;&#22810;&#20010;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#20010;&#25552;&#31034;&#21040;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#19981;&#21516;&#28145;&#24230;&#19978;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#35270;&#35273;&#34920;&#31034;&#30340;&#19978;&#19979;&#25991;&#28145;&#24230;&#65292;&#19982;&#20256;&#32479;&#21333;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#19979;&#28216;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36719;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#24494;&#35843;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23436;&#25104;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#21487;&#23398;&#20064;&#30340;&#25991;&#26412;&#26631;&#35760;&#19982;&#31867;&#21035;&#26631;&#35760;&#32452;&#21512;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#21442;&#25968;&#34987;&#20923;&#32467;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#20351;&#29992;&#21333;&#19968;&#25552;&#31034;&#26469;&#25551;&#36848;&#31867;&#21035;&#19978;&#19979;&#25991;&#65292;&#32780;&#19981;&#33021;&#20805;&#20998;&#25429;&#25417;&#31867;&#21035;&#30340;&#22810;&#26679;&#23646;&#24615;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#21010;&#20998;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;PMPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#25552;&#31034;&#25216;&#26415;&#65292;&#23558;&#36719;&#25552;&#31034;&#20174;&#19968;&#20010;&#21487;&#23398;&#20064;&#25552;&#31034;&#25193;&#23637;&#21040;&#22810;&#20010;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#28145;&#24230;&#36827;&#34892;&#20998;&#21106;&#65292;&#24182;&#23558;&#21487;&#23398;&#20064;&#25552;&#31034;&#36830;&#25509;&#21040;&#20998;&#31163;&#30340;&#35270;&#35273;&#28145;&#24230;&#19978;&#65292;&#20351;&#19981;&#21516;&#25552;&#31034;&#33021;&#22815;&#25429;&#25417;&#35270;&#35273;&#34920;&#31034;&#30340;&#23618;&#27425;&#19978;&#19979;&#25991;&#28145;&#24230;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22810;&#25552;&#31034;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#26495;&#21644;&#21487;&#23398;&#20064;&#30340;&#22810;&#25552;&#31034;&#30340;&#20808;&#39564;&#20449;&#24687;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, soft prompt learning methods have been proposed to fine-tune large-scale vision-language pre-trained models for various downstream tasks. These methods typically combine learnable textual tokens with class tokens as input for models with frozen parameters. However, they often employ a single prompt to describe class contexts, failing to capture categories' diverse attributes adequately. This study introduces the Partitioned Multi-modal Prompt (PMPO), a multi-modal prompting technique that extends the soft prompt from a single learnable prompt to multiple prompts. Our method divides the visual encoder depths and connects learnable prompts to the separated visual depths, enabling different prompts to capture the hierarchical contextual depths of visual representations. Furthermore, to maximize the advantages of multi-prompt learning, we incorporate prior information from manually designed templates and learnable multi-prompts, thus improving the generalization capabiliti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#22823;&#35268;&#27169;&#32844;&#20301;&#21457;&#24067;&#25968;&#25454;&#21450;&#22522;&#20110;&#32844;&#19994;&#30693;&#35782;&#22270;&#35889;&#24320;&#21457;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#39044;&#27979;&#20102;ChatGPT&#23545;&#26410;&#26469;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#30446;&#21069;&#32422;28&#65285;&#30340;&#32844;&#19994;&#38656;&#35201;ChatGPT&#30456;&#20851;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09823</link><description>&lt;p&gt;
ChatGPT&#21551;&#29992;&#30340;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#26410;&#26469;&#65306;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Future of ChatGPT-enabled Labor Market: A Preliminary Study. (arXiv:2304.09823v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#22823;&#35268;&#27169;&#32844;&#20301;&#21457;&#24067;&#25968;&#25454;&#21450;&#22522;&#20110;&#32844;&#19994;&#30693;&#35782;&#22270;&#35889;&#24320;&#21457;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#39044;&#27979;&#20102;ChatGPT&#23545;&#26410;&#26469;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#30446;&#21069;&#32422;28&#65285;&#30340;&#32844;&#19994;&#38656;&#35201;ChatGPT&#30456;&#20851;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#38750;&#20961;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;ChatGPT&#22312;&#21508;&#31181;&#29616;&#23454;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#25104;&#21151;&#24182;&#36234;&#26469;&#36234;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#21644;&#24037;&#20316;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#20063;&#25552;&#20986;&#20102;&#24191;&#27867;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;ChatGPT&#26679;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#26159;&#21542;&#20250;&#21462;&#20195;&#20154;&#31867;&#24037;&#20316;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#32780;&#19981;&#26159;&#23545;&#31435;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;ChatGPT&#21551;&#29992;&#30340;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#26410;&#26469;&#30340;&#21021;&#27493;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#20013;&#22269;&#26368;&#22823;&#30340;&#22312;&#32447;&#25307;&#32856;&#24179;&#21488;BOSS&#30452;&#32856;&#20013;&#30340;&#22823;&#35268;&#27169;&#32844;&#20301;&#21457;&#24067;&#25968;&#25454;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#21171;&#21160;&#21147;&#24066;&#22330;&#32422;&#26377;28&#65285;&#30340;&#32844;&#19994;&#38656;&#35201;ChatGPT&#30456;&#20851;&#25216;&#33021;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#20110;&#32844;&#19994;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#20197;&#39044;&#27979;&#26410;&#26469;&#32844;&#19994;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a phenomenal large language model, ChatGPT has achieved unparalleled success in various real-world tasks and increasingly plays an important role in our daily lives and work. However, extensive concerns are also raised about the potential ethical issues, especially about whether ChatGPT-like artificial general intelligence (AGI) will replace human jobs. To this end, in this paper, we introduce a preliminary data-driven study on the future of ChatGPT-enabled labor market from the view of Human-AI Symbiosis instead of Human-AI Confrontation. To be specific, we first conduct an in-depth analysis of large-scale job posting data in BOSS Zhipin, the largest online recruitment platform in China. The results indicate that about 28% of occupations in the current labor market require ChatGPT-related skills. Furthermore, based on a large-scale occupation-centered knowledge graph, we develop a semantic information enhanced collaborative filtering algorithm to predict the future occupation-skill
&lt;/p&gt;</description></item><item><title>TTIG&#27169;&#22411;&#26159;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#34917;&#20805;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19987;&#19994;&#20154;&#22763;&#23545;&#20110;TTIG&#24212;&#29992;&#21644;&#35748;&#30693;&#31561;&#26041;&#38754;&#23384;&#22312;12&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#25903;&#25345;TTIG&#30340;&#21487;&#25345;&#32493;&#37319;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.12601</link><description>&lt;p&gt;
"&#19968;&#31181;&#36866;&#24212;&#25110;&#28781;&#20129;&#30340;&#23616;&#38754;": &#28216;&#25103;&#34892;&#19994;&#19987;&#19994;&#20154;&#22763;&#23545;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#20154;&#24037;&#26234;&#33021;&#30340;&#24863;&#30693;&#12289;&#37319;&#29992;&#21644;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
"An Adapt-or-Die Type of Situation": Perception, Adoption, and Use of Text-To-Image-Generation AI by Game Industry Professionals. (arXiv:2302.12601v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12601
&lt;/p&gt;
&lt;p&gt;
TTIG&#27169;&#22411;&#26159;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#34917;&#20805;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19987;&#19994;&#20154;&#22763;&#23545;&#20110;TTIG&#24212;&#29992;&#21644;&#35748;&#30693;&#31561;&#26041;&#38754;&#23384;&#22312;12&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#25903;&#25345;TTIG&#30340;&#21487;&#25345;&#32493;&#37319;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;(TTIG)&#27169;&#22411;&#26159;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#34917;&#20805;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#65292;&#24320;&#22987;&#19982;&#19987;&#19994;&#21019;&#20316;&#32773;&#30340;&#20316;&#21697;&#31454;&#20105;&#24182;&#24341;&#21457;&#20102;&#26377;&#20851;&#21019;&#20316;&#24037;&#20316;&#12289;&#22833;&#19994;&#21644;&#29256;&#26435;&#31561;&#37325;&#35201;&#24433;&#21709;&#30340;&#35752;&#35770;&#12290;&#20026;&#20102;&#25903;&#25345;TTIG&#30340;&#21487;&#25345;&#32493;&#37319;&#29992;&#65292;&#25105;&#20204;&#24517;&#39035;&#25552;&#20379;&#19987;&#19994;&#20154;&#22763;&#24863;&#30693;&#12289;&#37319;&#29992;&#21644;&#20351;&#29992;TTIG&#30340;&#20016;&#23500;&#12289;&#21487;&#38752;&#21644;&#36879;&#26126;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#20844;&#20849;&#36777;&#35770;&#27973;&#34180;&#12289;&#29421;&#31364;&#19988;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#23398;&#26415;&#24037;&#20316;&#21017;&#38598;&#20013;&#20110;&#30740;&#31350;TTIG&#22312;&#19968;&#33324;&#33402;&#26415;&#23478;&#20154;&#32676;&#20013;&#30340;&#20351;&#29992;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#29305;&#23450;&#34892;&#19994;&#30340;&#19987;&#19994;&#20154;&#22763;&#30340;&#24863;&#30693;&#21644;&#24577;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#33452;&#20848;&#28216;&#25103;&#34892;&#19994;&#36827;&#34892;&#20102;&#19968;&#39033;&#23450;&#24615;&#30340;&#25506;&#32034;&#24615;&#35775;&#35848;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;TTIG&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;14&#20010;&#28216;&#25103;&#19987;&#19994;&#20154;&#22763;&#36827;&#34892;&#30340;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#30340;&#27169;&#26495;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;12&#20010;&#24635;&#20307;&#20027;&#39064;&#65292;&#32467;&#26500;&#21270;&#25104;49&#20010;&#23376;&#20027;&#39064;&#65292;&#25506;&#35752;&#20102;TTIG&#30340;&#24212;&#29992;&#21644;&#35748;&#30693;&#31561;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generation (TTIG) models, a recent addition to creative AI, can generate images based on a text description. These models have begun to rival the work of professional creatives, and sparked discussions on the future of creative work, loss of jobs, and copyright issues, amongst other important implications. To support the sustainable adoption of TTIG, we must provide rich, reliable and transparent insights into how professionals perceive, adopt and use TTIG. Crucially though, the public debate is shallow, narrow and lacking transparency, while academic work has focused on studying the use of TTIG in a general artist population, but not on the perceptions and attitudes of professionals in a specific industry. In this paper, we contribute a qualitative, exploratory interview study on TTIG in the Finnish videogame industry. Through a Template Analysis on semi-structured interviews with 14 game professionals, we reveal 12 overarching themes, structured into 49 sub-themes on pr
&lt;/p&gt;</description></item><item><title>ControlNet&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#20026;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#28155;&#21152;&#26465;&#20214;&#25511;&#21046;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#32534;&#30721;&#23618;&#23398;&#20064;&#22810;&#26679;&#30340;&#26465;&#20214;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#22312;&#25511;&#21046;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.05543</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#28155;&#21152;&#26465;&#20214;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Adding Conditional Control to Text-to-Image Diffusion Models. (arXiv:2302.05543v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05543
&lt;/p&gt;
&lt;p&gt;
ControlNet&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#20026;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#28155;&#21152;&#26465;&#20214;&#25511;&#21046;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#32534;&#30721;&#23618;&#23398;&#20064;&#22810;&#26679;&#30340;&#26465;&#20214;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#22312;&#25511;&#21046;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ControlNet&#65292;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#20026;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#28155;&#21152;&#31354;&#38388;&#26465;&#20214;&#25511;&#21046;&#12290;ControlNet&#38145;&#23450;&#20102;&#29983;&#20135;&#23601;&#32490;&#30340;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#37325;&#22797;&#20351;&#29992;&#23427;&#20204;&#20197;&#25968;&#21313;&#20159;&#24352;&#22270;&#20687;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#21644;&#31283;&#20581;&#30340;&#32534;&#30721;&#23618;&#20316;&#20026;&#24378;&#22823;&#30340;&#39592;&#24178;&#65292;&#20174;&#32780;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#26465;&#20214;&#25511;&#21046;&#12290;&#35813;&#31070;&#32463;&#26550;&#26500;&#19982;&#8220;&#38646;&#21367;&#31215;&#8221;&#65288;&#38646;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#23618;&#65289;&#36830;&#25509;&#65292;&#20174;&#38646;&#24320;&#22987;&#36880;&#28176;&#22686;&#21152;&#21442;&#25968;&#65292;&#24182;&#30830;&#20445;&#27809;&#26377;&#26377;&#23475;&#30340;&#22122;&#22768;&#24433;&#21709;&#24494;&#35843;&#12290;&#25105;&#20204;&#20351;&#29992;&#21333;&#20010;&#25110;&#22810;&#20010;&#26465;&#20214;&#36827;&#34892;&#31283;&#23450;&#25193;&#25955;&#27979;&#35797;&#20102;&#21508;&#31181;&#26465;&#20214;&#25511;&#21046;&#65292;&#20363;&#22914;&#36793;&#32536;&#12289;&#28145;&#24230;&#12289;&#20998;&#21106;&#12289;&#20154;&#20307;&#23039;&#21183;&#31561;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25110;&#27809;&#26377;&#25552;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ControlNets&#30340;&#35757;&#32451;&#23545;&#20110;&#23567;&#65288;&lt;50k&#65289;&#21644;&#22823;&#65288;&gt;1m&#65289;&#25968;&#25454;&#38598;&#26159;&#40065;&#26834;&#30340;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ControlNet&#21487;&#20197;&#20419;&#36827;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#20197;&#25511;&#21046;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with "zero convolutions" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (&lt;50k) and large (&gt;1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#65288;TPE-VMD-TFT&#65289;&#65292;&#29992;&#20110;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.01222</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#30340;&#20013;&#26399;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A novel framework for medium-term wind power prediction based on temporal attention mechanisms. (arXiv:2302.01222v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#65288;TPE-VMD-TFT&#65289;&#65292;&#29992;&#20110;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#33021;&#26159;&#19968;&#31181;&#24191;&#27867;&#20998;&#24067;&#12289;&#21487;&#20877;&#29983;&#21644;&#29615;&#20445;&#30340;&#33021;&#28304;&#65292;&#23545;&#32531;&#35299;&#20840;&#29699;&#21464;&#26262;&#21644;&#33021;&#28304;&#30701;&#32570;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#19981;&#30830;&#23450;&#24615;&#21644;&#27874;&#21160;&#24615;&#65292;&#22823;&#35268;&#27169;&#39118;&#30005;&#31995;&#32479;&#30340;&#32593;&#26684;&#38598;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20013;&#26399;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#21487;&#20197;&#20026;&#33021;&#37327;&#35843;&#24230;&#25552;&#20379;&#22522;&#26412;&#20381;&#25454;&#65292;&#22240;&#27492;&#31934;&#30830;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#21464;&#20998;&#27169;&#24335;&#20998;&#35299;&#65288;VMD&#65289;&#21644;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#65288;TFT&#65289;&#23450;&#20041;&#20102;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#30340;TPE-VMD-TFT&#26041;&#27861;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wind energy is a widely distributed, recyclable and environmentally friendly energy source that plays an important role in mitigating global warming and energy shortages. Wind energy's uncertainty and fluctuating nature makes grid integration of large-scale wind energy systems challenging. Medium-term wind power forecasts can provide an essential basis for energy dispatch, so accurate wind power forecasts are essential. Much research has yielded excellent results in recent years. However, many of them require additional experimentation and analysis when applied to other data. In this paper, we propose a novel short-term forecasting framework by tree-structured parzen estimator (TPE) and decomposition algorithms. This framework defines the TPE-VMD-TFT method for 24-h and 48-h ahead wind power forecasting based on variational mode decomposition (VMD) and time fusion transformer (TFT). In the Engie wind dataset from the electricity company in France, the results show that the proposed met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#26512;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#21453;&#21521;&#20256;&#25773;&#20013;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29983;&#25104;&#39640;&#25928;&#21487;&#35299;&#26512;&#30340;&#21453;&#21521;&#20256;&#25773;&#20248;&#21270;&#27169;&#22411;&#30340;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#36890;&#36807;&#20248;&#21270;&#26144;&#23556;&#32479;&#19968;&#23637;&#24320;&#21644;&#35299;&#26512;&#24494;&#20998;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2301.12047</link><description>&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#23637;&#24320;&#30340;&#25240;&#21472;&#20248;&#21270;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Backpropagation of Unrolled Solvers with Folded Optimization. (arXiv:2301.12047v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#26512;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#21453;&#21521;&#20256;&#25773;&#20013;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29983;&#25104;&#39640;&#25928;&#21487;&#35299;&#26512;&#30340;&#21453;&#21521;&#20256;&#25773;&#20248;&#21270;&#27169;&#22411;&#30340;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#36890;&#36807;&#20248;&#21270;&#26144;&#23556;&#32479;&#19968;&#23637;&#24320;&#21644;&#35299;&#26512;&#24494;&#20998;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#23558;&#32422;&#26463;&#20248;&#21270;&#27169;&#22411;&#20316;&#20026;&#32452;&#20214;&#38598;&#25104;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#19987;&#38376;&#30340;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26469;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#65292;&#32780;&#35813;&#35299;&#36890;&#24120;&#32570;&#20047;&#38381;&#21512;&#30340;&#24418;&#24335;&#12290;&#19968;&#31181;&#20856;&#22411;&#30340;&#31574;&#30053;&#26159;&#31639;&#27861;&#23637;&#24320;&#65292;&#23427;&#20381;&#36182;&#20110;&#36845;&#20195;&#27714;&#35299;&#22120;&#30340;&#33258;&#21160;&#24494;&#20998;&#25805;&#20316;&#12290;&#34429;&#28982;&#28789;&#27963;&#19988;&#36890;&#29992;&#65292;&#20294;&#22312;&#23454;&#38469;&#20013;&#65292;&#23637;&#24320;&#21487;&#33021;&#36935;&#21040;&#31934;&#24230;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;&#36890;&#36807;&#20248;&#21270;&#30340;&#35299;&#26512;&#24494;&#20998;&#21487;&#20197;&#36991;&#20813;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#24403;&#21069;&#30340;&#26694;&#26550;&#23545;&#20110;&#20248;&#21270;&#38382;&#39064;&#30340;&#24418;&#24335;&#26045;&#21152;&#20102;&#20005;&#26684;&#30340;&#35201;&#27714;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#23637;&#24320;&#20248;&#21270;&#21518;&#21521;&#20256;&#36882;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#39640;&#25928;&#21487;&#35299;&#26512;&#30340;&#21453;&#21521;&#20256;&#25773;&#20248;&#21270;&#27169;&#22411;&#30340;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#36890;&#36807;&#20248;&#21270;&#26144;&#23556;&#32479;&#19968;&#23637;&#24320;&#21644;&#35299;&#26512;&#24494;&#20998;&#30340;&#35270;&#35282;&#12290;&#22312;&#23454;&#39564;&#19978;&#36827;&#34892;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
The integration of constrained optimization models as components in deep networks has led to promising advances on many specialized learning tasks. A central challenge in this setting is backpropagation through the solution of an optimization problem, which typically lacks a closed form. One typical strategy is algorithm unrolling, which relies on automatic differentiation through the operations of an iterative solver. While flexible and general, unrolling can encounter accuracy and efficiency issues in practice. These issues can be avoided by analytical differentiation of the optimization, but current frameworks impose rigid requirements on the optimization problem's form. This paper provides theoretical insights into the backward pass of unrolled optimization, leading to a system for generating efficiently solvable analytical models of backpropagation. Additionally, it proposes a unifying view of unrolling and analytical differentiation through optimization mappings. Experiments over
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;SPTS v2&#26694;&#26550;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#21487;&#20197;&#20351;&#29992;&#26497;&#20302;&#25104;&#26412;&#30340;&#21333;&#28857;&#27880;&#37322;&#26469;&#35757;&#32451;&#22330;&#26223;&#25991;&#26412;&#23450;&#20301;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33258;&#22238;&#24402;Transformer&#19982;&#23454;&#20363;&#20998;&#37197;&#35299;&#30721;&#22120;&#65288;IAD&#65289;&#30340;&#20248;&#21183;&#65292;&#24182;&#20351;&#29992;&#24182;&#34892;&#35782;&#21035;&#35299;&#30721;&#22120;&#65288;PRD&#65289;&#36827;&#34892;&#25991;&#26412;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2301.01635</link><description>&lt;p&gt;
SPTS v2: &#21333;&#28857;&#22330;&#26223;&#25991;&#26412;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
SPTS v2: Single-Point Scene Text Spotting. (arXiv:2301.01635v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;SPTS v2&#26694;&#26550;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#21487;&#20197;&#20351;&#29992;&#26497;&#20302;&#25104;&#26412;&#30340;&#21333;&#28857;&#27880;&#37322;&#26469;&#35757;&#32451;&#22330;&#26223;&#25991;&#26412;&#23450;&#20301;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33258;&#22238;&#24402;Transformer&#19982;&#23454;&#20363;&#20998;&#37197;&#35299;&#30721;&#22120;&#65288;IAD&#65289;&#30340;&#20248;&#21183;&#65292;&#24182;&#20351;&#29992;&#24182;&#34892;&#35782;&#21035;&#35299;&#30721;&#22120;&#65288;PRD&#65289;&#36827;&#34892;&#25991;&#26412;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#20043;&#38388;&#30340;&#20869;&#22312;&#21327;&#21516;&#20316;&#29992;&#65292;&#31471;&#21040;&#31471;&#30340;&#22330;&#26223;&#25991;&#26412;&#23450;&#20301;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#25163;&#21160;&#26631;&#27880;&#65288;&#22914;&#27700;&#24179;&#30697;&#24418;&#12289;&#26059;&#36716;&#30697;&#24418;&#12289;&#22235;&#36793;&#24418;&#21644;&#22810;&#36793;&#24418;&#65289;&#35270;&#20026;&#24517;&#35201;&#26465;&#20214;&#65292;&#32780;&#36825;&#27604;&#20351;&#29992;&#21333;&#28857;&#35201;&#26114;&#36149;&#24471;&#22810;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#36890;&#36807;&#25552;&#20986;&#30340;&#21517;&#20026;SPTS v2&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#26497;&#20302;&#25104;&#26412;&#30340;&#21333;&#28857;&#27880;&#37322;&#26469;&#35757;&#32451;&#22330;&#26223;&#25991;&#26412;&#23450;&#20301;&#27169;&#22411;&#12290;SPTS v2&#36890;&#36807;&#20197;&#39034;&#24207;&#39044;&#27979;&#21516;&#19968;&#39044;&#27979;&#24207;&#21015;&#20013;&#25152;&#26377;&#25991;&#26412;&#23454;&#20363;&#30340;&#20013;&#24515;&#28857;&#65292;&#20445;&#30041;&#20102;&#33258;&#22238;&#24402;Transformer&#19982;&#23454;&#20363;&#20998;&#37197;&#35299;&#30721;&#22120;&#65288;IAD&#65289;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20351;&#29992;&#24182;&#34892;&#35782;&#21035;&#35299;&#30721;&#22120;&#65288;PRD&#65289;&#36827;&#34892;&#25991;&#26412;&#35782;&#21035;&#12290;&#36825;&#20004;&#20010;&#35299;&#30721;&#22120;&#20849;&#20139;&#30456;&#21516;&#30340;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20449;&#24687;&#20256;&#36755;&#36807;&#31243;&#36827;&#34892;&#20132;&#20114;&#36830;&#25509;&#65292;&#20197;&#20256;&#36882;&#26799;&#24230;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end scene text spotting has made significant progress due to its intrinsic synergy between text detection and recognition. Previous methods commonly regard manual annotations such as horizontal rectangles, rotated rectangles, quadrangles, and polygons as a prerequisite, which are much more expensive than using single-point. For the first time, we demonstrate that training scene text spotting models can be achieved with an extremely low-cost single-point annotation by the proposed framework, termed SPTS v2. SPTS v2 reserves the advantage of the auto-regressive Transformer with an Instance Assignment Decoder (IAD) through sequentially predicting the center points of all text instances inside the same predicting sequence, while with a Parallel Recognition Decoder (PRD) for text recognition in parallel. These two decoders share the same parameters and are interactively connected with a simple but effective information transmission process to pass the gradient and information. Compre
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22238;&#25918;&#36712;&#36857;&#32780;&#19981;&#26159;&#21407;&#22987;&#30340;&#21069;&#21521;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#22870;&#21169;&#20449;&#21495;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#36827;&#34892;&#24494;&#23567;&#30340;&#31639;&#27861;&#25913;&#21464;&#12290;</title><link>http://arxiv.org/abs/2212.14214</link><description>&lt;p&gt;
&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Backward Curriculum Reinforcement Learning. (arXiv:2212.14214v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14214
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22238;&#25918;&#36712;&#36857;&#32780;&#19981;&#26159;&#21407;&#22987;&#30340;&#21069;&#21521;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#22870;&#21169;&#20449;&#21495;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#36827;&#34892;&#24494;&#23567;&#30340;&#31639;&#27861;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#21069;&#21521;&#29983;&#25104;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#30340;&#25351;&#23548;&#19981;&#36275;&#20197;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#23613;&#21487;&#33021;&#22810;&#30340;&#25506;&#32034;&#12290;&#23613;&#31649;&#25105;&#20204;&#35748;&#35782;&#21040;&#24378;&#21270;&#23398;&#20064;&#32467;&#26524;&#26469;&#33258;&#20805;&#20998;&#30340;&#25506;&#32034;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#23384;&#22312;&#25240;&#34935;&#65292;&#36825;&#26159;&#24433;&#21709;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20351;&#29992;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#21644;&#32593;&#32476;&#32467;&#26500;&#20462;&#25913;&#26469;&#22686;&#21152;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#24456;&#22810;&#27493;&#39588;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#22238;&#25918;&#36712;&#36857;&#32780;&#19981;&#26159;&#21407;&#22987;&#30340;&#21069;&#21521;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#22312;&#26234;&#33021;&#20307;&#35757;&#32451;&#20043;&#21069;&#23545;&#36712;&#36857;&#30340;&#39034;&#24207;&#36827;&#34892;&#24494;&#23567;&#30340;&#25913;&#21464;&#65292;&#20351;&#24471;&#23454;&#29616;&#36215;&#26469;&#26356;&#21152;&#30452;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current reinforcement learning algorithms train an agent using forward-generated trajectories, which provide little guidance so that the agent can explore as much as possible. While realizing the value of reinforcement learning results from sufficient exploration, this approach leads to a trade-off in losing sample efficiency, an essential factor impacting algorithm performance. Previous tasks use reward-shaping techniques and network structure modification to increase sample efficiency. However, these methods require many steps to implement. In this work, we propose novel backward curriculum reinforcement learning that begins training the agent using the backward trajectory of the episode instead of the original forward trajectory. This approach provides the agent with a strong reward signal, enabling more sample-efficient learning. Moreover, our method only requires a minor change in the algorithm of reversing the order of the trajectory before agent training, allowing a straightforw
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;Vision Transformers&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#32593;&#32476;&#35774;&#35745;&#21644;&#25628;&#32034;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#19982;MobileNet&#31867;&#20284;&#22823;&#23567;&#21644;&#36895;&#24230;&#30340;Transformer&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.08059</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#36866;&#29992;&#20110;MobileNet&#36895;&#24230;&#21644;&#23610;&#23544;&#30340;Vision Transformers
&lt;/p&gt;
&lt;p&gt;
Rethinking Vision Transformers for MobileNet Size and Speed. (arXiv:2212.08059v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;Vision Transformers&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#32593;&#32476;&#35774;&#35745;&#21644;&#25628;&#32034;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#19982;MobileNet&#31867;&#20284;&#22823;&#23567;&#21644;&#36895;&#24230;&#30340;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Vision Transformers&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#30340;&#25104;&#21151;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#20248;&#21270;ViTs&#30340;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#23454;&#29616;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#39640;&#25928;&#37096;&#32626;&#12290;&#25552;&#20986;&#20102;&#22810;&#31181;&#26041;&#27861;&#26469;&#21152;&#36895;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25913;&#36827;&#20302;&#25928;&#30340;&#35774;&#35745;&#65292;&#25110;&#23558;&#31227;&#21160;&#35774;&#22791;&#21451;&#22909;&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#19982;ViTs&#32467;&#21512;&#24418;&#25104;&#28151;&#21512;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#22810;&#24180;&#21069;&#30340;MobileNet&#65292;ViT&#21450;&#20854;&#21464;&#31181;&#20173;&#28982;&#20855;&#26377;&#26356;&#39640;&#30340;&#24310;&#36831;&#25110;&#26356;&#22810;&#30340;&#21442;&#25968;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24310;&#36831;&#21644;&#22823;&#23567;&#23545;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30828;&#20214;&#19978;&#30340;&#39640;&#25928;&#37096;&#32626;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ViTs&#30340;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#37325;&#26032;&#23457;&#35270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#39640;&#21442;&#25968;&#25928;&#29575;&#30340;&#26032;&#22411;&#36229;&#32593;&#32476;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32454;&#31890;&#24230;&#32852;&#21512;&#25628;&#32034;&#31574;&#30053;&#65292;&#29992;&#20110;&#36890;&#36807;&#20248;&#21270;&#26469;&#23547;&#25214;&#39640;&#25928;&#30340;transformer&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the success of Vision Transformers (ViTs) in computer vision tasks, recent arts try to optimize the performance and complexity of ViTs to enable efficient deployment on mobile devices. Multiple approaches are proposed to accelerate attention mechanism, improve inefficient designs, or incorporate mobile-friendly lightweight convolutions to form hybrid architectures. However, ViT and its variants still have higher latency or considerably more parameters than lightweight CNNs, even true for the years-old MobileNet. In practice, latency and size are both crucial for efficient deployment on resource-constraint hardware. In this work, we investigate a central question, can transformer models run as fast as MobileNet and maintain a similar size? We revisit the design choices of ViTs and propose a novel supernet with low latency and high parameter efficiency. We further introduce a novel fine-grained joint search strategy for transformer models that can find efficient architectures by opt
&lt;/p&gt;</description></item><item><title>Nelson&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;Lov\'asz Local Lemma&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#27169;&#22411;&#29983;&#25104;&#28385;&#36275;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.00296</link><description>&lt;p&gt;
&#36890;&#36807; Lov\'asz Local Lemma &#36827;&#34892;&#37319;&#26679;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#23398;&#20064;&#32452;&#21512;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning Combinatorial Structures via Markov Random Fields with Sampling through Lov\'asz Local Lemma. (arXiv:2212.00296v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00296
&lt;/p&gt;
&lt;p&gt;
Nelson&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;Lov\'asz Local Lemma&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#27169;&#22411;&#29983;&#25104;&#28385;&#36275;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#32452;&#21512;&#32467;&#26500;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#30001;&#20110;&#23398;&#20064;&#30446;&#26631;&#21463;&#21040;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#21046;&#32422;&#65292;&#20854;&#26799;&#24230;&#20272;&#35745;&#38750;&#24120;&#22797;&#26434;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110; Lov\'asz Local Lemma &#30340;&#31070;&#32463;&#32593;&#32476;&#65288;Nelson&#65289;&#65292;&#23427;&#33021;&#22815;&#20174;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#27169;&#22411;&#30340;&#20998;&#24067;&#20013;&#29983;&#25104;&#28385;&#36275;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models for learning combinatorial structures have transformative impacts in many applications. However, existing approaches fail to offer efficient and accurate learning results. Because of the highly intractable nature of the gradient estimation of the learning objective subject to combinatorial constraints. Existing gradient estimation methods would easily run into exponential time/memory space, or incur huge estimation errors due to improper approximation. We develop NEural Lovasz Sampler (Nelson), a neural network based on Lov\'asz Local Lemma (LLL). We show it guarantees to generate samples satisfying combinatorial constraints from the distribution of the constrained Markov Random Fields model (MRF) under certain conditions. We further present a fully differentiable contrastive-divergence-based learning framework on constrained MRF (Nelson-CD). Meanwhile, Nelson-CD being fully differentiable allows us to take advantage of the parallel computing power of GPUs, resulting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#20026;&#31038;&#20132;&#23186;&#20307;&#25366;&#25496;&#38750;&#21307;&#23398;&#22788;&#26041;&#33647;&#29289;&#20351;&#29992;&#20449;&#24687;&#32780;&#24320;&#21457;&#30340;&#31471;&#21040;&#31471;&#31649;&#36947;&#65292;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2211.10443</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25366;&#25496;&#22788;&#26041;&#33647;&#29289;&#30340;&#27602;&#24615;&#30417;&#27979;: &#20174;&#22836;&#21040;&#23614;&#30340;&#31649;&#36947;&#65292;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media mining for toxicovigilance of prescription medications: End-to-end pipeline, challenges and future work. (arXiv:2211.10443v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#20026;&#31038;&#20132;&#23186;&#20307;&#25366;&#25496;&#38750;&#21307;&#23398;&#22788;&#26041;&#33647;&#29289;&#20351;&#29992;&#20449;&#24687;&#32780;&#24320;&#21457;&#30340;&#31471;&#21040;&#31471;&#31649;&#36947;&#65292;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#36136;&#20351;&#29992;&#12289;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#21644;&#19982;&#29289;&#36136;&#20351;&#29992;&#30456;&#20851;&#30340;&#36807;&#37327;&#26159;&#20840;&#29699;&#21644;&#32654;&#22269;&#30340;&#20027;&#35201;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#12290;&#20174;&#20844;&#20849;&#21355;&#29983;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#25913;&#36827;&#30417;&#27979;&#31995;&#32479;&#12290;&#20256;&#32479;&#30340;&#30417;&#27979;&#31995;&#32479;&#28382;&#21518;&#65292;&#32780;&#31038;&#20132;&#23186;&#20307;&#26159;&#21450;&#26102;&#25968;&#25454;&#30340;&#28508;&#22312;&#26377;&#29992;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#25366;&#25496;&#30693;&#35782;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#24320;&#21457;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#25366;&#25496;&#38750;&#21307;&#23398;&#22788;&#26041;&#33647;&#29289;&#20351;&#29992;&#20449;&#24687;&#30340;&#31471;&#21040;&#31471;&#31649;&#36947;&#65292;&#21363;Twitter&#21644;Reddit&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#37319;&#29992;&#20102;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;NLP&#26469;&#36807;&#28388;&#22122;&#38899;&#21644;&#25551;&#36848;&#20132;&#27969;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;&#22235;&#24180;&#20869;&#24320;&#21457;&#30340;&#31471;&#21040;&#31471;&#31649;&#36947;&#12290;&#38500;&#20102;&#25551;&#36848;&#25105;&#20204;&#30340;&#25968;&#25454;&#25366;&#25496;&#22522;&#30784;&#35774;&#26045;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Substance use, substance use disorder, and overdoses related to substance use are major public health problems globally and in the United States. A key aspect of addressing these problems from a public health standpoint is improved surveillance. Traditional surveillance systems are laggy, and social media are potentially useful sources of timely data. However, mining knowledge from social media is challenging, and requires the development of advanced artificial intelligence, specifically natural language processing (NLP) and machine learning methods. We developed a sophisticated end-to-end pipeline for mining information about nonmedical prescription medication use from social media, namely Twitter and Reddit. Our pipeline employs supervised machine learning and NLP for filtering out noise and characterizing the chatter. In this paper, we describe our end-to-end pipeline developed over four years. In addition to describing our data mining infrastructure, we discuss existing challenges 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#30340;&#32534;&#30721;&#22120;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#23398;&#35821;&#20041;&#65292;&#27604;&#33258;&#21160;&#32534;&#30721;&#22120;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08142</link><description>&lt;p&gt;
&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#35821;&#20041;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Semantic Representations of Mathematical Expressions in a Continuous Vector Space. (arXiv:2211.08142v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08142
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#30340;&#32534;&#30721;&#22120;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#23398;&#35821;&#20041;&#65292;&#27604;&#33258;&#21160;&#32534;&#30721;&#22120;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#31526;&#21495;&#22312;STEM&#25991;&#29486;&#20013;&#21344;&#25454;&#20102;&#24456;&#22823;&#19968;&#37096;&#20998;&#65292;&#20294;&#26159;&#65292;&#20026;&#20844;&#24335;&#25214;&#21040;&#35821;&#20041;&#34920;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#25968;&#23398;&#31526;&#21495;&#26159;&#31934;&#30830;&#30340;&#65292;&#22312;&#23383;&#31526;&#24494;&#23567;&#21464;&#21270;&#26102;&#20854;&#21547;&#20041;&#20250;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#33258;&#28982;&#25991;&#26412;&#30340;&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#30340;&#32534;&#30721;&#22120;&#65292;&#35757;&#32451;&#20854;&#22312;&#35270;&#35273;&#19978;&#19981;&#21516;&#20294;&#22312;&#25968;&#23398;&#19978;&#31561;&#20215;&#30340;&#34920;&#36798;&#24335;&#19978;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#65288;&#25110;&#23884;&#20837;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#21069;&#32773;&#26356;&#33021;&#25429;&#25417;&#25968;&#23398;&#35821;&#20041;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20221;&#31561;&#20215;&#30340;&#36229;&#36234;&#21644;&#20195;&#25968;&#34920;&#36798;&#24335;&#23545;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical notation makes up a large portion of STEM literature, yet, finding semantic representations for formulae remains a challenging problem. Because mathematical notation is precise, and its meaning changes significantly with small character shifts, the methods that work for natural text do not necessarily work well for mathematical expressions. In this work, we describe an approach for representing mathematical expressions in a continuous vector space. We use the encoder of a sequence-to-sequence architecture, trained on visually different but mathematically equivalent expressions, to generate vector representations (or embeddings). We compare this approach with an autoencoder and show that the former is better at capturing mathematical semantics. Finally, to expedite future research, we publish a corpus of equivalent transcendental and algebraic expression pairs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MemoNet&#30340;CTR&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#21704;&#24076;&#30721;&#26412;&#32593;&#32476;&#65288;HCNet&#65289;&#20316;&#20026;&#35760;&#24518;&#26426;&#21046;&#65292;&#39640;&#25928;&#22320;&#23398;&#20064;&#21644;&#35760;&#24518;&#20132;&#21449;&#29305;&#24449;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MemoNet&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23637;&#29616;&#20986;NLP&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#35268;&#24459;&#12290;</title><link>http://arxiv.org/abs/2211.01334</link><description>&lt;p&gt;
MemoNet: &#36890;&#36807;&#22810;&#21704;&#24076;&#30721;&#26412;&#32593;&#32476;&#39640;&#25928;&#22320;&#35760;&#24518;&#25152;&#26377;&#20132;&#21449;&#29305;&#24449;&#34920;&#31034;&#20197;&#23454;&#29616;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MemoNet: Memorizing All Cross Features' Representations Efficiently via Multi-Hash Codebook Network for CTR Prediction. (arXiv:2211.01334v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MemoNet&#30340;CTR&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#21704;&#24076;&#30721;&#26412;&#32593;&#32476;&#65288;HCNet&#65289;&#20316;&#20026;&#35760;&#24518;&#26426;&#21046;&#65292;&#39640;&#25928;&#22320;&#23398;&#20064;&#21644;&#35760;&#24518;&#20132;&#21449;&#29305;&#24449;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MemoNet&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23637;&#29616;&#20986;NLP&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#26032;&#21457;&#29616;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#35760;&#24518;&#33021;&#21147;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25104;&#21151;&#36215;&#21040;&#20102;&#24456;&#22823;&#20316;&#29992;&#12290;&#36825;&#21551;&#21457;&#25105;&#20204;&#23558;&#29420;&#31435;&#30340;&#35760;&#24518;&#26426;&#21046;&#24341;&#20837;CTR&#25490;&#21517;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#21644;&#35760;&#24518;&#20132;&#21449;&#29305;&#24449;&#30340;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#21704;&#24076;&#30721;&#26412;&#32593;&#32476;&#65288;HCNet&#65289;&#20316;&#20026;CTR&#20219;&#21153;&#20013;&#39640;&#25928;&#23398;&#20064;&#21644;&#35760;&#24518;&#20132;&#21449;&#29305;&#24449;&#34920;&#31034;&#30340;&#35760;&#24518;&#26426;&#21046;&#12290;HCNet&#20351;&#29992;&#22810;&#21704;&#24076;&#30721;&#26412;&#20316;&#20026;&#20027;&#35201;&#30340;&#35760;&#24518;&#20301;&#32622;&#65292;&#24182;&#30001;&#22810;&#21704;&#24076;&#23547;&#22336;&#12289;&#35760;&#24518;&#24674;&#22797;&#21644;&#29305;&#24449;&#32553;&#20943;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MemoNet&#30340;&#26032;&#22411;CTR&#27169;&#22411;&#65292;&#23558;HCNet&#19982;DNN&#39592;&#24178;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22312;&#32447;&#27979;&#35797;&#20013;&#34920;&#26126;&#65292;MemoNet&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;MemoNet&#23637;&#29616;&#20986;NLP&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#35268;&#24459;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#21487;&#20197;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
New findings in natural language processing (NLP) demonstrate that the strong memorization capability contributes a lot to the success of Large Language Models (LLM). This inspires us to explicitly bring an independent memory mechanism into CTR ranking model to learn and memorize cross features' representations. In this paper, we propose multi-Hash Codebook NETwork (HCNet) as the memory mechanism for efficiently learning and memorizing representations of cross features in CTR tasks. HCNet uses a multi-hash codebook as the main memory place and the whole memory procedure consists of three phases: multi-hash addressing, memory restoring, and feature shrinking. We also propose a new CTR model named MemoNet which combines HCNet with a DNN backbone. Extensive experimental results on three public datasets and online test show that MemoNet reaches superior performance over state-of-the-art approaches. Besides, MemoNet shows scaling law of large language model in NLP, which means we can enlarg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21338;&#24328;&#22343;&#34913;&#27010;&#24565;&#8212;&#8212;&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913;&#65292;&#22312;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#21487;&#20197;&#24110;&#21161;&#21019;&#24314;&#30495;&#27491;&#30340;&#31574;&#30053;&#20195;&#29702;&#12290;&#36825;&#31181;&#22343;&#34913;&#27010;&#24565;&#22312;&#20844;&#24320;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.16506</link><description>&lt;p&gt;
&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913; (Observable Perfect Equilibrium)
&lt;/p&gt;
&lt;p&gt;
Observable Perfect Equilibrium. (arXiv:2210.16506v5 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21338;&#24328;&#22343;&#34913;&#27010;&#24565;&#8212;&#8212;&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913;&#65292;&#22312;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#21487;&#20197;&#24110;&#21161;&#21019;&#24314;&#30495;&#27491;&#30340;&#31574;&#30053;&#20195;&#29702;&#12290;&#36825;&#31181;&#22343;&#34913;&#27010;&#24565;&#22312;&#20844;&#24320;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32435;&#20160;&#22343;&#34913;&#25104;&#20026;&#20102;&#21338;&#24328;&#35770;&#30340;&#26680;&#24515;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#65292;&#35768;&#22810;&#37325;&#35201;&#30340;&#21338;&#24328;&#21253;&#21547;&#22810;&#20010;&#32435;&#20160;&#22343;&#34913;&#65292;&#25105;&#20204;&#24517;&#39035;&#30830;&#23450;&#22914;&#20309;&#22312;&#20854;&#20013;&#36873;&#25321;&#65292;&#20197;&#21019;&#24314;&#30495;&#27491;&#30340;&#31574;&#30053;&#20195;&#29702;&#12290;&#20026;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#25552;&#20986;&#20102;&#20960;&#20010;&#32435;&#20160;&#22343;&#34913;&#32454;&#21270;&#27010;&#24565;&#65292;&#20854;&#20013;&#26368;&#31361;&#20986;&#30340;&#26159;&#39076;&#25238;&#25163;&#23436;&#32654;&#22343;&#34913;&#12289;&#25311;&#23436;&#32654;&#22343;&#34913;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#21333;&#20391;&#25311;&#23436;&#32654;&#22343;&#34913;&#12290;&#36825;&#20123;&#27010;&#24565;&#23545;&#26576;&#20123;&#20219;&#24847;&#23567;&#30340;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#20445;&#35777;&#22987;&#32456;&#23384;&#22312;&#12290;&#20294;&#25105;&#20204;&#35748;&#20026;&#65292;&#23545;&#20110;&#21457;&#23637;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#24378;&#22823;&#30340;&#20195;&#29702;&#20154;&#65292;&#36825;&#20123;&#27010;&#24565;&#37117;&#19981;&#27491;&#30830;&#12290;&#25105;&#20204;&#20026;&#28216;&#25103;&#26641;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#34913;&#27010;&#24565;&#8212;&#8212;&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913;&#65292;&#22312;&#20854;&#20013;&#65292;&#35299;&#20915;&#26041;&#26696;&#22312;&#20844;&#24320;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#65288;&#24182;&#19981;&#19968;&#23450;&#38024;&#23545;&#25152;&#26377;&#21487;&#33021;&#19981;&#21487;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#20855;&#26377;&#40065;&#26834;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Nash equilibrium has emerged as the central game-theoretic solution concept, many important games contain several Nash equilibria and we must determine how to select between them in order to create real strategic agents. Several Nash equilibrium refinement concepts have been proposed and studied for sequential imperfect-information games, the most prominent being trembling-hand perfect equilibrium, quasi-perfect equilibrium, and recently one-sided quasi-perfect equilibrium. These concepts are robust to certain arbitrarily small mistakes, and are guaranteed to always exist; however, we argue that neither of these is the correct concept for developing strong agents in sequential games of imperfect information. We define a new equilibrium refinement concept for extensive-form games called observable perfect equilibrium in which the solution is robust over trembles in publicly-observable action probabilities (not necessarily over all action probabilities that may not be observable by
&lt;/p&gt;</description></item><item><title>JAX-DIPS&#26159;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#31163;&#25955;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#20280;&#32553;&#31574;&#30053;&#65292;&#29992;&#20110;&#24320;&#21457;&#26080;&#32593;&#26684;&#28151;&#21512;&#31070;&#32463;&#31526;&#21495;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;&#20855;&#26377;&#38388;&#26029;&#30340;&#26925;&#22278;&#38382;&#39064;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.14312</link><description>&lt;p&gt;
JAX-DIPS&#65306;&#26377;&#38480;&#31163;&#25955;&#26041;&#27861;&#30340;&#31070;&#32463;&#24341;&#23548;&#27861;&#21450;&#20854;&#22312;&#20855;&#26377;&#38388;&#26029;&#30340;&#26925;&#22278;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
JAX-DIPS: Neural bootstrapping of finite discretization methods and application to elliptic problems with discontinuities. (arXiv:2210.14312v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14312
&lt;/p&gt;
&lt;p&gt;
JAX-DIPS&#26159;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#31163;&#25955;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#20280;&#32553;&#31574;&#30053;&#65292;&#29992;&#20110;&#24320;&#21457;&#26080;&#32593;&#26684;&#28151;&#21512;&#31070;&#32463;&#31526;&#21495;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;&#20855;&#26377;&#38388;&#26029;&#30340;&#26925;&#22278;&#38382;&#39064;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22522;&#20110;&#29616;&#26377;&#22522;&#20110;&#32593;&#26684;&#30340;&#25968;&#20540;&#31163;&#25955;&#26041;&#27861;&#24320;&#21457;&#26080;&#32593;&#26684;&#28151;&#21512;&#31070;&#32463;&#31526;&#21495;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;&#65288;i&#65289;&#21033;&#29992;&#20808;&#36827;&#25968;&#20540;&#26041;&#27861;&#12289;&#27714;&#35299;&#22120;&#21644;&#39044;&#22788;&#29702;&#22120;&#30340;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#24615;&#20197;&#21450;&#65288;ii&#65289;&#23558;&#20248;&#21270;&#38480;&#21046;&#22312;&#19968;&#38454;&#33258;&#21160;&#24494;&#20998;&#19978;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#35757;&#32451;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#31070;&#32463;&#24341;&#23548;&#26041;&#27861;&#65288;&#20197;&#19979;&#31616;&#31216;NBM&#65289;&#22522;&#20110;&#30456;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#38543;&#26426;&#21462;&#26679;&#28857;&#19978;&#38544;&#24335;&#31515;&#21345;&#23572;&#21333;&#20803;&#26684;&#19978;&#33719;&#24471;&#30340;PDE&#31995;&#32479;&#30340;&#26377;&#38480;&#31163;&#25955;&#27531;&#24046;&#30340;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24341;&#23548;&#26377;&#38480;&#31163;&#25955;&#26041;&#31243;&#20013;&#23384;&#22312;&#30340;&#23432;&#24658;&#23450;&#24459;&#21644;&#23545;&#31216;&#24615;&#33021;&#22815;&#21521;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#26377;&#20851;&#35299;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a scalable strategy for development of mesh-free hybrid neuro-symbolic partial differential equation solvers based on existing mesh-based numerical discretization methods. Particularly, this strategy can be used to efficiently train neural network surrogate models of partial differential equations by (i) leveraging the accuracy and convergence properties of advanced numerical methods, solvers, and preconditioners, as well as (ii) better scalability to higher order PDEs by strictly limiting optimization to first order automatic differentiation. The presented neural bootstrapping method (hereby dubbed NBM) is based on evaluation of the finite discretization residuals of the PDE system obtained on implicit Cartesian cells centered on a set of random collocation points with respect to trainable parameters of the neural network. Importantly, the conservation laws and symmetries present in the bootstrapped finite discretization equations inform the neural network about solution re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35270;&#39057;&#20013;&#29289;&#20307;6D&#23039;&#24577;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#24103;&#20449;&#24687;&#36827;&#34892;&#23039;&#24577;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#23039;&#24577;&#20272;&#35745;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#26102;&#38388;&#24207;&#21015;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;CNN&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20855;&#26377;33fps&#30340;&#22788;&#29702;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.13540</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#29289;&#20307;6D&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Video based Object 6D Pose Estimation using Transformers. (arXiv:2210.13540v2 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35270;&#39057;&#20013;&#29289;&#20307;6D&#23039;&#24577;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#24103;&#20449;&#24687;&#36827;&#34892;&#23039;&#24577;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#23039;&#24577;&#20272;&#35745;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#26102;&#38388;&#24207;&#21015;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;CNN&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20855;&#26377;33fps&#30340;&#22788;&#29702;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;VideoPose&#30340;&#22522;&#20110;Transformer&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24314;&#27169;&#26550;&#26500;&#65292;&#36890;&#36807;&#20851;&#27880;&#20808;&#21069;&#30340;&#24103;&#26469;&#20272;&#35745;&#35270;&#39057;&#20013;&#20934;&#30830;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#36827;&#34892;&#23039;&#24577;&#32454;&#21270;&#65292;&#21516;&#26102;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#39640;&#21644;&#40065;&#26834;&#24615;&#24378;&#30340;&#29305;&#28857;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#25512;&#29702;&#36828;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#35270;&#39057;&#24207;&#21015;&#19978;&#36827;&#34892;&#36845;&#20195;&#32454;&#21270;&#12290;&#23545;YCB-Video&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;Transformer&#26041;&#27861;&#25345;&#24179;&#65292;&#24182;&#30456;&#23545;&#20110;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27599;&#31186;&#33021;&#22788;&#29702;33&#24103;&#65292;&#26356;&#21152;&#39640;&#25928;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#38656;&#35201;&#23454;&#26102;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#35757;&#32451;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/ApoorvaBeedu/VideoPose&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a Transformer based 6D Object Pose Estimation framework VideoPose, comprising an end-to-end attention based modelling architecture, that attends to previous frames in order to estimate accurate 6D Object Poses in videos. Our approach leverages the temporal information from a video sequence for pose refinement, along with being computationally efficient and robust. Compared to existing methods, our architecture is able to capture and reason from long-range dependencies efficiently, thus iteratively refining over video sequences. Experimental evaluation on the YCB-Video dataset shows that our approach is on par with the state-of-the-art Transformer methods, and performs significantly better relative to CNN based approaches. Further, with a speed of 33 fps, it is also more efficient and therefore applicable to a variety of applications that require real-time object pose estimation. Training code and pretrained models are available at https://github.com/ApoorvaBeedu/VideoPose
&lt;/p&gt;</description></item><item><title>&#23545;&#27169;&#22411;&#35299;&#37322;&#30340;&#29992;&#25143;&#30740;&#31350;&#32508;&#36848;&#21457;&#29616;&#65292;&#21487;&#35299;&#37322;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27491;&#22312;&#26576;&#20123;&#24212;&#29992;&#39046;&#22495;&#24555;&#36895;&#25193;&#25955;&#65292;&#20294;&#29992;&#25143;&#35780;&#20272;&#20173;&#28982;&#31232;&#32570;&#19988;&#20960;&#20046;&#19981;&#28041;&#21450;&#35748;&#30693;&#25110;&#31038;&#20250;&#31185;&#23398;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2210.11584</link><description>&lt;p&gt;
&#26397;&#30528;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21487;&#35299;&#37322;&#22411;&#20154;&#24037;&#26234;&#33021;&#65306;&#23545;&#27169;&#22411;&#35299;&#37322;&#30340;&#29992;&#25143;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards Human-centered Explainable AI: A Survey of User Studies for Model Explanations. (arXiv:2210.11584v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11584
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27169;&#22411;&#35299;&#37322;&#30340;&#29992;&#25143;&#30740;&#31350;&#32508;&#36848;&#21457;&#29616;&#65292;&#21487;&#35299;&#37322;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27491;&#22312;&#26576;&#20123;&#24212;&#29992;&#39046;&#22495;&#24555;&#36895;&#25193;&#25955;&#65292;&#20294;&#29992;&#25143;&#35780;&#20272;&#20173;&#28982;&#31232;&#32570;&#19988;&#20960;&#20046;&#19981;&#28041;&#21450;&#35748;&#30693;&#25110;&#31038;&#20250;&#31185;&#23398;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19981;&#26029;&#25193;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#24517;&#38656;&#26465;&#20214;&#12290;&#23545;XAI&#29992;&#25143;&#38656;&#27714;&#30340;&#26356;&#22909;&#29702;&#35299;&#20197;&#21450;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#20154;&#26412;&#35780;&#20272;&#26082;&#26159;&#24517;&#35201;&#24615;&#20063;&#26159;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#30740;&#31350;&#20102;HCI&#21644;AI&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#36827;&#34892;XAI&#24212;&#29992;&#30340;&#29992;&#25143;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;&#36807;&#21435;&#20116;&#24180;&#20013;&#22522;&#20110;&#20154;&#31867;&#30340;XAI&#35780;&#20272;&#30340;97&#31687;&#26680;&#24515;&#35770;&#25991;&#36827;&#34892;&#35782;&#21035;&#21644;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;&#20854;&#25353;&#29031;&#35299;&#37322;&#26041;&#27861;&#30340;&#27979;&#37327;&#29305;&#24449;&#65288;&#20449;&#20219;&#12289;&#29702;&#35299;&#12289;&#21487;&#29992;&#24615;&#21644;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#34920;&#29616;&#65289;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;XAI&#22312;&#26576;&#20123;&#24212;&#29992;&#39046;&#22495;&#65288;&#22914;&#25512;&#33616;&#31995;&#32479;&#65289;&#25193;&#25955;&#26356;&#36805;&#36895;&#65292;&#20294;&#29992;&#25143;&#35780;&#20272;&#20173;&#30456;&#24403;&#31232;&#32570;&#65292;&#24182;&#19988;&#20960;&#20046;&#27809;&#26377;&#34701;&#20837;&#35748;&#30693;&#25110;&#31038;&#20250;&#31185;&#23398;&#30340;&#20219;&#20309;&#35265;&#35299;&#12290;&#22522;&#20110;&#32508;&#21512;&#35752;&#35770;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#21363;&#24120;&#35265;&#27169;&#22411;&#12289;&#35774;&#35745;&#36873;&#25321;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) is widely viewed as a sine qua non for ever-expanding AI research. A better understanding of the needs of XAI users, as well as human-centered evaluations of explainable models are both a necessity and a challenge. In this paper, we explore how HCI and AI researchers conduct user studies in XAI applications based on a systematic literature review. After identifying and thoroughly analyzing 97core papers with human-based XAI evaluations over the past five years, we categorize them along the measured characteristics of explanatory methods, namely trust, understanding, usability, and human-AI collaboration performance. Our research shows that XAI is spreading more rapidly in certain application domains, such as recommender systems than in others, but that user evaluations are still rather sparse and incorporate hardly any insights from cognitive or social sciences. Based on a comprehensive discussion of best practices, i.e., common models, design choices, and measures
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#30340;&#32852;&#37030;&#25968;&#25454;&#39537;&#21160;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#20445;&#25252;&#21407;&#22987;&#25968;&#25454;&#21644;&#36890;&#36807;&#26381;&#21153;&#22120;&#19978;&#30340;&#33719;&#24471;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#32780;&#33719;&#24471;&#30340;&#26032;&#22686;&#35299;&#20915;&#26041;&#26696;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#20195;&#29702;&#26356;&#26032;&#26102;&#65292;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23458;&#25143;&#31471;&#19978;&#36873;&#25321;&#26597;&#35810;&#28857;&#65292;&#20943;&#23569;&#20102;&#27844;&#28431;&#35299;&#20915;&#26041;&#26696;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2210.08295</link><description>&lt;p&gt;
&#19968;&#20010;&#23433;&#20840;&#30340;&#32852;&#37030;&#25968;&#25454;&#39537;&#21160;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Secure Federated Data-Driven Evolutionary Multi-objective Optimization Algorithm. (arXiv:2210.08295v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08295
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#30340;&#32852;&#37030;&#25968;&#25454;&#39537;&#21160;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#20445;&#25252;&#21407;&#22987;&#25968;&#25454;&#21644;&#36890;&#36807;&#26381;&#21153;&#22120;&#19978;&#30340;&#33719;&#24471;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#32780;&#33719;&#24471;&#30340;&#26032;&#22686;&#35299;&#20915;&#26041;&#26696;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#20195;&#29702;&#26356;&#26032;&#26102;&#65292;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23458;&#25143;&#31471;&#19978;&#36873;&#25321;&#26597;&#35810;&#28857;&#65292;&#20943;&#23569;&#20102;&#27844;&#28431;&#35299;&#20915;&#26041;&#26696;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#36827;&#21270;&#31639;&#27861;&#36890;&#24120;&#26088;&#22312;&#21033;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#26469;&#36827;&#34892;&#20248;&#21270;&#65292;&#24050;&#32463;&#22312;&#35299;&#20915;&#35768;&#22810;&#22797;&#26434;&#30340;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25968;&#25454;&#39537;&#21160;&#30340;&#36827;&#21270;&#31639;&#27861;&#37117;&#26159;&#38598;&#20013;&#24335;&#30340;&#65292;&#20174;&#32780;&#24341;&#21457;&#20102;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#32852;&#37030;&#25968;&#25454;&#39537;&#21160;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#20445;&#25252;&#21407;&#22987;&#25968;&#25454;&#21644;&#36890;&#36807;&#23545;&#26381;&#21153;&#22120;&#19978;&#30340;&#33719;&#24471;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#32780;&#33719;&#24471;&#30340;&#26032;&#22686;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#27599;&#27425;&#20195;&#29702;&#26356;&#26032;&#26102;&#65292;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23458;&#25143;&#31471;&#19978;&#36873;&#25321;&#26597;&#35810;&#28857;&#65292;&#36890;&#36807;&#35745;&#31639;&#35813;&#23458;&#25143;&#31471;&#19978;&#26410;&#35266;&#27979;&#28857;&#30340;&#33719;&#24471;&#20989;&#25968;&#20540;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#27844;&#28431;&#20851;&#20110;&#24453;&#37319;&#26679;&#35299;&#20915;&#26041;&#26696;&#30340;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven evolutionary algorithms usually aim to exploit the information behind a limited amount of data to perform optimization, which have proved to be successful in solving many complex real-world optimization problems. However, most data-driven evolutionary algorithms are centralized, causing privacy and security concerns. Existing federated Bayesian algorithms and data-driven evolutionary algorithms mainly protect the raw data on each client. To address this issue, this paper proposes a secure federated data-driven evolutionary multi-objective optimization algorithm to protect both the raw data and the newly infilled solutions obtained by optimizing the acquisition function conducted on the server. We select the query points on a randomly selected client at each round of surrogate update by calculating the acquisition function values of the unobserved points on this client, thereby reducing the risk of leaking the information about the solution to be sampled. In addition, since 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#19968;&#31995;&#21015;&#20998;&#23376;&#34920;&#24449;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#22522;&#20110;&#22266;&#23450;&#34920;&#24449;&#30340;&#27169;&#22411;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#20855;&#26377;&#19968;&#23450;&#20248;&#21183;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#27963;&#24615;&#26029;&#23830;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.13492</link><description>&lt;p&gt;
&#26242;&#20572;&#20998;&#23376;&#34920;&#24449;&#23398;&#20064;&#65306;&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Taking a Respite from Representation Learning for Molecular Property Prediction. (arXiv:2209.13492v3 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19968;&#31995;&#21015;&#20998;&#23376;&#34920;&#24449;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#22522;&#20110;&#22266;&#23450;&#34920;&#24449;&#30340;&#27169;&#22411;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#20855;&#26377;&#19968;&#23450;&#20248;&#21183;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#27963;&#24615;&#26029;&#23830;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20854;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#23601;&#26159;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12290;&#34429;&#28982;&#20998;&#23376;&#34920;&#24449;&#23398;&#20064;&#30340;&#25216;&#26415;&#22914;&#27492;&#21457;&#36798;&#65292;&#20294;&#20854;&#32972;&#21518;&#30340;&#22522;&#30784;&#38382;&#39064;&#21364;&#26410;&#34987;&#35748;&#30495;&#25506;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#20998;&#23376;&#34920;&#24449;&#23545;&#19968;&#31995;&#21015;&#20195;&#34920;&#24615;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;&#38500;&#20102;&#24120;&#29992;&#30340;MoleculeNet&#22522;&#20934;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;ChEMBL&#25968;&#25454;&#24211;&#21644;&#25991;&#29486;&#20013;&#25910;&#38598;&#20102;&#19968;&#22871;&#19982;&#38463;&#29255;&#31867;&#29289;&#36136;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#20004;&#20010;&#39069;&#22806;&#30340;&#27963;&#24615;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#32452;&#35013;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#21516;&#35268;&#27169;&#30340;&#25551;&#36848;&#31526;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24635;&#20849;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;62,820&#20010;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;50,220&#20010;&#20351;&#29992;&#22266;&#23450;&#34920;&#24449;&#30340;&#27169;&#22411;&#12289;4,200&#20010;&#20351;&#29992;SMILES&#24207;&#21015;&#30340;&#27169;&#22411;&#21644;8,400&#20010;&#20351;&#29992;&#20998;&#23376;&#22270;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#25968;&#25454;&#38598;&#20998;&#26512;&#65292;&#24182;&#24378;&#35843;&#20102;&#38463;&#29255;&#31867;&#29289;&#36136;&#20013;&#30340;&#27963;&#24615;&#26029;&#23830;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has been widely applied in drug discovery with a major task as molecular property prediction. Despite booming techniques in molecular representation learning, fundamentals underlying molecular property prediction haven't been carefully examined yet. In this study, we conducted a systematic evaluation on a collection of representative models using various molecular representations. In addition to the commonly used MoleculeNet benchmark datasets, we also assembled a suite of opioids-related datasets from ChEMBL and two additional activity datasets from literature. To interrogate the basic predictive power, we also assembled a series of descriptors datasets with varying sizes to evaluate the models' performance. In total, we trained 62,820 models, including 50,220 models on fixed representations, 4,200 models on SMILES sequences and 8,400 models on molecular graphs. We first conducted dataset profiling and highlighted the activity-cliffs issue in the opioids-r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30450;&#30446;&#26816;&#27979;&#36755;&#20837;&#38899;&#39057;&#30340;&#30495;&#23454;&#24615;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#38899;&#39057;&#27450;&#35784;&#38382;&#39064;&#30340;&#20998;&#31867;&#22120;&#12290;&#32780;&#36825;&#31181;&#20998;&#31867;&#22120;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#32771;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#30495;&#23454;&#26469;&#28304;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20986;&#27169;&#20223;&#38899;&#39057;&#12290;</title><link>http://arxiv.org/abs/2209.12573</link><description>&lt;p&gt;
&#25968;&#23383;&#38899;&#39057;&#21462;&#35777;&#65306;&#30450;&#30446;&#26816;&#27979;&#20154;&#31867;&#35821;&#38899;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Digital Audio Forensics: Blind Human Voice Mimicry Detection. (arXiv:2209.12573v4 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30450;&#30446;&#26816;&#27979;&#36755;&#20837;&#38899;&#39057;&#30340;&#30495;&#23454;&#24615;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#38899;&#39057;&#27450;&#35784;&#38382;&#39064;&#30340;&#20998;&#31867;&#22120;&#12290;&#32780;&#36825;&#31181;&#20998;&#31867;&#22120;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#32771;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#30495;&#23454;&#26469;&#28304;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20986;&#27169;&#20223;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#26159;&#20154;&#31867;&#20132;&#27969;&#20013;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#26041;&#24335;&#20043;&#19968;&#65292;&#20294;&#21516;&#26102;&#20063;&#24456;&#23481;&#26131;&#34987;&#35823;&#29992;&#26469;&#27450;&#39575;&#20154;&#20204;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#38761;&#21629;&#65292;&#30456;&#20851;&#25216;&#26415;&#29616;&#22312;&#23545;&#20960;&#20046;&#25152;&#26377;&#20154;&#37117;&#21487;&#29992;&#65292;&#36825;&#20351;&#24471;&#29359;&#32618;&#21644;&#20266;&#36896;&#21464;&#24471;&#26356;&#21152;&#31616;&#21333;&#12290;&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#30450;&#30446;&#20998;&#31867;&#36755;&#20837;&#38899;&#39057;&#20026;&#30495;&#23454;&#25110;&#32773;&#27169;&#20223;&#65307;&#8220;&#30450;&#30446;&#8221;&#25351;&#30340;&#26159;&#33021;&#22815;&#22312;&#27809;&#26377;&#21442;&#32771;&#25110;&#30495;&#23454;&#26469;&#28304;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20223;&#21046;&#38899;&#39057;&#30340;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#22312;&#19968;&#20010;&#22823;&#22411;&#38899;&#39057;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#19968;&#32452;&#37325;&#35201;&#29305;&#24449;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#34987;&#29992;&#20110;&#27979;&#35797;&#19981;&#21516;&#38899;&#39057;&#30340;&#30456;&#21516;&#29305;&#24449;&#38598;&#12290;&#25968;&#25454;&#25552;&#21462;&#33258;&#20004;&#20010;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#20026;&#36825;&#39033;&#24037;&#20316;&#32780;&#32534;&#20889;;&#19968;&#20010;&#20840;&#33521;&#25991;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#28151;&#21512;&#25968;&#25454;&#38598;&#65288;&#38463;&#25289;&#20271;&#35821;&#21152;&#33521;&#35821;&#65289;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#24050;&#36890;&#36807;GitHub&#20197;&#21407;&#22987;&#24418;&#24335;&#25552;&#20379;&#32473;&#30740;&#31350;&#31038;&#21306;&#65292;&#32593;&#22336;&#20026;https://github.com/SaSs7/Datas
&lt;/p&gt;
&lt;p&gt;
Audio is one of the most used ways of human communication, but at the same time it can be easily misused to trick people. With the revolution of AI, the related technologies are now accessible to almost everyone thus making it simple for the criminals to commit crimes and forgeries. In this work, we introduce a deep learning method to develop a classifier that will blindly classify an input audio as real or mimicked; the word 'blindly' refers to the ability to detect mimicked audio without references or real sources. The proposed model was trained on a set of important features extracted from a large dataset of audios to get a classifier that was tested on the same set of features from different audios. The data was extracted from two raw datasets, especially composed for this work; an all English dataset and a mixed dataset (Arabic plus English). These datasets have been made available, in raw form, through GitHub for the use of the research community at https://github.com/SaSs7/Datas
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20986;&#29616;&#21487;&#33021;&#20250;&#23548;&#33268;&#20854;&#36861;&#27714;&#19982;&#20154;&#31867;&#21033;&#30410;&#19981;&#23545;&#40784;&#30340;&#30446;&#26631;&#65292;&#24182;&#37319;&#29992;&#27450;&#39575;&#24615;&#34892;&#20026;&#21644;&#26435;&#21147;&#36861;&#27714;&#31574;&#30053;&#12290;&#38450;&#27490;&#36825;&#31181;&#24773;&#20917;&#30340;&#21457;&#29983;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2209.00626</link><description>&lt;p&gt;
&#20174;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#35282;&#30475;&#24453;&#23545;&#40784;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The alignment problem from a deep learning perspective. (arXiv:2209.00626v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00626
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20986;&#29616;&#21487;&#33021;&#20250;&#23548;&#33268;&#20854;&#36861;&#27714;&#19982;&#20154;&#31867;&#21033;&#30410;&#19981;&#23545;&#40784;&#30340;&#30446;&#26631;&#65292;&#24182;&#37319;&#29992;&#27450;&#39575;&#24615;&#34892;&#20026;&#21644;&#26435;&#21147;&#36861;&#27714;&#31574;&#30053;&#12290;&#38450;&#27490;&#36825;&#31181;&#24773;&#20917;&#30340;&#21457;&#29983;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#26469;&#20960;&#21313;&#24180;&#20869;&#65292;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#21487;&#33021;&#22312;&#35768;&#22810;&#20851;&#38190;&#20219;&#21153;&#19978;&#36229;&#36234;&#20154;&#31867;&#33021;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#27809;&#26377;&#22823;&#37327;&#21162;&#21147;&#26469;&#38450;&#27490;&#23427;&#65292;AGIs&#21487;&#33021;&#20250;&#23398;&#20250;&#36861;&#27714;&#19982;&#20154;&#31867;&#21033;&#30410;&#20914;&#31361;&#65288;&#21363;&#19981;&#23545;&#40784;&#65289;&#30340;&#30446;&#26631;&#12290;&#22914;&#26524;&#20687;&#29616;&#22312;&#26368;&#20855;&#33021;&#21147;&#30340;&#27169;&#22411;&#19968;&#26679;&#36827;&#34892;&#35757;&#32451;&#65292;AGIs&#21487;&#33021;&#20250;&#23398;&#20250;&#27450;&#39575;&#24615;&#22320;&#34892;&#21160;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#22870;&#21169;&#65292;&#23398;&#20250;&#22312;&#20854;&#24494;&#35843;&#20998;&#24067;&#20043;&#22806;&#36827;&#34892;&#20869;&#37096;&#30446;&#26631;&#30340;&#27867;&#21270;&#65292;&#24182;&#21033;&#29992;&#23547;&#27714;&#26435;&#21147;&#30340;&#31574;&#30053;&#26469;&#36861;&#27714;&#36825;&#20123;&#30446;&#26631;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#36825;&#20123;&#29305;&#24615;&#30340;&#26032;&#35777;&#25454;&#12290;&#20855;&#26377;&#36825;&#20123;&#29305;&#24615;&#30340;AGIs&#23558;&#24456;&#38590;&#36827;&#34892;&#23545;&#40784;&#65292;&#21363;&#20351;&#22312;&#19981;&#23545;&#40784;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#33021;&#34920;&#29616;&#20986;&#23545;&#40784;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#19981;&#23545;&#40784;&#30340;AGIs&#30340;&#37096;&#32626;&#22914;&#20309;&#21487;&#33021;&#20250;&#19981;&#21487;&#36870;&#22320;&#21066;&#24369;&#20154;&#31867;&#23545;&#19990;&#30028;&#30340;&#25511;&#21046;&#65292;&#24182;&#31616;&#35201;&#22238;&#39038;&#20102;&#26088;&#22312;&#38450;&#27490;&#36825;&#31181;&#32467;&#26524;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In coming decades, artificial general intelligence (AGI) may surpass human capabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIs could learn to pursue goals that conflict (i.e., are misaligned) with human interests. If trained like today's most capable models, AGIs could learn to act deceptively to receive higher reward, learn internally-represented goals which generalize beyond their fine-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. AGIs with these properties would be difficult to align and may appear aligned even when they are not. We outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and briefly review research directions aimed at preventing this outcome.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#25277;&#35937;&#35268;&#21010;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2208.07737</link><description>&lt;p&gt;
&#23398;&#20064;&#26377;&#25928;&#30340;&#36873;&#25321;&#39044;&#27979;&#30340;&#25277;&#35937;&#35268;&#21010;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Efficient Abstract Planning Models that Choose What to Predict. (arXiv:2208.07737v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07737
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#25277;&#35937;&#35268;&#21010;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#31354;&#38388;&#30340;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#21452;&#23618;&#35268;&#21010;&#65292;&#20854;&#20013;&#22312;&#29615;&#22659;&#30340;&#25277;&#35937;&#23618;&#19978;&#36827;&#34892;&#39640;&#32423;&#25628;&#32034;&#20197;&#25351;&#23548;&#20302;&#32423;&#20915;&#31574;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23398;&#20064;&#31526;&#21495;&#25805;&#20316;&#21644;&#31070;&#32463;&#37319;&#26679;&#22120;&#30340;&#25277;&#35937;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#36825;&#31181;&#21452;&#23618;&#35268;&#21010;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#31526;&#21495;&#25805;&#20316;&#23398;&#20064;&#26041;&#27861;&#22312;&#35768;&#22810;&#26426;&#22120;&#20154;&#39046;&#22495;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#30340;&#34892;&#21160;&#24448;&#24448;&#20250;&#24341;&#36215;&#25277;&#35937;&#29366;&#24577;&#20013;&#22823;&#37327;&#26080;&#20851;&#30340;&#21464;&#21270;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#35797;&#22270;&#23398;&#20064;&#20934;&#30830;&#39044;&#27979;&#25277;&#35937;&#29366;&#24577;&#20013;&#25152;&#26377;&#35266;&#23519;&#21040;&#30340;&#21464;&#21270;&#30340;&#25805;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;"&#36873;&#25321;&#35201;&#39044;&#27979;"&#30340;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#21482;&#23545;&#23454;&#29616;&#25351;&#23450;&#30446;&#26631;&#30340;&#25277;&#35937;&#35268;&#21010;&#25152;&#24517;&#38656;&#30340;&#21464;&#21270;&#24314;&#27169;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#20986;&#23548;&#33268;10&#20010;&#19981;&#21516;&#28151;&#21512;&#20219;&#21153;&#19978;&#30340;&#39640;&#25928;&#35268;&#21010;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
An effective approach to solving long-horizon tasks in robotics domains with continuous state and action spaces is bilevel planning, wherein a high-level search over an abstraction of an environment is used to guide low-level decision-making. Recent work has shown how to enable such bilevel planning by learning abstract models in the form of symbolic operators and neural samplers. In this work, we show that existing symbolic operator learning approaches fall short in many robotics domains where a robot's actions tend to cause a large number of irrelevant changes in the abstract state. This is primarily because they attempt to learn operators that exactly predict all observed changes in the abstract state. To overcome this issue, we propose to learn operators that 'choose what to predict' by only modelling changes necessary for abstract planning to achieve specified goals. Experimentally, we show that our approach learns operators that lead to efficient planning across 10 different hybr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#40065;&#26834;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#29305;&#24449;&#21644;&#20687;&#32032;&#32423;&#21035;&#30340;&#20998;&#24067;&#65292;&#25429;&#25417;&#23545;&#25239;&#25200;&#21160;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.08589</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#23618;&#20998;&#24067;&#24863;&#30693;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Distribution-Aware Testing of Deep Learning. (arXiv:2205.08589v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#40065;&#26834;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#29305;&#24449;&#21644;&#20687;&#32032;&#32423;&#21035;&#30340;&#20998;&#24067;&#65292;&#25429;&#25417;&#23545;&#25239;&#25200;&#21160;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#20854;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#38754;&#23545;&#23545;&#25239;&#25200;&#21160;&#65288;&#21363;&#23545;&#25163;&#26679;&#26412;&#65289;&#26102;&#24448;&#24448;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#25915;&#20987;&#21644;&#27979;&#35797;&#26041;&#27861;&#26469;&#26816;&#27979;&#23545;&#25163;&#26679;&#26412;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#36755;&#20837;&#20998;&#24067;&#21644;&#25200;&#21160;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;&#32467;&#26524;&#65292;&#26816;&#27979;&#21040;&#30340;&#23545;&#25163;&#26679;&#26412;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#30456;&#20851;&#65292;&#25110;&#32773;&#23545;&#20154;&#31867;&#35266;&#23519;&#32773;&#26469;&#35828;&#21487;&#33021;&#30475;&#36215;&#26469;&#19981;&#30495;&#23454;&#12290;&#36825;&#23548;&#33268;&#27979;&#35797;&#36164;&#28304;&#28010;&#36153;&#22312;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24456;&#23569;&#21457;&#29983;&#30340;&#31232;&#26377;&#23545;&#25163;&#26679;&#26412;&#19978;&#65292;&#38480;&#21046;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#25552;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#23545;&#25163;&#26679;&#26412;&#65292;&#32771;&#34385;&#21040;&#20102;&#29305;&#24449;&#32423;&#21035;&#20998;&#24067;&#21644;&#20687;&#32032;&#32423;&#21035;&#20998;&#24067;&#65292;&#25429;&#25417;&#20102;&#23545;&#25239;&#25200;&#21160;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;&#36825;&#20004;&#31181;&#32771;&#34385;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#26426;&#21046;&#26469;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) is increasingly used in safety-critical applications, raising concerns about its reliability. DL suffers from a well-known problem of lacking robustness, especially when faced with adversarial perturbations known as Adversarial Examples (AEs). Despite recent efforts to detect AEs using advanced attack and testing methods, these approaches often overlook the input distribution and perceptual quality of the perturbations. As a result, the detected AEs may not be relevant in practical applications or may appear unrealistic to human observers. This can waste testing resources on rare AEs that seldom occur during real-world use, limiting improvements in DL model dependability.  In this paper, we propose a new robustness testing approach for detecting AEs that considers both the feature level distribution and the pixel level distribution, capturing the perceptual quality of adversarial perturbations. The two considerations are encoded by a novel hierarchical mechanism. Fir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65288;IDML&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#25913;&#36827;&#20102;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.04449</link><description>&lt;p&gt;
&#22270;&#20687;&#26816;&#32034;&#30340;&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Introspective Deep Metric Learning for Image Retrieval. (arXiv:2205.04449v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.04449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65288;IDML&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#25913;&#36827;&#20102;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65288;IDML&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#22270;&#20687;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27604;&#36739;&#12290;&#20256;&#32479;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#22312;&#22270;&#20687;&#20043;&#38388;&#20135;&#29983;&#33258;&#20449;&#30340;&#35821;&#20041;&#36317;&#31163;&#65292;&#32780;&#19981;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#19968;&#20010;&#22909;&#30340;&#30456;&#20284;&#24615;&#27169;&#22411;&#24212;&#35813;&#35880;&#24910;&#32771;&#34385;&#35821;&#20041;&#24046;&#24322;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#27169;&#31946;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35821;&#20041;&#23884;&#20837;&#21644;&#20276;&#38543;&#30340;&#19981;&#30830;&#23450;&#24615;&#23884;&#20837;&#26469;&#34920;&#31034;&#22270;&#20687;&#65292;&#20998;&#21035;&#25551;&#36848;&#22270;&#20687;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#27169;&#31946;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#29992;&#20110;&#22312;&#32771;&#34385;&#22270;&#20687;&#30340;&#35821;&#20041;&#24046;&#24322;&#21644;&#27169;&#31946;&#24230;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30456;&#20284;&#24615;&#21028;&#26029;&#12290;&#25152;&#25552;&#20986;&#30340;IDML&#26694;&#26550;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#25913;&#36827;&#20102;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CUB-200-2011&#65292;Cars196&#21644;Stanford Online&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an introspective deep metric learning (IDML) framework for uncertainty-aware comparisons of images. Conventional deep metric learning methods produce confident semantic distances between images regardless of the uncertainty level. However, we argue that a good similarity model should consider the semantic discrepancies with caution to better deal with ambiguous images for more robust training. To achieve this, we propose to represent an image using not only a semantic embedding but also an accompanying uncertainty embedding, which describes the semantic characteristics and ambiguity of an image, respectively. We further propose an introspective similarity metric to make similarity judgments between images considering both their semantic differences and ambiguities. The proposed IDML framework improves the performance of deep metric learning through uncertainty modeling and attains state-of-the-art results on the widely used CUB-200-2011, Cars196, and Stanford Online
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#21151;&#33021;&#19981;&#21464;&#36335;&#24452;&#65288;FIP&#65289;&#30340;&#24046;&#20998;&#20960;&#20309;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#12289;&#36830;&#32493;&#36866;&#24212;&#65292;&#20197;&#24212;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#30446;&#26631;&#21644;&#32593;&#32476;&#31232;&#30095;&#21270;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2205.00334</link><description>&lt;p&gt;
&#36890;&#36807;&#36941;&#21382;&#21151;&#33021;&#19981;&#21464;&#36335;&#24452;&#65292;&#26500;&#24314;&#28789;&#27963;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Engineering flexible machine learning systems by traversing functionally-invariant paths. (arXiv:2205.00334v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#21151;&#33021;&#19981;&#21464;&#36335;&#24452;&#65288;FIP&#65289;&#30340;&#24046;&#20998;&#20960;&#20309;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#12289;&#36830;&#32493;&#36866;&#24212;&#65292;&#20197;&#24212;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#30446;&#26631;&#21644;&#32593;&#32476;&#31232;&#30095;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#22312;&#22522;&#30784;&#27169;&#22411;&#33539;&#20363;&#20013;&#65292;&#22823;&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;BERT&#12289;GPT3/4&#12289;Bloom&#12289;ViT&#65289;&#36890;&#36807;&#33258;&#30417;&#30563;&#20219;&#21153;&#65288;&#22914;&#35789;&#25110;&#22270;&#20687;&#23631;&#34109;&#65289;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#24494;&#35843;&#36866;&#24212;&#20110;&#19979;&#28216;&#29992;&#25143;&#24212;&#29992;&#65292;&#21253;&#25324;&#25351;&#20196;&#36319;&#38543;&#21644;&#38382;&#31572;&#12290;&#34429;&#28982;&#26377;&#35768;&#22810;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65288;&#22914;&#20302;&#31209;&#26435;&#37325;&#26356;&#26032;&#31574;&#30053;&#65292;&#22914;LoRA&#65289;&#65292;&#20294;&#20173;&#28982;&#23545;&#23454;&#29616;&#32593;&#32476;&#36866;&#24212;&#24615;&#32780;&#19981;&#25439;&#22833;&#30693;&#35782;&#30340;&#25968;&#23398;&#21407;&#29702;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24046;&#20998;&#20960;&#20309;&#26694;&#26550;&#65292;&#21151;&#33021;&#19981;&#21464;&#36335;&#24452;&#65288;FIP&#65289;&#65292;&#20026;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#30446;&#26631;&#21644;&#32593;&#32476;&#31232;&#30095;&#21270;&#30446;&#26631;&#25552;&#20379;&#28789;&#27963;&#21644;&#36830;&#32493;&#30340;&#31070;&#32463;&#32593;&#32476;&#36866;&#24212;&#12290;&#25105;&#20204;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#31354;&#38388;&#26500;&#24819;&#20026;&#19968;&#20010;&#26354;&#29575;&#30340;&#40654;&#26364;&#27969;&#24418;&#65292;&#24182;&#37197;&#22791;&#20102;&#19968;&#20010;&#24230;&#35268;&#24352;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have emerged as the state of the art neural network architecture for natural language processing and computer vision. In the foundation model paradigm, large transformer models (BERT, GPT3/4, Bloom, ViT) are pre-trained on self-supervised tasks such as word or image masking, and then, adapted through fine-tuning for downstream user applications including instruction following and Question Answering. While many approaches have been developed for model fine-tuning including low-rank weight update strategies (eg. LoRA), underlying mathematical principles that enable network adaptation without knowledge loss remain poorly understood. Here, we introduce a differential geometry framework, functionally invariant paths (FIP), that provides flexible and continuous adaptation of neural networks for a range of machine learning goals and network sparsification objectives. We conceptualize the weight space of a neural network as a curved Riemannian manifold equipped with a metric tenso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#30693;&#35782;&#39537;&#21160;&#30340;&#20998;&#23376;&#23398;&#20064;&#65292;&#20174;&#33539;&#24335;&#36716;&#31227;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;&#20854;&#19981;&#21516;&#33539;&#24335;&#30340;&#20998;&#31867;&#21644;&#26041;&#27861;&#35770;&#65292;&#24182;&#20998;&#26512;&#20102;&#39046;&#22495;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2202.10587</link><description>&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#20998;&#23376;&#23398;&#20064;&#65306;&#33539;&#24335;&#36716;&#31227;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge-informed Molecular Learning: A Survey on Paradigm Transfer. (arXiv:2202.10587v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#30693;&#35782;&#39537;&#21160;&#30340;&#20998;&#23376;&#23398;&#20064;&#65292;&#20174;&#33539;&#24335;&#36716;&#31227;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;&#20854;&#19981;&#21516;&#33539;&#24335;&#30340;&#20998;&#31867;&#21644;&#26041;&#27861;&#35770;&#65292;&#24182;&#20998;&#26512;&#20102;&#39046;&#22495;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#23398;&#20064;&#65292;&#26174;&#33879;&#25512;&#21160;&#20102;&#29983;&#29289;&#21270;&#23398;&#39046;&#22495;&#20869;&#30340;&#20998;&#23376;&#30740;&#31350;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#31867;&#30740;&#31350;&#30340;&#24314;&#27169;&#20027;&#35201;&#22260;&#32469;&#30528;&#19968;&#20123;&#33539;&#24335;&#23637;&#24320;&#12290;&#20363;&#22914;&#65292;&#39044;&#27979;&#33539;&#24335;&#32463;&#24120;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#31561;&#20219;&#21153;&#12290;&#20026;&#20102;&#22686;&#24378;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#29983;&#25104;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#23398;&#32773;&#20204;&#23558;&#29983;&#21270;&#39046;&#22495;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#36825;&#20123;&#20998;&#23376;&#30740;&#31350;&#27169;&#22411;&#20013;&#12290;&#36825;&#31181;&#34701;&#21512;&#24341;&#21457;&#20102;&#33539;&#24335;&#36716;&#31227;&#30340;&#39134;&#36895;&#21457;&#23637;&#65292;&#21363;&#36890;&#36807;&#23558;&#19968;&#20010;&#20998;&#23376;&#23398;&#20064;&#20219;&#21153;&#36716;&#21270;&#20026;&#21478;&#19968;&#20010;&#20219;&#21153;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#33539;&#24335;&#21576;&#29616;&#20986;&#36880;&#28176;&#36235;&#20110;&#32479;&#19968;&#30340;&#36235;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#33539;&#24335;&#36716;&#31227;&#30340;&#35282;&#24230;&#65292;&#23545;&#30693;&#35782;&#39537;&#21160;&#30340;&#20998;&#23376;&#23398;&#20064;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#33539;&#24335;&#36827;&#34892;&#20998;&#31867;&#12289;&#23457;&#35270;&#23427;&#20204;&#30340;&#26041;&#27861;&#35770;&#65292;&#24182;&#21078;&#26512;&#20102;&#39046;&#22495;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning, notably deep learning, has significantly propelled molecular investigations within the biochemical sphere. Traditionally, modeling for such research has centered around a handful of paradigms. For instance, the prediction paradigm is frequently deployed for tasks such as molecular property prediction. To enhance the generation and decipherability of purely data-driven models, scholars have integrated biochemical domain knowledge into these molecular study models. This integration has sparked a surge in paradigm transfer, which is solving one molecular learning task by reformulating it as another one. With the emergence of Large Language Models, these paradigms have demonstrated an escalating trend towards harmonized unification. In this work, we delineate a literature survey focused on knowledge-informed molecular learning from the perspective of paradigm transfer. We classify the paradigms, scrutinize their methodologies, and dissect the contribution of domain knowle
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LoNLI&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#38598;&#20307;&#27979;&#35797;NLI&#30340;&#19981;&#21516;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22823;&#22411;&#27979;&#35797;&#24179;&#21488;&#21644;&#30456;&#20851;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#21333;&#29420;&#27979;&#35797;&#21644;&#20998;&#26512;&#22810;&#20010;&#25512;&#29702;&#32500;&#24230;&#30340;&#33021;&#21147;&#65292;&#24182;&#36827;&#34892;&#36328;&#33021;&#21147;&#20449;&#24687;&#20869;&#23481;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#24110;&#21161;&#28145;&#20837;&#20102;&#35299;NLI&#21644;NLU&#39046;&#22495;&#30340;&#36923;&#36753;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2112.02333</link><description>&lt;p&gt;
LoNLI&#65306;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27979;&#35797;NLI&#30340;&#22810;&#26679;&#21270;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
LoNLI: An Extensible Framework for Testing Diverse Logical Reasoning Capabilities for NLI. (arXiv:2112.02333v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.02333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LoNLI&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#38598;&#20307;&#27979;&#35797;NLI&#30340;&#19981;&#21516;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22823;&#22411;&#27979;&#35797;&#24179;&#21488;&#21644;&#30456;&#20851;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#21333;&#29420;&#27979;&#35797;&#21644;&#20998;&#26512;&#22810;&#20010;&#25512;&#29702;&#32500;&#24230;&#30340;&#33021;&#21147;&#65292;&#24182;&#36827;&#34892;&#36328;&#33021;&#21147;&#20449;&#24687;&#20869;&#23481;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#24110;&#21161;&#28145;&#20837;&#20102;&#35299;NLI&#21644;NLU&#39046;&#22495;&#30340;&#36923;&#36753;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#34987;&#35748;&#20026;&#26159;&#27979;&#35797;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#30340;&#20195;&#34920;&#24615;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#20197;&#38598;&#20307;&#20294;&#26377;&#20998;&#31867;&#22320;&#27979;&#35797;NLI&#65288;&#20197;&#21450;NLU&#30340;&#24310;&#20280;&#65289;&#25152;&#38656;&#30340;&#22810;&#26679;&#21270;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#21463;&#34892;&#20026;&#27979;&#35797;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21322;&#21512;&#25104;&#30340;&#22823;&#22411;&#27979;&#35797;&#24179;&#21488;&#65288;363&#20010;&#27169;&#26495;&#65292;363k&#20010;&#20363;&#23376;&#65289;&#21644;&#19968;&#20010;&#30456;&#20851;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20197;&#19979;&#23454;&#29992;&#21151;&#33021;&#65306;1&#65289;&#21333;&#29420;&#27979;&#35797;&#21644;&#20998;&#26512;17&#20010;&#25512;&#29702;&#32500;&#24230;&#30340;&#25512;&#29702;&#33021;&#21147;&#65288;&#21253;&#25324;&#35821;&#29992;&#25512;&#29702;&#65289;&#65307;2&#65289;&#35774;&#35745;&#23454;&#39564;&#20197;&#30740;&#31350;&#36328;&#33021;&#21147;&#20449;&#24687;&#20869;&#23481;&#65288;&#25490;&#38500;&#19968;&#20010;&#25110;&#22686;&#21152;&#19968;&#20010;&#65289;&#65307;3&#65289;&#21512;&#25104;&#30340;&#24615;&#36136;&#20351;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;&#20154;&#24037;&#21644;&#20559;&#35265;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#33258;&#21160;&#21270;&#27979;&#35797;&#29992;&#20363;&#23454;&#20363;&#21270;&#26694;&#26550;&#65288;CheckList&#65289;&#21644;&#19968;&#20010;&#23450;&#20041;&#26126;&#30830;&#30340;&#33021;&#21147;&#20998;&#31867;&#27861;&#65292;&#20197;&#28085;&#30422;&#26356;&#24191;&#27867;&#12289;&#26356;&#22256;&#38590;&#30340;&#27979;&#35797;&#29992;&#20363;&#33539;&#22260;&#65292;&#21516;&#26102;&#21464;&#21270;&#30528;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Inference (NLI) is considered a representative task to test natural language understanding (NLU). In this work, we propose an extensible framework to collectively yet categorically test diverse Logical reasoning capabilities required for NLI (and, by extension, NLU). Motivated by behavioral testing, we create a semi-synthetic large test bench (363 templates, 363k examples) and an associated framework that offers the following utilities: 1) individually test and analyze reasoning capabilities along 17 reasoning dimensions (including pragmatic reasoning); 2) design experiments to study cross-capability information content (leave one out or bring one in); and 3) the synthetic nature enables us to control for artifacts and biases. We extend a publicly available framework of automated test case instantiation from free-form natural language templates (CheckList) and a well-defined taxonomy of capabilities to cover a wide range of increasingly harder test cases while varying 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27880;&#20837;&#24102;&#26377;&#27491;&#30830;&#26631;&#31614;&#30340;&#27602;&#21270;&#22270;&#20687;&#26469;&#23454;&#29616;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#35813;&#25915;&#20987;&#38590;&#20197;&#34987;&#26816;&#27979;&#21040;&#12290;&#36890;&#36807;&#20248;&#21270;&#21704;&#24076;&#30721;&#23398;&#20064;&#21644;&#20351;&#29992;&#26377;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#24615;&#36148;&#29255;&#20316;&#20026;&#21518;&#38376;&#35302;&#21457;&#22120;&#65292;&#21487;&#20197;&#25552;&#39640;&#25915;&#20987;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2109.08868</link><description>&lt;p&gt;
&#36890;&#36807;&#24178;&#20928;&#26631;&#31614;&#25968;&#25454;&#27602;&#21270;&#23545;&#22522;&#20110;&#21704;&#24076;&#30340;&#22270;&#20687;&#26816;&#32034;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attack on Hash-based Image Retrieval via Clean-label Data Poisoning. (arXiv:2109.08868v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.08868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27880;&#20837;&#24102;&#26377;&#27491;&#30830;&#26631;&#31614;&#30340;&#27602;&#21270;&#22270;&#20687;&#26469;&#23454;&#29616;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#35813;&#25915;&#20987;&#38590;&#20197;&#34987;&#26816;&#27979;&#21040;&#12290;&#36890;&#36807;&#20248;&#21270;&#21704;&#24076;&#30721;&#23398;&#20064;&#21644;&#20351;&#29992;&#26377;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#24615;&#36148;&#29255;&#20316;&#20026;&#21518;&#38376;&#35302;&#21457;&#22120;&#65292;&#21487;&#20197;&#25552;&#39640;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#26399;&#21518;&#38376;&#28145;&#24230;&#21704;&#24076;&#27169;&#22411;&#22312;&#21407;&#22987;&#26597;&#35810;&#22270;&#20687;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#24182;&#22312;&#20986;&#29616;&#29305;&#23450;&#35302;&#21457;&#27169;&#24335;&#26102;&#36820;&#22238;&#24102;&#26377;&#30446;&#26631;&#26631;&#31614;&#30340;&#22270;&#20687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#28102;&#25200;&#21160;&#24341;&#36215;&#30340;&#21518;&#38376;&#25915;&#20987;&#65288;CIBA&#65289;&#12290;&#23427;&#23558;&#23569;&#37327;&#24102;&#26377;&#27491;&#30830;&#26631;&#31614;&#30340;&#27602;&#21270;&#22270;&#20687;&#27880;&#20837;&#21040;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#20351;&#24471;&#25915;&#20987;&#38590;&#20197;&#34987;&#26816;&#27979;&#21040;&#12290;&#20026;&#20102;&#21046;&#20316;&#27602;&#21270;&#22270;&#20687;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#28151;&#28102;&#25200;&#21160;&#26469;&#24178;&#25200;&#21704;&#24076;&#30721;&#30340;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#21704;&#24076;&#27169;&#22411;&#21487;&#20197;&#26356;&#22810;&#22320;&#20102;&#35299;&#35302;&#21457;&#22120;&#12290;&#28151;&#28102;&#25200;&#21160;&#22312;&#27721;&#26126;&#31354;&#38388;&#20013;&#36890;&#36807;&#20248;&#21270;&#31867;&#20869;&#31163;&#25955;&#24230;&#21644;&#31867;&#38388;&#20559;&#31227;&#29983;&#25104;&#65292;&#20854;&#20960;&#20046;&#26080;&#27861;&#23519;&#35273;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26377;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#24615;&#36148;&#29255;&#20316;&#20026;&#21518;&#38376;&#35302;&#21457;&#22120;&#26469;&#25552;&#39640;&#25915;&#20987;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;CIBA&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/KuofengGao/CIBA&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
A backdoored deep hashing model is expected to behave normally on original query images and return the images with the target label when a specific trigger pattern presents. To this end, we propose the confusing perturbations-induced backdoor attack (CIBA). It injects a small number of poisoned images with the correct label into the training data, which makes the attack hard to be detected. To craft the poisoned images, we first propose the confusing perturbations to disturb the hashing code learning. As such, the hashing model can learn more about the trigger. The confusing perturbations are imperceptible and generated by optimizing the intra-class dispersion and inter-class shift in the Hamming space. We then employ the targeted adversarial patch as the backdoor trigger to improve the attack performance. We have conducted extensive experiments to verify the effectiveness of our proposed CIBA. Our code is available at https://github.com/KuofengGao/CIBA.
&lt;/p&gt;</description></item></channel></rss>